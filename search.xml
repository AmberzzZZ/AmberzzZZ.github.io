<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SegmentAnything]]></title>
    <url>%2F2023%2F05%2F15%2FSegmentAnything%2F</url>
    <content type="text"><![CDATA[overview task：propose a prompt-able segmentation task model：build SAM model，promptable，enabling zero-shot dataset：1B数据集，滚标注的半自动标注方案 introduction foundation model models that can generalize to unseen/untrained tasks and data often implemented with prompt engineering 图像领域最有名的就是CLIP/Align：aligns paired text and images from the web use contrastive learning SAM也被构建成foundation model pre-train it on a broad dataset，enables powerful generalization aim to solve a range of downstream segmentation problems on new data distributions using prompt engineering model constrains flexible prompt：point, box, and mask / initial results of free-form text prompts real-time：一张图的embedding可以被各种prompt复用，轻量的prompt encoder&amp;mask decoder ambiguity-aware：一个prompt可以预测多个mask，allowing SAM to naturally handle ambiguity data engine stage1：assisted-manual，传统滚标注 stage2：semi-automatic，用prompt的方式自动标注部分object，人工标注其余的 stage3：fully automatic，将prompt设定为a regular grid of foreground points，自动全图标注 Segment Anything Task promptable segmentation task prompt：a set of foreground / background points, a rough box or mask, free-form text… return a valid segmentation mask：multiple object when ambiguous pre-training input：for each training sample，simulates a sequence of prompts (e.g., points, boxes, masks) supervision：compares the model’s mask predictions against the ground truth zero-shot transfer at inference time，模型已经具备对任何prompt作出相应的能力 thus downstream tasks can be solved by engineering appropriate prompts，所以downstream tasks可以建模成prompts engineering的任务 Segment Anything Model image encoder MAE pretrained ViT：ViT-H/16 with 14x14 windowed attention，1024×1024 minimally adapted to process high resolution inputs outputs x16 image features：64x64 into image embeddings：conv1,dim256 - LN - conv3-dim256 - LN prompt encoder sparse prompts: point: a positional embedding + learned bg/fg embedding box: an embedding pair, (1) a left-top corner positional embedding + earned left-top corner embedding, (2) a bottom-right points/box: positional encodings + learned embeddings for each prompt type text: any text encoder from CLIP dim256 dense prompts: input x4 ds masks: downscale additional x4 2x2,s2,dim4 conv - LN - GeLU 2x2,s2,dim16 conv - LN - GeLU 1x1,s1,dim256 conv - LN - GeLU elewise-add on image embeddings if there is no mask prompt: add a learned ‘no mask’ mask embedding mask decoder insert a learned output token embedding：类似于cls token的东西，Nx256 use a two-layer decoder 4 steps inside each decoder self-attention on the tokens cross-attention from tokens to images：QKVdim128 a point-wise MLP updates each token cross-attention from the image to tokens：QKVdim128 MLP：drop0.1，with residual，use LN，dim2048 geometric和task type的强约束 image emb每次参与attention都要加上positional encodings tokens每次参与attention都要加上原始的original prompt tokens dynamic prediction head upsample the updated image embedding by 4× with two transposed convs 2×2, s2, dim64 TransConv - GeLU - LN - 2×2, s2, dim32 TransConv - GeLU 得到upscaled mask embedding，64x64x32 用updated image embedding和updated tokens再做一次attn，提取output token，经过一个3-layer MLP得到一个vector：Nx32 最后通过dot product得到Nx64x64的prediction mask ambiguity-aware use 3 output tokens，predict 3 masks(whole, part, and subpart) 计算3组mask loss，但是只回传the lowest loss的梯度 在output tokens上再接一个small head，用来estimates IOU，inference time用这个IoU prediction来rank3个mask loss mask loss：focal loss*20 + dice loss*1 iou loss：pred iou和mask iou的MSE Training algorithm for one sample init prompt：随机选取points/boxes作为input prompt points从gt masks中随机抽 boxes基于gt masks的外接框做随机noisy deviation 接下来的points从pred mask和gt mask的error region中抽取 然后把pred mask也作为一个mask prompt输入给model：用的是unthresholded mask logits，而不是binary mask 这样迭代8+2个iteration]]></content>
      <tags>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[knowledge distillation]]></title>
    <url>%2F2022%2F11%2F05%2Fknowledge-distillation%2F</url>
    <content type="text"><![CDATA[paper collection [Similarity KD 2019] Similarity-Preserving Knowledge Distillation [CWD 2020] Channel-wise Knowledge Distillation for Dense Prediction Similarity-Preserving Knowledge Distillation 与其他KD methods的区别： the student is not required to mimic the representation space of the teacher, but rather to preserve the pairwise similarities in its own representation space 监督student和teacher的similarity 适用场景： feature shape对不齐：channel/stride，也可以用1x1 conv对齐 CNN/transformer之间的KD，模型本质diversity就不同 method given activation map（feature）$F$ compute similarity：$F*F^T/L2norm(F)$ compute KD loss：$\frac{1}{b^2}MSE(t,s)$ code 123456789101112131415161718import torchimport torch.nn as nnimport torch.nn.functional as Fdef similarity_loss(self, f_s, f_t): bs = f_s.shape[0] f_s = f_s.view(bs, -1) f_t = f_t.view(bs, -1) G_s = f_s * f_s.T # bxb G_s = F.normalize(G_s, p=2, dim=1) G_t = f_t * f_t.T G_t = F.normalize(G_t, p=2, dim=1) G_diff = G_t - G_s loss = (G_diff * G_diff).view(-1).sum() / (bs * bs) return loss 其他KD methods response based：KL divergence loss 作用于softmax以后的probs（可以有temperature factor） $L_{RD} (p_t, p_s) = L_R (p_t, p_s)$ $L_R()$通常是KL divergence loss KL divergence loss 123456789101112# pred在调用KLDivLoss方法计算loss时先做log，防止先两个normed prob先做除法损失精度pred = F.log_softmax(torch.randn(3, 5, requires_grad=True))target = F.softmax(torch.rand(3, 5))loss = nn.KLDivLoss(reduction="batchmean")(pred, target)# target在调用KLDivLoss时也可以先做loglog_target = F.log_softmax(torch.rand(3, 5))output = nn.KLDivLoss(reduction="batchmean", log_target=True)(pred, log_target)def kl_categorical(p_logit, q_logit): p = F.softmax(p_logit, dim=-1) _kl = torch.sum(p * (F.log_softmax(p_logit, dim=-1) - F.log_softmax(q_logit, dim=-1)), -1) return torch.mean(_kl) feature based $L_{FD} (f_t, f_s) = L_F (\Phi_s(f_t), \Phi_s(f_s))$ $\Phi()$是transform function，用来align feature dimension $L_F()$是similarity function，通常可以是L1、L2、L_CE、L_MMD MMD：maximum mean discrepancy]]></content>
  </entry>
  <entry>
    <title><![CDATA[light-weight transfomers]]></title>
    <url>%2F2022%2F11%2F05%2Flight-weight-ViTs%2F</url>
    <content type="text"><![CDATA[papers [tinyViT ECCV2022] TinyViT: Fast Pretraining Distillation for Small Vision Transformers：微软，swin的todevice版本，用CLIP做unlabel KD [topformer CVPR2022] TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation： [mobileViT ICLR2022] MOBILEVIT: LIGHT-WEIGHT, GENERAL-PURPOSE, AND MOBILE-FRIENDLY VISION TRANSFORMER：苹果 [mobileformer CVPR2022] Mobile-Former: Bridging MobileNet and Transformer：微软 TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation abstract task：semantic segmentation take advantage of CNNs &amp; ViTs CNN-based Token Pyramid Module：MobileNetV2 blocks a ViT-based module Semantics Extractor verified on ADE20K：比mobileNetV3高5%的miou，同时latency更低 method overview Token Pyramid Module：输入image，输出token pyramid Semantics Extractor：输入token pyramid，输出scale-aware semantics Semantics Injection Module：将global semantics融合进对应level的local tokens里面，得到augmented representations Segmentation Head：fuse and predict Token Pyramid Module use stacked MobileNet blocks来生成token pyramids 不是为了丰富的语义信息，也不是为了更大的感受野，只是为了得到multi-scale tokens，所以用比较少的层 输入512x512的图 输出x4/x8/x16/x32的feature map 然后将multi-scale的features统一avg pool到x64，然后concat by dim，得到[b,(16+32+64+96),64,64]的tokens Scale-aware Semantics Extractor 这部分用vision transformer来提取语义信息 首先为了keep the shape，将linear layer换成1x1 conv，把layernorm换成了batchnorm 所有ViT-stype GeLU换成ReLU6 MSA的QK投影维度为16，V投影维度为32，降低similarity的计算量 FFN的两层1x1卷积之间加了一个depth-wise卷积 Semantics Injection Module fuse local tokens 和 scale-aware semantics to alleviate the semantic gap local token经过一个1x1 conv-bn，用semantic token经过一个1x1conv-bn-sigmoid生成的weight map做加权 再加上一个1x1 conv-bn的semantic token （这个semantic token应该是要上采样的吧？） Segmentation Head 所有的low-resolution tokens都上采样的high-resolution tokens的尺度（x8） 然后element-sum 然后两层卷积 实验 datasets ADE20k：1+150，25k（20k/2k/3k） PASCAL Context：1+59，4998/5105 COCO-Stuff10k：9k/1k training details iteration：ADE20k训练160k iteration，PASCAL和COCO训练80k iteration syncBN lr：baselr=0.00012，poly schedule with factor 1.0 weight decay：0.01 TinyViT: Fast Pretraining Distillation for Small Vision Transformers abstract tiny and efficient KD on large-scale datasets 用大模型的logits离线蒸馏 精度：21M：ImageNet 84.8%，use larger resolution：86.5% good transfer ability on downstream tasks linear probe few-shot learning object detection：COCO AP 50.2% overview 离线蒸馏 保存数据增强的参数和teacher logits 方法 Fast Pretraining Distillation direct pretraining small models on massive data does not bring much gains，especially when transferring to downstream tasks，但是蒸馏的boost就十分显著了 一些方法在finetuning阶段进行蒸馏，本文focus on pretraining distillation：inefficient and costly given image x save strong data augmentation A （RandAugment &amp; CutMix） save teacher prediction $\hat y=T(A(x))$ 因为RandAugment内置随机性，同样的增强方式每次结果也不同，因为每个iteration的A都要存下来 用ground truth的one-hot label去蒸馏的效果反而不如unlabeled logits，因为one-hot太overfit了 Sparse soft labels：imagnet21k的logits有21, 841个，为了节约只保存topK logits and indices，其余的用label smoothing Data augmentation encoding： 将一系列data aug params encode成一个scalar d 然后通过decoder将其还原：PCG输入single parameter，输出a sequence of parameters 实际实现的时候，就是存下一个d0，然后用PCG对T&amp;S解码增强参数 Model Architectures adopt hierarchical vision transformer，从大模型开始，定义一系列contraction factors，开始scaling down patch embedding：2个3x3 convs，stride2，pad1 stage1：MBConvs &amp; down sampling blocks stage2/3/4：transformer blocks with window attention attention biases a 3 × 3 depthwise convolution between attention and MLP 常规的residuals、LayerNorm依旧保留，conv的norm是BN，所有activation都是GELU Contraction factors embeded dimension：21M-[96,192,384,576]，11M-[64,128,256,448]，5M-[64,128,160,320] number of blocks：[2,2,6,2] window size：[7,14,7] channel expansion ratio of the MBConv block：4 expansion ratio of MLP：4 head dimension：32]]></content>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tiny dataset]]></title>
    <url>%2F2022%2F10%2F30%2Ftiny-dataset%2F</url>
    <content type="text"><![CDATA[说明：这个文档用来记录一些常用的dataset cityscape 主要用于语义分割 包含5000精细标注（gtFine/2975 training, 500 validation, and 1525 testing）和20000粗标注（gtCoarse） leftImg8bit里面是原图，8-bit LDR 30个类别 数据处理脚本：https://github.com/mcordts/cityscapesScripts （也包含3D标注/right stereo views &amp; disparity） ade20k 用于语义分割 25,000张图像（20ktrain，2k val，3ktest） *.png是原图 *_seg.png是mask：R和G通道编码对象类掩码，通道B对实例对象掩码进行编码 * _.txt是文本描述文件 包含室内外场景 共包含3688个类别，其中高频类别150类（100个thing和50个stuff，占所有像素的89％） coco-stuff COCO-Stuff是对COCO2017数据集中全部164K图片做了像素级的标注 包含80 thing classes, 91 stuff classes and 1 class ‘unlabeled’ 图像共用，标签分为stuffthingmaps_trainval2017.zip / stuff_trainval2017.zip (Stuff-only) / annotations_trainval2017.zip (thing-only)，用灰度图格式保存 PPM-100 人像抠图]]></content>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[real-time semantic segmentation]]></title>
    <url>%2F2022%2F10%2F30%2Freal-time-semantic-segmentation%2F</url>
    <content type="text"><![CDATA[cityscape leaderboard: [PIDNet 2022] PIDNet: A Real-time Semantic Segmentation Network Inspired from PID Controller [SFNet v1 2020] Semantic Flow for Fast and Accurate Scene Parsing [SFNet v2 2022] SFNet: Faster, Accurate, and Domain Agnostic Semantic Segmentation via Semantic Flow [PP-LiteSeg 2022] PP-LiteSeg: A Superior Real-Time Semantic Segmentation Model [DDRNet 2021] Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes [STDC-Seg 2021CVPR] Rethinking BiSeNet For Real-time Semantic Segmentation PP-LiteSeg: A Superior Real-Time Semantic Segmentation Model main contributions propose a Flexible and Lightweight Decoder (FLD)：主要就是FPN的通道数 propose a Unified Attention Fusion Module (UAFM)：强化特征表达 propose a Simple Pyramid Pooling Module (SPPM)：简化PPM，low computation cost real-time networks设计思路 Strengthening feature representations：decoder中low-level和high-level特征的融合方式 Contextual aggregation：花式PPM overview 方法 Flexible and Lightweight Decoder recent lightweight models中的decoder在恢复resolution的过程中通道数保持不变：cause computation redundancy FLD从high-level到low-level逐渐减少通道数 Unified Attention Fusion Module 用一个attention module来产生weight map 然后对不同尺度的特征做加权和 Spatial Attention Module 实际代码中用的这个 在channel维度上做mean &amp; max，两个feature共得到4个[1,h,w]的map，concat在一起[4,h,w] 然后做conv+sigmoid得到weight map [1,h,w] 然后做element-wise的mul&amp;add Channel Attention Module 在spacial map上做avg pooling和max pooling，两个feature共得到4个[c,1,1]的vector，concat在一起[4c,1,1] 然后做conv+sigmoid得到channel importance vec [c,1,1] 然后做channel-wise的mul&amp;add Simple Pyramid Pooling Module 主要改动 reduce intermediate and output channels remove the short-cut replace concat with add 就3个global average pooling，得到1x1、2x2、4x4的map，然后分别conv-bn-relu，然后resize回原来的尺度 然后add &amp; conv 实验 datasets Cityscapes：1+18，5,000（2975/500/1525），原始尺寸是2048x1024 CamVid：11类，701（367/101/233），原始尺寸是960x720 training settings SGD：momentum=0.9 lr：warmup，poly， Cityscapes训练160k，batchsize=16，baselr=0.005，weight decay=5e-4 CamVid训练1k，batchsize=24，baselr=0.01，weight decay=1e-4 dataaug random scale：scale range [0.125, 1.5] / [0.5, 2.5] for Cityscapes/CamVid random crop：crop size=[1024,512] / [960,720] random horizontal flipping / color jitting / normalization SFNet: Faster, Accurate, and Domain Agnostic Semantic Segmentation via Semantic Flow 动机 widely used atrous convolution &amp; feaature pyramid: computation intensive or ineffective we propose SFNet &amp; SFNet-Lite Flow Alignment Module (FAM) to learn sematic flow Gated Dual Flow Alignment Module (GD-FAM): directly align the highest &amp; lowest resolution feature maps speed &amp; accuracy verified on 4 driving datasets (Cityscapes, Mapillary, IDD and BDD) SFNet-Lite-R18 back: 80.1 mIoU / 60 FPS SFNet-Lite-STDC back: 78.8 mIoU / 120 FPS 方法 previous methods FCN 开创性的工作 lack of detailed object boundary information due to down-sampling deeplab atrous convolutions （last 4/5 stage） multiscale feature representation （ASPP） vision transformer 建模成query-based per-segment prediction strong performance but real time inference speed不理想 trade-off maintain detailed resolution：空洞卷积，resolution大计算量就大，很难实时 get features that exhibit strong semantic representation：biFPN style feature merge，有一定提升，但还是不如those hold large feature maps 本文的推测是：ineffective propagation of semantics from deep layers to shallow layers，across level的semantics are not well aligned，粗暴的downsample再upsample，无法恢复细节边缘信息 this paper propose FAM &amp; SFNet 不同stage的feature之间，先做alignment，warp low level feature之后再merge R18-back： 79.8% mIoU / 33 FPS on cityscape DF2-back：77.8% mIoU / 103 FPS propose GD-FAM &amp; SFNet-Lite 为了更快，把密集的FAM换成只做一次的GD-FAM 只merge this highest &amp; lowest尺度的features ResNet-18 back：80.1% mIoU / 49 FPS on cityscape STDCv1-back： 78.7 mIoU / 120 FPS 方法 出发点： the misalignment between feature maps caused by residual connection, repeated downsampling and up- sampling operations inspiration：dynamic upsampling interpolation FAM build in the FPN framework define Semantic Flow：between different levels in a feature pyramid pipeline 首先通道对齐 然后上采样，尺度对齐，$F \in R^{H\times W \times C}$ 然后concat两个level的特征，（但是没有像真正的光流对齐FlowNet一样再concat上grid coordinates） 然后用两层3x3的卷积做semantic flow field的预测，$\Delta_{low-level} \in R^{H\times W\times 2}$ 然后warp low-level feature 然后add &amp; halve 和deformable conv的区别 首先这个offset是通过融合两个尺度特征得到的，DCN是特征自己预测的 其次DCN是为了得到更大的/更自由的reception field，more like attention，本文是为了align feature 可以看到warp &amp; add以后的feature map相比较于直接上采样然后add，更加structurally neat，目标可以有更consistent的representation the whole network backbone ImageNet pretrained ResNet / DF series 4 stages stride2 in the first place per stage Pyramid Pooling Module (PPM)，ASPP/NL is also experimented，后面实验部分说精度不如PPM Aligned FPN decoder encoder那边过来的stage2/3/4的low-level feature，被FAM aligned，然后fused into their bottom levels 最终的x4的prediction feature，concat来自所有尺度的特征，考虑到也存在mis-alignment，本文也实验性的添加了PPM，但是在real-time application中没加 Gated Dual Flow Alignment Module and SFNet-Lite 上述版本的SFNet，在speed上是比BiSegNet慢的，thus we explore more compact decoder takes F1 &amp; F4 as inputs outputs a refined high-resolution map pipeline 首先将F4上采样的F1的resolution 然后concat，然后3x3convs，预测offsets，$\Delta \in R^{H\times W\times 4}$ 然后split成$\Delta_{F1}$和$\Delta_{F4}$，分别给到F1和F4做align 再用F1和F4生成一个gate map，attention-style的结构，用pooling，1x1 conv和sigmoid，给两个warped feature做gated sum，思路是make full use of high level semantic feature and let the low level feature as a supplement of high level feature SFNet-Lite structure 实验 STDC-Seg: Rethinking BiSeNet For Real-time Semantic Segmentation 动机 BiSeNet use extra path to encode spatial information (low-level) time consuming not convenient to use pretrained backbones the auxiliary path is always lack of low-level information guidance this paper 回归test-time singe-stream manner 用一个Detail guidance module来促进backbone的low-level stage学习spatial feature，有直接监督，且test-time free cost 设计了STDC backbone，主要包含STDC module (Short-Term Dense Concatentae Network)，有点类似denseNet的block verified on ImageNet Cityscapes: STDC1-Seg50 / 71.9% mIoU / 250.4 FPS, STDC2-Seg75 / 76.8% mIoU / 97.0 FPS CamVid overview single-stream STDC backbone 方法 Short-Term Dense Concatenate Module each module is separated into several blocks：block1永远是1x1，block2/3/4是3x3 the output gathers multi-scale information 一种是stride1的一种是stride2的，reception field如下 Classification Architecture stage1/2分别是一个conv-bn-relu stage3/4/5是STDC Module，每个stage的第一个module用stride2 Segmentation Architecture STDC back：用stage3/4/5的feature map（x8/16/32） stage3的feature作为low-level feature stage4/5以及global pooling的stage5的feature作为high-level context feature，做FPN：stage4/5通过Attention Refine Module（类似SE），然后和前一个feature level做add，然后上采样，然后conv 以上的feature通过Feature Fusion Module（也类似SE block）融合 SegHead：3x3conv-1x1conv stage3的feature上还接了一个DetailHead Detail Guidance of Low-level Features detail path是用来encode spatial detail（boundary/corner） 建模成binary segmentation task 首先将ground truth map通过Detail Aggregation module得到detail map 一个stride1/2/4的Laplacian operator（conv kernel） 然后是upsampling 然后fuse and 1x1 conv 最后用thresh 0.1转换成binary detail mask detail loss：dice + bce 有了detail guidance以后能够force backbone的stage3 feature保留更加detail的low-level feature 实验 backbone实验 对标MobileNetV3和EfficientNet-B0，精度和速度都是更好的 ImageNet精度&amp;Flops，Flops比较大，但都是3x3卷积，推理速度更快]]></content>
  </entry>
  <entry>
    <title><![CDATA[mask2former]]></title>
    <url>%2F2022%2F10%2F23%2Fmask2former%2F</url>
    <content type="text"><![CDATA[papers [maskformer 2021] Per-Pixel Classification is Not All You Need for Semantic Segmentation：Facebook， [mask2former 2022] Masked-attention Mask Transformer for Universal Image Segmentation： [mask2former+1] Mask2Former for Video Instance Segmentation：追加了一个在video上面的report [mask2former] Masked-attention Mask Transformer for Universal Image Segmentation]]></content>
      <tags>
        <tag>panoptic, semantic, instance, segmentation, transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CLIP系列]]></title>
    <url>%2F2022%2F10%2F22%2FCLIP%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[papers [2021 CLIP] Learning Transferable Visual Models From Natural Language Supervision [2022 MaskCLIP] Extract Free Dense Labels from CLIP [2022 DenseCLIP ]DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting CLIP: Learning Transferable Visual Models From Natural Language Supervision 动机 visual tasks通常建模成给定的图预测成给定类别的任务，大大限制了数据的可用量 this paper leverage language concepts build a simple pretraining task：predicting which caption goes with which image use 400 million image-text pairs from internet train from scratch enable zero-shot transfer to downstream tasks overview 图1是pretraining模型 图2是获取给定text set，构建prompt，然后encoder得到固定的linear classifier 图3是zero-shot classification，就是给每个样本的image embedding，用上述linear classifier做logistics regression，得到其匹配不同类别的概率 方法 Creating a Sufficiently Large Dataset construct a new dataset of 400 million image-text pairs from a variety of publicly available sources on the Internet base query：Wikipedia里面出现100次以上的单词，a set of 500,000 search policy：基于base query的单词表搜索，balancing by 20,000 paris per query Selecting an Efficient Pre-Training Method 第一次尝试： jointly train a image CNN &amp; a text transformer 发现transformer模型对有限的1000类都收敛极慢，对开放词典及其不友好 然后建立了a easier task 只需要预测哪个text和哪个image匹配，而不用预测the exact text 4x efficiency in zero-shot transfer formulation given a batch of N (image, text) pairs jointly train an image encoder &amp; a text encoder，将其映射到multi-modal embedding space predict NxN pairings 最大化pairs的cosine similarity，最小化$N^2-N$的incorrect pairings optimize a symmetric cross entropy loss Choosing and Scaling a Model two different structures for Image CNN ResNet &amp; ViT ResNet的gloval average pooling改成了transformer-stype的attention pooling 用global query作为feature representation text encoder 就是一个现成的transformer token向量operates on a lower-cased byte pair encoding (BPE) sentence最长76，add SOS &amp; EOS EOS token作为feature representation，LN &amp; linear projected into embedding space Model Zoo 5 ResNets ResNet-50, ResNet-101 RN50x4, RN50x16, and RN50x64：4x, 16x, and 64x computation models follow EfficientNet-style model scaling 3 ViTs ViT-B/32, a ViT-B/16, and a ViT-L/14 ViT-L/14@336px：在224基础上，用336的resolution train one additional epoch temperature parameter 0.07，clip to prevent scaling the logits by more than 100 原始的logits/T，但是新logits不超过100 这是因为cosine similarity的输出在[-1,1]，而一般用于分类预测的logits通常是不限幅的，所以用temperature factor来拉大cos logits之间的差异，提高正样本置信度 necessary for training stability 实验 Zero-Shot Transfer 实验发现CLIP对unseen datasets（没有用于训练当前模型的数据集）有很好的zero-shot transfer能力，主要是因为它在互联网上见的太多了 zero-shot classification pipeline：详见overview的图 用目标数据集，所有类别，作为text pairings set，然后预测样本的most probable image-text pair 首先获得各自的feature embedding，各自L2-norm 然后计算cosine similarity，scaled by temperature factor 然后normalized by softmax into probability 精度 leaderboard上ResNet101的精度：top1@80.98%，top5@95.51% leaderboard上ResNet50的精度：top1@79.25%，top5@94.65% Representation Learning linear-probe pipeline 固定住pretrained model fitting一个linear classifier 这样相比较于finetuning的好处是hyper比较少，同时特征比较general/class-agnostic findings small models（RN50/RN101）在ImageNet-21K上打不过对应模型 small models在同样数据集上也打不过efficientNet家族 但是大模型（RN50x64）能够打败目前最好的（Noisy Student EfficientNet-L2） CLIP transformers are 3x more compute efficient than CLIP ResNets，能够在同样的算力条件下获得更好的performance prompt engineering and ensembling 图像数据集的类别大多是id/一个单词 prompt将其构造成一个句子：a photo of {word} / a {specific} of {word} ensemble将多种构造的embedding求mean]]></content>
      <tags>
        <tag>clip, image-text</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sparseInst]]></title>
    <url>%2F2022%2F10%2F15%2FsparseInst%2F</url>
    <content type="text"><![CDATA[主题：query-based instance segmentation Sparse Instance Activation for Real-Time Instance Segmentation 动机 previous instance methods relies heavily on detection results this paper use a sparse set of instance activation maps：稀疏set作为前景目标的ROI aggregate the overall mask feature &amp; instance-level feature avoid NMS 性能 &amp; 精度 40 FPS 37.9AP on COCO 论点 两阶段methods的limitations dense anchors make redundant proposals伴随了heavy burden of computation multi-scale further aggravate the issue ROI-Align没法部署在终端/嵌入式设备上 后处理排序/NMS也耗时 this paper propose a sparse set called Instance Activation Maps（IAM） motivated by CAM 通过instance-aware activation maps对全局特征加权就可以获得instance-level feature label assignment use DETR‘s bipartitie matching avoid NMS object representation对比 方法 overview backbone：拿到x8/x16/x32的C3/C4/C5特征 encoder：feature pyramid，bottleneck用了PPM扩大感受野，然后bottom-up FPN，然后merge multi-scale feature to achieve single-level prediction IAM-based decoder： mask branch：provide mask features $M: D \times H \times W$ instance branch：provide instance activation maps $k: N\times H \times W$ to further acquire recognition kernels $z: N\times D$ IAM-based decoder 首先两个分支都包含了 a stack of convs 3x3 conv channel = 256 stack=4 location-sensitive features $H \times W \times 2$的归一化坐标值 concat到decoder的输入上 enhance instance的representation instance activation maps IAM用3x3conv+sigmoid：region proposal map 进一步地，还有用GIAM，换成group=4的3x3conv，multiple activations per object 点乘在输入上，得到instance feature，$N\times D$的vector 然后是三个linear layer，分别得到classification/objectness score/mask kernel IOU-aware objectness 拟合pred mask和gt mask的IoU，用于评价分割模型的好坏 主要因为proposal大多是负样本，这种不均衡分布会拉低cls分支的精度，导致cls score和mask分布存在misalignment inference阶段，最终使用的fg prob是$p=\sqrt{cls_prob, obj_score}$ mask head given instance features $w_i \in R^{1D}$and mask features $M\in R^{DHW}$ 直接用矩阵乘法：$m_i = w_i \cdot M$ 最后upsample到原始resolution matching loss bipartite matching dice-based cost $C(i,k)=p_i[cls_k]^{1-\alpha} * dice(m_i,gt_k)^\alpha$ $\alpha = 0.8$ dice用原始形式：$dice(x,y)=\frac{2 \sum x*y}{\sum x^2 + \sum y^2}$ Hungary algorithm: scipy weighted sum of loss cls：focal loss loss obj：bce loss mask：bce + dice 实验 dataset COCO 2017：118k train / 5k valid / 20k test metric AP：mask的average precision FPS：frames per second，在Nvidia 2080 Ti上，没有使用TensorRT/FP16加速 training details 8卡训练，总共64 images per-mini-batch AdamW：lr=5e-5，wd=1e-4 train 270k iterations learning rate：divided by 10 at 210k &amp; 250k backbone：用了ImageNet的预训练权重，同时frozenBN data augmentaion：random flip/scale/jitter，shorter side random from [416,640]，longer side&lt;=864 test/eval：use shorter size 640 loss weights：cls weight=2，dice weight=2，pixel bce weight=2，obj weight=1，后面实验发现提高pixel bce weight到5会有些精度gain proposals：N=100 results backbone主要是ResNet50 ResNet-d：bag-of-tricks paper里面的一个变种，resnet在stage起始阶段进行下采样，变种将下采样放在block里面，residual path用stride2的3x3 conv，identity path用avg pooling + 1x1 conv ResNet-DCN：参考的是deformable conv net v2，将最后一个卷积替换成deformable conv 在数据处理上，增加了random crop，以及large weight decay（0.05），为了和竞品对齐 ablation on coords / dconv ablation on FIAM：kernel size/n convs/activations / group conv]]></content>
      <tags>
        <tag>instance segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2022%2F08%2F15%2Fvideo%20matting%2F</url>
    <content type="text"><![CDATA[[RVM 2021] Robust High-Resolution Video Matting with Temporal Guidance：字节，temporal（ConvGRU），multi-task [SparseInst 2022] Sparse Instance Activation for Real-Time Instance Segmentation：自动化所，overview有点像DETR Robust High-Resolution Video Matting with Temporal Guidance 动机 human video matting 用于背景替换 现有技术不稳定，会产生artifacts performance robust real-time 4K at 76 FPS and HD at 104 FPS on Nvidia GTX 1080Ti high-resolution use recurrent structure instead of frame by frame：用时序网络，分割质量更好 propose a novel training strategy：同时进行matting和segmentation两个任务，模型更鲁邦 论点 matting formulation recollect $I = \alpha F + (1-\alpha)B$ matting methods Trimap-based matting：最classical的，需要额外的先验，且通常不分类，只做前景语义分割 Background-based matting：不要先验证trimap了，改要先验background map Segmentation：就是binary的语义分割，人像前景效果还好，但是背景容易出现各种artifacts，比较不稳定 Auxiliary-free matting：不需要额外输入的架构，MODNet更关注肖像，this paper更关注目标人 Video matting： MODNet用相邻帧图像的预测结果来相互压制伪影，本质上仍就是image-independent BGM用多帧图像作为多通道 Recurrent architecture：ConvLSTM/ConvGRU High-resolution matting Patch-based refinement：图像尺寸减小，以获取high resolution task的算力，但是 Deep guided filter：trainable，模块化，end-to-end将low-reso转换成high-reso use temporal structure temporal information boosts both quality and robustness 这种overtime的背景变换使得模型对背景信息的学习更加鲁邦和精确 introduce a new training strategy 大多数matting数据集都是合成的，包括在数据处理阶段也会做这种前景贴背景的操作，扩充样本量，这种图像太fake了，和实际场景有domain gap，泛化性差 也有方法尝试先在segmentation任务上做预训练、用真实图像做对抗等方式去解决图像假的问题，这样的缺点是multi step 同时训练matting &amp; segmentation任务就一步到位了，没有额外的adaptation steps 方法 model architecture overview encoder：编码individual frame’s features，mobileNetv3/resnet50 recurrent decoder：aggregates temporal information a Deep Guided Filter module：high-resolution upsampling Feature-Extraction Encoder MobileNetV3-Large + LR-ASPP module 最后一个block使用了空洞卷积 Recurrent Decoder ConvGRU at multiple scales bottleneck block：x16 level上 在LR-ASPP之后 后ConvGRU，with id path（split，一半通道用于id，一半通道用于GRU） 然后bilinear 2x Upsampling block：x8/x4/x2 level上 每个resolution stage 先merge（concat）前一个stage的feature 然后avg pooling，conv-bn-relu，transfer the feature 然后ConvGRU，with id path 然后bilinear 2x Output block：x1 level上 去做一个final prediction 先merge 然后【conv3x3-bn-relu】x2 然后conv1x1 head：1-channel alpha/3-channel fg/1-channel segmentation Deep Guided Filter Module given high- resolution videos such as 4K and HD 先下采样by a factor s 然后输入网络 最后网络的2个输出（alpha &amp; fg）、网络output block的hidden feature、以及HR的原图这四个信息都给到DGF，to produce high-resolution的alpha和foreground 实验 training details progressive learning：see longer sequences and higher resolution loss： matting loss（alpha / fg）：L1 &amp; pyramid Laplacian loss + additional temporal coherence loss segmentation loss：BCE Sparse Instance Activation for Real-Time Instance Segmentation 动机 fully convolutional real-time instance segmentation former work的实例分割通常与目标检测绑定 dense anchors fixed reception field by fixed anchors multi-level prediction ROI-Align对移动端/嵌入式设备不友好 NMS time-consuming this paper a sparse set of activation maps：类似detr的100个proposal 基于attention map得到instance-level的features 匈牙利算法来匹配proposed instance和gt，从而省略NMS，得到稀疏预测 40 FPS and 37.9 AP on the COCO benchmark repo：https://github. com/hustvl/SparseInst 论点 this paper IAM：instance activation maps，sparse set，motivated by CAM pixel-level：相比较于框里还有bg 全局context &amp; single-level simple op：avoid ROI-Align/NMS这些不可避免的循环操作 bipartite的稀疏监督：inhibit the redundant predictions, thus avoiding NMS recognition and segmentation：在IAM的instance feature基础上执行下游任务 overall structure encoder：backbone + PPM，giving x8 fused features decoder：multi-branch instance branch：IAM， mask branch：语义分割， 方法 IAM：Instance Activation Maps 首先一个基本假设：encoder得到的feature是redundant IAM的op 一个id分支，传入原始feature，[b,h,w,d] 一个feature selection分支（conv+sigmoid+norm），[b,h,w,N] 两个分支做矩阵乘法，[b,N,d]：feature selection分支，给出了基于原始feature的N forms of spatial reweighting方案，作为最终的attention proposals downstream task：recognition and segmentation kernel class score]]></content>
  </entry>
  <entry>
    <title><![CDATA[mediaPipe]]></title>
    <url>%2F2022%2F07%2F12%2FmediaPipe%2F</url>
    <content type="text"><![CDATA[requirements getting started model zoo: https://google.github.io/mediapipe/solutions/models]]></content>
      <tags>
        <tag>on-device</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型压缩]]></title>
    <url>%2F2022%2F07%2F11%2F%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[模型压缩常用方法 模型裁剪：网络中不需要的权重进行修剪，包括非结构化裁剪/结构化裁剪 模型量化：用uint8之类的低bit数域来映射和还原f32的浮点权重 知识蒸馏：teacher student，利用soft label 网络结构设计：MobileNet、ShuffleNet 模型裁剪 非结构化裁剪 裁剪掉某些不重要的神经元实现 裁剪力度较大，可以压缩几十倍 需要定制化的软硬件支持 结构化裁剪 channel、filter、shape的re-selection 灵活部署]]></content>
  </entry>
  <entry>
    <title><![CDATA[self-supervised]]></title>
    <url>%2F2022%2F06%2F10%2Fself-supervised%2F</url>
    <content type="text"><![CDATA[自监督papers MoCo系列：contrastive-based [2019 MoCo v1] Momentum Contrast for Unsupervised Visual Representation Learning，kaiming [2020 SimCLR] A Simple Framework for Contrastive Learning of Visual Representations，Google Brain，混进来是因为它improve based on MoCo v1，而MoCo v2/v3又都是基于它改进 [2020 MoCo v2] Improved Baselines with Momentum Contrastive Learning，kaiming [2021 MoCo v3] An Empirical Study of Training Self-Supervised Visual Transformers，kaiming MAE：reconstruct-based [2021 MAE] Masked Autoencoders Are Scalable Vision Learners：恺明，将BERT的掩码自监督模式搬到图像领域，设计基于masked patches的图像重建任务 MIM：reconstruct-based [2021 SimMIM] SimMIM: A Simple Framework for Masked Image Modeling：微软，swin v2的scale up模型用了这个自监督方法来缓解data hungary issue [2022 MIM] Revealing the Dark Secrets of Masked Image Modeling：微软，类似上一篇的展开实验part， SimMIM: a Simple Framework for Masked Image Modeling 动机 propose a simple framework of MIM without the need of special designs simple designs revealed strong learning performance major components 使用较大patch size：random masking of the input image with a moderately large masked patch size (e.g., 32) makes a powerful pre-text task 进行pixel-level的regression预测：predicting RGB values of raw pixels by direct regression performs no worse than the patch classification approaches with complex designs 轻量的预测头：the prediction head can be as light as a linear layer, with no worse performance than heavier ones proved on ImageNet 普通模型ViT-B：pretraing+finetuning，ImageNet-1k，83.8% top-1 大模型SwinV2-H：87.1% top-1 超大模型SwinV2-G：MIM能够leverage the data-hungry issue，使用更少量的数据训练超大模型至SOTA 论点 自监督里面的numerous pretext tasks gray-scale image colorization：图像上色 jigsaw puzzle solving：打乱的patches重新排序 split-brain auto-encoding：图像分成两部分，两条分支，交叉预测对方 rotation prediction：given变换前后的图像X&amp;Y，预测变换参数 learning to cluster：「特征聚类，将簇心标签赋给其成员，训练一个task，特征聚类」，迭代这个过程 Masked signal modeling 图像和NLP的主要区别 images exhibit stronger locality：neighbor pixels就是high related的，language的词序则不存在必然的距离相关性 visual signals are raw and low-level：因此预测pixel-level的重建任务是否对high-level recognition task有增益？ the visual signal is continuous, and the text token is discrete bridge the modality gaps through several special designs converting continuous signals into color clusters patch tokenization using an additional network block-wise masking strategy to break short-range connections this paper propose a simple framework，无需上述复杂设计 Random masking： patch-level的随机masking，适配vision transformer 大一点的patch size(32) works for a wide range of masking ratio 小的patch size(8) 需要masking ratio as high as 80% to perform well NLP里面的masking ratio通常比较小，如0.15，我们认为是因为info-level不一致 1-layer prediction head extremely lightweight 同时target resolution也不建议太大（12-96都可以好于192x192） achieves sligtly better transferring performance than heavy heads 这个自监督的头预训练完了要丢弃的，所以越小越好，不要过多承担模型能力 pixel-level reconstruction use simple L1 loss regression比较适配continuous signal performs no worse than classification approaches 【QUESTION】分类任务一般怎么设计：后面实验里面，把RGB灰度值分解成8/256个bin，然后分类 方法 A Masked Image Modeling Framework：4 major components masking strategy encoder architecture：ViT &amp; Swin prediction head prediction target：either the raw pixels or a transformation Masking Strategy mask token use a learnable mask token vector to replace each masked patch token dimension 和 visible那部分patch embedding一致 Patch-aligned random masking 就是以patch为单位随机masking swin的patch size是随阶段增长的，从4到32，we adopt 32 for ViT we adopt 32 Other masking strategies：用了16/32 square：随机放置的大方框 block-wise：复杂设计的 Prediction Head as light as a linear layer 实验也尝试过2-layer MLP、an inverse Swin-T、an inverse Swin-B 这种逐渐heavy的 上采样 if required： ViT编码得到x16的feature maps Swin编码得到x32的feature maps 用一个1x1 conv / linear，将feature dim扩展到patch size patch size 3，如swin-RGB就是32*32*3=3072 Prediction Targets regression 也可以考虑将grouth-truth降采样到feature size L1 loss：计算masked区域RGB像素的L1 loss，然后mean on pixels 实验也尝试了L2 / smoothL1 Other prediction targets previous approaches大多数将masked signals转化成clusters or classes，然后perform a classification task Color clustering（iGPT）：将巨型dataset的RGB values聚类成512个cluster，每个预测pixel is assigned to最邻近的cluster Vision tokenization（BEiT）：用一个pretrained discrete VAE network将image patches转化成token，并作为classification target Channel-wise bin color discretization：每个颜色通道独立分类，灰度值离散化为8/256 bins Evaluation protocols 首先将模型在imagenet1k上finetuning，然后看分类精度 或者其他down-stream tasks的指标来评估 实验 pre-training settings swinB：input 192x192，window size=6 dataset：ImageNet-1K，a light data augmentation (random resize cropping/random flipping /color normalization) AdamW：weight decay=0.05，beta=[0.9,0.999] cosine LR scheduler：100 epochs (warmup 10 ep)，baseLR=8e-4 batch size：2048 random masking：mask ratio=0.6，mask patch size=32 fine-tuning settings AdamW、batch size、masking 参数一致 cosine LR：baseLR=5e-3 a stochastic depth rate：0.1 a layer-wise learning rate decay：0.9 strong data augmentation：RandAug，Mixup，Cutmix，label smoothing，random erasing AvgDist measures the averaged Euclidean distance of masked pixels to the nearest visible ones：被遮挡的patch embedding与其最近的visible patch的embedding欧几里得距离 mask ratio越大，AvgDist越大 mask patch size越大，AvgDist越大 AvgDist的值在[10,20]区间时，模型的精度最高 一些精度记录 SwinV2-H achieves 87.1% top-1 accuracy，在只使用ImageNet-1K数据集里面精度最佳 SwinV2-G借助了外部数据，但比google用的少，40× smaller，achieves strong performance 84.0% top-1 on ImageNet-V2 63.1/54.4 box/mask mAP on COCO object detection 59.9 mIoU on ADE20K semantic segmentation 86.8% top-1 acc on Kinetics-400 action recognition Visualization 【20220614】目前初步实验下来，预训练的生成模型，生成的图片会呈现明显的棋盘格，因为每个x32的feature pixel代表了一个32x32的RGB patch，官方论文里面的图也很棋盘格，不知道该训练到啥程度算结束 What capability is learned? random masking：the shape and texture of masked parts can be well recovered，以及unmasked区域会观察到显著棋盘格效应，因为这部分区域在训练过程中是不回传梯度的 masking most parts of a major object：can still predict an existence of object by the negligible clues masking the full major object：the masked area will be inpainted with background textures Prediction v.s. reconstruction 比较了masked region recover和全图recover两个任务 从重建结果上看，后者视觉效果更好一点(棋盘格没那么明显，因为是全局预测)，但是精度则低了一个点：probably the model capacity is wasted at the recovery of the unmasked area which may not be that useful for fine-tuning auto-encoders and masked image modeling两个方法都是重建任务，but they are built on different philosophies： 前者是visible signal reconstruction 后者是prediction of invisible signals MIM也可以设计成全图重建，但这相当于融合了两个任务 prediction &amp; reconstruction 从finetuning精度上看two tasks are fundamentally different in their internal mechanisms，两个任务的内部机制不同，合起来做不会促进 the task to predict might be a more promising representation：看起来prediction任务学到的representation对下游任务更有用一些 【个人理解重建任务更local一点，所以细节更好看，prediction任务更long-range一些，但是为什么有说对low-level/fine-grained downstream task更好呢？】 Revealing the Dark Secrets of Masked Image Modeling 动机 Masked image modeling (MIM) as pre-training proved effective but how it works remains unclear we compare MIM with mainstream supervised models through visualizations &amp; experiments to cover the key representational differents visualizations 发现MIM brings locality inductive bias to all layers：信息流更不丢东西？ 相比之下supervised models在lower layers更关注局部信息，在higher layers则更关注全局信息 supervised models在last layers的attention head基本没啥差别（都是global semantic info），但是MIM在last layers仍旧能够keep diversity on attention heads less diversity harms the fine-tuning performance experiments 发现MIM相比较于supervised models更胜任tasks with weak semantics / fine-grained tasks 【猜测】image-level label 驱动 pixel-level study的效果更好？ 论点 masked signal modeling mask a portion of input signals and tries to predict them 属于比较经典的recover-based自监督任务设计 language, vision, and speech场景都有在用 masked image modeling (MIM) achieve very high fine-tuning accuracy thus this paper wants a deeper understanding we use SimMIM framework：就是基于ViT/swin-back+light-weight head重建pixel-level图像的任务 random masking with large patch size a-linear-layer prediction head predict raw RGB pixels use L1 loss Visualizations attention weights have a clear meaning：每个token比重多大 从三个方面来分析 averaged attention distance to measure whether it is local attention or global attention entropy of attention distribution to measure whether it is focused attention or broad attention：这跟上面不一个意思吗 KL divergence of different attention heads to investigate that attention heads are attending different tokens or similar ones Local Attention or Global Attention 图像信息带有strong locality：neighbor pixels天然地highly correlated，所以才会有conv这样的带有local priors的设计，但是transformer结构有没有这种inductive bias就值得讨论了 computing averaged attention distance in each attention head of each layer constastive模型与supervised模型表现类似，lower layer focus locally，higher layers focus more globally MIM模型每层的attention heads则表现的充满diversity，始终保有local &amp; global pixels 说明MIM brings locality inductive bias【不太理解】 Focused Attention or Broad Attention averaging the entropy of each head’s attention distribution constastive模型与supervised模型表现类似，lower layer的一些attention heads有非常focused attention，大部分higher layers的attention heads则focus very broadly MIM模型每层都很diverse，每层都兼顾了focused attention &amp; broad attention Diversity on Attention Heads 看每个attention head关注的token是否相似 computing the KL divergence between different heads constastive模型与supervised模型表现类似，diversity逐渐变小，最后几层甚至都没了 losing diversity limits the capacity of the model：损害了模型表达能力 去掉supervised模型的后面几层去进行下游任务精度会保持甚至提升，说明supervised pretrained model后面几层确实对下游任务有负面影响 Investigating the Representation Structures via CKA similarity 前面都是在看同层不同attention heads，这里观察不同层的feature maps via the CKA similarity between feature representations of different layers MIM和constastive模型表现类似，每层的feature representation structures高度相似 supervised模型则每层差异比较大 给这些预训练模型加载权重的时候随机调换一些层进行下游任务，MIM只有轻微掉点，但是supervised会受影响更大 Experiments on 3 types of downstream tasks semantic understanding tasks：classification，Concept Generalization (CoG) &amp; 12-dataset (K12) &amp; iNaturalist-18 (iNat18) geometric and motion tasks：pose/depth estimation/video tracking，COCO &amp; CrowdPose &amp; NYUv2 combined tasks：object detection，COCO Semantic Understanding Tasks 用了三个数据集，从ImageNet pretrained去transfer settings AdamW cosine learning rate schedule 100 epochs with 20 warm-up input 224x224 DropPath 发现ImageNet cover的类别supervised模型会好于MIM模型，没cover的类/fine-grained的类都是MIM精度更高，说明MIM的 representation power的transfer能力更强 Geometric and Motion Tasks 主要测试目标定位能力，不太关注高级语义信息 全面超越 Combined Task of Object Detection COCO目标检测 Mask-RCNN framework 也是clearly outperform 然后观察到MIM模型的定位task收敛的faster and better，supervised模型则对分类能力更有用，也说明了MIM更专注geometric and motion tasks]]></content>
      <tags>
        <tag>self-supervised, MoCo, MAE, MIM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FAN:full attention networks]]></title>
    <url>%2F2022%2F06%2F03%2FFAN-full-attention-networks%2F</url>
    <content type="text"><![CDATA[paper: https://arxiv.org/abs/2204.12451 repo: https://github.com/NVlabs/FAN Understanding The Robustness in Vision Transformers 动机 ViT展现出了strong robustness（图像出现各类corruption的时候仍旧保持特征提取能力），但是缺少systematic understanding this paper examine the role of self-attention further propose a family of fully attentional networks (FANs) that strengthen its capability verified state-of-the-art on ImageNet-1k：acc 87.1% downstream tasks：semantic segmentation and object detection 论点 ViT针对non-local relations的建模被认为是the robustness against various corruptions的重要要素 但是convNext用纯卷积网络也达到了相当的精度，raising the interest in the actual role of self-attention in robust generalization our approach 首先是观察： 在执行分类任务的时候，网络天然地做出了目标的分割——self-attention promotes mid-level representations 对每层ViT的输出tokens做spectral clustering，观察到了极大的特征值 发现极大特征值的数量与input perturbation是存在相关性的——这两个指标在mid-level layers中会显著降低，从而保持robustness，同时 indicates the symbiosis of grouping 然后进一步探究这个grouping phenomenon 发现self-attention类似于information bottleneck (IB)，这个暂时不理解 最后是对transformer结构的改动 原始的ViT block，用MSA（multi heads）去提取不同特征，然后用MLP去整合 本文在融合的时候，加了一个channel attention]]></content>
      <tags>
        <tag>attention, transformer, ViT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TokenLearner]]></title>
    <url>%2F2022%2F06%2F01%2FTokenLearner%2F</url>
    <content type="text"><![CDATA[recollect： [ViT 2020] AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE，Google，开启了vision transformer的固定范式，都是切割patches作为tokens，这也对应了文本的词/字符切割，但是一个patch和一个词向量的信息量是不一样的（像素信息更低级） [TokenLearner 2022] TokenLearner: What Can 8 Learned Tokens Do for Images and Videos? Google，使用更少数量的、能够挖掘重要信息的learnable tokens， repo：https://github.com/google-research/scenic/tree/main/scenic/projects/token_learner unofficial keras repo：https://github.com/ariG23498/TokenLearner TokenLearner: What Can 8 Learned Tokens Do for Images and Videos? 动机 tokens previous：densely sampled patches ours：a handful of adpatively learned tokens learn to mine important tokens in visual data find a few import visual tokens enable long range pair-wise attention applicable to both image &amp; video tasks strong performance computationally more efficient comparable results are verified on classifications tasks 与state-of-the-arts on ImageNet对比 video datasets, including Kinetics-400, Kinetics-600, Charades, and AViD 论点 the main challenge of ViTs require too many tokens：按照16x16的切割方式，512x512的图像也对应着1024个tokens transformer block的computation和memory是基于token length平方型增长的 因此限制了larger image/longer video thus we propose TokenLearner a learnable module take image as input generates a small set of tokens idea很直接：找到图像的重点区域regions-of-importance，然后用重点区域生成token 实验发现保留8-16个（之前transformer block通常保留200-500通道数）就能够保持甚至提升精度，同时降低flops 方法 TokenLearner formulation a space-time tensor input $X$：$X \in R^{T \times H \times W \times C}$ a temporal slice $X_t$：$X_t \in R^{H \times W \times C}$ T是时间维度，如果是image的话T=1，HWC是常规的长/宽/通道数 for every time frame t，we learn to generate a series of tokens $Z_t$ from the input frame $X_t$：$Z_t=[z_i]_{i=1}^S$ thus we use a tokenizer function $A_i$：$z_i=A_i(X_t)$，adaptively selects important combination of pixels 这样的function我们有S个，而且S远远小于HW，通常S=8 tokenizer function implemented with a spatial attention mechanism 首先生成一个spatial weight map (size HW1) 然后乘在$X_t$上，得到an intermediate weighted tensor (size HWC) 最后进行spatial维度的global averge pooling，将weighted maps转化成vector (size C) 所有的resulting tokens are gathered to form the output $Z_t =[z_i]_{i=1}^S\in R^{S \times C}$ spatial attention的实现有两种 本文v1.0使用了一层/多层卷积(channel=S)+sigmoid 本文v1.1使用了一个MLP(dense-gelu-dense) （这两个版本的参数量差距巨大啊） 图：将$R^{HWC}$的input image稀疏映射到$R^{SC}$ TokenFuser after the Transformer layers，此时的tensor flow还是$R^{SC}$ 引入TokenFuser fuse information across tokens，融合所有token remap the representation back to origin resolution，重映射 首先做fuse：give tokens $Y\in R^{ST \times C}$，乘以一个learnable weight $M (ST \times ST)$，得到tensor $\in R^{ST \times C}$，可以理解为空间（或时空）关联 然后做remap，对每个temporal slice $Y_t \in R^{SC}$: $X_t^{j+1} = B(Y_t, X_t^j) = B_w Y_t + X_t^j = \beta_i(X_t^j)Y_t+X_t^j$ $X_t^j$是TokenLinear的残差输入，也就是原图HWC，等待被reweight的分支 $X_t^{j+1}$是模块输出 $Y_t^j$是TokenFuser的fuse这步的结果，对应图上transformer output SC $\beta_i()$是个dense+sigmoid，作用在原图上，得到HWS的weight tensor $B_w$ 然后乘上Y得到HWC 再加上这个残差 整体架构 整体计算流程 两种模型结构（有/没有TokenFuser） 实验 settings tobeadded TokenFuser的ablation实验：整体有提升，模型越大提升越不明显]]></content>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cython & flask]]></title>
    <url>%2F2022%2F05%2F20%2Fcython-flask%2F</url>
    <content type="text"><![CDATA[python的编译 pyc： pyc是编译py之后生成的二进制文件，由python虚拟机来执行的 在模块被加载时，.pyc文件比.py文件更快 可直接反编译为源码 生成方式1： 生成pyc文件：python -m py_compile {file1,file2}.py / python -m compileall DIR 删除py文件：find . -name “*.py” |xargs rm -rf 删除pycache目录：find . -name “pycache” |xargs rm -rf 生成方式2： 1234567# single fileimport py_compilepy_compile.compile('$file_path')# dirimport compileallcompileall.compile_dir('$dir') cython编译成动态库.so： pyx文件由cython编译为.c文件，.c文件由C/C++编译器编译为.so动态库 能起到代码保护的作用 但是编译速度太慢了 注意在编译的时候每个库里必须有一个__init__.py文件 生成方式： 创建setup.py 运行python setup.py build_ext 会在执行目录下，新增build文件夹，里面放有.so python的反编译 pyc： 需要安装第三方库：pip install uncompyle6 uncompyle6 -o . *.pyc Flask（https://zhuanlan.zhihu.com/p/32202156） 一些概念 API：应用程序接口，由服务器（Server）提供，一般是web server（网络服务器），使得外部机器通过API读取、编辑网站数据，通俗来讲API是一套协议，规定了与外界的沟通方式——如何发送请求和接受响应 HTTP动词（GET、POST、PUT、DELETE）：它们分别对应四种基本操作：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源 装饰器：放在函数前面，相当于将函数对象传入这个wrapper方法中 startup：URL 12345678910111213141516171819202122232425from flask import Flaskapp = Flask(__name__)@app.route('/')def index(): return 'Index Page'@app.route('/hello')def hello(): return 'Hello World'@app.route('/user/&lt;string:username&gt;')def show_user_profile(username): # show the user profile for that user return 'User %s' % usernameif __name__ == '__main__': # app.run() app.run(debug=True) * app = Flask(\__name__): 新建一个Flask类的实例，这是一个wsgi（Web服务器网关接口，Python Web Server Gateway Interface） * @app.route(URL): 用route装饰器告诉Flask什么样的URL能够触发我们的函数 - @app.route(&#39;/&#39;)就代表默认根地址http://127.0.0.1:5000/ - @app.route(&#39;/hello&#39;)则代表http://127.0.0.1:5000/hello - @app.route(&#39;/user/&lt;str: username&gt;&#39;)则带了传入变量以及它的变量转换器，e.g. http://127.0.0.1:5000/user/amber &lt;img src=&quot;cython-flask/转换器.png&quot; width=&quot;60%;&quot; /&gt; - 同时可以看到，每执行一次访问，debug界面生成一次GET请求（&quot;GET /hello HTTP/1.1&quot;）以及服务器返回的内容（200），具体的解释在下面的章节 * def index()/hello_world()/show_user_profile(username): 这些函数在生成指定URL时被调用，这些方法都叫做视图函数 &lt;img src=&quot;cython-flask/url.png&quot; width=&quot;40%;&quot; /&gt; &lt;img src=&quot;cython-flask/get.png&quot; width=&quot;40%;&quot; /&gt; * HTTP方法 * HTTP 方法，通过浏览器告知服务器，客户端想对请求的页面做什么 * GET(方法)告知服务器：只获取页面上的信息并发给我，这是最常用的方法 * POST(方法)告诉服务器：想在 URL 上发布新信息。并且服务器必须确保数据已存储且仅存储一次，这是 HTML 表单通常发送数据到服务器的方法 * PUT(方法)类似 POST 但是服务器可能触发了存储过程多次，多次覆盖掉旧值。你可能会问这有什么用，当然这是有原因的。考虑到传输中连接可能会丢失，在这种情况下浏览器和服务器之间的系统可能安全地第二次接收请求，而不破坏其它东西。因为 POST 它只触发一次，所以用 POST 是不可能的 * DELETE(方法)删除给定位置的信息 * 默认情况下，路由只回应GET请求，如上图的GET info，在访问指定URL时候就被触发了，具体方法通过route()装饰器传递methods参数来指定 * 设计一个简单的应用todoList * 首先设计一个根URL：如http://[hostname]/todo/api/v1.0/ * hostname：[ip：port_id] * todo：应用名称 * api/v1.0：API版本 * 第二步规划数据结构和动作 * 动作 | HTTP方法 | URL | 动作 | | :------: | :---------------------------------------------: | :------------------: | | GET | http://[hostname]/todo/api/v1.0/tasks | 检索任务清单 | | GET | http://[hostname]/todo/api/v1.0/tasks/[task_id] | 检索指定任务 | | POST | http://[hostname]/todo/api/v1.0/tasks | 创建一个新的任务 | | PUT | http://[hostname]/todo/api/v1.0/tasks/[task_id] | 更新一个已存在的任务 | | DELETE | http://[hostname]/todo/api/v1.0/tasks/[task_id] | 删除一个任务 | | | | | ​ 可以看到我们通过指定HTTP method，可以在同一个URL上实现不同的请求 * 任务的数据结构 * id:唯一标识。整型。 * title:简短的任务描述。字符串型。 * description:完整的任务描述。文本型。 * done:任务完成状态。布尔值型。 * 第三步实现第一个方法：get_tasks() 1234global tasks@app.route('/todo/api/v1.0/tasks', methods=['GET'])def get_tasks(): return jsonify(&#123;'tasks': tasks&#125;) 通过访问http://127.0.0.1:5000/todo/api/v1.0/tasks可以查看task列表 * 第四步实现【获取指定任务/删除任务】方法：get_task(task_id)/delete_task(task_id) 1234567@app.route('/todo/api/v1.0/tasks/&lt;int:task_id&gt;', methods=['GET'])def get_task(task_id): pass@app.route('/todo/api/v1.0/tasks/&lt;int:task_id&gt;', methods=['DELETE'])def delete_task(task_id): pass 这两个方法比较类似，都是先查询，存在即执行操作，否则抛出404 * 第五步实现【创建一个新的任务】方法：create_task() 12345678910111213@app.route('/todo/api/v1.0/tasks', methods=['POST'])def create_task(): if not request.json or 'title' not in request.json: abort(400) # new task task = &#123; 'id': tasks[-1]['id'] + 1, 'title': request.json['title'], # not blank 'description': request.json.get('description', ""), 'done': False &#125; tasks.append(task) return jsonify(&#123;'task': task&#125;), 201 这里面出现了**request和状态码**，这部分内容放在下一节 * 第六步实现【更新一个任务】方法：update_task(task_id) 1234@app.route('/todo/api/v1.0/tasks/&lt;int:task_id&gt;', methods=['POST'])def update_task(task_id): # request.json pass * 配置服务器ip和端口号 HTTP请求（https://www.jianshu.com/p/4456b0906708） flask的工作，就是开启一个server，监听client端发出的请求，并作出响应，请求和响应都是以http request的方式 服务器端就是flask开启的web server，客户端通过浏览器向server传递指令 请求报文 request message 请求报文由请求行（HTTP方法、URL、协议版本）、首部字段(header)、空行、请求数据（内容实体）组成 request对象 假设请求的url是：http://helloflask.com/hello?name=Grey http：//：协议字符串，指定要使用的协议，这里是http协议 helloflask.com：服务器的地址（域名） /hello：资源路径（route） ?name=Grey：query string，查询字符串，查询字符串从?开始,以键值对的形式写出,多个键值对之间用&amp;分隔 属性和方法： 上面出现的request.json，就是在获取json格式的报文 本例中执行request.args.get(‘name’, ‘’)，就能获取到url请求中name的value——Grey 路由匹配 &amp; server处理 当server能够匹配到http request给出的host/URL和methods的时候，就会调用对应的视图函数，否则得到404 查看当前server定义的所有路由： export FLASK_APP=todo flask routes 每个路由对应：断点（Endpoint）、HTTP方法（Methods）和URL规则（Rule），其中static是flask添加的特殊路由，用来访问静态文件 生成响应 在flask程序中，客户端发出的请求触发响应的视图函数，获取的返回值会作为响应的主体最后生成完整的响应，即响应报文 视图函数可以返回最多由三个元素组成的元组：响应主体、状态码、首部字段 默认的状态码为200 首部字段可以为字典，或是两元素元组组成的列表 一个普通的响应可以只返回主体内容，其余保持默认值（200，{}） 在abort()函数中传入状态码即可返回对应的错误响应，不需要return，abort之后的代码将不会被执行 Flask中可以调用make_response()方法将视图函数返回值转换为响应对象 1234567from flask import make_response@app.route('/foo')def foo(): response = make_response('Hello, World!') response.mimetype = 'text/plain' return response 响应报文 response message 视图函数可以返回最多由三个元素组成的元组：响应主体、状态码、首部字段，这就是响应的主体 响应的报文首部包含一些关于响应和服务器的信息，这些内容由Flask生成 浏览器接收到响应后，会将响应主体解析并显示在浏览器窗口上 响应报文主要由协议版本、状态码（status code）、原因短语（reason phrase）、响应首部和响应主体组成 常见HTTP状态码]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matting]]></title>
    <url>%2F2022%2F05%2F05%2FMatting%2F</url>
    <content type="text"><![CDATA[papers： [DIM-Matting 2017] Deep Image Matting，Matting网络始祖，trimap-based [BGMv2 2021] Real-time high-resolution background matting，实现高分辨率图像的实时预测 [MODNet 2020] Is a Green Screen Really Necessary for Real-Time Portrait Matting，商汤，摒弃了辅助信息，直接实现Alpha预测 [PP-Matting 2022] High-Accuracy Natural Image Matting，百度，在MODNet基础上改进 PP-Matting: High-Accuracy Natural Image Matting 动机 抠图：aka. 抠前景，与segmentation的区别是在细节上更精准 现有技术的缺陷： trimap-based methods：需要提供trimap辅助信息 trimap-free methods：与trimap-based方法在抠图质量上有较大差距 本文方法 trimap-free architecture（a two-branch architecture） applies a high-resolution detail branch (HRDB)：high-resolution保留local细节信息 a semantic context branch (SCB)：分割分支负责全局语义分割 high-accuracy natural image matting test datasets Composition-1k Distinctions-646 论点 task formulation an image $I \in R^{H\times W\times 3}$ 可以看作前景和背景的linear combination foreground image $F \in R^{H\times W\times 3}$ background image $B \in R^{H\times W\times 3}$ $I^i = \alpha^i F^i + (1-\alpha^i)B^i$ $\alpha_i$：foreground opacity 可以看作是alpha mattes estimation problem 【没理解】The image matting problem is highly ill-posed with 7 values to be solved, but only 3 values are known for each pixel in a given RGB image. ill-posed problem：不适定问题， well-posed problem：适定问题，需满足三个条件，否则为不适定问题 a solution exists 解必须存在 the solution is unique 解必须唯一 the solution’s behavior changes continuously with the initial conditions 解连续变化，不会发生跳变，即必须稳定 GAN、图像超分辨率等任务，都不满足‘解唯一’（感觉生成系都不满足）—— In most cases, there are several possible output images corresponding to a given input image and the problem can be seen as a task of selecting the most proper one from all the possible outputs. methods trimap-based trimap：a rough segmentation of the image into three parts: foreground, background and transition (regions with unknown opacity) 作为image的辅助信息，并行输入 not feasible in video trimap-free multi-stage approaches：将trimap作为中间结果，串起两个任务，会有累积误差 end-to-end approaches：一个网络直接出结果 our method high-resolution detail branch (HRDB)：keep high resolution instead of encoder-decoder，fine-grained details semantic context branch (SCB) ：segmentation subtask， foreground-background ambiguity fuse：give the final alpha matte 方法 overview of network architecture two branches：SCB &amp; HRDB shared encoder PPM to strengthen semantic context guidance flow to merge shared encoder need both high-resolution details and high-level abstract semantics HRNet48 pre-trained on ImageNet Semantic Context Branch（SCB） 用来生成trimap（fg/bg/transition） 5 blocks each block：3xConvBNReLU+bilinear upsample 32x 下采样的特征图作为分割分支的输入，加一个PPM， 输出是semantic map，分割目标是3分类的segmentation mask，也就是trimap High-Resolution Detail Branch（HRDB） 3 residual blocks + 1 conv 2x和4x 的特征图上采样到原始resolution，然后combine，作为分支输入 SCB的中间结果作为guidance flow，也融合进HRD分支， to obtain semantic context 输出是detail map，focuses on details representation in the transition region Guidance Flow Gate Convolutional Layers (GCL) 用来生成guidance map $g \in R^{H \times W}$ $g = \sigma (C_{1 \times 1} (s||d))$ semanic map和detail map先concat，然后conv-bn-relu &amp; conv-sigmoid merge guidance flow和 original flow 得到最终的merged flow $\hat d$ $\hat d = (d \odot g + d)^T w $ detail map和guidance map先做element-wise的点乘，作为辅助信息 然后叠加detail map 最后进行channel-wise的re-weight 用semantic map的1、3、5 block的输出进行guidance Loss Function 3个losses [1] semantic loss $L_s$：pixel-level的3分类CE [2] detail loss $L_d$：是alpha loss $L_{\alpha}$和gradient loss $L_{grad}$的sum，而且只计算transition region the alpha-prediction loss：是 groud truth alpha（下标g）和 predict alpha（下标d）的absolute difference the gradient loss：是像素gradient magnitude的差值 $\epsilon=1e-6$ [3] fusion loss $L_f$：包含alpha loss $L_{\alpha}$、gradient loss $L_{grad}$、composition loss $L_{comp}$ based on the final alpha matte the composition loss：是ground truth RGB value与predict RGB value的差值 predict Image $I_p$是用predicted alpha matte对ground truth foreground &amp; background的加权得到 alpha loss和gradient loss的算法与上面保持一致，但是alpha matte的值是不同的，一个是detail map的结果，一个是fusion map的结果 the final weighted loss：$\lambda_1=\lambda_2=\lambda_3=1.0$ 实验 Datasets Distinctions-646：训练集包含596个前景目标及ground truth alpha mattes，测试集包含50个 Adobe Composition-1k：431/50 在训练中每个前景会被添加进100张背景中，测试是20张 Implementation Details input images：random crop into [512,640,800] /pad into [512,]，augmented by random [distortion, blurring, horizontal flipping] SGD：momentum=0.9，weight decay=4e-5 lr：初始0.01，300k iteration的时候pow by 0.9 batchsize = 4 conduct on a single Tesla V100 GPU Evaluation metrics：the lower，the better the sum of absolute differences (SAD) mean squared error (MSE) gradient (Grad) connectivity (Conn) MODNet: Real-Time Trimap-Free Portrait Matting via Objective Decomposition 动机 existing methods either require auxiliary inputs or involve multiple stages thus we propose MODNet (matting objective decomposition network) light-weight real-time end-to-end single-input portrait matting two novel techniques an Efficient Atrous Spatial Pyramid Pooling (e-ASPP) module：fuse multi-scale a self-supervised sub-objectives consistency (SOC) strategy：domain shift problem test device：GTX 1080Ti GPU dataset：a carefully designed photographic portrait matting (PPM-100) benchmark &amp; Adobe Matting Dataset 论点 Matting Approaches 本文方法能够同时自动化完成matting task的子任务：背景提取 &amp; 前景语义 Image Matting an alpha matte predicting task：$I^i = \alpha^i F^i + (1-\alpha^i)B^i$ ill-posed explanation：上面这个公式，等式右边的参数全是未知的，3通道像素值也就是3+3+1=7个未知数per pixel 所以通常才需要trimap辅助信息：提供0/0.5/1三种alpha构成的mask，划分为absolute foreground (α = 1), absolute background (α = 0), and unknown area (α = 0.5)，这样任务就简化为之需要预测unknown region的alpha probability 方法 overview divide into three parts semantic estimation：前背景 detail prediction：边界细节 semantic-detail fusion：信息融合，得到最终的alpha matte prediction Architecture Semantic Estimation the low-resolution branch S use an encoder MobileNetV2 to predict a coarse mask：16x downsamp ground truth也是粗糙版：ground truth matte也做16x下采样+blur，去除了一些fine feature，专心提前景整体 Efficient ASPP (e-ASPP)： 标准的ASPP能解决分割前景有洞的情况，但是huge computation 空洞卷积多尺度提取+常规卷积融合——modify it in three steps： 空洞卷积分解成depth-wise conv和point-wise conv 调换point-wise和fuse conv的顺序 fuse conv也替换为更cheaper的depth-wise conv L2 loss $s_p$：predict alpha $G(\alpha_g)$：粗糙化以后的ground truth alpha matte Detail Prediction the high-resolution branch D 用原始图像I、Semantic Branch的输出S(I)、以及Semantic Branch的中间结果(low-level features) 作为输入 Branch D 超级轻量 层数少：12个conv 通道数少：64 maximum 没有保留原始解析率：第一层就下采样到4x，最后两层再复原，impact is negligible owing to skip link L1 loss：focus on transition region only $m_d$：boundary mask，对$\alpha_g$先膨胀再腐蚀得到 $d_p$：D(I, S(I))，branch输出 $\alpha_g$：ground truth alpha matte Semantic-Detail Fusion the fusion branch F combine semantics and details upsample S(I) then concat S(I) &amp; D(I, S(I)) 然后是convs+sigmoid，得到final predict matte L1 loss + compositional loss $L_c$： the absolute difference between input image I and the composited image，跟PP-matting公式10一样，用均方根～ $\alpha_p$：final prediction train end-to-end through the sum of losses above $\lambda_s = \lambda_a = 1$ $\lambda_d = 10$ 实验]]></content>
      <tags>
        <tag>Matting, 抠图算法，瞄边大师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[咏柳皮肤病paper]]></title>
    <url>%2F2022%2F04%2F19%2F%E5%92%8F%E6%9F%B3%E7%9A%AE%E8%82%A4%E7%97%85paper%2F</url>
    <content type="text"><![CDATA[A Deep Learning Based Framework for Diagnosing Multiple Skin Diseases in a Clinical Environment 摘要 a novel framework based on deep learning backbone：eff-b4 output layer改成14个neuron 每个layer group后面接一个auxiliary classifier 用t-SNE对image feature可视化 a dataset that represents the real clinical environment 13,603 专家标注的皮肤镜图片 14类（扁平苔藓LP，红斑狼疮Rosa，疣VW，痤疮AV，瘢痕KAHS，湿疹和皮炎EAD，皮肤纤维瘤DF，脂溢性皮炎SD，脂溢性角化SK，黑素细胞痣MN，血管瘤Hem，银屑病Pso，暗红色斑PWS，基底细胞癌BCC） 精度 overall acc：0.948 sensitivity：0.934 specificity：0.950 AUC：0.985 与280个权威专家比赛：showed a comparable performance level in an 8-class diagnostic task 方法 previous work不太适用于实际场景：不符合亚洲人发病率 database this paper调研了北大医学部皮肤病科的database from October 2016 to April 2020 由同一个技师使用皮肤镜，对着病灶从不同角度，连续拍摄多张 2个5年以上经验的专家，结合患者病史，临床表现，皮肤镜特征打标签 2人意见不同的时候通过咨询第3人达成一致 劣质数据（图像质量、病史不完整、病灶在黏膜/指甲）被排除 提取了14 most frequently encountered 常见病，13,603 clinical images from 2,538 patient cases 网络 eff-b4，gradually unfroze 7 auxiliary classifiers + 1 final classifiers，element-wise summation 无语子 Comparison with dermatologists 280个专家， 用独立的测试集：consists of 200 cases with a clinical image and a dermoscopic image，8类（MN, SK, BCC, EAD, SD, Pso, VW and Rosa），每类25 模型只用皮肤镜图片 都是8选1 一些精度 metrics dataset 皮肤镜图像示例 总体精度 混淆矩阵]]></content>
      <tags>
        <tag>paper, skin disease</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ArcFace]]></title>
    <url>%2F2022%2F03%2F28%2FArcFace%2F</url>
    <content type="text"><![CDATA[一些metric loss特点的总结： * margin-loss：样本与自身类簇心的距离要小于样本与其他类簇心的距离——标准center loss * intra-loss：对样本和对应类簇心的距离做约束——小于一定距离 * inter-loss：对样本和其他类簇心的距离做约束——大于一定距离 * triplet-loss：样本与同类样本的距离要小于样本与其他类样本的距离 ArcFace: Additive Angular Margin Loss for Deep Face Recognition 动机 场景：人脸， 常规要素： hypersphere：投影空间 metric learning：距离(Angles/Euclidean) &amp; class centres we propose an additive angular margin loss：ArcFace has a clear geometric interpretation SOTA on face &amp; video datasets 论点 face recognition given face image pose normalisation Deep Convolutional Neural Network (DCNN) into feature that has small intra-class and large inter-class distance two main lines train a classifier：softmax 最后的分类层参数量与类别数成正比 not discriminative enough for the open-set train embedding：triplet loss triplet-pair的数量激增，大数据集的iterations特别多 sampling mining很重要，对精度&amp;收敛速度 to enhance softmax loss center loss：在分类的基础上，压缩feature vecs的类内距离 multiplicative angular margin penalty：类特别多的时候，center就不好更新了，用last fc weights能够替代center，但是会不稳定 CosFace：直接计算logit的cosine margin penalty，better &amp; easier ArcFace improve the discriminative power stabilise the training meanwhile margin-loss：Distance(类内)+m &lt; Distance(类间) 核心idea：normed feature和normed weights的dot product等价于在求他俩的 cosine distance，我们用arccos就能得到feature vec和target weight的夹角，给这个夹角加上一个margin，然后求回cos，作为pred logit，最后softmax 方法 ArcFace transitional softmax not explicitly enforce intra-class similarity &amp; inter-class diversity 对于类内variations大/large-scale测试集的场景往往有performance gap our modification fix the bias $b_j=0$ for simplicity transform the logit $W_j^T x=||W_j||\ ||x||cos\theta_j$，$\theta_j$是weight $W_j \in R^d$和样本feature $x \in R^d$的夹角 fix the $||W_j||$ by l2 norm：$||W_j||=1$ fix the embedding $||x||$ by l2 norm and rescale： $||x||=s$ thus only depend on angle：这使得feature embedding分布在一个高维球面上，最小化与gt class的轴（对应channel的weight vec，也可以看作class center）夹角 add an additive angular margin penalty：simultaneously enhance the intra-class compactness and inter-class discrepancy 作用 softmax produce noticeable ambiguity in decision boundaries ArcFace loss can enforce a more evident gap pipeline 实现 ​]]></content>
      <tags>
        <tag>fine-grained, metric learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DA-WSOL: object localization]]></title>
    <url>%2F2022%2F03%2F21%2FDA-WSOL-object-localization%2F</url>
    <content type="text"><![CDATA[Weakly Supervised Object Localization as Domain Adaption 动机 Weakly supervised object localization (WSOL) localizing objects only with the supervision of image-level classification label previous method use classification structure and CAM to generate the localization score：CAM通常不完全focus在目标上，定位能力weak our method 将任务建模成域迁移任务，domain-adaption(DA) task score estimiator用image-level信息来训练，用像素级信息来测试 a DA-WSOL pipeline a target sampling strategy domain adaption localization (DAL) loss 论点 CAM的表现不佳 核心在于domain shift：用分类架构，训练一个分类任务，是对image-level feature的优化，但是目标却是 localization score，这是pixel-level feature，两者之间没有建立联系 最终estimator会get overfitting on source domain(也就是image-level target) 一个直观的想法：引入DA，align the distribution of these two domains，avoid overfitting——activating the discriminative parts of objects mechanisms B: Multi-instance learning(MIL) based WSOL 分类架构 类别目标驱动 通过各种data augmentation/cut mix来strengthen 印象里原论文是先训一个纯分类网络（CNN+fc），然后去掉头，改成CNN+GAP+fc，做finetuning，得到能产生CAM的网络(提取对应类别权重对特征图加权)，因为需要两步训练，后面如果要看cam一半都是用grad-cam(用梯度的平均作为权重，无需重新训练)，performance据说等价 C: Separated-structure based WSOL 一个目标分类任务 一个目标定位任务：伪标签、多阶段 A: Domain Adaption 引入DA to better assist WSOL task：align feature distribution between src/tgt domain end-to-end a target sampling strategy target domain features has a much larger scale than source domain features：显然image-level task下，训练出的特征提取器更多的保留前景特征，但是pixel-level下还包含背景之类的 sampling旨在选取source-related target samples &amp; source unseen target samples domain adaption localization (DAL) loss 上述的两类samples fed into这个loss source-related target samples：to solve the sample imbal- ance between two domains source unseen target samples：viewed as the Universum to perceive target cues 方法 Revisiting the WOSL task description：给定$image X\in R^{3 \times N} $，3通道N个pixel，需要分辨任意像素$X_{:,i}$是否属于a certain class 0-k a feature extractor f(~)：用来提取pixel-level features $Z = f(X) \in R^{C \times N}$ a score estimator e(~)：用来估计pixel的localization score $Y=e(Z) \in R^{K \times N}$ 在有监督场景下，pixel-level target Y是直接给定的，但是在无监督场景下，我们只有image-level mask，即$y=(max(Y_{0:}), max(Y_{1:}), …, max(Y_{k:})) \in R^{K \times 1}$，即每个feature map的global max/avg value构成的feature vector an additional aggregator g (~)：用来聚合feature map，将pixel-level feature转换成image-level $z=g(Z) \in R^{C\times 1}$，如GAP 然后再fed into the score estimator above：$y^* = e(z) \in R^{K \times 1}$ 用classification loss来监督$y$和$y^*$，这就是一个分类任务 but at test time：the estimator is projected back onto the pixel-level feature Z to predict the localization scores，这就是获取CAM Modeling WSOL as Domain Adaption 对每个sample X，建立两个feature sets S &amp; T source domain：$s = z = (gf)(X)$ target domain：$\{t_1,t_2,…,t_N\} =\{Z_{1,:},Z{2,:},…,Z{N,:}\}=f(X)$ aim at minimizing the target risk without accessing the target label set (pixel-level mask)，可以转化为： minimizing source risk minimizing domain discrepancy $L(S,Y^S,T) = L_{cls}(S,Y^S) + L_a(S,T)$ loss L_cls就是常规的分类loss，在image-level上train L_a是proposed adaption loss，用来最小化S和T的discrepancy，会force f(~)和g (~)去学习domain-invariant features 使得 e(~)在S和在T上的performance相似 properties to notice some samples在set T中存在，而在set S中不存在，如background，不能强行align两个set 两个分布的scale比例十分imbalance，1:N——the S set in some degree insufficient 两个分布的差异是已知的，就是aggregator g (~)，这是个先验知识 mechanism as in figure 起初两个分布并不一致，方框1/2是image level feature，圆圈1/2是pixel level feature，圆圈问号类是pixel map上的bg patches 用class loss去监督如CAM method，能够区分方框1/2，在一定程度上也能够区分圆圈1/2，但是不能精准定位目标，因为S和T存在discrepancy——bg patches 引入domain adaption loss，能够tighten两个set，使得两个分布更加align，这样class bound在方框1/2和圆圈1/2上的效果都一样好 引入Universum regularization，推动decision boundary into Universum samples，使得分类边界也有意义 Domain Adaption Localization Loss $L_a$(~) 首先进一步切分target set T Universum $T^u$：不包含前景类别/bg样本 the fake $T^f$：和source domain的sample highly correlated的样本（在GAP时候被保留下来的样本） the real $T^t$：aside from above two的样本 recall the properties the fake之所以会highly correlated source domain，就是因为先验知识GAP (property3)，我们知道他就是在GAP阶段被留下来的target sample 我们可以将其作为source domain的补充样本，以弥补insufficient issue (property2) 关于unmatched data space (property1)，T-Universum就与S保持same label space了 based on these subsets，overall的DAL loss包含两个部分 domain adaption loss $L_d$：UDA，unsupervised，align the distribution Universum regularization $L_u$：feature-based L1 regularization，所有bg像素的绝对值之和，如果他们都在分类边界上，不属于任何一个前景类，localization score的响应值就都是0，那么loss就是0 $L_a(S,T) = \lambda_1L_d(S \cup T^f, T^t) + \lambda_2 L_u(T^u)$ Target Sampling Strategy （这个有点不太理解） a target sample assigner(TSA) a cache matrix $M \in R^{C \times (K+1)}$ column 0：represents the anchor of $T^u$ the rest column：represents the anchor of certain class of $T^t$ 感觉就是记录了每类column vec的簇心 init column 0：zero-init the rest：当遇到第一个这一类的样本的时候，用src vec $z+\epsilon$初始化 update 首先基于image-level label得到类别id：$k = argmax(y)$，注意使用ground truth，不是prediction vec 然后拿到cache matrix中对应的anchor：$a^u = M_{:,0}, \ \ a^t = M_{:,k+1}$ 然后再加上image-level predict作为初始的cluster：$C_{init} = \{a^u, a^t, z\} \in R^{C \times 3}$ 对当前target samples做K-means，得到三类样本，进而计算adaption loss 用聚类得到的新center C，加权平均更新cache matrix，权重$r_k$是对应类images的数目的倒数 overall pipeline summary 首先获得S和T，f(~)是classification backbone(resnet/inception)，g(~)是 global average pooling，e(~)是作用在source domain feature vector的 fully-connected layer ，generate the image-level classification score，supervised with cross-entropy 然后通过S、T以及ground truth label id得到3个target subsets $T^u$用来计算$L_u$ $S$和$T^f$和$T^t$用来计算$L_d$：MMD (Maximum Mean Discrepancy)，h(~)是高斯kernel]]></content>
      <tags>
        <tag>弱监督</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ImageSearch]]></title>
    <url>%2F2022%2F03%2F07%2FImageSearch%2F</url>
    <content type="text"><![CDATA[以图搜图 两大类 pHash + hamming距离 CNN + cos距离 pHash cv2的dct变换和库函数imagehash调用的scipy.fftpack.dct结果不太一样，所以编码结果也不一样 123456789101112131415161718192021222324252627import numpy as npimport cv2import imagehashfrom PIL import Imagedef pHash(img_file): # step1: gray, 0-255, 32x32 img = cv2.imread(img_file, 0) img = cv2.resize(img, (32,32), interpolation=cv2.INTER_CUBIC) # step2: dct, left-top 8x8 img = cv2.dct(img.astype(np.float32)) img = img[:8,:8] # step3: flatten, mean, 0-1 binary img = img.reshape((-1)).tolist() mean = sum(img)/len(img) img_print = ['1' if i&gt;mean else '0' for i in img] # hex encoding return ''.join(['%x' % int(''.join(img_print[i:i+4]),2) for i in range(0,32*32,4)]) cv_hash = pHash(img_file)scipy_hash = imagehash.phash(Image.open(img_file), hash_size=8, highfreq_factor=4) # imagehash objectscipy_hash = scipy_hash.__str__() 以上编码得到16位的16进制编码，类似这张图像的低级特征指纹。 Hamming距离：两个等长字符串，在对应位置的不同字符的个数 如果不同字符数不超过5，说明近似 如果大于10，说明不同]]></content>
  </entry>
  <entry>
    <title><![CDATA[Faiss]]></title>
    <url>%2F2022%2F03%2F02%2FFaiss%2F</url>
    <content type="text"><![CDATA[Faiss: A library for efficient similarity search official site： 主页：https://ai.facebook.com/tools/faiss/ repo/wiki：https://github.com/facebookresearch/faiss]]></content>
      <tags>
        <tag>以图搜图，图像检索</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HMC: Hierarchical Multi-Label Classification Networks]]></title>
    <url>%2F2022%2F02%2F25%2FHMC-Hierarchical-Multi-Label-Classification-Networks%2F</url>
    <content type="text"><![CDATA[ICML2018，multi-label，hierarchical 理想数据集的类别间是互斥的，但是现实往往存在层级/包含关系，多个数据集合并时也会有这个情况 reference code: https://github.com/Tencent/NeuralNLP-NeuralClassifier/blob/master/model/classification/hmcn.py HMCN: Hierarchical Multi-Label Classification Networks 动机 HMC：hierarchical multi-label classification classes are hierarchically structured，类别是有层级关系的 objects can be assigned to multiple paths，目标可能点亮多条tree path——多标签 application domains text classification image annotation bioinformatics tasks such as protein function prediction propose HMCN local + global loss local：discover local hierarchical class-relationships global：global information from the entire class while penalizing hierarchical violations 论点 common methods local-based： 建立层级的top-down局部分类器，每个局部分类器用于区分当前层级，combine losses computation expensive，更善于提取wordTree局部的信息，容易overfitting global-based： 只有一个分类器，将global structure associate起来 cheap，没有error-propagation problem，容易underfitting our novel approach combine两者的优点 recurrent / non-recurrent版本都有 由multiple outputs构成 每个class hierarchy level有一个输出：local output 全局还有一个global output also introduce a hierarchical violation penalty 方法 a feed-forward architecture (HMCN-F) notations feature vec $x \in R^{D}$：输入向量 $C^h$：每层的节点 $|H|$：总层数 $|C|$：总类数 global flow 第一行横向的data flow 将$i^{th}$层的信息carry到第$(i+1)^{th}$层 第一层：$A_G^1 = \phi(W_G^1 x +b_G^1)$ 接下来的层：$A_G^h = \phi(W_G^h(A_G^{h-1} \odot x) +b_G^h)$ 最终的global prediction：$P_G=\sigma(W_G^{H+1}A_G^{H}+b_G^{H+1}) \in R^{|C|}$ local flow start from 每个level的global hidden layer local hidden layer：$A_L^h = \phi(W_T^hA_G^{h} +b_T^h)$ local prediction：$P_L^h = \sigma(W_L^hA_L^{h} +b_L^h) \in R^{C^h}$ merge information 将local的prediction vectors concat起来 然后和global preds相加 $P_F = \beta (P_L^1 \odot P_L^2 \odot… P_L^1) + (1-\beta) P_G$ hyperparams $\beta=0.5$ fc-bn-dropout：dim=384，drop_rate=0.6 a recurrent architecture (HMCN-R) training details small datasets with large number of classes Adam lr=1e-3 实验 【小batch反而结果更好】one can achieve better results by training HMCN models with smaller batches YOLO9000: 回顾yolov2的wordTree 动机 联合训练，为了扩展类数 检测样本梯度回传full loss 分类样本只梯度回传分类loss Hierarchical classification 构建WordTree 对每个节点的预测是一个条件概率：$Pr(child_node|parent_node)$ 这个节点的绝对概率是整条链路的乘积 每个样本的根节点概率$Pr(object)$是1 对每个节点下面的所有children做softmax 首先论文就先用darknet19训了一个1369个节点的层次分类任务 1000类flat softmax on ImageNet：72.9% top-1，91.2% top-5 1369类wordTree softmax on ImageNet：71.9% top-1，90.4% top-5 观察到Performance degrades gracefully：总体精度下降很少，而且即使分不清是什么狗品种，狗这一类的概率还是能比较高 然后用在检测上 每个目标框的根节点概率$Pr(object)$是yolo的obj prob 仍旧对每个节点做softmax，标签是高于0.5的最深节点，不用连乘条件概率 take the highest confidence path at every split until we reach some threshold and we predict that object class 对一个分类样本 我们用全图类别概率最大的bounding box，作为它的分类概率 然后还有objectness loss，预测的obj prob用0.3IOU来threshold：即如果这个bnd box的obj prob&lt;0.3是要算漏检的]]></content>
  </entry>
  <entry>
    <title><![CDATA[ConvNext]]></title>
    <url>%2F2022%2F01%2F21%2FConvNext%2F</url>
    <content type="text"><![CDATA[facebook，2022，https://github.com/facebookresearch/ConvNeXt inductive biases（归纳偏置） 卷积具有较强的归纳偏置：即strong man-made settings，如local kernel和shared weights，只有spatial neighbor之间有关联，且在不同位置提取特征的卷积核共享——视觉边角特征与空间位置无关 相比之下，transformer结构就没有这种很人为的先验的设定，就是global的优化目标，所以收敛也慢 A ConvNet for the 2020s 动机 reexamine the design spaces and test the limits of what a pure ConvNet can achieve 精度 achieving 87.8% ImageNet top-1 acc outperforming Swin Transformers on COCO detection and ADE20K segmentation 论点 conv a sliding window strategy is intrinsic built-in inductive biases：卷积的归纳偏置是locality和spatial invariance 即空间相近的grid elements有联系而远的没有：translation equivariance is a desirable property 空间不变性：shared weights，inherently efficient ViT 除了第一层的patchify layer引入卷积，其余结构introduces no image-specific inductive bias global attention这个设定的主要问题是平方型增长的计算量 使得这个结构在classification任务上比较work，但是在其他任务场景里面（需要high resolution，需要hierarchical features）使用受限 Hierarchical Transformers hybrid approach：重新引入local attention这个理念 能够用于各类任务 揭露了卷积/locality的重要性 this paper brings back convolutions propose a family of pure ConvNets called ConvNeXt a Roadmap：from ResNet to ConvNet 方法 from ResNet to ConvNet ResNet-50 / Swin-T：FLOPs around 4.5e9 ResNet-200 / Swin-B around 15e9 首先用transformer的训练技巧训练初始的resnet，作为baseline，然后逐步改进结构 macro design ResNeXt inverted bottleneck large kernel size various layer-wise micro designs Training Techniques 300 epochs AdamW aug：Mixup，CutMix，RandAugment，Random Erasing reg：Stochastic Depth，Label Smoothing 这就使得resnet的精度从76.1%提升到78.8% Macro Design 宏观结构就是multi-stage，每个stage的resolution不同，涉及的结构设计有 stage compute ratio stem cell swin的stage比是1:1:3:1，larger model是1:1:9:1，因此将resnet50的3:4:6:3调整成3:3:9:3，acc从 78.8% 提升至 79.4% 将stem替换成更加aggressive的patchify，4x4conv，s4，non-overlapping，acc从 79.4% 提升至 79.5% ResNeXt-ify 用分组卷积来实现更好的FLOPs/acc的trade-off 分组卷积带来的model capacity loss用增加网络宽度来实现 使用depthwise convolution，同时width从64提升到96 groups=channels similar to the weighted sum of self-attention：在spatial-dim上mix information acc提升至80.5%，FLOPs增加5.3G Inverted Bottleneck transformer block的ffn中，hidden layer的宽度是输入宽度的4倍 MobileNet &amp; EfficientNet里面也有类似的结构：中间大，头尾小 而原始的resne(X)t是bottleneck结构：中间小，两头大，为了节约计算量 reduce FLOPs：因为shortcut上面的1x1计算量小了 精度稍有提升：80.5% to 80.6%，R200/Swin-B上则更显著一点，81.9% to 82.6% Large Kernel Sizes 首先将conv layer提前，类比transformer的MSA+FFN reduce FLOPs，同时精度下降至79.9% 然后增大kernel size，尝试[3,5,7,9,11]，发现在7的时候精度饱和 acc：from 79.9% (3×3) to 80.6% (7×7) Micro Design：layer-level的一些尝试 Replacing ReLU with GELU：原始的transformer paper里面也是用的ReLU，但是后面的先进transformer里面大量用了GeLU，实验发现可以替换，但是精度不变 Fewer activation functions：transformer block里面有QKV dense，有proj dense，还有FFN里的两个fc层，其中只有FFN的hidden layer接了个GeLU，而原始的resnet每个conv后面都加了relu，我们将resnet也改成只有类似线性层的两个1x1 conv之间有激活函数，acc提升至81.3%，nearly match Swin Fewer normalization layers：我们比transformer还少用一个norm（因为实验发现加上入口那个LN没提升），acc提升至81.4%，already surpass Swin Substituting BN with LN：BN对于convNet，能够加快收敛抑制过拟合，直接给resnet替换LN会导致精度下降，但是在逐步改进的block上面替换则会slightly提升，81.5% Separate downsampling layers：学Swin，不再将stride2嵌入resnet conv，而是使用独立的2x2 s2conv，同时发现在resolution改变的时候加入norm layer能够stabilize training——每个downsamp layer/stem/final GAP之后都加一个LN，acc提升至82%，significantly exceeding Swin overall structural params]]></content>
  </entry>
  <entry>
    <title><![CDATA[how to train ViT]]></title>
    <url>%2F2022%2F01%2F20%2Fhow-to-train-ViT%2F</url>
    <content type="text"><![CDATA[炼丹大法： [Google 2021] How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers，Google，rwightman，这些个点其实原论文都提到过了，相当于补充实验了 [Google 2022] Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time，多个模型权重做平均 [Facebook DeiT 2021] Training data-efficient image transformers &amp; distillation through attention，常规技巧大全 How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers 动机 ViT vs. CNN 没有了平移不变形 requires large dataset and strong AugReg 这篇paper的contribution是用大量实验说明，carefully selected regularization and augmentation比憨憨增加10倍数据量有用，简单讲就是在超参方面给一些insight 方法 basic setup pre-training + transfer-learning：是在google research的原版代码上，TPU上跑的 inference是在timm的torch ViT，用V100跑的 data pretraining：imagenet transfer：cifar models [ViT-Ti, ViT-S, ViT-B and ViT-L][https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py] 决定模型scale的几个要素： depth：12，24，32，40，48 embed_dim：192，384，768，1024，1280，1408 mlp_ratio：4，48/11，64/13 num_heads：3，6，12，16 还有影响计算量的变量： resolution：224，384 patch_size：8，16，32 和原版唯一的不同点是去掉了MLP head里面的hidden layer——那个fc-tanh，据说没有performance提升，还会引发optimization instabilities Regularization and data augmentations dropout after each dense-act except the Dense_QKV：0.1 stochastic depth：线性增长dbr，till 0.1 Mixup：beta分布的alpha RandAugment：randLayers L &amp; randMagnitude M weight decay：0.1 / 0.03，注意这个weight decay是裸的，实际计算是new_p = new_p - p*weight_decay*lr，这个WD*lr可以看作实际的weight decay，也就1e-4/-5量级 Pre-training Adam：[0.9,0.999] batch size：4096 cosine lr schedule with linear warmup(10k steps) gradients clip at global norm 1 crop &amp; random horizontal flipping epochs：ImageNet-1k 300 epochs，ImageNet-21k [30,300] epochs Fine-tuning SGD：0.9 batch size：512 cosine lr schedule with linear warmup gradients clip at global norm 1 resolution：[224,384] 结论 Scaling datasets with AugReg and compute：加大数据量，加强aug&amp;reg proper的AugReg和10x的数据量都能引导模型精度提升，而且是差不多的水平 Transfer is the better option：永远用预权重去transfer，尤其大模型 在数据量有限的情况下，train from scratch基本上不能追上transfer learning的精度 More data yields more generic models：加大数据，越大范化性越好 Prefer augmentation to regularization：非要比的话aug &gt; reg，成年人两个都要 for mid-size dataset like ImageNet-1k any kind of AugReg helps for a larger dataset like ImageNet-21k regularization almost hurts，但是aug始终有用 Choosing which pre-trained model to transfer Prefer increasing patch-size to shrinking model-size：显存有限情况下优先加大patch size 相似的计算时间，Ti-16要比S-32差 因为patch-size只影响计算量，而model-size影响了参数量，直接影响模型性能 Training data-efficient image transformers &amp; distillation through attention 动机 大数据+大模型的高精度模型不是谁都负担得起的 we produce competitive model use Imagenet only on single computer，8-gpu less than 3 days，53 hours pretraining + 20 hours finetuning 模型：86M，top-1 83.1% 脸厂接地气！！！ we also propose a tranformer-specific teacher-student strategy token-based distillation use a convnet as teacher 论点 本文就是在探索训练transformer的hyper-parameters、各种训练技巧 Knowledge Distillation (KD) 本文主要关注teacher-student 用teacher生成的softmax结果（soft label）去训练学生，相当于用student蒸馏teacher the class token a trainable vector 和patch token接在一起 然后接transformer layers 然后 projected with a linear layer to predict the class 这种结构force self-attention在patch token和class token之间进行信息交换 因为class token是唯一的监督信息，而patch token是唯一的输入变量 contributions scaling down models：DeiT-S和DeiT-Ti，向下挑战resnet50和resnet18 introduce a new distillation procedure based on a distillation token，类似class token的角色 特殊的distillation机制使得transformer相比较于从同类结构更能从convnet上学到更多 well transfer 方法 首先假设我们有了一个strong teacher，我们的任务是通过exploiting the teacher来训练一个高质量的transformer Soft distillation teacher的softmax logits不直接做标签，而是计算两个KL divergence CE + KL loss Hard-label distillation 就直接用作label CE + CE 实验发现hard比soft结果好 Distillation token 在token list上再添加一个new token 跟class token的工作任务一样 distillation token的优化目标是上述loss的distillation component 与class token相辅相成 作为对比，也尝试了用原本的CE loss训练两个独立的class token，发现这样最终两个class token的cosine similarity高度接近1，说明额外的class token没有带来有用的东西，但是class token和distillation token的相似度最多也就0.93，说明distillation branch给模型add something，【难道不是因为target不同所以才不同吗？？？】 Fine-tuning with distillation finetuning阶段用teacher label还是ground truth label？ 实验结果是teacher label好一点 Joint classifiers 两个softmax head相加 然后make the prediction Training details &amp; ablation Initialization Transformers are highly sensitive to initialization，可能会导致不收敛 推荐是weights用truncated normal distribution Data-Augmentation Auto-Augment, Rand-Augment, random erasing, Mixup等等 transformers require a strong data augmentation：几乎都有用 除了Dropout：所以我们把Dropout置零了 Optimizers &amp; Regularization AdamW 和ViT一样的learning rate 但是much smaller weight decay：发现weight decay会hurt convergence]]></content>
      <tags>
        <tag>ViT, transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MAE]]></title>
    <url>%2F2022%2F01%2F13%2FMAE%2F</url>
    <content type="text"><![CDATA[papers [MAE] Masked Autoencoders Are Scalable Vision Learners：恺明，将BERT的掩码自监督模式搬到图像领域，设计基于masked patches的图像重建任务 [VideoMAE] VideoMAE: Masked Autoencoders are Data-Efﬁcient Learners for Self-Supervised Video Pre-Training：腾讯AI Lab，进一步搬运到video领域 Masked Autoencoders Are Scalable Vision Learners 动机 一种自监督训练(pretraining)方式，用来提升模型泛化性能 技术方案： mask &amp; reconstruct encoder-decoder architecture encoder operates only on visible patches：首先对input patches做random sampling，只选取少量patches给到encoder lightweight decoder run reconstruction on (masked) tokens：将encoded patches和mask tokens组合，给到decoder，用于重建原始图像 精度 with MAE pretraining，ViT-Huge on ImageNet-1k：87.8% 论点 自监督路线能给到模型更大体量的数据，like NLP，masked autoencoding也是经典的BERT训练方式，but现实是autoencoding methods in vision lags behind NLP information density：NLP是通过提取高级语义信息去推断空缺的，而图像如果有充足的邻里低级空间信息，就能重建出来不错的效果，导致模型没学到啥高级语义信息就收敛了，本文的解决方案是random mask极大比例的patches，largely reduce redundancy decoder plays a different role be- tween reconstructing text and images：和上一条呼应，visual decoder重建的是像素，低级信息，NLP decoder重建的是词向量，是高级表征，因此BERT用了个贼微小的结构来建模decoder——一个MLP，但是图像这边decoder的设计就重要的多——plays a key role in determining the semantic level of the learned latent representations our MAE 不对称encoder-decoder high portion masking：既提升acc又减少计算量，easy to scale-up workflow MAE pretraing：encode random sampled patches，decode encoded&amp;masked tokens down stream task：save encoder for recognition tasks 方法 masking 切分图像：non-overlapping patches 随机采样：random sample the patches following a uniform distribution high masking ratio：一定要构建这样的task，不能简单通过邻里低级信息恢复出来，必须要深入挖掘高级语义信息，才能推断出空缺是啥 MAE encoder ViT given patches：linear proj + PE operates on a small visible subset(25%) of the full set 负责recognition任务 MAE decoder a series of Transformer blocks：远小于encoder，narrower and shallower，单个token的计算量是encoder的&lt;10% given full set tokens mask tokens：a shared &amp; learned vector 用来表征missing patches add PE：从而区别不同的mask token 负责reconstruction任务 Reconstruction target decoder gives the full set reconstructed tokens：[b,N,D] N：patch sequence length D：patch pixel values reshape：[b,H,W,C] 重建loss，per-pixel MSE：compute only on masked patches 【QUESTION，这个还没理解】还有一个变体，官方代码里叫norm_pix_loss，声称是for better representation learning，以每个patch的norm作为target： 对每个masked patch，计算mean&amp;std， 然后normalize， 这个normed patch作为reconstruction target]]></content>
      <tags>
        <tag>self-supervised</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[det-transformers]]></title>
    <url>%2F2021%2F12%2F08%2Fdet-transformers%2F</url>
    <content type="text"><![CDATA[目标检测leaderboard: https://paperswithcode.com/sota/object-detection-on-coco boxAP swin开启了霸榜时代：家族第一名63.1 接着是YOLO家族：家族第一名57.3，YOLOv4是55.8 DETR：论文里是44.9(没在榜单上)，只有一个deformable DETR是52.3 时代的眼泪Cascade Mask R-CNN：42.8 anchor-free系列：FCOS是44.7，centerNet是43.5 目前检测架构的几个霸榜算法 DETR系列end-to-end Swin放在传统二阶段架构里面 YOLO tricks加持：multi-scale、TTA、self-training、cascade、GIoU papers [DETR 2020] End-to-End Object Detection with Transformers：Facebook，首个端到端检测架构，无需nms等后处理，难优化，MSA的显存/计算量 [Swin 2021] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows：微软，主要是swin-back的建模能力强，放在啥框架里都很行 [deformable DETR 2021] DEFORMABLE DETR: DEFORMABLE TRANSFORMERS FOR END-TO-END OBJECT DETECTION：商汤，将MSA卷积话，解决transformer的high-resolution困境 [anchor DETR 2022] Anchor DETR: Query Design for Transformer-Based Object Detection：旷视，new query design，也是针对attention结构的变种（cross-attention），精度更高，显存更少，速度更快，收敛更快 [DDQ 2022] What Are Expected Queries in End-to-End Object Detection? 商汤，基于DETR，讨论新的dense queries repo https://github.com/facebookresearch/detr https://github.com/facebookresearch/3detr，3D版本 https://github.com/fundamentalvision/Deformable-DETR https://github.com/megvii-research/AnchorDETR https://github.com/jshilong/DDQ，暂时没开源代码，只有主页 Swin details for object detection integrate Swin backbone into 4 frameworks in mmdetection Cascade Mask R-CNN ATSS RepPoints v2 Sparse RCNN basic setttings multi-scale training：resize输入使得shorter side is between 480 and 800 AdamW：lr=0.0001，weight decay=0.05 batch size=16 stochastic depth=0.2 3x schedule (36 epochs with the learn- ing rate decayed by 10× at epochs 27 and 33) pretrained：use a ImageNet-22K pre-trained model as initialization compare to ResNe(X)t R50 vs. Swin-T：Swin-T普遍优于R50，4个框架Cascade Mask R-CNN &gt; RepPoints V2 &gt; Sparse R-CNN &gt; ATSS X101 vs. Swin-S &amp; X101-64 vs. Swin-B：Swin普遍优于RX System-level Comparison：进一步加强Swin-L HTC++ stonger multi-scale input(400-1400) 6x schedule (72 epochs) soft- NMS ImageNet-22K pre-trained DETR: End-to-End Object Detection with Transformers第一次看时有些技术细节不太理解，重新梳理一下： encoder feature inputs： 用了resnet最后一个阶段的输出，(H0/32,W0/32,C)，C=2048 然后用1x1 conv降维，(H,W,d)，作为attention layer的输入 没有batch dim，one image per GPU，外加DDP DC：distillation conv fixed PE： 给每一层attention layer的输入query和key都加了fixed PE 注意是QK不是QKV 论文的示例代码为了简洁用了learnt PE，而且只加在input层 decoder object queries：全0初始化，100是建议长度，补充材料里面有个实验，图像包含40以下目标时候基本不会漏检，再往上就开始漏检了 learnt PE prediction heads 首先做bipartite matching 将pred box和gt box一一对应，没配上的pred box与no object对齐 matching loss：寻找到最优的pred box排列，使得matching cost最小，优化算法是Hungarian algorithm，matching cost也可以理解为匹配质量 L_{match_i}=-1_{\{c_i \neq \Phi\}} \hat {clsProb}(\hat c_i) + 1_{\{c_i \neq \Phi\}} L_{box}(b_i,\hat b_i) 第一项是匹配上的某个box，它的预测概率，越大说明越confident，匹配质量越好 第二项是匹配上的某个box，它与gtbox的box loss，越大匹配质量越不好 然后计算detection loss cls loss：CE box loss：L1 + GIoU DEFORMABLE DETR: DEFORMABLE TRANSFORMERS FOR END-TO-END OBJECT DETECTION 动机 DETR的痛点 slow convergence limited feature spatial resolution：小目标往往需要放大输入resolution才能检出，但是transformer负担不起high-resolution计算 处理attention module的计算限制 only attend to a small set of key sampling points 那应该类似两阶段？先选格子，再fine-regress performace better than DETR 10x less training epochs 两阶段架构上，用作RPN，performance有进一步提升 论点 Modern object detectors：not fully end-to-end anchor training target assignment NMS 都是hand-crafted components，引入了超参的 DETR：fully end-to-end 直接回归box，结构极简 但是有痛点 high-resolution slow convergence：attention weights需要很久才能focus到sparse object上 deformable convolution a powerful and efficient mech- anism to attend to sparse spatial locations while lacks the element relation modeling mechanism we propose Deformable DETR combines the best of deformable conv and Transformers deformable conv：sparse spatial sampling Transformers：relation modeling capability deformable attention module 替换原始的Transformer attention modules 不是在all featuremap pixels上做计算，而是先pre-filter一部分locations 可以extended to multi-scale：naturally aggregate，无需特征金字塔 核心技术回顾 Multi-Head Attention in Transformers given Q,K,V attention values：$Softmax(\frac{QK^T}{\sqrt d}) V$ multi-head：concat + dense 计算量随着feature map的size的二次方增长 DETR given CNN feature maps 用一个encoer-decoder的结构，将feature maps转化成a set of object queries encoder是self-attention：Q和K都是feature map pixels decoder是cross-attention + self-attention： cross的query来自decoder的额外输入——N object queries represented by learnable positional embeddings，key来自encoder self的query和key都是decoder的额外输入——object queries 方法 Deformable Attention Module Transformer的attention layer的计算量和feature map的size正相关 Deformable attention的一个点，只和周围一个固定pattern上的点计算relationship，控制了计算量 assign only a small fixed number of keys for each query 公式 DeformAttn(z_q,p_q,x)=\sum_{m=1}^M W_m[\sum_{k=1}^KA_{mqk}W_m^{'}x(p_q + \Delta p_{mqk})] given：input $x\in R^{C\times H\times W}$，2d point $p_q$，query feature $z_q$ $m$ indexes the attention head $k$ indexes the sampled keys $K$ is the total sampled key number：远小于HW $A_{mqk}$和$\Delta p_{mqk}$是每个head的attention weights &amp; sampling offsets，是从输入feature经过一个线性层得到的 前者还加了一个softmax，normed into [0,1] 后者是feature level的绝对距离，范围无界 $W_m^{‘}x_q$是query values $x(p_q + \Delta_p{mqk})$用了bilinear interpolation Multi-scale Deformable Attention Module 将坐标$p_q$转换成normalized形式$\hat p_q$，输入一组不同scale的inputs feature map，将不同scale上这个点的weighted query加在一起就好了 公式 MSDeformAttn(z_q,\hat p_q,\{x_l\}_{l=1}^L)=\sum_{m=1}^M W_m[\sum_{l=1}^L\sum_{k=1}^KA_{mlqk}W_m^{'}x_l(\phi_l(\hat p_q) + \Delta p_{mlqk})] $A_{mlqk}$ is normalized by $\sum_{l=1}^L \sum_{k=1}^K A_{mlqk}=1$：attention weights的softmax是在所有level feature的sampled points上的，也就是LK个points $\phi(\hat p_q)$将normed coords转换到对应feature level Deformable Transformer Encoder C3-C6，L=4，channel=256 用Resnet stage3到stage5的featuremap接一个1x1conv，作为multi-scale feature maps C5的output再接一下3x3 s2 conv得到C6 堆叠Multi-scale Deformable Attention Module module的输入和输出都是same resolution的feature maps add a scale-level embedding $e_l$：用来指示输入的query pixel来自哪个scale level，但是它是随机初始化的，然后随着网络训练【？？？】 query是pixels，reference points就是它自身：代码里是query embed + fc来实现 Deformable Transformer Decoder cross-attention query是object queries key是encoder的输出 object queries are extracting features from the feature maps self-attention query和key都是object queries object queries interact with each other 这里仅给cross-attention module用了Multi-scale Deformable Attention Module，因为decoder的self-att的key不是feature maps了 query的reference points is predicted from its object query embedding：fc + sigmoid，也作为box center的initial guess detection head预测的是reference point的偏移量 Anchor DETR: Query Design for Transformer-Based Object Detection 动机 previous DETRs decoder输入的object queries是一系列learned embeddings do not have explicit physical meanings difficult to optimize we propose Anchor DETR a novel query design based on anchors：enable ‘one region multiple objects’ an attention variant：reduce memory better performance and fewer training epochs verified on MSCOCO ResNet50-DC5 feature，44.2 AP with 19 FPS 论点 Visualization of the prediction slots a图是DETR的prediction boxes的中心点，绿-红-蓝表示box由小到大，可以看到绿box分布在全图，红蓝则集中在中心，其实类似枚举，没有什么物理意义 b图是Anchor DETR的prediction slots，黑点是anchor points，可以看到box的中心点都分布在anchor附近 说明本文方法are more related to a specific position than DETR 回看CNN anchors are highly related to position contain interpretable physical meanings we propose this novel query design 首先用anchor coordinates去编码query 其次针对一个位置多个目标的情况：adding multiple patterns to each anchor point CNN是highly anchor-driven，位置和尺寸都包含了，DETR是完全放飞，随意初始化，本文方法在中间，用了anchor position，但是没有scale 这样还是保证网络预测的格子都在anchor附近：easier to optimize we also propose an attention variant that we call Row-Column Decouple Attention (RCDA) 行列解耦：2D key feature decouple into 1D row and 1D column 串行执行row attention &amp; column attention reduce memory cost similar or better performance 这个其实可以理解的，MSA的global attention太dense computation了，所以才会出现Swin那种WMSA去分块，deformable DETR那种先filter出attention区域，包括本文的解耦，都是在尝试稀疏化 方法 anchor points CNN detector里面anchor points永远对应着feature grids 但是在transformer里面，这个点可以更flexible，可以是learned points sequence 本文两种都尝试了 fixed anchor points就是grid coordinates learned anchors就是random uniform初始化，然后加入learned layers，最后输出的learned coordinates 最终的网络预测都加在这个anchor coordinates上，也就是网络又变成预测偏移量了 attention formulation：the DETR-like attention 也就是最原始的transformer里面的MSA，QKV首先各自过一层linear layer，然后如下计算： 下标f是feature，下标p是position decoder里面有两种attention：self-attention和cross-attention self-attention里面$Q_f,K_f, V_f$是一样的，来自前面的输出，$Q_p, K_p$是一样的，来自learned positional embedding decoder的第一个query输入$Q^{init}_f \in R^{N_q \times D}$可以是一个常量/一个learned embedding cross-attention里面Q的来源不变，但是KV变成了encoder的输出，$K_p$是sine-cosine positional embedding，是个常量 anchor points to object query 原始的DETR用learned positional embedding作为object query，用来distinguishing different objects，缺少可解释性 we use anchor points $Pos_q \in R^{N_A \times 2}$ $N_A$个点坐标 2是xy-dim，range 0-1 encode as the object queries $Q_p$ we use a small MLP with 2 linear layers $Q_p = Encode(Pos_q) \in R^{N_A \times D}$ multiple objects issue one position may have multiple objects 回想原始的object query，是用embedding生成的$Q_f^{init}$，每个$Q_f^i \in R^D$相当于一个pattern，用来代表当前位置/index 如果给到多个pattern给一个object query： use a small set pattern embedding $Q_f^i \in R^{N_p \times D}$ 用embedding来生成：$Q_f^i = Embedding(N_p, D)$ 相当于to detect objects with different patterns at each position $N_p=3$，类似scale overall的object queries就是$Q_f \in R^{N_pN_A \times D}$ positional embeddings则是$Q_p \in R^{N_pN_A \times D}$，它的Np是复制过来的（3个pattern的PE相同） Row-Column Decoupled Attention (RCDA) memory issue，限制了resolution decouple the key feature $K_f \in R^{H \times W \times D}$ to the row feature $K_{fx} \in R^{ W \times D}$ and column feature $K_{fy} \in R^{H \times D}$：通过1D global average pooling then perform row attention and column attention successively $g_{1D}$是1D的position encoding function：learned MLP for Q &amp; sin-cos for K 之前的计算量：(Nq)*(HW) 现在的计算量：(Nq)*(H+W) overall pipeline 宏观结构跟DETR一毛一样 但就是encoder/decoder内部的attention module变成了RCDA 然后就是pattern embeddings从Embedding(100,256)变成了Embedding(Np,D)，用(Na,D)的anchor grids一广播就变成了(NpNa,D)的query inputs 实验 settings]]></content>
      <tags>
        <tag>object detection, transoformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[swin]]></title>
    <url>%2F2021%2F11%2F30%2Fswin%2F</url>
    <content type="text"><![CDATA[papers [swin] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows：微软，multi-level features，window-based [swin V2] Swin Transformer V2: Scaling Up Capacity and Resolution：卷死了卷死了，同年就上V2， [PVT 2021] Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions，商汤，也是金字塔形结构，引入reduction ratio来降低computation cost [Twins 2021] Twins: Revisiting the Design of Spatial Attention in Vision Transformers，美团 [MiT 2021] SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers，是一篇语义分割的paper里面，提出了a family of Mix Transformer encoders (MiT)，based on PVT，引入reduction ratio对K降维，起到计算量线性增长的效果 Swin Transformer: Hierarchical Vision Transformer using Shifted Windows swin V1的paper note在：https://amberzzzz.github.io/2021/01/18/transformers/，我们简单look back： input embedding：absolute PE 替换成 relative PE basic stage basic swin block：W-MSA &amp; SW-MSA patch merging classification head a quick look back on MSA layer： scaled dot-Product attention dot：计算每个query和所有keys的similarity，$QK^T$ scaled：归一化dot-product的结果，用$\sqrt {d_k}$做分母，$\frac{QK^T}{\sqrt {d_k}}$ weighted sum softmax：计算query和所有keys的weights，$softmax(\frac{QK^T}{\sqrt {d_k}})$ sum：计算query在所有values上的加权和，作为其全局representation，$softmax(\frac{QK^T}{\sqrt {d_k}})V$ multi-head attention linear project：将输入QKV投影成h个D/h-dim的sub-QKV-pairs attention in parallel：并行对每组sub-QKV计算sub-Q的global attention concat：concat这些heads的输出，也就是所有query的global attention linear project：增加表征复杂度 masking used in decoder inputs decoder inputs query只计算它之前出现的keys的attention：将其之后的similarity value置为-inf，这样weights就无限接近0了 3类regularization： Residual dropout：在每个residual path上面都增加了residual dropout PE dropout：在输入的PE embedding之后添加dropout adds dropout：在每个residual block的sums之后添加dropout $P_{drop}=0.1$ training details pretraning： AdamW：weight decay=0.01 learning rate：linear decay，5-epoch warm-up，initial=0.001 batch size：4096 epochs：60 an increasing degree of stochastic depth：0.2、0.3、0.5 for Swin-T, Swin-S, and Swin-B finetuning： on larger resolution batch size：1024 epochs：30 a constant learning rate of 1e−5 a weight decay of 1e−8 the stochastic depth ratio to 0.1 weights transfer different resolution：swin是window-based的，resolution的改变不直接影响权重 different window size：relative position bias需要插值到对应的window size，bi-cubic Swin Transformer V2: Scaling Up Capacity and Resolution 动机 scaling up： capacity and resolution generally applicable for vision models facing issues instability low resolution pre-trained models到high-resolution downstream task的有效transfer GPU memory consumption we present techniques a post normalization technique：instability a scaled cosine attention approach：instability a log-spaced continuous position bias：transfer implementation details that lead to significant GPU savings 论点 大模型有用这个事在NLP领域已经被充分证实了：Bert/GPT都是pretrained huge model + downsteam few-shot finetuning、 CV领域的scaling up稍微lagging behind一点：而且existing model只是用于image classification tasks instability 大模型不稳定的主要原因是residual path上面的value直接add to the main branch，the amplitudes accumulate 提出post-norm，将LN移动到residual unit后面，限幅 提出scaled cosine attention，替换原来的dot product attention，好处是cosine product是与amplitude无关的 看图：三角是norm后置，圆形是norm前置 transfer 提出log-spaced continous position bias (Log-CPB) 之前是bi-cubic interpolation of the position bias maps 看图：第一行是swin V1的差值，transfer到别的window size会显著drop，下面两个CPB一个维持，一个enhance GPU usage zero optimizer activation check pointing a novel implementation of sequential self-attention computation our model 3 billion params 1536x1536 resolution Nvidia A100-40G GPUs 用更少的数据finetuning就能在downstream task上获得更好的表现 方法 overview normalization configuration common language Transformers和vanilla ViT都是前置norm layer 所以swin V1就inherit这个设置了 但是swin V2重新安排了 relative position bias key component in V1 没有用PE，而是在MSA里面引入了bias term：$Attention(Q,K,V)=Softmax(QK^T/\sqrt d + B)V$ 记一个window里面patches的个数是$M^2$，那么$B \in R^{M^2}$，两个轴上的相对位置范围都是[-M+1,M-1)，有bias matrix $\hat B \in R^{(2M-1)\times(2M-1)}$，然后从中得到$B$，源代码实现的时候用了一个truncated_normal来随机生成$\hat B$，然后在$\hat B$里面取$B$ windows size发生变化的时候，bias matrix就进行bi-cubic插值变换 Scaling Up Model Capacity 在 pre-normalization的设置下 the output activation values of each residual block are directly merged back to the main branch main branch在deeper layer的amplitude就越来越大 导致训练不稳定 Post normalization 就是MSA、MLP和layerNorm的顺序调换 在largest model training的时候，在main branch也引入了layerNorm，每6个Transformer block就引入一个norm unit Scaled cosine attention 原始的similarity term是Q.dot(K) 但是在post-norm下，发现the learnt attention map容易被个别几对pixel pairs主导 所以改成cosine：$Sim(q_i,k_i)=cos(q_i,k_i)/\tau + B_{ij}$ $\tau$ 是learnable scalar larger than 0.01 non-shared across heads &amp; layers Scaling Up Window Resolution Continuous relative position bias 用一个小网络来生成relative bias，输入是relative coordinates 2-layer MLP + ReLU in between Log-spaced coordinates 将坐标值压缩到log空间之后，插值的时候，插值空间要小得多 Implementation to save GPU memory Zero-Redundancy Optimizer (ZeRO)：通常并行情况下，优化器的states是复制多份在每个GPU上的，对大模型极其不友好，解决方案是divided and distributed to multiple GPUs Activation check-pointing：没展开说，就说high resolution下，feature map的存储也占了很大memory，用这个可以提高train speed 30% Sequential self-attention computation：串行计算self-attention，不是batch computation，这个底层矩阵效率优化不理解 Joining with a self-supervised approach 大模型需要海量数据驱动 一个是扩充imageNet数据集到五倍大，using noisy labels 还进行了自监督学习：additionally employ a self-supervised learning approach to better exploit this data：《Simmim: A simple framework for masked image modeling》，这个还没看过]]></content>
      <tags>
        <tag>swin, visual transformer, Pyramid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[seg-transformers]]></title>
    <url>%2F2021%2F11%2F18%2Fseg-transformers%2F</url>
    <content type="text"><![CDATA[之前那篇《transformers》太长了，新开一个分割方向的专题，papers： ——————————previous—————————- [SETR] Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers，复旦，水，感觉就是把FCN的back换成transformer [UNETR 2021] UNETR: Transformers for 3D Medical Image Segmentation，英伟达，直接使用transformer encoder做unet encoder [TransUNet 2021] TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation，encoder stream里面加transformer block [TransFuse 2021] TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation，大学，CNN feature和Transformer feature进行bifusion ———————————-new———————————- [Swin-Unet 2021] Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation，TUM，2D的Unet-like pure transformer，用swin做encoder，和与其对称的decoder [nnFormer 2021] nnFormer: Interleaved Transformer for Volumetric Segmentation，港大，对标nn-Unet，3D版本的Swin-Unet，完全就是照着上一篇写的 [UPerNet 2018] Unified Perceptual Parsing for Scene Understanding，PKU&amp;字节，Swin Segmentaion的补充材料，Swin的down-stream task选择用UperNet as base framework [SegFormer 2021] SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers，港大&amp;英伟达，参照FCN范式的(CNN+FPN+seg head)，设计了swin+MLP decoder的全linear网络，用于分割 Swin Transformer for Semantic Segmentaion补充Swin paper附录里面关于分割的描述： dataset： ADE20K：semantic segmentation 150 categories 25K/20K/2K/3K for total/train/val/test UperNet as base framework benchmark：https://paperswithcode.com/sota/semantic-segmentation-on-ade20k?p=swin-transformer-hierarchical-vision nnFormer: Interleaved Transformer for Volumetric Segmentation 动机 用transformer的ability to exploit long-term dependencies，去弥补卷积神经网络先天的spatial inductive bias recently transformer-based approaches 将transformer作为一个辅助模块，用于编码global context 没有将transformer最核心的，self-attention，有效的整合进CNN nnFormer：not-another transFormer volume-based self-attention，极大降低计算量 打败了Swin-Unet和nnUnet 论点 Transformers self-attention capture long-range dependencies give predictions more consisitent with humans previous approaches TransUNet：Unet结构类似，CNN提取特征，再接一个transformer辅助编码全局信息，但是一两层的transformer layer并不足以提取到这种长距离约束 Swin-UNet：有了appropriate的下采样方法，transformer能够学习hierarchical object concepts at different scales，但它是一个纯transformer的结构，用hierarchical的transformer block构造encoder和decoder，整体也是Unet结构，没有探索如何将卷积和self-attention有机结合 nnFormer contributions hybrid stem：卷积和self-attention都用上了，并且都能充分发挥能力，他的encoder： 首先是一个轻量的conv embedding layer，好处是卷积能够提供更precise的spatial information， 然后是交替的transformer blocks和convolutional down-sampling blocks，capture long-term dependencies at various scales V-MSA：volume-based multi-head self-attention a computational-efficient way to capture inter-slice dependencies 计算复杂度降低90%以上 应该就是类似于swin那种inter-patch &amp; inter-patch吧？ 方法 overview U-net结构： embedding block + encoder + decoder + patch expanding block 三次下采样 &amp; 三次上采样 long residual connections encoder input：3D patch $X \in R^{H \times W \times D}$ embedding block 将3D patch转化成patch tokens，$X_e \in R^{\frac{H}{4}\frac{W}{4}\frac{D}{2}C}$，代表的是high-resolution spatial information $\frac{H}{4}\frac{W}{4}\frac{D}{2}$是token个数 C是tensor channel，192/96 4个连续的kernel3x3的卷积层替代Swin里面的big kernel：小卷积核给出的解释是计算量&amp;感受野，没什么特别的，用卷积embedding给出的解释是pixel-level编码局部spatial信息，more precisely 前三层卷积后面+GELU+LN，stride在1、3层，如图 transformer block hierarchical compute self-attention within 3D local volumes (instead of 2D local windows) input：tokens representation of 3D patch， $X_t \in R^{L \times C}$ 首先reshape：对token sequence，再次划分local volume，$\tilde X_t \in R^{N_V \times N_T \times C}$ local volume里面包含一组空间相邻的tokens $N_V$是volume的数目（类似Swin里面window的数目） $N_T=S_H \times S_W \times S_D$ 是每个local volumes里面token的个数，{4,4,4}/{5,5,3} 然后跟Swin一样，两个连续的transformer blocks，3D windows instead of 2D V-MSA：volume-based multi-head self-attention SV-MSA：shifted version 反正就是3D版的swin，回去看swin更清晰 down-sampling block 就是strided conv，说是相比较于neighboring concatenation，能产生更hierarchical的representation，有助于learn at multi scales decoder 和encoder高度对称 down-samp对标替换成strided deconvolution 然后和encoder之间还有long-range connection，融合semantic和fine-grained information 最后的expanding block也是用了deconv 实验 Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation 动机 Unet-like pure Transformer 用Swin transformer做encoder 对称的decoder，用patch expanding layer做上采样 outperforms full-convolution / combined methods 论点 CNN的局限性 提取explicit global &amp; long-range information meanwhile Swin在各项任务上SOTA了 Swin-Unet the first pure Transformer-based Unet-shaped architecture consists of encoder, bottleneck, decoder and skip connections token：non-overlapping patches split from the input image fed into encoder：得到context features fed into decoder：将global features再upsample回input resolution patch expanding layer：不用conv/interpolation，实现spatial和feature-dim的increase skip connection：对Transformer-based的Unet结构仍旧有效 方法 overview patch partition 将图像切分成不重叠的patches，patch size是4x4 每个patch的feature dimension就是4x4x3=48，也就是48-dim vec linear embedding 将固定的patch dimension映射到任意给定维度C 交替的Swin Transformer blocks和Patch Merging generate hierarchical feature representations Swin Transformer block 是负责学feature representation的 Patch Merging是负责维度变换（下采样/上采样）的 对称的decoder：交替的Swin Transformer blocks和Patch Expanding Patch Expanding将相邻特征图拼接起来，组成2x大的特征图，同时减少特征维度 最后一个Patch Expanding layer则执行4倍上采样 Swin Transformer block based on shifted windows 两个连续的Transformer block为一组 每个block内部都是LN-MSA-LN-MLP，residual，GELU 第一个block的MSA是W-MSA 第二个block的MSA是SW-MSA encoder input：C-dim tokens，$\frac{H}{4} \times \frac{W}{4}$个tokens patch merging layer 将patches切分成2x2的4个parts 然后将4个part在特征维度上concat 然后接一个linear layer，将特征维度的dim转换为2C 这样spatial resolution就downsampled by 2x 特征维度加倍了2x bottleneck encoder和decoder中间那个部分 用了两个连续的Swin transformer block 【QUESTION】也是shifited windows的吗？ 这个part特征维度不变 decoder patch expanding layer given input features：$(\frac{W}{32} \times \frac{H}{32}\times 8C)$ 先是一个linear layer，加倍feature dim：$(\frac{W}{32} \times \frac{H}{32}\times 16C)$ 然后合并相邻4个patch tokens：$(\frac{W}{16} \times \frac{H}{16}\times 4C)$ skip connection concat以后接一个linear layer，保持特征维度不变 实验 UPerNet: Unified Perceptual Parsing for Scene Understanding 动机 人类对于图像的识别是存在多个层次的 scenes objects inside compositional parts textures and surfaces our work study a new task caled Unified Perceptual Parsing（UPP）：建立了一个“统一感知解析”的任务 要求模型recognize as many visual concepts as possible propose a multi-task framework UPerNet &amp; a training strategy repo：https://github.com/CSAILVision/unifiedparsing semantic segmentation multi-task 论点 various visual recognition tasks are mostly studied independently 过去的task总是将不同level的视觉信息分开研究 is it possible for a neural network to solve several visual recognition tasks simultaneously? thus we propose Unified Perceptual Parsing（UPP）task 有两个data issue no single dataset annotated with all levels of visual information 不同perceptual levels的标注形式也不统一 thus we propose UPerNet overcome the heterogeneity of different datasets learns to detect various visual concepts jointly 主要实现方式是每个iteration只选取一种数据集，同时只更新相关网络层 we further propose a training method enable the network to predict pixel-wise texture labels using only image-level annotations 方法 Defining Unified Perceptual Parsing 统一感知解析：从一张图中获取各种维度的视觉信息 scene labels objects parts of objects materials and textures of objects datasets 使用了Broadly and Densely Labeled Dataset：整合了好几个数据集，contains various visual concepts Objects, object parts and materials are segmented down to pixel level while textures and scenes are annotated at image level：目标、组成成分、材质是像素级标注，纹理和场景是图像级标注 standardize调整 data imabalance issue：丢掉一部分尾部数据 merge across dataset：合并不同数据集的同类数据 merge under-sampled labels：合并子类 our Broden+ 57, 095 images in total：51,617 training /5, 478 validation 22, 210 images from ADE20K, 10, 103 images from Pascal-Context and Pascal-Part, 19, 142 images from Open- Surfaces and 5, 640 images from DTD metrics Pixel Accuracy (P.A.)：the proportion of correctly classified pixels mean IoU (mIoU)：目标前景的平均IoU，会影响bg分割的表现 mIoU-bg：前景占比很小的时候，再加上bg IoU，object parts top-1 acc：图像级标注使用top1 acc，scene and texture classification Designing Networks for Unified Perceptual Parsing overview 因为包含high/low level visual tasks，所以网络也是multi-level的：FPN with a PPM scene head是image-level classification label，直接接PPM的输出 object and part heads是多尺度的，使用FPN fusion的输出 material head是细粒度任务，使用FPN的highest resolution featuremap的输出 texture head是更加细粒度任务，接在backobne的Res-2 block后面，而且是在网络训练完其他任务以后再fine-tuning的 FPN multi-level feature use [top-down path + lateral connections] to fuse high-level semantic information into middle &amp; low conv-BN-ReLU，channel = 512 PPM from PSPNet 用来解决CNN理论上感受野足够大，但实际上相当小这个问题 相比较于用dilated methods去扩大感受野的方式，好处是down-sampling rate更大（还是x32），能够提取high-level semantics ResNet 使用每个stage的输出作为level feature map，{C2, C3,C4,C5}，x4-x32 FPN的输出feature map，{P2, P3,P4,P5}，P5是PPM的输出 heads scene head：分类器，接PPM输出，global average pooling + linear layer object/parts head：实验发现使用fusion map表现好于P2，fusion map通过bilinear interpolating &amp; concat &amp; conv materials head：on top of P2 rather than fused features texture head： texture label是图像级的，而且来自non-natural images directly fusing these images with other natural images is harmful to other tasks 同时我们希望预测是pixel-level 我们把它接在C2上，append several convolutional layers，感受野small enough，而且backbone layers不回传梯度，只更新head layers training images使用64x64的，确保模型只focus在local detail上 only fine-tune a few epochs training settings poly learning rate，initial=0.2，power=0.9 weight decay=1e-4，momentum=0.9 training inputs：常用的目标检测rescale方法，随机将shorter side变成{300, 375, 450, 525, 600} inference inputs：使用fixed shorter side 450 longer side &lt; 1200 为了防止noisy gradient，每个mini-batch随机选一种data source，按照数据集大小采样，只梯度更新相关path的参数 object and material segmentation计算前景loss part segmentation计算前景+背景 on each GPU a mini-batch involves 2 images sync-SGD &amp; sync BN across 8 GPUs training iterations of ADE20k (20k images) is 100k，其他数据集对应rescale Design discussion previous segmentation networks主要是FCN，用pretrained backbones搭配dilated convs，扩大感受野的同时维持比较大的resolution 但是原始的backobne，通常在stage4/5的时候有比较多的层，如resnet101的res4&amp;res5就占了78层 改用dilated convs一是计算量memory飙升 二是有违最初的设计逻辑，未必还能发挥出原始的效能 第三就是不好兼顾本文任务的classification task 实验 SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers repo: https://github.com/NVlabs/SegFormer 动机 propose a semantic segmentation framework SegFormer simple, lightweight, efficient, powerful hierarchical transformer + MLP decoder 特点 does not need positional encoding：在inference阶段切换图像分辨率不会引起性能变化 avoids complex decoders：MLP decoder主要就是merge multi levels scale up to obtain a family of models：SegFormer-B0 to SegFormer-B5 verified on SegFormer-B4：50.3% mIoU on ADE20K，SOTA SegFormer-B5：84.0% mIoU on Cityscapes 论点 SETR ViT-back + several CNN decoders ViT主要是计算量 &amp; single-scale issue 后续methods提出PVT、Swin、Twins等，主要focus在优化multi-scale的backbone，忽略了decoder的设计 this paper （SegFormer） redesigns both the encoder and the decoder 改进的Transformer encoder：hierarchical &amp; positional-encoding-free 设计的all-MLP decoder：lightweight but powerful，设计的核心思想是 to take advantage of the Transformer-induced features where the attentions of lower layers tend to stay local, whereas the ones of the highest layers are highly non-local 方法 overview encoder： 使用了4x4的patch size，相比较于16x16的ViT，fine-grained patches is more preferred by semantic segmentation multi-level features：x4，x8，x16，x32 decoder 输入上述的multi-level features 输出x4的segmentation mask Hierarchical Transformer Encoder We design a series of Mix Transformer encoders (MiT)：MiT-B0 to MiT-B5 基于PVT的efficient self-attention module 针对原始attention block的平方时间复杂度 use a reduction ratio R to reduce the length of sequence K：注意是改变K的长度，而不是Q given原始序列长度$N=HW$，feature dimensions $C$ 先reshape：$\hat K = Reshape(\frac{N}{R},CR)(K)$ 再降维：$K=Linear(CR,C)(\hat K)$ 计算量从O(N^2)下降到O(N^2/R) set R to [64, 16, 4, 1] from stage-1 to stage-4 同时提出了several novel designs overlapped patch merging 本文的一个论点是ViT用non-overlapping patches去做patch merging，相邻patch之间没有保留local continuity，所以需要positional encoding 所以use an overlapping patch merging process notations patch size K=7/3 stride S=4/2 padding size P=3/1 (valid padding) patch merging操作仍旧通过卷积来实现 positional-encoding-free design ViT修改resolution要同步interpolatePE，还是会引起掉点 we introduce Mix-FFN 在FFN中夹了一个conv3x3 sufficient to provide positional information 甚至可以用depth-wise convolutions节省参数量 we argure that adding PE is not necessary in semantic segmentation Lightweight All-MLP Decoder idea是transformer有着远大于传统CNN的有效感受野，所以decoder可以轻量一点，不用再堆block 4 main steps unify：multi-level的feature maps各自通过MLP layer to unify the channel dimension upsample：所有的features上采样到x4，biliear interp fuse：concat + MLP(实际代码里用了1x1conv-bn-relu) seg head：MLP，预测mask仍在x4尺度上 实验 training settings AdamW：lr=2e-4，weight decay=1e-4 poly LR：power=0.9，by iteration]]></content>
      <tags>
        <tag>transformer, segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GRAPH ATTENTION NETWORKS]]></title>
    <url>%2F2021%2F11%2F17%2FGRAPH-ATTENTION-NETWORKS%2F</url>
    <content type="text"><![CDATA[official repo: https://github.com/PetarV-/GAT reference: https://zhuanlan.zhihu.com/p/34232818 归纳学习（Inductive Learning）：先从训练样本中学习到一定的模式，然后利用其对测试样本进行预测（即首先从特殊到一般，然后再从一般到特殊），这类模型如常见的贝叶斯模型。 转导学习（Transductive Learning）：先观察特定的训练样本，然后对特定的测试样本做出预测（从特殊到特殊），这类模型如k近邻、SVM等。 GRAPH ATTENTION NETWORKS 动机 task：node classification 在GCN基础上引入 masked self-attentional layers specify different weights to different nodes in a neighborhood，感觉是用attention矩阵替换邻接矩阵？ 论点 the attention architecture properties parallelizable，计算高效 can be applied to graph nodes having different degrees，这个邻接矩阵也可以啊 directly applicable to inductive learning problems，是说原始GCN那种semi-supervised场景吗 感觉后面两点有点牵强 GCN 可以避免复杂的矩阵运算 但是依赖固定图结构，不能直接用于其他图 methods graph attentional layer input：node features N个节点，F-dim representation $h=\{\overrightarrow {h_1},\overrightarrow {h_2},…,\overrightarrow {h_N} \}$，$\overrightarrow {h_i} \in R^F$ output：a new set of node features $h=\{\overrightarrow {h_1^{‘}},\overrightarrow {h_2^{‘}},…,\overrightarrow {h_N^{‘}} \}$，$\overrightarrow {h_i} \in R^{F^{‘}}$ a weight matrix $W \in R^{F \times F^{‘}}$ applied to every node then self-attention compute attention coefficients：$e_{ij} = a(W\overrightarrow {h_i},W\overrightarrow {h_j})$ attention mechanism a：是一个single-layer feedforward neural network + LeakyReLU(0.2) weight vector $\in R^{2F^{‘}}$ softmax norm overall expression 两个feature vector concat到一起 然后全连接层+LeakyReLU 然后softmax 表达的是节点j对节点i的重要性 masked attention：inject graph structure，只计算节点i的neighborhood的importance neighborhood：the first-order neighbors 加权和 + nonlinearity multi-head attention： trainable weights有多组，一个节点与其neighborhood的attention coefficients有多组 最后每组weights计算出那个new node feature（加权平均+nonlinear unit），可以选择concat/avg，作为最终输出 concat 如果是网络最后一层的MHA layer，先avg，再非线性激活函数： overall comparisons to related work our proposed GAT layer directly address several issues that were present in prior approaches computationally efficient，说是比矩阵操作高效，这个不懂 assign different importance to nodes of a neighborhood，这个GCN with tranable adjacent matrix不也是一样性质的吗，不懂 enable 有向图 enable inductive learning，可以被直接用于解决归纳学习问题，即可以对从未见过的图结构进行处理，为啥可以不懂 数学表达上看，attention和adjacent matrix本质上都是用来建模graph edges的 adj-trainable GCN：dag paper里面那种，adjacent matrix本身就是一个可训练变量(N,N)，随着训练更新参数 GAT：attention的更新引入了新的线性层权重 $ W \in R^{2F^{‘}}$]]></content>
      <tags>
        <tag>GCN, attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GPT]]></title>
    <url>%2F2021%2F10%2F27%2FGPT%2F</url>
    <content type="text"><![CDATA[GPT papers，openAI三部曲，通用预训练模型 [2018 GPT-1] Improving Language Understanding by Generative Pre-Training：transformer-based，pre-training+task-specific finetuning，将所有的task的输入都整合成sequence-to-sequence form，结构上不需要task-specific architecture [2019 GPT-2] Language Models are Unsupervised Multitask Learners：对GPT-1结构上微调，引入huge dataset进行无监督训练 [2020 GPT-3] Language models are few-shot learners：scaling up LMs，zero-shot BERT有3亿参数 GPT-1: Improving Language Understanding by Generative Pre-Training 动机 NLP tasks textual entailment：文本蕴含 question answering semantic similarity assessment document classification labeled data少，unlabeled corpora充足 large gains can be realized by generative pre-training of a language model on diverse unlabeled corpus，无监督general model，learn universal representations discriminative fine-tuning on specific task，有监督task-specific model，adapt to wide range of tasks general task-agnostic model能够打败discriminatively trained models use task-aware input transformations 论点 learn from raw text &amp;alleviate the dependence on supervised learning still challenging： 不清楚选什么optmization objectives：language modeling/machine translation/discourse coherence effective way to transfer：加task-specific模型结构改动/auxiliary learning objectives/learning schemes two-stage training procedure pretrain + fine-tuning use Transformer：better handling long-term dependencies task-specific input adaptions将输入处理成structured词向量序列 evaluate on natural language inference question answering semantic similarity text classification 方法 overview architecture：transformer decoder training objectives unsupervised：text prediction，前文预测后文 supervised：task classifier，对整个序列分类 Unsupervised pre-training given unsupervised corpus of tokens $U={u_1, …, u_n}$ context window size $k$ use standard language modeling objective：$L_1(U)=\sum log P(u_i|u_{i-k},…,u_{i-1};\Theta)$ use multi-layer Transformer decoder input：$h_0 = UW_e + W_p$ attention blocks：$h_l = tranformer_block(h_{l-1}), \forall l\in[1,n]$ output：$P(u)=softmax(h_l W_e^T)$ use SGD Supervised fine-tuning given labeled dataset $C$ consists of $[x^1,…,x^m;y]$ instances use the final transformer block’s activation $h_l^m$ fed into an linear+softmax output layer：$P(y|x^1,…,x^m)=softmax(h_l^mW_y)$ 优化目标是y：$L_2(C) = \sum log(P(y|x^1,…,x^m))$ 实验发现加上Unsupervised loss helps learning：提升泛化性，加速收敛 L_3(C) = L_2(C) + \lambda * L_1(C) Task-specific input transformations certain tasks has structured inputs如问答pairs/triplets we convert them into ordered sequences Textual entailment：将前提premise和推理hypothesis concat在一起 Similarity tasks：两个文本没有先后顺序关系，所以一对文本变成顺序交换的两个sequence，最后的hidden units $h^m_l$相加，然后接输出层 Question Answering and Commonsense Reasoning：given document $z$, question $q$, and possible answers $\{a_k\}$，context $zq$和每个答案$a_i$都构造一组连接，然后分别independently processed with our model，最后共同接入一个softmax，生成对所有possible answers的概率分布 所有的连接都使用分隔符$ 所有的sequence的首尾都加上一个randomly initialized start&amp;end tokens 实验 GPT-2: Language Models are Unsupervised Multitask Learners 动机 more general models which can perform many tasks train language model without explicit supervision trained on a new dataset of millions of webpages called WebText outperforms several baselines GPT-2：a 1.5B parameter Transformer 论点 Machine learning systems are sensitive to slight changes of data distribution task specification ‘narrow experts’ lack of generalization since ingle task training on single domain datasets methods multitask training：还不成熟 pretraining + finetuning：still require supervised training this paper connect the two lines above perform donw-stream tasks in a zero-shot setting 方法 natural sequential characteristic makes the general formulation $p(output|input)$ p(x) = \Pi_{i=1}^n p(s_{n-k}, ..., s_n|s_1, ..., s_{n-k-1}) task specific system requires the probabilistic framework also condition on the task to be performed $p(output|input, task)$ architectural level：task-specific encoders/decoders algorithmic level：like MAML or in a more flexible way to specify tasks：write all as sequences translation：(translate to french, english text, french text) comprehension：(answer the question, document, question, answer) training dataset 海量document可以通过爬虫获得but significant data quality issues 与target dataset similar的外部doc的子集能够给到提升 因此本文设定了一个搜集文本的机制：Reddit的外链，去掉Wikipedia input representation word-level language model VS byte-level language model word-level performs better 但是受到vocabulary限制 Byte Pair Encoding (BPE) combine the empirical benefits of word-level LMs with the generality of byte-level approaches 具体改进还没理解 model Transformer-based，few modifications on GPT-1 model layer normalization was moved to the input of each sub-block additional layer normalization was added after the final self-attention block initialization on residual path：N个residual layers，就将residual weights rescale $\frac{1}{\sqrt{N}}$ context size：1024 batch size：512 residual block 实验 GPT-3: Language Models are Few-Shot Learners 动机 zero-shot：pretraining+finetuning scheme还是需要task-specific finetuning datset scale-up：scaling up language models greatly improves general few-shot performance]]></content>
      <tags>
        <tag>GPT, transformer, semi-supervised</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dag]]></title>
    <url>%2F2021%2F10%2F11%2Fdag%2F</url>
    <content type="text"><![CDATA[美研院的论文，检测，用于腰椎/髋关节关键点提取 preparations hrnet pspModule Structured Landmark Detection via Topology-Adapting Deep Graph Learning 动机 landmark detection 特征点检测 identify the locations of predefined fiducial points capture relationships among 解剖学特征点 一个难点：遮挡/复杂前景状态下，landmark的准确检测和定位——structual information the proposed method 用于facial and medical landmark detection topology-adapting：learnable connectivity learn end-to-end with two GCNs 论点 heatmap regression based methods 将landmarks建模成heatmaps，然后回归 lacking a global representation 核心要素有bottom-up/top-down paths &amp; multi-scale fusions &amp; high resolution heatmap outputs coordinate regression based methods potentially incorporate structural knowledge but a lot yet to be explored falls behind heatmap-based ones 核心要素是cascaded &amp; global &amp; local 好处是结构化，不丢点，不多点，但是不一定准 graph methods 基于landmark locations和landmark-to-landmark-relationships构建图结构 most methods relies on heatmap detection results we would directly regress landmark locations from raw input image we propose DAG：deep adaptive graph 将landmarks建模成graph图 employ global-to-local cascaded Graph Convolution Networks逐渐将landmark聚焦在目标位置 graph signals combines local image features graph shape features cascade two GCNs 第一个预测一个global transform 第二个预测local offsets to further adjust contributions effectively exploit the structural knowledge allow rich exchange among landmarks narrow the gap between coordinate &amp; heatmap based methods 方法 the cascaded-regression framework input image initial landmarks from the mean shape outputs predicted landmark coordinates in multiple steps feature use graph representation G = (V,E,F) V是节点，代表landmarks，也就是特征点，表示为(x,y)的坐标 E是边，代表connectivity between landmarks，表示为(id_i, id_k)的无向/有向映射，整体的E matrix是个稀疏矩阵 F是graph signals，capturing appearance and shape information，表示为高维向量，如256-dim vec，与节点V一一对应，用于储存节点信息，在GCN中实际进行计算交互 overview summary cascade：一个GCN-global做粗定位，迭代多个GCN-local做precise定位 interpolation：feature map到feature nodes的转换，通过interpolation，【是global interp吗，是基于initial mean coords吗】 regression：【targets的具体坐标表示？？？】 inital graph：训练集的平均值 graph signal：visual feature和shape feature Cascaded GCNs GCN-global：global transformation GCN-local：coordinate offsets share the same GCN architecture graph convolution 核心思想就是：给定一个图结构（with connectivity E），每一次堆叠graph convolution，就是在对每个图节点，基于其自身$f_k^i$和邻居节点$f_k^j$的当前graph feature，weighted aggregating，结果作为这个节点这次图卷积的输出$f_{k+1}^i$ f_{k+1}^i = W_1 f_k^i + \sum_j e_{ij}W_2 f_k^j learnable weight matrices $W_1$ 和 $W_2$ 可以看作是邻居节点间信息交互的一种方式 Global Transformation GCN 这个model的作用是将initial landmarks变换到coarse targets 参照STN， recall STN 使用perspective transformation透视变换，引入9个scalars，进行图形变 workflow given a target image initialize landmark locations $V^0$ using trainingset mean GCN-global + GIN 预测perspective transformation 进而得到变换后的节点位置 graph isomorphism network (GIN) 图的线性层 输入是GCN-global的graph features $\{f_k^i\}$ 输出是9-dim vector 计算方式 READOUT：sum the features from all nodes CONCAT：得到一个高维向量 MLP：9-dim fc 最后得到9-dim的perspective transformation scalar coordinate update 将9-dim $f^G$ reshape成3x3 transformation matrix M 然后在当前的landmark locations $V^0$上施加变换——矩阵左乘 Local Refinement GCN GCN结构与global的一致，但是不share权重 最后的GIN头变了 输出改成2-dim vector represents coordinate offsets coordinate update 加法，分别在x/y轴坐标上 we perform T=3 iterations Graph signal with appearance and shape information Visual Feature denote CNN输出的feature map H with D channels encoding整个feature map：bi-linear interpolation at the landmark location $v_i$，记作$p_i$，是个D-dim vector Shape Feature visual feature对节点间关系的建模，基于global map全局信息提取，比较隐式、间接 事实上图结构能够直接对global landmarks shape进行encoding 本文用displacement vectors，就是距离，每个节点的displacement vector记作$q_i=\{v_j-v_i\}_{j!=i}$，flatten成一维，对有N个节点的图，每个节点的q-vec维度为2*(N-1) shape feature保存了structural information，当人脸的嘴被遮住的情况下，基于眼睛和鼻子以及结构性信息，就能够推断嘴的位置，这是Visual Feature不能直接表达的 graph signal concat result in a feature vector $f_i \in R^{D+2(N-1)}$ Landmark graph with learnable connectivity 大多数方法的图基于先验知识构建 we learn task-specific graph connectivity during training phase 图的connectivity serves as a gate，用邻接矩阵表示，并将其作为learnable weights training GCN-global margin loss $v_i^1$是GCN-global的预测节点坐标 m是margin $[u]_+$是$max(0,u)$ push节点坐标到比较接近ground truth就停止了，防止不稳定 GCN-local L1 loss $v_i^T$是第T个iteration GCN-local的预测节点坐标 overall loss 加权和 网络结构 GCN-global 三层basic graph convolution layer with residual（id path） concat distance vector 一层basic graph convolution mean axis1（node axis） fc，输出9-dim scalar，(b,9) GCN-local 三层basic graph convolution layer with residual（id path） relu concat distance vector 一层basic graph convolution fc，输出2-dim coords for each node，(b,24,2)]]></content>
      <tags>
        <tag>GCN, landmark detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KL Divergence]]></title>
    <url>%2F2021%2F09%2F27%2FKL-Divergence%2F</url>
    <content type="text"><![CDATA[KL divergence用于度量两个分布P和Q的差异，这种度量【不具有】对称性 P是实际分布（pred probs） Q是建模分布（gt） $D_{KL}(P||Q)=\sum_i P(i)ln\frac{P(i)}{Q(i)}$ 散度定义为分布P和分布Q之间的对数差异的加权和，用P的概率去加权 当Q是one-hot label的时候，要先clip再log 方法 torch.nn.functional.kl_div(input, target, size_average=None, reduce=None, reduction=’mean’) input：对数概率 target：概率 tf.distributions.kl_divergence(distribution_a, distribution_b, allow_nan_stats=True, name=None) distribution_a&amp;b 来自tf.distributions.Categorical(logits=None, prob=None, …) 传入logits/probs，先转换成distribution，再计算kl divergence torch.nn.KLDivLoss tf.keras.losses.KLDivergence tf.keras.losses.kullback_leibler_divergence code 12345678910111213141516171819202122232425# torch versionimport torch.nn as nnimport torch.nn.functional as Fclass KL(nn.Module): def __init__(self, args): super(KL, self).__init__() self.T = args.temperature def forward(self, logits_p, logits_q): log_p = F.log_softmax(logits_p/self.T, dim=1) q = F.softmax(logits_q/self.T, dim=1) loss = F.kl_div(log_p, p_t) # keras versionimport tensorflow as tfimport keras.backend as Kdef kl_div(logits_p, logits_q): T = 4. log_p = tf.nn.log_softmax(logits_p/T) # (b,cls) log_q = tf.nn.log_softmax(logits_q/T) p = K.exp(log_p) return K.sum(p*(log_p-log_q), axis=-1) # (b,)]]></content>
      <tags>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Self-Knowledge Distillation]]></title>
    <url>%2F2021%2F09%2F17%2FSelf-Knowledge-Distillation%2F</url>
    <content type="text"><![CDATA[Refine Myself by Teaching Myself : Feature Refinement via Self-Knowledge Distillation 动机 传统的知识蒸馏 by stage：先训练庞大的teacher self knowledge distillation without the pretrained network 分为data augmentation based approach 和 auxiliary network based approach data augmentation approach如UDA，通过监督原始图像和增强图像的一致性，但是会loose local information，对pixel-level tasks不友好，而且监督信息是从logits层，没有直接去refine feature maps our approach FRSKD auxiliary network based approach utilize both soft label and featuremap distillation 论点 various distillation methods a是传统知识蒸馏，深绿色是pretrained teacher，浅绿色是student，橙色箭头是feature蒸馏，绿色箭头是soft label蒸馏 b是data augmentation based 自蒸馏，shared 网络，原图和增强后的图，用soft logits来蒸馏 c是auxiliary classifier based 自蒸馏，cascaded分类头，每个分类器都接前一个的 d是本文自蒸馏，和c最大的不同是bifpn结构使得两个分类器每个level的特征图之间都有连结，监督方式一样的 FPN PANet：上行+下行 biFPN：上行+下行+同层级联 方法 overview notations dataset $D=\{(x_1,y_1), (x_2,y_2),…, (x_N,y_N)\}$ feature map $F_{i,j}$，i-th sample，j-th block channel dimension $c_j$，j-th block self-teacher network self-teacher network的目的是提供refined feature map和soft labels作为监督信息 inputs：feature maps $F_1, F_2, …, F_n$，也就是说teacher在进行梯度回传的时候到F就停止了，不会更新student model的参数 modified biFPN 第一个不同：别的FPN都是在fuse之前先用一个fixed-dim 1x1 conv将所有level的feature map转换成相同通道数（如256），we design $d_i$ according to $c_i$，引入一个宽度系数width，$d_i=width*c_i$， 第二个不同：使用depth-wise convolution notations BiFPN：每层dim固定的版本 BiFPNc：每层dim随输入变化的版本 self-feature distillation feature distillation adapt attention transfer 对feature map先进行channel-wise的pooling，然后L2 norm，提取spatial information soft label distillation 两个分类头的KL divergence CE with gt 两个分类头分别还有正常的CE loss overall 总的loss是4个loss相加：$L_{FRSKD}(x,y,\theta_c, \theta_t, K)=L_{CE}(x,y,\theta_c)+L_{CE}(x,y,\theta_t)+\alpha L_{KD}(x,\theta_c,\theta_t, K) + \beta L_{F}(T,F,\theta_c,\theta_T)$ $\alpha \in [1,2,3]$ $\beta \in [100,200]$ 【QUESTION】FRSKD updates the parameters by the distillation loss，$L_{KD}$ and $L_F$，which is only applied to the student network，这个啥意思暂时没理解 实验 experiment settings FRSKD\F：只做soft label的监督，不做feature map的监督 FRSKD：标准的本文方法 FRSKD+SLA：本文方法的基础上attach data augmentation based distillation]]></content>
  </entry>
  <entry>
    <title><![CDATA[L2 Regularization and Batch Norm]]></title>
    <url>%2F2021%2F09%2F16%2FL2-Regularization-and-Batch-Norm%2F</url>
    <content type="text"><![CDATA[reference： https://blog.janestreet.com/l2-regularization-and-batch-norm/ https://zhuanlan.zhihu.com/p/56142484 https://vitalab.github.io/article/2020/01/24/L2-reg-vs-BN.html 解释了之前的一个疑点： 在keras自定义的BN层中，没有类似kernel_regularizer这样的参数 在我们写自定义optmizer的时候，BN层也不进行weight decay的 L2 Regularization versus Batch and Weight Normalization 动机 两个common tricks：Normalization（BN、WN、LN等）和L2 Regularization 发现两者结合时L2 regularization对normalization层没有正则效果 L2 regularization反而对norm layer的scale有影响，间接影响了learning rate 现代优化器如Adam只能间接消除这种影响 论点 BN popular in training deep networks solve the problem of covariate shift 使得每个神经元的输入保持normal分布，加速训练 mean &amp; variance：training time基于每个mini-batch计算，test time使用所有iteration的mean &amp; variance的EMA usually trained with SGD with L2 regularization result in weight decay：从数学表示上等价于对权重做衰减 每一步权重scaled by a 小于1的数 但是normalization strategies是对scale of the weights invariant的，因为在输入神经元之前都会进行norm therefore there is no regularizing effect rather strongly influence the learning rate??👂 L2 Regularization formulation： 在loss的基础上加一个regularization term，$L_{\lambda}(w)=L(w)+\lambda ||w||^2_2$ loss是每个样本经过一系列权重运算，$L(w)=\sum_N l_i (y(X_i;w,\gamma,\beta))$ 当使用normalization layer的时候：$y(X_i;w,\alpha,\beta)=y(X_i;\alpha w,\gamma,\beta)$，即loss term不会变 $L_{\lambda}(\alpha w)=L(w)+\lambda||w||^2_2$ 在有normalization layer的时候，L2 penalty还是能够通过reg term force权重的scale越来越小，但是不会影响优化进程（不影响main objective value），因为loss term不变 Effect of the Scale of Weights on Learning Rate BN层的输出是scale invariant的，但是梯度不是，梯度是成反比被抑制的！ 所以weights在变小，同时梯度在变大！ 在减小weight scale的时候，网络的梯度会变大，等价于学习率在变大，会引起震荡不稳定 所以在设定hyper的时候，如果我们要适当加大weight decay $\lambda$，就要反比scale学习率 Effect of Regularization on the Scale of Weights during training the scale of weights will change the gradients of the loss function will cause the norm of the weights to grow the regularization term causes the weights to shrink]]></content>
      <tags>
        <tag>正则化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SAM loss]]></title>
    <url>%2F2021%2F09%2F10%2FSAM-loss%2F</url>
    <content type="text"><![CDATA[google brain，引用量51，但是ImageNet榜单/SOTA模型的对比实验里面经常能够看到这个SAM，出圈形式为分类模型+SAM SAM：Sharpness-Aware Minimization，锐度感知最小化 official repo：https://github.com/google-research/sam Sharpness-Aware Minimization for Efficiently Improving Generalization 动机 heavily overparametered models：training loss能训到极小，但是generalization issue we propose Sharpness-Aware Minimization (SAM) 同时最小化loss和loss sharpness improve model generalization robustness to label noise verified on CIFAR 10&amp;100 ImageNet finetuning tasks 论点 typical loss &amp; optimizer population loss：我们实际想得到的是在当前训练集所代表的分布下的最优解 training set loss：但事实上我们只能用所有的训练样本来代表这个分布 因为loss函数是non-convex的，所以可能存在多个local even global minima对应的loss value是一样的，但是generalization performance确是不同的 成熟的全套防止过拟合手段 loss optimizer dropout batch normalization mixed sample augmentations our approach directly leverage the geometry of the loss landscape and its connection to generalization (generalization bound) proved additive to existing techniques 方法 motivation rather than 寻找一个weight value that have low loss，我们寻找的是那种连带他临近的value都能有low loss的value 也就是既有low loss又有low曲度 sharpness term $\max \limits_{||\epsilon||_p &lt; \rho} L_s(w+\epsilon) - L_s(w)$ 衡量模型在w处的sharpness Sharpness-Aware Minimization (SAM) formulation sharpness term再加上train loss再加上regularization term $L_S^{SAM}(w)=\max\limits_{a} L_s(w+\epsilon)$ $\min \limits_{w} L_S^{SAM}(w) + \lambda ||w||^2_2$ prevent the model from converting to a sharp minimum effective approximation bound with $\frac{1}{p} + \frac{1}{q} = 1$ approximation pseudo code given a min-batch 首先计算当前batch的training loss，和当前梯度，$w_t$ to $w_{t+1}$ 然后计算近似为梯度norm的步长$\hat\epsilon(w)$，equation2，$w_t$ to $w_{adv}$，这里面的adv联动了另一篇论文《AdvProp: Adversarial Examples Improve Image Recognition》 然后计算近似的sharpness term，可以理解为training loss在w邻居处的梯度，equation3，应该是蓝色箭头的反方向，图上没标记出来 用w邻居的梯度来更新w的权重，用负梯度（蓝色箭头） overll就是：要向前走之前，先回退，缺点是两次梯度计算，时间double 实验结论 能优化到损失的最平坦的最小值的地方，增强泛化能力]]></content>
  </entry>
  <entry>
    <title><![CDATA[MuST谷歌多任务自训练]]></title>
    <url>%2F2021%2F09%2F01%2FMuST%2F</url>
    <content type="text"><![CDATA[recollect [SimCLR] [MoCo] Multi-Task Self-Training for Learning General Representations 动机 learning general feature representations expect a single general model 相比较于training specialized models for various tasks harness from independent specialized teacher models with a multi-task pseudo dataset trained with multi-task learning evalutate on 6 vision tasks image recognition (classification, detection, segmentation) 3D geometry estimation 论点 pretraining &amp; transfer learning transformer一般都是这个套路，BiT&amp;ViT pretraining supervised / unsupervised learn feature representations transfer learning on downstream tasks the features may not necessarily be useful 最典型的就是ImageNet pre-training并不能improve COCO segmentation，但是Objects365能够大幅提升 pretraining tasks必须要和downstream task align，learn specialized features，不然白费 learning general features a model simultaneously do well on multiple tasks NLP的bert是一个典型用多任务提升general ability的 CV比较难这样做是因为标签variety，没有这样的大型multi-task dataset multi-task learning shared backbone (如ResNet-FPN) small task-specific heads self-training use a supervised model to generate pseudo labels on unlabeled data then a student model is trained on the pseudo labeled data 在各类任务上都proved涨点 但是迄今为止都是focused on a single task in this work lack of large scale multi-task dataset的issue，通过self-training to fix，用pseudo label specialized/general issue，通过多任务，训练目标就是六边形战士，absorb the knowledge of different tasks in the shared backbone three steps trains specialized teachers independently on labeled datasets （分类、分割、检测、深度估计） the specialized teachers are then used to label a larger unlabeled dataset（ImageNet） to create a multi- task pseudo labeled dataset train a student model with multi-task learning MuST的特质 improve with more unlabeled data，数据越多general feature越好 can improve upon already strong checkpoints，在海量监督高精度模型基础上fine-tune，仍旧能在downstream tasks涨点 方法 Specialized Teacher Models 4 teacher models classification：train from scratch，ImageNet detection：train from scratch，Object365 segmentation：train from scratch，COCO depth estimation：fine-tuning from pre-trained checkpoint pseudo labeling unlabeled / partially labeled datasets for detection：hard score threshold of 0.5 for segmentation：hard score threshold of 0.5 for classification：soft labels——probs distribution for depth：直接用 Multi-Task Student Model 模型结构 shared back C5：for classification feature pyramids {P3,P4,P5,P6,P7}：for detection fused P2：for pixel-wise prediction，把feature pyramids rescale到level2然后sum heads classification head：ResNet design，GAP C5 + 线性层 object detection task：Mask R-CNN design，RPN是2 hidden convs，Fast R-CNN是4 hidden convs + 1 fc pixel-wise prediction heads：3 hiddent convs + 1 linear conv head，分割和深度估计任务independent，不share heads Teacher-student training using the same architecture same data augmentation teacher和student的main difference就是dataset和labels Learning From Multiple Teachers every image has supervision for all tasks labels may come from supervised or pseudo labels 如果使用ImageNet数据集，classification就是真标签，det/seg/depth supervision则是伪标签 balance the loss contribution 加权和，task-specific weights for ImageNet，use $w_i = \frac{b_slr_{it}}{b_{it}lr_{s}}$ follow the scaling rule：lr和batch size成正比 except for depth loss Cross Dataset Training training across ImageNet, object365 and COCO 有标签的就用原标签，没有的用伪标签，supervised labels and pseudo labels are treated equally，而不是分别采样和训练 balance the datasets：合在一起然后均匀采样 Transfer Learning 得到general student model以后，fine-tune on 一系列downstream tasks 这些downstream datasets与MuST model的训练数据都是not align的 这个实验要证明的是supervised model（如teacher model）和self-supervised model（如用pseudo label训练出来的student model），在downstream tasks上迁移学习能performance是差不多的，【注意⚠️：如果迁移datasets前后align就不是这样了，pretrain显然会更好！！！】]]></content>
      <tags>
        <tag>multi-task，self-training，</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GHM]]></title>
    <url>%2F2021%2F08%2F31%2FGHM%2F</url>
    <content type="text"><![CDATA[families: [class-imbalanced CE] [focal loss] [generalized focal loss] focal loss(CE)的连续版本 [ohem] keras implementation: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def weightedCE_loss(y_true, y_pred): alpha = .8 pt = K.abs(y_true-y_pred) # clip pt = K.clip(pt, K.epsilon(), 1-K.epsilon()) # ce ce = -K.log(1.-pt) # pos/neg reweight wce = tf.where(y_true&gt;0.5, alpha* , (1-alpha)* ) return wcedef focal_loss(y_true, y_pred): alpha = .25 gamma = 2 pt = K.abs(y_true-y_pred) # clip pt = K.clip(pt, K.epsilon(), 1-K.epsilon()) # easy/hard reweight fl = -K.pow(pt, gamma) * K.log(1.-pt) # pos/neg reweight fl = tf.where(y_true&gt;0.5, alpha*fl, (1-alpha)*fl) return fl def generalized_focal_loss(y_true, y_pred): # CE = -ytlog(yp)-(1-yt)log(1-yp) # GFL = |yt-yp|^beta * CE beta = 2 # clip y_pred y_pred = K.clip(y_pred, K.epsilon(), 1-K.epsilon()) # ce ce = -y_true*K.log(y_pred) - (1-y_true)*K.log(1-y_pred) # [N,C] # easy/hard reweight gfl = K.pow(K.abs(y_true-y_pred), beta) * ce return gfldef ce_ohem(y_true, y_pred): pt = K.abs(y_true-y_pred) # clip pt = K.clip(pt, K.epsilon(), 1-K.epsilon()) # ce ce = -K.log(1.-pt) # sort loss k = 50 ohem_loss, indices = tf.nn.top_k(ce, k=k) # topk loss: [k,], topk indices: [k,], idx among 0-b mask = tf.where(ce&gt;=ohem_loss[k-1], tf.ones_like(ce), tf.zeros_like(ce)) return mask*ce Gradient Harmonized Single-stage Detector 动机 one-stage detector 核心challenge就是imbalance issue imbalance between positives and negatives imbalance between easy and hard examples 这两项都能归结为对梯度的作用：a term of the gradient we propose a novel gradient harmonizing mechanism (GHM) balance the gradient flow easy to embed in cls/reg losses like CE/smoothL1 GHM-C for anchor classification GHM-R for bounding box refinement proved substantial improvement on COCO 41.6 mAP surpass FL by 0.8 论点 imbalance issue easy and hard： OHEM directly abandon examples 导致训练不充分 positive and negative focal loss 有两个超参，跟data distribution绑定 not adaptive 通常正样本既是少量样本又是困难样本，而且可以通通归结为梯度分布不均匀的问题 大量样本只贡献很小的梯度，通常对应着大量负样本，总量多了也可能会引导梯度（左图） hard样本要比medium样本数量大，我们通常将其看作离群点，因为模型稳定以后这些hard examples仍旧存在，他们会影响模型稳定性（左图） GHM的目标就是希望不同样本的gradient contribution保持harmony，相比较于CE和FL，简单样本和outlier的total contribution都被downweight，比较harmony（右图） we propose gradient harmonizing mechanism (GHM) 希望不同样本的gradient contribution保持harmony 首先研究gradient density，按照梯度聚类样本，并相应reweight 针对分类和回归设计GHM-C loss和GHM-R loss verified on COCO GHM-C比CE好得多，sligtly better than FL GHM-R也比smoothL1好 attains SOTA dynamic loss：adapt to each batch 方法 Problem Description define gradient norm $g = |p - p^*|$ the distribution g from a converged model easy样本非常多，不在一个数量级，会主导global gradient 即使收敛模型也无法handle一些极难样本，这些样本梯度与其他样本差异较大，数量还不少，也会误导模型 Gradient Density define gradient density $GD(g) = \frac{1}{l_{\epsilon}(g)} \sum_{k=1} \delta_{\epsilon}(g_k,g)$ given a gradient value g 统计落在中心value为$g$，带宽为$\epsilon$的范围内的梯度的样本量 再用带宽去norm define the gradient density harmony parameter $\beta_i = \frac{N}{GD(g_i)}$ N是总样本量 其实就是与density成反比 large density对应样本会被downweight GHM-C Loss 将harmony param作为loss weight，加入现有loss 可以看到FL主要压简单样本（基于sample loss），GHM两头压（基于sample density） 最终harmonize the total gradient contribution of different density group dynamic wrt mini-batch：使得训练更加efficient和robust Unit Region Approximation 将gradient norm [0,1]分解成M个unit region 每个region的宽度$\epsilon = \frac{1}{M}$ 落在每个region内的样本数计作$R_{ind(g)}$，$ind(g)$是g所在region的start idx the approximate gradient density：$\hat {GD}(g) = \frac{R_{ind(g)}}{\epsilon} =R_{ind(g)}M $ approximate harmony parameter &amp; loss： we can attain good performance with quite small M 一个密度区间内的样本可以并行计算，计算复杂度O(MN) EMA 一个mini-batch可能是不稳定的 所以通过历史累积来更新维稳：SGDM和BN都用了EMA 现在每个region里面的样本使用同一组梯度，我们对每个region的样本量应用了EMA t-th iteraion j-th region we have $R_j^t$ apply EMA：$S_j^t = \alpha S_j^(t-1) + (1-\alpha )R_j^t$ $\hat GD(g) = S_{ind(g)} M$ 这样gradient density会更smooth and insensitive to extreme data GHM-R loss smooth L1： 通常分界点设置成$\frac{1}{9}$ SL1在线性部分的导数永远是常数，没法去distinguishing of examples 用$|d|$作为gradient norm则存在inf 所以先改造smooth L1：Authentic Smooth L1 $\mu=0.02$ 梯度范围正好在[0,1) define gradient norm as $gr = |\frac{d}{\sqrt{d^2+\mu^2}}|$ 观察converged model‘s gradient norm for ASL1，发现大量是outliers 同样用gradient density进行reweighting 收敛状态下，不同类型的样本对模型的gradient contribution regression是对所有正样本进行计算，主要是针对离群点进行downweighting 这里面的一个观点是：在regression task里面，并非所有easy样本都是不重要的，在分类task里面，easy样本大部分都是简单的背景类，但是regression分支里面的easy sample是前景box，而且still deviated from ground truth，仍旧具有充分的优化价值 所以GHM-R主要是upweight the important part of easy samples and downweight the outliers 实验]]></content>
      <tags>
        <tag>single-stage detector, data imbalance, loss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R-FCN]]></title>
    <url>%2F2021%2F08%2F31%2FR-FCN%2F</url>
    <content type="text"><![CDATA[reference：https://zhuanlan.zhihu.com/p/32903856 引用量：4193 R-FCN: Object Detection via Region-based Fully Convolutional Networks 动机 region-based： 先框定region of interest的检测算法 previous methods：Fast/Faster R-CNN，apply costly per-region subnetwork hundreds of times fully convolutional 旨在解决Faster R-CNN第二阶段计算不共享，效率低的问题 we propose position-sensitive score maps translation-invariance in image classification translation-variance in object detection verified on PASCAL VOC 论点 主流的两阶段检测架构 two subnetworks a shared fully convolutional：这一部分提取通用特征，作用于全图 an RoI-wise subnetwork：这一部分不能共享计算，作用于proposals，因为是要针对每个位置的ROI进行分类和回归 也就是说，第一部分是位置不敏感的，第二部分是位置敏感的 网络越深越translation invariant，目标怎么扭曲、平移最终的分类结果都不变，多层pooling后的小feature map上也感知不到小位移，平移可变性（translation variance），对定位任务不友好 所以resnet-back-detector我们是把ROI Pooling放在stage4后面，跟一个RoI-wise的stage5 improves accuracy lower speed due to RoI-wise R-FCN 要解决的根本问题是RoI-wise部分不共享，速度慢：300个proposal要计算300次 单纯地将网络提前放到shared back里面不行，会造成translation invariant，位置精度会下降 必须通过其他方法加强网络的平移可变性，所以提出了position-sensitive score map 将全图划分为kxk个区域 position-sensitive score map：生成kxkx(C+1)个特征图 每个位置对应C+1个特征图 做RoIPooling的时候，每个bin来自每个position对应的C+1个map（这咋想的，space dim到channel dim再到space dim？） 方法 overview two-stage region proposal：RPN region classification：the R-FCN R-FCN 全卷积 输出conv层有kxkx(C+1)个channel kxk对应grid positions C+1对应C个前景+background 最后是position-sensitive RoI pooling layer aggregates from last conv and RPN？ generate scores for each RoI each bin aggregates responses from对应的position的channel score maps，而不是全部通道 force模型在通道上形成对不同位置的敏感能力 R-FCN architecture back：ResNet-101，pre-trained on ImageNet，block5 输出是2048-d 然后接了random initialized 1x1 conv，降维 cls brach 接$k^2(C+1)$的conv生成score maps 然后是Position-sensitive RoI pooling 将每个ROI均匀切分成kxk个bins 每个bin在对应的Position-sensitive score maps中找到唯一的通道，进行average pooling 最终得到kxk的pooling map，C+1个通道 将pooling map performs average pooling，得到C+1的vector，然后softmax box branch 接$4k^2$的conv生成score maps Position-sensitive RoI pooling 得到kxk的pooling map，4个通道 average pooling，得到4d vector，作为回归值$(t_x,t_y,t_w,t_h)$ there is no learnable layer after the ROI layer，enable nearly cost-free region-wise computation Training R-FCN positives / negatives：和gt box的IoU&gt;0.5的proposasl adopt OHEM sort all ROI loss and select the highest 128 其他settings基本和Faster-RCNN一致 Atrous and stride 特别地，对resnet的block5进行了改变 stride2改成stride1 所有的conv改成空洞卷积 RPN是接在block4的输出上，所以不受空洞卷积的影响，只影响R-FCN head]]></content>
      <tags>
        <tag>目标检测，全卷积，region-based，two-stage detector</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Meta Pseudo Labels]]></title>
    <url>%2F2021%2F08%2F23%2FMeta-Pseudo-Labels%2F</url>
    <content type="text"><![CDATA[papers [MPL 2021] Meta Pseudo Labels [UDA 2009] Unsupervised Data Augmentation for Consistency Training [Entropy Minimization 2004] Semi-supervised Learning by Entropy Minimization Meta Pseudo Labels 动机 semi-supervised learning Pseudo Labels：fixed teacher Meta Pseudo Labels：constantly adapted teacher by the feedback of the student SOTA on ImageNet：top-1 acc 90.2% 论点 Pseudo Labels methods teacher generates pseudo labels on unlabeled images pseudo labeled images are then combined with labeled images to train the student confirmation bias problem：student的精度取决于伪标签的质量 we propose Meta Pseudo Labels teacher observes how its pseudo labels affect the student then correct the bias the feedback signal is the performance of the student on the labeled dataset 总的来说，teacher和student是train in parallel的 student learns from pseudo labels from the teacher teacher learns from reward signal from how well student perform on labeled set dataset ImageNet as labeled set JFT-300M as unlabeled set model teacher：EfficientNet-L2 student：EfficientNet-L2 main difference Pseudo Labels方法中，teacher在单向的影响student Meta Pseudo Labels方法中，teacher和student是交互作用的 方法 notations models teacher model T &amp; $\theta_T$ student model S &amp; $\theta_S$ data labeled set $(x_l, y_l)$ unlabeled set $(x_u)$ predictions soft predictions by teacher $T(x_u, \theta_T)$ student $S(x_u, \theta_S)$ &amp; $S(x_l, \theta_S)$ loss $CE(q,p)$，其中$q$是one-hot label，e.g. $CE(y_l, S(x_l, \theta_S))$ Pseudo Labels given a fixed teacher $\theta_T$ train the student model to minimize the cross-entropy loss on unlabeled data \theta_S^{PL} = argmin_{\theta_S}CE(T(x_u,\theta_T), S(x_u, \theta_S)) $\theta_S^{PL}$ also achieve a low loss on labeled data $\theta_S^{PL}$ explicitly depends on $\theta_T$：$\theta_S^{PL}(\theta_T)$ student loss on labeled data is also a function of $\theta_T$：$L_l(\theta_S^{PL}(\theta_T))$ Meta Pseudo Labels intuition：minimize $L_l$ with respect to $\theta_T$ 但是实际上dependency of $\theta_S^{PL}(\theta_T)$ on $\theta_T$ 非常复杂 因为我们用了teacher prediction的hard labels去训练student an alternating optimization procedure teacher’s auxiliary losses augment the teacher’s training with a supervised learning objective and a semi-supervise learning objective supervised objective train on labeled data CE semi-supervised objective train on unlabeled data UDA(Unsupervised Data Augmentation)：将样本进行简单增强，通过衡量一致性损失，模型的泛化效果得到提升 consistency training loss：KL散度 finetuning student 在meta pseudo labels训练过程中，student only learns from the unlabeled data 所以在训练过程结束后，可以finetune it on labeled data to improve accuracy overall algorithm * 这里面有一处下标写错了，就是teacher的UDA gradient，是在unlabeled data上面算的，那两个$x_l$得改成$x_u$ * UDA loss论文里使用两个predicted logits的散度，这里是CE Unsupervised Data Augmentation for Consistency Training 动机 data augmentation in previous works 能在一定程度上缓解需要大量标注数据的问题 多用在supervised model上 achieved limited gains we propose UDA apply data augmentation in semi-supervised learning setting use harder and more realistic noise to generate the augmented samples encourage the prediction to be consistent between unlabeled &amp; augmented unlabeled sample 在越小的数据集上提升越大 verified on six language tasks three vision tasks ImageNet-10%：：top1/top5 68.7/88.5% ImageNet-extra unlabeled：top1/top5 79.0/94.5% 论点 semi-supervised learning three categories graph-based label propagation via graph convolution and graph embeddings modeling prediction target as latent variables consistency / smoothness enforcing 最后这一类方法shown to work well， enforce the model predictions on the two examples to be similar 主要区别在于perturbation function的设计 we propose UDA use state-of-the-art data augmentation methods we show that better augmentation methods(AutoAugment) lead to greater improvements minimizes the KL divergence can be applied even the class distributions of labeled and unlabeled data mismatch we propose TSA a training technique prevent overfitting when much more unlabeled data is avaiable than labeled data 方法 formulation given an input $x\in U$ and a small noise $\epsilon$ compute the output distribution $p_{\theta}(y|x)$ and $p_{\theta}(y|x,\epsilon)$ minimize the divergence between two predicted distributions $D(p_{\theta}(y|x)||p_{\theta}(y|x,\epsilon))$ add a CE loss on labeled data UDA的优化目标 enforce the model to be insensitive to perturbation thus smoother to the changes in the input space $\lambda=1$ for most experiments use different batchsize for labeled &amp; unlabeled Augmentation Strategies for Different Tasks AutoAugment for Image Classification 通过RL搜出来的一组optimal combination of aug operations Back translation for Text Classification TF-IDF based word replacing for Text Classification Trade-off Between Diversity and Validity for Data Augmentation 对原始sample做变换的时候，有一定概率导致gt label变化 AutoAugment已经是optmial trade-off了，所以不用管 text tasks需要调节temperature Additional Training Techniques TSA(Training Signal Annealing) situation：unlabeled data远比labeled data多的情况，我们需要large enough model去充分利用大数据，但又容易对小trainset过拟合 for each training step set a threshold $\frac{1}{K}\leq \eta_t\leq 1$，K is the number of categories 如果样本在gt cls上的预测概率大于这个threshold，就把这个样本的loss去掉 $\eta_t$ serves as a ceiling to prevent the model from over-training on examples that the model is already confident about gradually release the training signals of the labeled examples，缓解overfitting schedules of $\eta_t$ log-schedule：$\lambda_t = 1-exp(-\frac{t}{T}*5)$ linear-schedule：$\lambda_t = \frac{t}{T}$ exp-schedule：$\lambda_t = exp((\frac{t}{T}-1)*5)$ 如果模型非常容易过拟合，用exp-schedule，反过来（abundant labeled data/effective regularizations），用log-schedule Sharpening Predictions situation：the predicted distributions on unlabeled examples tend to be over-flat across categories，task比较困难，训练数据比较少时，在unlabeled data上每类的预测概率都差不多低，没有倾向性 这时候KL divergence的监督信息就很弱 thus we need to sharpen the predicted distribution on unlabeled examples Confidence-based masking：将current model not confident enough to predict的样本过滤掉，只保留最大预测概率大于0.6的样本计算consistency loss Entropy minimization：add an entropy term to the overall objective softmax temperature：在计算softmax时先对logits进行rescale，$Softmax(logits/\tau)$，a lower temperature corresponds to a sharper distribution in practice发现Confidence-based masking和softmax temperature更适用于小labeled set，Entropy minimization适用于相对大一点的labeled set Domain-relevance Data Filtering 其实也是Confidence-based masking，先用labeled data训练一个base model，然后inference the out-of-domain dataset，挑出预测概率较大的样本 Semi-supervised Learning by Entropy Minimization]]></content>
      <tags>
        <tag>semi-supervised learning, teacher-student, classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Generalized Focal Loss]]></title>
    <url>%2F2021%2F08%2F20%2FGeneralized-Focal-Loss%2F</url>
    <content type="text"><![CDATA[Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection 动机 one-stage detectors dense prediction three fundamental elements class branch box localization branch an individual quality branch to estimate the quality of localization current problems the inconsistent usage of the quality estimation in train &amp; test the inflexible Dirac delta distribution：将box regression的value建模成真值附近的脉冲分布，用来描述边界不清晰/遮挡的case可能不准确 we design new representations for these three elements merge quality estimation into class prediction：将objectness/centerness整合进cls prediction，直接用作NMS score continout labels propose GFL(Generalized Focal Loss) that generalizes Focal Loss from discrete form into continous version test on COCO ResNet-101-?-GFL: 45.0% AP defeat ATSS 论点 inconsistent usage of localization quality estimation and classification score 训练的时候quality和cls branch是independent branch box branch的supervision只作用在positive样本上：which is unreliable on predicting negatives 测试阶段将quality和cls score乘起来有可能拉高负样本的分数，以至于在NMS阶段把低分正样本挤掉 inflexible representation of bounding boxes most method建模成脉冲分布：只在IoU大于一定阈值的格子上有响应，别的格子都是0 some recent work建模成高斯分布 in fact the real distribution can be more arbitrary and flexible，连续且不严格镜像 thus we propose merge the quality representation into the class branch： class vector的每个元素代表了格子的localization quality(如IoU score) 在inference阶段也是直接用作cls score propose arbitrary/general distribution 有明确边界的目标的边的分布是比较sharp的 没有明显边界的边分布就是flatten一点 Generalized Focal Loss (GFL) joint class representation是continuous IoU label (0∼1) imbalance问题仍然存在，但是standart Focal Loss仅支持[0,1] sample 修改成continuous形式，同时specialized into Quality Focal Loss (QFL) and Distribution Focal Loss (DFL) QFL for cls branch：focuses on a sparse set of hard examples DFL for box branch： focus on learning the probabilities of values around the continuous target locations 方法 Focal Loss (FL) standard CE part：$-log(p_t)$ scaling factor：down-weights the easy examples，focus on hard examples Quality Focal Loss (QFL) soft one-hot label：正样本在对应类别上有个(0,1]的float score，负样本全0 float score定义为预测框和gt box的IoU score we adopt multiple binary classification with sigmoid modify FL CE part 改成complete form：$-ylog(\hat y)-(1-y)log(1-\hat y)$ scaling part用vector distance替换减法：$|y-\hat y |^{\beta}$ $\beta$ controls the down-weighting rate smoothly &amp; $\beta=2$ works best Distribution Focal Loss (DFL) use relative offsets from the location to the four sides of a bounding box as the regression targets 回归问题formulation 连续：$\hat y = \int_{y_0}^{y_n}P(x)xdx$ 离散化：$\hat y = \sum_{i=0}^n P(y_i)y_i$ P(x) can be easily implemented through a softmax layer containing n+1 units： DFL force predictions to focus values near label $y$：explicitly enlarge the probabilities of $y_i$和$y_{i+1}$，given $y_i \leq y \leq y_{i+1}$ $log(S_i)$ force the probabilities gap balance the 上下限，使得$\hat y$的global mininum solution无限逼近真值$y$，如果真值接近的是$\hat y_{i+1}$，可以看到$log(S_i)$那项被downscale了 Generalized Focal Loss (GFL) 以前的cls preditions在测试阶段要结合quality predictions作为NMS score，现在直接就是 以前regression targets每个回归一个值，现在是n+1个值 overall 第一项cls loss，就是QFL，dense on 所有格子，用正样本数去norm 第二项box loss，GIoU loss + DFL，$\lambda_0$默认2，$\lambda_1$默认1/4，只计算有IoU的格子 we also utilize the quality scores to weight $L_B$ and $L_D$ during training 彩蛋 IoU branch always superior than centerness-branch centerness天生值较小，影响召回，IoU的值较大]]></content>
      <tags>
        <tag>one-stage detector, object-detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[soft teacher]]></title>
    <url>%2F2021%2F08%2F12%2Fsoft-teacher%2F</url>
    <content type="text"><![CDATA[keywords：semi-supervised, curriculum, pseudo labels, End-to-End Semi-Supervised Object Detection with Soft Teacher 动机 end-to-end training：相比较于其他方法的multi-stage semi-supervised：用外部unlabeled数据，以及pseudo-label based approach propose two techniques soft teacher mechanism：pseudo样本的classification loss用teacher model的prediction score来加权 box jittering mechanism：挑选reliable pseudo boxes verified use SWIN-L as baseline metric on COCO：60.4 mAP if pretrained with Object365：61.3 mAP 论点 we present this end-to-end pseudo-label based semi-supervised object detection framework simultaneously performs pseudo-labeling：teacher training detector use the current pseudo-labels &amp; a few training sample：student teacher is an exponential moving average (EMA) of the student model mutually enforce each other soft teacher approach teacher model的作用是给student model生成的box candidates打分， 高于一定阈值的为前景，但是可能有部分前景被归类为背景，所以用这个score作为reliability measure，给标记为背景框的cls loss进行加权 reliability measure 方法 overview 两个model：student和teacher teacher model用来生成pseudo labels：two set of pseudo boxes，一个用于class branch，一个用于regression branch student model用supervised&amp;unsupervised sample的loss来更新 teacher model用student model的EMA来更新 two crucial designs soft teacher box jittering 整体的工作流程就是，每个training iteration，先按照一定比例抽取labeled&amp;unlabeled sample构成data batch，然后用teacher model生成unlabeled data的pseudo label（thousands of box candidates+NMS+score filter），然后将其作为unlabeled sample的ground truth，训练student model，overall loss是supervised loss和unsupervised loss的加权和 在训练开始阶段，两个模型都是随机初始化的，teacher模型随着student模型的更新而更新 FixMatch： 输入给teacher模型的样本使用weak aug 输入给student模型的样本使用strong aug soft teacher detector的pseudo-label质量很重要 所以用score thresh=0.9去定义box candidates的前/背景 但是这时候如果用传统的IoU来定义student model的box candidates的pos/neg，会有一部分前景框被当作背景 to alleviate assess the reliability of each student-generated box candidate to be a real background given a student-generated box candidate，用teacher model的detection head去预测这个框的background score overall unsupervised cls loss L_u^{cls} = \frac{1}{N_b^{fg}} \sum_i^{N_b^{fg}} l_{cls} (b_i^{fg}, G_{cls}) + \sum_j^{N_b^{bg}}w_j l_{cls} (b_j^{bg}, G_{cls})\\ w_j = \frac{r_j}{\sum_{k=1}^{N_b^{bg}}r_k} $G_{cls}$是the set of boxes teacher generated for classification，就是teacher model预测的top1000经过nms和score filter之后的boxes $b_i^{fg}$是student candidates中被assign为前景的框，$b_i^{bg}$是student candidates中被assign为背景的框，assign的原则就是score&gt;0.9 $w_j$是对assign为背景的框的加权 $r_k$是reliability score，是student model通过hard score thresh assign为背景的框，用teacher model的detection head去预测的bg score box jittering fg score thresh和box iou并不呈现strong positive correlation，说明基于这个原则产生的框pseudo-labels并不一定适合box regression localization reliability： 衡量一个pseudo box的consistency given a pseudo box，sample一系列jitter box around it，再用teacher model去预测这些jitter box得到refined boxes refined box和pseudo box的variance越小，说明这个框的localization reliability越高 \hat b_i = refine(jitter(b_i))\\ \overline \sigma_i = \frac{1}{4} \sum_1^4 \hat \sigma_k\\ \hat \sigma_k = \frac{\sigma_k}{0.5 (h(b_i)+w(b_i))} $\hat b_i$是refined boxes $\sigma_k$是refine boxes的四个坐标基于原始box的标准差 $\hat \sigma_k$是上面那个标准差基于原始box的尺度进行归一化 $\overline\sigma$是refine boxes四个坐标的normed std的平均值 只计算teacher box candidates里面，fg score&gt;0.5的那部分 overall unsupervised reg loss L_u^{reg} = \frac{1}{N_b^{fg}} \sum_1^{N_b^{fg}} l_{reg} (b_i^{fg}, G_{reg}) $b_i^{fg}$是student candidates中被assign为前景的框，即cls score&gt;0.9那些预测框 $G_{cls}$是the set of boxes teacher generated for regression，就是jittered reliability大于一定阈值的candidates overall unsupervised loss：cls loss和reg loss之和，然后用样本数进行norm 实验]]></content>
      <tags>
        <tag>semi-supervised, object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GNN&GCN]]></title>
    <url>%2F2021%2F07%2F13%2FGNN-GCN%2F</url>
    <content type="text"><![CDATA[综述 reference：https://www.cnblogs.com/siviltaram/p/graph_neural_network_2.html key concepts 图神经网络（Graph Neural Network，GNN） 图卷积神经网络（Graph Convolutional Neural Network） 频域（Spectral-domain） 空域（Spatial-domain） 图神经网络 image &amp; graph 节点（Node） 每个节点有其特征，用$x_v$表示 边（Edge） 连接两个节点的边也有其特征，用$x_{v,u}$表示 隐藏状态 图的学习目标是获得每个节点的隐藏状态 局部输出函数 选取一个节点 图卷积 一张图片就可以看作一个非常稠密的图，阴影部分代表卷积核，右侧则是一个普通的图，和图卷积核 在image为代表的欧式空间中，结点的邻居数量都是固定的，但在graph这种非欧空间中，结点有多少邻居并不固定 传统的卷积核不能直接用于抽取图上结点的特征 两个主流思路 把非欧空间的图转换成欧式空间，然后使用传统卷积 找出一种可处理变长邻居结点的卷积核在图上抽取特征]]></content>
      <tags>
        <tag>图神经网络，图卷积</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[few-shot]]></title>
    <url>%2F2021%2F06%2F22%2Ffew-shot%2F</url>
    <content type="text"><![CDATA[综述 few-shot few-shot learning：通过少量样本学习识别模型 问题：过拟合&amp;泛化性，数据增强和正则能在一定程度上缓解但不解决，还是推荐从大数据上迁移学习 共识： 样本量有限的情况下，不依靠外部数据很难得到不错的结果，当下所有的解决方案都是借助外部数据作为先验知识，构造学习任务 迁移数据也不是随便找的，数据集的domain difference越大，迁移效果越差（e.g. 用miniImagenet做类间迁移，效果不错，但是用miniImagenet做base class用CUB做novel class，学习效果会明显下降） 数据集： miniImagenet：自然图像，600张，100类 Omniglot：手写字符，1623张，50类 CUB：鸟集，11788张，200类，可用于细粒度，可以用于zero-shot methods pretraining + finetuning pretraining阶段用base class训练一个feature extractor finetuning阶段fix feature extractor重新训练一个classifier 基于度量学习 引入distance metric其实都算度量学习，所以上面（pretraining+finetuning）和下面（meta learning）的方法都有属于度量学习的方法 基于元学习 base class&amp;novel class：base class是已有的大数据集，多类别，大样本量，novel class是我们要解决的小数据集，类别少，每类样本也少 N-way-K-shot：基于novel class先在base class上构建多个子任务，N-way就是构建随机N个类别的分类任务，K-shot就是每个类别对应样本量为K supportset S &amp; queryset Q：N-way-K-shot的训练集和测试集，来自base class中相同的类别，均用于training procedure 与传统分类任务对比： leaderboard：https://few-shot.yyliu.net/miniimagenet.html papers [2015 siamese]：Siamese Neural Networks for One-shot Image Recognition，核心思想就是基于孪生网络构建similarity任务，用一个大数据集构造的same/diff pairs去训练，然后直接用在novel set上，metric是reweighted L1 [2016 MatchingNet]：Matching Networks for One Shot Learning，本质上也是孪生网络+metric learning，监督的是support set S和test set B的相似度——在S下训练的模型在B的预测结果误差最小，网络上的创新是用了memory&amp;attention，train procedure的创新在于“test and train conditions must match N-way-K-shot”， [2017 ProtoNet]：Prototypical Networks for Few-shot Learning， [2019 few-shot综述]：A CLOSER LOOK AT FEW-SHOT CLASSIFICATION Siamese Neural Networks for One-shot Image Recognition 动机 learning good features is expensive when little data is available：一个典型任务one-shot learning we desire generalize to the new distribution without extensive retraining we propose train a siamese network to rank similarity between inputs capitalize on powerful discriminative features generalize the network to new data/new classes experiment on character recognition 方法 general strategy learn image representation：supervised metric-based approach，siamese neural network reuse the feature extractor：on new data，without any retraining why siamese we hypothesize that networks which do well at verification tasks should generalize to one-shot classification siamese nets twin networks accept distinct inputs that are joined by an energy function at the top twin back shares the weights：symmetric 原始论文用了contrastive energy function：contains dual terms to increase like-pairs energy &amp; decrease unlike-pairs energy in this paper we use weighted L1 + sigmoid model conv-relu-maxpooling：conv of varying sizes 最后一个conv-relu完了接flatten-fc-sigmoid得到归一化的feature vector 然后是joined layer：计算两个feature vector的L1 distance后learnable reweighting 然后接sigmoid loss binary classifier regularized CE loss function里面加了layer-wise-L2正则 bp的时候两个孪生网络的bp gradient是additive的 weight initialization conv weights：mean 0 &amp; std var 0.01 conv bias：mean 0.5 &amp; std var 0.01 fc weights：mean 0 &amp; std var 0.2 fc bias：mean 0.5 &amp; std var 0.01 learning schedule uniform lr decay 0.01 individual lr rate &amp; momentum annealing augmentation individual affine distortions 每个affine param的probability 0.5 &lt;img src=&quot;few-shot/affine.png&quot; width=&quot;45%;&quot; /&gt; 实验 dataset Omniglot：50个字母（international/lesser known/fictitious） 训练用的子集：60% of the total data，12个drawer创建的30个字母，每类样本数一样多 validation：4个drawer的10个字母 test：4个drawer的10个字母 8个affine transforms：9倍样本量，same&amp;different pairs 在不经过微调训练的条件下，模型直接应用在MNIST数据集，仍有70%的准确率：泛化能力 评价 孪生网络对于两个图像之间的差异是非常敏感的 一只黄色的猫和黄色的老虎之间的差别要比一只黄色的猫和黑色的猫之间的差别更小 一个物体出现在图像的左上角和图像的右下角时其提取到的特征信息可能截然不同 尤其经过全连接层，空间位置信息被破坏 手写字符数据集相比较于ImageNet太简单了 优化网络结构：MatchingNet 更好的训练策略：meta learning 现在去复现已经没啥意义，算是metric learning在小样本学习上的一个startup吧 MatchingNet: Matching Networks for One Shot Learning 动机 learning new concepts rapidly from little data employ ideas metric learning memory cell define one-shot learning problems Omniglot ImageNet language tasks 论点 parametric models learns slow and require large datasets non-parametric models rapidly assimilate new examples we aim to incorporate both we propose Matching Nets uses recent advances in attention and memory that enable rapid learning test and train conditions must match：如果要测试一个n类的新分布，就要在m类大数据集上训类似的minibatch——抽n个类，每类show a few examples 方法 build one-shot learning within the set-to-set framework 训练以后的模型不需要进一步tuning就能produce sensible test labels for unobserved classes given a small support set $S=\{(x_i,y_i)\}^k_{i=0}$ train a classifier $c_S$ given a test example $\hat x$：we get a probability distribution $\hat y=c_S(\hat x)$ define the mapping：$S \rightarrow c_S $ to be $P(\hat y| \hat x ,S)$ when given a new support set $S^{‘}=\{\hat x\}$：直接用模型P去预测$\hat y$就可以了 simplest form： \hat y = \sum_{i=1}^k a(\hat x, x_i)y_i a是attention mechanism：如果和测试样本$\hat x$最远的b个支持样本$x_i$的attention是0，其余为一个定值，这就等价于一个k-b-NN机制 $y_i$ act as memories：可以把每个$y_i$看作是每个$x_i$提取到的信息保存成memory workflow定义：given a input，我们基于attention锁定corresponding samples in the support set，并retrieve the label attention kernel 用一个embedding function先将$\hat x$和$x_i$转化成embeddings 然后计算和每个$x_i$ embedding的cosine distance 然后softmax，得到每个的attention value softmax之后的attention value，大部分是N选1，如果每个attention value都不高，说明query sample和训练集每类都不像，是个novel Full Context Embeddings（FCE） 简单的模式下f和g就是两个shared weights的CNN feature extractor，FCE是接在常规feature vector后面精心设计的一个结构 设计思路 g：support set don’t get embedded individually f：support set modify how we embed the test image the first issue： bidirectional Long-Short Term Memory encoder the whole support set as contexts，each time step的输入是$g^{‘}(x_i)$ skip connection g(x_i) = \overrightarrow{h_i}+\overleftarrow{h_i}+g^{'}(x_i) the second issue LSTM with read attention over the whole set S $f(\hat x, S)=attLSTM(f^{‘}(\hat x), g(S), K)$ $f^{‘}(\hat x)$是query sample的feature vector，作为LSTM each time step的输入 $K$是fixed number of unrolling steps，限制LSTM计算的step，也就是feature vector参与LSTM循环计算的次数，最终的输出是$h_K$ skip connection as above support set S的引入： content based attention + softmax $r_{k-1}$和$h_{k-1}$是concat到一起，作为hidden states：【QUESTION】这样lstm cell的hidden size就变了啊？？？ attention of K fixed unrolling steps encode $x_i$ in the context of the support set S training strategy the training procedure has to be chosen carefully so as to match the never seen task define：从全集中选取few unique classes(e.g. 5)，每个类别选取few examples(e.g. 1-5)，构成support set S，再从对应类别抽一个batch B，训练目标就是minimise the error predicting the labels in the batch B conditioned on the support set S batch B的预测过程就是figure1：需要$g(S(x_i,y_i))$和$f(\hat x)$计算$P(\hat y|\hat x, S)$，然后和$gt(\hat y)$计算log loss 实验 模式 N-way-K-shot train one-shot test：用唯一的one-shot novel sample生成对应类别的feature vector，然后对每个test sample计算cosine distance，选择最近的作为其类别 comparing methods baseline classifier + NN MANN Convolutional Siamese Net + NN further finetuning：one-shot 结论 using more examples for k-shot classification helps all models 5-way is easier than 20-way siamese net在5-shot的时候跟our method差不多，但是one-shot degrades rapidly FCE在简单数据集（Omniglot）上没啥用，在harder task（miniImageNet）显著提升 A CLOSER LOOK AT FEW-SHOT CLASSIFICATION 动机 为主流方法提供一个consistent comparative analysis，并且发现： deeper backbones significantly reduce differences reducing intra-class variation is an important factor when shallow backbone propose a modified baseline method achieves com- petitive performance verified on miniImageNet &amp; CUB in realistic cross-domain settings generalization analysis baseline method with standard fine-tuning win 论点 three main categories of methods initialization based aims to learn good model initialization to achieve rapid adaption with a limited number of training samples have difficulty in handling domain shifts metric learning based 训练目标是learn to compare if a model can determine the similarity of two images, it can classify an unseen input image with the labeled instances：本质是similarity计算器，脱离label level 花式训练策略：meta learning/graph 花式距离metric：cosine/Euclidean turns out大可不必： a simple baseline method with a distance- based classifier is competitive to the sophisticated algorithms simply reducing intra-class variation in a baseline method leads to competitive performance hallucination based 用base class训练一个生成模型，然后用生成模型给novel class造假数据 通常和metric-based模型结合起来用，不单独分析 two main challenges 没法统一横向比较 implementation details有差异，baseline approach被under-estimated：无法准确量化the relative performance gain lack of domain shift between base &amp; novel datasets：makes the evaluation scenarios unrealistic our work 针对代表性方法conduct consistent comparative experiments on common ground discoveries on deeper backbones 轻微改动baseline method获得显著提升 replace the linear classifier with distance-based classifier practical sceneries with domain shift 发现这种现实场景下，那些代表性的few-shot methods反而干不过baseline method open source code：https://github.com/wyharveychen/CloserLookFewShot 方法 baseline standard transfer learning：pre-training + fine-tuning training stage train a feature extractor $f_{\theta}$ and a classifier $C_{W_b}$ use abundant base class labeled data standard CE loss fine-tuning stage fix feature extractor $f_{\theta}$ train a new classifier $C_{W_n}$ use the few labeled novel samples standard CE loss baseline++ variant of the baseline：唯一的不同就在于classifier design 显式地reduce intra-class varation among features during training，和center loss思路有点像，但是center loss的质心是滑动平均的，这里面的质心是learnable的 training stage write the weight matrix $W_b$ as $[w_1, w_2, …, w_c]$，类似每类的簇心 for an input feature，compute cosine similarity multiply a class-wise learnable scalar to adjust origin [-1,1] value to fit softmax 然后用softmax对similarity vector进行归一化，作为predict label the softmax function prevents the learned weight vectors collapsing to zeros：每类的预测distance都是0是网络比较容易陷入的局部最优解 【in fine-tuning stage？？】 meta-learning algorithms three distance metric based methods：MatchingNet，ProtoNet，RelationNet one initialization based method：MAML meta-training stage a collection of N-way-K-shot tasks 使得模型$M(*|S)$学会的是一种学习模式——在有限数据下做预测 meta-testing stage 所有的novel data都作为对应类别的support set (class mean) 模型就用这个新的support set来进行预测 Different meta-learning methods主要区别在于如何基于support set做预测，也就是classifier的设计 MatchingNet计算的是query和support set的每个cosine distance，然后mean per class ProtoNet是先对support features求class mean，然后Euclidean distance RelationNet先对support features求class mean，然后将距离计算模块替换成learnable relation module 实验 three scenarios generic object recognition：mini-ImageNet，100类，600张per class，【64-base，16-val，20-novel】 fine-grained image classification：CUB-200-2011，200类，总共11,788张，【random 100-base，50-val，50-novel】 cross-domain adaptation：mini-ImageNet —&gt; CUB，【100-mini-ImageNet-base，50-CUB-val，50-CUB-test】 training details baseline和baseline++模型：train 400 epochs，batch size 16 meta learning methods： train 60000 episodes for 5-way-1-shot tasks，train 40000 episodes for 5-way-5-shot tasks use validation set to select the training episodes with the best acc k-shot for support set，16 instances for query set Adam with 1e-3 initial lr standard data augmentation：crop，left-right flip，color jitter testing stage average over 600 experiments each experiment randomly choose 5-way-k-shot support set + 16 instances query set meta learning methods直接基于support set给出对query set的预测结果 baseline methods基于support set训练一个新的分类头，100 iterations，batch size 4 模型details baseline++的similarity乘上了class-wise learnable scalar MachingNet用了FCE classification layer without fine-tuning版本，也乘了class-wise learnable scalar RelationNet将L2 norm替换成softmax加速训练 MAML使用了一阶梯度近似for efficiency 初步结果 4-layer conv backbone input size 84x84 origin和re-implementation的精度对比 原始的baseline没加data augmentation，所以过拟合了精度差，被underestimated了 MatchingNet加了那个scalar shift的改进以后精度有显著提升 ProtoNet原论文是20-shot&amp;30-shot，本文主要比较1-shot和5-shot，精度都放出来了 our experiment setting下各模型的精度对比 baseline++大幅提升精度，已经跟meta learning methods差不多了 说明few-shot的key factor是reduce intra-class variation 但是要注意的是这是在4-layer-conv的backbone setting下，deeper backbone can inherently reduce intra-class variation 增加网络深度 上面说了，deeper backbone能够隐式地降低类内距离 deeper models conv4 conv6：相对于conv4那个模型，加了两层conv blocks without pooling resnet10：简化版resnet18，r18里面conv block的两层卷积换成一层 resnet18：origin paper resnet34：origin paper 随着网络加深，各方法的精度差异缩小，baseline方法甚至反超了一些meta learning方法 effect of domain shift 一个现实场景：mini-ImageNet —&gt; CUB，收集general class data相对容易，收集fine-grained数据集则更困难 用resnet18实验 Baseline outperforms all meta-learning methods under this scenario 因为meta learning methods的学习完全依赖于base support class，not able to adapt 随着domain difference get larger，Baseline相对于其他方法的gap也逐渐拉大 说明了在domain shift场景下，adaptation based method的必要性 further adapt meta-learning methods MatchingNet &amp; ProtoNet：跟baseline方法一样，fix feature extractor，然后用novel set train a new classifier MAML：not feasible to fix the feature，用novel set finetune整个网络 RelationNet：features是conv maps而不是vector，randomly split一部分novel set作为训练集 MatchingNet &amp; MAML都有大幅精度提升，尤其在domain shift场景下，但是ProtoNet会掉点，说明adaptation是影响精度的key factor，但是还没有完美解决方案]]></content>
      <tags>
        <tag>小样本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ATSS]]></title>
    <url>%2F2021%2F06%2F17%2FATSS%2F</url>
    <content type="text"><![CDATA[ATSS: Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection 动机 anchor-based和anchor-free方法的本质区别是对正负样本的定义，这也直接导致了performance gap we propose ATSS adaptive training sample selection automatically select positive and negative samples according to statistical characteristics of objects anchor-based&amp;anchor-free模型上都涨点 discuss tiling multiple anchors 论点 主流anchor-based方法 one-stage/two-stage tile a large number of preset anchors on the image output these refined anchors as detection results anchor-free detectors主要分成两种 key-point based：预测角点/轮廓点/heatmap，然后bound轮廓得到框 center-based：预测中心点，然后基于中心点回归4个距离 消除pre-defined anchors的hyper-params：强化generalization ability 举例对比RetinaNet&amp;FCOS RetinaNet：one-stage anchor-based FCOS：center-based anchor-free 主要区别1：anchor数量，RetinaNet是hxwx9，FCOS是hxwx1 主要区别2：正样本定义，RetinaNet是与gt box的IOU大于一定阈值的anchor，FCOS是featuremap上所有落进框内的格子点 主要区别3：回归方式，RetinaNet是回归gt相对pos anchor的相对偏移量，FCOS是预测四条边相对中心点的绝对距离 Difference Analysis of Anchor-based and Anchor-free Detection we focus on the last two differences：正负样本定义 &amp; 回归starting status 设定RetinaNet也是one square anchor per location，和FCOS保持一致 experiment setting MS COCO：80类前景，common split ImageNet pretrained ResNet-50 resize input SGD，90K iterations，0.9 momentum，1e-4 weight decay，16 batch size，0.01 lr with 0.1 lr decay/60K testing： 0.05 score to filter out bg boxes output top 1000 detections per feature pyramid 0.6 IoU thresh per class NMS to give final top 100 detections per image inconsistency removal 五大improvements加在FCOS上进一步boost the gap 我们将其逐步加在RetinaNet上，能够拉到37%，和FCOS还有0.8个点的差距 分析essential difference 训练一个检测模型，首先要分出正负样本，然后用正样本来回归 Classification RetinaNet用anchor boxes与gt box的IoU决定正负样本：best match anchor和大于一定IoU thresh的anchor是正样本，小于一定IoU thresh的anchor是负样本，其他的是ignore样本 FCOS用spatial and scale constraints选择正负样本：gt box以内的所有像素作为候选正样本，然后去掉部分尺度不匹配的候选样本，正样本以外都是负样本，没有ignore 两个模型在两种样本选择策略上实验：Spatial and Scale Constraint相比较于IoU都会显著提点 当两种方法都使用Spatial and Scale Constraint策略选择正负样本，模型精度就没啥差别了 Regression RetinaNet regresses from the anchor box with 4 offsets：回归gt box相对于anchor box的偏移量，regression starting status是个box FCOS regresses from the anchor point with 4 distances：回归gt box四条边相对于anchor center的距离，regression starting status是个point 上面那个表说明了选择同样的正负样本，regression starting status就是个无关项，不影响精度 Adaptive Training Sample Selection （ATSS） 影响检测模型精度的essential difference在于how to define positive and negative training samples previous strategies都有sensitive hyperparameters（anchors/scale），some outer objects may be neglected we propose ATSS almost no hyper divides pos/neg samples according to data statistical characteristics 对每个gt box，首先在每个level上，基于L2 center distance，找到k-closest anchor——k*L个candidates per gt box 计算每个candidates的mean &amp; var 基于mean &amp; var 计算这个gt box的IoU thresh 在candidates里面选取大于等于IoU thresh，同时anchor center在gt box内的，留作正样本 如果一个acnhor box匹配了多个gt box，选择IoU大的那个作为标签 基于center distance选择anchor box：因为越靠近目标中心，越容易produce高品质框 用mean+var作为IoU thresh： higher mean indicates high-quality candidates，对应的IoU thresh应该高一点 higher variation indicates level specific，mean+var作为thresh能将candidates里面IoU较高的筛选出来 limit the anchor center in object：anchor中心不在目标框内显然不是个好框，用于筛掉前两步里的漏网之鱼，双保险 fair between different objects 统计下来每类目标都有差不多0.2kL个正样本，与尺度无关 但是RetinaNet和FCOS都是大目标正样本多，小目标正样本少 hyperparam-free：只有一个k，【还有anchor-setting呢？？？】 verification lite version：被FCOS官方引用并称作center sampling，scale limit still exists in this version full version：本文版本 两个方法选择candidates的方法完全一致，就是select final postives的方法不同 hyperparam的鲁棒性 k在一定范围内（7-17）相对insensitive，太多了低质量框太多，太少了less statistical 尝试不同的fix-ratio anchor scale和fix-scale anchor ratio，发现精度相对稳定，说明robust to anchor settings multi-anchors settings RetinaNet在不同的anchor setting下，精度基本不变，说明主要正样本选的好，不管一个location绑定几个anchor结果都一样]]></content>
      <tags>
        <tag>目标检测，anchor-free&amp;anchor-based，tricks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[re-labeling]]></title>
    <url>%2F2021%2F05%2F27%2Fre-labeling%2F</url>
    <content type="text"><![CDATA[Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels 动机 label noise single-label benchmark but contains multiple classes in one sample a random crop may contain an entirely different object from the gt label exhaustive multi-label annotations per image is too cost mismatch researches refine the validation set with multi-labels propose new multi-label evaluation metrics 但是造成数据集的mismatch we propose re-label use a strong image classifier trained on extra source of data to generate the multi-labels use pixel-wise multi-label predictions before GAP：addtional location-specific supervision then trained on re-labeled samples further boost with CutMix from single to multi-labels：多标签 from global to localized：dense prediction map 论点 single-label 和multi-label validation set的mismatch random crop augmentation加剧了问题 除了多目标还有前背景，只有23%的random crops IOU&gt;0.5 ideally label the full set of classes——multi-label where each objects——localized label results in a dense pixel labeling $L\in \{0,1\}^{HWC}$ we propose a re-labeling strategy ReLabel strong classifier external training data generate feature map predictions LabelPooling with dense labels &amp; random crop pooling the label scores from crop region evaluations baseline r50：77.5% r50 + ReLabel：78.9% r50 + ReLabel + CutMix：80.2% 【QUESTION】同样是引入外部数据实现无痛长点，与noisy student的区别/好坏？？？ 目前论文提到的就只有efficiency，ReLabel是one-time cost的，知识蒸馏是iterative&amp;on-the-fly的 方法 Re-labeling super annotator state-of-the-art classifier trained on super large dataset fine-tuned on ImageNet and predict ImageNet labels we use open-source trained weights as annotators though trained with single-label supervision still tend to make multi-label predictions EfficientNet-L2 input size 475 feature map size 15x15x5504 output dense label size 15x15x1000 location-specific labels remove GAP heads add a 1x1 conv 说白了就是一个fcn original classifier的fc层权重与新添加的1x1 conv层的权重是一样的 label的每个channel对应了一个类别的heatmap，可以看到disjointly located at each object’s position LabelPooling loads the pre-computed label map region pooling (RoIAlign) on the label map GAP + softmax to get multi-label vector train a classifier with the multi-label vector uses CE choices space consumption 主要是存储label map的空间 store only top-5 predictions per image：10GB time consumption 主要是说生成label map的one-shot-inference time和labelPooling引入的额外计算时间 relabeling：10-GPU hours labelPooling：0.5% additiona training time more efficient than KD annotators 标注工具哪家强：目前看下来eff-L2的supervision效果最强 supervision confidence 随着image crop与前景物体的IOU增大，confidence逐渐增加 说明supervision provides some uncertainty when low IOU 实验]]></content>
      <tags>
        <tag>pretaining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mlp系列]]></title>
    <url>%2F2021%2F05%2F27%2Fmlp%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[[papers] [MLP-Mixer] MLP-Mixer: An all-MLP Architecture for Vision，Google [ResMLP] ResMLP: Feedforward networks for image classification with data-efficient training，Facebook [references] https://mp.weixin.qq.com/s?__biz=MzUyMjE2MTE0Mw==&amp;mid=2247493478&amp;idx=1&amp;sn=2be608d776b2469b3357da30c42d9770&amp;chksm=f9d2b9fecea530e8cbf07847c2029a1dabb131dbc1d6bd91ed227e41a396dd333afc83b64cf8&amp;scene=21#wechat_redirect https://mp.weixin.qq.com/s/8f9yC2P3n3HYygsOo_5zww MLP-Mixer: An all-MLP Architecture for Vision 动机 image classification task neither of [CNN, attention] are necessary our proposed MLP-Mixer 仅包含multi-layer-perceptrons independently to image patches repeated applied across either spatial locations or feature channels two types applied independently to image patches applied across patches 方法 overview 输入是token sequences non-overlapping image patches linear projected to dimension C Mixer Layer maintain the input dimension channel-mixing MLP operate on each token independently 可以看作是1x1的conv token-mixing MLP operate on each channel independently take each spatial vectors (hxw)x1 as inputs 可以看作是一个global depth-wise conv，s1，same pad，kernel size是(h,w) 最后对token embedding做GAP，提取sequence vec，然后进行类别预测 idea behind Mixer clearly separate the per-location operations &amp; cross-location operations CNN是同时进行这俩的 transformer的MSA同时进行这俩，MLP只进行per-location operations Mixer Layer two MLP blocks given input $X\in R^{SC}$，S for spatial dim，C for channel dim 先是token-mixing MLP acts on S dim maps $R^S$ to $R^S$ share across C-axis LN-FC-GELU-FC-residual 然后是channel-mixing MLP acts on C dim maps $R^C$ to $R^C$ share across S-axis LN-FC-GELU-FC-residual fixed width，更接近transformer/RNN，而不是CNN那种金字塔结构 不使用positional embeddings the token-mixing MLPs are sensitive to the order of the input tokens may learn to represent locations 实验 ResMLP: Feedforward networks for image classification with data-efficient training 动机 entirely build upon MLP alternates from a simple residual network a linear layer to interact with image patches a two-layer FFN to interact independently with each patch affine transform替代LN是一个特别之处 trained with modern strategy heavy data-augmentation optionally distillation show good performace on ImageNet classification 论点 strongly inspired by ViT but simpler 没有attention层，只有fc层+gelu 没有norm层，因为much more stable to train，但是用了affine transformation 方法 overview takes flattened patches as inputs typically N=16：16x16 linear project the patches into embeddings form $N^2$ d-dim embeddings ResMLP Layer main the dim throughout $[N^2,d]$ a simple linear layer interaction between the patches applied to all channels independently 类似depth-wise conv with global kernel的东西，线性！！ a two-layer-mlp fc-GELU-fc independently applied to all patches 非线性！！ average pooled $[d-dim]$ + linear classifier $cls-dim$ Residual Multi-Layer Perceptron Layer a linear layer + a FFN layer each layer is paralleled with a skip-connection 没用LN，但是用了learnable affine transformation $Aff_{\alpha, \beta} (x) = Diag(\alpha) x + \beta$ rescale and shifts the input component-wise：对每个patch，分别做affine变换 在推理阶段可以与上一层线性层合并：no cost 用了两次 第一个用在main path上用来替代LN：初值为identity transform(1,0) 第二个在residual path里面，down scale to boost，用一个small value初始化 given input： $d\times N^2$ matrix $X$ affine在d-dim上做 第一个Linear layer在$N^2-dim$上做：参数量$N^2 \times N^2$ 第二、三个Linear layer在$d-dim$上做：参数量$d \times 4d$ &amp; $4d \times d$]]></content>
      <tags>
        <tag>mlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[torch-note]]></title>
    <url>%2F2021%2F05%2F24%2Ftorch-note%2F</url>
    <content type="text"><![CDATA[常用库函数 1.1 torch.flatten(input, start_dim=0, end_dim=-1)：展开start_dim到end_dim之间的dim成一维 1.2 [einops][https://ggcgarciae.github.io/einops/2-einops-for-deep-learning/].rearrange(element, pattern)：贼强，用高级pattern指导张量变换 torch.cuda.amp 自动混合精度：FloatTensor &amp; HalfTensor 安装 使用 torch.jit.script 将模型从纯Python程序转换为能够独立于Python运行的TorchScript程序 [torch.nn.DataParallel &amp; DistributedDataParallel][https://blog.csdn.net/kuweicai/article/details/120516410] DP和DDP都是实现数据并行方式的分布式训练，主要区别如下： DP是单进程多线程，DDP是采用多进程（有进程通信） DP只能在单机上使用，DDP单机和多机都可以使用 DDP相比于DP训练速度要快 DP的架构是有一个main GPU，然后一对多通信&amp;同步其他子GPU，全程负责将切分的数据和复制的模型发布到子GPU上，通信时间和卡数目成正比，DDP的通信结构是环状worker，从一开始就分配给每个进程独立的获取数据&amp;构建模型任务，每个GPU接收的数据量&amp;信息量恒定，通信成本恒定（[reference][https://blog.csdn.net/qiumokucao/article/details/120179961]） DP使用 简单的单机多卡，forward pass在多卡上做，然后汇总的主卡，模型更新只在主卡上做，然后再分发到各子GPU GPU利用率低 只需要给single model做一个封装 1net = torch.nn.DataParallel(model,device_ids=[0,1,2]) [DDP使用][https://zhuanlan.zhihu.com/p/107139605] 每个batch以后，分发模型权重，太麻烦了，可以考虑同步梯度，把所有卡的loss同步，然后各自梯度更新（需要设置同样的seed）就好了——【重复计算好多遍】 要快过 【io通信分发】 相对复杂，核心配置参数： group：进程组，默认情况下，只有一个组 world size：全局进程个数，如果是多机多卡就表示机器数量，如果是单机多卡就表示 GPU 数量 rank：进程号，多机多卡表示机器编号，单机多卡表示 GPU编号 local_rank：进程内GPU 编号 两种代码封装方式 spawn launch：一般看到的都是torch.distributed.launch torch.distributed.launch 用法：python3 -m torch.distributed.launch [—usage] single_training_script.py [—training_script_args] 123[-h] [--nnodes NNODES] [--node_rank NODE_RANK][--nproc_per_node NPROC_PER_NODE] [--master_addr MASTER_ADDR][--master_port MASTER_PORT] [--use_env] [-m] [--no_python] * -h/--help：查看帮助 * --nnodes NNODES：节点数 * --node_rank NODE_RANK：当前节点的rank * --nproc_per_node NPROC_PER_NODE：每个节点的GPU数量 * --master_addr MASTER_ADDR：node 0的IP/host name，单机多卡时候就是127.0.0.1 * --master_port MASTER_PORT：node 0的free port，用来节点间通信 * --use_env：读取环境变量的LOCAL_RANK，然后用来传递local rank * -m：类似python -m，如果single_training_script.py被打包成python module了，可以-m调用 * --no_python：用不上 * 查看帮助：python -m torch.distributed.launch --help 指定GPU 在代码里面指定 1os.environ['CUDA_VISIBLE_DEVICES'] = '0' 在命令行运行脚本/文件时指定 12CUDA_VISIBLE_DEVICES=0,1 python3 train.pyCUDA_VISIBLE_DEVICES=0,1 sh run.sh 在sh脚本中指定 123source bashrcexport CUDA_VISIBLE_DEVICES=gpu_ids &amp;&amp; python3 train.py # 两个命令CUDA_VISIBLE_DEVICES=gpu_ids python3 train.py # 1个命令 优先级：代码&gt;命令&gt;脚本 ============================== 分隔符 ================================ .cuda()指定 123model.cuda(gpu_id) # 只能指定一张显卡model.cuda('cuda:'+str(gpu_ids)) # 可以多卡model.cuda('cuda:1,2') torch.cuda.set_device()指定 12torch.cuda.set_device(gpu_id) # 单卡torch.cuda.set_device('cuda:'+str(gpu_ids)) # 可指定多卡 优先级：.cuda() &gt; torch.cuda.set_device() ============================== 分隔符 ================================ 另外分隔符上下两种指定方式，指定的GPU设备的效果，会叠加： 1234567# run shellCUDA_VISIBLE_DEVICES=2,3,4,5 python3 train.py# 代码内部model.cuda(1)loss.cuda(1)tensor.cuda(1) 此时代码会运行在GPU3上，因为首先指定GPU 2 3 4 5作为VISIBLE_DEVICES，内部编号0 1 2 3，然后在代码内部指定1号卡，也就是外部的3号 推荐os.environ[‘CUDA_VISIBLE_DEVICES’] = ‘0’ 方式，童叟无欺 随机种子 为了保证每次训练的可复现性，在程序开始的时候固定torch的随机种子，同时也把numpy的随机种子固定 123456np.random.seed(0)torch.manual_seed(0)torch.cuda.manual_seed_all(0)torch.backends.cudnn.deterministic = True # 每次卷积计算算法固定torch.backends.cudnn.benchmark = False # 同上，组合使用 多卡同步 BN 默认情况下，各卡用各自的数据独立计算均值和标准差 因为最开始的任务mini-batch够大 数据同步通信浪费时间 【QUESTION】一个疑问，滑动平均最终都是近似样本均值的吧，是不是只影响训练初期的收敛速度啊，和精度有直接影响吗？？【一个解释】因为总体的数据会切分，然后分配给每个卡，这样多卡的情况下，其实不能完全保证一张卡是跑过全集的，所以可能导致每个 GPU 过拟合自己那份数据 同步BN用所有卡的数据一起计算均值和标准差，BP的时候计算全局梯度，对检测任务提升较大 12sync_bn = torch.nn.SyncBatchNorm(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) 张量 基本属性 1234tensor = torch.randn(3,4,5)print(tensor.type()) # 数据类型print(tensor.size()) # 张量的shape，是个元组print(tensor.dim()) # 维度的数量 轴命名 &amp; 替代axis-index 1234567891011121314# Tensor[N, C, H, W]images = torch.randn(32, 3, 56, 56)images.sum(dim=1)images.select(dim=1, index=0)# PyTorch 1.3之后NCHW = [‘N’, ‘C’, ‘H’, ‘W’]images = torch.randn(32, 3, 56, 56, names=NCHW)images.sum('C')images.select('C', index=0)# 也可以这么设置tensor = torch.rand(3,4,1,2,names=('C', 'N', 'H', 'W'))# 使用align_to可以对维度方便地排序tensor = tensor.align_to('N', 'C', 'H', 'W') 数据类型转换 12345678910111213141516171819202122# 设置默认类型，pytorch中的FloatTensor远远快于DoubleTensortorch.set_default_tensor_type(torch.FloatTensor)# 类型转换tensor = tensor.cuda() # cuda类型的tensor仅用于在GPU上进行计算，不能与其他类型混用tensor = tensor.cpu() # cpu类型的tensor可以与ndarrray/PIL.Image自由转换tensor = tensor.float()tensor = tensor.long()# ndarrayndarray = tensor.cpu().numpy()tensor = torch.from_numpy(ndarray).float()tensor = torch.from_numpy(ndarray.copy()).float()# PIL.Imageimage = PIL.Image.fromarray(torch.clamp(tensor*255, min=0, max=255).byte().permute(1,2,0).cpu().numpy()) # byte()=uint8(), char()=int8(), [C,H,W]-&gt;[H,W,C]image = torchvision.transforms.functional.to_pil_image(tensor)tensor = torch.from_numpy(np.asarray(PIL.Image.open(path))).permute(2,0,1).float() / 255 # 0-1的f32, [C,H,W]tensor = torchvision.transforms.functional.to_tensor(PIL.Image.open(path)) # scalarvalue = torch.rand(1).item() 张量基本操作 123456789101112131415161718192021222324# 负步长，pytorch不支持tensor[::-1]这样的负步长操作，需要通过张量索引实现tensor = tensor[:,:,:,torch.arange(tensor.size(3) - 1, -1, -1).long()] # [N,C,H,W] 水平翻转# 复制张量tensor.clone() # new memory, still in computation graphtensor.detach() # shared memory, not in computation graphtensor.detach.clone()() # new memory, not in computation graph# 张量比较torch.allclose(tensor1, tensor2) # float tensortorch.equal(tensor1, tensor2) # int tensor# 矩阵乘法# Matrix multiplcation: (m*n) * (n*p) -&gt; (m*p).result = torch.mm(tensor1, tensor2)# Batch matrix multiplication: (b*m*n) * (b*n*p) -&gt; (b*m*p)result = torch.bmm(tensor1, tensor2)# Element-wise multiplication: (m*n) * (m*n) -&gt; (m*n)result = tensor1 * tensor2result = torch.mul(tensor1, tensor2)# xjb乘之matmul: 不限定输入几维矩阵，始终后两维进行矩阵乘法，前面的维度broadcasta = torch.ones(2,1,3,4)b = torch.ones(5,4,2)c = torch.matmul(a,b) # torch.Size([2,5,3,2]) 数据集Dataset, DataLoader torch.utils.data.Dataset：Dataset可以理解为一个list，上层调用时候会传给他一个index，dataset则复制读取、变换、预处理指定文件，返回一个(input_x, target_y)-pair，主体结构如下： 12345678910111213141516171819class CustomDataset(torch.utils.data.Dataset): def __init__(self): # TODO # 1. Initialize file path or list of file names. pass def __getitem__(self, index): # TODO # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open). # 2. Preprocess the data (e.g. torchvision.Transform). # 3. Return a data pair (e.g. image and label). #这里需要注意的是，第一步：read one data，是一个data pass def __len__(self): # You should change 0 to the total size of your dataset. return 0# Dataset的长度代表样本量# DataLoader的长度代表batch steps torch.utils.data.DataLoader：DataLoader是真正对接模型这一层，负责整合batch data，同时调整采样策略、workers、shuffle等一系列设置，用如下参数将其实例化（加粗为常用）： dataset(Dataset): 传入的数据集 batch_size(int, optional): 每个batch有多少个样本 shuffle(bool, optional): 在每个epoch开始的时候，对数据进行重新排序 sampler(Sampler, optional): 自定义从数据集中取样本的策略，如果指定这个参数，那么shuffle必须为False batch_sampler(Sampler, optional): 与sampler类似，但是一次只返回一个batch的indices（索引），需要注意的是，一旦指定了这个参数，那么batch_size,shuffle,sampler,drop_last就不能再制定了（互斥——Mutually exclusive） num_workers (int, optional): 这个参数决定了有几个进程来处理data loading。0意味着所有的数据都会被load进主进程。（默认为0） collate_fn (callable, optional): 将一个list的sample组成一个mini-batch的函数 pin_memory (bool, optional)： 如果设置为True，那么data loader将会在返回它们之前，将tensors拷贝到CUDA中的固定内存（CUDA pinned memory）中 drop_last (bool, optional): 如果设置为True：这个是对最后的未完成的batch来说的，比如你的batch_size设置为64，而一个epoch只有100个样本，那么训练的时候后面的36个就被扔掉了。 如果为False（默认），那么会继续正常执行，只是最后的batch_size会小一点 timeout(numeric, optional):如果是正数，表明等待从worker进程中收集一个batch等待的时间，若超出设定的时间还没有收集到，那就不收集这个内容了。这个numeric应总是大于等于0。默认为0 worker_init_fn (callable, optional): 每个worker初始化函数 If not None, this will be called on eachworker subprocess with the worker id (an int in [0, num_workers - 1]) as input, after seeding and before data loading. (default: None) 采样器Sampler 所有的采样器都继承自torch.utils.data.sampler 1234567891011121314class SequentialSampler(Sampler): r"""Samples elements sequentially, always in the same order. Arguments: data_source (Dataset): dataset to sample from """ # 产生顺序 迭代器 def __init__(self, data_source): self.data_source = data_source def __iter__(self): return iter(range(len(self.data_source))) # 主要区别在这里 def __len__(self): return len(self.data_source) 已有Sampler * SequentialSampler(data_source)：按顺序采集，data_source可以是一个Dataset，返回一个indices的生成器 RandomSampler(data_source, replacement=False, num_samples=None)：随机、有/无放回、采集指定数目的样本 SubsetRandomSampler(indices)：无放回采样，就是打乱全集，是RandomSampler的懒人常用写法 WeightedRandomSampler(weights, num_samples, replacement=True)：也是RandomSampler的衍生，样本带了权重 BatchSampler(sampler, batch_size, drop_last)：将以上Sampler包装成批索引返回 模型 nn.Module：定义模型时继承的基类，因为基类封装了train/eval/梯度回传等高级功能，就相当于keras的Model类 自定义层、模型、Loss都是继承这个类 迭代模型的所有子层： 12345678for layer in model.modules(): # 返回所有子层 if isinstance(layer, torch.nn.Conv2d): torch.nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')for layer in model.named_modules(): # 返回所有的[名字,子层]pairs if isinstance(layer[1],nn.Conv2d): conv_model.add_module(layer[0],layer[1]) model(x) 前用 model.train() 和 model.eval() 切换网络状态 nn.ModuleList：是个List，可以把任意 nn.Module 的子类加入到这个List，而且是会自动注册到整个网络上（在computation graph上），但是用普通的python List定义则不会真正添加进网络结构里（应该跟局部定义域有关吧） module的执行顺序根据 forward 函数来决定 一个module可以在 forward 函数中被调用多次，但是参数共享 nn.Sequential：它更进一步，已经在内部实现了forward方法——定义即实现，必须按照层顺序去定义 nn.Xxx &amp; nn.functional.xxx：如nn.Conv2d和nn.functional.conv2d，这就类似keras.Layer.Conv2d和tf.nn.conv2d，一个是封装的层，需要实例化使用，一个是函数借口，直接使用但是要传入参数 模型参数量：torch.numel 12total_parameters = sum(torch.numel(p) for p in model.parameters())trained_parameters = sum(torch.numel(p) for p in model.parameters() if p.requires_grad) 模型参数： 12345678910model.parameters() # 生成器model.state_dict() # dictmodel.load_state_dict(torch.load('model.pth'), strict=False)# 模型参数量：torch.numelsum_parameters = sum(torch.numel(parameter) for parameter in model.parameters())# 浮点运算次数：GFLOPsmodel.layers[0].flops() / 1e9 1个special case：BN层，在调用.parameters()方法的时候，可以看到BN层只有两个参数，但是实际上还有running mean &amp; running std，这两个变量严格来说不算网络参数，而是一个数值统计，所以在state_dict()里面可以看到 以较大学习率微调全连接层，较小学习率微调卷积层 123456model = torchvision.models.resnet18(pretrained=True)finetuned_parameters = list(map(id, model.fc.parameters()))conv_parameters = (p for p in model.parameters() if id(p) not in finetuned_parameters)parameters = [&#123;'params': conv_parameters, 'lr': 1e-3&#125;, &#123;'params': model.fc.parameters()&#125;]optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4) pytorch-summary]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LV-ViT]]></title>
    <url>%2F2021%2F05%2F21%2FLV-ViT%2F</url>
    <content type="text"><![CDATA[[LV-ViT 2021] Token Labeling: Training an 85.4% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNet，新加坡国立&amp;字节，主体结构还是ViT，deeper+narrower+multi-layer-cnn-patch-projection+auxiliary label&amp;loss 同等参数量下，能够达到与CNN相当的分类精度 26M——84.4% ImageNet top1 acc 56M——85.4% ImageNet top1 acc 150M——86.2% ImageNet top1 acc ImageNet &amp; ImageNet-1k：The ImageNet dataset consists of more than 14M images, divided into approximately 22k different labels/classes. However the ImageNet challenge is conducted on just 1k high-level categories (probably because 22k is just too much) Token Labeling: Training an 85.4% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNet 动机 develop a bag of training techniques on vision transformers slightly tune the structure introduce token labeling——a new training objective ImageNet classificaiton task 论点 former ViTs 主要问题就是需要大数据集pretrain，不然精度上不去 然后模型也比较大，need huge computation resources DeiT和T2T-ViT探索了data augmentation/引入additional token，能够在有限的数据集上拉精度 our work rely on purely ImageNet-1k data rethink the way of performing patch embedding introduce inductive bias we add a token labeling objective loss beside cls token predition provide practical advice on adjusting vision transformer structures 方法 overview &amp; comparison 主体结构不变，就是增加了两项 a MixToken method a token labeling objective review the vision transformer patch embedding 将固定尺寸的图片转换成patch sequence，例如224x224的图片，patch size=16，那就是14x14个small patches 将每个patch(16x16x3=768-dim) linear project成一个token(embedding-dim) concat a class token，构成全部的input tokens position encoding added to input tokens fixed sinusoidal / learnable multi-head self-attention 用来建立long-range dependency multi-heads：所有attention heads的输出在channel-dim上concat，然后linear project回单个head的channel-dim feed-forward layers fc1-activation-fc2 score predition layer 只用了cls token对应的输出embedding，其他的discard training techniques network depth add more transformer blocks 同时decrease the hidden dim of FFN explicit inductive bias CNN逐步扩大感受野，擅长提取局部特征，具有天然的平移不变性等 transformer被发现failed to capture the low-level and local structures we use convolutions with a smaller stride to provide an overlapped information for each nearby tokens 在patch embedding的时候不是independent crop，而是有overlap 然后用多层conv，逐步扩大感受野，smaller kernel size同时降低了计算量 rethinking residual connection 给残差分支add a smaller ratio $\alpha$ enhance the residual connection since less information will go to the residual branch improve the generalization ability re-labeling label is not always accurate after cropping situations are worse on smaller images re-assign each image with a K-dim score map，在1k类数据集上K=1000 cheap operation compared to teacher-student 这个label是针对whole image的label，是通过另一个预训练模型获取 token-labeling based on the dense score map provided by re-labeling，we can assign each patch an individual label auxiliary token labeling loss 每个token都对应了一个K-dim score map 可以计算一个ce given outputs of the transformer $[X^{cls}, X^1, …, X^N]$ K-dim score map $[y^1, y^2, …, y^N]$ whole image label $y^{cls}$ loss auxiliary token labeling loss：$L_{aux} = \frac{1}{N} \sum_1^N CE(X^i, y^i)$ cls loss：$L_{cls} = CE(X^{cls}, y^{cls})$ total loss：$L_{total} = L_{cls}+\beta L_{aux}$，$\beta=0.5$ MixToken 从Mixup&amp;CutMix启发来的 为了确保each token have clear content，我们基于token embedding进行mixup given token sequence $T_1=[t^1_1, t^2_1, …, t^N_1]$ &amp; $T_2=[t^1_2, t^2_2, …, t^N_2]$ token labels $y_1=[y^1_1, y^2_1, …, y^N_1]$ &amp; $Y_2=[y^1_2, y^2_2, …, y^N_2]$ binary mask M MixToken mixed token sequence：$\hat T = T_1 \odot M + T_2 \odot (1-M)$ mixed labels：$\hat Y = Y_1 \odot M + Y_2 \odot (1-M)$ mixed cls label：$\hat {Y^{cls}} = \overline M y_1^{cls} + (1-\overline M) y_2^{cls}$，$\overline M$ is the average of $M$ 实验 training details AdamW linear lr scaling：larger when use token labeling weight decay dropout：hurts small models，use Stochastic Depth instead Training Technique Analysis more convs in patch embedding enhanced residual smaller scaling factor the weight get larger gradients in residual branch more information can be preserved in main branch better performance faster convergence re-labeling use NFNet-F6 to re-label the ImageNet dataset and obtain the 1000-dimensional score map for each image NFNet-F6 is trained from scratch given input 576x576，获得的score map是18x18x1000（s32） store the top5 probs for each position to save storage MixToken 比baseline的CutMix method要好 同时看到token labeling比relabeling要好 token labeling relabeling是在whole image上 token labeling是进一步地，在token level添加label和loss augmentation techniques 发现MixUp会hurt Model Scaling 越大越好]]></content>
      <tags>
        <tag>visual transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[memory bank]]></title>
    <url>%2F2021%2F05%2F19%2Fmemory-bank%2F</url>
    <content type="text"><![CDATA[2018年的paper official code：https://github.com/zhirongw/lemniscate.pytorch memory bank NCE Unsupervised Feature Learning via Non-Parametric Instance Discrimination 动机 unsupervised learning can we learn good feature representation that captures apparent similarity among instances instead of classes formulate a non-parametric classification problem at instance-level use noise contrastive estimation our non-parametric model highly compact：128-d feature per image，only 600MB storage in total enable fast nearest neighbour retrieval 【QUESTION】无类别标签，单靠similarity，最终的分类模型是如何建立的？ verified on ImageNet 1K classification semi-supervised learning object detection tasks 论点 observations ImageNet top-5 err远比top-1 err小 second highest responding class is more likely to be visually related 说明模型隐式地学到了similarity apparent similarity is learned not from se- mantic annotations, but from the visual data themselves 将class-wise supervision推到一个极限 就变成了instance-level 类别数变成了the whole training set：softmax to many more classes becomes infeasible approximate the full softmax distribution with noise-contrastive estimation(NCE) use a proximal regularization to stablize the learning process train &amp; test 通常的做法是learned representations加一个线性分类器 e.g. SVM：但是train和test的feature space是不一致的 我们用了KNN：same metric space 方法 overview to learn a embedding function $f_{\theta}$ distance metric $d_{\theta}(x,y) = ||f_{\theta}(x)-f_{\theta}(y)||$ to map visually similar images closer instance-level：to distinct between instances Non-Parametric Softmax Classifier common parametric classifier given网络预测的N-dim representation $v=f_{\theta}(x)$ 要预测C-classes的概率，需要一个$W^{NC}$的projection：$P(i|v) = \frac{exp (W^T_iv)}{\sum exp (W^T_jv)}$ Non-Parametric version enforce $||v||=1$ via L2 norm replace $W^T$ with $v^T$ then the probability：$P(i|v) = \frac{exp (v^T_iv/\tau)}{\sum exp (v^T_jv / \tau)}$ temperature param $\tau$：controls the concentration level of the distribution the goal is to minimize the negative log-likelihood 意义：L2 norm将所有的representation映射到了一个128-d unit sphere上面，$v_i^T v_j$度量了两个projection vec的similarity，我们希望同类的vec尽可能重合，不同类的vec尽可能正交 class weights $W$ are not generalized to new classes but feature representations $V$ does memory bank 因为是instance level，C-classes对应整个training set，也就是说${v_i}$ for all the images are needed for loss Let $V={v_i}$ 表示memory bank，初始为unit random vectors every learning iterations $f_\theta$ is optimized by SGD 输入$x_i$所对应的$f_i$更新到$v_i$上 也就是只有mini-batch中包含的样本，在这一个step，更新projection vec Noise-Contrastive Estimation non-parametric softmax的计算量随着样本量线性增长，millions level样本量的情况下，计算太heavy了 we use NCE to approximate the full softmax assume noise samples的uniform distribution：$P_n =\frac{1}{n}$ noise samples are $m$ times frequent than data samples 那么sample $i$ matches vec $v$的后验概率是：$h(i,v)=\frac{P(i|v)}{P(i|v)+mP_n}$ approximated training object is to minimize the negative log-likelihood of $h(i,v)$ normalizing constant $Z$的近似 主要就是分母这个$Z_i$的计算比较heavy，我们用Monte Carlo采样来近似： ${j_k}$ is a random subset of indices：随机抽了memory bank的一个子集来approx全集的分母，实验发现取batch size大小的子集就可以，m=4096 Proximal Regularization the learning process oscillates a lot we have one instance per class during each training epoch each class is only visited once we introduce an additional term overall workflow：在每一个iteration t，feature representation是$v_i^t=f_{\theta}(x_i)$，而memory bank里面的representations来自上一个iteration step $V={v^{t-1}}$，我们从memory bank里面采样，并计算NCE loss，然后bp更新网络权重，然后将这一轮fp的representations update到memory bank的指定样本上，然后下一轮 可以发现，在初始random阶段，梯度更新会比较快而且不稳定 我们给positive sample的loss上额外加了一个$\lambda ||v_i^t-v_i^{t-1}||^2_2$，有点类似weight decay那种东西，开始阶段l2 loss会占主导，引导网络收敛 stabilize speed up convergence improve the learned representations Weighted k-Nearest Neighbor Classifier a test time，先计算feature representation，然后跟memory bank的vectors分别计算cosine similarity $s_i=cos(v_i, f)$，选出topk neighbours $N_k$，然后进行weighted voting weighted voting： 对每个class c，计算它在topk neighbours的total weight，$w_c =\sum_{i \in N_k} \alpha_i 1(c_i=c)$ $\alpha_i = exp(s_i/\tau)$ k = 200 $\tau = 0.07$]]></content>
      <tags>
        <tag>Unsupervised Learning, NCE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MoCo系列]]></title>
    <url>%2F2021%2F04%2F30%2FMoCo%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[papers： [2019 MoCo v1] Momentum Contrast for Unsupervised Visual Representation Learning，kaiming [2020 SimCLR] A Simple Framework for Contrastive Learning of Visual Representations，Google Brain，混进来是因为它improve based on MoCo v1，而MoCo v2/v3又都是基于它改进 [2020 MoCo v2] Improved Baselines with Momentum Contrastive Learning，kaiming [2021 MoCo v3] An Empirical Study of Training Self-Supervised Visual Transformers，kaiming preview: 自监督学习 Self-supervised Learning reference：https://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html overview 就是无监督 针对的痛点（有监督训练模型） 标注成本高 迁移性差 会基于数据特点，设置Pretext tasks（最常见的任务就是生成/重建），构造Pesdeo Labels来训练网络 通常模型用来作为其他学习任务的预训练模型 被认为是用来学习图像的通用视觉表示 methods 从结构上区分主要就是两大类方法 生成式：通过encoder-decoder结构还原输入，监督信号是输入输出尽可能相似 重建任务开销大 没有建立直接的语义学习 外加GAN的判别器使得任务更加复杂难训 判别式：输入两张图片，通过encoder编码，监督信号是判断两张图是否相似，判别式模型也叫Contrastive Learning 从Pretext tasks上划分主要分为三类 基于上下文（Context based） ：如bert的MLM，在句子/图片中随机扣掉一部分，然后推动模型基于上下文/语义信息预测这部分/相对位置关系 基于时序（Temporal Based）：如bert的NSP，视频/语音，利用相邻帧的相似性，构建不同排序的序列，判断B是否是A的下一句/是否相邻帧 基于对比（Contrastive Based）：比较正负样本，最大化相似度的loss在这里面被叫做InfoNCE memory-bank Contrastive Based方法最常见的方式是在一个batch中构建正负样本进行对比学习 end-to-end 每个mini-batch中的图像增强前后的两张图片互为正样本 字典大小就是minibatch大小 memory bank包含数据集中所有样本编码后特征 随机采样一部分作为keys 每个迭代只更新被采样的样本编码 因为样本编码来自不同的training step，一致性差 MoCo 动态编码库：out-of-date的编码出列 momentum update：一致性提升 InfoNCE deep mind在CPC(Contrastive Predictive Coding)提出，论文以后有机会再展开 unsupervised encoder：encode x into latent space representations z，resnet blocks autoregressive model：summarize each time-step set of {z} into a context representation c，GRUs probabilistic contrastive loss Noise-Contrastive Estimation Importance Sampling 训练目标是输入数据x和context vector c之间的mutual information 每次从$p(x_{t+k}|c_t)$中采样一个正样本：正样本是这个序列接下来预测的东西，和c的相似性肯定要高于不想干的token 从$p(x_{t+k})$中采样N-1个负样本：负样本是别的序列里面随机采样的东西 目标是让正样本与context相关性高，负样本低 MoCo v1: Momentum Contrast for Unsupervised Visual Representation Learning 动机 unsupervised visual representation learning contrastive learning dynamic dictionary large consisitent verified on 7 down-stream tasks ImageNet classification VOC &amp; COCO det/seg 论点 Unsupervised representation learning highly successful in NLP，in CV supervised is still the main-stream 两个核心 pretext tasks loss functions loss functions 生成式方法的loss是基于prediction和一个fix target来计算的 contrastive-based的key target则是vary on-the-fly during training Adversarial losses没展开 pretext tasks tasks involving recover：auto-encoder task involving pseudo-labels：通常有个exemplar/anchor，然后计算contrastive loss contrastive learning VS pretext tasks 大量pretext tasks可以通过设计一些contrastive loss来实现 recent approaches using contrastive loss dynamic dictionaries 由keys组成：sampled from data &amp; represented by an encoder train the encoder to perform dictionary look-up given an encoded query similar to its matching key and dissimilar to others desirable dictionary large：better sample consistent：training target consistent MoCo：Momentum Contrast queue 每个it step的mini-batch的编码入库 the oldest are dequeued EMA： a slowly progressing key encoder momentum-based moving average of the query encoder similar的定义：q &amp; k are from the same image 方法 contrastive learning a encoded query $q$ a set of encoded samples $\{k_0, k_1, …\}$ assume：there is a single key $k_+$ in the dictionary that $q$ matches similarity measurement：dot product InfoNCE： $L_q = -log \frac{exp(qk_+/\tau)}{\sum_0^K exp(qk/\tau)}$ 1 positive &amp; K negtive samples 本质上是个softmax-based classifier，尝试将$q$分类成$k_+$ unsupervised workflow with a encoder network $f_q$ &amp; $f_k$ thus we have query &amp; sample representation $q=f_q(x^q)$ &amp; $k=f_k(x^k)$ inputs $x$ can be images/patches/context(patches set) $f_q$ &amp; $f_k$ can be identical/partially shared/different momentum contrast dictionary as a key the dictionary always represents a sampled subset of all data the current mini-batch入列 the oldest mini-batch出列 momentum update large dictionary没法对keys进行back-propagation：因为sample太多了 only $f_q$ are updated by back-propagation：mini-batch naive solution：copy $f_q$的参数给$f_k$，yields poor results，因为key encoder参数变化太频繁了，representation inconsistent issue momentum update：$f_k = mf_k + (1-m)f_q$，$m=0.999$ 三种更新方式对比 第一种end-to-end method： use samples in current mini-batch as the dictionary keys are consistently encoded dictionary size is limited 第二种memory bank A memory bank consists of the representations of all samples in the dataset the dictionary for each mini-batch is randomly sampled from the memory bank，不进行bp，thus enables large dictionary key representation is updated when it was last seen：inconsistent 有些也用momentum update，但是是用在representation上，而不是encoder参数 pretext task define positive pair：if the query and the key come from the same image 我们从图上take two random views under random augmentation to form a positive pair 然后用各自的encoder编码成q &amp; k 每一对计算similarity：pos similarity 然后再计算input queries和dictionary的similarity：neg similarity 计算ce，update $f_q$ 用$f_q$ update $f_k$ 把k加入dictionary队列 把最早的mini-batch出列 技术细节 resnet：last fc dim=128，L2 norm temperature $\tau=0.07$ augmentation random resize + random(224,224) crop random color jittering random horizontal flip random grayscale conversion shuffling BN 实验发现使用resnet里面的BN会导致不好的结果：猜测是intra-batch communication引导模型学习了一种cheating的low-loss solution 具体做法是给$f_k$的输入mini-batch先shuffle the order，然后进行fp，然后再shuffle back，这样$f_q$和$f_k$的BN计算的mini-batch的statics就是不同的 实验 SimCLR: A Simple Framework for Contrastive Learning of Visual Representations 动机 simplify recently proposed contrastive self-supervised learning algorithms systematically study the major components data augmentations learnable unlinear prediction head larger batch size and more training steps outperform previous self-supervised &amp; semi-supervised learning methods on ImageNet 论点 discriminative approaches based on contrastive learning maximizing agreement between differently augmented views of the same data sample via a contrastive loss in the latent space major components &amp; conclusions 数据增强很重要，unsupervised比supervised benefits more 引入的learnable nonlinear transformation提升了representation quality contrastive cross entropy loss受益于normalized embedding和adjusted temperature parameter larger batch size and more training steps很重要，unsupervised比supervised benefits more 方法 common framework 4 major components 随机数据增强 results in two views of the same sample，构成positive pair crop + resize back + color distortions + gaussian blur base encoder 用啥都行，本文用了resnet including the GAP a projection head 将representation dim映射到the space where contrastive loss is applied（given 1 pos pair &amp; N neg pair，就是N+1 dim） 之前有方法直接用linear projection 我们用了带一个hidden layer的MLP：fc-bn-relu-fc a contrastive loss overall workflow random sample a minibatch of N random augmentation results in 2N data points 对每个样本来讲，有1个positive pair，其余2(N-1)个data points都是negative samples set cosine similarity $sim(u,v)=u^Tv/|u||v|$ given positive pair $(i,j)$ then the loss is $l_{i,j} = -log \frac{exp(s_{i,j}/\tau)}{\sum_{k\neq i}^{2N} exp(s_{i,k}/\tau)}$ 对每个positive pair都计算，包括$(i,j)$ 和$(j,i)$，叫那个symmetrized loss update encoder training with large batch size batch 8192，negatives 16382 大batch时，linear learning rate scaling可能不稳定，所以用了LARS optmizer global BN，aggregate BN mean &amp; variance over all devices TPU MoCo v2: Improved Baselines with Momentum Contrastive Learning 动机 still working on contrastive unsupervised learning simple modifications on MoCo introduce two effective SimCLR’s designs： an MLP head more data augmentation requires smaller batch size than SimCLR，making it possible to run on GPU verified on ImageNet classification VOC detection 论点 MoCo &amp; SimCLR contrastive unsupervised learning frameworks MoCo v1 shows promising SimCLR further reduce the gap we found two design imrpovements in SimCLR 在两个方法中都work，而且用在MoCo中shows better transfer learning results an MLP projection head stronger data augmentation 同时MoCo framework相比较于SimCLR ，远不需要large training batches SimCLR based on end-to-end mechanism，需要比较大的batch size，来提供足够多的negative pair MoCo则用了动态队列，所以不限制batch size SimCLR improves the end-to-end method larger batch：to provide more negative samples output layer：replace fc with a MLP head stronger data augmentation MoCo a large number of negative samples are readily available 所以就把后两项引入进来了 方法 MLP head 2-layer MLP(hidden dim=2048, ReLU) 仅影响unsupervised training，有监督transfer learning的时候换头 temperature param调整：从default 0.07 调整成optimal value 0.2 augmentation add blur SimCLR还用了stronger color distortion：we found stronger color distortion in SimCLR hurts in our MoCo，所以没加 实验 ablation MLP：在分类任务上的提升比检测大 augmentation：在检测上的提升比分类大 comparison large batches are not necessary for good acc：SimCLR longer training那个版本精度更高 end-to-end的方法肯定more costly in memory and time：因为要bp两个encoder MoCo v3: An Empirical Study of Training Self-Supervised Visual Transformers 动机 self-supervised frameworks that based on Siamese network, including MoCo ViT：study the fundamental components for training self-supervised ViT MoCo v3：an incremental improvement of MoCo v1/2，striking for a better balance of simplicity &amp; accuracy &amp; scalability instability is a major issue scaling up ViT models ViT-Large ViT-Huge 论点 we go back to the basics and investigate the fundamental components of training deep neural networks batch size learning rate optmizer instability instability is a major issue that impacts self-supervised ViT training but may not result in catastrophic failure，只会导致精度损失 所以称之为hidden degradation use a simple trick to improve stability：freeze the patch projection layer in ViT and observes increasement in acc NLP里面基于masked auto-encoding的framework效果要比基于contrastvie的framework好，图像正好反过来 方法 MoCo v3 take two crops for each image under random augmentation encoded by two encoders $f_q$ &amp; $f_k$ into vectors $q$ &amp; $k$ we use the keys that naturally co-exist in the same batch abandon the memory queue：因为发现batch size足够大（4096）的时候，memory queue就没啥acc gain了 回归到batch-based sample pair 但是encoder k仍旧不回传梯度，还是基于encoder q进行动量更新 symmetrized loss： $ctr(q_1, k_2) + ctr(q_2,k_1)$ InfoNCE temperature 两个crops分别计算ctr encoder encoder $f_q$ a backbone a projection head an extra prediction head encoder $f_k$ a backbone a projection head encoder $f_k$ is updated by the moving average of $f_q$，excluding the prediction head baseline acc basic settings，主要变动就是两个： dynamic queue换成large batch encoder $f_q$的extra prediction head use ViT 直接用ViT替换resnet back met instability issue batch size ViT里面的一个观点就是，model本身比较heavy，所以large batch is desirable 实验发现 a batch of 1k &amp; 2k produces reasonably smooth curves：In this regime, the larger batch improves accuracy thanks to more negative samples a batch of 4k 有明显的untable dips： a batch of 6k has worse failure patterns：我们解读为在跳水点，training is partially restarted and jumps out of the current local optimum learning rate lr较小，training比较稳定，但是容易欠拟合 lr过大，会导致unstable，也会影响acc 总体来说精度还是决定于stability optimizer default adamW，batch size 4096 有些方法用了LARS &amp; LAMB for large-batch training LAMB sensitive to lr optmal lr achieves slightly better accuracy than AdamW 但是lr一旦过大，acc极速drop 但是training curves still smooth，虽然中间过程有drop：我们解读为LAMB can avoid sudden change in the gradients，但是避免不了negative compact，还是会累加 a trick for improving stability we found a spike in gradient causes a dip in the training curve we also observe that gradient spikes happen earlier in the first layer (patch projection) 所以尝试freezing the patch projection layer during training，也就是一个random的patch projection layer This stability benefits the final accuracy The improvement is bigger for a larger lr 在别的ViT-back-framework上也有效（SimCLR、BYOL） we also tried BN，WN，gradient clip BN/WN does not improve gradient clip在threshold足够小的时候有用，推到极限就是freezing了 implementation details AdamW batch size 4096 lr：warmup 40 eps then cosine decay MLP heads projection head：3-layers，4096-BN-ReLU-4096-BN-ReLU-256 prediction head：2-layers，4096-BN-ReLU-256 loss ctr里面有个scale的参数，$2\tau$ makes it less sensitive to $\tau$ value $\tau=0.2$ ViT architecture 跟原论文保持一致 输入是224x244的image，划分成16x16/14x14的patch sequence，project成256d/196d的embedding 加上sine-cosine-2D的PE 再concat一个cls token 经过一系列transformer blocks The class token after the last block (and after the final LayerNorm) is treated as the output of the backbone，and is the input to the MLP heads]]></content>
      <tags>
        <tag>self-supervised learning, transformer, contrastive loss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[optimizers优化器]]></title>
    <url>%2F2021%2F03%2F15%2Foptimizers%E4%BC%98%E5%8C%96%E5%99%A8%2F</url>
    <content type="text"><![CDATA[0. overviewkeywords：SGD, moment, Nesterov, adaptive, ADAM, Weight decay 优化问题Optimization to minimize目标函数 grandient decent gradient numerical：数值法，approx，slow analytical：解析法，exact，fast Stochastic 用minibatch的梯度来approximate全集 $\theta_{k+1} = \theta_k - v_{t+1}(x_i,y_i)$ classic optimizers：SGD，Momentum，Nesterov‘s momentum adaptive optimizers：AdaGrad，Adadelta，RMSProp，Adam Newton modern optimizers for large-batch AdamW LARS LAMB common updating steps for current step t： step1：计算直接梯度，$g_t = \nabla f(w_t)$ step2：计算一阶动量和二阶动量，$m_t \&amp; V_t$ step3：计算当前时刻的下降梯度，$\eta_t = \alpha m_t/\sqrt {V_t}$ step4：参数更新，$w_{t+1} = w_t - \eta_t$ 各种优化算法的主要差别在step1和step2上 滑动平均/指数加权平均/moving average/EMA 局部均值，与一段时间内的历史相关 $v_t = \beta v_{t-1}+(1-\beta)\theta_t$，大致等于过去$1/(1-\beta)$个时刻的$\theta$的平均值，但是在起始点附近偏差较大 $v_{tbiased} = \frac{v_t}{1-\beta^t}$，做了bias correction t越大，越不需要修正，两个滑动均值的结果越接近 优缺点：不用保存历史，但是近似 SGD SGD没有动量的概念，$m_t=g_t$，$V_t=I^2$，$w_{t+1} = w_t - \alpha g_t$ 仅依赖当前计算的梯度 缺点：下降速度慢，可能陷在local optima上持续震荡 SGDW (with weight decay) 在权重更新的同时进行权重衰减 $w_{t+1} = (1-\lambda)w_t - \alpha g_t$ 在SGD form的优化器中weight decay等价于在loss上L2 regularization 但是在adaptive form的优化器中是不等价的！！因为historical func（ERM）中regularizer和gradient一起被downscale了，因此not as much as they would get regularized in SGDW SGD with Momentum 引入一阶动量，$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$，使用滑动均值，抑制震荡 梯度下降的主要方向是此前累积的下降方向，略微向当前时刻的方向调整 SGD with Nesterov Acceleration look ahead SGD-momentum 在local minima的时候，四周没有下降的方向，但是如果走一步再看，可能就会找到优化方向 先跟着累积动量走一步，求梯度：$g_t = \nabla f(w_t-\alpha m_{t-1}/\sqrt {V_{t-1}})$ 用这个点的梯度方向来计算滑动平均，并更新梯度 Adagrad 引入二阶动量，开启“自适应学习率”，$V_t = \sum_0^t g_k^2$，度量历史更新频率 对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些 $\eta_t = \alpha m_t / \sqrt{V_t}$，本质上为每个参数，对学习率分别rescale 缺点：二阶动量单调递增，导致学习率单调衰减，可能会使得训练过程提前结束 AdaDelta/RMSProp 参考momentum，对二阶动量也计算滑动平均，$V_t = \beta_2 V_{t-1} + (1-\beta_2)g_t^2$ 避免了二阶动量持续累积、导致训练过程提前结束 Adam 集大成者：把一阶动量和二阶动量都用起来，Adaptive Momentum SGD-M在SGD基础上增加了一阶动量 AdaGrad和AdaDelta在SGD基础上增加了二阶动量 一阶动量滑动平均：$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$ 二阶动量滑动平均：$V_t = \beta_2 V_{t-1} + (1-\beta_2)g_t^2$ Nadam look ahead Adam 把Nesterov的one step try加上：$g_t = \nabla f(w_t-\alpha m_{t-1}/\sqrt {V_{t-1}})$ 再Adam更新两个动量 经验超参 $momentum=0.9$ $\beta_1=0.9$ $\beta_2=0.999$ $m_0 = 0$ $V_0 = 0$ 上面的图上可以看出，初期的$m_t$和$V_t$会无限接近于0，此时可以进行误差修正：$factor=\frac{1}{1-\beta^t}$ AdamW 在adaptive methods中，解耦weight-decay和loss-based gradient在ERM过程中的绑定downscale的关系 实质就是将导数项后移]]></content>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[regnet]]></title>
    <url>%2F2021%2F03%2F11%2Fregnet%2F</url>
    <content type="text"><![CDATA[RegNet: Designing Network Design Spaces 动机 study the network design principles design RegNet outperforms efficientNet and 5x faster top1 error：20.1 （eff-b5：21.5） larger batch size 1/4 的 train/test latency 论点 manual network design AlexNet, ResNet family, DenseNet, MobileNet focus on discovering new design choices that improve acc the recent popular approach NAS search the best in a fixed search space of possible networks limitations：generalize to new settings，lack of interpretability network scaling 上面两个focus on 找出一个basenet for a specific regime scaling rules aims at tuning the optimal network in any target regime comparing networks the reliable comparison metric to guide the design process our method combines the disadvantages of manual design and NAS first AnyNet then RegNet 方法]]></content>
  </entry>
  <entry>
    <title><![CDATA[mongodb]]></title>
    <url>%2F2021%2F03%2F09%2Fmongodb%2F</url>
    <content type="text"><![CDATA[download：https://www.mongodb.com/try/download/enterprise install 123456789101112# 将解压以后的文件夹放在/usr/local下sudo mv mongodb-osx-x86_64-4.0.9/ /usr/local/sudo ln -s mongodb-macos-x86_64-4.4.4 mongodb# ENV PATHexport PATH=/usr/local/mongodb/bin:$PATH# 创建日志及数据存放的目录sudo mkdir -p /usr/local/var/mongodbsudo mkdir -p /usr/local/var/log/mongodbsudo chown [amber] /usr/local/var/mongodbsudo chown [amber] /usr/local/var/log/mongodb configuration 12345678# 后台启动mongod --dbpath /usr/local/var/mongodb --logpath /usr/local/var/log/mongodb/mongo.log --fork# 控制台启动mongod --config /usr/local/etc/mongod.conf# 查看状态ps aux | grep -v grep | grep mongod run 123# 在db环境下启动一个终端cd /usr/local/mongodb/bin ./mongo original settings 123456789101112131415161718192021# 显示所有数据的列表&gt; show dbsadmin 0.000GBconfig 0.000GBlocal 0.000GB# 三个系统保留的特殊数据库# 连接/创建一个指定的数据库&gt; use localswitched to db local# 显示当前数据库, 如果没use默认为test&gt; dbtest# 【！！重要】关闭服务之前服务器被kill -9强制关闭，数据库丢失了&gt; use adminswitched to db admin&gt; db.shutdownServer()server should be down... concepts 文档document 一组key-value对，如上面左图中的一行记录，如上面右图中的一个dict 集合collection 一张表，如上面左图和上面右图 主键primary key 唯一主键，ObjectId类型，自定生成，有标准格式 常用命令 10.1 创建/删除/重命名db 12345678910111213141516171819202122232425262728# 切换至数据库test1&gt; use test1# 插入一条doc, db.COLLECTION_NAME.insert(document)# db要包含至少一条文档，才能在show dbs的时候显示（才真正创建）&gt; db.sheet1.insert(&#123;'name': img0&#125;)# 显示当前已有数据库&gt; show dbs# 删除指定数据库&gt; use test1&gt; db.dropDatabase()# 旧版本(before4.0)重命名：先拷贝一份，在删除旧的&gt; db.copyDatabase('OLDNAME', 'NEWNAME');&gt; use old_name&gt; db.dropDatabase()# 新版本重命名：dump&amp;restore，这个东西在mongodb tools里面，要另外下载，可执行文件放在bin下# mongodump # 将所有数据库导出到bin/dump/以每个db名字命名的文件夹下# mongodump -h dbhost -d dbname -o dbdirectory# -h: 服务器地址:端口号# -d: 需要备份的数据库# -o: 存放位置（需要已存在）mongodump -d test -o tmp/# 在恢复备份数据库的时候换个名字：mongorestore -h dbhost -d dbname pathmongorestore -d test_bkp tmp/test# 这时候可以看到一个新增了一个叫test_bkp的db 10.2 创建/删除/重命名collection 12345678910111213141516# 创建：db.createCollection(name, options)&gt; db.createCollection('case2img')# 显示已有tables&gt; show collections# 不用显示创建，在db insert的时候会自动创建集合&gt; db.sheet2.insert(&#123;"name" : "img2"&#125;)# 删除：db.COLLECTION_NAME.drop()&gt; db.sheet2.drop()# 重命名：db.COLLECTION_NAME.renameCollection('NEWNAME')&gt; db.sheet2.renameCollection('sheet3')# 复制：db.COLLECTION_NAME.aggregate(&#123;$out: 'NEWNAME'&#125;)&gt; db.sheet2.aggregate(&#123; $out : "sheet3" &#125;) 10.3 插入/显示/更新/删除document 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# 插入db.COLLECTION_NAME.insert(document)db.COLLECTION_NAME.save(document)db.COLLECTION_NAME.insertOne()db.COLLECTION_NAME.insertMany()# 显示已有docdb.COLLECTION_NAME.find()# 更新doc的部分内容db.COLLECTION_NAME.update( &lt;query&gt;, # 查询条件 &lt;update&gt;, # 更新操作 &#123; upsert: &lt;boolean&gt;, # if true 如果不存在则插入 multi: &lt;boolean&gt;, # find fist/all match writeConcern: &lt;document&gt; &#125;)&gt; db.case2img.insert(&#123;"case": "s0", "name": "img0"&#125;)&gt; db.case2img.insert(&#123;"case": "s1", "name": "img1"&#125;)&gt; db.case2img.find()&gt; db.case2img.update(&#123;'case': 's1'&#125;, &#123;$set: &#123;'case': 's2', 'name': 'img2'&#125;&#125;)&gt; db.case2img.find()# 给doc的某个key重命名db.COLLECTION_NAME.updateMany(&#123;&#125;,&#123;'$rename': &#123;"old_key": "new_key"&#125;&#125;)# 更新整条文档by object_iddb.COLLECTION_NAME.save( &lt;document&gt;, &#123; writeConcern: &lt;document&gt; &#125;)&gt; db.case2img.save(&#123;"_id": ObjectId("60474e4b77e21bad9bd4655a"), "case":"s3", "name":"img3"&#125;)# 删除满足条件的docdb.COLLECTION_NAME.remove( &lt;query&gt;, &#123; justOne: &lt;boolean&gt;, # find fist/all match writeConcern: &lt;document&gt; &#125;)&gt; db.case2img.remove(&#123;"case": "s0"&#125;)# 删除所有doc&gt; db.case2img.remove(&#123;&#125;) 10.4 简单查询find 1234567891011121314151617&gt; db.case2img.insert(&#123;"case": "s0", "name": "img0"&#125;)&gt; db.case2img.insert(&#123;"case": "s1", "name": "img1"&#125;)&gt; db.case2img.insert(&#123;"case": "s2", "name": "img2"&#125;)&gt; db.case2img.insert(&#123;"case": "s2", "name": "img3"&#125;)# 查询表中的doc：db.COLLECTION_NAME.find(&#123;query&#125;)&gt; db.case2img.find(&#123;'case': s2&#125;)&gt; db.case2img.find(&#123;'case': 's1'&#125;, &#123;"name":1&#125;) # projection的value在对应的key-value是list的时候有意义# 格式化显示查询结果：db.COLLECTION_NAME.find(&#123;query&#125;).pretty()&gt; db.case2img.find(&#123;'case': s2&#125;).pretty()# 读取指定数量的数据记录：db.COLLECTION_NAME.find(&#123;query&#125;).limit(NUMBER)&gt; db.case2img.find(&#123;'case': &#123;$type: 'string'&#125;&#125;).limit(1)# 跳过指定数量的数据：db.COLLECTION_NAME.find(&#123;query&#125;).skip(NUMBER)&gt; db.case2img.find(&#123;'case': &#123;$type: 'string'&#125;&#125;).skip(1) 10.5 条件操作符 123456789101112131415(&gt;) 大于 - $gt(&lt;) 小于 - $lt(&gt;=) 大于等于 - $gte(&lt;=) 小于等于 - $lte(or) 或 - $or&gt; db.case2img.update(&#123;'case':'s1'&#125;, &#123;$set: &#123;"name":'img1', 'size':100&#125;&#125;)WriteResult(&#123; "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 &#125;)&gt; db.case2img.update(&#123;'case':'s2'&#125;, &#123;$set: &#123;"name":'img2', 'size':200&#125;&#125;)# 查询size&gt;150的doc&gt; db.case2img.find(&#123;'size': &#123;$gt: 150&#125;&#125;)# 查询满足任意一个条件的doc&gt; db.case2img.find(&#123;'$or': [&#123;'case':'s1'&#125;, &#123;'size': &#123;$gt: 150&#125;&#125;]&#125;) 10.6 数据类型操作符 12345type(KEY)等于 - $type# 比较对象可以是字符串/对应的reflect NUM&gt; db.case2img.find(&#123;'case': &#123;$type: 'string'&#125;&#125;)&gt; db.case2img.find(&#123;'case': &#123;$type: '0'&#125;&#125;) 10.7 排序find().sort 1234# 通过指定字段&amp;指定升序/降序来对数据排序：db.COLLECTION_NAME.find().sort(&#123;KEY:1/-1&#125;)&gt; db.case2img.find().sort(&#123;'name':1&#125;)# skip(), limilt(), sort()三个放在一起执行的时候，执行的顺序是先 sort(), 然后是 skip()，最后是显示的 limit()。 10.8 索引 skip 10.9 聚合aggregate 123456789101112131415161718# 用于统计 db.COLLECTION_NAME.aggregate(AGGREGATE_OPERATION)# by group&gt; db.case2img.aggregate([&#123;$group: &#123;_id: '$case', img_num:&#123;$sum:1&#125;&#125;&#125;])group by key value 'case'count number of items in each grouprefer to the number as img_num&gt; db.case2img.aggregate([&#123;$group: &#123;_id: '$case', img_num:&#123;$sum:'$size'&#125;&#125;&#125;])计算每一个group内，size值的总和# by match&gt; db.case2img.aggregate([&#123;$match: &#123;'size': &#123;$gt:150&#125;&#125;&#125;, &#123;$group:&#123;_id: null, totalsize: &#123;$sum: '$size'&#125;&#125;&#125;])类似shell的管道，match用来筛选条件，符合条件的送入下一步统计&gt; db.case2img.aggregate([&#123;$skip: 4&#125;, &#123;$group:&#123;_id: null, totalsize: &#123;$sum: '$size'&#125;&#125;&#125;]) 快速统计distinct 12db.case2img.distinct(TAG_NAME)# 注意如果distinct的内容太长，超过16M，会报distinct too big的error，推荐用聚合来做统计 123456789101112 12. pymongo 用python代码来操作数据库 先安装：pip install pymongo 11.1 连接client ```python from pymongo import MongoClient Client = MongoClient() 11.2 获取数据库 12db = Client.DB_NAMEdb = Client['DB_NAME'] 11.3 获取collection 12collection = db.COLLECTION_NAMEcollection = db['COLLECTION_NAME'] 11.4 插入doc 123456789101112# insert onedocument1 = &#123;'x':1&#125;document2 = &#123;'x':2&#125;post_1 = collection.insert_one(document1).inserted_idpost_2 = collection.insert_one(document2).inserted_idprint(post_1)# insert manynew_document = [&#123;'x':1&#125;,&#123;'x':2&#125;]# new_document = [document1,document2] 注意doc是神拷贝，只能作为一条doc被插入一次result = collection.insert_many(new_document).inserted_idsprint(result) 11.5 查找 12345678910from bson.objectid import ObjectId# find one 返回一条docresult = collection.find_one()result = collection.find_one(&#123;'case': 's0'&#125;)result = collection.find_one(&#123;'_id': ObjectId('604752f277e21bad9bd46560')&#125;)# find 返回一个迭代器for _, item in enumerate(collection.find()): print(item) 11.6 更新 123456# update onecollection.update_one(&#123;'case':'s1'&#125;,&#123;'$set':&#123;'size':300&#125;&#125;)collection.update_one(&#123;'case':'s1'&#125;,&#123;'$push':&#123;'add':1&#125;&#125;) # 追加数组内容# update manycollection.update_many(&#123;'case':'s1'&#125;,&#123;'$set':&#123;'size':300&#125;&#125;) 11.7 删除 123# 在mongo shell里面是remove方法，在pymongo里面被deprecated成delete方法collection.delete_one(&#123;"case": "s2"&#125;)collection.delete_many(&#123;"case": "s1"&#125;) 11.8 统计 12345# 计数：count方法已经被重构print(collection.count_documents(&#123;'case':'s0'&#125;))# unique：distinct方法print(collection.distinct('case')) ​ 11.9 正则 ​ mongo shell命令行里的正则和pymongo脚本里的正则写法是不一样的，因为python里面有封装正则方法，然后通过bson将python的正则转换成数据库的正则 12345678910# pymongoimport reimport bsonpattern = re.compile(r'(.*)-0[345]-(.*)')regex = bson.regex.Regex.from_native(pattern)result = collection.aggregate([&#123;'$match': &#123;'date': regex&#125;&#125;])# mongo shell&gt; db.collection.find(&#123;date:&#123;$regex:"(.*)-0[345]-(.*)"&#125;&#125;)]]></content>
      <tags>
        <tag>数据库，NoSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker]]></title>
    <url>%2F2021%2F03%2F04%2Fdocker%2F</url>
    <content type="text"><![CDATA[shartup 部署方案 古早年代 虚拟机 docker image镜像 &amp; container容器 &amp; registry仓库 镜像：相当于是一个 root 文件系统，提供容器运行时所需的程序、库、资源、配置等 容器：镜像运行时的实体，可以被创建、启动、停止、删除、暂停等 仓库：用来保存镜像 官方仓库：docker hub：https://hub.docker.com/r/floydhub/tensorflow/tags?page=1&amp;ordering=last_updated 常用命令 拉镜像 docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签] 地址可以是官方地址，也可以是第三方（如Harbor） 仓库名由作者名和软件名组成（如zhangruiming/skin） 标签用来指定某个版本的image，省略则默认latest 列出所有镜像 docker images 删除镜像 docker rmi [-f] [镜像id] 删除镜像之前要kill/rm所有使用该镜像的container：docker rm [容器id] 运行镜像并创建一个容器 docker run [-it] [仓库名] [命令] 选项 -it：为容器配置一个交互终端 选项 -d：后台运行容器，并返回容器ID（不直接进入终端） 选项 —name=’xxx’：为容器指定一个名称 选项-v /host_dir:/container_dir：将主机上指定目录映射到容器的指定目录 [命令]参数必须要加，而且要是那种一直挂起的命令（/bin/bash），如果是ls/cd/直接不填，那么命令运行完容器就会停止运行，docker ps -a查看状态，发现都是Exited 创建容器 docker run 查看所有容器 docker ps 启动一个已经停止的容器/停止正在运行的容器 docker start [容器id] docker stop [容器id] 进入容器 docker exec -it [容器id] [linux命令] 删除容器 docker rm [容器id] 删除所有不活跃的容器 docker container prune 提交镜像到远端仓库 docker tag [镜像id] [用户名]/[仓库]:[标签] # 重命名 docker login # 登陆用户 docker push 案例 123456789101112131415161718192021222324252627282930313233# 拉镜像docker pull 地址/仓库:标签# 显示镜像docker images# 运行指定镜像docker run -itd --name='test' 地址/仓库:标签# 查看运行的容器docker ps# 进入容器docker exec -it 容器id或name /bin/bash# 一顿操作完退出容器exit# 将修改后的容器保存为镜像docker commit 容器id或name 新镜像名字docker images可以看到这个镜像了# 保存镜像到本地docker save -o tf_torch.rar tf_torch# 还原镜像docker load --input tf_torch.tar# 重命名镜像docker tag 3db0b2f40a70 amberzzzz/tf1.14_torch1.4_cuda10.0:v1# 提交镜像docker push amberzzzz/tf1.14-torch0.5-cuda10.0:v1 dockerfile Dockerfile 是用来说明如何自动构建 docker image 的指令集文件 常用命令 FROM image_name，指定依赖的镜像 RUN command，在 shell 或者 exec 的环境下执行的命令 COPY srcfile_path_inhost dstfile_incontainer，将本机文件复制到容器中 ADD srcfile_path dstfile_incontainer，将本机文件复制到容器中，src文件不仅可以是local host，也可以是网络地址 CMD [“executable”,”param1”,”param2”]，指定容器启动默认执行的命令 WORKDIR path_incontainer，指定 RUN、CMD 与 ENTRYPOINT 命令的工作目录 VOLUME [“/data”]，授权访问从容器内到主机上的目录 basic image 从nvidia docker开始：https://hub.docker.com/r/nvidia/cuda/tags?page=1&amp;ordering=last_updated&amp;name=10. 选一个喜欢的：如docker pull nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04 然后编辑dockerfile 1234567891011121314151617181920FROM nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04MAINTAINER amber &lt;amber.zhang@tum.de&gt;# install basic dependenciesRUN apt-get update RUN apt-get install -y wget vim cmake# install Anaconda3RUN wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.2.0-Linux-x86_64.sh -O ~/anaconda3.shRUN bash ~/anaconda3.sh -b -p /home/anaconda3 &amp;&amp; rm ~/anaconda3.sh ENV PATH /home/anaconda3/bin:$PATH# RUN echo "export PATH=/home/anaconda3/bin:$PATH" &gt;&gt; ~/.bashrc &amp;&amp; /bin/bash -c "source /root/.bashrc" # change mirrorRUN mkdir ~/.pip \ &amp;&amp; cd ~/.pip RUN echo '[global]\nindex-url = https://pypi.tuna.tsinghua.edu.cn/simple/' &gt;&gt; ~/.pip/pip.conf# install tensorflowRUN /home/anaconda3/bin/pip install tensorflow-gpu==1.8.0 * 然后build dockerfile 1docker build -t &lt;docker_name&gt; .]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[layer norm]]></title>
    <url>%2F2021%2F03%2F02%2Flayer-norm%2F</url>
    <content type="text"><![CDATA[综述 papers [batch norm 2015] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift，inceptionV2，Google Team，归一化层的始祖，加速训练&amp;正则，BN被后辈追着打的主要痛点：approximation by mini-batch，test phase frozen [layer norm 2016] Layer Normalization，Toronto+Google，针对BN不适用small batch和RNN的问题，主要用于RNN，在CNN上不好，在test的时候也是active的，因为mean&amp;variance由于当前数据决定，有负责rescale和reshift的layer params [weight norm 2016] Weight normalization: A simple reparameterization to accelerate training of deep neural networks，OpenAI， [cosine norm 2017] Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks，中科院， [instance norm 2017] Instance Normalization: The Missing Ingredient for Fast Stylization，高校report，针对风格迁移，IN在test的时候也是active的，而不是freeze的，单纯的instance-independent norm，没有layer params [group norm 2018] Group Normalization，FAIR Kaiming，针对BN在small batch上性能下降的问题，提出batch-independent的 [weight standardization 2019] Weight Standardization，Johns Hopkins， [batch-channel normalization &amp; weight standardization 2020] BCN&amp;WS: Micro-Batch Training with Batch-Channel Normalization and Weight Standardization，Johns Hopkins， why Normalization 独立同分布：independent and identically distribute 白化：whitening（[PCA whitening][http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/]） 去除特征之间的相关性 使所有特征具有相同的均值和方差 样本分布变化：Internal Covariate Shift 对于神经网络的各层输入，由于stacking internel byproduct，每层的分布显然各不相同，但是对于某个特定的样本输入，他们所指示的label是不变的 即源空间和目标空间的条件概率是一致的，但是边缘概率是不同的 P_s(Y|X=x) = P_t(Y|X=x) \\ P_s(X) \neq P_t(X) 每个神经元的数据不再是独立同分布，网络需要不断适应新的分布，上层神经元容易饱和：网络训练又慢又不稳定 how to Normalization preparation unit：一个神经元（一个op），输入[b,N,C_in]，输出[b,N,1] layer：一层的神经元（一系列op，$W\in R^{M*N}$），在channel-dim上concat当前层所有unit的输出[b,N,C_out] dims b：batch dimension N：spatial dimension，1/2/3-dims C：channel dimension unified representation：本质上都是对数据在规范化 $h = f(g*\frac{x-\mu}{\sigma}+b)$：先归一化，在rescale &amp; reshift $\mu$ &amp; $\sigma$：compute from上一层的特征值 $g$ &amp; $b$：learnable params基于当前层 $f$：neurons’ weighting operation 各方法的主要区别在于mean &amp; variance的计算维度 对数据 BN：以一层每个神经元的输出为单位，即每个channel的mean&amp;var相互独立 LN：以一层所有神经元的输出为单位，即每个sample的mean&amp;var相互独立 IN：以每个sample在每个神经元的输出为单位，每个sample在每个channel的mean&amp;var都相互独立 GN：以每个sample在一组神经元的输出为单位，一组包含一个神经元的时候变成IN，一组包含一层所有神经元的时候就是LN 示意图： 对权重 WN：将权重分解为单位向量和一个固定标量，相当于神经元的任意输入vec点乘了一个单位vec（downscale），再rescale，进一步地相当于没有做shift和reshift的数据normalization WS：对权重做全套（归一化再recale），比WN多了shift，“zero-center is the key” 对op CosN： 将线性变换op替换成cos op：$f_w(x) = cos = \frac{w \cdot x}{|w||x|}$ 数学本质上又退化成了只有downscale的变换，表征能力不足 Whitening白化 purpose images的adjacent pixel values are highly correlated，thus redundant linearly move the origin distribution，making the inputs share the same mean &amp; variance method 首先进行PCA预处理，去掉correlation mean on sample（注意不是mean on image） \overline x = \frac{1}{N}\sum_{i=1}^N x_i\\ x^{'} = x - \overline x 协方差矩阵 X \in R^{d*N}\\ S = \frac{1}{N}XX^T 奇异值分解svd(S) S = U \Sigma V^T $\Sigma$为对角矩阵，对角上的元素为奇异值 $U=[u_1,u_2,…u_N]$中是奇异值对应的正交向量 投影变换 X^{'} = U_p^T X 取投影矩阵$U_p$ from $U$，$U_p \in R^{N*d}$表示将数据空间从N维投影到$U_p$所在的d维空间上 recover（投影逆变换） X^{''} = U_p^T X^{'} * 取投影矩阵$U_r=U_p^T$，就是将 数据空间从d维空间再投影回N维空间上 * PCA白化： * 对PCA投影后的新坐标，做归一化处理：基于特征值进行缩放 $$ X_{PCAwhite} = \Sigma^{-\frac{1}{2}}X^{&#39;} = \Sigma^{-\frac{1}{2}}U^TX $$ * $X_{PCAwhite}$的协方差矩阵$S_{PCAwhite} = I$，因此是去了correlation的 * ZCA白化：在上一步做完之后，再把它变换到原始空间，所以ZCA白化后的特征图更接近原始数据 * 对PCA白化后的数据，再做一步recover $$ X_{ZCAwhite} = U X_{PCAwhite} $$ * 协方差矩阵仍旧是I，合法白化 Layer Normalization 动机 BN reduces training time compute by each neuron require moving average depend on mini-batch size how to apply to recurrent neural nets propose layer norm [unlike BN] compute by each layer [like BN] with adaptive bias &amp; gain [unlike BN] perform the same computation at training &amp; test time [unlike BN] straightforward to apply to recurrent nets work well for RNNs 论点 BN reduce training time &amp; serves as regularizer require moving average：introduce dependencies between training cases the approxmation of mean &amp; variance expectations constraints on the size of a mini-batch intuition norm layer提升训练速度的核心是限制神经元输入输出的变化幅度，稳定梯度 只要控制数据分布，就能保持训练速度 方法 compute over all hidden units in the same layer different training cases have different normalization terms 没啥好说的，就是在channel维度计算norm further的GN把channel维度分组做norm，IN在直接每个特征计算norm gain &amp; bias 也是在对应维度：(hwd)c-dim https://tobiaslee.top/2019/11/21/understanding-layernorm/ 后续有实验发现，去掉两个learnable rescale params反而提点 考虑是在training set上的过拟合 实验 RNN上有用 CNN上比没有norm layer好，但是没有BN好：因为channel是特征维度，特征维度之间有明显的有用/没用，不能简单的norm Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks 动机 reparameterizing the weights decouple length &amp; direction no dependency between samples which suits well for recurrent reinforcement generative no additional memory and computation testified on MLP with CIFAR generative model VAE &amp; DRAW reinforcement DQN 论点 a neuron： get inputs from former layers(neurons) weighted sum over the inputs add a bias elementwise nonlinear transformation batch outputs：one value per sample intuition of normalization： give gradients that are more like whitened natural gradients BN：make the outputs of each neuron服从std norm our WN： inspired by BN does not share BN’s across-sample property no addition memory and tiny addition computation Instance Normalization: The Missing Ingredient for Fast Stylization 动机 stylization：针对风格迁移网络 with a small change：swapping BN with IN achieve qualitative improvement 论点 stylized image a content image + a style image both style and content statistics are obtained from a pretrained CNN for image classification methods optimization-based：iterative thus computationally inefficient generator-based：single pass but never as good as our work revisit the feed-forward method replace BN in the generator with IN keep them at test time as opposed to freeze 方法 formulation given a fixed stype image $x_0$ given a set of content images $x_t, t= 1,2,…,n$ given a pre-trained CNN with a variable z controlling the generation of stylization results compute the stylied image g($x_t$, z) compare the statistics：$min_g \frac{1}{n} \sum^n_{t=1} L(x_0, x_t, g(x_t, z))$ comparing target：the contrast of the stylized image is similar to the constrast of the style image observations the more training examples, the poorer the qualitive results the result of stylization still depent on the constrast of the content image intuition 风格迁移本质上就是将style image的contrast用在content image的：也就是rescale content image的contrast constrast是per sample的：$\frac{pixel}{\sum pixels\ on\ the\ map}$ BN在norm的时候将batch samples搅合在了一起 IN instance-specfic normalization also known as contrast normalization 就是per image做标准化，没有trainable/frozen params，在test phase也一样用 Group Normalization 动机 for small batch size do normalization in channel groups batch-independent behaves stably over different batch sizes approach BN’s accuracy 论点 BN requires sufficiently large batch size (e.g. 32) Mask R-CNN frameworks use a batch size of 1 or 2 images because of higher resolution, where BN is “frozen” by transforming to a linear layer synchronized BN 、BR LN &amp; IN effective for training sequential models or generative models but have limited success in visual recognition GN能转换成LN／IN WN normalize the filter weights, instead of operating on features 方法 group it is not necessary to think of deep neural network features as unstructured vectors 第一层卷积核通常存在一组对称的filter，这样就能捕获到相似特征 这些特征对应的channel can be normalized together normalization transform the feature x：$\hat x_i = \frac{1}{\sigma}(x_i-\mu_i)$ the mean and the standard deviation： \mu_i=\frac{1}{m}\sum_{k\in S_i}x_k\\ \sigma_i=\sqrt {\frac{1}{m}\sum_{k\in S_i}(x_k-\mu_i)^2+\epsilon} the set $S_i$ BN： $S_i=\{k|k_C = i_C\}$ pixels sharing the same channel index are normalized together for each channel, BN computes μ and σ along the (N, H, W) axes LN $S_i=\{k|k_N = i_N\}$ pixels sharing the same batch index (per sample) are normalized together LN computes μ and σ along the (C,H,W) axes for each sample IN $S_i=\{k|k_N = i_N, k_C=i_C\}$ pixels sharing the same batch index and the same channel index are normalized together LN computes μ and σ along the (H,W) axes for each sample GN $S_i=\{k|k_N = i_N, [\frac{k_C}{C/G}]=[\frac{i_C}{C/G}]\}$ computes μ and σ along the (H, W ) axes and along a group of C/G channels linear transform to keep representational ability per channel scale and shift：$y_i = \gamma \hat x_i + \beta$ relation to LN LN assumes all channels in a layer make “similar contributions” which is less valid with the presence of convolutions GN improved representational power over LN to IN IN can only rely on the spatial dimension for computing the mean and variance it misses the opportunity of exploiting the channel dependence 【QUESTION】BN也没考虑通道间的联系啊，但是计算mean和variance时跨了sample implementation reshape learnable $\gamma \&amp; \beta$ computable mean &amp; var 实验 GN相比于BN，training error更低，但是val error略高于BN GN is effective for easing optimization loses some regularization ability it is possible that GN combined with a suitable regularizer will improve results 选取不同的group数，所有的group&gt;1均好于group=1（LN） 选取不同的channel数（C／G），所有的channel&gt;1均好于channel=1（IN） Object Detection frozen：因为higher resolution，batch size通常设置为2/GPU，这时的BN frozen成一个线性层$y=\gamma(x-\mu)/\sigma+beta$，其中的$\mu$和$sigma$是load了pre-trained model中保存的值，并且frozen掉，不再更新 denote as BN* replace BN* with GN during fine-tuning use a weight decay of 0 for the γ and β parameters WS: Weight Standardization 动机 accelerate training micro-batch： 以BN with large-batch为基准 目前BN with micro-batch及其他normalization methods都不能match这个baseline operates on weights instead of activations 效果 match or outperform BN smooth the loss 论点 two facts BN的performance gain与reduction of internal covariate shift没什么关系 BN使得optimization landscape significantly smoother 因此our target is to find another technique achieves smooth landscape work with micro-batch normalization methods focus on activations 不展开 focus on weights WN：just length-direction decoupling 方法 Lipschitz constants BN reduces the Lipschitz constants of the loss function makes the gradient more Lipschitz BN considers the Lipschitz constants with respect to activations，not the weights that the optimizer is directly optimizing our inspiration standardize the weights也同样能够smooth the landscape 更直接 smoothing effects on activations and weights是可以累积的，因为是线性运算 Weight Standardization reparameterize the original weights $W$ 对卷积层的权重参数做变换，no bias $W \in R^{O * I}$ $O=C_{out}$ $I=C_{in}*kernel_size$ optimize the loss on $\hat W$ compute mean &amp; var on I-dim 只做标准化，无需affine，因为默认后续还要接一个normalization layer对神经元进行refine WS normalizes gradients 拆解： eq5：$W$ to $\dot W$，减均值，zero-centered eq6：$\dot W$ to $\hat W$，除方差，one-varianced eq8：$\delta \hat W$由前一步的梯度normalize得到 eq9：$\delta \dot W$也由前一步的梯度normalize 最终用于梯度更新的梯度是zero-centered WS smooths landscape 判定是否smooth就看Lipschitz constant的大小 eq5和eq6都能reduce the Lipschitz constant 其中eq5 makes the major improvements eq6 slightly improves，因为计算量不大，所以保留 实验 ImageNet BN的batchsize是64，其余都是1，其余的梯度更新iterations改成64——使得参数更新次数同步 所有的normalization methods加上WS都有提升 裸的normalization methods里面batchsize1的GN最好，所以选用GN+WS做进一步实验 GN+WS+AF：加上conv weight的affine会harm code 123456# official release# 放在WSConv2D子类的call里面kernel_mean = tf.math.reduce_mean(kernel, axis=[0, 1, 2], keepdims=True, name='kernel_mean')kernel = kernel - kernel_meankernel_std = tf.keras.backend.std(kernel, axis=[0, 1, 2], keepdims=True)kernel = kernel / (kernel_std + 1e-5)]]></content>
      <tags>
        <tag>CNN, layer, normalization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NFNet]]></title>
    <url>%2F2021%2F02%2F22%2FNFNet%2F</url>
    <content type="text"><![CDATA[NFNet: High-Performance Large-Scale Image Recognition Without Normalization 动机 NF： normalization-free aims to match the test acc of batch-normalized networks attain new SOTA 86.5% pre-training + fine-tuning上也表现更好89.2% batch normalization 不是完美解决方案 depends on batch size non-normalized networks accuracy instabilities：develop adaptive gradient clipping 论点 vast majority models variants of deep residual + BN allow deeper, stable and regularizing disadvantages of batch normalization computational expensive introduces discrepancy between training &amp; testing models &amp; increase params breaks the independence among samples methods seeks to replace BN alternative normalizers study the origin benefits of BN train deep ResNets without normalization layers key theme when removing normalization suppress the scale of the residual branch simplest way：apply a learnable scalar recent work：suppress the branch at initialization &amp; apply Scaled Weight Standardization，能追上ResNet家族，但是没追上Eff家族 our NFNets’ main contributions propose AGC：解决unstable问题，allow larger batch size and stronger augmentatons NFNets家族刷新SOTA：又快又准 pretraining + finetuning的成绩也比batch normed models好 方法 Understanding Batch Normalization four main benefits downscale the residual branch：从initialization就保证残差分支的scale比较小，使得网络has well-behaved gradients early in training，从而efficient optimization eliminates mean-shift：ReLU是不对称的，stacking layers以后数据分布会累积偏移 regularizing effect：mini-batch作为subset对于全集是有偏的，这种noise可以看作是regularizer allows efficient large-batch training：数据分布稳定所以loss变化稳定，同时大batch更接近真实分布，因此我们可以使用更大的learning rate，但是这个property仅在使用大batch size的时候有效 NF-ResNets recovering the benefits of BN：对residual branch进行scale和mean-shift residual block：$h_{i+1} = h_i + \alpha f_i (h_i/\beta_i)$ $\beta_i = Var(h_i)$：对输入进行标准化（方差为1），这是个expected value，不是算出来的，结构定死就定死了 Scaled Weight Standardization &amp; scaled activation 比原版的WS多了一个$\sqrt N$的分母 源码实现中比原版WS还多了learnable affine gain 使得conv-relu以后输出还是标准分布 $\alpha=0.2$：rescale residual branch上，最终的输出为$\alpha*$标准分布，方差是$\alpha^2$ id path上，输出还是$h_{i}$，方差是$Var(h_i)$ update这个block输出的方差为$Var(h_{i+1}) = Var(h_i)+\alpha^2$，来更新下一个block的 $\beta$ variance reset 每个transition block以后，把variance重新设定为$1+\alpha^2$ 在接下来的non-transition block中，用上面的update公式更新expected std 再加上additional regularization（Dropout和Stochastic Depth两种正则手段），就满足了BN benefits的前三条 在batch size较小的时候能够catch up甚至超越batch normalized models 但是large batch size的时候perform worse 对于一个标准的conv-bn-relu，从workflow上看 origin：input——一个free的conv weighting——BN（norm &amp; rescale）——activation NFNet：input——standard norm——normed weighting &amp; activation——rescale Adaptive Gradient Clipping for Efficient Large-Batch Training 梯度裁剪： clip by norm：用一个clipping threshold $\lambda$ 进行rescale，training stability was extremely sensitive to 超参的选择，settings（model depth, the batch size, or the learning rate）一变超参就要重新调 clip by value：用一个clipping value进行上下限截断 AGC given 某层的权重$W \in R^{NM}$ 和 对应梯度$G \in R^{NM}$ ratio $\frac{||G||_F}{||W||_F}$ 可以看作是梯度变化大小的measurement 所以我们直观地想到将这个ratio进行限幅：所谓的adaptive就是在梯度裁剪的时候不是对所有梯度一刀切，而是考虑其对应权重大小，从而进行更合理的调节 但是实验中发现unit-wise的gradient norm要比layer-wise的好：每个unit就是每行，对于conv weights就是(hxwxCin)中的一个 scalar hyperparameter $\lambda$ * the optimal value may depend on the choice of optimizer, learning rate and batch size * empirically we found $\lambda$ should be smaller for larger batches ablations for AGC 用pre-activation NF-ResNet-50 和 NF-ResNet-200 做实验，batch size选择从256到4096，学习率从0.1开始基于batch size线性增长，超参$\lambda$的取值见右图 左图结论1：在batch size较小的情况下，NF-Nets能够追上甚至超越normed models的精度，但是batch size一大（2048）情况就恶化了，但是有AGC的NF-Nets则能够maintaining performance comparable or better than～～～ 左图结论2：the benefits of using AGC are smaller when the batch size is small 右图结论1：超参$\lambda$的取值比较小的时候，我们对梯度的clipping更strong，这对于使用大batch size训练的稳定性来说非常重要 whether or not AGC is beneficial for all layers * it is always better to not clip the final linear layer * 最开始的卷积不做梯度裁剪也能稳定训练 最终we apply AGC to every layer except for the final linear layer Normalizer-Free Architectures begin with SE-ResNeXt-D model about group width * set group width to 128 the reduction in compute density means that 只减少了理论上的FLOPs，没有实际加速 about stages * R系列模型加深的时候是非线性增长，疯狂叠加stage3的block数，因为这一层resolution不大，channel也不是最多，兼顾了两侧计算量 我们给F0设置为[1,2,6,3]，然后在deeper variants中对每个stage的block数用一个scalar N线形增长 about width * 仍旧对stage3下手，[256,512,1536,1536] * roughly preserves the training speed 一个论点：stage3 is the best place to add capacity，因为deeper enough同时have access to deeper levels同时又比最后一层有slightly higher resolution about block * 实验发现最有用的操作是adding an additional 3 × 3 grouped conv after the first * overview about scaling variants * eff系列采用的是R、W、D一起增长，因为eff的block比较轻量 但是对R系列来说，只增长D和R就够了 补充细节 * 在inference阶段使用比训练阶段slightly higher resolution 随着模型加大increase the regularization strength： scale the drop rate of Dropout 调整stochastic depth rate和weight decay则not effective se-block的scale乘个2 SGD params: Nesterov=True, momentum=0.9, clipnorm=0.01 lr： 先warmup再余弦退火：increase from 0 to 1.6 over 5 epochs, then decay to zero with cosine annealing 余弦退火cosine annealing summary 总结来说，就是拿来一个SE-ResNeXt-D 先做结构上的调整，modified width and depth patterns以及a second spatial convolution，还有drop rate，resolution 再做对梯度的调整：除了最后一个线形分类层以外，全用AGC，$\lambda=0.01$ 最后是训练上的trick：strong regularization and data augmentation detailed view of NFBlocks transition block：有下采样的block 残差branch上，bottleneck的narrow ratio是0.5 每个stage的3x3 conv的group width永远是128，而group数目是在随着block width变的 skip path接在 $\beta$ downscaling 之后 skip path上是avg pooling + 1x1 conv non-transition block：无下采样的block bottleneck-ratio仍旧是0.5 3x3conv的group width仍旧是128 skip path接在$\beta$ downscaling 之前 skip path就是id 实验]]></content>
      <tags>
        <tag>CNN, classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[repVGG]]></title>
    <url>%2F2021%2F02%2F09%2FrepVGG%2F</url>
    <content type="text"><![CDATA[RepVGG: Making VGG-style ConvNets Great Again 动机 plain ConvNets simply efficient but poor performance propose a CNN architecture RepVGG 能够decouple为training-time和inference-time两个结构 通过structure re-paramterization technique inference-time architecture has a VGG-like plain body faster 83% faster than ResNet-50 or 101% faster than ResNet-101 accuracy-speed trade-off reaches over 80% top-1 accuracy outperforms ResNets by a large margin verify on classification &amp; semantic segmentation tasks 论点 well-designed CNN architectures Inception，ResNet，DenseNet，NAS models deliver higher accuracy drawbacks multi-branch designs：slow down inference and reduce memory utilization，对高并行化的设备不友好 some components：depthwise &amp; channel shuffle，increase memory access cost MAC(memory access cost) constitutes a large time usage in groupwise convolution：我的groupconv实现里cardinality维度上计算不并行 FLOPs并不能precisely reflect actual speed，一些结构看似比old fashioned VGG/resnet的FLOPs少，但实际并没有快 multi-branch 通常multi-branch model要比plain model表现好 因为makes the model an implicit ensemble of numerous shallower models so that avoids gradient vanishing benefits are all for training drawbacks are undesired for inference the proposed RepVGG advantages plain architecture：no branches 3x3 conv &amp; ReLU组成 没有过重的人工设计痕迹 training time use identity &amp; 1x1 conv branches at inference time identity 可以看做degraded 1x1 conv 1x1 conv 可以看做degraded 3x3 conv 最终整个conv-bn branches能够整合成一个3x3 conv inference-time model只包含conv和ReLU：没有max pooling！！ fewer memory units：分支会占内存，直到分支计算结束，plain结构的memory则是immediately released 方法 training-time ResNet-like block id + 1x1 conv + 3x3 conv multi-branches use BN in each branch with n blocks, the model can be interpreted as an ensemble of $3^n$ models stride2的block应该没有id path吧？？ simply stack serveral blocks to construct the training model inference-time re-param inference-time BN也是一个线性计算 两个1x1 conv都可以转换成中通的3x3 kernel，有权/无权 要求各branch has the same strides &amp; padding pixel要对齐 architectural specification variety：depth and width does not use maxpooling：只有一种operator：3x3 conv+relu head：GAP + fc / task specific 5 stages 第一个stage处理high resolution，stride2 第五个stage shall have more channels，所以只用一层，save parameters 给倒数第二个stage最多层，考虑params和computation的balance RepVGG-A：[1,2,4,14,1]，用来compete against轻量和中量级model RepVGG-B：deeper in s2,3,4，[1,4,6,16,1]，用来compete against high-performance ones basic width：[64, 128, 256, 512] width multiplier a &amp; b a控制前4个stage宽度，b控制最后一个stage [64a, 128a, 256a, 512b] 第一个stage的宽度只接受变小不接受变大，因为大resolution影响计算量，min(64,64a) further reduce params &amp; computation groupwise 3x3 conv 跳着层换：从第三开始，第三、第五、 number of groups：1，2，4 globally 实验 分支的作用 结构上的微调 id path去掉BN 把所有的BN移动到add的后面 每个path加上relu ImageNet分类任务上对标其他模型 simple augmentation strong：Autoaugment, label smoothing and mixup]]></content>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[transform in CNN]]></title>
    <url>%2F2021%2F02%2F03%2Ftransform-in-CNN%2F</url>
    <content type="text"><![CDATA[综述 几何变换 STN： 普通的CNN能够隐式的学习一定的平移、旋转不变性，让网络能够适应这种变换：降采样结构本身能够使得网络对变换不敏感 从数据角度出发，我们还会引入各种augmentation，强化网络对变化的不变能力 deepMind为网络设计了一个显式的变换模块来学习各种变化，将distorted的输入变换回去，让网络学习更简单的东西 参数量：就是变换矩阵的参数，通常是2x3的纺射变化矩阵，也就是6个参数 deformable conv： based on STN 针对分类和检测分别提出deformable convolution和deformable RoI pooling： 感觉deformable RoI pooling和guiding anchor里面的feature adaption是一个东西 参数量：regular kernel params 3x3 + deformable offsets 3x3x2 what’s new？ 个人认为引入更多的参数引入的变化 首先STN是从output到input的映射，使用变换矩阵M通常只能表示depictable transformation，且全图只有1个transformation 其次STN的sampling kernel也是预定义的算法，对kernel内的所有pixel使用相同的变化，也就是1个weight factor deformable conv是从input到output的映射，映射可以是任意的transformation，且3x3x2的参数最多可以包含3x3种transformation sampling kernel对kernel内的每个点，也可以有不同的权重，也就是3x3个weight factor 还有啥跟形变相关的 attention机制 spatial attention：STN，sSE channel attention：SENet 同时使用空间attention和通道attention机制：CBAM papers [STN] STN: Spatial Transformer Networks，STN的变换是pre-defined的，是针对全局featuremap的变换 [DCN 2017] Deformable Convolutional Networks ，DCN的变换是更随机的，是针对局部kernel分别进行的变化，基于卷积核添加location-specific shift [DCNv2 2018] Deformable ConvNets v2: More Deformable, Better Results，进一步消除irrelevant context，基于卷积核添加weighted-location-specific shift，提升performance [attention系列paper] [SENet &amp;SKNet &amp; CBAM &amp; GC-Net][https://amberzzzz.github.io/2020/03/13/attention%E7%B3%BB%E5%88%97/] STN: Spatial Transformer Networks 动机 传统卷积：lack the ability of spacially invariant propose a new learnable module can be inserted into CNN spatially manipulate the data without any extra supervision models learn to be invariant to transformations 论点 spatially invariant the ability of being invariant to large transformations of the input data max-pooling 在一定程度上spatially invariant 因为receptive fields are fixed and local and small 必须叠加到比较深层的时候才能实现，intermediate feature layers对large transformations不太行 是一种pre-defined mechanism，跟sample无关 spatial transformation module conditioned on individual samples dynamic mechanism produce a transformation and perform it on the entire feature map task场景 distorted digits分类：对输入做tranform能够simplify后面的分类任务 co-localisation： spatial attention related work 生成器用来生成transformed images，从而判别器能够学习分类任务from transformation supervision 一些methods试图从网络结构、feature extractors的角度的获得invariant representations，while STN aims to achieve this by manipulating the data manipulating the data通常就是基于attention mechanism，crop涉及differentiable问题 方法 formulation localisation network：predict transform parameters grid generator：基于predicted params生成sampling grid sampler：element-multiply localisation network input feature map $U \in R^{hwc}$ same transformation is applied to each channel generate parameters of transformation $\theta$：1-d vector fc / conv + final regression layer parameterised sampling grid sampling kernel applied by pixel general affine transformation：cropping，translation，rotation，scale，skew ouput map上任意一点一定来自变换前的某一点，反之不一定，input map上某一点可能是bg，被crop掉了，所以pointwise transformation写成反过来的： target points构成的点集就是sampling points on the input feature map differentiable image sampling 通过上一步的矩阵transformation，得到input map上需要保留的source point set 对点集中每一点apply kernel 通用的插值表达式： 最近邻kernel是个pulse函数 bilinear kernel是个distance&gt;1的全mute掉，分段可导 STN：Spatial Transformer Networks 把spatial transformer嵌进CNN去：learn how to actively transform the features to help minimize the overall cost computationally fast 几种用法 feed the output of the localization network $\theta$ to the rest of the network：因为transform参数explicitly encodes目标的位置姿态信息 place multiple spatial transformers at increasing depth：串行能够让深层的transformer学习更抽象的变换 place multiple spatial transformers in parallel：并行的变换使得每个变换针对不同的object 实验 R、RTS、P、E：distortion ahead aff、proj、TPS：transformer predefined aff：给定角度？？ TPS：薄板样条插值 Deformable Convolutional Networks 动机 CNN：fixed geometric structures enhance the transformation modeling capability deformable convolution deformable RoI pooling without additional supervision share similiar spirit with STN 论点 to accommodate geometric variations data augmentation is limited to model large, unknown transformations fixed receptive fields is undesirable for high level CNN layers that encode the semantics 使用大量增广的数据，枚举不全，而且收敛慢，所需网络参数量大 对于提取语义特征的高层网络来讲，固定的感受野对不同目标不友好 introduce two new modules deformable convolution learning offsets for each kernel via additional convolutional layers deformable RoI pooling learning offset for each bin partition of the previous RoI pooling 方法 overview operate on the 2D spatial domain remains the same across the channel dimension deformable convolution 正常的卷积： $y(p_0) = \sum w(p_n)*x(p_0 + p_n)$ $p_n \in R\{(-1,-1),(-1,0),…, (0,0), (1,1)\}$ deformable conv：with offsets $\Delta p_n$ $y(p_0) = \sum w(p_n)*x(p_0 + p_n + \Delta p_n)$ offset value is typically fractional bilinear interpolation： $x(p) = \sum_q G(q,p)x(q)$ 其中$G(q,p)$是条件：$G(q,p)=max(0, 1-|q_x-p_x|)*max(0, 1-|q_y-p_y|)$ 只计算和offset点距离小于1个单位的邻近点 实现 offsets conv和特征提取conv是一样的kernel：same spatial resolution and dilation（N个position） the channel dimension 2N：因为是x和y两个方向的offset deformable RoI pooling RoI pooling converts an input feature map of arbitrary size into fixed size features 常规的RoI pooling divides ROI into k*k bins and for each bin：$y(i,j) = \sum_{p \in bin(i,j)} x(p_0+p)/n_{ij}$ 对feature map上划分到每个bin里面所有的点 deformable RoI pooling：with offsets $\Delta p_{ij}$ $y(i,j) = \sum_{p \in bin(i,j)} x(p_0+p+\Delta p_{ij})/n_{ij}$ scaled normalized offsets：$\Delta p_{ij} = \gamma \Delta p_{ij} (w,h) $ normalized offset value is fractional bilinear interpolation on the pooled map as above 实现 fc layer：k*k*2个element（sigmoid？） position sensitive RoI Pooling fully convolutional input feature map先通过卷积扩展成k*k*(C+1)通道 对每个C+1(包含kk个feature map)，conv出全图的offset(2\k*k个) deformable convNets initialized with zero weights learning rates are set to $\beta$ times of the learning rate for the existing layers $\beta=1.0$ for conv $\beta=0.01$ for fc feature extraction back：ResNet-101 &amp; Aligned-Inception-ResNet withoutTop：A randomly initialized 1x1 conv is added at last to reduce the channel dimension to 1024 last block stride is changed from 2 to 1 the dilation of all the convolution filters with kernel size&gt;1 is changed from 1 to 2 Optionally last block use deformable conv in res5a,b,c segmentation and detection deeplab predicts 1x1 score maps Category-Aware RPN run region proposal with specific class modified faster R-CNN：add ROI pooling at last conv optional faster R-CNN：use deformable ROI pooling R-FCN：state-of-the-art detector optional R-FCN：use deformable ROI pooling 实验 Accuracy steadily improves when more deformable convolution layers are used：使用越多层deform conv越好，经验取了3 the learned offsets are highly adaptive to the image content：大目标的间距大，因为reception field大，consistent in different layers atrous convolution also improves：default networks have too small receptive fields，但是dilation需要手调到最优 using deformable RoI pooling alone already produces noticeable performance gains, using both obtains significant accuracy improvements Deformable ConvNets v2: More Deformable, Better Results 动机 DCN能够adapt一定的geometric variations，但是仍存在extend beyond image content的问题 to focus on pertinent image regions increased modeling power more deformable layers updated DCNv2 modules stronger training propose feature mimicking scheme verified on incorporated into Faster-RCNN &amp; Mask RCNN COCO for det &amp; set still lightweight and easy to incorporate 论点 DCNv1 deformable conv：在standard conv的基础上generate location-specific offsets which are learned from the preceding feature maps deformable pooling：offsets are learned for the bin positions in RoIpooling 通过可视化散点图发现有部分散点落在目标外围 propose DCNv2 equip more convolutional layers with offset modified module each sample not only undergoes a learned offset but also a learned feature amplitude effective trainin use RCNN as the teacher network since RCNN learns features unaffected by irrelevant info outside the ROI feature mimicking loss 方法 stacking more deformable conv layers replace more regular conv layers by their deformable counterparts： resnet50的stage3、4、5的3x3conv都替换成deformable conv：13个conv layer DCNv1是把stage5的3个resblock的3x3 conv替换成deformable conv：3个deconv layer 因为DCNv1里面在PASCAL上面实验发现再多的deconv精度就饱和了，但是DCNv2是在harder dataset COCO上面的best-acc-efficiency-tradeoff modulated deformable conv modulate the input feature amplitudes from different spacial locations/bins set the learnable offset &amp; scalar for the k-th location：$\Delta p_k$和$\Delta m_k$ set the conv kernel dilation：$p_k$，resnet里面都是1 the value for location p is：$y(p) = \sum_{k=1}^K w_k x(p+p_k+\Delta p_k)\Delta m_k$，bilinear interpolation 目的是抑制无关信号 learnable offset &amp; scalar obtained via a separate conv layer over the same input feature map x 输出有3K个channel：2K for xy-offset，K for scalar offset的conv后面没激活函数，因为范围无限 scalar的conv后面有个sigmoid，将range控制在[0,1] 两个conv全0初始化 两个conv layer的learning rate比existing layers小一个数量级 modulated deformable RoIpooling given an input ROI split into K(7x7) spatial bins average pooling over the sampling points for each bin计算bin的value the bin value is：$y(k) = \sum_{j=1}^{n_k} x(p_{kj}+\Delta p_k)\Delta m_k /n_k$，bilinear interpolation a sibling branch 2个1024d-fc：gaussian initialization with 0.01 std dev 1个3Kd-fc：全0初始化 last K channels + sigmoid learning rate跟existing layers保持一致 RCNN feature mimicking 发现无论是conv还是deconv，error-bound都很大 尽管从设计思路上，DCNv2是带有mute irrelevant的能力的，但是事实上并没做到 说明such representation cannot be learned well through standard FasterRCNN training procedure： 说白了就是supervision力度不够 需要additional guidance feature mimic loss enforced only on positive ROIs：因为背景类往往需要更长距离/更大范围的context信息 architecture add an additional RCNN branch RCNN input cropped images，generate 14x14 featuremaps，经过两个fc变成1024-d 和FasterRCNN里对应的counterpart，计算cosine similarity 这个太扯了不展开了]]></content>
      <tags>
        <tag>几何变换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spineNet]]></title>
    <url>%2F2021%2F01%2F28%2FspineNet%2F</url>
    <content type="text"><![CDATA[SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization 动机 object detection task requiring simultaneous recognition and localization solely encoder performs not well while encoder-decoder architectures are ineffective propose SpineNet scale-permuted intermediate features cross-scale connections searched by NAS on detection COCO can transfer to classification tasks 在轻量和重量back的一阶段网络中都涨点领先 论点 scale-decreasing backbone throws away the spatial information by down-sampling challenging to recover 接一个轻量的FPN： scale-permuted model scales of features can increase/decrease anytime：retain the spacial information connections go across scales：multi-scale fusion searched by NAS 是一个完整的FPN，不是encoder-decoder那种可分的形式 directly connect to classification and bounding box regression subnets base on ResNet50 use bottleneck feature blocks two inputs for each feature blocks roughly the same computation 方法 formulation overall architecture stem：scale-decreased architecture scale-permuted network blocks in the stem network can be candidate inputs for the following scale-permuted network scale-permuted network building blocks：$B_k$ feature level：$L_3 - L_7$ output features：1x1 conv，$P_3 - P_7$ search space scale-permuted network： block只能从前往后connect based on resNet blocks channel 256 for $L_5, L_6, L_7$ cross-scale connections： two input connections for each block from lower ordering block / stem resampling narrow factor $\alpha$：1x1 conv 上采样：interpolation 下采样：3x3 s2 conv element-wise add block adjustment intermediate blocks can adjust its scale level &amp; type level from {-1, 0, 1, 2} select from bottleneck / residual block family of models R[N] - SP[M]：N feature layers in stem &amp; M feature layers in scale-permuted layers gradually shift from stem to SP with size decreasing spineNet family basic：spineNet-49 spineNet-49S：channel数scaled down by 0.65 spineNet-96：double the number of blocks spineNet-143：repeat 3 times，fusion narrow factor $\alpha=1$ spineNet-190：repeat 4 times，fusion narrow factor $\alpha=1$，channel数scaled up by 1.3 实验 在mid/heavy量级上，比resnet-family-FPN涨出两个点 在light量级上，比mobileNet-family-FPN涨出一个点]]></content>
      <tags>
        <tag>backbone</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[guided anchoring]]></title>
    <url>%2F2021%2F01%2F27%2Fguided-anchoring%2F</url>
    <content type="text"><![CDATA[原作者知乎reference：https://zhuanlan.zhihu.com/p/55854246 不完全是anchor-free，因为还是有decision grid to choose from的，应该说是adaptive anchor instead of hand-picked 为了特征和adaptive anchor对齐，引入deformable conv Region Proposal by Guided Anchoring 动机 most methods predefined anchors do a uniformed dense prediction our method use sematic features to guide the anchoring anchor size也是网络预测参数，compute from feature map arbitrary aspect ratios feature inconsistency 不同的anchor loc都是对应feature map上某一个点 变化的anchor size和固定的位置向量之间存在inconsistency 引入feature adaption module use high-quality proposals GA-RPN提升了proposal的质量 因此我们对proposal进入stage2的条件更严格 adopt in Fast R-CNN, Faster R-CNN and RetinaNet均涨点 RPN提升显著：9.1 MAP也有涨点：1.2-2.7 还可以boosting trained models boosting a two-stage detector by a fine-tuning schedule 论点 alignment &amp; consistency 我们用feature map的pixels作为anchor representations，那么anchor centers必须跟feature pixels保持align 不同pixel的reception field必须跟对应的anchor size保持匹配 previous sliding window scheme对每个pixel都做一样的操作，用同样一组anchor，因此是align和consist的 previous progressly refining scheme对anchor的位置大小做了refinement，ignore the alignment &amp; consistency issue，是不对的！！ disadvantage of predefined anchors hard hyperparams huge pos/neg imbalance &amp; computation we propose GA-RPN learnable anchor shapes to mitigate the hand-picked issue feature adaptation to solve the consistency issue key concerns in this paper learnable anchors joint anchor distribution alignment &amp; consistency high-quality proposals 方法 formulation $p(x,y,w,h|I) = p(x,y|I)p(w,h|x,y,I)$ 将问题解耦成位置和尺寸的预测，首先anchor的loc服从full image的均匀分布，anchor的size建立在loc存在的基础上 two branches for loc &amp; shape prediction loc：binary classification，hxwx1 shape：location-dependent shapes，hxwx2 anchors：loc probabilities above a certain threshold &amp; correponding ‘most probable’ anchor shape multi-scale the anchor generation parameters are shared feature adaptation module adapts the feature according to the anchor shape anchor location prediction indicates the probability of an object’s center 一层卷积：1x1 conv，channel1，sigmoid transform back：each grid(i,j) corresponds to coords ((i+0.5)*s, (j+0.5)*s) on the origin map filter out 90% of the regions thus replace the ensuing conv layers by masked convs groud truth binary label map each level：center region &amp; ignore region &amp; outside region，基于object center的方框 $\sigma_1=0.2，\sigma_2=0.5$：region box的长宽系数 ？？？用centerNet的heatmap会不会更好？？？ focal loss $L_{loc}$ anchor shape prediction predicts the best shape for each location best shape：a shape that lead to best iou with the nearest gt box 一层卷积：1x1 conv，channel2，[-1,1] transform layer：transform direct [-1,1] outputs to real box shape $w = \sigma s e^{dw}$ $h = \sigma s e^{dh}$ s：stride $\sigma$：经验参数，8 in experiments set 9 pairs of (w,h) as RetinaNet，calculate the IoU of these sampled anchors with gt，take the max as target value bounded iou loss：$L_{shape} = L_1(1-min(\frac{w}{w_g}, \frac{w_g}{w})) + L_1(1-min(\frac{h}{h_g}, \frac{h_g}{h}))$ feature adaptation intuition：the feature corresponding to different size of anchor shapes应该encode different content region inputs：feature map &amp; anchor shape location-dependent transformation：3x3 deformable conv deformable conv的offset是anchor shape得到的 outputs：adapted features with adapted features then perform further classification and bounding-box regression training jointly optimize：$L = \lambda_1 L_{loc} + \lambda_2 L_{shape} + L_{cls} + L_{reg}$ $\lambda_1=0.2，\lambda_2=0.5$ each level of feature map should only target objects of a specific scale range：但是ASFF论文主张说这种arrange by scale的模式会引入前背景inconsistency？？ High-quality Proposals set a higher positive/negative threshold use fewer samples]]></content>
      <tags>
        <tag>目标检测，one/two-stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ASFF]]></title>
    <url>%2F2021%2F01%2F25%2FASFF%2F</url>
    <content type="text"><![CDATA[Learning Spatial Fusion for Single-Shot Object Detection 动机 inconsistency when fuse across different feature scales propose ASFF suppress the inconsistency spatially filter conflictive information：想法应该跟SSE-block类似 build on yolov3 introduce a bag of tricks anchor-free pipeline 论点 ssd is one of the first to generate pyramidal feature representations deeper layers reuse the formers bottom-up path small instances suffers low acc because containing insufficient semanic info FPN use top-down path shares rich semantics at all levels improvement：more strengthening feature fusion 在使用FPN时，通常不同scale的目标绑定到不同的level上面 inconsistency：其他level的feature map对应位置的信息则为背景 some methods set ignore region in adjacent features 方法 introduce advanced techniques mixup cosine learning rate schedule sync-bn an anchor-free branch to run jointly with anchor-based ones L1 loss + IoU loss fusion 全联接而非adjacent merge：三个level的fuse map都来自三个level的feature map 上采样： 1x1 conv：对齐channel upsamp with interpolation 下采样： s2：3x3 s2 conv s4：maxpooling + 3x3 s2 conv adaptive fusion pixel level的reweight shared across channels：hxwx1 对来自三个level的feature map，resolution对齐以后，分别1x1conv，channel 1 norm the weights：softmax 为啥能suppress inconsistency：三个level的像素点，只激活一个另外两个是0的情况是绝对不harm的，相当于上面ignore那个方法拓展成adaptive training apply mixup on the classification pretraining of D53 turn off mixup augmentation for the last 30 epochs. inference the detection header at each level first predicts the shape of anchors？？？这个不太懂 ASFF &amp; ASFF* enhanced version of ASFF by integrating other lightweight modules dropblock &amp; RFB 实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768class ASFF(nn.Module): def __init__(self, level, activate, rfb=False, vis=False): super(ASFF, self).__init__() self.level = level self.dim = [512, 256, 128] self.inter_dim = self.dim[self.level] if level == 0: self.stride_level_1 = conv_bn(256, self.inter_dim, kernel=3, stride=2) self.stride_level_2 = conv_bn(128, self.inter_dim, kernel=3, stride=2) self.expand = conv_bn(self.inter_dim, 512, kernel=3, stride=1) elif level == 1: self.compress_level_0 = conv_bn(512, self.inter_dim, kernel=1) self.stride_level_2 = conv_bn(128, self.inter_dim, kernel=3, stride=2) self.expand = conv_bn(self.inter_dim, 256, kernel=3, stride=1) elif level == 2: self.compress_level_0 = conv_bn(512, self.inter_dim, kernel=1, stride=1) self.compress_level_1= conv_bn(256,self.inter_dim,kernel=1,stride=1) self.expand = conv_bn(self.inter_dim, 128, kernel=3, stride=1) compress_c = 8 if rfb else 16 self.weight_level_0 = conv_bn(self.inter_dim, compress_c, 1, 1, 0) self.weight_level_1 = conv_bn(self.inter_dim, compress_c, 1, 1, 0) self.weight_level_2 = conv_bn(self.inter_dim, compress_c, 1, 1, 0) self.weight_levels = conv_bias(compress_c * 3, 3, kernel=1, stride=1, padding=0) self.vis = vis def forward(self, x_level_0, x_level_1, x_level_2): # 跟论文描述一样：上采样先1x1conv对齐，再upinterp，下采样3x3 s2 conv if self.level == 0: level_0_resized = x_level_0 level_1_resized = self.stride_level_1(x_level_1) level_2_downsampled_inter = F.max_pool2d(x_level_2, 3, stride=2, padding=1) level_2_resized = self.stride_level_2(level_2_downsampled_inter) elif self.level == 1: level_0_compressed = self.compress_level_0(x_level_0) sh = torch.tensor(level_0_compressed.shape[-2:])*2 level_0_resized = F.interpolate(level_0_compressed, tuple(sh), 'nearest') level_1_resized = x_level_1 level_2_resized = self.stride_level_2(x_level_2) elif self.level == 2: level_0_compressed = self.compress_level_0(x_level_0) sh = torch.tensor(level_0_compressed.shape[-2:])*4 level_0_resized = F.interpolate(level_0_compressed, tuple(sh), 'nearest') level_1_compressed = self.compress_level_1(x_level_1) sh = torch.tensor(level_1_compressed.shape[-2:])*2 level_1_resized = F.interpolate(level_1_compressed, tuple(sh),'nearest') level_2_resized = x_level_2 # 这里得到的resized特征图不直接转换成一通道的weighting map， # 而是先1x1conv降维到8/16，然后concat，然后3x3生成3通道的weighting map # weighting map相当于一个prediction head，所以是conv_bias_softmax，无bn level_0_weight_v = self.weight_level_0(level_0_resized) level_1_weight_v = self.weight_level_1(level_1_resized) level_2_weight_v = self.weight_level_2(level_2_resized) levels_weight_v = torch.cat((level_0_weight_v, level_1_weight_v, level_2_weight_v), 1) levels_weight = self.weight_levels(levels_weight_v) levels_weight = F.softmax(levels_weight, dim=1) # reweighting fused_out_reduced = level_0_resized * levels_weight[:, 0:1, :, :] + \ level_1_resized * levels_weight[:, 1:2, :, :] + \ level_2_resized * levels_weight[:, 2:, :, :] # 3x3的conv，是特征图平滑 out = self.expand(fused_out_reduced) if self.vis: return out, levels_weight, fused_out_reduced.sum(dim=1) else: return out]]></content>
      <tags>
        <tag>目标检测，yolov3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VoVNet]]></title>
    <url>%2F2021%2F01%2F22%2FVoVNet%2F</url>
    <content type="text"><![CDATA[An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection 动机 denseNet dense path：diverse receptive fields heavy memory cost &amp; low efficiency we propose a backbone preserve the benefit of concatenation improve denseNet efficiency VoVNet comprised of One-Shot Aggregation (OSA) apply to one/two stage object detection tasks outperforms denseNet &amp; resNet based ones better small object detection performance 论点 main difference between resNet &amp; denseNet aggregation：summation &amp; concatenation summation would washed out the early features concatenation last as it preserves GPU parallel computation computing utilization is maximized when operand tensor is larger many 1x1 convs for reducing dimension dense connections in intermediate layers are inducing the inefficiencies VoVNet hypothesize that the dense connections are redundant OSA：aggregates intermediate features at once test as object detection backbone：outperforms DenseNet &amp; ResNet with better energy efficiency and speed factors for efficiency FLOPS and model sizes are indirect metrics energy per image and frame per second are more practical MAC： memory accesses cost，$hw(c_i+c_o) + k^2 c_ic_o$ memory usage不止跟参数量有关，还跟特征图尺寸相关 MAC can be minimized when input channel size equals the output FLOPs/s splitting a large convolution operation into several fragmented smaller operations makes GPU computation inefficient as fewer computations are processed in parallel 所以depthwise/bottleneck理论上降低了计算量FLOP，但是从GPU并行的角度efficiency降低，并没有显著提速：cause more sequential computations 以时间为单位的FLOPs才是fair的 方法 hypothesize dense connection makes similar between neighbor layers redundant OSA dense connection：former features concats in every following features one-shot connection：former features concats once in the last feature 最开始跟dense block保持参数一致：一个block里面12个layers，channel20，发现深层特征contributes less，所以换成浅层，5个layers，channel43，发现有涨点：implies that building deep intermediate feature via dense connection is less effective than expected in/out channel数相同 much less MAC： denseNet40：3.7M OSA：5layers，channel43，2.5M 对于higher resolution的detection任务impies more fast and energy efficient GPU efficiency 不需要那好几十个1x1 architecture stem：3个3x3conv downsamp：s2的maxpooling stages：increasing channels enables more rich semantic high-level information，better feature representation deeper：makes more modules in stage3/4 实验 one-stage：refineDet two-stage：Mask-RCNN]]></content>
  </entry>
  <entry>
    <title><![CDATA[GCN]]></title>
    <url>%2F2021%2F01%2F18%2FGCN%2F</url>
    <content type="text"><![CDATA[reference：https://mp.weixin.qq.com/s/SWQHgogAP164Kr082YkF4A 图 $G = (V,E)$：节点 &amp; 边，连通图 &amp; 孤立点 邻接矩阵A：NxN，有向 &amp; 无向 度矩阵D：NxN对角矩阵，每个节点连接的节点 特征矩阵X：NxF，每个1-dim F是每个节点的特征向量 特征学习 可以类比CNN：对其邻域（kernel）内特征进行线性变换（w加权），然后求和，然后激活函数 $H^{k+1} = f(H^{k},A) = \sigma(AH^{k}W^{k})$ H：running updating 特征矩阵，NxFk A：0-1邻接矩阵，NxN W：权重，$F_k$x$F_{k+1}$ 权重所有节点共享 节点的邻接节点可以看做感受野 网络加深，感受野增大：节点的特征融合了更多节点的信息 图卷积 A中没有考虑自己的特征：添加自连接 A = A + I 加法规则对度大的节点，特征会越来越大：归一化 使得邻接矩阵每行和为1：左乘度矩阵的逆 数学实质：求平均 one step further：不单对行做平均，对度较大的邻接节点也做punish GCN网络 实现 weights：in x out，kaiming_uniform_initialize bias：out，zero_initialize activation：relu A x H x W：左乘是系数矩阵乘法 邻接矩阵的结构从输入开始就不变了，和每层的特征矩阵一起作为输入，传入GCN 分类头：最后一层预测Nxn_class的特征向量，提取感兴趣节点F(n_class)，然后softmax，对其分类 归一化 123456789101112131415161718# 对称归一化def normalize_adj(adj): """compute L=D^-0.5 * (A+I) * D^-0.5""" adj += sp.eye(adj.shape[0]) degree = np.array(adj.sum(1)) d_hat = sp.diags(np.power(degree, -0.5).flatten()) norm_adj = d_hat.dot(adj).dot(d_hat) return norm_adj # 均值归一化def normalize_adj(adj): """compute L=D^-1 * (A+I)""" adj += sp.eye(adj.shape[0]) degree = np.array(adj.sum(1)) d_hat = sp.diags(np.power(degree, -1).flatten()) norm_adj = d_hat.dot(adj) return norm_adj 应用场景 [半监督分类GCN]：SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS，提出GCN [skin GCN]：Learning Differential Diagnosis of Skin Conditions with Co-occurrence Supervision using Graph Convolutional Networks，体素，一个单独的基于图的相关性分支，给feature加权 [Graph Attention]：Graph Attention Networks，图注意力网络 Learning Differential Diagnosis of Skin Conditions with Co-occurrence Supervision using Graph Convolutional Networks 动机 皮肤病：发病率高，experts少 differential diagnosis：鉴别诊断，就是从众多疾病类别中跳出正确类别 still challenging：timely and accurate propose a DLS(deep learning system) clinical images multi-label classification 80 conditions，覆盖病种 labels incompleteness：用GCN建模成Co-occurrence supervision，benefit top5 论点 google的DLS 26中疾病 建模成multi-class classification problem：非0即1的多标签表达破坏了类别间的correlation our DLS：GCN-CNN multi-label classification task over 80 conditions incomplete image labels：GCN that characterizes label co-occurrence supervision combine the classification network with the GCN 数据量：136,462 clinical images 精度：test on 12,378 user taken images，top-5 acc 93.6% GCN original application： nodes classification，only a small subset of nodes had their labels available：半监督文本分类问题，只有一部分节点用于训练 the graph structure is contructed from data ML-GCN： multi-label classification task correlation map（图结构）则是通过数据直接建立 图节点是每个类别的semantic embeddings 方法 overview 一个trainable的CNN，将图片转化成feature vector 一个GCN branch：两层图卷积，都是order-1，图结构是基于训练集计算，无向图，encoding的是图像labels之间的dependency，用它 implicitly supervises the classification task 然后两个feature vector相乘，给出最终结果 GCN branch two graph convolutional (GC) layers 一种estimated图结构：build co-occurence graph using only training data node embed semantic meaning to labels 边的值定义有点像类别间的相关性强度：$e_{ij} = 1(\frac{C(i,j)}{C(i)+C(j)} \geq t)$，分子是有两种标签的样本量，分母是各自样本量 一种designed图结构：intial value是基于有经验的专家构建 node representation graph branch的输入 label embedding 用了BioSentVec，一个基于生物医学语料库训练的word bag GCN randomly initialize GCN-0：dim 700 GCN-1：dim 1024 GCN-2：dim 2048 最终得到(cls,2048)的node features cls branch input：downsized to 448x448 resnet101：执行到FC-2048，作为image features 先训练300 epochs，lr 0.1，step decay GCN-CNN 先预训练resnet backbone， 然后整体一起训练300 epochs，lr 0.0003， image feature和node features通过dot product融合，得到(cls, )的cls vec， 实验 图结构不能random initialization，会使结果变差 基于数据集估计的graph initialization有显著提升 基于专家设计的graph initialization有进一步提升，但是不明显，考虑到标注工作繁重不太推荐 SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS reference http://tkipf.github.io/graph-convolutional-networks/，官方博客 https://zhuanlan.zhihu.com/p/35630785，知乎笔记 论点 场景 semi-supervised learning on graph-structured data 比如：在一个citation network，classifying nodes (such as documents)，labels are only available for a small subset of nodes，任务的目标是对大部分未标记的节点预测类别 previous approach Standard Approach loss由两部分组成：单个节点的fitting error，和相邻节点的distance error 基于一个假设：相邻节点间的label相似 限制了模型的表达能力 Embedding-based Approach 分两步进行：先学习节点的embedding，再基于embedding训练分类器 不end-to-end，两个task分别执行，不能保证学到的embedding是适合第二个任务的 思路 train on a supervised target for nodes with labels 然后通过图的连通性，trainable adjacency matrix，传递梯度给unlabeled nodes 使得全图得到监督信息 contributions introduce a layer-wise propagation rule，使得神经网络能够operate on graph，实现end-to-end的图结构分类器 use this graph-based neural network model，训练一个semi-supervised classification of nodes的任务 方法 fast approximate convolutions on graphs given： layer input：$H^l$ layer output：$H^{l+1}$ kernel pattern：$A$，在卷积里面是fixed kxk 方格，在图里面就是自由度更高的邻接矩阵 kernel weights：$W$ general layer form：$H^{l+1}=f(H^l,A)$ inspiration：卷积其实是一种特殊的图，每个grid看作一个节点，每个节点都加上其邻居节点的信息，也就是： H^{l+1}=\sigma(AH^lW) W是在对grids加权 A是在对每个grids加上他的邻接节点 details in practice 自环：保留自身节点信息，$\hat A=A+I$ 正则化：stabilize the scale，$H^{l+1}=\sigma(\hat D^{-\frac{1}{2}}\hat A\hat D^{-\frac{1}{2}}H^lW)$ 一个实验：只利用图的邻接矩阵，就能够学得效果不错 semi-supervised node classification 思路就是在所有有标签节点上计算交叉熵loss 模型结构 input：X，(b,N,D) 两层图卷积 GCN1-relu：hidden F，(b,N,F) GCN2-softmax：output Z，(b,N,cls) 计算交叉熵 code torch/keras/tf官方都有： https://github.com/tkipf/gcn，论文里给的tf这个链接 torch和keras的readme里面有说明，initialization scheme, dropout scheme, and dataset splits和tf版本不同，不是用来复现论文 python setup.py bdist_wheel 数据集：Cora dataset，是一个图数据集，用于分类任务，数据集介绍https://blog.csdn.net/yeziand01/article/details/93374216 cora.content是所有论文的独自的信息，总共2708个样本，每一行都是论文编号+词向量1433-dim+论文类别 cora.cites是论文之间的引用记录，A to B的reflect pair，5429行，用于创建邻接矩阵]]></content>
      <tags>
        <tag>图卷积，graph-conv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[transformers]]></title>
    <url>%2F2021%2F01%2F18%2Ftransformers%2F</url>
    <content type="text"><![CDATA[startupreference1：https://mp.weixin.qq.com/s/Rm899vLhmZ5eCjuy6mW_HA reference2：https://zhuanlan.zhihu.com/p/308301901 NLP &amp; RNN 文本涉及上下文关系 RNN时序串行，建立前后关系 缺点：对超长依赖关系失效，不好并行化 NLP &amp; CNN 文本是1维时间序列 1D CNN，并行计算 缺点：CNN擅长局部信息，卷积核尺寸和长距离依赖的balance NLP &amp; transformer 对流入的每个单词，建立其对词库的权重映射，权重代表attention 自注意力机制 建立长距离依赖 put in CV 插入类似的自注意力层 完全抛弃卷积层，使用Transformers RNN &amp; LSTM &amp; GRU cell 标准要素：输入x、输出y、隐层状态h RNN RNN cell每次接收一个当前输入$x_t$，和前一步的隐层输出$h_{t-1}$，然后产生一个新的隐层状态$h_t$，也是当前的输出$y_t$ formulation：$y_t, h_t = f(x_t, h_{t-1})$ same parameters for each time step：同一个cell每个time step的权重共享 一个问题：梯度消失/爆炸 考虑hidden states’ chain的简化形式：$h_t = \theta^t h_0$，一个sequence forward下去就是same weights multiplied over and over again 另外tanh也是会让神经元梯度消失/爆炸 LSTM key ingredient cell：增加了一条cell state workflow，优化梯度流 gate：通过门结构删选携带信息，优化长距离关联 可以看到LSTM的循环状态有两个：细胞状态$c_t$和隐层状态$h_t$，输出的$y_t$仍旧是$h_t$ GRU LSTM的变体，仍旧是门结构，比LSTM结构简单，参数量小，据说更好训练 papers [一个列了很多论文的主页] https://github.com/dk-liang/Awesome-Visual-Transformer [经典考古] ​ * [Seq2Seq 2014] Sequence to Sequence Learning with Neural Networks，Google，最早的encoder-decoder stacking LSTM用于机翻 ​ * [self-attention/Transformer 2017] Transformer: Attention Is All You Need，Google， ​ * [bert 2019] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding，Google，NLP，输入single sentence/patched sentences，用Transformer encoder提取bidirectional cross sentence representation，用输出的第一个logit进行分类 [综述] ​ * [综述2020] Efficient Transformers: A Survey，Google， ​ * [综述2021] Transformers in Vision: A Survey，迪拜， ​ * [综述2021] A Survey on Visual Transformer，华为， [classification] ​ * [ViT 2020] AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE，Google，分类任务，用transformer的encoder替换CNN再加分类头，每个feature patch作为一个input embedding，channel dim是vector dim，可以看到跟bert基本一样，就是input sequence换成patch，后续基于它的提升有DeiT、LV-ViT ​ * [BotNet 2021] Bottleneck Transformers for Visual Recognition，Google，将CNN backbone最后几个stage替换成MSA ​ * [CvT 2021] CvT: Introducing Convolutions to Vision Transformers，微软， ​ * [Swin 2021] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows，微软 ​ * [PVT2021] Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions，跟swin一样也是multi-scale features [detection] ​ * [DeTR 2020] DeTR: End-to-End Object Detection with Transformers，Facebook，目标检测，CNN+transformer(en-de)+预测头，每个feature pixel作为一个input embedding，channel dim是vector dim ​ * [Deformable DETR] ​ * [Anchor DETR] ​ * 详见《det-transformers》 [segmentation] ​ * [SETR] Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers，复旦，水，感觉就是把FCN的back换成transformer [Unet+Transformer]： ​ * [UNETR 2021] UNETR: Transformers for 3D Medical Image Segmentation，英伟达，直接使用transformer encoder做unet encoder ​ * [TransUNet 2021] TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation，encoder stream里面加transformer block ​ * [TransFuse 2021] TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation，大学，CNN feature和Transformer feature进行bifusion ​ * 详见《seg-transformers》 Sequence to Sequence [a keras tutorial][https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html] general case extract the information of the entire input sequence then start generate the output sequence seq2seq model workflow a (stacking of) RNN layer acts as encoder processes the input sequence returns its own internal state：不要RNN的outputs，只要internal states encoder编码得到的东西叫Context Vector a (stacking of) RNN layer acts as decoder given previous characters of the target sequence it is trained to predict the next characters of the target sequence teacher forcing： 输入是target sequence，训练目标是使模型输出offset by one timestep的target sequence 也可以不teacher forcing：直接把预测作为next step的输入 Context Vector的同质性：每个step，decoder都读取一样的Context Vector作为initial_state when inference 第一步获取input sequence的state vectors repeat 给decoder输入input states和out sequence(begin with a 起始符) 从prediction中拿到next character append the character to the output sequence until：得到end character / hit the character limit implementation https://github.com/AmberzzZZ/transformer/blob/master/seq2seq.py one step further 改进方向 bi-directional RNN：粗暴反转序列，有效涨点 attention：本质是将encoder的输出Context Vector加权 ConvS2S：还没看 主要都是针对RNN的缺陷提出 动机 present a general end-to-end sequence learning approach multi-layered LSTMs encode the input seq to a fix-dim vector decode the target seq from the fix-dim vector LSTM did not have difficulty on long sentences reversing the order of the words improved performance 方法 standard RNN given a sequence $(x_1, x_2, …, x_T)$ iterating： h_t = sigm(W^{hx} x_t + W^{hh}h_{t-1})\\ y_t = W^{yh} h_t 如果输入、输出的长度事先已知且固定，一个RNN网络就能建模seq2seq model了 如果输入、输出的长度不同、并且服从一些更复杂的关系？就得用两个RNN网络，一个将input seq映射成fixed-sized vector，另一个将vector映射成output seq，but long-term-dependency issue LSTM LSTM是始终带着全部seq的信息的，如上图那样 our actual model use two LSTMs：encoder-decoder能够增加参数量 an LSTM with four layers：deeper input sequence倒序：真正的句首更接近trans的句首，makes it easy for SGD to establish communication training details LSTM：4 layers，1000 cells word-embedding：1000-dim，(input vocab 160,000, output vocab 80,000) naive softmax uniform initialization：(-0.08, 0.08) SGD，lr=0.7，half by every half epoch，total 7.5 epochs gradient norm [10, 25] all sentences in a minibatch are roughly of the same length Transformer: Attention Is All You Need 动机 sequence2sequence models encoder + decoder RNN / CNN + an attention path we propose Transformer base solely on attention mechanisms more parallelizable and less training time 论点 sequence modeling 主流：RNN，LSTM，gated align the positions to computing time steps sequential本质阻碍并行化 Attention mechanisms acts as a integral part in previous work used in conjunction with the RNN 为了并行化 some methods use CNN as basic building blocks difficult to learn dependencies between distant positions we propose Transformer rely entirely on an attention mechanism draw global dependencies self-attention relating different positions of a single sequence to generate a overall representation of the sequence 方法 encoder-decoder encoder：doc2emb given an input sequence of symbol representation $(x_1, x_2, …, x_n)$ map to a sequence of continuous representations $(z_1, z_2, …, z_n)$，(embeddings) decoder：hidden layers given embeddings z generate an output sequence $(y_1, y_2, …, y_m)$ one element at a time the previous generated symbols are served as additional input when computing the current time step Transformer Architecture Transformer use for both encoder and decoder stacked self-attention and point-wise fully-connected layers encoder N=6 identical layers each layer has 2 sub-layers multi-head self-attention mechanism postision-wise fully connected layer residual for two sub-layers independently add &amp; layer norm d=512 decoder N=6 identical layers 3 sub-layers [new] masked multi-head self-attention：combine了先验知识，output embedding只能基于在它之前的time-step的embedding计算 multi-head self-attention mechanism postision-wise fully connected layer residual attention reference：https://bbs.cvmart.net/articles/4032 step1：project embedding to query-key-value pairs $Q = W_Q^{dd} A^{dN}$ $K = W_K^{dd} A^{dN}$ $V = W_V^{dd} A^{dN}$ step2：scaled dot-product attention $A^{N*N}=softmax(K^TQ/\sqrt{d})$ $B^{dN} = V^{dN}A^{N*N}$ multi-head attention 以上的step1&amp;step2操作performs a single attention function 事实上我们可以用多组projection得到多组$\{Q,K,V\}^h$，in parallel地执行attention运算，得到多组$\{B^{d*N}\}^h$ concat &amp; project concat in d-dim：$B\in R^{(dh)N}$ linear project：$out = W^{d(dh)} B$ h=8 $d_{in}/h=64$：embedding的dim $d_{out}=64$：query-key-value的dim positional encoding 数学本质是一个hand-crafted的映射矩阵$W^P$和one-hot的编码向量$p$： \left[ \begin{array}{ccc} a\\ e \end{array} \right ] = [W^I, W^P] \left[ \begin{array}{ccc} x\\ p \end{array} \right ] 用PE表示e pos是sequence x上的position 2i和2i+1是embedding a上的idx point-wise feed-forward network fc-ReLU-fc dim_fc=2048 dim_in &amp; dim_out = 512 运行过程 encoder是可以并行计算的 输入是sequence embedding和positional embedding：$A\in R^{d*N}$ 经过repeated blocks 输出是另外一个sequence：$B\in R^{d*N}$ self-attention：Q、K、V是一个东西 encoder的本质就是在解析自注意力： 并行的全局两两比较，一步到位 RNN要by step CNN要stack layers decoder是在训练阶段是可以并行的，在inference阶段by step 输入是encoder的输出和上一个time-step decoder的输出embedding 输出是当前time-step对应position的输出词的概率 第一个attention layer是out embedding的self-attention：要实现像RNN一样依次解码出来，每个time step要用到上一个位置的输出作为输入——masking given输入sequence是\ I have a cat，5个元素 那么mask就是$R^{5*5}$的下三角矩阵 输入embedding经过transformation变成Q、K、V三个矩阵 仍旧是$A=K^TQ$计算attention 这里有一些attention是非法的：位置靠前的query只能用到比他位置更靠前的query，因此要乘上mask矩阵：$A=M A$ softmax：$A=softmax(A)$ scale：$B = VA$ concat &amp; projection 第二个attention layer是in &amp; out sequence的注意力，其key和value来自encoder，query来自上一个decoder block的输出 why self-attention 衡量维度 total computational complexity per layer amount of computation that can be parallelized path-length between long-range dependencies given input sequence with length N &amp; dim $d_{in}$，output sequence with dim $d_{out}$ RNN need N sequencial operations of $W\in R^{d_{in} * d_{out}}$ CNN need N/k stacking layers of $d_{in}d_{out}$ sequence operations of $W\in R^{kk}$，generally是RNN的k倍 training optimizer：$Adam(lr, \beta_1=0.9, \beta_2=0.98, \epsilon=10^{-9})$ lrschedule：warmup by 4000 steps，then decay dropout residual dropout：就是stochastic depth dropout to the sum of embeddings &amp; PE for both encoder and decoder drop_rate = 0.1 label smoothing：smooth_factor = 0.1 实验 A：vary the number of attention heads，发现多了少了都hurts B：reduce the dim of attention key，发现hurts C &amp; D：大模型+dropout helps E：learnable &amp; sincos PE：nearly identical 最后是big model的参数 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 动机 BERT：Bidirectional Encoder Representations from Transformers Bidirectional Encoder Representations Transformers workflow pretrain bidirectional representations from unlabeled text tune with one additional output layer to obtain the model SOTA GLUE score 80.5% 论点 pretraining is effective in NLP tasks feature-based method：use task-specfic architectures，仅使用pretrained model的特征 fine-tuining method：直接fine-tune预训练模型 两种方法在预训练阶段训练目标一致：use unidirectional language models to learn general language representations reduce the need for many heavily-engineered task- specific architectures current methods’ limitations unidirectional： limit the choice of architectures 事实上token的上下文都很重要，不能只看上文 简单的concat两个independent的L2R和R2L模型（biRNN） independent shallow concat BERT masked language model：在一个sequence中预测被遮挡的词 next sentence prediction：trains text-pair representations 方法 two steps pre-training unlabeled data different pretraining tasks fine-tuning labeled data of the downstream tasks fine-tune all the params 两个阶段的模型，只有输出层不同 例如问答模型 pretraining阶段，输入是两个sentence，输入的起始有一个CLS symbol，两个句子的分隔有一个SEP symbol fine-tuning阶段，输入分别是问和答，【输出是啥？】 architecture multi-layer bidirectional Transformer encoder number of transfomer blocks L hidden size H number of self-attention heads A FFN dim 4H Bert base：L=12，H=768，A=12 Bert large：L=24，H=1024，A=16 input/output representations a single sentence / two packed up sentence： 拼接的sentence用特殊token SEP衔接 segment embedding：同时add a learned embedding to every token indicating who it belongs use WordPiece embeddings with 30000 token vocabulary 输入sequence的第一个token永远是一个特殊符号CLS，它对应的final state输出作为sentence整体的representation，用于分类任务 overall网络的input representation是通过将token embeddings拼接上上特殊符号，加上SE和PE得到 pre-training two unsupervised tasks Masked LM (MLM) mask some percentage of the input tokens at random：15% 80%的概率用MASK token替换 10%的概率用random token替换 10%的概率unchanged then predict those masked tokens the final hidden states corresponding to the masked tokens are fed into a softmax 相比较于传统的left2right/right2left/concat模型 既有前文又有后文 只预测masked token，而不是全句预测 Next Sentence Prediction (NSP) 对于relationship between sentences： 例如question&amp;answer，句子推断 not direatly captured by language modeling，模型直观学习的是token relationship binarized next sentence prediction task 选取sentence A&amp;B： 50%的概率是真的上下文（IsNext） 50%的概率是random（NotNext） 构成了一个二分类问题：仍旧用CLS token对应的hidden state C来预测 fine-tuning BERT兼容many downstream tasks：single text or text pairs 直接组好输入，end-to-end fine-tuning就行 输出还是用CLS token对应的hidden state C来预测，接分类头 A Survey on Visual Transformer 动机 provide a comprehensive overview of the recent advances in visual transformers discuss the potential directions for further improvement develop timeline 按照应用场景分类 backbone：分类 high/mid-level vision：通常是语义相关的，检测/分割/姿态估计 low-level vision：对图像本身进行操作，超分/图像生成，目前应用较少 video processing revisiting transformer key-concepts：sentence、embedding、positional encoding、encoder、decoder、self-attention layer、encoder-decoder attention layer、multi-head attention、feed-forward neural network self-attention layer input vector is transformed into 3 vectors input vector is embedding+PE(pos,i)：pos是word在sequence中的位置，i是PE-element在embedding vec中的位置 query vec q key vec k value vec v $d_q = d_k = d_v = d_{model} = 512$ then calculate：$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$ encoder-decoder attention layer K和V是从encoder中拿到 Q是从前一层拿到 计算是相似的 multi-head attention 一个attention是一个softmax，对应了一对强相关，同时抑制了其他word的相关性 考虑一个词往往与几个词强相关，这就需要多个attention multi-head：different QKV matrices are used for different heads given a input vector，the number of heads h 先产生h个 pairs $d_q=d_k=d_v=d_{model}/h=64$ 这h个pair，分别计算attention vector，得到h个[b,d]的context vector concat along-d-axis and linear projection to final [b,d] vector residual &amp; layer-norm：layer-norm在residual-add以后 feed-forward network fc-GeLU-fc $d_h=2048$ final-layer in decoder dense+softmax $d_{words}=$ number of words in the vocabulary when applied in CV tasks most transformers adopt the original transformer’s encoder module used as a feature selector 相比较于CNN，能够capture long-distance characteristics，derive global information 相比较于RNN，能够并行计算 计算量 首先是三个线性层：线性时间复杂度O(n)，计算量与$d_{model}$成正比 然后是self-attention层：QKV矩阵乘法运算，平方时间复杂度O(n^2) multi-head的话，还有一个线性层：平方时间复杂度O(n^2) revisiting transformers for NLP 最早期的RNN + attention：rnn的sequential本质影响了长距离/并行化/大模型 transformer的solely attention结构：解决以上问题，促进了large pre-trained models (PTMs) for NLP BERT and its variants are a series of PTMs built on the multi-layer transformer encoder architecture pre-trained Masked language modeling Next sentence prediction fine-tuned add an output layer Generative Pre-trained Transformer models (GPT) are another type of PTMs based on the transformer decoder architecture masked self-attention mechanisms pre-trained 与BERT最大的不同是有向性 visual transformer 【category1】: backbone for image classification transformer的输入是tokens，在NLP里是embedding形式的分词序列，在CV里就是representing a certain semantic concept的visual token visual token可以来自CNN的feature 也可以直接来自image的小patch purely use transformer来做image classification任务的模型有iGPT、ViT、DeiT iGPT pretraining stage + finetuning stage pre-training stage self-supervised：自监督，所以结果较差 given an unlabeled dataset train the model by minimizing the -log(density)，感觉是在force光栅排序正确 fine-tuning stage average pool + fc + softmax jointly train with L_gen &amp; L_CE ViT pre-trained on large datasets standard transformer’s encoder + MLP head treats all patches equally 有一个类似BERT class token的东西 从训练的角度，gather knowledge of the entire class inference的时候，只拿了这第一个logit用来做预测 fine-tuning 换一个zero-initialized的MLP head use higher resolution &amp; 插值pe DeiT Data-efficient image transformer better performance with a more cautious training strategy and a token-based distillation 【category2】: High/Mid-level Vision 【category3】: Low-level Vision 【category4】: Video Processing efficient transformer：瘦身&amp;加速 Pruning and Decomposition Knowledge Distillation Quantization Compact Architecture Design ViT: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE 动机 attention in vision either in conjunction with CNN or replace certain part of a CNN overall都还是CNN-based use a pure transformer to sequence of image patches verified on image classification tasks in supervised fashion 论点 transformer lack some inductive biases inherent to CNNs，所以在insufficient data上not generalize well however large scale training trumps inductive bias，大数据集上ViT更好 naive application of self-attention 建立pixel之间的两两关联：计算量太大了 需要approximation：local/改变size we use transformer wih global self-attention to full-sized images 方法 input 1D-embedding sequence 将image $x\in R^{HWC}$ 展开成patches $\{x_p \in R^{P^2C}\}$ thus sequence length $N=HW/P^2$ patch embedding： use a trainable linear projection fixed dimension size through-all position embedding： add to patch embedding standard learnable 1D position embedding prepended embedding： 前置的learnable embedding $x_{class}$ similar to BERT’s class token 以上三个embedding组合起来，作为输入sequence transformer encoder follow the original Transformer 交替的MSA和MLP layer norm LN residual GELU hybrid architecture input sequence也可以来源于CNN的feature maps patch size可以是1x1 classification head attached to $z_L^0$：是class token用来做预测 pre-training的时候是MLP fine-tuning的时候换一个zero-initialized的single linear layer workflow typically先pre-train on large datasets 再fine-tune to downstream tasks fine-tune的时候替换一个zero-initialized的新线性分类头 when feeding images with higher resolution keep the patch size results in larger sequence length 这时候pre-trained PE就no longer meaningful了 we therefore perform 2D interpolation基于它在原图上的位置 training details Adam：$\beta_1=0.9，\beta_2=0.999$ batch size 4096 high weight decay 0.1 linear lr warmup &amp; decay fine-tuning details SGDM cosine LR no weight decay 【？？？？】average 0.9999 win Transformer: Hierarchical Vision Transformer using Shifted Windows 动机 use Transformer as visual tasks’ backbone challenges of Transformer in vision domain large variations of scales of the visual entities high resolution of pixels we propose hierarchical Transformer shifted windows self-attention in local windows cross-window connection verified on classification：ImageNet top1 acc 86.4 detection：COCO box-MAP 58.7 segmentation：ADE20K this paper主要介绍分类，检测是以swin作为backbone，用MaskRCNN等二阶段架构来训练的，分割是以swin作为backbone，用UperNet去训练的，具体模型配置official repo的readme里面有详细列表 论点 when transfer Transformer’s high performance in NLP domain to CV domain differences between the two modalities scale：NLP里面，word tokens serves as the basic element，但是CV里面，patch的形态大小都是可变的，previous methods里面，都是统一设定固定大小的patch token resolution：主要问题就是self-attention的计算复杂度，是image size的平方 we propose Swin Transformer hierarchial feature maps linear computatoinal complexity to image size hierarchical start from small patches merge in deeper layers 所以对不同尺度的特征patch进行了融合 linear complexity compute self-attention locally in each window 每个window的number of patches是设定好的，window数是与image size成正比的 所以是线性 shifted window approach 跨层的window shift，建立起相邻window间的桥梁 【QUESTION】all query patches within a window share the same key set previous attemptations of Transformer self-attention based backbone architectures 将部分/全部conv layers替换成self-attention 模型主体架构还是ResNet slightly better acc larger latency caused by self-att self-attention complement CNNs 作为additional block，给到backbone/head，提供长距离信息 有些检测/分割网络也开始用了transformer的encoder-decoder结构 transformer-based vision backbones 主要就是ViT及其衍生品 ViT requires large-scale training sets DeiT introduces training strategies 但是还存在high resolution计算量的问题 方法 overview Swin-T：tiny version 第一步是patch partition： 将RGB图切成non-overlapping patches patches：token，basic element feature input dim：with patch size 4x4，dim=4x4x3=48 然后是linear embedding layer 将raw feature re-projection到指定维度 指定维度C：default=96 接下来是Swin Transformer blocks the number of tokens maintain patch merging layers负责reduce the number of tokens 第一个patch merging layer concat 所有2x2的neighbor patches：4C-dim vec each 然后用了一个线性层re-projection number of tokens（resolution）：（H/4*W/4）/4 = （H/8*W/8），跟常规的CNN一样变化的 token dims：2C 后面接上一个Transformer blocks 合起来叫stage2（stage3、stage4） Swin Transformer blocks 跟原始的Transformer block比，就是把原始的MSA替换成了window-based的MSA 原始的attention：global computation leads to quadratic complexity window-based attention： attention的计算只发生在每个window内部 non-overlapping partition 很显然lacks connections across windows shifted window partitioning in successive blocks 两个attention block 第一个用常规的window partitioning strategy：从左上角开始，take M=4，window size 4x4（一个window里面包含4x4个patch） 第二层的window，基于前一层，各平移M/2 introduce connections between neighbor non-overlapping windows in the previous layer efficient computation shifted window会导致window尺寸不一致，不利于并行计算 relative position bias 我们在MxM的window内部计算local attention：也就是input sequence的time-step是$M^2$ Q、K、V $\in R ^ {M^2 d}$ $Attention(Q,K,V)=Softmax(QK^T/\sqrt{d}+B)V$ 这个B作为local的position bias，在二维上，在每个轴上的变化范围[-M+1,M-1] we parameterized a smaller-sized bias matrix $\hat B\in R ^{(2M-1)*(2M-1)}$ values in $B \in R ^ {M^2*M^2}$ are taken from $\hat B$ the learnt relative position bias可以用来initialize fine-tuned model Architecture variants base model：Swin-B，参数量对标ViT-B Swin-T：0.25x，对标ResNet-50 (DeiT-S) Swin-S：0.5x，对标ResNet-101 Swin-L：2x window size：M=7 query dim：d=32，（每个stage的input sequence dim逐渐x2，heads num逐渐x2） MLP：expansion ratio=4 channel number C：第一个stage的embdding dim，（后续逐渐x2） hypers： drop_rate：0.0 drop_path_rate：0.1 acc official repo: https://github.com/microsoft/Swin-Transformer/blob/main/get_started.md keras官方也出了一版：https://github.com/keras-team/keras-io/blob/master/examples/vision/swin_transformers.py model zoo model | resolution | C | num_layers | num_heads | window_size Swin-T | 224 | 96 | {2,2,6,2} | {3,6,12,24} | 7 Swin-S | 224 | 96 | {2,2,18,2} | {3,6,12,24} | 7 Swin-B | 224/384 | 128 | {2,2,18,2} | {4,8,16,32} | 7/12 Swin-L | 224/384 | 192 | {2,2,18,2} | {6,12,24,48} | 7/12 models/build.py SwinTransformer &amp; SwinMLP：前者就是论文里的，basic block是transformer的MSA加上MLP layers，后者是没用MSA，就用MLP来建模相邻windows之间的global relationship的，用的conv1d。 DETR: End-to-End Object Detection with Transformers 动机 new task formulation：a direct set prediction problem main gradients a set-based global loss a transformer en-de architecture remove the hand-designed componets like nms &amp; anchor acc &amp; run-time on par with Faster R-CNN on COCO significantly better performance on large objects lower performances on small objects 论点 modern detectors run object detection in an indirect way 基于格子/anchor/proposals进行回归和分类 算法性能受制于nms机制、anchor设计、target-anchor的匹配机制 end-to-end approach transformer的self-attention机制，explicitly model all pairwise interactions between elements：内含了去重（nms）的能力 bipartite matching：set loss function，将预测和gt的box一一匹配，run in parallel DETR does not require any customized layers, thus can be reproduced easily expand to segmentation task：a simple segmentation head trained on top of a pre-trained DETR set prediction：to predict a set of bounding boxes and the categories for each basic：multilabel classification detection task has near-duplicates issues set prediction是postprocessing-free的，它的global inference schemes能够avoid redundancy usual loss：bipartite match object detection set-based loss modern detectors use non-unique assignment rules together with NMS bipartite matching是target和pred一一对应 方法 overall three main components a CNN backbone an encoder-decoder transformer a simple FFN backbone conventional r50 input：$[H_0, W_0, 3]$ output：$[H,W,C], H=\frac{H_0}{32}, W=\frac{W_0}{32}, C=2048$ transformer encoder reduce channel dim to $d$：1x1 conv，$d=512$ collapse the spatial dimensions：feature sequence [d, HW]，每个spatial pixel作为一个feature fixed positional encodings： added to the input of each attention layer 【QUESTION】加在K和Q上还是embedding上？ transformer decoder 输入N个dim=d的embedding 叫object queries：表示我们预测固定值N个目标 因为decoder也是permutation-invariant的（因为all shared），所以要输入N个不一样的embedding learnt positional encodings add them to the input of each attention layer decodes the N objects in parallel prediction FFN 3 layer，ReLU， box prediction：normalized center coords &amp; height &amp; width class prediction： an additional class label $\varnothing$ 表示no object auxiliary losses each decoder layer后面都接一个FFN prediction和Hungarian loss shared FFN an additional shared LN to norm the inputs of FFN three components of the loss class loss：CE loss box loss GIOU loss L1 loss technical details AdamW： initial transformer lr=10e-4 initial backbone lr=10e-5 weight decay=10e-4 Xavier init imagenet-pretrained resnet weights with frozen batchnorm layers：r50 &amp; r101，DETR &amp; DETR-R101 a variant： increase feature resolution version remove stage5’s stride and add a dilation DETR-DC5 &amp; DETR-DC5-R101 improve performance for small objects overall 2x computation increase augmentation resize input random crop：with 0.5 prob then resize transformer default dropout 0.1 lr schedule 300 epochs drop by factor 10 after 200 epochs 4 images per GPU，total batch 64 for segmentation task：全景分割 给decoder outputs加mask head compute multi-head attention among decoder box predictions encoder outputs generate M attention heatmaps per object add a FPN styled CNN to recover resolution pixel-wise argmax UNETR: Transformers for 3D Medical Image Segmentation 动机 unet结构用于医学分割 encoder learns global context decoder utilize the representations to predict the semanic ouputs the locality of CNN limits long-range spatial dependency our method use a pure transformer as the encoder learn sequence representations of the input volume global multi-scale encoder directly connects to decoder with skip connections 论点 unet结构 encoder用来提取全图特征 decoder用来recover skip connections用来补充spatial information that is lost during downsampling localized receptive fields： disadvantage in capturing multi-scale contextual information 如不同尺寸的脑肿瘤 缓和手段：atrous convs，still limited transformer self-attention mechanism in NLP highlight the important features of word sequences learn its long-range dependencies in ViT an image is represented as a patch embedding sequence our method formulation 1D seq2seq problem use embedded patches the first completely transformer-based encoder other unet- transformer methods 2D (ours 3D) employ only in the bottleneck (ours pure transformer) CNN &amp; transformer in separate streams and fuse 方法 overview transformer encoder input：1D sequence of input embeddings given 3D volume $x \in R^{HWDC}$ divide into flattened uniform non-overlapping patches $x\in R^{LCN^3}$ $L=HWD/N^3$：the sequence length $N^3$：patch dimension linear projection to K-dim $E \in R^{LCK}$：remain constant through transformer 1D learnable positional embedding $E_{pos} \in R^LD$ 12 self-att blocks：MSA + MLP decoder &amp;skip connections 选取encoder第{3,6,9,12}个block的输出 reshape back to 3D volume $[\frac{H}{N},\frac{W}{N},\frac{D}{N},C]$ consecutive 3x3x3 conv+BN+ReLU bottleneck deconv by 2 to increase resolution then concat with the previous resized feature then jointly consecutive conv then upsample with deconv… concat到原图resolution以后，consecutive conv以后，再1x1x1 conv+softmax loss dice loss dice：for each class channel，计算dice，然后求类平均 1-dice ce loss for each pixel，求bce，然后求所有pixel的平均]]></content>
      <tags>
        <tag>transformer, self-attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pre-training & self-training]]></title>
    <url>%2F2021%2F01%2F17%2Fpre-training-self-training%2F</url>
    <content type="text"><![CDATA[[pre-training] Rethinking ImageNet Pre-training，He Kaiming，imageNet pre-training并没有真正helps acc，只是speedup，random initialization能够reach no worse的结果，前提是数据充足增强够猛，对小门小户还是没啥用，我们希望speedup [pre-training &amp; self-training] Rethinking Pre-training and Self-training，Google Brain，提出task-specific的pseudo label要比pre-training中搞出来的各种标签要好，前提还是堆数据，对小门小户没啥用，low-data下还是pre-train保平安 总体上都是针对跨任务下，imageNet pre-training意义的探讨， 分类问题还是可以继续pretrained kaiming这个只是fact，没有现实指导意义 google这个one step further，提出了self-training在现实条件中可以一试 Rethinking Pre-training and Self-training 动机 given fact：ImageNet pre-training has limited impact on COCO object detection investigate self-training to utilize the additional data 论点 common practice pre-training supervised pre-training 首先要求数据有标签 pre-train the backbone on ImageNet as a classification task 弱监督学习 with pseudo/noisy label kaiming：Exploring the limits of weakly supervised pretraining self-supervised pre-training 无标签的海量数据 构造学习目标：autoencoder，contrastive，… https://zhuanlan.zhihu.com/p/108906502 self-training paradigm on COCO train an object detection model on COCO generate pseudo labels on ImageNet both labeled data are combined to train a new model 基本基于noisy student的方法 observations with stronger data augmentation, pre-training hurts the accuracy, but helps in self-training both supervised and self-supervised pre-training methods fails the benefit of pre-training does not cancel out the gain by self-training flexible about unlabeled data sources, model architectures and computer vision tasks 方法 data augmentation vary the strength of data augmentation as 4 levels pre-training efficientNet-B7 AutoAugment weights &amp; noisy student weights self-training noisy student scheme 实验发现self-training with this standard loss function can be unstable implement a loss normalization technique experimental settings object detection COCO dataset for supervised learning unlabeled ImageNet and OpenImages dataset for self-training：score thresh 0.5 to generate pesudo labels retinaNet &amp; spineNet batch：half supervised half pesudo semantic segmentation PASCAL VOC 2012 for supervised learning augmented PASCAL &amp; COCO &amp; ImageNet for self-training：score thresh 0.5 to generate pesudo masks &amp; multi-scale NAS-FPN 实验 pre-training Pre-training hurts performance when stronger data augmentation is used：因为会sharpen数据差异？ More labeled data diminishes the value of pre-training：通常我们的实验数据fraction都比较小的相对imageNet，所以理论上不会harm？ self-supervised pre-training也会一样harm，在augment加强的时候 self-training Self-training helps in high data/strong augmentation regimes, even when pre-training hurts：不同的augment level，self-training对最终结果都有加成 Self-training works across dataset sizes and is additive to pre-training：不同的数据量，也都有加成，但是low data regime下enjoys the biggest gain discussion weak performance of pre-training is that pre-training is not aware of the task of interest and can fail to adapt jointly training also helps：address the mismatch between two dataset noisy labeling is worse than targeted pseudo labeling 总体结论：小样本量的时候，pre-training还是有加成的，再加上self-training进一步提升，样本多的时候就直接self-training Rethinking ImageNet Pre-training 动机 thinking random initialization &amp; pre-training ImageNet pre-training speed up but not necessarily improving random initialization can achieve no worse result robust to data size, models, tasks and metrics rethink current paradigm of ‘pre- training and fine-tuning’ 论点 no fundamental obstacle preventing us from training from scratch if use normalization techniques appropriately if train sufficiently long pre-training speed up when fine-tuning on small dataset new hyper-parameters must be selected to avoid overfitting localization-sensitive task benefits limited from pre-training aimed at communities that don’t have enough data or computational resources 方法 normalization form normalized parameter initialization normalization layers BN layers makes training from scratch difficult small batch size degrade the acc of BN fine-tuning可以freeze BN alternatives GN：对batch size不敏感 syncBN with appropriately normalized initialization可以train from scratch VGG这种不用BN层的 convergence pre-training model has learned low-level features that do not need to be re-learned during random-initial training need more iterations to learn both low-level and semantic features 实验 investigate maskRCNN 替换BN：GN/sync-BN learning rate： training longer for the first (large) learning rate is useful but training for longer on small learning rates often leads to overfitting 10k COCO往上，train from scratch results能够catch up pretraining results，只要训的够久 1k和3.5k的COCO，converges show no worse，但是在验证集上差一些：strong overfitting due to lack of data PASCAL的结果也差一点，因为instance和category都更少，not directly comparable to the same number of COCO images：fewer instances and categories has a similar negative impact as insufficient training data]]></content>
  </entry>
  <entry>
    <title><![CDATA[long-tailed]]></title>
    <url>%2F2021%2F01%2F11%2Flong-tailed%2F</url>
    <content type="text"><![CDATA[[bag of tricks] Bag of Tricks for Long-Tailed Visual Recognition with Deep Convolutional Neural Networks：结论就是两阶段，input mixup + CAM-based DRS + muted mixup fine-tuning组合使用最好 [balanced-meta softmax] Balanced Meta-Softmax for Long-Tailed Visual Recognition：商汤 [eql] Equalization Loss for Long-Tailed Object Recognition [eql2] Equalization Loss v2: A New Gradient Balance Approach for Long-tailed Object Detection [Class Rectification Loss] Imbalanced Deep Learning by Minority Class Incremental Rectification：提出CRL使得模型能够识别分布稀疏的小类们的边界，以此避免大类主导的影响 Bag of Tricks for Long-Tailed Visual Recognition with Deep Convolutional Neural Networks 动机 to give a detailed experimental guideline of common tricks to obtain the effective combinations of these tricks propose a novel data augmentation approach 论点 long-tailed datasets poor accuray on the under-presented minority long-tailed CIFAR： 指数型衰减 imbalance factor：50/100 test set unchanged ImageNet-LT sampling the origin set follow the pareto distribution test set is balanced iNaturalist extremely imbalanced real world dataset fine-grained problem different learning paradigms metric learning meta learning knowledge transfer suffer from high sensitivity to hyper-parameters training tricks re-weighting re-sample mixup two-stage training different tricks might hurt each other propose a novel data augmentation approach based on CAM：generate images with transferred foreground and unchanged background 方法 start from baseline re-weighting baseline：CE re-weighting methods： cost-sensitive CE：按照样本量线性加权$\frac{n_c}{n_{min}}$ focal loss：困难样本加权 class-balanced loss： effective number rather than 样本量$n_c$ hyperparameter $\beta$ and weighting factor：$\frac{1-\beta}{1-\beta^{n_c}}$ 在cifar10上有效，但是cifar100上就不好了 directly application in training procedure is not a proper choice especially when类别增多，imbalance加剧的时候 re-sampling re-sampling methods over-sampling： 随机复制minority might leads to overfitting under-sampling 随机去掉一些majority be preferable to over-sampling 有规律地sampling 大体都是imbalanced向着lighter imbalanced向着balanced推动 artificial sampling methods create artificial samples sample based on gradients and features likely to introduce noisy data 观察到提升效果不明显 mixup input mixup：input mixup can be further improved if we remove the mixup in last several epochs manifold mixup：on only one layer 观察到两种mixup功效差不多，后面发现input mixup更好些 input mixup去掉再finetuning几个epoch结果又提升，manifold则会变差 two-stage training imbalanced training + balanced fine-tuning vanilla training schedule on imbalanced data 先学特征 fine-tune on balanced subsets 再调整recognition accuracy deferred re-balancing by re-sampling (DRS) ：propose CAM-based sampling deferred re-balancing by re-weighting (DRW) proposed CAM-based sampling DRS only replicate or remove for each sampled image, apply the trained model &amp; its ground truth label to generate CAM 用heatmap的平均值作为阈值来区分前背景 对前景apply transformations horizontal flipping translation rotating scaling 发现fine-tuning时候再resample比直接resample的结果好 proposed CAM-based sampling好于其他sampling，其中CAM-based balance- sampling最好 ImageTrans balance-sampling只做变换，不用CAM区分前背景，结果不如CAM-based，证明CAM有用 发现fine-tuning时候再reweight比直接reweight的结果好 其中CSCE（按照样本量线性加权）最好 整体来看DRS的结果稍微比DRW好一点 trick combinations two-stage的CAM-based DRS略好于DRW，两个同时用不会further improve 再加上mixup的话，input比manifold好一些 结论就是：input mixup + CAM-based DRS + mute fine-tuning，apply the tricks incrementally Balanced Meta-Softmax for Long-Tailed Visual Recognition 动机 long-tailed：mismatch between training and testing distributions softmax：biased gradient estimation under the long-tailed setup propose Balanced Softmax：an elegant unbiased extension of Softmax apply a complementary Meta Sampler：optimal sample rate classification &amp; segmentation 论点 raw baseline：a model that minimizes empirical risk on long-tailed training datasets often underperforms on a class-balanced test set most methods use re-sampling or re-weighting to simulate a balanced dataset may under-class the majority or have gradient issue meta-learning optimize the weight per sample need a clean and unbiased dataset decoupled training 就是上面一篇论文中的两阶段，第一阶段先学表征，第二阶段调整分布fine-tuning not adequate for datasets with extremely high imbalance factor LDAM Label-Distribution-Aware Margin Loss larger generalization error bound for minority suit for binary classification we propose BALMS Balanced Meta-Softmax theoretically equivalent with generalization error bound for datasets with high imbalance factors should combine Meta Sampler 方法 balanced softmax biased：从贝叶斯条件概率公式看，standard softmax上默认了均匀采样的p(y)，在长尾分布的时候，就是有偏的 加权： 加在softmax项里面 基于样本量线性加权 数学意义上：we need to focus on minimizing the training loss of the tail classes meta sampler resample和reweight直接combine可能会worsen performance class balance resample可能有over-balance issue combination procedures 对当前分布，先计算balanced-softmax，保存一个梯度更新后的模型 计算这个临时模型在meta set上的CE，对分布embedding进行梯度更新：评估当前分布咋样，往一定方向矫正 对真正的模型，用最新的分布，计算balanced-softmax，进行梯度更新：用优化后的分布，引导模型学习 实验 CE的结果呈现明显的长尾同分布趋势 CBS有缓解 BS更好 BS+CBS会over sample BS+meta最好 Imbalanced Deep Learning by Minority Class Incremental Rectification 动机 significantly imbalanced training data propose batch-wise incremental minority class rectification model Class Rectification Loss (CRL) bring benefits to both minority and majority class boundary learning 论点 Most methods produce learning bias towards the majority classes to eliminate bias lifting the importance of minority classes：over-sampling can easily cause model overfitting，可能造成对小类别的过分关注，而对大类别不够重视，影响模型泛化能力 cost-sensitive learning：difficult to optimise threshold-adjustment technique：given by experts previous methods mainly investigate single-label binary-class with small imbalance ratio real data large ratio：power-law distributions Subtle appearance discrepancy hard sample mining hard negatives are more informative than easy negatives as they violate a model class boundary we only consider hard mining on the minority classes for efficiency our batch-balancing hard mining strategy：eliminating exhaustive searching LMLE 唯一的竞品：考虑了data imbalance的细粒度分类 not end-to-end global hard mining computationally complex and expensive 方法 CRL overview explicitly imposing structural discrimination of minority classes batch-wise operate on CE forcus on minority class only：the conventional CE loss can already model the majority classes well limitations of CE CE treat the individual samples and classes as equally important the learned model is suboptimal boundaries are biased towards majority classes profile the class distribution for each class hard mining overview minority class hard sample mining selectively “borrowing” majority class samples from class decision boundary to minority class’s perspective：mining both hard-positive and hard-negative samples define minority class：selected in each mini-batch Incremental refinement： eliminates the LMLE’s drawback in assuming that local group structures of all classes can be estimated reliably by offline global clustering mini-batch的data distribution和训练集不是完全一致的 steps profile the minority and majority classes per label in each training mini-batch for each sample，for each class $j$，for each pred class $k$，we have $h^j=[h_1^j, …, h_k^j, …, h_{n_cls}^j]$ sort $h_k^j$ in descent order，define the minority classes for each class with $C_{min}^j = \sum_{k\in C_{min}^j}h_k^j \leq \rho * n_{bs}$，with $\rho=0.5$ hard mining hardness score based：prediction score，class-level feature based：feature distance，instance-level class-level，for class c hard-positives：same gt class，but low prediction hard-negative：different gt class，with high prediction instance-level，for each sample in class c hard-positives：same gt class，large distance with current sample hard-negative：different gt class，small distance with current sample top-k mining hard-positives：bottom-k scored on c/top-k distance on c hard-negative：top-k scored on c/bottom-k distance on c score-based yields superior to distance-based CRL final weighted loss：$L = \alpha L_{crl}+(1-\alpha)L_{ce}$，$\alpha=\eta\Omega_{imbalance}$ class imbalance measure $\Omega$：more weighting is assigned to more imbalanced labels form triplet loss：类内+类间 contrastive loss：类内 modelling the distribution relationship of positive and negative pairs：没看懂 总结 就是套用现有的metric learning，定义了一个变化的minority class，垃圾。 说到底就是大数据——CE，小数据——metric learning。]]></content>
      <tags>
        <tag>长尾分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[refineDet]]></title>
    <url>%2F2021%2F01%2F08%2FrefineDet%2F</url>
    <content type="text"><![CDATA[和refineNet没有任何关系 RefineDet: Single-Shot Refinement Neural Network for Object Detectio 动机 inherit the merits of both two-stage and one-stage：accuracy and efficiency single-shot multi-task refineDet anchor refinement module (ARM) object detection module (ODM) transfer connection block (TCB) 论点 three advantages that two-stage superior than one-stage RPN：handle class imbalance two step regress：coarse to refine two stage feature：RPN任务和regression任务有各自的feature 模拟二阶段检测的RPN，把classifier任务中的大量阴性框先排掉，但不是以两个阶段的形式，而是multi-task并行 将一阶段检测的objectness和box regression任务解耦，两个任务通过transfer block连接 ARM remove negative anchors to reduce search space for the classifier coarsely adjust the locations and sizes of anchors to provide better initialization for regression ODM further improve the regression predict multi labels TCB transfer the features in the ARM to handle the more challenging tasks in the ODM 方法 Transfer Connection Block 没什么新的东西，上采样用了deconv，conv-relu，element-wise add Two-Step Cascaded Regression fisrt step ARM prediction for each cell，for each predefined anchor boxes，predict 4 offsets and 2 scores obtain refined anchor boxes second step ODM prediction with justified feature map，with refined anchor boxes generate accurate boxes offset to refined boxes and multi-class scores，c+4 Negative Anchor Filtering reject well-classified negative anchors if the negative confidence is larger than 0.99，discard it in training the ODM ODM接收所有pred positive和hard negative Training and Inference details back：VGG16 &amp; resnet101 fc6 &amp; fc7变成两个conv different feature scales L2 norm two extra convolution layers and one extra residual block 4 feature strides each level：1 scale &amp; 3 ratios ensures that different scales of anchors have the same tiling density on the image matching 每个GT box match一个score最高的anchor box 为每个anchor box找到最匹配的iou大于0.5的gt box 相当于把ignore那部分也作为正样本了 Hard Negative Mining select negative anchor boxes with top loss values n &amp; p ratio：3:1 Loss Function ARM loss binary class：只计算正样本？？？ box：只计算正样本 ODM loss pass the refined anchors with the negative confidence less than the threshold multi-class：计算均衡的正负样本 box：只计算正样本 正样本数为0的时候，loss均为0：纯阴性样本无效？？]]></content>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CPNDet]]></title>
    <url>%2F2021%2F01%2F05%2FCPNDet%2F</url>
    <content type="text"><![CDATA[Corner Proposal Network for Anchor-free, Two-stage Object Detection 动机 anchor-free two-stage 先找potential corner keypoints classify each proposal corner-based方法：对于objects of various scales有效，在训练中避免产生过多的冗余false-positive proposals，但是在结果上会出现更多的fp 得到的是competitive results 论点 anchor-based methods对形状奇怪的目标容易漏检 anchor-free methods容易引入假阳caused by mistakely grouping thus an individual classifier is strongly required Corner Proposal Network (CPN) use key-point detection in CornerNet 但是group阶段不再用embedding distance衡量，而是用a binary classifier 然后是multi-class classifier，operate on the survived objects 最后soft-NMS]]></content>
      <tags>
        <tag>目标检测，two-stage，anchor-free</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[refineNet]]></title>
    <url>%2F2021%2F01%2F05%2FrefineNet%2F</url>
    <content type="text"><![CDATA[RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation引用量1452，但是没有几篇技术博客？？ 动机 语义分割 dense classification on every single pixel refineNet long-range residual connections chained residual pooling 论点 pooling/conv stride： losing finer image structure deconv is not able to recover the lost info atrous high reso：large computation dilated conv：coarse sub-sampling of feature FCN fuse features from all levels stage-wise rather than end-to-end???存疑 this paper main idea：effectively exploit middle layer features RefineNet fuse all level feature residual connections with identity skip chained residual pooling to capture background context：看描述感觉像inception downsamp end-to-end 是整个分割网络中的一个component 方法 backbone pretrained resnet 4 blocks：x4 - x32，each block：pool-residual connection：每个输出连接一个RefineNet unit 4-cascaded architecture final ouput： high-resolution feature maps dense soft-max bilinear interpolation to origin resolution cascade inputs output from backbone block ouput from previous refineNet block refineNet block adapt conv： to adapt the dimensionality and refine special task BN layers are removed channel 512 for R4，channel 256 for the rest fusion： 先用conv to adapt dimension and recale the paths 然后upsamp summation 如果single input：walk through and stay unchanged chained residual pooling： aims to capture background context from a large image region chained：efficiently pool features with multiple window sizes pooling blocks：s1 maxpooling+conv in practice用了两个pooling blocks use one ReLU in the chained residual pooling block output conv： 一个residual：to employ non-linearity dimension remains unchanged final level：two additional RCUs before the final softmax prediction residual identity mappings a clean information path not block by any non-linearity：所有relu都在residual path里面 只有chained residual pooling模块起始时候有个ReLU：one single ReLU in each RefineNet block does not noticeably reduce the effectiveness of gradient flow linear operations： within the fusion block dimension reduction operations upsamp operations 其他结构 级联的就叫cascaded 一个block就叫single 多个input resolution就叫mult-scale 实验 4-cascaded works better than 1-cas &amp; 2-cas 2-scale works better than 1-scale]]></content>
      <tags>
        <tag>语义分割</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centerNet]]></title>
    <url>%2F2020%2F12%2F29%2FcenterNet%2F</url>
    <content type="text"><![CDATA[[papers] [centerNet] 真centerNet: Objects as Points，utexas，这个是真的centerNet，基于分割架构，预测中心点的heatmap，以及2-N个channel其他相关参数的回归 [cornet-centerNet] centerNet: Keypoint Triplets for Object Detection，这个抢先叫了centerNet，但是我觉得叫corner-centerNet更合适，它是基于cornerNet衍生的，在cornerNet的基础上再加一刀判定，基于角点pair的中心点是否是前景来决定是否保留这个框 [centerNet2] Probabilistic two-stage detection，utexas， centerNet: Objects as Points 动机 anchor-based exhaustive list of potential locations wasteful, inefficient, requires additional post-processing our detector center：use keypoint estimation to find center points other properties：regress tasks object detection 3d object detection multi-person human pose estimation 论点 相比较于传统一阶段、二阶段检测 anchor： box &amp; kp：一个是框，一个是击中格子 nms：take local peaks，no need of nms larger resolution：hourglass架构，输出x4的heatmap，eliminates the need for multiple anchors 相比较于key point estimantion network them：require grouping stage our：只定位一个center point，no need for group or post-processing 方法 loss 关键点loss center point关键点定义：每个目标的gt point只有一个，以它为中心，做object size-adaptive的高斯penalty reduction，overlap的地方取max focal loss：基本与cornetNet一致 L_k = \frac{-1}{N}\sum_{x,y,c} \begin{cases} (1-\hat Y)^\alpha log(\hat Y), if Y=1\\ (1-Y)^\beta \hat Y^\alpha log(1-\hat Y), otherwise \end{cases} $\alpha=2, \beta=4$ background points有penalty，根据gt的高斯衰减来的 offset loss 只有两个通道(x_offset &amp; y_offset)：shared among categories gt的offset是原始resolution/output stride向下取整得到 L1 loss centerNet output 第一个部分：中心点，[h,w,c]，binary mask for each category 第二个部分：offset，[h,w,2]，shared among 第三个部分：size，[h,w,2]，shared among L1 loss，use raw pixel coordinates overall C+4 channels，跟传统检测的formulation是一致的，只不过传统检测gt是基于anchor计算的相对值，本文直接回归绝对值 $L_{det} = L_k + \lambda_{size} L_{size} + \lambda_{off} L_{off}$ 其他task的formulation看第一张图 inference workflow local peaks： for each category channel all responses greater or equal to its 8-connected neighbors：3x3 max pooling keep the top100 generate bounding boxes 组合offset &amp; size predictions ？？？？没有后处理了？？？假阳？？？？ encoder-decoder backbone：x4 hourglass104 stem：x4 modules：两个 resnet18/101+deformable conv upsampling 3x3 deformable conv, 256/128/64 bilinear interpolation DLA34+deformable conv upsampling heads independent heads one 3x3 conv，256 1x1 conv for prediction 总结 个人感觉，centerNet和anchor-based的formulation其实是一样的， center的回归对标confidence的回归，区别在于高斯/[0,1]/[0,-1,1] size的回归变成了raw pixel，不再基于anchor hourglass结构就是fpn，级联的hourglass可以对标bi-fpn 多尺度变成了单一大resolution特征图，也可以用多尺度预测，需要加NMS centerNet2: Probabilistic two-stage detection 动机 two-stage probabilistic interpretation the suggested pipeline stage1：infer proper object-backgroud likelihood，专注前背景分离 stage2：inform the overall score verified on COCO faster and more accurate than both one and two stage detectors outperform yolov4 extreme large model：56.4 mAP standard ResNeXt- 32x8d-101-DCN back：50.2 mAP 论点 one-stage detectors dense predict jointly predict class &amp; location anchor-based：RetinaNet用focal loss来deal with 前背景imbalance anchor-free：FCOS &amp; CenterNet不基于anchor基于grid，缓解imbalance deformable conv：AlignDet在output前面加一层deformable conv to get richer features sound probablilistic interpretation heavier separate classification and regression branches than two-stage models：如果类别特别多的情况，头会非常重，严重影响性能 misaligned issue：一阶段预测是基于local feature，感受野、anchor settings都会影响与目标的对齐程度 two-stage detectors first RPN generates coarse object proposals then per-region head to classify and refine ROI heads：Faster-RCNN用了两个fc层作为ROI heads cascaded：CascadeRCNN用了三个连续的Faster-RCNN，with a different positive threshold semantic branch：HTC用了额外的分割分支enhance the inter-stage feature flow decouple：TSD将cls&amp;pos两个ROI heads解耦 weak RPN：因为尽可能提升召回率，proposal score也不准，丧失了一个clear的probabilistic interpretation independent probabilistic interpretation：两个阶段各训各的，最后的cls score仅用第二阶段的 slow：proposals太多了所以slow down other detectors point-based：cornetNet预测&amp;组合两个角点，centerNet预测中心点并基于它回归长宽 transformer：DETR直接预测a set of bounding boxes，而不是传统的结构化的dense output 网络结构 one/two-stage detectors：image classification network + lightweight upsampling layers + heads point-based：FCN，有symmetric downsampling and upsampling layer，预测一个小stride的heatmap DETR：feature extraction + transformer decoder our method 第一个阶段 做二分类的one-stage detector，提前景， 实现上就用region-level feature+classifier（FCN-based） 第二阶段 做position-based类别预测 实现上既可以用一个Faster-RCNN，也可以用classifier 最终的loss由两个阶段合并得到，而不是分阶段训练 跟former two-stage framework的主要不同是 加了joint probabilistic objective over both stages 以前的二阶段RPN的用途主要是最大化recall，does not produce accurate likelihoods faster and more accurate 首先是第一个阶段的proposal更少更准 其次是第二个阶段makes full use of years of progress in two-stage detection，二阶段的设计站在伟人的肩膀上 方法 joint class distribution：先介绍怎么将一二阶段联动 【第一阶段的前背景score】 乘上 【第二阶段的class score】 $P(C_k) = \sum_o P(C_k|O_k=o)P(O_k=o)$ maximum likelihood estimation for annotated objects 退阶成independent maximum-likelihood $log P(C_k) = log P(C_k|O_k=1) + log P(O_k=1)$ for background class 不分解 $log P(bg) = log( P(bg|O_k=1) * P(O_k=1) + P(O_k=0))$ lower bounds，基于jensen不等式得到两个不等式 $log P(bg) \ge P(O_k=1) * log( P(bg|O_k=1))$：如果一阶段前景率贼大，那么就 $log P(bg) \ge P(O_k=0)$： optimize both bounds jointly works better network design：介绍怎么在one-stage detector的基础上改造出一个two-stage probabilistic detector experiment with 4 different designs for first-stage RPN RetinaNet RetinaNet其实和two-stage的RPN高度相似核心区别在于： a heavier head design：4-conv vs 1-conv RetinaNet是backbone+fpn+individual heads RPN是backbone+fpn+shared convs+individual heads a stricter positive and negative anchor definition：都是IoU-based anchor selection，thresh不一样 focal loss first-stage design 以上三点都在probabilistic model里面保留 然后将separated heads改成shared heads centerNet 模型升级 升级成multi-scale：use ResNet-FPN back，P3-P7 头是FCOS那种头：individual heads，不share conv，然后cls branch预测centerness+cls，reg branch预测regress params 正样本也是按照FCOS策略：position &amp; scale-based 升级模型进行one-stage &amp; two-stage实验：centerNet* ATSS 是一个adaptive IoU thresh的方法，centerness来表示一个格子的score 我们将centerness*classification score定义为这个模型的proposal score 另外就是two-stage下还是将RPN的cls &amp; reg heads合并 GFL：还没看过，先跳过吧 second-stage designs：FasterRCNN &amp; CascadeR- CNN deformable conv：这个在centerNetv1的ResNet和DLA back里面都用了，在v2里面，主要是用ResNeXt-32x8d-101-DCN， hyperparameters for two-stage probabilistic model 两阶段模型通常是用P2-P6，一阶段通常用P3-P7：我们用P3-P7 increase the positive IoU threshold：0.5 to [0.6,0.7,0.8] maximum of 256 proposals （对比origin的1k） increase nms threshold from 0.5 to 0.7 SGD，90K iterations base learning rate：0.02 for two-stage &amp; 0.01 for one-stage，0.1 decay multi-scale training：短边[640,800]，长边不超过1333 fix-scale testing：短边用800，长边不超过1333 first stage loss weight：0.5，因为one-stage detector通常用0.01 lr开始训练 实验 4种design的对比 所有的probabilistic model都比one-stage model强，甚至还快（因为简化了脑袋） 所有的probabilistic FasterRCNN都比原始的RPN-based FasterRCNN强，也快（因为P3-P7比P2-P6的计算量小一半，而且第二阶段fewer proposals） CascadeRCNN-CenterNet design performs the best：所以以后就把它叫CenterNet2 和其他real-time models对比 大多是real-time models都是一阶段模型 可以看到二阶段不仅能够比一阶段模型还快，精度还更高 SOTA对比 报了一个56.4%的sota，但是大家口碑上好像效果很差 不放图了 corner-centerNet: Keypoint Triplets for Object Detection 动机 based on cornerNet triplet corner keypoints：weak grouping ability cause false positives correct predictions can be determined by checking the central parts cascade corner pooling and center poolling 论点 whats new in CenterNet triplet inference workflow after a proposal is generated as a pair of corner keypoints checking if there is a center keypoint of the same class center pooling for predicting center keypoints by making the center keypoints on feature map having the max sum Hori+Verti responses cascade corner pooling equips the original corner pooling module with the ability of perceiving internal information not only consider the boundary but also the internal directions CornetNet痛点 fp rate高 small object的fp rate尤其高 一个idea：cornerNet based RPN 但是原生RPN都是复用的 计算效率？ 方法 center pooling geometric centers &amp; semantic centers center pooling能够有效地将语义信息最丰富的点（semantic centers）传达到物理中心点（geometric centers），也就是central region]]></content>
      <tags>
        <tag>目标检测，anchor-free, one-stage&amp;two-stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[equlization loss]]></title>
    <url>%2F2020%2F12%2F21%2Fequlization-loss%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[megDet]]></title>
    <url>%2F2020%2F12%2F18%2FmegDet%2F</url>
    <content type="text"><![CDATA[MegDet: A Large Mini-Batch Object Detector 动机 past methods mainly come from novel framework or loss design this paper studies the mini-batch size enable training with a large mini-batch size warmup learning rate policy cross-gpu batch normalization faster &amp; better acc 论点 potential drawbacks with small mini-batch sizes long training time inaccurate statistics for BN：previous methods use fixed statistics from ImageNet which is a sub-optimal trade-off positive &amp; negative training examples are more likely imblanced 加大batch size以后，正负样本比例有提升，所以yolov3会先锁着back开大batchsize做warmup learning rate dilemma large min-batch size usually requires large learning rate large learning rate is likely leading to convergence failure a smaller learning rate often obtains inferior results solution of the paper linear scaling rule warmup Cross-GPU Batch Normalization (CGBN) 方法 warmup set up the learning rate small enough at the be- ginning then increase the learning rate with a constant speed after every iteration, until fixed Cross-GPU Batch Normalization 两次同步 tensorpack里面有 一次同步 异步BN：batch size 较小时，每张卡计算得到的统计量可能与整体数据样本具有较大差异 同步： 需要同步的是每张卡上计算的统计量，即BN层用到的均值$\mu$和方差$\sigma^2$ 这样多卡训练结果才与单卡训练效果相当 两次同步： 第一次同步均值：计算全局均值 第二次同步方差：基于全局均值计算各自方差，再取平均 一次同步： 核心在于方差的计算 首先均值：$\mu = \frac{1}{m} \sum_{i=1}^m x_i$ 然后是方差： \sigma^2 = \frac{1}{m} \sum_{i=1}^m (x_i-\mu)^2 = \frac{1}{m} \sum_{i=1}^m x_i^2 - \mu^2\\ =\frac{1}{m} \sum_{i=1}^m x_i^2 - (\frac{1}{m} \sum_{i=1}^m x_i)^2 * 计算每张卡的$\sum x_i$和$\sum x_i^2$，就可以一次性算出总均值和总方差]]></content>
      <tags>
        <tag>目标检测，large mini-batch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RFB]]></title>
    <url>%2F2020%2F12%2F16%2FRFB%2F</url>
    <content type="text"><![CDATA[RFB: Receptive Field Block Net for Accurate and Fast Object Detection 动机 RF block：Receptive Fields strengthen the lightweight features using a hand-crafted mechanism：轻量，特征表达能力强 assemble RFB to the top of SSD 论点 lightweight enhance feature representation 人类 群智感受野（pRF）的大小是其视网膜图中偏心率的函数 感受野随着偏心率而增加 更靠近中心的区域在识别物体时拥有更高的比重或作用 大脑在对于小的空间变化不敏感 fixed sampling grid (conv) probably induces some loss in the feature discriminability as well as robustness inception RFs of multiple sizes but at the same center ASPP with different atrous rates the resulting feature tends to be less distinctive Deformable CNN sampling grid is flexible but all pixels in an RF contribute equally RFB varying kernel sizes applies dilated convolution layers to control their eccentricities 组合来模拟human visual system concat 1x1 conv for fusion main contributions RFB module: enhance deep features of lightweight CNN networks RFB Net: gain on SSD assemble on MobileNet 方法 Receptive Field Block 类似inception的multi-branch dilated pooling or convolution layer RFB Net SSD-base 头上有较大分辨率的特征图的conv层are replaced by the RFB module 特别头上的conv层就保留了，因为their feature maps are too small to apply filters with large kernels like 5 × 5 stride2 module：每个conv stride2，那id path得变成1x1 conv？]]></content>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PANet]]></title>
    <url>%2F2020%2F12%2F02%2FPANet%2F</url>
    <content type="text"><![CDATA[PANet: Path Aggregation Network for Instance Segmentation 动机 boost the information flow bottom-up path shorten information path enhance accurate localization adaptive feature pooling aggregate all levels avoiding arbitrarily assigned results mask prediction head fcn + fc captures different views, possess complementary properties subtle extra computational 论点 previous skills: fcn, fpn, residual, dense findings 高层特征类别准，底层特征定位准，但是高层和底层特征之间的path太长了，不利于双高 past proposals make predictions based on one level PANet bottom-up path shorten information path enhance accurate localization adaptive feature pooling aggregate all levels avoiding arbitrarily assigned results mask prediction head fcn + fc captures different views, possess complementary properties 方法 framework b: bottom-up path c: adaptive feature pooling e: fusion mask branch bottom-up path fpn’s top-down path: to propagate strong semantical information to ensure reasonable classification capability long path: red line, 100+ layers bottom-up path: enhances the localization capability short path: green line, less than 10 layers for each level $N_l$ input: $N_{l+1}$ &amp; $P_l$ $N_{l+1}$ 3x3 conv &amp; $P_l$ id path - add - 3x3 conv channel 256 ReLU after conv adaptive feature pooling pool features from all levels, then fuse, then predict steps map each proposal to all feature levels roi align go through one layer of the following sub-networks independently fusion operation (element-wise max or sum) 例如，box branch是两个fc层，来自各个level的roi align之后的proposal features，先各自经过一个fc层，再share the following till the head，mask branch是4个conv层，来自各个level的roi align之后的proposal features，先各自经过一个conv层，再share the following till the head fusion mask branch fc layers are location sensitive helpful to differentiate instances and recognize separate parts belonging to the same object conv分支 4个连续conv+1个deconv：3x3 conv，channel256，deconv factor=2 predict mask of each class：output channel n_classes fc分支 from conv分支的conv3输出 2个连续conv，channel256，channel128 fc，dim=28x28，特征图尺寸，用于前背景分类 final mask：add 实验 heavier head 4 consecutive 3x3 convs shared among reg &amp; cls 在multi-task的情况下，对box的预测有效]]></content>
      <tags>
        <tag>实例分割</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSPNet]]></title>
    <url>%2F2020%2F11%2F17%2FCSPNet%2F</url>
    <content type="text"><![CDATA[CSPNET: A NEW BACKBONE THAT CAN ENHANCE LEARNING CAPABILITY OF CNN 动机 propose a network from the respect of the variability of the gradients reduces computations superior accuracy while being lightweightening 论点 CNN architectures design ResNeXt：cardinality can be more effective than width and depth DenseNet：reuse features partial ResNet：high cardinality and sparse connection，the concept of gradient combination introduce Cross Stage Partial Network (CSPNet) strengthening learning ability of a CNN：sufficient accuracy while being lightweightening removing computational bottlenecks：hoping evenly distribute the amount of computation at each layer in CNN reducing memory costs：adopt cross-channel pooling during fpn 方法 结构 Partial Dense Block：节省一半计算 Partial Transition Layer：fusion last能够save computation同时精度不掉太多 论文说fusion first使得大量梯度得到重用，computation cost is significantly dropped，fusion last会损失部分梯度重用，但是精度损失也比较小(0.1)。 it is obvious that if one can effectively reduce the repeated gradient information, the learning ability of a network will be greatly improved. Apply CSPNet to Other Architectures 因为只有一半的channel参与resnet block的计算，所以无需再引入bottleneck结构了 最后两个path的输出concat EFM fusion * 特征金字塔（FPN）：融合当前尺度和以前尺度的特征。 * 全局融合模型（GFM）：融合所有尺度的特征。 * 精确融合模型（EFM）：融合anchor尺寸上的特征。 EFM assembles features from the three scales：当前尺度&amp;相邻尺度 同时又加了一组bottom-up的融合 Maxout technique对特征映射进行压缩 结论 从实验结果来看， 分类问题中，使用CSPNet可以降低计算量，但是准确率提升很小； 在目标检测问题中，使用CSPNet作为Backbone带来的提升比较大，可以有效增强CNN的学习能力，同时也降低了计算量。本文所提出的EFM比GFM慢2fps，但AP和AP50分别显著提高了2.1%和2.4%。]]></content>
  </entry>
  <entry>
    <title><![CDATA[nms]]></title>
    <url>%2F2020%2F10%2F29%2Fnms%2F</url>
    <content type="text"><![CDATA[Non-maximum suppression：非极大值抑制算法，本质是搜索局部极大值，抑制非极大值元素 [nms]：standard nms，当目标比较密集、存在遮挡时，漏检率高 [soft nms]：改变nms的hard threshold，用较低的分数替代0，提升recall [softer nms]：引入box position confidence，通过后处理提高定位精度 [DIoU nms]：采用DIoU的计算方式替换IoU，因为DIoU的计算考虑到了两框中心点位置的信息，效果更优 [fast nms]：YOLOACT引入矩阵三角化，会比Traditional NMS抑制更多的框，性能略微下降 [cluster nms]：CIoU提出，弥补Fast NMS的性能下降，运算效率比Fast NMS下降了一些 [mask nms]：mask iou计算有不可忽略的延迟，因此比box nms更耗时 [matrix nms]：SOLO将mask IoU并行化，比FAST-NMS还快，思路和FAST-NMS一样从上三角IoU矩阵出发，可能造成过多抑制。 [WBF]：加权框融合，Kaggle胸片异物比赛claim有用，速度慢，大概比标准NMS慢3倍，WBF实验中是在已经完成NMS的模型上进行的 nms 过滤+迭代+遍历+消除 首先过滤掉大量置信度较低的框，大于confidence thresh的box保留 将所有框的得分排序，选中最高分的框 遍历其余的框，如果和当前最高分框的IOU大于一定阈值(nms thresh)，就将框删除(score=0) 从未处理的框中继续选一个得分最高的，重复上述过程 when evaluation iou thresh：留下的box里面，与gt box的iou大于iou thresh的box作为正例，用于计算出AP和mAP，通过调整confidence thresh可以画出PR曲线 softnms 基本流程还是nms的贪婪思路，过滤+迭代+遍历+衰减 re-score function：high overlap decays more linear： for each $iou(M,b_i)&gt;th$， $s_i=s_i(1-iou)$ not continuous，sudden penalty gaussian： for all remaining detection boxes，$s_i=s_i e^{-\frac{iou(M,b_i)}{\sigma}}$ 算法流程上未做优化，是针对精度的优化 softer nms 跟soft nms没关系 具有高分类置信度的边框其位置并不是最精准的 新增加了一个定位置信度的预测，使其服从高斯分布 infer阶段边框的标准差可以被看做边框的位置置信度，与分类置信度做加权平均，作为total score 算法流程上未做优化，完全是精度的优化 DIoU nms 也是为了解决hard nms在密集场景中漏检率高的问题 但是不同于soft nms的是，D的改进在iou计算上，而不是在score diou的计算：$diou = iou-\frac{\rho^2(b_1, b_2)}{c^2}$ 算法流程上未做优化，仍旧是精度的优化 fast nms yoloact提出 主要效率提升在于用矩阵操作替换遍历，所有框同时被filter掉，而非依次遍历删除 iou上三角矩阵 iou上三角矩阵的每一个元素都是行号小于列号 iou上三角矩阵的每一个行，对应一个bnd box，与其他所有score小于它的bnd box的iou iou上三角矩阵的每一个列，对应一个bnd box，与其他所有score大于它的bnd box的iou fast nms在iou矩阵每一列上求最大值，如果这个最大值大于iou thresh，说明当前列对应的bnd box，存在一个score大于它，且和它重叠度较高的bnd box，因此要把这个box过滤掉 有精度损失 场景： 如果是hard nms的话，首先遍历b1的其他box，b2就被删除了，这是b3就不存在高重叠框了，b3就会被留下，但是在fast nms场景下，所有框被同时删除，因此b2、b3都没了。 cluster nms 针对fast nms性能下降的弥补 fast nms性能下降，主要问题在于过度抑制，并行操作无法及时消除high score框抹掉对后续low score框判断的影响 算法流程上，将fast nms的一次阈值操作，转换成少数几次的迭代操作，每次都是一个fast nms 图中X表示iou矩阵，b表示nms阈值二值化以后的向量，也就是fast nms里面那个保留／抑制向量 每次迭代，算法将b展开成一个对角矩阵，然后左乘iou矩阵 直到出现某两次迭代后， b保持不变了，那么这就是最终的b cluster nms的迭代操作，其实就是在省略上一次Fast NMS迭代中被抑制的框对其他框的影响 数学归纳法证明，cluster nms的结果与hard nms完全一致，运算效率比fast nms下降了一些，但是比hard nms快得多 cluster nms的运算效率不与cluster数量有关，只与需要迭代次数最多的那一个cluster有关 mask nms 从检测框形状的角度拓展出来，包括但不限于mask nms、polygon nms以及inclined nms iou的计算方式有一种是mmi：$mmi=max(\frac{I}{I_A}, \frac{I}{I_B})$ matrix nms 学习soft nms：decay factor one step further：迭代改并行 对于某个object $m_j$的score进行penalty的时候考虑两部分影响 迭代某个$m_i$时，对后续lower score的$m_j$的影响 一是正面影响$f(iou_{i,j})\ linear/guassian$：这个框保留，那么后续框都要基于与其的iou做decay 二是反向影响$f(iou_{*,i})=max_{\forall s_k&gt;s_i}f(iou_{k,i})$：如果这个框不保留，那么对于后续框来讲，应该消除这个框对其的decay，选最大值的意义是当前mask被抑制最有可能就是和他重叠度最大的那个mask干的（因为对应的正面影响1-iou最小） final decay factor：$decay_j=min_{\forall s_i &gt; s_j}\frac{f(iou_{i,j})}{f(iou_{*,i})}$ 算法流程 &lt;img src=&quot;nms/matrixnms.png&quot; width=&quot;50%;&quot; /&gt; * 按照原论文的实现，decay永远大于等于1，因为每一列的iou_cmax永远大于等于iou，从论文的思路来看，每个mask的decay是它之前所有mask的影响叠加在一起，所以应该是乘积而不是min： 123456789101112# 原论文实现if method=='gaussian': decay = np.exp(-(np.square(iou)-np.square(iou_cmax))/sigma)else: decay = (1-iou)/(1-iou_cmax)decay = np.min(decay, axis=0)# 改进实现if method=='gaussian': decay = np.exp(-(np.sum(np.square(iou),axis=0)-np.square(iou_cmax))/sigma)else: decay = np.prod(1-iou)/(1-iou_cmax)]]></content>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MimicDet]]></title>
    <url>%2F2020%2F10%2F14%2FMimicDet%2F</url>
    <content type="text"><![CDATA[[MimicDet] ResNeXt-101 backbone on the COCO: 46.1 mAP MimicDet: Bridging the Gap Between One-Stage and Two-Stage Object Detection 动机 mimic task：knowledge distillation mimic the two-stage features a shared backbone two heads for mimicking end-to-end training specialized designs to facilitate mimicking dual-path mimicking staggered feature pyramid reach two-stage accuracy 论点 one-stage detectors adopt a straightforward fully convolutional architecture two-stage detectors use RPN + R-CNN advantages of two-stage detectors avoid class imbalance less proposals enables larger cls net and richer features RoIAlign extracts location consistent feature -&gt; better represenation regress the object location twice -&gt; better refined one-stage detectors’ imitation RefineDet：cascade detection flow AlignDet：RoIConv layer still leaves a big gap network mimicking knowledge distillation use a well-trained large teacher model to supervise difference mimic in heads instead of backbones teacher branch instead of model trained jointly this method not only mimic the structure design, but also imitate in the feature level contains both one-stage detection head and two-stage detection head during training share the same backbone two-stage detection head, called T-head one-stage detection head, called S-head similarity loss for matching feature：guided deformable conv layer together with detection losses specialized designs decomposed detection heads conduct mimicking in classification and regression branches individually staggered feature pyramid 方法 overview back &amp; fpn RetinaNet fpn：with P6 &amp; P7 crucial modification：P2 ～ P7 staggered feature pyramid high-res set {P2 to P6}：for T-head &amp; accuray low-res set {P3 to P7}：for S-head &amp; computation speed refinement module filter out easy negatives：mitigate the class imbalance issue adjust the location and size of pre-defined anchor boxes：anchor initialization module on top of the feature pyramid one 3x3 conv two sibling 1x1 convs binary classification：bce loss bounding box regression：the same as Faster R-CNN，L1 loss top-ranked boxes transferred to T-head and S-head one anchor on each position：avoid feature sharing among proposals assign the objects to feature pyramid according to their scale positive area：0.3 times shrinking of gt boxes from center positive sample： valid scale range：gt target belongs to this level central point of anchor lies in the positive area detection heads T-head heavy head run on a sparse set of anchor boxes use the staggered feature pyramid generate 7x7 location-sensitive features for each anchor box cls branch two 1024-d fc layers one 81-d fc layer + softmax：ce loss reg branch four 3x3 convs，ch256 flatten 1024-d fc 4-d fc：L1 loss mimicking target 81-d classification logits 1024-d regression feature S-head light-weight directly dense detection on fpn 【不太理解】introducing the refinement module will break the location consistency between the anchor box and its corresponding features：我的理解是refine以后的anchor和原始anchor对应的特征图misalign了，T-head用的是refined anchor，S-head用的是original grid，所以misalign use deformable convolution to capture the misaligned feature deformation offset is computed by a micro-network takes the regression output of the refinement module as input three 1x1 convs，ch64/128／18(50) 3x3 Dconv for P3 and 5x5 for others，ch256 two sibling 1x1 convs，ch1024 cls branch：1x1 conv，ch80 reg branch：1x1 conv，ch4 head mimicking cosine similarity cls logits &amp; refine params To get the S-head feature of an adjusted anchor box trace back to its initial position extract the pixel at that position in the feature map loss：$L_{mimic} = 1 - cosine(F_i^T, F_i^S)$ multi-task training loss $L = L_R + L_S + L_T + L_{mimic}$ $L_R$：refine module loss，bce+L1 $L_S$：S-head loss，ce+L1 $L_T$：T-head loss，ce+L1 $L_{mimic}$：mimic loss training details network：resnet50/101，resize image with shorter side 800 refinement module run NMS with 0.8 IoU threshold on anchor boxes select top 2000 boxes T-head sample 128 boxes from proposal p／n：1/3 S-head hard mining：select 128 boxes with top loss value inference take top 1000 boxes from refine module NMS with 0.6 IoU threshold and 0.005 score threshold 【？？】finally top 100 scoring boxes：这块不太理解，最后应该不是结构化输出了啊，应该是一阶段检测头的re-refine输出啊]]></content>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[metrics]]></title>
    <url>%2F2020%2F10%2F09%2Fmetrics%2F</url>
    <content type="text"><![CDATA[分类指标 recall：召回率 precision：准确率 accuracy：正确率 F-Measure sensitivity：灵敏度 specificity：特异度 TPR FPR ROC AUC 混淆矩阵 | | gt is p | gt is n | | :———-: | :————: | :—————: | | pred is p | tp | fp（假阳性） | | pred is n | fn（漏检） | tn | 注意区分fp和fn fp：被错误地划分为正例的个数，即实际为负例但被分类器划分为正例的实例数 fn：被错误地划分为负例的个数，即实际为正例但被分类器划分为负例的实例数 recall 衡量查全率 对gt is p做统计 $recall = \frac{tp}{tp+fn}$ precision 衡量查准率 对pred is p做统计 $precision = \frac{tp}{tp+fp}$ accuracy 对的除以所有 $accuracy = \frac{tp+tn}{p+n}$ sensitivity 衡量分类器对正例的识别能力 对gt is p做统计 $sensitivity = \frac{tp}{p}=\frac{tp}{tp+fn}$ specificity 衡量分类器对负例的识别能力 对gt is n做统计 $specificity =\frac{tn}{n}= \frac{tn}{fp+tn}$ F-measure 综合考虑P和R，是Precision和Recall加权调和平均 $F = \frac{(a^2+1)PR}{a^2*P+R}$ $F_1 = \frac{2PR}{P+R}$ TPR 将正例分对的概率 对gt is t做统计 $TPR = \frac{tp}{tp+fn}$ FPR 将负例错分为正例的概率 对gt is n做统计 $FPR = \frac{fp}{fp+tn}$ FPR = 1 - 特异度 ROC 每个点的横坐标是FPR，纵坐标是TPR 描绘了分类器在TP（真正的正例）和FP（错误的正例）间的trade-off 通过变化阈值，得到不同的分类统计结果，连接这些点就形成ROC curve 曲线在对角线左上方，离得越远说明分类效果好 P/R和ROC是两个不同的评价指标和计算方式，一般情况下，检索用前者，分类、识别等用后者 AUC AUC的值就是处于ROC curve下方的那部分面积的大小 通常，AUC的值介于0.5到1.0之间]]></content>
      <tags>
        <tag>评价指标</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[metric learning系列]]></title>
    <url>%2F2020%2F09%2F25%2Fmetric-learning%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[参考：https://gombru.github.io/2019/04/03/ranking_loss/，博主实验下来觉得Triplet Loss outperforms Cross-Entropy Loss 综述 metric learning 常规的cls loss系列(CE、BCE、MSE)的目标是predict a label metric loss系列的目标则是predict relative distances between inputs 常用场景：人脸 &amp; fine-grained relation between samples first get the embedded representation then compute the similarity score binary (similar / dissimilar) regression (euclidian distance) 大类：不管叫啥，主体上就两类，二元组和三元组 common target：拉近类内距离，拉大类间距离 pairs anchor + sample positive pairs：distance —&gt; 0 negative pairs：disctance &gt; a margin triplets anchor + pos sample + neg sample target：(dissimilar distance - similar distance) —&gt; a margin papers [siamese network] Signature Verification using a ‘Siamese’ Time Delay Neural Network：1993，lecun，孪生网络始祖，俩个子网络sharing weights，距离用的是cosine distance，loss直接优化距离，优化target是个定值cosine=1.0/-1.0 [contrastive loss] Dimensionality Reduction by Learning an Invariant Mapping：2006，lecun，contrastive loss始祖，研究的是高维特征向量向低维映射的非线性层，距离用的是euclidean distance，loss优化的是squared distance，优化target是0和m，similar pairs仍旧会被推向一个定点，没有解决论文声称的uniform distribution [triplet-loss] Learning Fine-grained Image Similarity with Deep Ranking：2014，Google，用了三元组，提出了triplet-loss [facenet] FaceNet: A Unified Embedding for Face Recognition and Clustering：2015，Google，用来识别人脸，用了三元组和triplet-loss，squared euclidean distance，优化目标是同类和异类pair之间的相对距离，困难样本（semi-hard &amp; hard）对收敛起作用（加速／local minima），triplet-loss考虑了类间的离散性，但没有考虑类内的紧凑性 [center-loss] A Discriminative Feature Learning Approach for Deep Face Recognition：2016，也是用在人脸任务上，优化目标是类内的绝对距离，而不是建模相对关系，center-loss直接优化的是类间的间凑性，类间的离散性靠的是softmax loss [triplet-center-loss] Triplet-Center Loss for Multi-View 3D Object Retrieval：2018，东拼西凑水论文 [Hinge-loss] SVM margin [circle-loss] Circle Loss: A Unified Perspective of Pair Similarity Optimization：2020CVPR，旷视，提出了cls loss和metric loss的统一形式$minimize(s_n - s_p+m)$，在此基础上提出circle loss作为优化目标$(\alpha_n s_n - \alpha_p s_p) = m$，在toy scenario下展示了分类边界和梯度的改善。 ～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～ [Hierarchical Similarity] Learning Hierarchical Similarity Metrics：2012CVPR， [Hierarchical Triplet Loss] Deep Metric Learning with Hierarchical Triplet Loss：2018ECCV， Hierarchical classicification应该单独做一个系列，tobeadded 一些待明确的问题 anchor怎么选：facenet中说明，每个mini-batch中每个类别必须都有 pairs怎么实现（困难的定义）：facenet中说明，hard distance sample in mini-batch hingeloss &amp; SVM推导 常规使用？结合cls loss和metric loss还是只用metric loss：cls loss和metric loss本质上是一样的，都是希望同类样本输出一样，不同类样本输出不一样，只不过前者具有概率意义，后者具有距离意义。上面列出来的只有center loss是要跟cls loss结合起来用的，因为他只针对类内，不足以推动整个模型。 Signature Verification using a ‘Siamese’ Time Delay Neural Network 动机 verification of written signatures propose Siamese two identical sub-networks joined at their outputs measure the distance verification process a stored feature vector a chosen threshold 方法 network two inputs：extracting features two sub-networks：share the same weights one output：cosine of the angle between two feature vectors target two real signatures：cosine=1.0 with one forgery：cosine=-0.9 and cosine=-1.0 dataset 50% genuine:genuine pairs 40% genuine:forgery pairs 10% genuine:zero-effort pairs Dimensionality Reduction by Learning an Invariant Mapping 动机 dimensionality reduction propose Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) globally co- herent non-linear function relies solely on neighbor- hood relationships invariant to certain transformations of the inputs 论点 most existing dimensionality reduction techniques they do not produce a function (or a mapping) from input to manifold new points with unknown relationships with training samples cannot be processed they tend to cluster points in output space a uniform distribution in the outer manifolds is desirable proposed DrLIM globally coherent non-linear function neighborhood relationships that are independent from any distance metric invariant to complicated non-linear trnasformations lighting changes geometric distortions can be used to map new samples empoly contrastive loss neighbors are pulled together non-neighbors are pushed apart energy based model euclidean distance approximates the “semantic similarity”of the inputs in input space 方法 contrastive loss conventional loss sum over samples contrastive loss sum over pairs $(X_1, X_2, Y)$ similar pairs：$Y=0$ dissimilar：$Y=1$ euclidean distance $L = (1-Y)\sum L_s ||G_w(X_1)-G_w(X_2)||_2 + Y\sum L_d ||G_w(X_1)-G_w(X_2)||_2$ $L_s$ should results in low values for similar pairs $L_d$ should results in high values for dissimilar pairs exact form：$L(W,Y,X_1,X_2) = (1-Y)\frac{1}{2}(D^2) + (Y)\frac{1}{2} \{max(0,m-D)\}^2$ spring model analogy similar partial loss相当于给弹簧施加了一个恒定的力，向中心点挤压 dissimilar partial loss只对圈内的点施力，推出圈外就不管了 FaceNet: A Unified Embedding for Face Recognition and Clustering 动机 face tasks face verification: is this the same person face recognition: who is the person clustering: find common people among the faces learn a mapping compact Euclidean space where the Euclidean distance directly correspond to face similarity 论点 traditionally training classification layer generalizes well to new faces？ indirectness large dimension feature representation inefficiency use siamese pairs the loss encourages all faces of one identity to project onto a single point this paper employ triplet loss target：separate the positive pair from the negative by a distance margin allows the faces of one identity to live on a manifold obtain face embedding l2 norm a fixed d-dims hypersphere large dataset to attain the appropriate invariances to pose, illumination, and other variational conditions architecture explore two different deep network 方法 input：三元组，consist of two matching face thumbnails and a non-matching one ouput：特征描述，a compact 128-D embedding living on the fixed hypersphere $||f(x)||_2=1$ triple-loss target：all anchor-pos distances are smaller than any anchor-neg distances with a least margin $\alpha$ $L = \sum_i^N [||f(x_i^a) - f(x_i^p)||_2^2 - ||f(x_i^a) - f(x_i^n)||_2^2 + \alpha]$ hard triplets hard samples $argmax_{x_i^p}||f(x_i^a) - f(x_i^p)||_2^2$ $argmin_{x_i^n}||f(x_i^a) - f(x_i^n)||_2^2$ infeasible to compute over the whole set：mislabelled and poorly imaged faces would dominate the hard positives and negatives off-line：use recent checkpoint to compute on a subset online：select in mini-batch mini-batch： 每个类别都必须有正样本 负样本是randomly sampled hard sample use all anchor-positive pairs selecting the hard negatives hardest negatives can lead to bad local minima in early stage 先pick semi-hard：$||f(x_i^a) - f(x_i^p)||_2^2 &lt; ||f(x_i^a) - f(x_i^n)||_2^2$ network 一种straight的网络，引入了1x1 conv先压缩通道 Inception models：20x fewer params，5x fewer FLOPS metric same／different是由a squared L2 distance决定 因此测试结果是d的函数 定义true accepts：圈内对的，$TA(d)=\{(i,j)\in P_{same}, with D(x_i,x_j)\leq d\}$ 定义false accepts：圈内错的，$FA(d)=\{(i,j)\in P_{diff}, with D(x_i,x_j)\leq d\}$ 定义validation rate：$VAL(d) = \frac{|TA(d)|}{|P_{same}|}$ 定义false accept rate：$FAR(d) = \frac{|FA(d)|}{|P_{diff}|}$ A Discriminative Feature Learning Approach for Deep Face Recognition 动机 enhance the discriminationative power of the deeply learned features joint supervision softmax loss center loss two key learning objectives inter-class dispension intra-class compactness 论点 face recognition task requirement the learned features need to be not only separable but also discriminative generalized enough for the new unseen samples the softmax loss only encourage the separability of features 对分类边界、类内类间分布没有直接约束 contrastive loss &amp; triplet loss training pairs or triplets dramatically grows slow convergence and instability we propose learn a center simultaneously update the center and optimize the distances joint supervision softmax loss forces the deep features of different classes staying apart center loss efficiently pulls the deep features of the same class to their centers to be more discriminationative the inter-class features differences are enlarged the intra-class features variations are reduced 方法 softmax vis 最后一层hidden layer使用两个神经元 so that we can directly plot separable but still show significant intra-class variations center loss $L_c = \frac{1}{2} \sum_1^m ||x_i - c_{y_i}||_2^2$ update class center on mini-batch： \frac{\partial L_c}{\partial x_i} = x_i - c_{y_i}\\ \Delta c_j = \frac{\sum_i^m \delta (y_i=j) * (c_j - x_i)}{1+\sum_i^m \delta (y_i=j)} joint supervision： L = L_{CE} + \lambda L_c discussion necessity of joint supervision solely softmax loss —-&gt; large intra-class variations solely center loss —-&gt; features and centers will degraded to zeros compared to contrastive loss and triplet loss using pairs：suffer from dramatic data expansion hard mining：complex recombination optimizing target： center loss直接针对intra-class compactness，类内用距离来约束，类间用softmax来约束 contrastive loss也是直接优化绝对距离，类内&amp;类间都用距离来约束 triplet loss是建模相对关系，类内&amp;类间都用距离来约束 architecture local convolution layer：当数据集在不同的区域有不同的特征分布时，适合用local-Conv，典型的例子就是人脸识别，一般人的面部都集中在图像的中央，因此我们希望当conv窗口滑过这块区域的时候，权重和其他边缘区域是不同的 参数量暴增：kernel_size kernel_size output_size output_size input_channel * output_channel 实验 hyperparam：$\lambda$ and $\alpha$ fix $\alpha=0.5$ and vary $\lambda$ from 0-0.1 fix $\lambda=0.003$ and vary $\alpha$ from 0.01-1 结论是remains stable across a large range，没有给出最佳／建议 我的实验 加比不加训练慢得多 在Mnist上测试同样的epoch加比不加准确率低 之所以Center Loss是针对人脸识别的Loss是有原因的，个人认为人脸的中心性更强一些，也就是说一个人的所有脸取平均值之后的人脸我们还是可以辨识是不是这个人，所以Center Loss才能发挥作用 Circle Loss: A Unified Perspective of Pair Similarity Optimization 动机 pair similarity circular decision boundary unify cls-based &amp; metric-based data class-level labels pair-wise labels 论点 there is no intrinsic difference between softmax loss &amp; metric loss minimize between-class similarity $s_n$ maximize within- class similarity $s_p$ reduce $s_n - s_p$ short-commings lack of flexibility：$s_p$和$s_n$的优化速度可能不同，一个快收敛了一个还很差，这时候用同样的梯度去更新就非常inefficient and irrational，就左图来说，下面的点相对上面的点，$s_n$更小（更接近op），$s_p$更小（更远离op），vice versa，但是决策平面对三个点相对于$s_n$和$s_p$的梯度都是一样的（1和-1）。 ambiguous convergence status：用一个hard distance margin来描述decision boundary还不够discriminative，hard decision boundary上各点其实还是有差别的，假设存在一个optimum（$s_p=1 \ \&amp; \ s_n=0$），那么左图决策平面上两个点，相对optimum的意义明显不一样，决策平面应该是个围绕optimum的圆圈。 propose circle loss independent weighting factors：离optimum越远的penalty strength越大，这一项直接以距离为优化目标的loss都是满足的 different penalty strength：$s_p$和$s_n$ learn at different paces，类内加权，加权系数是learnable params $(\alpha_n s_n - \alpha_p s_p) = m$：yielding a circle shape 方法 核心：$(\alpha_n s_n - \alpha_p s_p) = m$ self-paced weighting given optimum $O_p$ and $O_n$，for each similarity score： \begin{cases} a_p^i = [O_p - s_p^i]_+ \\ a_n^j = [s_n^j - O_n]_+ \end{cases} cut-off at zero 对于远离optimum的点梯度放大，接近optimum的点（快收敛）梯度缩小 softmax里面通常不会对同类样本间做这种rescaling的，因为它希望所有样本value都达到贼大 Circle loss abandons the interpretation of classifying a sample to its target class with a large probability margin adding a margin m reinforces the optimization take toy scenario 最终整理成：$(s_n-0)^2 + (s_p-1)^2 = 2m^2$ op target：$s_p &gt; 1-m$，$s_n &lt; m$ relaxation factor $m$：controls the radius of the decision boundary unified perspective tranverse all the similarity pairs：$\{s_p^i\}^K$和$\{s_n^j\}^N$ to reduce $(s_n^j - s_p^i)$：$L_{uni}=log[1+\sum^K_i \sum^N_j exp(\lambda (s_n^j - s_p^i + m))]$ 解耦（不会同时是$s_p$和$s_n$）：$L_{uni}=log[1+\sum^N_j exp(\lambda (s_n^j + m))\sum^K_i exp(\lambda (-s_p^i))]$ given class labels： we get $(N-1)$ between-class similarity scores and $(1)$ within-class similarity score 分母翻上去：$L = -log \frac{exp(\lambda (s_p-m))}{exp(\lambda (s_p-m)) + \sum^{N-1}_j exp(\lambda (s_n^j))}$ 就是softmax given pair-wise labels： triplet loss with hard mining：find pairs with large $s_n$ and low $s_p$ use infinite：$L=lim_{\lambda \to \inf} \frac{1}{\lambda} L_{uni}$ 实验 Face recognition noisy and long-tailed data：去噪并且去掉稀疏样本 resnet &amp; 512-d feature embeddings &amp; cosine distance $\lambda=256$，$m=0.25$ Person re-identification $\lambda=128$，$m=0.25$ Fine-grained image retrieval 车集和鸟集 bn-inception &amp; 512-d embeddings P-K sampling $\lambda=80$，$m=0.4$ hyper-params the scale factor $\lambda$： determines the largest scale of each similarity score Circle loss exhibits high robustness on $\lambda$ the other two becomes unstable with larger $\lambda$ owing to the decay factor the relaxation factor m： determines the radius of the circular decision boundary surpasses the best performance of the other two in full range robustness inference 对人脸类任务，通常用训练好的模型生成一个人脸标准底库，然后每次推理的时候得到测试数据的特征向量，并在标准底库中搜索相似度最高的特征，完成人脸识别过程。]]></content>
      <tags>
        <tag>度量学习，loss &amp; network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[efficient周边]]></title>
    <url>%2F2020%2F09%2F23%2Fefficient%E5%91%A8%E8%BE%B9%2F</url>
    <content type="text"><![CDATA[因为不是googlenet家族官方出品，所以放在外面 [EfficientFCN] EfficientFCN: Holistically-guided Decoding for Semantic Segmentation：商汤，主要针对upsampling是局部感受野，重建失真多，分割精度差的问题，提出了Holistically-guided Decoder (HGD) ，用来recover the high-resolution (OS=8) feature maps，想法上接近SCSE-block，数学表达上接近bilinear-CNN，性能提升主要归因于eff back吧。 EfficientFCN: Holistically-guided Decoding for Semantic Segmentation 动机 Semantic Segmentation dilatedFCN：computational complexity encoder-decoder：performance proposed EfficientFCN common back without dilated convolution holistically-guided decoder balance performance and efficiency 论点 key elements for semantic segmentation high-resolution feature maps pre-trained weights OS32 feature map：the fine-grained structural information is discarded dilated convolution：no extra parameters introduced but equire high computational complexity and memory consumption encoder-decoder based methods repeated upsampling + skip connection procedure upsampling concat／add successive convs Even with the skip connections, lower-level high-resolution feature maps cannot provide abstractive enough features for achieving high- performance segmentation The bilinear upsampling or deconvolution operations are conducted in a local manner(from a limited receptive filed) improvements reweight：SE-block scales each feature channel but maintains the original spatial size and structures：【scse block对spacial有加权啊】 propose EfficientFCN widely used classification model Holistically-guided Decoder (HGD) take OS8, OS16, OS32 feature maps from backbone OS8和OS16用来spatially guiding the feature upsampling process OS32用来encode the global context然后基于guidance进行上采样 linear assembly at each high-resolution spatial location：感觉就是对上采样特征图做了加权 方法 Holistically-guided Decoder multi-scale feature fusion holistic codebook generation from high-level feature maps holistic codewords：without any spatial order codeword assembly multi-scale feature fusion we observe the fusion of multi-scale feature maps generally result in better performance compress：separate 1x1 convs bilinear downsamp／upsamp concatenate fused OS32 $m_{32}$ &amp; fused OS8 $m_8$ holistic codebook generation from $m_{32}$ two separate 1x1 conv a codeword based map $B \in R^{1024(H/32)(W/32)}$：每个位置用一个1024-dim的vector来描述 n spatial weighting map $A\in R^{n(H/32)(W/32)}$：highlight 特征图上不同区域 softmax norm in spatial-dim $\widetilde A_i(x,y)=\frac{exp(A_i(x,y))}{\sum_{p,q} exp(A_i(p,q))}, i\in [0,n)$ codeword $c_i \in R^{1024}$ global description for each weighting map weighted average of B on all locations $c_i = \sum_{p,q} \widetilde A_i(p,q) B(p,q)$ each codeword captures certain aspect of the global context orderless high-level global features $C \in R^{1024*n}$ $C = [c_1, …, c_n]$ codeword assembly raw guidance map $G \in R^{1024(H/8)(W/8)}$：1x1 conv on $m_8$ fuse semantic-rich feature map $\overline B \in R^{1024}$：global average vector novel guidance feature map $\overline G = G \oplus \overline B $：location-wise addition【？？？？】 linear assembly weights of the n codewords $W \in R^{n(H/8)(W/8)}$：1x1 conv on $\overline G$ holistically-guided upsampled feature $\tilde f_8 = W^T C$：reshape &amp; dot final feature map $f_8$：concat $\tilde f_8$ and $G$ final segmentation 1x1 conv further upsampling 实验 numer of holistic codewords 32-512：increase 512-1024：slight drop we observe the number of codewords needed is approximately 4 times than the number of classes]]></content>
      <tags>
        <tag>语义分割</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[data aug]]></title>
    <url>%2F2020%2F09%2F18%2Fdata-aug%2F</url>
    <content type="text"><![CDATA[[mixup] mixup: BEYOND EMPIRICAL RISK MINIMIZATION：对不同类别的样本，不仅可以作为数据增广手段，还可以用于semi-supervised learning（MixMatch） [mixmatch] MixMatch: A Holistic Approach to Semi-Supervised Learning：针对半监督数据的数据增广 [mosaic] from YOLOv4 [AutoAugment] AutoAugment: Learning Augmentation Policies from Data：google [RandAugment] RandAugment: Practical automated data augmentation with a reduced search space：google RandAugment: Practical automated data augmentation with a reduced search space 动机 AutoAugment separate search phase run on a subset of a huge dataset unable to adjust the regularization strength based on model or dataset size RandAugment significantly reduced search space can be used uniformly across tasks and datasets match or exceeds the previous val acc 方法 formulation always select a transformation with uniform prob $\frac{1}{K}$ given N transformations for an image：there are $K^N$ potential policies fixied magnitude schedule M：we choose Constant，因为只要一个hyper run naive grid search 疑问：这样每个op等概率，就不再data-specific了，也看不出自然图像更prefer color transformation这种结论了 AutoAugment: Learning Augmentation Policies from Data 动机 search for data augmentation policies propose AutoAugment create a search space composed of augmentation sub-policies one sub-policy is randomly choosed per image per mini-batch a sub-policy consists of two base operations find the best policy：yields the highest val acc on the target dataset the learned policy can transfer 论点 data augmentation to teach a model about invariance in data domain is easier than hardcoding it into model architecture currently dataset-specific and often do not transfer： MNIST：elastic distortions, scale, translation, and rotation CIFAR &amp; ImageNet：random cropping, image mirroring and color shifting / whitening GAN：直接生成图像，没有归纳policy we aim to automate the process of finding an effective data augmentation policy for a target dataset each policy： operations in certain order probabilities after applying magnitudes use reinforcement learning as the search algorithm contributions SOTA on CIFAR &amp; ImageNet &amp; SVHN new insight on transfer learning：使用预训练权重没有显著提升的dataset上，使用同样的aug policies则会涨点 方法 formulation search space of policies policy：a policy consists of 5 sub-policies sub-policy：each sub-policy consisting of two image operations operation：each operation is also associated with two hyperparameters probability：of applying the operation，uniformly discrete into 11 values magnitude：of the operation，uniformly discrete into 10 values a mini-batch share the same chosen sub-policy operations：16 in total，mainly use PIL https://blog.csdn.net/u011583927/article/details/104724419有各种operation的可视化效果 shear是砍掉图像一个角的畸变 equalize是直方图均衡化 solarize是基于一定阈值的invert，高于阈值invert，低于阈值不变 posterize也是一种像素值截断操作 color是调整饱和度，mag&lt;1趋近灰度图 sharpness决定图像模糊/锐化 sample pairing：两张图加权求和，但是不改变标签 searching goal with $(161011)^2$ choices of sub-policies we want 5 example 一个sub-policy包含两个operation 每个operation有一定的possibility做/不做 每个operation有一定的magnitude决定做后的效果 结论 On CIFAR-10, AutoAugment picks mostly color-based transformations on ImageNet, AutoAugment focus on color-based transformations as well, besides geometric transformation and rotate is commonly used one of the best policy overall results mixup: BEYOND EMPIRICAL RISK MINIMIZATION 动机 classification task memorization and sensitivity issue reduces the memorization of corrupt labels increases the robustness to adversarial examples improves the generalization can be used to stabilize the training of GANs propose convex combinations of pairs of examples and their labels 论点 ERM(Empirical Risk Minimization)：issue of generalization allows large neural networks to memorize (instead of generalize from) the training data even in the presence of strong regularization neural networks change their predictions drastically when evaluated on examples just outside the training distribution VRM(Vicinal Risk Minimization)：introduce data augmentation e.g. define the vicinity of one image as the set of its horizontal reflections, slight rotations, and mild scalings vicinity share the same class does not model the vicinity relation across examples of different classes ERM中的training set并不是数据的真实分布，只是用有限数据来近似真实分布，memorization也会最小化training error，但是对training seg以外的sample就leads to undesirable behaviour mixup就是VRM的一种，propose a generic vicinal distribution，补充vicinity relation across examples of different classes 方法 mixup constructs virtual training examples x = \lambda x_i + (1-\lambda)x_j \\ y = \lambda y_i + (1-\lambda)y_j use two examples drawn at random：raw inputs &amp; raw one-hot labels 理论基础：linear interpolations of feature vectors should lead to linear interpolations of the associated targets hyper-parameter $\alpha$ $\lambda = np.random.beta(\alpha, \alpha)$ controls the strength of interpolation 初步结论 three or more examples mixup does not provide further gain but more computation interpolating only between inputs with equal label did not lead to the performance gains key elemets——two inputs with different label vis decision boundaries有了一个线性过渡 更准确 &amp; 梯度更小：error少所以loss小所以梯度小？？ 实验 初步分类实验 $\alpha \in [0.1, 0.4]$ leads to improved performance，largers leads to underfitting models with higher capacities and/or longer training runs are the ones to benefit the most from mixup memorization of corrupted labels 将数据集中一部分label换成random noise ERM直接过拟合，在corrupted sample上面training error最小，测试集上test error最大 dropout有效防止过拟合，但是mixup outperforms它 corrupted label多的情况下，dropout+mixup performs the best robustness to adversarial examples Adversarial examples are obtained by adding tiny (visually imperceptible) perturbations 常规操作data augmentation：produce and train on adversarial examples add significant computational：样本数量增多，梯度变化大 mixup results in a smaller loss and gradient norm：因为mixup生成的假样本“更合理一点”，梯度变化更小 ablation study mixup is the best：绝对领先第二mix input + label smoothing the effect of regularization ERM需要大weight decay，mixup需要小的——说明mixup本身的regularization effects更强 高层特征mixup需要更大的weight decay——随着层数加深regularization effects减弱 AC+RP最强 label smoothing和add Gaussian noise to inputs 相对比较弱 mix inputs only(SMOTE) shows no gain MixMatch: A Holistic Approach to Semi-Supervised Learning 动机 semi-supervised learning unify previous methods proposed mixmatch guessing low-entropy labels mixup labeled and unlabeled data useful for differentially private learning 论点 semi-supervised learning add a loss term computed on unlabeled data and encourages the model to generalize better to unseen data the loss term entropy minimization：decision boundary应该尽可能远离数据簇，因此prediction on unlabeled data也应该是high confidence consistency regularization：增强前后的unlabeled data输出分布一致 generic regularization：weight decay &amp; mixup MixMatch unified all above introduces a unified loss term for unlabeled data 方法 overview given：a batch of labeled examples $X$ and a batch of labeled examples $U$ augment+label guess：a batch of augmented labeled examples $X^{‘}$ and a batch of augmented labeled examples $U^{‘}$ compute：separate labeled and unlabeled loss terms $L_X$ and $L_U$ combine：weighted sum MixMatch data augmentation 常规augmentation 作用于每一个$x_b$和$u_b$ $u_b$做$K$次增强 label guessing 对增强的$K$个$u_b$分别预测，然后取平均 average class prediction sharpening reduce the entropy of the label distribution 拉高最大prediction，拉小其他的 $Sharpen (p, T)_i =\frac{p_i^{\frac{1}{T}}}{\sum^{N}_j p_j^{\frac{1}{T}}} $ $T$趋近于0的时候，processed label就接近one-hot了 mixup slightly modified form of mixup to make the generated sample being more closer to the original loss function labeled loss：typical cross-entropy loss unlabeled loss：squared L2，bounded and less sensitive to completely incorrect predictions hyperparameters sharpening temperature $T$：fixed 0.5 number of unlabeled augmentations $K$：fixed 2 MixUp Beta parameter $\alpha$：0.75 for start unsupervised loss weight $\lambda_U$：100 for start Algorithm 实验 [mosaic] from YOLOv4]]></content>
      <tags>
        <tag>数据增强，样本不平衡，半监督学习，度量学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hrnet]]></title>
    <url>%2F2020%2F09%2F18%2Fhrnet%2F</url>
    <content type="text"><![CDATA[papers [v1 2019] Deep High-Resolution Representation Learning for Human Pose Estimation：base HRNet，提出parallel multi-resolution subnetworks，highest resolution output作为输出 [v2 2019] High-Resolution Representations for Labeling Pixels and Regions：simple modification，在末端输出的时候加了一步融合，将所有resolution-level的feature上采样到output-level然后concat Deep High-Resolution Representation Learning for Human Pose Estimation 动机 human pose estimation high-resolution representations through existing methods recover high-res feature from the low，大多数方法是recover系 this methods maintain the high-res from start to the end，本文是maintaining系 add high-to-low resolution subnetworks repeated multi-scale fusions more accurate and spatially more precise estimate on the high-res output，最后的high-res representation作为输出，接各种task heads 论点 in parallel rather than in series：potentially spatially more precise，相比较于recover类的架构，不会导致过多的spatial resolution loss，recover类的架构有时会用空洞卷积来维持resolution来降低spatial resolution loss repeated multi- scale fusions：boost both high&amp;low representations，more accurate pose estimation probabilistic graphical model regression heatmap High-to-low and low-to-high frameworks Symmetric high-to-low and low-to-high：Hourglass Heavy high-to-low and light low-to-high：ResNet back + simple bilinear upsampling Heavy high-to-low with dilated convolutions and further lighter low-to-high：ResNet with atrous conv + fewer bilinear upsampling high-to-low part和low-to-hight part：有对称和不对称两种，对称就如Hourglass，不对称就是down-path使用heavy classification backboens，up-path使用轻量的上采样 fusion： a和b都有skip-connections，将down-path和up-path的特征融合，目的是融合low-level和high-level的特征 a里面还有不同resolution level的融合 fusion方式有sum/concat refinenet：也就是up-path，可以用upSampling/transpose convs 方法 task description human pose estimation = keypoint detection detect K keypoints from an Image (H,W,3) state-of-the art methods：predict K heatmaps，each indicates one of the keypoint a stem with 2 strided conv a body outputting features with the same input resolution a regressor estimating heatmaps we focus on the design of the main body sequential &amp; parallel multi-resolution networks notation：$N_{sr}$ s is the stage r is the resolution index，denotes $\frac{1}{2^{r-1}}$ of the resolution of the first subnetwork sequential parallel overview four stages channels double when halve the res 1st stage 第一个stage是一个high-resolution subnetwork，没有下采样，没有parallel分支 4 residual units，bottleneck resblock width=64 3x3 conv reducing width to C 2、3、4 stages 接下来的stage gradually add high-to-low subnetwork 是multi-resolution subnetworks 每个subnetwork都比前一个多一个extra lower one resolution contain 1, 4, 3 exchange blocks respectively exchange block conv：4 residual units，two 3x3 conv exchange unit width C：width of the high-resolution subnetworks in last three stages other three parallel subnetworks HRNet-W32：64, 128, 256 HRNet-W48：96, 192, 384 repeated multi-scale fusion exchange blocks：每个high-to-low subnetwork包含多个parallel分支，每条path称为exchange block，每个exchange block包含一系列3-conv-units + a exchange unit 3-conv-units：堆叠卷积核，提取特征，加深网络 exchange unit：交换不同resolution level的信息 notations：一系列输入$\{X_1,X_2, …, X_r\}$，一系列输出$\{Y_1,Y_2, …, Y_r\}$，如果跨stage还有一个$Y_{r+1}$ 每个$Y_k$都是一个aggregation of the input maps：$Y_k=\sum^s_i a(X_i,k)$ i&lt;k：需要下采样，每下采样一倍都是一个stride2-3x3-conv i=k：identify connection i&gt;k：需要上采样，nearest neighbor upsamp + 1x1-align-conv k=$r+1$：需要在$Y_r$的基础上，在执行一次stride2-3x3-conv下采样得到 fusion：sum，所以上/下采样都需要通道对齐，输出map和对应level的输入map保持尺寸不变 heatmap estimation from the last high-res exchange unit mse gt gassian map：std=1 network instantiation stem + 4 stages 每个new stage input：res halved and channel doubled stem 两个s2-conv-bn-relu，channel 64 first stage： 使用和ResNet-50中一样的4个residual units，channel 64 然后用一个3x3-conv调整channel到一个起始channel C 2/3/4 stage 堆叠exchange blocks，分别有1/4/3个exchange block 每个exchange block使用4个residual units和1个exchange unit 也就是总共有8次multi-scale fusion channel C/2C/4C HRNet-32：C=64 HRNet-48：C=96 HRNet v2: High-Resolution Representations for Labeling Pixels and Regions 动机 High-resolution representation很重要 HRNet v1已经有不错的结果 a further study on high resolution representations a small modification：之前只关注high-resolution representations，现在关注所有level的output representations 论点 获得high resolution representation的两大方式 recover系：先下采样，然后用low-resolution重建，Hourglass，U-net，encoder-decoder maintain系：始终保留high-resolution的representation，同时不断用parallel low-resolution representations来strengthen，HRNet HRNet maintains high-resolution representations connecting high-to-low resolution convolutions in parallel repeatedly conducting multi-scale fusions across levels 简单来说，就是在每个阶段，保留现有resolution level，同时 不仅representation足够强大（融合了low-level high semantic info），还spatially precise our modification HRNetV2 HRNet 里面我们只关注最上面的high-resolution representation HRNet V2里面我们探索所有high-to-low parallel paths上面的representations 在语意分割任务中我们使用output high resolution representations来生成heatmaps 在检测任务中我们将multi-level的representations给到FastRCNN 方法 Architecture multi-resolution block multi-resolution group convolution：在每个representation level分别执行分组卷积，deeper multi-resolution convolution：发生在所有representation level上 下采样：stride-2 3x3 conv 上采样：bilinear /nearest neighbor Modification HRNetV1：只把最后一个阶段 highest resolution的representation作为输出 HRNetV2：最后一个阶段，每个resolution level的representations都上采样到highest，然后concat作为输出，甚至还将这个输出进一步下采样得到feature pyramid HRNet for classification：也可以反向操作，将最后一个阶段每个resolution level的representations都下采样到lowest，然后sum，最后output 2048-dim representation is fed into the classifier 实验]]></content>
      <tags>
        <tag>高分辨率，人体姿态估计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bilinear CNN]]></title>
    <url>%2F2020%2F09%2F18%2Fbilinear-CNN%2F</url>
    <content type="text"><![CDATA[17年的paper，引用量15，提出了网路结构，但是没分析为啥有效，垃圾 Bilinear CNNs for Fine-grained Visual Recognition 动机 fine-grained classification propose a pooled outer product of features derived from two CNNs 2 CNNs a bilinear layer a pooling layer outperform existing models and fairly efficient effective at other image classification tasks such as material, texture, and scene recognition 论点 fine-grained classification tasks require recognition of highly localized attributes of objects while being invariant to their pose and location in the image previous techniques part-based models construct representations by localizing parts more accurate but requires part annotations holistic models construct a representation of the entire image texture descriptors：FV，SIFT STN：augment CNNs with parameterized image transformations attention：use segmentation as a weakly-supervised manner Our key insight is that several widely-used texture representations can be written as a pooled outer product of two suitably designed features several widely-used texture representations two suitably designed features the bilinear features are highly redundant dimensionality reduction trade-off between accuracy We also found that feature normalization and domain-specific fine-tuning offers additional benefits combination concatenate：additional parameters to fuse an outer product：no parameters sum product：can achieve similar approximations “two-stream” architectures one used to model two- factor variations such as “style” and “content” for images in our case is to model two factor variations in location and appearance of parts：但并不是explicit modeling因为最终是个分类头 one used to analyze videos modeling the temporal aspect and the spatial aspect dimension reduction two 512-dim feature results in 512x512-dim earlier work projects one feature to a lower-dimensional space, e.g. 64-dim—&gt;512x64-dim we use compact bilinear pooling to generate low-dimensional embeddings (8-32x) 方法 architecture input $(l,I)$：takes an image and a location，location generally contains position and scale quadruple $B=(f_A, f_B, P, C)$ A、B两个CNN：conv+pooling layers， P：pooling function combined A&amp;B outputs using the matrix outer product average pooling C：logistic regression or linear SVM we found that linear models are effective on top of bilinear features CNN independent／partial shared／fully shared bilinear combination for each location $bilinear(l,I,f_A,f_B)=f_A(l,I)^T f_B(l,I)$ pooling function combines bilinear features across all locations $\Phi (I) = \sum_{l\in L} bilinear(l,I,f_A,f_B)$ same feature dimension K for A &amp; B，e.g. KxM &amp; KxN respectively，$\Phi(I)$ is size MxN Normalization a signed square root：$y=sign(x)\sqrt {|x|}$ follow a l2 norm：$z = \frac{y}{||y||_2}$ improves performance in practice classification logistic regression or linear SVM we found that linear models are effective on top of bilinear features back propagation $\frac{dl}{dA}=B(\frac{dl}{dx})^T$，$\frac{dl}{dB}=A(\frac{dl}{dx})^T$ Relation to classical texture representations：放在这一节撑篇幅？？ texture representations can be defined by the choice of the local features, the encoding function, the pooling function, and the normalization function choice of local features：orderless aggregation with sum／max operation encoding function：A non-linear encoding is typically applied to the local feature before aggregation normalization：normalization of the aggregated feature is done to increase invariance end-to-end trainable]]></content>
      <tags>
        <tag>细粒度，特征融合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[label smoothing]]></title>
    <url>%2F2020%2F09%2F14%2Flabel-smoothing%2F</url>
    <content type="text"><![CDATA[动机 to understand label smoothing improving generalization improves model calibration changes the representations learned by the penultimate layer of the network effect on knowledge distillation of a student network soft targets：a hard target and the uniform distribution of other classes 论点 label smoothing implicitly calibrates the learned models 能让confidences更有解释性——more aligned with the accuracies of their predictions label smoothing impairs distillation——teacher用了label smoothing，student会表现变差，this adverse effect results from loss of information in the digits 方法 modeling penultimate layer：fc with activation $p_k = \frac{e^{wx}}{\sum e^{wx}}$ outputs：loss $H(y,p)=\sum_{k=1}^K -y_klog(p_k)$ hard targets：$y_k$ is 1 for the correct class and 0 for the rest label smoothing：$y_k^{LS} = y_k(1-\alpha)+ \alpha /K$ visualization schem 将dimK activation vector投影到正交平面上，a dim2 vector per example clusters are much tighter because label smoothing encourages that each example in training set to be equidistant from all the other class’s templates 3 classes shows triangle structure since ‘equidistant’ predictions‘ absolute values are much bigger without LM, representing over-confident semantically similar classes are harder to separate，但是总体上cluster形态还是好一点 training without label smoothing there is continuous degree of change between two semantically similar classes，用了LM以后就观察不到了——相似class之间的语义相关性被破坏了，’erasure of information’ have similar accuracies despite qualitatively different clustering，对分类精度的提升不明显，但是从cluster形态上看更好看 model calibration making the confidence of its predictions more accurately represent their accuracy metric：expected calibration error (ECE) reliability diagram better calibration compared to the unscaled network Despite trying to collapse the training examples to tiny clusters, these networks generalize and are calibrated：在训练集上的cluster分布非常紧凑，encourage每个样本都和其他类别的cluster保持相同的距离，但是在测试集上，样本的分布就比较松散了，不会限定在小小的一坨内，说明网络没有over-confident，representing the full range of confidences for each prediction knowledge distillation even when label smoothing improves the accuracy of the teacher network, teachers trained with label smoothing produce inferior student networks As the representations collapse to small clusters of points, much of the information that could have helped distinguish examples is lost 看training set的scatter，LM会倾向于将一类sample集中成为相似的表征，sample之间的差异性信息丢了：Therefore a teacher with better accuracy is not necessarily the one that distills better]]></content>
      <tags>
        <tag>分类，loss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[noisy student]]></title>
    <url>%2F2020%2F09%2F11%2Fnoisy-student%2F</url>
    <content type="text"><![CDATA[Self-training with Noisy Student improves ImageNet classification 动机 semi-supervised learning（SSL） semi-supervised approach when labeled data is abundant use unlabeled images to improve SOTA model improve self-training and distillation accuracy and robustness better acc, mCE, mFR EfficientNet model on labeled images student even or larger student model on labeled &amp; pseudo labeled images noise, stochastic depth, data augmentation generalizes better process iteration by putting back the student as the teacher 论点 supervised learning which requires a large corpus of labeled images to work well robustness noisy data：unlabeled images that do not belong to any category in ImageNet large margins on much harder test sets training process teacher EfficientNet model on labeled images student even or larger student model on labeled &amp; pseudo labeled images noise, stochastic depth, data augmentation generalizes better process iteration by putting back the student as the teacher improve in two ways it makes the student larger：因为用了更多数据 noised student is forced to learn harder：因为label有pseudo labels，input有各类augmentation，网络有dropout／stochastic depth main difference compared with Knowledge Distillation use noise ——— KD do not use use equal/larger student ——— KD use smaller student to learn faster think of as Knowledge Expansion giving the student model enough capacity and difficult environments want the student to be better than the teacher 方法 algorithm train teacher use labeled images use teacher to inference unlabedled images, generating pseudo labels, soft/one-hot train student model use labeled &amp; unlabeld images make student the new teacher, jump to the inter step noise enforcing invariances：要求student网络能够对各种增强后的数据预测label一样，ensure consistency required to mimic a more powerful ensemble model：teacher网络在inference阶段进行dropout和stochastic depth，behaves like an ensemble，whereas the student behaves like a single model，这就push student网络去学习一个更强大的模型 other techniques data filtering we filter images that the teacher model has low confidences 这部分data与training data的分布范围内 data balancing duplicate images in classes where there are not enough images take the images with the highest confidence when there are too many soft／hard pseudo labels both work soft slightly better 实验 dataset benchmarked dataset：ImageNet 2012 ILSVRC unlabeled dataset：JFT fillter &amp; balancing： use EfficientNet-B0 trained on ImageNet，inference over JFT take images with confidence over 0.3 130M at most per class models EfficientNet-L2 further scale up EfficientNet-B7 wider &amp; deeper lower resolution train-test resolution discrepancy first perform normal training with a smaller resolution for 350 epochs then finetune the model with a larger resolution for 1.5 epochs on unaugmented labeled images shallow layers are fixed during finetuning noise stochastic depth：stochastic depth 0.8 for the final layer and follow the linear decay rule for other layers dropout：dropout 0.5 for the final layer RandAugment：two random operations with magnitude set to 27 iterative training 【teacher】first trained an EfficientNet-B7 on ImageNet 【student】then trained an EfficientNet-L2 with the unlabeled batch size set to 14 times the labeled batch size 【new teacher】trained a new EfficientNet-L2 【new student】trained an EfficientNet-L2 with the unlabeled batch size set to 28 times the labeled batch size 【iteration】… robustness test difficult images common corruptions and perturbations FGSM attack metrics improves the top-1 accuracy reduces mean corruption error (mCE) reduces mean flip rate (mFR) ablation study noisy 如果不noise the student，当student model的预测和teacher预测的unlabeled数据完全一样的情况下，loss为0，不再学习，这样student就不能outperform teacher了 injecting noise to the student model enables the teacher and the student to make different predictions The student performance consistently drops with noise function removed removing noise leads to a smaller drop in training loss，说明noise的作用不是为了preventing overfitting，就是为了enhance model iteration iterative training is effective in producing increas- ingly better models larger batch size ratio for latter iteration]]></content>
      <tags>
        <tag>classification, semi-supervised, teacher-student</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[complement cross entropy]]></title>
    <url>%2F2020%2F09%2F08%2Fcomplement-cross-entropy%2F</url>
    <content type="text"><![CDATA[summary 使用complement loss的主要动机是one-hot的label下，ce只关注拉高正样本概率，丧失掉了其他incorrect类别的信息 事实上对于incorrect类别，可以让其输出概率值分布的熵尽可能的大——也就是将这个分布尽可能推向均匀分布，让它们之间互相遏制从而凸显出ground truth的概率 但这是建立在“各个标签之间相互独立”这个假设上，如果类别间有hierarchical的关系／multi-label，就不行了。 在数学表达上， 首先仍然是用ce作用于correct label，希望正样本概率gt_pred尽可能提高，接近真实值 然后是作用于incorrect label的cce，在除了正例pred possibility以外的几个概率上，计算交叉熵，希望这几个概率尽可能服从均匀分布，概率接近$\frac{1-gt_pred}{K-1}$ 我感觉这就是label smoothing，主要区别就是cce上有个norm项，label smoothin在计算ce的时候，vector中每一个incorrect label的熵都与correct label等权重，cce对整个incorrect vector的权重与correct label等同，且可以调整。 Imbalanced Image Classification with Complement Cross Entropy 动机 class-balanced datasets motivated by COT(complement objective training) suppressing softmax probabilities on incorrect classes during training propose cce keep ground truth probability overwhelm the other classes neutralizing predicted probabilities on incorrect classes 论点 class imbalace limits generalization resample oversampling on minority classes undersampling on majority classes reweight neglect the fact that samples on minority classes may have noise or false annotations might cause poor generalization observed degradation in imbalanced datasets using CE cross entropy mostly ignores output scores on wrong classes neutralizing predicted probabilities on incorrect classes helps improve accuracy of prediction for imbalanced image classification 方法 complement entropy calculated on incorrect classes N samples，K-dims class vector $C(y,\hat y)=-\frac{1}{N}\sum_{i=1}^N\sum_{j=1,j \neq g}^K \frac{\hat y^j}{1-\hat y^g}log\frac{\hat y^j}{1-\hat y^g} $ the purpose is to encourage larger gap between ground truth and other classes —— when the incorrect classes obey normal distribution it reaches optimal balanced complement entropy add balancing factor $C^{‘}(y,\hat y) = \frac{1}{K-1}C(y,\hat y)$ forming COT： twice back-propagation per each iteration first cross entropy second complement entropy CCE (Complement Cross Entropy) add modulating factor：$\tilde C(y, \hat y) = \frac{\gamma}{K-1}C(y, \hat y)$，$\gamma=-1$ combination：CE+CCE 实验 dataset： cifar class-balanced originally construct imbalanced variants with imbalance ratio $\frac{N_{min}}{N_{max}}$ test acc 论文的实验结果都是在cifar上cce好于cot好于focal loss，在road上cce好于cot，没放fl 咱也不知道。。。]]></content>
      <tags>
        <tag>loss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[regression loss]]></title>
    <url>%2F2020%2F09%2F07%2Fregression-loss%2F</url>
    <content type="text"><![CDATA[损失函数用来评价模型预测值和真实值的不一样程度 两系损失函数： 绝对值loss $L(Y,f(x))=|Y-f(x)|$ 平均绝对值损失，MAE，L1 对异常点有更好的鲁棒性 更新的梯度始终相同，对于很小的损失值，梯度也很大，不利于模型学习——手动衰减学习率 平方差loss $L(Y, f(x)) = (Y-f(x))^2$ 均方误差损失，MSE，L2 因为取了平方，会赋予异常点更大的权重，会以牺牲其他样本的误差为代价，朝着减小异常点误差的方向更新，降低模型的整体性能 Huber loss $L = \begin{cases} \frac{1}{2}(y-f(x))^2,\text{ for }|y-f(x)|&lt;\delta,\\ \delta |y-f(x)|-\frac{1}{2}\delta^2, \text{ otherwise} \end{cases} $ 超参决定了对与异常点的定义，只对较小的异常值敏感 对数loss L(Y, P(Y|X)) = -log(P(Y|X)) cross-entropy loss 二分类双边计算： L = ylna + (1-y)ln(1-a) 多分类单边计算： L = ylna 指数loss L(Y, f(x)) = exp[-yf(x)] Hinge loss L(Y, f(x)) = max(0, 1-yf(x)) perceptron loss L(Y, f(x)) = max(0, -yf(x)) cross-entropy loss 二分类双边计算： L = ylna + (1-y)ln(1-a) 多分类单边计算： L = ylna]]></content>
      <tags>
        <tag>回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pseudo-3d]]></title>
    <url>%2F2020%2F09%2F02%2Fpseudo-3d%2F</url>
    <content type="text"><![CDATA[[3d resnet] Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition：真3d，for comparison，分类 [C3d] Learning Spatiotemporal Features with 3D Convolutional Networks：真3d，for comparison，分类 [Pseudo-3D resnet] Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks：伪3d，resblock，S和T花式连接，分类 [2.5d Unet] Automatic Segmentation of Vestibular Schwannoma from T2-Weighted MRI by Deep Spatial Attention with Hardness-Weighted Loss：patch输入，先2d后3d，针对各向异性，分割 [two-pathway U-Net] Combining analysis of multi-parametric MR images into a convolutional neural network: Precise target delineation for vestibular schwannoma treatment planning：patch输入，3d网络，xy和z平面分别conv &amp; concat，分割 [Projection-Based 2.5D U-net] Projection-Based 2.5D U-net Architecture for Fast Volumetric Segmentation：mip，2d网络，分割，重建 [New 2.5D Representation] A New 2.5D Representation for Lymph Node Detection using Random Sets of Deep Convolutional Neural Network Observations：横冠矢三个平面作为三个channel输入，2d网络，检测 Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks 动机 spatio-temporal video the development of a very deep 3D CNN from scratch results in expensive computational cost and memory demand new framework 1x3x3 &amp; 3x1x1 Pseudo-3D Residual Net which exploits all the variants of blocks outperforms 3D CNN and frame-based 2D CNN 论点 3d CNN的model size：making it extremely difficult to train a very deep model fine-tuning 2d 好于 train from scrach 3d RNN builds only the temporal connections on the high-level features，leaving the correlations in the low-level forms not fully exploited we propose 1x3x3 &amp; 3x1x1 in parallel or cascaded 其中的3x3 conv可以用2d conv来初始化 a family of bottleneck building blocks：enhance the structural diversity 方法 P3D Blocks direct／indirect influence：S和T之间是串联还是并联 direct／indirect connected to the final output：S和T的输出是否直接与identity path相加 bottleneck： 头尾各接一个1x1x1的conv 头用来narrow channel，尾用来widen back 头有relu，尾没有relu Pseudo-3D ResNet mixing blocks：循环ABC better performance &amp; small increase in model size fine-tuning resnet50： randomly cropped 224x224 freeze all BN except for the first one add an extra dropout layer with 0.9 dropout rate further fine-tuning P3D resnet： initialize with r50 in last step randomly cropped 16x160x160 horizontally flipped mini-batch as 128 frames future work attention mechanism will be incorporated Projection-Based 2.5D U-net Architecture for Fast Volumetric Segmentation 动机 MIP：2D images containing information of the full 3D image faster, less memory, accurate 方法 2d unet MIP：$\alpha=36$ 3x3 conv, s2 pooling, transpose conv, concat, BN, relu, filters：begin with 32, end with 512 dropout：0.5 in the deepest convolutional block and 0.2 in the second deepest blocks 3d unet overfitting &amp; memory space filters：begin with 4, end with 16 dropout：0.5 in the deepest convolutional block and 0.4 in the second deepest blocks Projection-Based 2.5D U-net 2d slice：loss of connection 2d mip：disappointing results 2d volume：long training time the proposed 2.5D U-net： N(x) = T R_p F_p \left[ \begin{matrix} U M_{\alpha_1}(x) \\ ... \\ U M_{\alpha_p}(x) \end{matrix} \right] $M_{i}$：MIP，p=12 $U$：2d-Unet like above $F_p$：learnable filtration，1x3 conv，for each projection，抑制重建伪影 $R_p$：reconstruction operator $T$：fine-tuning operator，shift &amp; scale back to 0-1 mask Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition 动机 3D kernels tend to overfit 3D CNNs is relatively shallow propose a 3D CNNs based on ResNets better performance not overfit deeper than C3D 论点 two-stream architecture：consists of RGB and optical flow streams is often used to represent spatio-temporal information 3D CNNs：trained on relatively small video datasets performs worse than 2D CNNs pretrained on large datasets Very deep 3D CNNs：not explored yet due to training difficulty 方法 Network Architecture main difference：kernel dimensions stem：stride2 for S，stride1 for T resblock：conv_bn_relu&amp;conv + id identity shortcuts：use zero-padding for increasing dimensions，to avoid increasing the number of parameters stride2 conv：conv3_1、 conv4_1、 conv5_1 input clips：3x16x112x112 large learning rate and batch size was important 实验 在小数据集上3d-r18不如C3D，overfit了：shallow architecture of the C3D and pretraining on the Sports-1M dataset prevent the C3D from overfitting 在大数据集上3d-r34好于C3D，同时C3D的val acc明显高于train acc——太shallow欠拟合了，r34则表现更好，而且不需要预训练 RGB-I3D achieved the best performance 3d-r34是更deeper的 RGB-I3D用了更大的batch size：Large batch size is important to train good models with batch normalization High resolutions：3x64x224x224 Learning Spatiotemporal Features with 3D Convolutional Networks 动机 generic efficient simple 3d ConvNet with 3x3x3 conv &amp; a simple linear classifier 论点 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets 2D ConvNets lose temporal information of the input signal right after every convolution operation 2d conv在channel维度上权重都是一样的，相当于temporal dims上没有重要性特征提取 方法 basic network settings 5 conv layers + 5 pooling layers + 2 fc layers + softmax filters：[64，128，256，256，256] fc dims：[2048，2048] conv kernel：dx3x3 pooling kernel：2x2x2，s2 except for the first layer with the intention of not to merge the temporal signal too early also to satisfy the clip length of 16 frames varing settings temporal kernel depth homogeneous：depth-1/3/5/7 throughout varying：increasing-3-3-5-5-7 &amp; decreasing-7- 5-5-3-3 depth-3 throughout performs the best depth-1 is significantly worse We also verify that 3D ConvNet consistently performs better than 2D ConvNet on a large-scale internal dataset C3D 8 conv layers + 5 pooling layers + 2 fc layers + softmax homogeneous：3x3x3 s1 conv thtoughout pool1：1x2x2 kernel size &amp; stride，rest 2x2x2 fc dims：4096 C3D video descriptor：fc6 activations + L2-norm deconvolution visualizing： conv5b feature maps starts by focusing on appearance in the first few frames tracks the salient motion in the subsequent frames compactness PCA 压缩到50-100dim不太损失acc 压缩到10dim仍旧是最高acc projected to 2-dimensional space using t-SNE C3D features are semantically separable compared to Imagenet quantitatively observe that C3D is better than Imagenet Action Similarity Labeling predicting action similarity extract C3D features: prob, fc7, fc6, pool5 for each clip L2 normalization compute the 12 different distances for each feature：48 in total linear SVM is trained on these 48-dim feature vectors C3D significantly outperforms the others]]></content>
      <tags>
        <tag>3d CNN, 2.5d CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSD]]></title>
    <url>%2F2020%2F08%2F13%2FSSD%2F</url>
    <content type="text"><![CDATA[SSD: Single Shot MultiBox Detector 动机 single network speed &amp; accuracy 59 FPS / 74.3% mAP 论点 prev methods two-stage：生成稀疏的候选框，然后对候选框进行分类与回归 one-stage：均匀地在图片的不同位置，采用不同尺度和长宽比，进行密集抽样，然后利用CNN提取特征后直接进行分类与回归 fundamental speed improvement eliminating bounding box proposals eliminating feature resampling other improvements small convolutional filter for bbox categories and offsets（针对yolov1的全连接层说） separate predictors by aspect ratio multiple scales 这些操作都不是原创 The core of SSD is predicting category scores and box offsets for a fixed set of default bounding boxes using small convolutional filters applied to feature maps. 方法 Model Multi-scale feature maps for detection：采用了多尺度的特征图，逐渐用s2降维，大尺度特征图上有更多的单元，用来回归小物体 Convolutional predictors for detection：针对yolov1里面的fc层 Default boxes and aspect ratios：一个单元4种size的先验框，对每个先验框都预测一组4+(c+1)，其中的1可以看作背景类，也可以看做是有无目标的置信度，各用一个conv3x3的head backbone 参考：https://www.cnblogs.com/sddai/p/10206929.html VGG16前四个conv block保留 无dropout和fc conv5的池化由2x2-s2变成3x3-s1 conv6和conv7是3x3x1024和1x1x1024的空洞卷积，输出19x19x1024 conv8是1x1x256和3x3x512 s2的conv，输出10x10x512 conv9都是1x1x128和3x3x256 s2的conv，输出5x5x256 conv10、conv11都是1x1x128和3x3x256 s1 p0的conv，输出3x3x256、1x1x256 Training Matching strategy：match default box和gt box 首先为每一个gt box找到一个overlap最大的default box 然后找到所有与gt box的overlap大于0.5的default box 一个gt box可能对应多个default box 一个default box只能对应一个gt box（overlap最大的） Objective loss loc loss：smooth L1，offsets like Faster R-CNN cls loss：softmax loss weighted sum：$L = \frac{1}{N} (L_{cls} + \alpha L_{loc})$， N is the number of matched default boxes loss=0 when N=0 Choosing scales and aspect ratios for default boxes 每个level的feature map感受野不同，default box的尺寸也不同 数量也不同，conv4、conv10和conv11是4个，conv7、conv8、conv9是6个 ratio：{1,2,3,1/2,1/3}，4个的没有3和1/3 L2 normalization for conv4： $y_i = \frac{x_i}{\sqrt{\sum_{k=1}^n x_k^2}}$ 作用是将不同尺度的特征都归一化成模为1的向量 scale：可以是固定值，也可以是可学习参数 为啥只针对conv4？作者的另一篇paper(ParseNet)中发现conv4和其他层特征的scale是不一样的 predictions all default boxes with different scales and aspect ratio from all locations of many feature maps significant imbalance for positive/negative Hard negative mining sort using the highest confidence loss pick the top ones with n/p at most 3:1 faster optimization and a more stable training Data augmentation sample a patch with specific IoU resize 性质 much worse performance on smaller objects, increasing the input size can help improve Data augmentation is crucial, resulting in a 8.8% mAP improvement Atrous is faster, 保留pool5不变的话，the result is about the same while the speed is about 20% slower]]></content>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python多线程&多进程]]></title>
    <url>%2F2020%2F08%2F04%2Fpython%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E5%A4%9A%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Reference： https://www.cnblogs.com/kaituorensheng/p/4465768.html https://zhuanlan.zhihu.com/p/46368084 https://www.runoob.com/python3/python3-multithreading.html 名词 进程(process)和线程(thread) cpu在处理任务时，把时间分成若干个小时间段，这些时间段很小的，系统中有很多进程，每个进程中又包含很多线程，在同一时间段 内，电脑CPU只能处理一个线程，下一个时间段，可能又去执行别的线程了（时间片轮转，从而实现伪多任务），具体顺序取决于其调度逻辑 多核cpu可以实现真正的并行，同一个时刻每个cpu上都可以跑一个任务 多进程：每个进程分别执行指定任务，进程间互相独立，每个时刻并行的实际进程数取决于cpu数量 多线程：单个cpu同一时刻只能处理一个线程，一个任务可能由多个工人来完成，工人们相互协同，这则是多线程 python的多进程：multiprocess模块 python的多线程：threading模块 每个进程在执行过程中拥有独立的内存单元，而一个进程的多个线程在执行过程中共享内存。 多进程multiprocess 母进程：当我们执行一个python脚本，if main下面实际运行的主体就是母进程 子进程：我们使用multiprocess显式创建的进程，都是子进程 join()方法：用来让母进程阻塞，等待所有子进程执行完成再结束 使用multiprocess的多进程，可以通过process方法和pool方法 process方法：适用进程较少时候，无法批量开启/关闭 pool方法：批量管理 参数：输入参数都差不多，第一个是要执行的函数方法target/func，第二个是输入参数args 🌰Process方法： 1234567891011121314151617181920212223from multiprocessing import Processimport osimport timedef long_time_task(i): print('子进程: &#123;&#125; - 任务&#123;&#125;'.format(os.getpid(), i)) time.sleep(2) print("结果: &#123;&#125;".format(8 ** 20))if __name__=='__main__': print('当前母进程: &#123;&#125;'.format(os.getpid())) start = time.time() p1 = Process(target=long_time_task, args=(1,)) p2 = Process(target=long_time_task, args=(2,)) print('等待所有子进程完成。') p1.start() p2.start() p1.join() p2.join() end = time.time() print("总共用时&#123;&#125;秒".format((end - start))) process方法使用Process实例化一个进程对象，然后调用它的start方法开启进程 🌰Pool方法： 123456789101112131415161718192021222324252627282930from multiprocessing import Pool, cpu_countimport osimport timedef long_time_task(i): print('子进程: &#123;&#125; - 任务&#123;&#125;'.format(os.getpid(), i)) time.sleep(2) print("结果: &#123;&#125;".format(8 ** 20)) return True # 用于演示pool适用于有返回值if __name__=='__main__': print("CPU内核数:&#123;&#125;".format(cpu_count())) # 4 print('当前母进程: &#123;&#125;'.format(os.getpid())) start = time.time() p = Pool(4) results = [] for i in range(5): # p.apply_async(long_time_task, args=(i,)) results.append(p.apply_async(long_time_task, args=(i,))) print('等待所有子进程完成。') p.close() p.join() end = time.time() print("总共用时&#123;&#125;秒".format((send - start))) # 查看返回值 for res in results: print(res.get()) apply_async(func, args=(), kwds={}, callback=None)：向进程池提交需要执行的函数及参数，各个进程采用非阻塞（异步）的调用方式，即每个子进程只管运行自己的，不管其它进程是否已经完成。 close()：关闭进程池（pool），不再接受新的任务。 join()：主进程阻塞等待子进程的退出， 调用join()之前必须先调用close()或terminate()方法，使其不再接受新的Process。 多线程threading python的多线程是伪多线程，因为主进程只有一个，所以只用了单核，只是通过碎片化进程、调度、全局锁等操作，cpu利用率提升了 所以我想并行处理百万量级的数据入库操作时，多进程的效率明显高于多线程 【问题】从我观察上看多线程基本就是串行？？ 🌰threading 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import threadingimport timedef long_time_task(): print('当子线程: &#123;&#125;'.format(threading.current_thread().name)) time.sleep(2) print("结果: &#123;&#125;".format(8 ** 20))if __name__=='__main__': start = time.time() print('这是主线程：&#123;&#125;'.format(threading.current_thread().name)) for i in range(5): t = threading.Thread(target=long_time_task, args=()) t.setDaemon(True) t.start() t.join() end = time.time() print("总共用时&#123;&#125;秒".format((end - start))) # 继承&amp;有返回值的写法def long_time_task(i): time.sleep(2) return 8**20class MyThread(threading.Thread): def __init__(self, func, args , name='', ): threading.Thread.__init__(self) self.func = func self.args = args self.name = name self.result = None def run(self): print('开始子进程&#123;&#125;'.format(self.name)) self.result = self.func(self.args[0],) print("结果: &#123;&#125;".format(self.result)) print('结束子进程&#123;&#125;'.format(self.name)) def get_result(self): threading.Thread.join(self) # 等待线程执行完毕 return self.resultif __name__=='__main__': start = time.time() threads = [] for i in range(1, 3): t = MyThread(long_time_task, (i,), str(i)) threads.append(t) for t in threads: t.start() for t in threads: t.join() end = time.time() print("总共用时&#123;&#125;秒".format((end - start))) join方法：等待所有进程执行完，主进程再执行完 setDaemon(True)：主线程执行完就退出]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IoU]]></title>
    <url>%2F2020%2F08%2F03%2FIoU%2F</url>
    <content type="text"><![CDATA[reference: https://bbs.cvmart.net/articles/1396 IoU IoU = Intersection / Union $Loss_{IoU} = 1 - IoU$ [0,1] 无法直接优化没有重叠的部分：如果两个框没有交集，IoU=0，没有梯度回传，无法进行学习训练 尺度不敏感 无法精确的反映两者的重合质量 GIoU(Generalized Intersection over Union) $GIoU = IoU - \frac{|A_c - U|}{|A_c|}$，$A_c$是包含两个框的最小外接框 $Loss_{GIoU} = 1 - GIoU$ GIoU倾向于先增大bbox的大小来增大与GT的交集，然后通过IoU项引导最大化bbox的重叠区域 [-1,1]：对称区间 能够关注到非重合区域：引入了外接框C 尺度不敏感 两个框为包含关系时，退化为IoU 如果之间用来替换mse，前期收敛会比较慢 一般地，GIoU loss不能很好地收敛SOTA算法，反而造成不好的结果 DIoU (Distance-IoU) $DIoU = IoU - \frac{d^2}{c^2}$，d是两个中心点间的欧式距离，c是两个框的最小外接框的对角线距离 $Loss_{DIoU} = 1 - DIoU$ * 直接最小化两个目标框的距离，收敛快得多，而且稳定 * 也能够关注到非重合区域：引入外接对角线 * 对于包含关系的两个框，仍旧有距离损失，不会退化为IoU：因为中心点距离 * 可以替换NMS中的IoU：原始的IoU仅考虑了重叠区域，对包含的情况没有很好的处理 $$ score = score\text{ if }IoU - dis(box_{max}, box)&gt;\epsilon \text{, else } 0 $$ * 没有考虑形状（长宽比） CIoU (Complete-IoU) $CIoU = IoU - \frac{d^2}{c^2}-av$，在DIoU的基础上新增了惩罚项av，a是权重系数，v用来评价长宽比： v = \frac{4}{\pi ^ 2}(arctan\frac{w^{gt}}{h^{gt}}-arctan\frac{w}{h})^2\\ a = \frac{v}{1-IoU+v} $Loss_{CIoU} = 1 - CIoU$ v的梯度中有$\frac{1}{w^2+h^2}$，长宽在[0,1]之间，可能很小，会导致梯度爆炸，用的时候 clamp一下上下限 分母中的$w^2+h^2$替换成1 \frac{\partial v }{\partial w} = \frac{8}{\pi ^ 2}(arctan\frac{w^{gt}}{h^{gt}}-arctan\frac{w}{h})\frac{h}{w^2+h^2}\\ \frac{\partial v }{\partial w} = \frac{8}{\pi ^ 2}(arctan\frac{w^{gt}}{h^{gt}}-arctan\frac{w}{h})\frac{w}{w^2+h^2}]]></content>
      <tags>
        <tag>目标检测，loss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YOLOACT]]></title>
    <url>%2F2020%2F07%2F17%2FYOLOACT%2F</url>
    <content type="text"><![CDATA[[YOLACT] Real-time Instance Segmentation：33 FPS/30 mAP [YOLACT++] Better Real-time Instance Segmentation：33.5 FPS/34.1 mAP YOLACT: Real-time Instance Segmentation 动机 create a real-time instance segmentation base on fast, one-stage detection model forgoes an explicit localization step (e.g., feature repooling) doesn’t depend on repooling (RoI Pooling) produces very high-quality masks set two parallel subtasks prototypes——conv mask coefficients——fc 之后将模板mask和实例mask系数进行线性组合来获得实例的mask ‘prototypes’: vocabulary fully-convolutional localization is still translation variant Fast NMS 论点 State-of-the-art approaches to instance segmentation like Mask R- CNN and FCIS directly build off of advances in object detection like Faster R-CNNand R-FCN focus primarily on performance over speed these methods “re-pool” features in some bounding box region inherently sequential therefore difficult to accelerate One-stage instance segmentation methods generate position sensitive maps still require repooling or other non-trivial computations prototypes related works use prototypes to represent features (Bag of Feature) we use them to assemble masks for instance segmentation we learn prototypes that are specific to each image, rather than global prototypes shared across the entire dataset Bag of Feature BOF假设图像相当于一个文本，图像中的不同局部区域或特征可以看作是构成图像的词汇(codebook) 所有的样本共享一份词汇本，针对每个图像，统计每个单词的频次，即可得到图片的特征向量 方法 parallel tasks The first branch uses an FCN to produce a set of image-sized “prototype masks” that do not depend on any one instance. The second adds an extra head to the object detection branch to predict a vector of “mask coefficients” for each anchor that encode an instance’s rep- resentation in the prototype space. linearly combining Rationale masks are spatially coherent：mash是空间相关的，相邻像素很可能是一类 卷积层能够利用到这种空间相关性，但是fc层不能 而one-stage检测器的检测头通常是fc层？？ making use of fc layers, which are good at producing semantic vectors and conv layers, which are good at producing spatially coherent masks Prototype 在backbone feature layer P3上接一个FCN taking protonet from deeper backbone features produces more robust masks higher resolution prototypes result in both higher quality masks and better performance on smaller objects upsample到x4的尺度to increase performance on small objects head包含k个channels 梯度回传来源于最终的final assembled mask，不是当前这个头 unbounded：ReLU or no nonlinearity We choose ReLU for more interpretable prototypes Mask Coefficients a third branch in parallel with detection heads nonlinearity：要有正负，所以tanh Mask Assembly linear combination + sigmoid: $M=\sigma(PC^T)$ loss cls loss：w=1, 和ssd一样，c+1 softmax box reg loss：w=1.5, 和ssd一样，smooth-L1 mask loss：w=6.125， BCE crop mask eval：用predict box去crop train：用gt box去crop，同时还要给mask loss除以gt box的面积，to preserve small objects Emergent Behavior 不crop也能分割中大目标： YOLACT learns how to localize instances on its own via different activations in its prototypes 而不是靠定位结果 translation variant the consistent rim of padding in modern FCNs like ResNet gives the network the ability to tell how far away from the image’s edge a pixel is，所以用一张纯色的图能够看出kernel实际highlight的是哪部分特征 同一种kernel，同一种五角星，在画面不同位置，对应的响应值是不同的，说明fcn是能够提取物体位置这样的语义信息的 prototypes are compressible： 增加模版数目反而不太有效，because predicting coefficients is difficult， the network has to play a balancing act to produce the right coef- ficients, and adding more prototypes makes this harder, We choose 32 for its mix of performance and speed Network speed as well as feature richness backbone参考RetinaNet，ResNet-101 + FPN 550x550 input，resize 去掉P2，add P6&amp;P7 3 anchors per level，[1, 1/2, 2] P3的anchor尺寸是24x24，接下来每层double the scale 检测头：shared conv+parallel conv OHEM single GPU：batch size 8 using ImageNet weights，no extra bn layers Fast NMS 构造cxnxn的矩阵，c代表每个class 然后搞成上三角，求column-wise max 再IoU threshold 15.0 ms faster with a performance loss of 0.3 mAP Semantic Segmentation Loss using modules not executed at test time P3上1x1 conv，sigmoid and c channels w=1 +0.4 mAP boost YOLACT++: Better Real-time Instance Segmentation]]></content>
      <tags>
        <tag>实例分割</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cornerNet]]></title>
    <url>%2F2020%2F07%2F17%2FcornerNet%2F</url>
    <content type="text"><![CDATA[CornerNet: Detecting Objects as Paired Keypoints 动机 corner formulation top-left corner bottom-right corner anchor-free corner pooling no multi-scale 论点 anchor box drawbacks huge set of anchors boxes to ensure sufficient overlap，cause huge imbalance hyperparameters and design choices cornerNet detect and group heatmap to predict corners 从数学表达上看，全图wh个tl corner，wh个bt corner，可以表达wwhh个框 anchor-based，全图wh个中心点，9个anchor size，只能表达有限的框，且可能match不上 embeddings to group pairs of corners corner pooling better localize corners which are usually out of the foreground modifid hourglass architecture add our novel variant of focal loss 方法 two prediction modules heatmaps C channels, C for number of categories binary mask each corner has only one ground-truth positive penalty the neighbored negatives within a radius that still hold high iou (0.3 iou) determine the radius penalty reduction $=e^{-\frac{x^2+y^2}{2\sigma^2}}$ variant focal loss L_{det} = \frac{-1}{N} \sum^C \sum^H \sum^W \begin{cases} (1-p_{i,j})^\alpha log(p_{i,j}), \ \ if y_{ij}=1\\ (1-y_{ij})^\beta (p_{i,j})^\alpha log(1-p_{i,j}), \ \ otherwise \end{cases} $\alpha=2, \beta=4$ N is the number of gts embeddings associative embedding use 1-dimension embedding pull and push loss on gt positives $L_{pull} = \frac{1}{N} \sum^N [(e_{tk}-e_k)^2 + (e_{bk}-e_k)^2]$ $L_{push} = \frac{1}{N(N-1)} \sum_j^N\sum_{k\neq j}^N max(0, \Delta -|e_k-e_j|)$ $e_k$ is the average of $e_{tk}$ and $e{bk}$ $\Delta$ = 1 offsets 从heatmap resolution remapping到origin resolution存在精度损失 o_k = （\frac{x_k}{n} - \lfloor \frac{x_k}{n} \rfloor， \frac{y_k}{n} - \lfloor \frac{y_k}{n} \rfloor） greatly affect the IoU of small bounding boxes shared among all categories smooth L1 loss on gt positives $$ L_{off} = \frac{1}{N} \sum^N SmoothL1(o_k, \hat o_k) $$ corner pooling top-left pooling layer： * 从当前点(i,j)开始， * 向下elementwise max所有feature vecor，得到$t_{i,j}$ * 向右elementwise max所有feature vecor，得到$l_{i,j}$ * 最后两个vector相加 bottom-right corner：向左向上 Hourglass Network hourglass modules series of convolution and max pooling layers series of upsampling and convolution layers skip layers multiple hourglass modules stacked：reprocess the features to capture higher-level information intermediate supervision 常规的中继监督： 下一级hourglass module的输入包括三个部分 前一级输入 前一级输出 中继监督的输出 本文使用了中继监督，但是没把这个结果加回去 hourglass2 input：1x1 conv-BN to both input and output of hourglass1 + add + relu Our backbone 2 hourglasses 5 times downsamp with channels [256,384,384,384,512] use stride2 conv instead of max-pooling upsamp：2 residual modules + nearest neighbor upsampling skip connection: 2 residual modules，add mid connection: 4 residual modules stem: 7x7 stride2, ch128 + residual stride2, ch256 hourglass2 input：1x1 conv-BN to both input and output of hourglass1 + add + relu 实验 training details randomly initialized, no pretrained bias：set the biases in the convolution layers that predict the corner heatmaps input：511x511 output：128x128 apply PCA to the input image full loss：$L = L_{det} + \alpha L_{pull} + \beta L_{push} + \gamma L_{off}$ 配对loss：$\alpha=\beta=0.1$ offset loss：$\gamma=1$ batch size = 49 = 4+5x9 test details NMS：3x3 max pooling on heatmaps pick：top100 top-left corners &amp; top100 bottom-right corners filter pairs： L1 distance greater than 0.5 from different categories fusion：combine the detections from the original and flipped images + soft nms Ablation Study corner pooling is especially helpful for medium and large objects penalty reduction especially benefits medium and large objects CornerNet achieves a much higher AP at 0.9 IoU than other detectors：更有能力生成高质量框 error analysis：the main bottleneck is detecting corners CornerNet-Lite: Efficient Keypoint-Based Object Detection 动机 keypoint-based methods detecting and grouping accuary but with processing cost propose CornerNet-Lite CornerNet-Saccade：attention mechanism CornerNet-Squeeze：a new compact backbone performance 论点 main drawback of cornerNet inference speed reducing the number of scales or the image resolution cause a large accuracy drop two orthogonal directions reduce the number of pixels to process：CornerNet-Saccade reduce the amount of processing per pixel： CornerNet-Saccade downsized attention map select a subset of crops to examine in high resolution for off-line：AP of 43.2% at 190ms per image CornerNet-Squeeze inspired by squeezeNet and mobileNet 1x1 convs bottleneck layers depth-wise separable convolution for real-time：AP of 34.4% at 30ms combined?? CornerNet-Squeeze-Saccade turns out slower and less accurate than CornerNet- Squeeze Saccades：扫视 to generate interesting crops RCNN系列：single-type &amp; single object AutoFocus：add a branch调用faster-RCNN，thus multi-type &amp; mixed-objects，有single branch有multi branch CornerNet-Saccade： single-type &amp; multi object crops can be much smaller than number of objects 方法 CornerNet-Saccade step1：obtain possible locations downsize：two scales，255 &amp; 192，zero-padding predicts 3 attention maps small object：longer side&lt;32 pixels medium object：32-96 large object：&gt;96 so that we can control the zoom-in factor：zoom-in more for smaller objects feature map：different scales from the upsampling layers attention map：3x3 conv-relu + 1x1 conv-sigmoid process locations where scores &gt; 0.3 step2：finer detection zoom-in scales：4，2，1 for small、medium、large objects apply CornerNet-Saccade on the ROI 255x255 window centered at the location step3：NMS soft-nms remove the bounding boxes which touch the crop boundary CornerNet-Saccade uses the same network for attention maps and bounding boxes 在第一步的时候，对一些大目标已经有了检测框 也要zoom-in，矫正一下 efficiency regions/croped images都是processed in batch/parallel resize/crop操作在GPU中实现 suppress redundant regions using a NMS-similar policy before prediction new hourglass backbone 3 hourglass module，depth 54 downsize twice before hourglass modules downsize 3 times in each module，with channels [384,384,512] one residual in both encoding path &amp; skip connection mid connection：one residual，with channels 512 CornerNet-Squeeze to replace the heavy hourglass104 use fire module to replace residuals downsizes 3 times before hourglass modules downsize 4 times in each module replace the 3x3 conv in prediction head with 1x1 conv replace the nearest neighboor upsampling with 4x4 transpose conv]]></content>
      <tags>
        <tag>目标检测，anchor-free</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SOLO]]></title>
    <url>%2F2020%2F07%2F17%2FSOLO%2F</url>
    <content type="text"><![CDATA[[SOLO] SOLO: Segmenting Objects by Locations：字节，目前绝大多数方法实例分割的结构都是间接得到——检测框内语义分割／全图语义分割聚类，主要原因是formulation issue，很难把实例分割定义成一个结构化的问题 [SOLOv2] SOLOv2: Dynamic, Faster and Stronger：best 41.7% AP SOLO: Segmenting Objects by Locations 动机 challenging：arbitrary number of instances form the task into a classification-solvable problem direct &amp; end-to-end &amp; one-stage &amp; using mask annotations solely on par accuracy with Mask R-CNN outperforming recent single-shot instance segmenters 论点 formulating Objects in an image belong to a fixed set of semantic categories——semantic segmentation can be easily formulated as a dense per-pixel classification problem the number of instances varies existing methods 检测／聚类：step-wise and indirect 累积误差 core idea in most cases two instances in an image either have different center locations or have different object sizes location： think image as a divided grid of cells an object instance is assigned to one of the grid cells as its center location category encode center location categories as the channel axis size FPN assign objects of different sizes to different levels of feature maps SOLO converts coordinate regression into classification by discrete quantization One feat of doing so is the avoidance of heuristic coordination normalization and log-transformation typically used in detectors【？？？不懂这句话想表达啥】 方法 problem formulation divided grids simultaneous task category-aware prediction instance-aware mask generation category prediction predict instance for each grid：$SSC$ grid size：$S*S$ number of classes：$C$ based on the assumption that each cell must belong to one individual instance C-dim vec indicates the class probability for each object instance in each grid mask prediction predict instance mask for each positive cell：$HWS^2$ the channel corresponding to the location position sensitive：因为每个grid中分割的mask是要映射到对应的channel的，因此我们希望特征图是spatially variant 让特征图spatially variant的最直接办法就是加一维spatially variant的信息 inspired by CoordConv：添加两个通道，normed_x和normed_y，[-1,1] original feature tensor $HWD$ becomes $HW(D+2)$ final results gather category prediction &amp; mask prediction NMS network backbone：resnet FCN：256-d heads：weights are shared across different levels except for the last 1x1 conv learning positive grid：falls into a center region mask：mask center $(c_x, c_y)$，mask size $(h,w)$ center region：$(c_x,c_y,\epsilon w, \epsilon h)$，set $\epsilon = 0.2$ loss：$L = L_{cate} + \lambda L_{seg}$ cate loss：focal loss seg loss：dice，$L_{mask} = \frac{1}{N_{pos}}\sum_k 1_{p^_{i,j}&gt;0} dice(m_k, m^_k) $，带星号的是groud truth inference use a confidence threshold of 0.1 to filter out low spacial predictions use a threshold of 0.5 to binary the soft masks select the top 500 scoring masks NMS Only one instance will be activated at each grid and one in- stance may be predicted by multiple adjacent mask channels keep top 100 实验 grid number 适当增加有提升，主要提升还是在FPN fpn 五个FPN pyramids 大特征图，小感受野，用来分配小目标，grid数量要增大 feature alignment 在分类branch，$HW$特征图要转换成$SS$的特征图 interpolation：bilinear interpolating adaptive-pool：apply a 2D adaptive max-pool region-grid- interpolation：对每个cell，采样多个点做双线性插值，然后取平均 is no noticeable performance gap between these variants （可能因为最终是分类任务 head depth 4-7有涨点 所以本文选了7 decoupled SOLO mask branch预测的channel数是$S^2$，其中大部分channel其实是没有贡献的，空占内存 prediction is somewhat redundant as in most cases the objects are located sparsely in the image element-wise multiplication 实验下来 achieves the same performance efficient and equivalent variant SOLOv2: Dynamic, Faster and Stronger 动机 take one step further on the mask head dynamically learning the mask head decoupled into mask kernel branch and mask feature branch propose Matrix NMS faster &amp; better results try object detection and panoptic segmentation 论点 SOLO develop pure instance segmentation instance segmentation requires instance-level and pixel-level predictions simultaneously most existing instance segmentation methods build on the top of bounding boxes SOLO develop pure instance segmentation SOLOv2 improve SOLO mask learning：dynamic scheme mask NMS：parallel matrix operations，outperforms Fast NMS Dynamic Convolutions STN：adaptively transform feature maps conditioned on the input Deformable Convolutional Networks：learn location 方法 revisit SOLOv1 redundant mask prediction decouple dynamic：dynamically pick the valid ones from predicted $s^2$ classifiers and perform the convolution SOLOv2 dynamic mask segmentation head mask kernel branch mask feature branch mask kernel branch prediction heads：4 convs + 1 final conv，shared across scale no activation on the output concat normalized coordinates in two additional input channels at start ouputs D-dims kernel weights for each grid：e.g. for 3x3 conv with E input channels, outputs $SS9E$ mask feature branch predict instance-aware feature：$F \in R^{HWE}$ unified and high-resolution mask feature：只输出一个尺度的特征图，encoded x32 feature with coordinates info we feed normalized pixel coordinates to the deepest FPN level (at 1/32 scale) repeated 【3x3 conv, group norm, ReLU, 2x bilinear upsampling】 element-wise sum last layer：1x1 conv, group norm, ReLU instance mask mask feature branch conved by the mask kernel branch：final conv $HWS^2$ mask NMS train loss：$L = L_{cate} + \lambda L_{seg}$ cate loss：focal loss seg loss：dice，$L_{mask} = \frac{1}{N_{pos}}\sum_k 1_{p^_{i,j}&gt;0} dice(m_k, m^_k) $，带星号的是groud truth inference category score：first use a confidence threshold of 0.1 to filter out predictions with low confidence mask branch：run convolution based on the filtered category map sigmoid use a threshold of 0.5 to convert predicted soft masks to binary masks Matrix NMS Matrix NMS decremented functions linear：$f(iou_{i,j}=1-iou_{i,j})$ gaussian：$f(iou_{i,j}=exp(-\frac{iou_{i,j}^2}{\sigma})$ the most overlapped prediction for $m_i$：max iou $f(iou_{*,i}) = min_{s_k}f(iou_{k,i})$ decay factor $decay_i = min \frac{f(iou_{i,j})}{f(iou_{*,i})}$]]></content>
      <tags>
        <tag>实例分割</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[polarMask]]></title>
    <url>%2F2020%2F06%2F29%2FpolarMask%2F</url>
    <content type="text"><![CDATA[PolarMask: Single Shot Instance Segmentation with Polar Representation 动机 instance segmentation anchor-free single-shot modified on FCOS 论点 two-stage methods FCIS, Mask R-CNN bounding box detection then semantic segmentation within each box single-shot method formulate the task as instance center classification and dense distance regression in a polar coordinate FCOS can be regarded as a special case that the contours has only 4 directions this paper two parallel task： instance center classification dense distance regression Polar IoU Loss can largely ease the optimization and considerably improve the accuary Polar Centerness improves the original idea of “Centreness” in FCOS, leading to further performance boost 方法 architecture back &amp; fpn are the same as FCOS model the instance mask as one center and n rays conclude that mass-center is more advantageous than box center the angle interval is pre-fixed, thus only the length of the rays is to be regressed positive samples：falls into 1.5xstrides of the area around the gt mass-center，that is 9-16 pixels around gt grid distance regression 如果一条射线上存在多个交点，取最长的 如果一条射线上没有交点，取最小值$\epsilon=10^{-6}$ potential issuse of the mask regression branch dense regression task with such as 36 rays, may cause imbalance between regression loss and classification loss n rays are relevant and should be trained as a whole rather than a set of independent values—-&gt;iou loss inference multiply center-ness with classification to obtain final confidence scores, conf thresh=0.05 take top-1k predictions per fpn level use the smallest bounding boxes to run NMS, nms thresh=0.5 polar centerness to suppress low quality detected centers $polar\ centerness=\sqrt{\frac{min(\{d_1,d_2, …, d_n\})}{max(\{d_1,d_2, …, d_n\})}}$ $d_{min}$和$d_{max}$越接近，说明中心点质量越好 Experiments show that Polar Centerness improves accuracy especially under stricter localization metrics, such as $AP_{75}$ polar IoU loss polar IoU：$IoU=lim_{N\to\inf}\frac{\sum_{i=1}^N\frac{1}{2} d_{min}^2 \Delta \theta}{\sum_{i=1}^N\frac{1}{2} d_{max}^2 \Delta \theta}$ empirically observe that 去掉平方项效果更好：$polar\ IoU=\frac{\sum_{i=1}^n d_{min}}{\sum_{i=1}^n d_{max}}$ polar iou loss：bce of polar IoU，$-log(\frac{\sum_{i=1}^n d_{min}}{\sum_{i=1}^n d_{max}})$ advantage differentiable, enable bp regards the regression targets as a whole keep balance with classification loss]]></content>
      <tags>
        <tag>实例分割，极坐标，one-stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FCOS]]></title>
    <url>%2F2020%2F06%2F23%2FFCOS%2F</url>
    <content type="text"><![CDATA[FCOS: Fully Convolutional One-Stage Object Detection 动机 anchor free proposal free avoids the complicated computation related to anchor boxes calculating overlapping during training avoid all hyper-parameters related to anchor boxes size &amp; shape positive／ignored／negative leverage as many foreground samples as possible 论点 anchor-based detectors detection performance is sensitive to anchor settings encounter difficulties in cases with large shape variations hamper the generalization ability of detectors dense propose：the excessive number of negative samples aggravates the imbalance involve complicated computation：such as calculating the IoU with gt boxes FCN-based detector predict a 4D vector plus a class category at each spatial location on a level of feature maps do not work well when applied to overlapped bounding boxes with FPN this ambiguity can be largely eliminated anchor-free detector yolov1：only the points near the center are used，low recall CornerNet：complicated post-processing to match the pairs of corners DenseBox：difficulty in handling overlapping bounding boxes this methos use FPN to deal with ambiguity dense predict：use all points in a ground truth bounding box to predict the bounding box introduce “center-ness” branch to predict the deviation of a pixel to the center of its corresponding bounding box can be used as a RPN in two-stage detectors and can achieve significantly better performance 方法 ground truth boxes，$B_i=(x_0, y_0, x_1, y_1, c)$，corners + cls anchor-free：each location (x,y)，map into abs input image (xs+[s/2], ys+[s/2]) positive sample：if a location falls into any ground-truth box ambiguous sample：location falls into multiple gt boxes，choose the box with minimal area regression target：l t r b distance，location to the four sides cls branch C binary classifiers C-dims vector p focal loss $\frac{1}{N_{pos}} \sum_{x,y}L_{cls}(p_{x,y}, c_{x,y}^*)$ calculate on both positive/negative samples box reg branch 4-dims vector t IOU loss $\frac{1}{N_{pos}} \sum_{x,y}1_{\{c_{x,y}^&gt;0\}}L_{reg}(t_{x,y}, t_{x,y}^)$ calculate on positive samples inference choose the location with p &gt; 0.05 as positive samples two possible issues large stride makes BPR low, which is actually not a problem in FCOS overlaps gt boxes cause ambiguity, which can be greatly resolved with multi-level prediction FPN P3, P4, P5：1x1 conv from C3, C4, C5, top-down connections P6, P7: stride2 conv from P5, P6 limit the bbox regression for each level $m_i$：maximum distance for each level if a location’s gt bbox satifies：$max(l^,t^,r^,b^)&gt;m_i$ or $max(l^,t^,r^,b^)&lt;m_{i-1}$，it is set as a negative sample，not regress at current level objects with different sizes are assigned to different feature levels：largely alleviate一部分box overlapping问题 for other overlapping cases：simply choose the gt box with minimal area sharing heads between different feature levels to regress different size range：use $exp(s_ix)$ trainable scalar $s_i$ slightly improve center-ness low-quality predicted bounding boxes are produced by locations far away from the center of an object predict the “center-ness” of a location normalized distance centerness^* = \sqrt {\frac{min(l^*,r^*)}{max(l^*,r^*)}* \frac{min(t^*,b^*)}{max(t^*,b^*)}} sqrt to slow down the decay [0,1] use bce loss when inference center-ness is mutiplied with the class score：can down-weight the scores of bounding boxes far from the center of an object, then filtered out by NMS an alternative of the center-ness：use of only the central portion of ground-truth bounding box as positive samples，实验证明两种方法结合效果最好 architecture two minor differences from the standard RetinaNet use Group Normalization in the newly added convolutional layers except for the last prediction layers use P5 instead of C5 to produce P6&amp;P7 ​]]></content>
      <tags>
        <tag>目标检测，全卷积，one-stage，centerness，anchor free</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FCIS]]></title>
    <url>%2F2020%2F06%2F22%2FFCIS%2F</url>
    <content type="text"><![CDATA[Fully Convolutional Instance-aware Semantic Segmentation 动机 instance segmentation： 实例分割比起检测，需要得到目标更精确的边界信息 比起语义分割，需要区分不同的物体 detects and segments simultanously FCN + instance mask proposal 论点 FCNs do not work for the instance-aware semantic segmentation task convolution is translation invariant：权值共享，一个像素值对应一个响应值，与位置无关 instance segmentation operates on region level the same pixel can have different semantics in different regions Certain translation-variant property is required prevalent method step1: an FCN is applied on the whole image to generate shared feature maps step2: a pooling layer warps each region of interest into fixed-size per-ROI feature maps step3: use fc layers to convert the per-ROI feature maps to per-ROI masks the translation-variant property is introduced in the fc layer(s) in the last step drawbacks the ROI pooling step losses spatial details the fc layers over-parametrize the task InstanceFCN position-sensitive score maps sliding windows sub-tasks are separated and the solution is not end-to-end blind to the object categories：前背景分割 In this work extends InstanceFCN end-to-end fully convolutional operates on box proposals instead of sliding windows per-ROI computation does not involve any warping or resizing operations 方法 position-sensitive score map FCN predict a single score map predict each pixel’s likelihood score of belonging to each category at instance level the same pixel can be foreground on one object but background on another a single score map per-category is insufficient to distinguish these two cases a fully convolutional solution for instance mask proposal k x k evenly partitioned cells of object thus obtain k x k position-sensitive score maps Each score represents 当前像素在当前位置（score map在cells中的位置）上属于某个物体实例的似然得分 assembling (copy-paste) jointly and simultaneously The same set of score maps are shared for the two sub-tasks For each pixel in a ROI, there are two tasks: detection：whether it belongs to an object bounding box segmentation：whether it is inside an object instance’s boundary separate：two 1x1 conv heads fuse：inside and outside high inside score and low outside score：detection+, segmentation+ low inside score and high outside score：detection+, segmentation- low inside score and low outside score：detection-, segmentation- detection score average pooling over all pixels‘ likelihoods for each class max(detection score) represent the object segmentation softmax(inside, outside) for each pixel to distinguish fg／bg All the per-ROI components are implemented through convs local weight sharing property：a regularization mechanism without involving any feature warping, resizing or fc layers the per-ROI computation cost is negligible architecture ResNet back produce features with 2048 channels a 1x1 conv reduces the dimension to 1024 x16 output stride：conv5 stride is decreased from 2 to 1, the dilation is increased from 1 to 2 head1：joint det conf &amp; segmentation 1x1 conv，generates $2k^2(C+1)$ score maps 2 for inside／outside $k^2$ for $k^2$个position $(C+1)$ for fg／bg head2：bbox regression 1x1 conv，$4k^2$ channels RPN to generate ROIs inference 300 ROIs pass through the bbox regression obtaining another 300 ROIs pass through joint head to obtain detection score&amp;fg mask for all categories mask voting：每个ROI (with max det score) 只包含当前类别的前景，还要补上框内其他类别背景 for current ROI, find all the ROIs (from the 600) with IoU scores higher than 0.5 their fg masks are averaged per-pixel and weighted by the classification score training ROI positive／negative：IoU&gt;0.5 loss softmax detection loss over C+1 categories softmax segmentation loss over the gt fg mask, on positive ROIs bbox regression loss, , on positive ROIs OHEM：among the 300 proposed ROIs on one image, 128 ROIs with the highest losses are selected to back-propagate their error gradients RPN： 9 anchors sharing feature between FCIS and RPN 实验 metric：mAP FCIS (translation invariant)： set k=1，achieve the worst mAP indicating the position sensitive score map is vital for this method back 50-101：increase 101-152：saturate tricks * r]]></content>
      <tags>
        <tag>实例分割，全卷积，带位置信息的scoremap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天池脊柱MRI]]></title>
    <url>%2F2020%2F06%2F11%2F%E5%A4%A9%E6%B1%A0%E8%84%8A%E6%9F%B1MRI%2F</url>
    <content type="text"><![CDATA[MRI T1加权相上面密度高的骨头会比较亮(就是高信号)，还有脂肪和甲状腺也是高信号，水份一般都是无信号， T2加权相里水是高信号所以水比较亮，因为很多的病变有水肿，所以T2加权相通俗可以说是看病变(毕竟比较明显)， 视觉直观上来看，T1看解剖，T2看病变 ——怎么fusion一个case（标注只有一张） 数据集 T1、T2矢状位，T2轴状位， 关键点：基于T2矢状位的中间帧， 标注范围：从胸12（T12）腰1（L1）间的椎间盘开始，到腰5（L5）骶1（S1）间的椎间盘结束 类别：椎块有编号（T12到L5），间盘通过上下椎块的编号表示（T12-L1到L5-S1） 病灶： * 椎块有两类：正常V1和退行性病变V2， * 椎间盘有7类：正常V1，退行性改变V2，弥漫性膨出，非对称性膨出，突出，脱出，疝出V7 json结构： uid，dim，spacing等一些header info annotation： slice：难道不是T2矢状位的中间帧吗？ point：关键点坐标，病灶类别，关键点类别 评估指标 distance&lt;8mm TP：多个命中取最近的，其余忽略 FP：假阳性，distance超出所有gt的8mm圈圈／落进圈圈但是类别错了 FN：假阴性，gt点没有被TP precision：TP/(TP+FP) recall：TP/(TP+FN) AP MAP]]></content>
      <tags>
        <tag>competition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[group normalization]]></title>
    <url>%2F2020%2F06%2F08%2Fgroup-normalization%2F</url>
    <content type="text"><![CDATA[Group Normalization 动机 for small batch size do normalization in channel groups batch-independent behaves stably over different batch sizes approach BN’s accuracy 论点 BN requires sufficiently large batch size (e.g. 32) Mask R-CNN frameworks use a batch size of 1 or 2 images because of higher resolution, where BN is “frozen” by transforming to a linear layer synchronized BN 、BR LN &amp; IN effective for training sequential models or generative models but have limited success in visual recognition GN能转换成LN／IN WN normalize the filter weights, instead of operating on features 方法 group it is not necessary to think of deep neural network features as unstructured vectors 第一层卷积核通常存在一组对称的filter，这样就能捕获到相似特征 这些特征对应的channel can be normalized together normalization transform the feature x：$\hat x_i = \frac{1}{\sigma}(x_i-\mu_i)$ the mean and the standard deviation： \mu_i=\frac{1}{m}\sum_{k\in S_i}x_k\\ \sigma_i=\sqrt {\frac{1}{m}\sum_{k\in S_i}(x_k-\mu_i)^2+\epsilon} the set $S_i$ BN： $S_i=\{k|k_C = i_C\}$ pixels sharing the same channel index are normalized together for each channel, BN computes μ and σ along the (N, H, W) axes LN $S_i=\{k|k_N = i_N\}$ pixels sharing the same batch index (per sample) are normalized together LN computes μ and σ along the (C,H,W) axes for each sample IN $S_i=\{k|k_N = i_N, k_C=i_C\}$ pixels sharing the same batch index and the same channel index are normalized together LN computes μ and σ along the (H,W) axes for each sample GN $S_i=\{k|k_N = i_N, [\frac{k_C}{C/G}]=[\frac{i_C}{C/G}]\}$ computes μ and σ along the (H, W ) axes and along a group of C/G channels linear transform to keep representational ability per channel scale and shift：$y_i = \gamma \hat x_i + \beta$ relation to LN LN assumes all channels in a layer make “similar contributions” which is less valid with the presence of convolutions GN improved representational power over LN to IN IN can only rely on the spatial dimension for computing the mean and variance it misses the opportunity of exploiting the channel dependence 【QUESTION】BN也没考虑通道间的联系啊，但是计算mean和variance时跨了sample implementation reshape learnable $\gamma \&amp; \beta$ computable mean &amp; var 实验 GN相比于BN，training error更低，但是val error略高于BN GN is effective for easing optimization loses some regularization ability it is possible that GN combined with a suitable regularizer will improve results 选取不同的group数，所有的group&gt;1均好于group=1（LN） 选取不同的channel数（C／G），所有的channel&gt;1均好于channel=1（IN） Object Detection frozen：因为higher resolution，batch size通常设置为2/GPU，这时的BN frozen成一个线性层$y=\gamma(x-\mu)/\sigma+beta$，其中的$\mu$和$sigma$是load了pre-trained model中保存的值，并且frozen掉，不再更新 denote as BN* replace BN* with GN during fine-tuning use a weight decay of 0 for the γ and β parameters]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则化]]></title>
    <url>%2F2020%2F06%2F02%2Fregularization%2F</url>
    <content type="text"><![CDATA[综述 正则 正则化是用来解决神经网络过拟合的问题，通过降低模型的复杂性和约束权值，迫使神经网络学习可泛化的特征 正则化可以定义为我们为了减少泛化误差而不是减少训练误差而对训练算法所做的任何改变 对权重进行约束 对目标函数添加额外项（间接约束权值）：L1 &amp; L2正则 数据增强 降低网络复杂度：dropout，stochastic depth early stopping 我们在对网络进行正则化时不考虑网络的bias：正则表达式只是权值的表达式，不包含bias bias比weight具有更少的参数量 对bias进行正则化可能引入太多的方差，引入大量的欠拟合 L1 &amp; L2： 要惩罚的是神经网络中每个神经元的权重大小 L2关注的是权重的平方和，是要网络中的权重接近0但不等于0，“权重衰减” \frac{d}{dW}(\frac{\lambda}{2m}W^2) = \frac{\lambda}{m} W L1关注的是权重的绝对值，权重可能被压缩成0，权重更新时每次减去的是一个常量 \frac{d}{dW}(\frac{\lambda}{m}W) = \frac{\lambda}{m} sgn(W) L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0 dropout 每个epoch训练的模型都是随机的 在test的时候相当于ensemble多个模型 权重共享 数据增强 隐式正则化：其出现的目的不是为了正则化，而正则化的效果是其副产品，包括early stopping，BN，随机梯度下降 dropout &amp; drop connect（[Reference][https://zhuanlan.zhihu.com/p/108024434]） dropout： 2012年Hinton提出，在模型训练时以概率p随机让隐层节点的输出变成0，暂时认为这些节点不是网络结构的一部分，但是会把它们的权重保留下来（不更新）。 标准dropout相当于在一层神经元之后再添加一个额外的层，这些神经元在训练期间以一定的概率将值设置为零，并在测试期间将它们乘以p。 drop connect： 不是随机的将隐层节点的输出变成0，而是将节点中的每个与其相连的输入权值以1-p的概率变成0。（一个是输出一个是输入） 训练阶段，对每个example／mini-batch, 每个epoch都随机sample一个mask矩阵 Dropconnect在测试期间采用了与标准dropout不同的方法。作者提出了dropconnect在每个神经元处的高斯近似，然后从这个高斯函数中抽取一个样本并传递给神经元激活函数。这使得dropconnect在测试时和训练时都是一种随机方法。 伯努利分布：0-1分布 dropout &amp; drop connect 通常只作用于全连接层上：这俩是用来防止过多参数导致过拟合 卷积层参数贼少，所以没必要， 针对卷积通道有spacial dropout：按照channel随机扔 dropblock：是针对卷积层的正则化方法，相比较于dropout的random mute，能够更有效地remove掉部分语义信息，block size=1的时候退化成dropout papers [dropout] Improving neural networks by preventing co-adaptation of feature detectors，丢节点 [drop connect] Regularization of neural networks using dropconnect，丢weight path [Stochastic Depth] Deep Networks with Stochastic Depth，丢layer [DropBlock] A regularization method for convolutional networks drop大法一句话汇总 dropout：各维度完全随机扔 spacial dropout：按照channel随机扔 stochastic depth：按照res block随机扔 dropblock：在feature map上按照spacial块随机扔 cutout：在input map上按照spacial块随机扔 dropconnect：扔连接不扔神经元 Deep Networks with Stochastic Depth 动机 propose a training procedure：stochastic depth，train short and test deep for each mini-batch randomly drop a subset of layers and bypass them with the identity function short：reduces training time reg：improves the test error can increase the network depth 论点 deeper expressiveness vanishing gradients diminishing feature reuse resnet skip connection when输入输出channel数不match：redefine id(·) as a linear projection to reduce the dimensions dropout Dropout reduces the effect known as “co- adaptation” of hidden nodes Dropout loses effectiveness when used in combination with Batch Normalization our approach higher diversity shorter instead of thinner work with Batch Normalization 方法 stochastic depth randomly dropping entire ResBlocks $H_l = ReLU(b_l Res_l(H_{l-1}) + id(H_{l-1}))$ survival probabilities $p_l = Pr(b_l=1)$ set uniformly / set following a linear decay rule set $p_0=1, p_L=0.5$： p_l = 1 - \frac{l}{L}(1-p_L) intuition：the earlier layers extract low-level features that will be used by later layers and should therefore be more reliably present Expected network depth $E(L) \approx 3L/4$ approximately 25% of training time could be saved during testing all res path are active each res path is weighted by its survival probability $H_l^{Test} = ReLU(b_l Res_l(H_{l-1}, W_l) + id(H_{l-1}))$ 跟dropout一样]]></content>
      <tags>
        <tag>正则化，dropout，dropconnect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RetinaNet]]></title>
    <url>%2F2020%2F05%2F30%2FRetinaNet%2F</url>
    <content type="text"><![CDATA[[det] RetinaNet: Focal Loss for Dense Object Detection [det+instance seg] RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free [det+semantic seg] Retina U-Net: Embarrassingly Simple Exploitation of Segmentation Supervision for Medical Object Detection Focal Loss for Dense Object Detection 动机 dense prediction(one-stage detector) focal loss：address the class imbalance problem RetinaNet：design and train a simple dense detector 论点 accuracy trailed two-stage：classifier is applied to a sparse set of candidate one-stage：dense sampling of possible object locations the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause loss standard cross entropy loss：down-weights the loss assigned to well-classified examples proposed focal loss：focuses training on a sparse set of hard examples R-CNN系列two-stage framework proposal-driven the first stage generates a sparse set of candidate object locations the second stage classifies each candidate location as one of the foreground classes or as background class imbalance：在stage1大部分背景被filter out了，stage2训练的时候强制固定前背景样本比例，再加上困难样本挖掘OHEM faster：reducing input image resolution and the number of proposals ever faster：one-stage one-stage detectors One stage detectors are applied over a regular, dense sampling of object locations, scales, and aspect ratios dense：regularly sampling(contrast to selection)，基于grid以及anchor以及多尺度 the training procedure is still dominated by easily classified background examples class imbalance：通常引入bootstrapping和hard example mining来优化 Object Detectors Classic：sliding-window+classifier based on HOG，dense predict Two-stage：selective Search+classifier based on CNN，shared network RPN One-stage：‘anchors’ introduced by RPN，FPN loss Huber loss：down-weighting the loss of outliers (hard examples) focal loss：down-weighting inliers (easy examples) 方法 focal loss CE：$CE(p_t)=-log(p_t)$ even examples that are easily classified ($p_t&gt;0.5$) incur a loss with non-trivial magnitude summed CE loss over a large number of easy examples can overwhelm the rare class WCE：$WCE(p_t)=-\alpha_t log(p_t)$ balances the importance of positive/negative examples does not differentiate between easy/hard examples FL：$FL(p_t)=-\alpha_t(1-p_t)^\gamma log(p_t)$ as $\gamma$ increases the modulating factor is likewise increased $\gamma=2$ works best in our experiments ​ two-stage detectors通常不会使用WCE或FL cascade stage会过滤掉大部分easy negatives 第二阶段训练会做biased minibatch sampling Online Hard Example Mining (OHEM) construct minibatches using high-loss examples scored by loss + nms completely discards easy examples RetinaNet compose：backbone network + two task-specific subnetworks backbone：convolutional feature map over the entire input image subnet1：object classification subnet2：bounding box regression ResNet-FPN backbone rich, multi-scale feature pyramid，二阶段的RPN也用了FPN each level can be used for detecting objects at a different scale P3 - P7：8x - 128x downsamp FPN channels：256 anchors anchor ratios：{1:2, 1:1, 2:1}，长宽比 anchor scales：{$2^0$, $2^\frac{1}{3}$, $2^\frac{2}{3}$}，大小，同一个scale的anchor，面积相同，都是size*size，长宽通过ratio求得 anchor size per level：[32, 64, 128, 256, 512]，基本的正方形anchor的边长 total anchors per level：A=9 KA：each anchor is assigned a length K one-hot vector of classification targets 4A：and a 4-vector of box regression targets anchors are assigned to ground-truth object boxes using an intersection-over-union (IoU) threshold of 0.5 anchors are assigned background if their IoU is in [0, 0.4) anchor is unassigned between [0.4, 0.5), which is ignored during training each anchor is assigned to at most one object box for each anchor classification targets：one-hot vector box regression targets：each anchor和其对应的gt box的offset rpn offset：中心点、宽、高 $$ t_x = (x - x_a) / w_a\\ t_y = (y - y_a) / h_a\\ t_w = log(w/ w_a)\\ t_h = log(h/ h_a) $$ or omitted if there is no assignment 【QUESTION】所谓的anchor state {-1:ignore, 0:negative, 1:positive} 是针对cls loss来说的，相当于人为丢弃了一部分偏向中立的样本，这对分类效果有提升吗？？ classification subnet for each spatial position，for each anchor，predict one among K classes，one-hot input：C channels feature map from FPN structure：four 3x3 conv + ReLU，each with C filters head：3x3 conv + sigmoid，with KA filters share across levels not share with box regression subnet focal loss： sum over all ～100k anchors * and normalized by the number of anchors assigned to a ground-truth box * 因为是sum，所以要normailize，norm项用的是number of assigned anchors（这是包括了前背景？） * vast majority of anchors are **easy negatives** and receive negligible loss values under the focal loss（确实包含背景框） * $\alpha$：In general $alpha$ should be decreased slightly as $\gamma$ is increased strong effect on negatives：FL can effectively discount the effect of easy negatives, focusing all attention on the hard negative examples box regression subnet class-agnostic bounding box regressor same structure：four 3x3 conv + ReLU，each with C filters * head：4A linear outputs * L1 loss inference keep top 1k predictions per FPN level * all levels are merged and non-maximum suppression with a threshold of 0.5 train initialization： cls head bias initialization，encourage more foreground prediction at the start of training prevents the large number of background anchors from generating a large, destabilizing loss network design anchors * one-stage detecors use fixed sampling grid to generate position * use multiple ‘anchors’ at each spatial position to cover boxes of various scales and aspect ratios * beyond 6-9 anchors did not shown further gains in AP * speed/accuracy trade-off * outperforms all previous methods * bigger resolution bigger AP * Retina-101-600与ResNet101-FRCNN的AP持平，但是比他快 gradient： 梯度有界 the derivative is small as soon as $x_t &gt; 0$ &lt;img src=&quot;RetinaNet/gradient.png&quot; width=&quot;70%;&quot; /&gt; ​ RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free 动机 improve single-shot detectors to the same level as current two-stage techniques improve on RetinaNet integrating instance mask prediction adaptive loss additional hard examples Group Normalization same computational cost as the original RetinaNet but more accurate：同样的参数量级比orgin RetinaNet准，整体的参数量级大于yolov3，acc快要接近二阶段的mask RCNN了 论点 part of improvements of two-stage detectors is due to architectures like Mask R-CNN that involves multiple prediction heads additional segmentation task had only been added to two-stage detectors in the past two-stage detectors have the cost of resampling(ROI-Align) issue：RPN之后要特征对齐 add addtional heads in training keeps the structure of the detector at test time unchanged potential improvement directions data：OHEM context：FPN additional task：segmentation branch this paper’s contribution add a mask prediction branch propose a new self-adjusting loss function include more of positive samples—&gt;those with low overlap 方法 best matching policy speical case：outlier gt box，跟所有的anchor iou都不大于0.5，永远不会被当作正样本 use best matching anchor with any nonzero overlap to replace the threshold self-adjusting Smooth L1 loss bbox regression smooth L1： L1 loss is used beyond $\beta$ to avoid over-penalizing outliers the choice of control point is heuristic and is usually done by hyper parameter search f(x) = \begin{cases} 0.5 \frac{x^2}{\beta} \text{, if } |x| < \beta \\ |x| - 0.5\beta \text{, otherwise } \end{cases} self-adjusting control point running mean &amp; variance \mu_B = \frac{1}{n}\sum_{i=1}^n |x_i|\\ \sigma_B^2 = \frac{1}{n}\sum_{i=1}^n(|x_i|-\mu_B)^2 minibatch update：m=0.9 \mu_R = \mu_R * m + \mu_B*(1-m)\\ \sigma_R^2 = \sigma_R^2*m+\sigma_B^2*(1-m) control point：$[0, \hat \beta]$ clip to avoid unstable \beta = max(0, min(\hat \beta, \mu_R-\sigma_R^2)) mask module detection predictions are treated as mask proposals extract the top N scored predictions distribute the mask proposals to sample features from the appropriate layers k = [k_0 + log_2 \sqrt{wh}/224] $k_0=4$，如果size小于224*224，proposal会被分配给P3，如果大于448*448，proposal会被分配给P5 using more feature layers shows no performance boost architecture r50&amp;r101 back：freezing all of the Batch Nor- malization layers fpn feature channel：256 classification branch 4 conv layers：conv3x3+relu，channel256 head：conv3x3+sigmoid，channel n_anchors*n_classes regression branch 4 conv layers：conv3x3+relu，channel256 head：conv3x3，channel n_anchors*4 aggregate the boxes to the FPN layers ROI-Align yielding 14x14 resolution features mask head 4 conv layers：conv3x3 a single transposed convolutional layer：convtranspose2d 2x2，to 28*28 resolution prediction head：conv1x1 training min side &amp; max side：800&amp;1333 limited GPU：reduce the batch size，increasing the number of training iterations and reducing the learning rate accordingly positive/ignore/negative：0.5，0.4 focal loss for classification gaussian initialization $\alpha=0.25, \lambda=2.0$ $FL=-\alpha_t(1-p_t)^\lambda log(p_t)$ FL = \left\{ \begin{array}{lr} -\alpha (1-p)^{\gamma}log(p), \ \ y=1\\ -(1-\alpha) p^{\gamma}log(1-p), \ \ y=0\\ \end{array} \right. gamma项控制的是简单样本的衰减速度，alpha项控制的是正负样本比例，可以默认值下正样本的权重是0.25，负样本的权重是0.75，和想象中的给正样本更多权重不一样，因为alpha和gamma是耦合起来作用的，（可能检测场景下困难的负样本相比于正样本更少？背景就是比前景好学？不确定不确定。。。） self-adjusting L1 loss for box regression limit running params：[0, 0.11] mask loss top-100 predicted boxes + ground truth boxes inference box confidence threshold 0.05 nms threshold 0.4 use top-50 boxes for mask prediction Retina U-Net: Embarrassingly Simple Exploitation of Segmentation Supervision for Medical Object Detection 动机 localization pixel-level predict ad-hoc heuristics when mapping back to object-level scores semantic segmentation auxiliary task overall one-stage leveraging available supervision signals 论点 monitoring pixel-wise predictions are clinically required medical annotations is commonly performed in pixel- wise full semantic supervision fully exploiting the available semantic segmentation signal results in significant performance gains one-stage explicit scale variance enforced by the resampling operation in two-stage detectors is not helpful in the medical domain two-stage methods predict proposal-based segmentations mask loss is only evaluated on cropped proposal：no context gradients ROI-Align：not suggested in medical image depends on the results of region proposal：serial vs parallel gradients of the mask loss do not flow through the entire model 方法 model back： ResNet50 fpn： shift p3-p6 to p2-p5 change sigmoid to softmax 3d head channels：64 anchor size：$\{P_2: 4^2, P_3: 8^2,, P_4: 16^2,, P_5: 32^2\}$ 3d z-scale：{1，2，4，8}，考虑到z方向的low resolution segmentation supervision p0 &amp; p1 with skip connections without detection heads segmentation loss calculates on p0 logits dice + ce h weighted box clustering patch crop tiling strategies &amp; model ensembling causes multi predictions per location nms选了一类中score最大的box，然后抑制所有与它同类的IoU大于一定阈值的box weighted box作用于这一类所有的box，计算一个融合的结果 coordinates confidence：$o_c = \frac{\sum c_i s_i w_i}{\sum s_i w_i}$ score confidence：$o_s = \frac{\sum s_i w_i}{\sum w_i + n_{missing * \overline w}}$ $w_i$：$w=f a p$ overlap factor f：与highest scoring box的overlap area factor a：higher weights to larger boxes，经验 patch center factor p：相对于patch center的正态分布 score confidence的分母上有一个down-weight项$n_{missing}$：基于prior knowledge预期prediction的总数得到 论文给的例子让我感觉好比nms的点 一个cluster里面一类最终就留下一个框：解决nms一类大框包小框的情况 这个location上prediction明显少于prior knowledge的类别confidence会被显著拉低：解决一个位置出现大概率假阳框的情况]]></content>
      <tags>
        <tag>目标检测，focalloss，实例分割，自适应smoothL1</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DCGAN]]></title>
    <url>%2F2020%2F05%2F27%2FDCGAN%2F</url>
    <content type="text"><![CDATA[UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS 动机 unsupervised learning learns a hierarchy of representations from object parts to scenes used for novel tasks 论点 GAN Learning reusable feature representations from large unlabeled datasets generator and discriminator networks can be later used as feature extractors for supervised tasks unstable to train we propose a set of constraints on the architectural topology making it stable to train use the trained discriminators for image classification tasks visualize the filters show that the generators have interesting vector arithmetic properties unsupervised representation learning clustering, hierarchical clustering auto-encoders learn good feature representations generative image models samples often suffer from being blurry, being noisy and incomprehensible further use for supervised tasks 方法 architecture all convolutional net：没有池化，用stride conv eliminating fully connected layers： generator：输入是一个向量，reshape以后接的全是卷积层 discriminator：最后一层卷积出来直接flatten Batch Normalization generator输出层 &amp; discriminator输入层不加 resulted in sample oscillation and model instability ReLU generator输出层用Tanh discriminator用leakyReLU train image preprocess：rescale to [-1,1] LeakyReLU(0.2) lr：2e-4 momentum term $\beta 1$：0.5, default 0.9 实验 evaluate apply them as a feature extractor on supervised datasets evaluate the performance of linear models on top of these features model use the discriminator’s convolutional features from all layers maxpooling to 4x4 grids flattened and concatenated to form a 28672 dimensional vector regularized linear L2-SVM 相比之下：the discriminator has many less feature maps, but larger total feature vector size visualizing walking in the latent space 在vector Z上差值，生成图像可以观察到smooth transitions visualize the discriminator feature 特征图可视化，能观察到床结构 manipulate the generator representation generator learns specific object representations for major scene components use logistic regression to find feature maps related with window, drop the spatial locations on feature-maps most result forgets to draw windows in the bedrooms, replacing them with other objects vector arithmetic averaging the Z vector for three examplars semantically obeyed the arithmetic]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[densenet]]></title>
    <url>%2F2020%2F05%2F27%2Fdensenet%2F</url>
    <content type="text"><![CDATA[动机 embrace shorter connections the feature-maps of all preceding layers are used as inputs advantages alleviate vanishing-gradient encourage feature reuse reduce the number of parameters 论点 Dense each layer obtains additional inputs from all preceding lay- ers and passes on its own feature-maps to all subsequent layers feature reuse combine features by concatenating：the summation in ResNet may impede the information flow in the network information preservation id shortcut/additive identity transformations fewer params DenseNet layers are very narrow add only a small set of feature-maps to the “collective knowledge” gradients flow each layer has direct access to the gradients from the loss function have regularizing effect 方法 architecture dense blocks concat BN-ReLU-3x3 conv $x_l = H_l([x_0, x_1, …, x_{l-1}])$ transition layers change the size of feature-maps BN-1x1 conv-2x2 avg pooling growth rate k $H_l$ produces feature- maps narrow：e.g., k = 12 One can view the feature-maps as the global state of the network The growth rate regulates how much new information each layer contributes to the global state bottleneck —- DenseNet-B in dense block stage 1x1 conv reduce dimension first number of channels：4k compression —- DenseNet-C in transition stage reduce the number of feature-maps number of channels：$\theta k$ structure configurations 1st conv channels：第一层卷积通道数 number of dense blocks L：dense block里面的layer数 k：growth rate B：bottleneck 4k C：compression 0.5k 讨论 concat replace sum： seemingly small modification lead to substantially different behaviors of the two network architectures feature reuse：feature can be accessed anywhere parameter efficient：同样参数量，test acc更高，同样acc，参数量更少 deep supervision：classifiers attached to every hidden layer weight assign All layers spread their weights over multi inputs (include transition layers) least weight are assigned to the transition layer, indicating that transition layers contain many redundant features, thus can be compressed overall there seems to be concentration towards final feature-maps, suggesting that more high-level features are produced late in the network]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GANomaly]]></title>
    <url>%2F2020%2F05%2F25%2FGANomaly%2F</url>
    <content type="text"><![CDATA[GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training 动机 Anomaly detection highly biased towards one class (normal) insufficient sample size of the other class (abnormal) semi-supervised learning detecting the unknown/unseen anomaly case trained on normal samples tested on normal and abnormal samples encoder-decoder-encoder minimizing the distance between the images and the latent vectors a larger distance metric 论点 supervised approaches heavily depend on large, labeled datasets Generative Adversarial Networks (GAN) have emerged as a leading methodology across both unsupervised and semi-supervised problems reconstruction-based anomaly techniques Overall prior work strongly supports the hypothesis that the use of autoencoders and GAN 方法 GAN unsupervised to generate realistic images compete generator tries to generate an image, decoder- alike network, map input to latent space discriminator decides whether the generated image is a real or a fake, classical classification architecture, reading an input image, and determining its validity Adversarial Auto-Encoders (AAE) encoder + decoder reconstruction: maps the input to latent space and remaps back to input data space train autoencoders with adversarial setting inverse mapping with the additional use of an encoder, a vanilla GAN network is capable of learning inverse mapping model learns both the normal data distribution and minimizes the output anomaly score two encoder, one decoder, a discriminator encoder convolutional layers followed by batch-norm and leaky ReLU() activation compress to a vector z decoder convolutional transpose layers, ReLU() activation and batch-norm a tanh layer at the end 2nd encoder with the same architectural but different parametrization discriminator DCGAN discriminator Adversarial Loss 不是基于GAN的traditional 0/1 ouput 而是选了一个中间层，计算real／fake(reconstructed)的L2 distance Contextual Loss L1 yields less blurry results than L2 计算输入图像和重建图像的L1 distance Encoder Loss an additional encoder loss to minimize the distance of the bottleneck features 计算两个高维向量的L2 distance 在测试的时候用它来scoring the abnormality]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[resnets]]></title>
    <url>%2F2020%2F05%2F23%2Fresnets%2F</url>
    <content type="text"><![CDATA[overview papers [resnet] ResNet: Deep Residual Learning for Image Recognition [resnext] ResNext: Aggregated Residual Transformations for Deep Neural Networks [resnest] ResNeSt: Split-Attention Networks [revisiting resnets] Revisiting ResNets: Improved Training and Scaling Strategies ResNext: Aggregated Residual Transformations for Deep Neural Networks 动机 new network architecture new building blocks with the same topology propose cardinality increasing cardinality is able to improve classification accuracy is more effective than going deeper or wider classification task 论点 VGG &amp; ResNets： stacking building blocks of the same topology deeper reduces the free choices of hyper-parameters Inception models split-transform-merge strategy split：1x1conv spliting into a few lower-dimensional embeddings transform：a set of specialized filters merge：concat approach the representational power of large and dense layers, but at a considerably lower computational complexity modules are customized stage-by-stage our architecture adopts VGG/ResNets’ repeating layers adopts Inception‘s split-transform-merge strategy aggregated by summation cardinality：the size of the set of transformations（split path数） 多了1x1 conv的计算量 少了3x3 conv的计算量 要素 Multi-branch convolutional blocks Grouped convolutions：通道对齐，稀疏连接 Compressing convolutional networks Ensembling 方法 architecture a template module if producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes) when the spatial map is downsampled by a factor of 2, the width of the blocks is multiplied by a factor of 2 grouped convolutions：第一个1x1和3x3conv的width要根据C进行split equivalent blocks BN after each conv ReLU after each BN except the last of block ReLU after add r Model Capacity improve accuracy when maintaining the model complexity and number of parameters adjust the width of bottleneck, the according C to maintain capacity：C=1的时候退化成ResNet block ResNeSt: Split-Attention Networks 动机 propose a modular Split-Attention block enables attention across feature-map groups preserve the overall ResNet structure for downstream applications such as object detection and semantic segmentation prove improvement on detection &amp; segmentation tasks 论点 ResNet simple and modular design limited receptive-field size and lack of cross-channel interaction image classification networks have focused more on group or depth-wise convolution do not transfer well to other tasks isolated representations cannot capture cross-channel relationships a versatile backbone improving performance across multiple tasks at the same time a network with cross-channel representations is desirable a Split-Attention block divides the feature-map into several groups (along the channel dimension) finer-grained subgroups or splits weighted combination featuremap attention mechanism：NiN’s 1x1 conv Multi-path：GoogleNet channel-attention mechanism：SE-Net 结构上，全局上看，模仿ResNext，引入cardinality和group conv，局部上看，每个group内部继续分组，然后模仿SK-Net，融合多个分支的split-attention，大group之间concat，而不是ResNext的add，再经1x1 conv调整维度，add id path 方法 Split-Attention block enables feature-map attention across different feature-map groups within a block：controlled by cardinality within a cardinal group：introduce a new radix hyperparameter R indicating the number of splits split-attention 多个in-group branch的input输入进来 fusion：先做element-wise summation channel-wise global contextual information：做global average pooling 降维：Dense-BN-ReLU 各分支Dense(the attention weight function)：学习各自的重要性权重 channel-wise soft attention：对全部的dense做softmax 加权：原始的各分支input与加权的dense做乘法 和：加权的各分支add r=1：退化成SE-blockaverage pooling shortcut connection for blocks with a strided convolution or combined convolution-with-pooling can be applied to the id concat average pooling downsampling for dense prediction tasks：it becomes essential to preserve spatial information former work tend to use strided 3x3 conv we use an average pooling layer with 3x3 kernel 2x2 average pooling applied to strided shortcut connection before 1x1 conv Revisiting ResNets: Improved Training and Scaling Strategies 动机 disentangle the three aspects model architecture training methodology scaling strategies improve ResNets to SOTA design a family of ResNet architectures, ResNet-RS use improved training and scaling strategies and combine minor architecture changes 在ImageNet上打败efficientNet 在半监督上打败efficientNet-noisystudent 论点 ImageNet上榜大法 Architecture 人工系列：AlexNet，VGG，ResNet，Inception，ResNeXt NAS系列：NasNet-A，AmoebaNet-A，EfficientNet Training and Regularization Methods regularization methods dropout，label smoothing，stochastic depth，dropblock，data augmentation significantly improve generalization when training more epochs training learning rate schedules Scaling Strategies model dimension：width，depth，resolution efficientNet提出的均衡增长，在本文中shows sub-optimal for both resnet and efficientNet Additional Training Data pretraining on larger dataset semi-supervised the performance of a vision model architecture：most research focus on training methods and scaling strategy：less publicized but critical unfair：使用modern training method的新架构与使用dated methods的老网络直接对比 we focus on the impact of training methods and scaling strategies training methods： We survey the modern training and regularization techniques 发现引入其他正则方法的时候降低一点weight decay有好处 scaling strategies： We offer new perspectives and practical advice on scaling 可能出现过拟合的时候就加depth，否则先加宽 resolution慢点增长，more slowly than prior works 从acc图可以看到：我们的scaling strategies与网络结构的lightweight change正交，是additive的 re-scaled ResNets, ResNet-RS 仅improve training &amp; scaling strategy就能大幅度涨点 combine minor architectural changes进一步涨点 方法 architecture use ResNet with two widely used architecture changes ResNet-D stem的7x7conv换成3个3x3conv stem的maxpooling去掉，每个stage的首个3x3conv负责stride2 residual path上前两个卷积的stride互换（在3x3上下采样） id path上的1x1 s2conv替换成2x2 s2的avg pooling+1x1conv SE in bottleneck use se-ratio of 0.25 training methods match the efficientNet setup train for 350 epochs use cosine learning rate instead of exponential decay RandAugment instead of AutoAugment use Momentum optimizer instead of RMSProp regularization weight decay label smoothing dropout stochastic depth data augmentation we use RandAugment EfficientNet use AutoAugment which slightly outperforms RandAugment hyper： droprate increase the regularization as the model size increase to limit overfitting label smoothing = 0.1 weight decay = 4e-5 improved training methods additive study 总体上看都是additive的 increase training epochs在添加regularization methods的前提下才不hurt，否则会overfitting dropout在不降低weight decay的情况下会hurt weight decay 少量/没有regularization methods的情况下：大weight decay防止过拟合，1e-4 多/强regularization methods的情况下：适当减小weight decay能涨点，4e-5 improved scaling strategies search space width multiplier：[0.25, 0.5, 1.0, 1.5, 2.0] depth：[26, 50, 101, 200, 300, 350, 400] resolution：[128, 160, 224, 320, 448] increase regularization as model size increase observe 10/100/350 epoch regime we found that the best scaling strategies depends on training regime strategy1：scale depth Depth scaling outperforms width scaling for longer epoch regimes width scaling is preferable for shorter epoch regimes scaling width可能会引起overfitting，有时候会hurt performance depth scaling引入的参数量也比width小 strategy2：slow resolution scaling efficientNets/resNeSt lead to very large images our experiments：大可不必 实验]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Receptive Field]]></title>
    <url>%2F2020%2F05%2F18%2FReceptive-Field%2F</url>
    <content type="text"><![CDATA[综述 感受野 除了卷积和池化，其他层并不影响感受野大小 感受野与卷积核尺寸kernel_size和步长stride有关 递归计算： N\_RF = kernel\_size + (cur\_RF-1)*stride 其中$cur_RF$是当前层（start from 1），$kernel_size$、$stride$是当前层参数，$N_RF$是上一层的感受野。 感受野计算器 https://fomoro.com/research/article/receptive-field-calculator Understanding the Effective Receptive Field in Deep Convolutional Neural Networks 动机 effective receptive field the effect of nonlinear activations, dropout, sub-sampling and skip connections on it 论点 it is critical for each output pixel to have a big receptive field, such that no important information is left out when making the prediction deeper network：increase the receptive field size linearly Sub-sampling：increases the receptive field size multiplicatively it is easy to see that pixels at the center of a receptive field have a much larger impact on an output：前向传播的时候，中间位置的像素点有更多条path通向output 方法看不懂直接看结论 dropout does not change the Gaussian ERF shape Subsampling and dilated convolutions turn out to be effective ways to increase receptive field size quickly Skip-connections make ERFs smaller ERFs are Gaussian distributed uniformly和随机初始化都是perfect Gaus- sian shapes 加上非线性激活函数以后是near Gaussian shapes with different nonlinearities $\sqrt n$ absolute growth and $1/\sqrt n$ relative shrinkage：RF是随着layer线性增长的，ERF在log上0.56的斜率，约等于$\sqrt n$ Subsampling &amp; dilated convolution increases receptive field The reference baseline is a convnet with 15 dense convolution layers Subsampling：replace 3 of the 15 convolutional layers with stride-2 convolution dilated：replace them with dilated convolution with factor 2,4 and 8，rectangular ERF shape evolves during training as the networks learns, the ERF gets bigger, and at the end of training is significantly larger than the initial ERF classification 32*32 cifar 10 theoretical receptive field of our network is actually 74 × 74 segmentation CamVid dataset the theoretical receptive field of the top convolutional layer units is quite big at 505 × 505 实际的ERF都很小，都没到原图大小 increase the effective receptive field New Initialization： makes the weights at the center of the convolution kernel to have a smaller scale, and the weights on the outside to be larger 30% speed-up of training 其他效果不明显 Architecturalchanges sparsely connect each unit to a larger area dilated convolution or even not grid-like g]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SqueezeNet]]></title>
    <url>%2F2020%2F05%2F18%2FSqueezeNet%2F</url>
    <content type="text"><![CDATA[SQUEEZENET: ALEXNET-LEVEL ACCURACY WITH 50X FEWER PARAMETERS AND &lt;0.5MB MODEL SIZE 动机 Smaller CNN achieve AlexNet-level accuracy model compression 论点 model compression SVD sparse matrix quantization (to 8 bits or less) CNN microarchitecture extensively 3x3 filters 1x1 filters higher level building blocks bypass connections automated designing approaches this paper eschew automated approaches propose and evaluate the SqueezeNet architecture with and without model compression explore the impact of design choices 方法 architectural design strategy Replace 3x3 filters with 1x1 filters Decrease the number of input channels to 3x3 filters （squeeze） Downsample late in the network so that convolution layers have large activation maps：large activation maps (due to delayed downsampling) can lead to higher classification accuracy the fire module squeeze：1x1 convs expand：mix of 1x1 and 3x3 convs, same padding relu concatenate the SqueezeNet a standalone convolution layer (conv1) followed by 8 Fire modules (fire2-9) ending with a final conv layer (conv10) stride2 max-pooling after layers conv1, fire4, fire8, and conv10 dropout with a ratio of 50% is applied after the fire9 module GAP understand the impact each Fire module has three dimensional hyperparameters, to simplify： define $base_e$：the number of expand filters in the first Fire module for layer i：$e_i=base_e + (incr_e*[\frac{i}{freq}])$ expand ratio $pct_{3x3}$：the percentage of 3x3 filters in expand layers squeeze ratio $SR$：the number of filters in the squeeze layer／the number of filters in the expnad layer normal setting：$base_e=128, incre_e=128, pct_{3x3}=0.5, freq=2, SR=0.125$ SR increasing SR leads to higher accuracy and larger model size Accuracy plateaus at 86.0% with SR=0.75 further increasing provides no improvement pct increasing pct leads to higher accuracy and larger model size Accuracy plateaus at 85.6% with pct=50% further increasing provides no improvement bypass Vanilla simple bypass：when in &amp; out channels have the same dimensions complex bypass：includes a 1x1 convolution layer alleviate the representational bottleneck introduced by squeeze layers both yielded accuracy improvements simple bypass enabled higher accuracy]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hausdorff Distance]]></title>
    <url>%2F2020%2F05%2F14%2FHausdorff-Distance%2F</url>
    <content type="text"><![CDATA[Reducing the Hausdorff Distance in Medical Image Segmentation with Convolutional Neural Networks 动机 novel loss function to reduce HD directly propose three methods 2D&amp;3D，ultra &amp; MR &amp; CT lead to approximately 18 − 45% reduction in HD without degrading other segmentation performance criteria 论点 HD is one of the most informative and useful criteria because it is an indicator of the largest segmentation error current segmentation algorithms rarely aim at minimizing or reducing HD directly HD is determined solely by the largest error instead of the overall segmentation performance HD‘s sensitivity to noise and outliers —&gt; modified version the optimization diffculty thus we propose an “HD- inspired” loss function 方法 denotations probability：$q$ binary mask：$\bar p$、$\bar q$ boundary：$\delta p$、$\delta q$ single hd：$hd(\bar p, \bar q)$、$hd(\bar q, \bar p)$ based on distance transforms distance map $d_p$：define the distance map as the unsigned distance to the boundary $\delta p$ DT_X[i,j] = min_{[k,l]\in X}d([i,j], [k,l]) 距离场定义为：每个点到目标区域(X)的距离的最小值 HD based on DT： hd_{DT}(\delta p, \delta q) = max((\bar p \triangle \bar q)\circ d_p)\\ \bar p \triangle \bar q = |\bar p - \bar q| finally have： HD_{DT}(\delta p, \delta q) = max(hd_{DT}(\delta p, \delta q), hd_{DT}(\delta q, \delta p)) modified loss version of HD： Loss_{DT}(q,p) = \frac{1}{|\Omega|}\sum_{\Omega}((p-q)^2\circ(d_p^{\alpha}+d_q^{\alpha})) penalizely focus on areas instead of single point $\alpha$ determines how strongly we penalize larger errors use possibility instead of thresholded value use $(p-q)^2$ instead of $|p-q|$ correlations $HD_{DT}$：Pearson correlation coefficient above 0.99 $Loss_{DT}$：Pearson correlation coefficient above 0.93 drawback high computational cost especially in 3D $q$ changes along with training process thus $d_q$ changes while $d_p$ remains modified one-sided HD (OS)： Loss_{DT-OS}(q,p) = \frac{1}{|\Omega|}\sum_{\Omega}((p-q)^2\circ(d_p^{\alpha})) HD using Morphological Operations morphological erosion： S \ominus B = \{z\in \Omega | B(z) \subseteq S\} 腐蚀操作定义为：在原始二值化图的前景区域，以每个像素为中心点，run structure element block B，如果B完全在原图内，则当前中心点在腐蚀后也是前景。 HD based on erosion： HD_{ER}(\delta p, \delta q)=2r^*\\ where\ r^* = min_r \{(\bar p \triangle \bar q) \ominus B_r = \varnothing\} $HD_{ER}$ is a lower bound of the true value can be computed more efficiently using convolutional operations modifid loss version： Loss_{ER}(q,p) = \frac{1}{|\Omega|}\sum_k \sum_{\Omega}((p-q)^2 \ominus_k B)k^{\alpha} k successive erosions cross-shaped kernel whose elements sum to one followed by a soft thresholding at 0.50 correlations $HD_{ER}$：Pearson correlation coefficient above 0.91 $Loss_{ER}$：Pearson correlation coefficient above 0.83 HD using circular-shaped convolutional kernel circular-shaped kernel HD based on circular-shaped kernel： hd_{CV}(\delta p, \delta q)=max(r_1, r_2)\\ where \ r_1=max_r (max_{\Omega}f_h(\bar p ^C * B_r)\circ(\bar q \backslash \bar p))\\ where \ r_2=max_r (max_{\Omega}f_h(\bar p * B_r)\circ(\bar p \backslash \bar q))\\ $\bar p^C$：complement 补集 $f_h$：hard thresholding setting all values below 1 to zero modified loss version： Loss_{CV}(q,p)=\frac{1}{|\Omega|}\sum_{r\in R}r^{\alpha}\sum_{\Omega}[f_s(Br*\bar p^C)\circ f_{\bar q\backslash \bar p} + f_s(B_r * \bar p) \circ f_{\bar p \backslash \bar q}\\ +f_s(Br*\bar q^C)\circ f_{\bar p\backslash \bar q} + f_s(B_r * \bar q) \circ f_{\bar q \backslash \bar p}] soft thresholding f_{\bar p\backslash \bar q} = (p-q)^2*p correlations $HD_{CV}$：Pearson correlation coefficient above 0.99 $Loss_{CV}$：Pearson correlation coefficient above 0.88 computation： kernel size $HD_{ER}$ is computed using small fixed convolutional kernels (of size 3) $Loss_{CV}$ require applying filters of increasing size(we use a maximum kernel radius of 18 pixels in 2D and 9 voxels in 3D) steps choose R based on the expected range of segmentation errors set R = {3, 6, . . . 18} for 2D images and R = {3,6,9} for 3D training standard Unet augment our HD-based loss term with a DSC loss term for more stable training reweight both loss after every epoch d]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SE block]]></title>
    <url>%2F2020%2F04%2F30%2FSE-block%2F</url>
    <content type="text"><![CDATA[综述图像特征的提取能力是CNN的核心能力，而SE block可以起到为CNN校准采样的作用。 根据感受野理论，特征矩阵主要来自于样本的中央区域，处在边缘位置的酒瓶的图像特征很大概率会被pooling层抛弃掉。而SE block的加入就可以通过来调整特征矩阵，增强酒瓶特征的比重，提高它的识别概率。 [SE-Net] Squeeze-and-Excitation Networks [SC-SE] Concurrent Spatial and Channel ‘Squeeze &amp; Excitation’ in Fully Convolutional Networks [CMPE-SE] Competitive Inner-Imaging Squeeze and Excitation for Residual Network SENet: Squeeze-and-Excitation Networks 动机 prior research has investigated the spatial component to achieve more powerful representations we focus on the channel relationship instead SE-block：adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels enhancing the representational power in a computationally efficient manner 论点 stronger network： deeper NiN-like bocks cross-channel correlations in prior work mapped as new combinations of features through 1x1 conv concentrated on the objective of reducing model and computational complexity In contrast, we found this mechanism can ease the learning process and significantly enhance the representational power of the network Attention Attention can be interpreted as a means of biasing the allocation of available computational resources towards the most informative components Some works provide interesting studies into the combined use of spatial and channel attention 方法 SE-block The channel relationships modelled by convolution are inherently implicit and local we would like to provide it with access to global information squeeze：using global average pooling excitation：nonlinear &amp; non-mutually-exclusive using sigmoid bottleneck：a dimensionality-reduction layer $W_1$ with reduction ratio $r$ and ReLU and a dimensionality-increasing layer $W_2$ $s = F_{ex}(z,W) = \sigma (W_2 \delta(W_1 z))$ integration insert after the non-linearity following each convolution inception：take the transformation $F_{tr}$ to be an entire Inception module residual：take the transformation $F_{tr}$ to be the non-identity branch of a residual module model and computational complexity ResNet50 vs. SE-ResNet50：0.26% relative increase GFLOPs approaching ResNet10’s accuracy the additional parameters result solely from the two FC layers, among which the final stage FC claims the majority due to being performed across the greatest number of channels the costly final stage of SE blocks could be removed at only a small cost in performance ablations FC removing the biases of the FC layers in the excitation facilitates the modelling of channel dependencies reduction ratio performance is robust to a range of reduction ratios In practice, using an identical ratio throughout a network may not be optimal due to the distinct roles performed by different layers squeeze global average pooling vs. global max pooling：average pooling slightly better excitation Sigmoid vs. ReLU vs. tanh： tanh：slightly worse ReLU：dramatically worse stages each stages brings benefits combination make even better integration strategy fairly robust to their location, provided that they are applied prior to branch aggregation inside the residual unit：fewer channels, fewer parameters, comparable accuracy primitive understanding squeeze the use of global information has a significant influence on the model performance excitation the distribution across different classes is very similar at the earlier layers (general features) the value of each channel becomes much more class-specific at greater depth SE_5_2 exhibits an interesting tendency towards a saturated state in which most of the activations are close to one SE_5_3 exhibits a similar pattern emerges over different classes, up to a modest change in scale suggesting that SE_5_2 and SE_5_3 are less important than previous blocks in providing recalibration to the network (thus can be removed) APPENDIX 在ImageNet上SOTA的模型是SENet-154，top1-err是18.68，被标记在了efficientNet论文的折线图上 SE-ResNeXt-152（64x4d） input=(224,224)：top1-err是18.68 input=320/299：top1-err是17.28 further difference each bottleneck building block的第一个1x1 convs的通道数减半 stem的第一个7x7conv换成了3个连续的3x3 conv 1x1的s2 conv换成了3x3的s2 conv fc之前添加dropout layer label smoothing 最后几个training epoch将BN层的参数冻住，保证训练和测试的参数一致 64 GPUs，batch size=2048（32 per GPU） initial lr=1.0 SC-SE: Concurrent Spatial and Channel ‘Squeeze &amp; Excitation’ in Fully Convolutional Networks 动机 image segmentation task 上面SE-Net提出来主要是针对分类 three variants of SE modules squeezing spatially and exciting channel-wise (cSE) squeezing channel-wise and exciting spatially (sSE) concurrent spatial and channel squeeze &amp; excitation (scSE) integrate within three different state-of-the- art F-CNNs (DenseNet, SD-Net, U-Net) 论点 F-CNNs have become the tool of choice for many image segmentation tasks core：convolutions that capturing local spatial pattern along all input channels jointly SE block factors out the spatial dependency by global average pooling to learn a channel specific descriptor (later refered to as cSE /channel-SE) while for image segmentation, we hypothesize that the pixel-wise spatial information is more informative thus we propose sSE(spatial SE) and scSE(spatial and channel SE) can be seamlessly integrated by placing after every encoder and decoder block 方法 cSE GAP：embeds the global spatial information into a vector FC-ReLU-FC-Sigmoid：adaptively learns the importance recalibrate sSE 1x1 conv：generating a projection tensor representing the linearly combined representation for all channels C for a spatial location (i,j) Sigmoid：rescale recalibrate scSE by element-wise addition encourages the network to learn more meaningful feature maps ———- relevant both spatially and channel-wise 实验 F-CNN architectures： 4 encoder blocks, one bottleneck layer, 4 decoder blocks and a classification layer class imbalance：median frequency balancing ce dice cmp：scSE &gt; sSE &gt; cSE &gt; vanilla 小区域类别的分割，观察到使用cSE可能会差于vanilla： might have got overlooked by only exciting the channels 定性分析： 一些under segmented的地方，scSE improves with the inclusion 一些over segmented的地方，scSE rectified the result Competitive Inner-Imaging Squeeze and Excitation for Residual Network 动机 for residual network the residual architecture has been proved to be diverse and redundant model the competition between residual and identity mappings make the identity flow to control the complement of the residual feature maps 论点 For analysis of ResNet, with the increase in depth, the residual network exhibits a certain amount of redundancy with the CMPE-SE mechanism, it makes residual mappings tend to provide more efficient supplementary for identity mappings 方法 主要提出了三种变体： 第一个变体： 两个分支id和res分别GAP出一个vector，然后fc reduct by ratio，然后concat，然后channel back Implicitly, we can believe that the winning of the identity channels in this competition results in less weights of the residual channels 第二个变体： 两种方案 2x1 convs：对上下相应位置的元素求avg 1x1 convs：对全部元素求avg，然后flatten 第三个变体： 两边的channel-wise vector叠起来，然后reshape成矩阵形式，然后3x3 conv，然后flatten 比较扯，不浪费时间分析了。]]></content>
      <tags>
        <tag>papers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cv2&numpy&tobeadded]]></title>
    <url>%2F2020%2F04%2F19%2Fcv2-numpy-tobeadded%2F</url>
    <content type="text"><![CDATA[矩阵乘法 np.dot(A,B)：真正的矩阵乘法 np.multiply(A,B) &amp; np重载的*：element-wise product，矩阵中对应元素相乘 cv的A.dot(B) &amp; cv重载的*：真正的矩阵乘法 cv的A.mul(B) ：element-wise product，矩阵中对应元素相乘 图像旋转 通过仿射矩阵cv2.getRotationMatrix2D和仿射变换函数cv2.warpAffine来实现 src：输入图像 M：变换矩阵 dsize：输出图像的大小（基于图像原点裁剪） flags：插值方法 borderMode：边界像素模式 borderValue：边界填充值，默认为0 cv2.getRotationMatrix2D(center, angle, scale)：返回一个2x3的变换矩阵 center：旋转中心 angle：旋转角度，正值是逆时针旋转 scale：缩放因子 cv2.warpAffine(src, M, dsize[, dst[, flags[, borderMode[, borderValue]]]]))：返回变换后的图像 src：输入图像 M：变换矩阵 dsize：输出图像的大小（基于图像原点裁剪） flags：插值方法 borderMode：边界像素模式 borderValue：边界填充值，默认为0 12345678910111213141516171819def rotate_img(angle, img, interpolation=cv2.INTER_LINEAR, points=[]): h, w = img.shape rotataMat = cv2.getRotationMatrix2D((w/2, h/2), math.degrees(angle), 1) # rotate_img1: 输出图像尺寸不变，超出原图像部分被cut掉 rotate_img1 = cv2.warpAffine(img, rotataMat, dsize=(w, h), flags=interpolation, borderMode=cv2.BORDER_CONSTANT, borderValue=0) # rotate_img2: 输出图像尺寸变大，保留超出原图像部分，新的坐标原点保证旋转中心仍旧位于图像中心 new_h = int(w*math.fabs(math.sin(angle)) + h*math.fabs(math.cos(angle))) new_w = int(h*math.fabs(math.sin(angle)) + w*math.fabs(math.cos(angle))) rotataMat[0, 2] += (new_w - w) / 2 rotataMat[1, 2] += (new_h - h) / 2 rotate_img2 = cv2.warpAffine(img, rotataMat, dsize=(new_w, new_h), flags=interpolation, borderMode=cv2.BORDER_CONSTANT, borderValue=0) # 坐标点的变换 rotated_points = [] for point in points: point = rotataMat.dot([[point[0]], [point[1]], [1]]) rotated_points.append((int(point[0]), int(point[1]))) return rotate_img2, rotated_points 使用tips： 如果不修改仿射变换矩阵的平移参数，坐标原点的位置不发生改变 dsize指定的输出图像是从原点位置开始裁剪 坐标点的变换满足公式： dst(x,y) = src(M_{11}x+M_{12}y+M_{13}, M_{21}x+M_{22}y+M_{23}) np.meshgrid(*xi,**kwargs) 这个函数神他妈坑，作用是Return coordinate matrices from coordinate vectors. Make N-D coordinate arrays for vectorized evaluations of N-D scalar/vector fields over N-D grids, given one-dimensional coordinate arrays x1, x2,…, xn. 但是尝试一下会发现： 12345x = np.arange(0,10,1)y = np.arange(0,20,1)z = np.arange(0,30,1)x, y, z= np.meshgrid(x, y, z)print(x.shape) # (20, 10, 30) xy轴坐标是反过来的，这是因为optional args里面有一个indexing： indexing : {‘xy’, ‘ij’}, Cartesian (‘xy’, default) or matrix (‘ij’) indexing of output. 我们想要得到的坐标系和输入的轴一一对应，得指定参数indexing=&#39;ij&#39; 12345x = np.arange(0,10,1)y = np.arange(0,20,1)z = np.arange(0,30,1)x, y, z= np.meshgrid(x, y, z, indexing='ij')print(x.shape) # (10, 20, 30) 还有一个参数sparse，因为每根轴的坐标都是复制的，所以可以稀疏存储，此时函数返回值变化： sparse : bool, If True a sparse grid is returned in order to conserve memory. Default is False. 12345678910x = np.arange(0,10,1)y = np.arange(0,20,1)xx, yy = np.meshgrid(x, y)print(xx) # a 20x10 listxx, yy = np.meshgrid(x, y, sparse=True)print(xx) # a 1*10 listprint(yy) # a 20*1 list# 所以整体上还是个20*10的矩阵 二维可视化： 12345import matplotlib.pyplot as pltz = xx**2 + yy**2 # xx和yy既可以是dense convervation也可以是sparse convervationh = plt.contourf(x,y,z)plt.show() np.tile(A,reps) 这个函数挺有用的，把数组沿着指定维度复制，比stack、concat啥的都优雅，能自动创建新的维度 A：array_like, The input array. reps：array_like, The number of repetitions of A along each axis. np.reshape(a, newshape, order=’C’) 这个函数贼常用，但是一般用于二维的时候没考虑重组顺序这件事 order: {‘C’, ‘F’, ‘A’}, optional，简单理解，reshape的通用实现方式是先将真个array拉直，然后依次取数据填入指定维度，C是从最里面的维度开始拉直&amp;构造，F是从最外面的维度开始拉直&amp;构造，A for auto 123456789101112a = np.arange(6)array([0, 1, 2, 3, 4, 5])# C-like index orderingnp.reshape(a, (2, 3))array([[0, 1, 2], [3, 4, 5]])# Fortran-like index orderingnp.reshape(a, (2, 3), order='F')array([[0, 4, 3], [2, 1, 5]]) tf和keras里面也有reshape，是没有order参数的，默认是’C’]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MobileNets]]></title>
    <url>%2F2020%2F04%2F16%2FMobileNets%2F</url>
    <content type="text"><![CDATA[preview 动机 计算力有限 模型压缩／使用小模型 深度可分离卷积 Depthwise Separable Convolution 将标准卷积拆分为两个操作：深度卷积(depthwise convolution) 和逐点卷积(pointwise convolution) 标准卷积：参数量k*k*input_channel*output_channel 深度卷积(depthwise convolution) ：针对每个输入通道采用不同的卷积核，参数量k*k*input_channel 逐点卷积(pointwise convolution)：就是普通的卷积，只不过其采用1x1的卷积核，参数量1*1*input_channel*output_channel with BN and ReLU： DW没有改变通道数的能力，如果输入层的通道数很少，DW也只能在低维空间提特征，因此V2提出先对原始输入做expansion，用一个非线性PW升维，然后DW，然后再使用一个PW降维，值得注意的是，第二个PW不使用非线性激活函数，因为作者认为，relu作用在低维空间上会导致信息损失。 进一步缩减计算量 通道数缩减：宽度因子 alpha 分辨率缩减：分辨率因子rho papers [V1 CVPR2017] MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications，Google，主要贡献Depthwise Separable Convolution [V2 CVPR2018] MobileNetV2: Inverted Residuals and Linear Bottlenecks，Google，主要贡献inverted residual with linear bottleneck [V3 ICCV2019] Searching for MobileNetV3，Google，模型结构升级多了SE(inverted-res-block + SE-block)，是通过NAS而非手动设计 [EfficientNet-lite ICML2019] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks，efficientNet家族的scale-down版本，原始的EfficientNet是基于Mobile3的basic block，而Lite版本有很多专供移动端的改动：去掉SE、改用RELU6、 [MobileOne 2022] An Improved One millisecond Mobile Backbone，Apple， MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications 动机 efficient models：uses depthwise separable convolutions and two simple global hyper-parameters resource and accuracy tradeoffs a class of network architectures that allows a model developer to specifically choose a small network that matches the resource restrictions (latency, size) for their application 论点： the general trend has been to make deeper and more complicated networks in order to achieve higher accuracy not efficient on computationally limited platform building small and efficient neural networks：either compressing pretrained networks or training small networks directly Many papers on small networks focus only on size but do not consider speed speed &amp; size 不完全对等 size：depthwise separable convolutions, bottleneck approaches, compressing pretrained networks, distillation 方法 depthwise separable convolutions a form of factorized convolutions：a standard conv splits into 2 layers factorize the filtering and combination steps of standard conv drastically reducing computation and model size to $\frac{1}{N} + \frac{1}{D_k^2}$ use both batchnorm and ReLU nonlinearities for both layers MobileNet uses 3 × 3 depthwise separable convolutions which bring between 8 to 9 times less computation MobileNet the first layer is a full convolution, the rest depthwise separable convolutions down sampling is handled with strided convolution all layers are followed by a BN and ReLU nonlinearity a final average pooling reduces the spatial resolution to 1 before the fully connected layer. the final fully connected layer has no nonlinearity and feeds into a softmax layer for classification training so few parameters RMSprop less regularization and data augmentation techniques because small models have less trouble with overfitting it was important to put very little or no weight decay (l2 regularization) do not use side heads or label smoothing or image distortions Width Multiplier: Thinner Models thin a network uniformly at each layer the input channels $M$ and output channels $N$ becomes $\alpha M$ and $\alpha N$ $\alpha=1$：baseline MobileNet $\alpha&lt;1$：reduced MobileNet reduce the parameters roughly by $\alpha^2$ Resolution Multiplier: Reduced Representation apply this to the input image the input resolution of the network is typically 224, 192, 160 or 128 $\rho=1$：baseline MobileNet $\rho&lt;1$：reduced MobileNet reduce the parameters roughly by $\rho^2$ 结论 using depthwise separable convolutions compared to full convolutions only reduces accuracy by 1% on ImageNet but saving tremendously on mult-adds and parameters at similar computation and number of parameters, thinner MobileNets is 3% better than making them shallower trade-offs based on the two hyper-parameters MobileNetV2: Inverted Residuals and Linear Bottlenecks 动机 a new mobile architecture based on an inverted residual structure remove non-linearities in the narrow layers in order to maintain representational power prove on multiple tasks object detection：SSDLite semantic segmentation：Mobile DeepLabv3 方法 Depthwise Separable Convolutions replace a full convolutional opera- tor with a factorized version depthwise convolution, it performs lightweight filtering per input channel pointwise convolution, computing linear combinations of the input channels Linear Bottlenecks ReLU results in information loss in lower dimension space expansion ratio：if we have lots of channels, information might still be preserved in the other channels linear：bottleneck上面不包含非线性激活单元 Inverted residuals bottlenecks actually contain all the necessary information expansion layer acts merely as an implementation detail that accompanies a non-linear transformation parameter count： basic building block is a bottleneck depth-separable convolution with residuals * interpretation * provides a natural separation between the input/output * expansion：capacity * layer transformation：expressiveness * MobileNetV2 model architecture * initial filters：32 * ReLU6：use ReLU6 as the non-linearity because of its robustness when used with low-precision computation * use constant expansion rate between 5 and 10 except the 1st：smaller network inclines smaller and larger larger &lt;img src=&quot;MobileNets/MobileNetV2.png&quot; width=&quot;40%&quot; /&gt; * comparison with other architectures &lt;img src=&quot;MobileNets/cmpV2.png&quot; width=&quot;40%&quot; /&gt; 实验 Object Detection evaluate the performance as feature extractors replace all the regular convolutions with separable convolutions in SSD prediction layers：backbone没有改动，只替换头部的卷积，降低计算量 achieves competitive accuracy with significantly fewer parameters and smaller computational complexity Semantic Segmentation build DeepLabv3 heads on top of the second last feature map of MobileNetV2 DeepLabv3 heads are computationally expensive and removing the ASPP module significantly reduces the MAdds ablation inverted residual connections：shortcut connecting bottleneck perform better than shortcuts connecting the expanded layers 在少通道的特征上进行短连接 linear bottlenecks：linear bottlenecks improve performance, providing support that non-linearity destroys information in low-dimensional space Searching for MobileNetV3 动机 automated search algorithms and network design work together classification &amp; detection &amp; segmentation a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP) new efficient versions of nonlinearities 论点 reducing the number of parameters the number of operations (MAdds) inference latency related work SqueezeNet：1x1 convolutions MobileNetV1：separable convolution MobileNetV2：inverted residuals ShuffleNet：group convolutions CondenseNet：group convolutions ShiftNet：shift operation MnasNet：MobileNetV2+SE-block，attention modules are placed after the depthwise filters in the expansion 方法 base blocks combination of ideas from [MobileNetV1, MobileNetV2, MnasNet] inverted-res-block + SE-block swish nonlinearity hard sigmoid Network Search use platform-aware NAS to search for the global network structures use the NetAdapt algorithm to search per layer for the number of filters Network Improvements redesign the computionally-expensive layers at the beginning and the end of the network the last block of MobileNetV2’s inverted bottleneck structure move this layer past the final average pooling：移动到GAP后面去，作用在1x1的featuremap上instead of 7x7，曲线救国 a new nonlinearity, h-swish the initial set of filters are also expensive：usually start with 32 filters in a full 3x3 convolution to build initial filter banks for edge detection reduce the number of filters to 16 and use the hard swish nonlinearity swish\ [x]=x*\sigma(x)\\ h\_swish\ [x]=x\frac{ReLU6(x+3)}{6} most of the benefits swish are realized by using them only in the deeper layers：只在后半段网络中用 SE-block ratio：all to fixed to be 1/4 of the number of channels in expansion layer MobileNetV3 architecture 实验 Detection use MobileNetV3 as replacement for the backbone feature extractor in SSDLite：改做backbone了 reduce the channel counts of C4&amp;C5’s block：因为MobileNetV3原本是被用来输出1000类的，transfer到90类的coco数据集上有些redundant Segmentation as network backbone compare two segmentation heads R-ASPP：reduced design of the Atrous Spatial Pyramid Pooling module with only two branches Lite R-ASPP：类SE-block的设计，大卷积核，大步长 EfficientNet-lite 没有专门的paper 基于efficientNet向移动端改进 https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite https://blog.tensorflow.org/2020/03/higher-accuracy-on-vision-models-with-efficientnet-lite.html model zoo | model | width | depth | resolution | droprate | | ————————— | ——- | ——- | ————— | ———— | | efficientnet-lite0 | 1. | 1. | 224 | .2 | | efficientnet-lite1 | 1. | 1.1 | 240 | .2 | | efficientnet-lite2 | 1.1 | 1.2 | 260 | .3 | | efficientnet-lite3 | 1.2 | 1.4 | 280 | .3 | | efficientnet-lite4 | 1.4 | 1.8 | 300 | .3 | | | | | | | 关键数据： lite4精度可以达到80.4%，同时保持在Pixel 4 CPU上real-time运行：30ms/image latency：10-30ms model size：5M-15M &lt;img src=&quot;MobileNets/lite-size.png&quot; width=&quot;40%;&quot; /&gt; 3. challenges * Quantization量化：移动端设备支持的浮点精度有限——训练后量化，将浮点模型tf model转化成tfLite model（全整数int8/半浮点float16）， * Heterogeneous hardware移动端设备参差不齐：好多操作不支持，尽量替换成底层支持的op 4. modifications * Removed squeeze-and-excitation networks：去掉SE，not well supported * swish替换成RELU6：有利于post-training quantization * 一开始将浮点替换成整型的时候观察到huge acc drop：75 -&gt; 48 * 发现是因为浮点太wide-ranged了，直接映射到int8太多精度损失 * 所以替换成激活区间有限的relu6 [0,6] &lt;img src=&quot;MobileNets/int8.png&quot; width=&quot;40%;&quot; /&gt;&lt;img src=&quot;MobileNets/quantization.png&quot; width=&quot;40%;&quot; /&gt; * Fixed the stem and head while scaling models up：stem的resolution大，head的channel大，scaleup都对参数量/计算量影响比较大，只scaleup中间的stage MobileOne: An Improved One millisecond Mobile Backbone 动机 FLOPs &amp; 参数量等指标并不直接和移动端latency相关 this paper 调研了各种mobileNets的优化瓶颈：architectural and optimization bottlenecks 提出了MobileOne 精度 低于1ms/image的速度，top 1 acc 75.9% 上面的eff-lite0要10ms，top 1 acc 74.+%，涨点2.3% Mobile- Former精度近似，速度要快38x 论点 previous methods 大部分foucs on 优化FLOPs 而且会引入new architecture designs &amp; custom layers，如hard-swish，这在移动端通常不原生支持 existing metric &amp; latency FLOPs does not account for memory cost and degree of parallelism sharing parameters leads to higher FLOPS but smaller model size skip-connections / branching incur memory costs 结构优化 要找到真正限制on-device latency的要素 训练优化 直接训练小模型精度肯定差，通常是decoupling train-time &amp; test-time architecture 进一步地还做了relaxing regularization the proposed MobileOne use basic operators introduces linear branches which get re-parameterized at inference-time：与之前方法的区别是引入了over-parameterization branches【repVGG是把常规的resblock搞成一个线形op了，本文的branch有k个，是把好多个branch合并一起，所以起名叫over？】 inference time model has only feed-forward structure generalizes well to other tasks：在分类、检测、分割上都outperforming 方法 Metric Correlations parameter count and FLOPs 很多模型参数量很大，但是latency更小——只要右边的点纵坐标比左边的低都是这种case，如3(efficientnet-b0)&amp;2(shufflenet-v2) FLOPs和参数量近似的情况下，卷积模型通常比他们的transformer counterpart latency更小——可以看到transformer模型都在右下角 CPU correlation mobile device与FLOPs适度相关，与参数量基本无关 CPU的latency更无关 结构的影响更大，SE-block和skip都影响挺大的 Key Bottlenecks Activation Functions 搞了同样的网络架构，用不同的激活函数 激活函数越花，latency越大，归因于synchronization cost，个别激活函数有通过特殊硬件加速的实现 从通用角度，本文选用ReLU Architectural Blocks 根本因素是memory access cost &amp; degree of parallelism 分支越多，memory access cost越大，因为存在多节点交互更多 而像global pooling这种force synchronization需要同步计算的，也影响overall run-time 上面截图有这个 MobileOne Architecture MobileOne Block training &amp; test time 不一样 training time basic block还是类似mobileV1的depth-wise + point-wise 每个3x3 donv 和 1x1 pconv都变成了一个多分枝的block block有k个 re-parameterizable branch，k是个超参，1-5 除此之外还有两个常规的branch：1x1 dconv &amp; BN inference time 只有一条data steam 合并思路类似repvgg：所有的线形计算都可以合并 Model Scaling 提供了5个尺寸的模型，主要变化在channel，深度是没变的 Training 小模型训练，need less regularization，但是weight decay对训练初期又很重要 cosine schedule for both LR &amp; weight decay 还用了efficientNetV2里面提到的progressive learning：从简单任务开始训练，逐渐增加难度（resolution &amp; dataaug） &amp; 超参（regularization） 还有EMA：model ensemble]]></content>
      <tags>
        <tag>papers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[verseg]]></title>
    <url>%2F2020%2F04%2F15%2Fverseg%2F</url>
    <content type="text"><![CDATA[challenge Large Scale Vertebrae Segmentation Challenge task1:Vertebra Labelling，关键点检测 task2:Vertebra Segmentation，多类别分割 data variation：数据affine轴不统一，尺寸不统一，扫描范围不统一，FOV区域不统一 nii的两大解析工具：nibabel库load data的xyz顺序与axcode的顺序一致，e.g.[‘R’,’A’,’S’]的orientation会得到xyz的array，而sitk的读取刚好反过来，sitk的arr会是zyx。我们之前在将dicom写入nii时，会指定一个不为np.eye(4)的affine，就是为了transpose这三个轴。 model team paper \ 三阶段：第一阶段，due to large variation FOV of the dataset，粗分割定位脊柱位置，第二阶段，higher resolution多类别关键点定位center，获得each located vertebra，第三阶段，二类分割for each located vertebra。 keywords：1. uniform voxel spacing：不要随意resize，todo: trilinear interp；2. on-the-fly data augmentation：using SimpleITK 第一阶段：Spine Localization Unet regress the Gaussian heatmap of spinal centerline L2-loss uniform voxel spacing of 8mm input shape：[64,64,128]，pad？ 第二阶段：Vertebrae Localization SpatialConfiguration-Net regress each located vertebra‘s heatmap in individual channel resampling：bi/tricubic interpolation norm：maxmin on the whole dataset uniform voxel spacing of 2mm input shape：[96,96,128]，z-axis random crop，xy-plane use ROI from stage1 第三阶段：Vertebrae Segmentation Unet binary segment the mask of each vertebrae sigmoid ce-loss uniform voxel spacing of 1mm input shape：[128,128,96]，crop origin image &amp; heatmap image based on centroids reference paper\ 核心贡献：1.MIP：combines the information across reformations，3D to 2D，2. 基于判别器的训练机制：encodes local spine structure as an anatomical prior，加固椎块间类别&amp;位置的spacial information MIP： localisation and identification rely on a large context large receptive field in full-body scans where spine is not spatially centred or is obstructed by the ribcage, such cases are handled with a pre-processing stage detecting the occluded spine adversarial learning： FCN用于分割 AE用于评估分割的好坏 do not ‘pre-train’ it (the AE) loss：an anatomically-inspired supervision instead of the usual binary adversarial supervision (vanilla GAN) 先说FCN——Btrfly Network 建模成回归问题，每个关键点对应一个通道的高斯heatmap，背景channel为$1-max_i (y_i)$ 双输入双输出（sagittal &amp; coronal） 两个视角的feature map在网络深层做了融合，to learn their inter-dependency Batch- normalisation is used after every convolution layer, along with 20% dropout in the fused layers of Btrfly loss：l2 distance + weighted ce L_{sag} = ||Y_{sag} - \hat{Y}_{sag}||^2 + \omega CE(softmax(Y_{sag}, softmax(\hat{Y}_{sag})) $\omega$ is the median frequency weighing map, boosting the learning of less frequent classes(ECB) 再说判别器——Energy-based adversary for encoding prior fully-convolutional：its predictions across voxels are independent of each other owing to the spatial invariance of convolutions to impose the anatomical prior of the spine’s shape onto the Btrfly net look at $\hat{Y}_{sag}$ and $\hat{Y}_{cor}$ as a 3D volume and employ a 3D AE with a receptive field covering a part of the spine $\hat{Y}_{sag}$ consists of Gaussians：less informative than an image, avoid using max-pooling by resorting to average pooling employ spatially dilated convolution kernels mission of AE：predict the l2 distance of input and its reconstruction, it learns to discriminate by predicting a low E for real annotations, while G learns to generate annotations that would trick D L = D(Y_x) + max(0, m-D(Y_g))\\ L_G = D(Y_g) + L_{fcn} inference： The values below a threshold (T) are ignored in order to remove noisy predictions 用外积，$\hat{Y}=\hat{Y}_{sag}\otimes\hat{Y}_{cor}$ 每个channel的最大值作为centroids experiments 【IMPORTANT】10 MIPs are obtained from one 3D scan per view, each time randomly choosing half the slices of interest 对于每个视角，每次随机抽取一半数目的slice用于计算MIP similar local appearance： strong spatial configuration：凡是涉及到椎块-wise的信息，从全局信息入手]]></content>
      <tags>
        <tag>challenge</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GoogLeNet系列]]></title>
    <url>%2F2020%2F04%2F13%2FGoogLeNet%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[综述 papers [V1] Going Deeper with Convolutions, 6.67% test error [V2] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, 4.8% test error [V3] Rethinking the Inception Architecture for Computer Vision, 3.5% test error [V4] Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, 3.08% test error [Xception] Xception: Deep Learning with Depthwise Separable Convolutions [EfficientNet] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks [EfficientDet] EfficientDet: Scalable and Efficient Object Detection [EfficientNetV2] EfficientNetV2: Smaller Models and Faster Training 大体思路 inception V1：打破传统的conv block，设计了Inception block，将1*1、3*3、5*5的卷积结果concat，增加网络宽度 inception V2：加入了BN层，减少Internal Covariate Shift，用两个3*3替代5*5，降低参数量 inception V3：提出分解Factorization，7*7改成7*1和1*7，参数减少加速计算，增加网络深度和非线性 inception V4：结合Residual Connection Xception：针对inception V3的分解结构的改进，使用可分离卷积 EfficientNet：主要研究model scaling，针对网络深度、宽度、图像分辨率，有效地扩展CNN EfficientDet：将EfficientNet从分类任务扩展到目标检测任务 review review0122：conv-BN层合并运算 reference：https://nenadmarkus.com/p/fusing-batchnorm-and-conv/ freezed BN可以看成1x1的卷积运算 两个线性运算是可以合并的 given $W_{conv} \in R^{CC_{prev}kk}$，$b_{conv} \in R^C $，$W_{bn}\in R^{CC}$，$b_{bn}\in R^C$ F = W_{bn} * (W_{conv} * F_{prev} + b_{conv}) + b_{bn} V1: Going deeper with convolutions 动机 improved utilization of the computing resources increasing the depth and width of the network while keeping the computational budget 论点 the recent trend has been to increase the number of layers and layer size, while using dropout to address the problem of overfitting major bottleneck：large network，large number of params，limited dataset，overfitting methods use filters of different sizes in order to handle multiple scales NiN use 1x1 convolutional layers to easily integrate in the current CNN pipelines we use 1x1 convs with a dual purpose of dimension reduction 方法 Architectural 1x1 conv+ReLU for compute reductions an alternative parallel pooling path since pooling operations have been essential for the success overall architecture : 细节： rectified linear activation mean subtraction a move from fully connected layers to average pooling improves acc the use of dropout remained essential adding auxiliary classifiers(on 4c&amp;4d) with a discount weight 5x5 avg pool, stride 3 1x1 conv+relu, 128 filters 1024 fc+relu 70% dropout 1000 fc+softmax asynchronous stochastic gradient descent with 0.9 momentum fixed learning rate schedule (de- creasing the learning rate by 4% every 8 epochs photometric distortions useful to combat overfitting random interpolation methods for resizing V2: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift 动机 use much higher learning rates be less careful about initialization also acts as a regularizer, eliminating the need for Dropout 论点 SGD：optimizes the parameters $\theta$ of the network, so as to minimize the loss \theta = argmin_{\theta} \frac{1}{N}\sum_{i=1}^N loss(x_i, \theta) 梯度更新：\theta_{next-timestep} = \theta - \alpha \frac{\partial[\frac{1}{N}\sum_Nloss(\theta)]}{\partial \theta}，$x_i$ is the full set batch approximation：use $\frac{1}{m} \sum_M \frac{\partial loss(\theta)}{\partial \theta}$，$x_i$ is the mini-batch set quality improves as the batch size increases computation over a batch is much more efficient than m computations for individual examples the learning rate and the initial values require careful tuning Internal covariate shift the input distribution of the layers changes consider a gradient descent step above，$x$的数据分布改变了，$\theta$就要相应地 readjust to compensate for the change in the distribution of x activation 对于神经元$z = sigmoid(Wx+b)$，前面层的参数变化，很容易导致当前神经元的响应值不在有效活动区间，从而导致过了当前激活函数以后梯度消失，slow down the convergence In practice, using ReLU &amp; careful initialization &amp; small learning rates 如果我们能使得the distribution of nonlinearity inputs remains more stable as the network trains，就不会出现神经元饱和的问题 whitening 对training set的预处理：linearly transformed to have zero means and unit variances, and decorrelated 使得输入数据的分布保持稳定，normal distribution 同时去除了数据间的相关性 batch normalization fixes the means and variances of layer inputs reducing the dependence of gradients on the scale of the parameters or of their initial values makes it possible to use saturating nonlinearities * full whitening of each layer is costly * so we normalize each layer independently, full set--&gt; mini-batch * standard normal distribution并不是每个神经元所需的（如identity transform）：introduce, for each activation $x(k)$ , a pair of parameters $\gamma(k)$, $\beta(k)$, which scale and shift the normalized value to maintain the representation ability of the neuron ​ * for convolutional networks * we add the BN transform immediately before the nonlinearity, $z = g(Wx+b)$ to $z = g(BN(Wx))$ * since we normalize $Wx+b$, the bias b can be ignored * obey the convolutional property——different elements of the same feature map, at different locations, are normalized in the same way * We learn a pair of parameters $\gamma(k)$ and $\beta(k)$ **per feature map**, rather than per activation * properties * back-propagation through a layer is unaffected by the scale of its parameters * Moreover, larger weights lead to smaller gradients, thus stabilizing the parameter growth * regularizes the model：因为网络中mini-batch的数据之间是有互相影响的而非independent的 方法 batch normalization full whitening of each layer is costly so we normalize each layer independently, full set—&gt; mini-batch standard normal distribution并不是每个神经元所需的（如identity transform）：introduce, for each activation $x(k)$ , a pair of parameters $\gamma(k)$, $\beta(k)$, which scale and shift the normalized value to maintain the representation ability of the neuron bp： inference阶段： 首先两个可学习参数$\gamma$和$\beta$是定下来的 而均值和方差不再通过输入数据来计算，而是载入训练过程中维护的参数（moving averages） ​ for convolutional networks we add the BN transform immediately before the nonlinearity, $z = g(Wx+b)$ to $z = g(BN(Wx))$ since we normalize $Wx+b$, the bias b can be ignored obey the convolutional property——different elements of the same feature map, at different locations, are normalized in the same way We learn a pair of parameters $\gamma(k)$ and $\beta(k)$ per feature map, rather than per activation properties back-propagation through a layer is unaffected by the scale of its parameters Moreover, larger weights lead to smaller gradients, thus stabilizing the parameter growth regularizes the model：因为网络中mini-batch的数据之间是有互相影响的而非independent的 V3: Rethinking the Inception Architecture for Computer Vision 动机 go deeper and wider： enough labeled data computational efficiency parameter count to scale up networks utilizing the added computation as efficiently give general design principles and optimization ideas factorized convolutions aggressive regularization 论点 GoogleNet does not provide a clear description about the contributing factors that lead to the various design 方法 General Design Principles Avoid representational bottlenecks：特征图尺寸应该gently decrease，resolution的下降必须伴随着channel数的上升，避免使用max pooling层进行下采样，因为这样导致信息损失较大 Higher dimensional representations are easier to process locally within a network. Increasing the activa- tions per tile in a convolutional network allows for more disentangled features. The resulting networks will train faster：前半句懂了，high-reso的特征图focus在局部信息，后半句不懂，根据上一篇paper，用了batch norm以后，scale up神经元不影响bp，同时会lead to smaller gradients，为啥能加速？ Spatial aggregation can be done over lower dimensional embeddings：adjacent unit之间有strong correlation，所以可以reduce the dimension of the input representation before the spatial aggregation，不会有太大的信息损失，并且promotes faster learning The computational budget should therefore be distributed in a balanced way between the depth and width of the network. Factorizing Convolutions Filter Size into smaller convolutions 大filter都可以拆解成多个3x3 单纯去等价线性分解可以不使用非线性activation，但是我们使用了batch norm（increase variaty），所以观察到使用ReLU以后拟合效果更好 into Asymmetric Convolutions n*n的filter拆解成1*n和n*1 this factorization does not work well on early layers, but gives very good results on medium grid-sizes (ranges between 12 and 20, using 1x7 and 7x1 Utility of Auxiliary Classifiers did not result in improved convergence early in the training：训练开始阶段没啥用，快收敛时候有点点acc提升 removal of the lower auxiliary branch did not have any adverse effect on the final quality：拿掉对最终结果没影响 所以最初的设想（help evolving the low-level features） 是错的，仅仅act as regularizer，auxiliary head里面加上batch norm会使得最终结果better Efficient Grid Size Reduction下采样模块不再使用maxpooling dxdxk feature map expand to (d/2)x(d/2)x2k： 1x1x2k conv，stride2 pool：kxdxdx2k computation 1x1x2k stride2 conv：kx(d/2)x(d/2)x2k computation，计算量下降，但是违反principle1 parallel stride P and C blocks：kx(d/2)x(d/2)xk computation，符合principle1:reduces the grid-size while expands the filter banks Inception-v3 开头的7x7conv已经换成了多个3x3 中间层featuremap降维到17x17的时候，开始用Asymmetric Factorization block 到8x8的时候，做了expanding the filter bank outputs Label Smoothing （https://zhuanlan.zhihu.com/p/116466239） used the uniform distribution $u(k)=1/K$ q(k) = (1-\epsilon)\delta(k) + \frac{\epsilon}{K} 对于softmax公式：$p(k)=\frac{exp(y_k)}{\sum exp(y_i)}$，这个loss训练的结果就是$y_k$无限趋近于1，其他$y_i$无限趋近于0， 交叉熵loss：$ce=\sum -y_{gt}log(y_k)$，加了label smoothing以后，loss上增加了阴性样本的regularization，正负样本的最优解被限定在有限值，通过抑制正负样本输出差值，使得网络有更强的泛化能力。 V4: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning 动机 residual：whether there are any benefit in combining the Inception architecture with residual connections inceptionV4：simplify the inception blocks 论点 residual connections seems to improve the training speed greatly：但是没有也能训练深层网络 made uniform choices for the Inception blocks for each grid size Inception-A for 35x35 Inception-B for 17x17 Inception-C for 8x8 for residual versions use cheaper Inception blocks for residual versions：简化module，因为identity部分（直接相连的线）本身包含丰富的特征信息 没有使用pooling replace the filter concatenation stage of the Inception architecture with residual connections：原来block里面的concatenation主体放在残差path中 Each Inception block is followed by filter-expansion layer (1 × 1 convolution without activation) to match the depth of the input for addition：相加之前保证channel数一致 used batch-normalization only on top of the traditional layers, but not on top of the summations：浪费内存 number of filters exceeded 1000 causes instabilities scaling down the residuals before adding by factors between 0.1 and 0.3：残差通道响应值不要太大 blocks V4 ABC： Res ABC： Xception: Deep Learning with Depthwise Separable Convolutions 动机 Inception modules have been replaced with depthwise separable convolutions significantly outperforms Inception V3 on a larger dataset due to more efficient use of model parameters 论点 early LeNet-style models simple stacks of convolutions for feature extraction and max-pooling operations for spatial sub-sampling increasingly deeper complex blocks Inception modules inspired by NiN be capable of learning richer repre- sentations with less parameters The Inception hypothesis a single convolution kernel is tasked with simultaneously mapping cross-channel correlations and spatial correlations while Inception factors it into a series of operations that independently look at cross-channel correlations(1x1 convs) and at spatial correlations(3x3/5x5 convs) suggesting that cross-channel correlations and spatial correlations are sufficiently decoupled that it is preferable not to map them jointly inception block先用1x1的conv将原输出映射到3-4个lower space（cross-channel correlations），然后在这些小的3d spaces上做regular conv（maps all correlations ）——进一步假设，彻底解耦，第二步只做spatial correlations main differences between “extreme ” Inception and depthwise separable convolution order of the operations：1x1 first or latter non-linearity：depthwise separable convolutions are usually implemented without non-linearities【QUESTION：这和MobileNet里面说的不一样啊，M里面的depthwise也是每层都带了BN和ReLU的】 要素 Convolutional neural networks The Inception family Depthwise separable convolutions Residual connections 方法 architecture a linear stack of depthwise separable convolution layers with residual connections all conv are followed by BN keras的separableConv和depthwiseConv：前者由后者加上一个pointwiseConv组成，最后有activation，中间没有 cmp Xception and Inception V3 have nearly the same number of parameters marginally better on ImageNet much larger performance increasement on JFT Residual connections are clearly essential in helping with convergence, both in terms of speed and final classification performance. Effect of intermediate activation：the absence of any non-linearity leads to both faster convergence and better final performance EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks 动机 common sense：scaled up the network for better accuracy we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance propose a new scaling method：using compound coefficient to uniformly scale all dimensions of depth/width/resolution on MobileNets and ResNet a new baseline network family EfficientNets much better accuracy and efficiency 论点 previous work scale up one of the three dimensions depth：more layers width：more channels image resolution：higher resolution arbitrary scaling requires tedious manual tuning and often yields sub-optimal accuracy and efficiency uniformly scaling：Our empirical study shows that it is critical to balance all dimensions of network width/depth/resolution, and surprisingly such balance can be achieved by simply scaling each of them with constant ratio. neural architecture search：becomes increasingly popular in designing efficient mobile-size ConvNets 方法 problem formulation ConvNets：$N = \bigodot_{i=1…s} F_i^{L_i}(X_{})$, s for stage, L for repeat times, F for function simplify the design problem fixing $F_i$ all layers must be scaled uniformly with constant ratio an optimization problem：d for depth coefficients, w for width coefficients, r for resolution coefficients max_{d,w,r} \ \ \ Accuracy(N(d,w,r))\\ s.t. \ \ \ N(d,w,r) = \bigodot_{i=1...s} F_i^{d*L_i}(X_{}) observation 1 Scaling up any dimension of network (width, depth, or resolution) improves accuracy, but the accuracy gain diminishes for bigger models. 准确率都会提升，最终都会饱和 depth：deeper ConvNet can capture richer and more complex features width：wider networks tend to be able to capture more fine-grained features and are easier to train （commonly used for small size models）但是深度和宽度最好匹配，一味加宽shallow network会较难提取高级特征 resolution：higher resolution input can potentially capture more fine-grained patterns observation 2 compound scaling：it is critical to balance all dimensions of network width, depth, and resolution different scaling dimensions are not independent 输入更高的resolution，就需要更深的网络，以获取更大的感受野，同时还需要更宽的网络，以捕获更多的细粒度特征 compound coefficient $\phi$： d = \alpha ^ \phi\\ w = \beta ^ \phi\\ r = \gamma ^ \phi\\ s.t. \alpha * \beta^2 * \gamma^2 \approx 2, \ \alpha\geq1,\ \beta\geq1,\ \gamma\geq1 $\alpha, \beta, \gamma$ are constants determined by a small grid search, controling the assign among the 3 dimensions [d,w,r] $\phi$ controls how many more resources are available for model scaling the total FLOPS will approximately increase by $2^\phi$ efficientNet architecture having a good baseline network is also critical thus we developed a new mobile-size baseline called EfficientNet by leveraging a multi-objective neural architecture search that optimizes both accuracy and FLOPS compound scaling：fix $\phi=1$ and grid search $\alpha, \beta, \gamma$, fix $\alpha, \beta, \gamma$ and use different $\phi$ 实验 on MobileNets and ResNets compared to other single-dimension scaling methods compound scaling method improves the accuracy on all on EfficientNet model with compound scaling tends to focus on more relevant regions with more object details while other models are either lack of object details or unable to capture all objects in the images implementing details RMSProp: decay=0.9, momentum(rho)=0.9，tpu上使用lars BN: momentum=0.99 weight decay = 1e-5 lr: initial=0.256, decays by 0.97 every 2.4 epochs SiLU activation AutoAugment Stochastic depth: survive_prob = 0.8 dropout rate: 0.2 to 0.5 for B0 to B7 EfficientDet: Scalable and Efficient Object Detection 动机 model efficiency for object detection：based on one-stage detector 特征融合：propose a weighted bi-directional feature pyramid network (BiFPN) 网络rescale：uniformly scales the resolution, depth, and width for all backbone achieve better accuracy with much fewer parameters and FLOPs also test on Pascal VOC 2012 semantic segmentation 论点 previous work tends to achieve better efficiency by sacrificing accuracy previous work fuse feature at different resolutions by simply summing up without distinction EfficientNet backbone：combine EfficientNet backbones with our propose BiFPN scale up：jointly scales up the resolution/depth/width for all backbone, feature network, box/class prediction network Existing object detectors two-stage：have a region-of-interest proposal step one-stage：have not, use predefined anchors 方法 BiFPN：efficient bidirectional cross-scale connec- tions and weighted feature fusion FPN：limit是只有top-bottom一条information flow PANet：加上了一条bottom-up path，better accuracy但是more parameters and computations NAS-FPN：基于网络搜索出的结构，irregular and difficult to interpret or modify BiFPN remove those nodes that only have one input edge：只有一条输入的节点，没做到信息融合 add an extra edge from the original input to output node if they are at the same level：fuse more features without adding much cost repeat blocks Weighted Feature Fusion since different input features are at different resolutions, they usually contribute to the output feature unequally learnable weight that can be a scalar (per-feature), a vector (per-channel), or a multi-dimensional tensor (per-pixel) weight normalization Softmax-based：$O=\sum_i \frac{e^{w_i}}{\sum_j e^{w_j}}*I_i$ Fast normalized：$O=\sum_i \frac{w_i}{\epsilon + \sum_j w_j}*I_i$，Relu is applied after each $w_i$ to keep non-negative P_6^{td} = Conv(\frac{w_1P_6^{in}+w_2Resize(P_7^{in})}{\epsilon+w_1+w_2})\\ P_6^{out} = Conv(\frac{w_1P_6^{in}+w_2P_6^{td}+w_3Resize(P_5^{out})}{\epsilon+w_1+w_2+w_3}) EfficientDet ImageNet-pretrained Effi- cientNets as the backbone BiFPN serves as the feature network the fused features(level 3-7) are fed to a class and box network respectively compound scaling backbone：reuse the same width/depth scaling coefficients of EfficientNet-B0 to B6 feature network： depth(layers)：$D=3+\phi$ width(channes)：$W=64 \cdot (1.35^{\phi}) $ box/class prediction network： depth：$D=3+[\phi/3]$ width：same as FPN resolution use feature 3-7：must be dividable by $2^7$ $R=512+128*\phi$ EfficientDet-D0 ($\phi=0$) to D7 ($\phi=7$) 实验 for object detection train Learning rate is linearly increased from 0 to 0.16 in the first training epoch and then annealed down employ commonly-used focal loss 3x3 anchors compare low-accuracy regime：低精度下，EfficientDet-D0和yoloV3差不多 中等精度，EfficientDet-D1和Mask-RCNN差不多 EfficientDet-D7 achieves a new state-of-the-art for semantic segmentation modify keep feature level {P2,P3,…,P7} in BiFPN but only use P2 for the final per-pixel classification set the channel size to 128 for BiFPN and 256 for classification head Both BiFPN and classification head are repeated by 3 times compare 和deeplabv3比的，COCO数据集 better accuracy and fewer FLOPs ablation study backbone improves accuracy v.s. resnet50 BiFPN improves accuracy v.s. FPN BiFPN achieves similar accuracy as repeated FPN+PANet BiFPN + weghting achieves the best accuracy Normalized：softmax和fast版本效果差不多，每个节点的weight在训练开始迅速变化（suggesting different features contribute to the feature fusion unequally） Compound Scaling：这个比其他只提高一个指标的效果好就不用说了 超参： efficientNet和efficientDet的resolution是不一样的，因为检测还有neck和head，层数更深，所以resolution更大 EfficientNetV2: Smaller Models and Faster Training 动机 faster training speed and better parameter efficiency use a new op: Fused-MBConv propose progressive learning：adaptively adjuts regularization &amp; image size 方法 review of EfficientNet large image size large memory usage，small batch size，long training time thus propose increasing image size gradually in V2 extensive depthwise conv often cannot fully utilize modern accelerators thus introduce Fused-MBConv in V2：When applied in early stage 1-3, Fused-MBConv can improve training speed with a small overhead on parameters and FLOPs equally scaling up proved sub-optimal in nfnets since the stages are not equally contributed to the efficiency &amp; accuracy thus in V2 use a non-uniform scaling strategy：gradually add more layers to later stages(s5 &amp; s6) restrict the max image size EfficientNet V2 Architecture basic ConvBlock use fused-MBConv in the early layers use MBConv in the latter layers expansion ratios use smaller expansion ratios 因为同样的通道数，fused-MB比MB的参数量大 kernel size 全图3x3，没有5x5了 add more layers to compensate the reduced receptive field last stride 1 stage effv1是7个stage effv2有6个stage scaling policy compound scaling：R、W、D一起scale 但是限制了最大inference image size=480（train=384） gradually add more layers to later stages (s5 &amp; s6) progressive learning large models require stronger regularization larger image size leads to more computations with larger capacity，thus also needs stronger regularization training process in the early training epochs, we train the network with smaller images and weak regularization gradually increase image size but also making learning more difficult by adding stronger regularization adaptive params image size dropout rate randAug magnitude mixup alpha 给定最大最小值，stage N，使用linear interpolation train&amp;test details RMSProp optimizer with decay 0.9 and momentum 0.9 batch norm momentum 0.99 weight decay 1e-5 trained for 350 epochs with total batch size 4096 Learning rate is first warmed up from 0 to 0.256, and then decayed by 0.97 every 2.4 epochs exponential moving average with 0.9999 decay rate stochastic depth with 0.8 survival probability 4 stages (87 epochs per stage)：early stage with weak regularization &amp; later stronger maximum image size for training is about 20% smaller than inference &amp; no further finetuning]]></content>
      <tags>
        <tag>papers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FCN]]></title>
    <url>%2F2020%2F03%2F28%2FFCN%2F</url>
    <content type="text"><![CDATA[FCN: Fully Convolutional Networks for Semantic Segmentation 动机 take input of arbitrary size pixelwise prediction (semantic segmentation) efficient inference and learning end-to-end with superwised-pretraining 论点 fully connected layers brings heavy computation patchwise/proposals training with less efficiency (为了对一个像素分类，要扣它周围的patch，一张图的存储容量上升到k*k倍，而且相邻patch重叠的部分引入大量重复计算，同时感受野太小，没法有效利用全局信息) fully convolutional structure are used to get a feature extractor which yield a localized, fixed-length feature Semantic segmentation faces an inherent tension between semantics and location: global information resolves what while local information resolves where. Deep feature hierarchies jointly encode location and semantics in a local-to-global pyramid. other semantic works (RCNN) are not end-to-end 要素 把全连接层换成1*1卷积，用于提取特征，形成热点图 反卷积将小尺寸的热点图上采样到原尺寸的语义分割图像 a novel “skip” architecture to combine deep, coarse, semantic information and shallow, fine, appearance information 方法 fully convolutional network receptive fields: Locations in higher layers correspond to the locations in the image they are path-connected to typical recognition nets: fixed-input patches the fully connected layers can be viewed as convolutions with kernels that cover their entire input regions our structure: arbitrary-input the computation is saved by computing the overlapping regions of those patches only once output size corresponds to the input(H/16, W/16) heatmap: the (H/16 * W/16) high-dims feature-map corresponds to the 1000 classes coarse predictions to dense OverFeat introduced 对于高维特征图上一个元素，对应了原图感受野一片区域，将reception field中c位填上这个元素的值 移动原图，相应的感受野对应的图片也发生了移动，高维特征图的输出变了，c位变了 移动范围stride*stride，就会得到原图尺寸的输出了 upsampling simplest: bilinear interpolation in-network upsampling: backwards convolution (deconvolution) with an output stride of f A stack of deconvolution layers and activation functions can even learn a nonlinear upsampling factor: FCN里面inputsize和outputsize之间存在线性关系，就是所有卷积pooling层的累积采样步长乘积 kernelsize：$2 * factor - factor \% 2$ stride：$factor$ padding：$ceil((factor - 1) / 2.)$ 这块的计算有点绕，$stride=factor$比较好确定，这是将特征图恢复的原图尺寸要rescale的尺度。然后在输入的相邻元素之间插入s-1个0元素，原图尺寸变为$(s-1)(input_size-1)+input_size = sinput_size + (s-1)$，为了得到$output_size=s*input_size$输出，再至少$padding=[(s-1)/2]_{ceil}$，然后根据： (s-1) * (in-1) + in + 2p -k + 1 = out有： 2p-k+2 = s在keras里面可以调用库函数Conv2DTranspose来实现： 123456789101112131415161718192021x = Input(shape=(64,64,16))y = Conv2DTranspose(filters=16, kernel_size=20, strides=8, padding='same')(x)model = Model(x, y)model.summary()# input: (None, 64, 64, 16) output: (None, 512, 512, 16) params: 102,416x = Input(shape=(32,32,16))y = Conv2DTranspose(filters=16, kernel_size=48, strides=16, padding='same')(x)# input: (None, 32, 32, 16) output: (None, 512, 512, 16) params: 589,840x = Input(shape=(16,16,16))y = Conv2DTranspose(filters=16, kernel_size=80, strides=32, padding='same')(x)# input: (None, 16, 16, 16) output: (None, 512, 512, 16) params: 1,638,416# 参数参考：orig unet的total参数量为36,605,042# 各级transpose的参数量为：# (None, 16, 16, 512) 4,194,816# (None, 32, 32, 512) 4,194,816# (None, 64, 64, 256) 1,048,832# (None, 128, 128, 128) 262,272# (None, 256, 256, 32) 16,416 可以看到kernel_size变大，对参数量的影响极大。（kernel_size设置的小了，只能提取到单个元素，我觉得kernel_size至少要大于stride） Segmentation Architecture use pre-trained model convert all fully connected layers to convolutions append a 1*1 conv with channel dimension(including background) to predict scores followed by a deconvolution layer to upsample the coarse outputs to dense outputs skips the 32 pixel stride at the final prediction layer limits the scale of detail in the upsampled output 逐层upsampling，融合前几层的feature map，element-wise add finer layers: “As they see fewer pixels, the finer scale predictions should need fewer layers.” 这是针对前面的卷积网络来说，随着网络加深，特征图上的感受野变大，就需要更多的channel来记录更多的低级特征组合 add a 1*1 conv on top of pool4 (zero-initialized) adding a 2x upsampling layer on top of conv7 (We initialize this 2xupsampling to bilinear interpolation, but allow the parameters to be learned) sum the above two stride16 predictions (“Max fusion made learning difficult due to gradient switching”) 16x upsampled back to the image 做到第三行再往下，结果又会变差，所以做到这里就停下 总结 在升采样过程中，分阶段增大比一步到位效果更好 在升采样的每个阶段，使用降采样对应层的特征进行辅助 8倍上采样虽然比32倍的效果好了很多，但是结果还是比较模糊和平滑，对图像中的细节不敏感，许多研究者采用MRF算法或CRF算法对FCN的输出结果做进一步优化 x8为啥好于x32：1. x32的特征图感受野过大，对小物体不敏感 2. x32的放大比例造成的失真更大 unet的区别： unet没用imagenet的预训练模型，因为是医学图像 unet在进行浅层特征融合的时候用了concat而非element-wise add 逐层上采样，x2 vs. x8/x32 orig unet没用pad，输出小于输入，FCN则pad+crop 数据增强，FCN没用这些‘machinery’，医学图像需要强augmentation 加权loss ​]]></content>
      <tags>
        <tag>papers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cs231n-RNN-review]]></title>
    <url>%2F2020%2F03%2F15%2Fcs231n-RNN-review%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[attention系列]]></title>
    <url>%2F2020%2F03%2F13%2Fattention%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[0. 综述 attention的方式分为两种（Reference） 学习权重分布 部分加权（hard attention）／全部加权（soft attention） 原图上加权／特征图上加权 空间尺度加权／channel尺度加权／时间域加权／混合域加权 CAM系列、SE-block系列：花式加权，学习权重，non-local的模块，作用于某个维度 任务分解 设计不同的网络结构（或分支）专注于不同的子任务， 重新分配网络的学习能力，从而降低原始任务的难度，使网络更加容易训练 STN、deformable conv：添加显式的模块负责学习形变/receptive field的变化，local模块，apply by pixel local / non-local local模块的结果是pixel-specific的 non-local模块的结果是全局共同计算的的 基于权重的attention（Reference） 注意力机制通常由一个连接在原神经网络之后的额外的神经网络实现 整个模型仍然是端对端的，因此注意力模块能够和原模型一起同步训练 对于soft attention，注意力模块对其输入是可微的，所以整个模型仍可用梯度方法来优化 而hard attention要离散地选择其输入的一部分，这样整个系统对于输入不再是可微的 papers [STN] Spatial Transformer Networks [deformable conv] Deformable Convolutional Networks [CBAM] CBAM: Convolutional Block Attention Module [SE-Net] Squeeze-and-Excitation Networks [SE-block的一系列变体] SC-SE（for segmentation）、CMPE-SE（复杂又没用） [SK-Net] Selective Kernel Networks：是attension module，但是主要改进点在receptive field，trick大杂烩 [GC-Net] GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond CBAM: Convolutional Block Attention Module 动机 attention module lightweight and general improvements in classification and detection 论点 deeper： can obtain richer representation increased width：can outperform an extremely deep network cardinality：results in stronger representation power than depth and width attention：improves the representation of interests humans exploit a sequence of partial glimpses and selectively focus on salient parts Residual Attention Network：computes 3d attention map we decompose the process that learns channel attention and spatial attention separately SE-block：use global average-pooled features we suggest to use max-pooled features as well 方法 sequentially infers a 1D channel attention map and a 2D spatial attention map broadcast and element-wise multiplication Channel attention module focuses on ‘what’ is meaningful squeeze the spatial dimension use both average-pooled and max-pooled features simultaneously both descriptors are then forwarded to a shared MLP to reduce dimension 【QUESTION】看论文MLP是线性的吗，没写激活函数 then use element-wise summation sigmoid function Spatial attention module focuses on ‘where’ apply average-pooling and max-pooling along the channel axis and concatenate 7x7 conv sigmoid function Arrangement of attention modules in a parallel or sequential manner we found sequential better than parallel we found channel-first order slightly better than the spatial-first integration apply CBAM on the convolution outputs in each block in residual path before the add operation 实验 Ablation studies Channel attention：两个pooling path都有效，一起用最好 Spatial attention：1x1conv直接squeeze也行，avg+max更好，7x7conv略好于3x3conv arrangement：前面说了，比SE的单spacial squeeze好，channel在前好于在后，串行好于并行 Classification results：outperform baselines and SE Network Visualization cover the target object regions better the target class scores also increase accordingly Object Detection results apply to detectors：right before every classifier apply to backbone SK-Net: Selective Kernel Networks 动机 生物的神经元的感受野是随着刺激变化而变化的 propose a selective kernel unit adaptively adjust the RF multiple branches with different kernel sizes guided fusion 大杂烩：multi-branch&amp;kernel，group conv，dilated conv，attention mechanism SKNet by stacking multiple SK units 在分类任务上验证 论点 multi-scale aggregation inception block就有了 but linear aggregation approach may be insufficient multi-branch network two-branch：以resnet为代表，主要是为了easier to train multi-branch：以inception为代表，主要为了得到multifarious features grouped/depthwise/dilated conv grouped conv：reduce computation，提升精度 depthwise conv：reduce computation，牺牲精度 dilated conv：enlarge RF，比dense large kernel节省参数量 attention mechanism 加权系列： SENet&amp;CBAM： 相比之下SKNet多了adaptive RF 动态卷积系列： STN不好训练，训好以后变换就定死了 deformable conv能够在inference的时候也动态的变化变换，但是没有multi-scale和nonlinear aggregation thus we propose SK convolution multi-kernels：大size的conv kernel是用了dilated conv nonlinear aggregation computationally lightweight could successfully embedded into small models workflow split fuse select main difference from inception less customized adaptive selection instead of equally addition 方法 selective kernel convolution split multi-branch with different kernel size grouped/depthwise conv + BN + ReLU 5x5 kernel can be further replaced with dilated conv fuse to learn the control of information flow from different branches element-wise summation global average pooling fc-BN-ReLU：reduce dimension，at least 32 select channel-wise weighting factor A &amp; B &amp; more：A+B + more = 1 fc-softmax 在2分支的情况下，一个权重矩阵A就够了，B是冗余的，因为可以间接算出来 reweighting network start from resnext repeated SK units：类似bottleneck 1x1 conv SK conv 1x1 conv hyperparams number of branches M=2 group number G=32：cardinality of each path reduction ratio r=16：fuse operator中dim-reduction的参数 嵌入到轻量的网络结构 MobileNet/shuffleNet 把其中的3x3 depthwise卷积替换成SK conv 实验 比sort的resnet、densenet、resnext精度都要好 GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond 动机 Non-Local Network (NLNet) capture long-range dependencies obtain query-specific global context but we found global contexts are almost the same for different query positions we produce query-independent formulation smiliar structure as SE-Net aims at global context modeling 论点 Capturing long-range dependency mainly by sdeeply stacking conv layers：inefficient non-local network via self-attention mechanism computes the pairwise relations between the query position then aggregate 但是不同位置query得到的attention map基本一致 we simply the non-local block query-independent maintain acc &amp; save computation our proposed GC-block unifies both the NL block and the SE block three steps global context modeling： feature transform module：capture channel-wise interdependency fusion module：merge into the original features 多种任务上均有涨点 但都是在跟resnet50对比 revisit NLNet non-local block $f(x_i, x_j)$： encodes the relationship between position i &amp; j 计算方式有Gaussian、Embedded Gaussian、Dot product、Concat different instantiations achieve comparable performance $C(x)$：norm factor $x_i + \sum^{N_p} F(x_j)$：aggregates a specific global feature on $x_i$ widely-used Embedded Gaussian： 嵌入方式： Mask R-CNN with FPN and Res50 only add one non-local block right before the last residual block of res4 observations &amp; inspirations distances among inputs show that input features are discriminated outputs &amp; attention maps are almost the same：global context after training is actually independent of query position inspirations simplify the Non-local block no need of query-specific 方法 simplifying form of NL block：SNL 求一个common的global feature，share给全图每个position 进一步简化：把$x_j$的1x1 conv提到前面，FLOPs大大减少，因为feature scale从HW变成了1x1 the SNL block achieves comparable performance to the NL block with significantly lower FLOPs global context modeling SNL可以抽象成三部分： global attention pooling：通过$W_k$ &amp; softmax获取attention weights，然后进行global pooling feature transform：1x1 conv feature aggregation：broadcast element-wise add SE-block也可以分解成类似的抽象 global attention pooling：用了简单的global average pooling feature transform：用了squeeze &amp; excite的fc-relu-fc-sigmoid feature aggregation：broadcast element-wise multiplication Global Context Block integrate the benefits of both SNL global attention pooling：effective modeling on long-range dependency SE bottleneck transform：light computation（只要ratio大于2就会节省参数量和计算量） 特别地，在SE transform的squeeze layer上，又加了BN ease optimization benefit generalization fusion：add 嵌入方式： GC-ResNet50 add GC-block to all layers (c3+c4+c5) in resnet50 with se ratio of 16 relationship to SE-block 首先是fusion method reflects different goals SE基于全局信息rescales the channels，间接使用 GC直接使用，将long-range dependency加在每个position上 其次是norm layer ease optimization 最后是global attention pooling SE的GAP是a special case weighting factors shows superior]]></content>
      <tags>
        <tag>注意力机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deeplab系列]]></title>
    <url>%2F2020%2F02%2F24%2FDeeplab%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[综述 papers deeplabV1: SEMANTIC IMAGE SEGMENTATION WITH DEEP CONVOLUTIONAL NETS AND FULLY CONNECTED CRFS，主要贡献提出了空洞卷积，使得feature extraction阶段输出的特征图维持较高的resolution deeplabV2: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs，主要贡献是多尺度ASPP结构 deeplabV3: Rethinking Atrous Convolution for Semantic Image Segmentation，提出了基于ResNet的串行&amp;并行两种结构，细节上提到了multi-grid，改进了ASPP模块 deeplabV3+: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation 分割结果比较粗糙的原因 池化：将全图抽象化，降低分辨率，会丢失细节信息，平移不变性，使得边界信息不清晰 没有利用标签之间的概率关系：CNN缺少对空间、边缘信息等约束 对此，deeplabV1引入了 空洞卷积：VGG中提出的多个小卷积核代替大卷积核的方法，只能使感受野线性增长，而多个空洞卷积串联，可以实现指数增长。 全连接条件随机场CRF：作为stage2，提高模型捕获细节的能力，提升边界分割精度 大小物体同时分割 deeplabV2引入 多尺度ASPP(Atrous Spatial Pyramid Pooling)：并行的采用多个采样率的空洞卷积提取特征，再进行特征融合 backbone model change：VGG16改为ResNet 使用不同的学习率 进一步改进模型架构 deeplabV3引入 ASPP嵌入ResNet后几个block 去掉了CRF 使用原始的Conv/pool操作，得到的low resolution score map，pool stride会使得过程中丢弃一部分信息，上采样会得到较大的失真图像，使用空洞卷积，保留特征图上的全部信息，同时keep resolution，减少了信息损失 DeeplabV3的ASPP相比较于V2，增加了一条1x1 conv path和一条image pooling path，加GAP这条path是因为，实验中发现，随着rate的增大，有效的weight数目开始减少（部分超出边界无法有效捕捉远距离信息），因此利用global average pooling提取了image-level的特征并与ASPP的特征并在一起，来补充因为dilation丢失的信息 空洞卷积的path，V2是每条path分别空洞卷积然后接两个1x1conv（没有BN），V3是空洞卷积和BatchNormalization组合 fusion方式，V2是sum fusion，V3是所有path concat然后1x1 conv，得到最终score map DeeplabV3的串行版本，“In order to maintain original image size, convolutions are replaced with strous convolutions with rates that differ from each other with factor 2”，ppt上说后面几个block复制了block4，每个block里面三层conv，其中最后一层conv stride2，然后为了maintain output size，空洞rate*2，这个不太理解。 multi-grid method：对每个block里面的三层卷积采用不同空洞率，unit rate（e.g.(1,2,4)） * rate （e.g. 2） deeplabV1: SEMANTIC IMAGE SEGMENTATION WITH DEEP CONVOLUTIONAL NETS AND FULLY CONNECTED CRFS 动机 brings together methods from Deep Convolutional Neural Networks and probabilistic graphical models poor localization property of deep networks combine a fully connected Conditional Random Field (CRF) be able to localize segment boundaries beyond previous accuracies speed: atrous accuracy: simplicity: cascade modules 论点 DCNN learns hierarchical abstractions of data, which is desirable for high-level vision tasks (classification) but it hampers low-level tasks, such as pose estimation and semantic segmentation, where we want precise localization, rather than abstraction of spatial details two technical hurdles in DCNNs when applying to image labeling tasks pooling, loss of resolution: we employ the ‘atrous’ (with holes) for efficient dense computation spacial invariance: we use the fully connected pairwise CRF to capture fine edge details Our approach treats every pixel as a CRF node exploits long-range dependencies and uses CRF inference to directly optimize a DCNN-driven cost function 方法 structure fully convolutional VGG-16 keep the first 3 subsampling blocks for a target stride of 8 use hole algorithm conv filters for the last two blocks keep the pooling layers for the purpose of fine-tuing，change strides from 2 to 1 for dense map(h/8), the first fully convolutional 7*7*4096 is computational, thus change to 4*4 / 3*3 convs further computation decreasement: reduce the fc channels from 4096 to 1024 train label：ground truth subsampled by 8 loss function：cross-entropy test x8：simply bilinear interpolation fcn：stride32 forces them to use learned upsampling layers, significantly increasing the complexity and training time CRF short-range：used to smooth noisy fully connected model：to recover detailed local structure rather than further smooth it energy function: E(x) = \sum_{i}\theta_i(x_i) + \sum_{ij}\theta_{ij}(x_i, x_j)\\ \theta_i(x_i) = -logP(x_i)\\ $P(x_i)$ is the bi-linear interpolated probability output of DCNN. \theta_{ij}(x_i, x_j) = \mu(x_i, x_j)\sum_{m=1}^K \omega_m k^m (f_i,f_j)\\ \mu(x_i, x_j) = \begin{cases} 1& \text{if }x_i \neq x_j\\ 0& \text{otherwise} \end{cases} $k^m(f_i, f_j)$ is the Gaussian kernel depends on features (involving pixel positions &amp; pixel color intensities) multi-scale prediction to increase the boundary localization accuracy we attach to the input image and the output of each of the first four max pooling layers a two-layer MLP (first layer: 128 3x3 convolutional filters, second layer: 128 1x1 convolutional filters) the feature maps above is concatenated to the main network’s last layer feature map the new outputs is enhanced by 128*5=640 channels we only adjust the newly added weights introducing these extra direct connections from fine-resolution layers improves localization performance, yet the effect is not as dramatic as the one obtained with the fully-connected CRF 空洞卷积dilated convolution 空洞卷积相比较于正常卷积，多了一个 hyper-parameter——dilation rate，指的是kernel的间隔数量(正常的convolution dilatation rate是1) fcn：先pooling再upsampling，过程中有信息损失，能不能设计一种新的操作，不通过pooling也能有较大的感受野看到更多的信息呢？ 如图(b)的2-dilated conv，kernel size只有3x3，但是这个卷积的感受野已经增大到了7x7（假设前一层是3x3的1-dilated conv） 如图(c)的4-dilated conv，kernel size只有3x3，但是这个卷积的感受野已经增大到了15x15（假设前两层是3x3的1-dilated conv和3x3的2-dilated conv） 而传统的三个3x3的1-dilated conv堆叠，只能达到7x7的感受野 dilated使得在不做pooling损失信息的情况下，加大了感受野，让每个卷积输出都包含较大范围的信息 The Gridding Effect：如下图，多次叠加3x3的2-dilated conv，会发现我们将愿输入离散化了。因此叠加卷积的 dilation rate 不能有大于1的公约数。 Long-ranged information：增大dilation rate对大物体有效果，对小物体可能有弊无利 HDC(Hybrid Dilated Convolution)设计结构 叠加卷积的 dilation rate 不能有大于1的公约数，如[2,4,6] 将 dilation rate 设计成锯齿状结构，例如 [1, 2, 5, 1, 2, 5] 循环结构，锯齿状能够同时满足小物体大物体的分割要求(小 dilation rate 来关心近距离信息，大 dilation rate 来关心远距离信息) 满足$M_i = max [M_{i+1}-2r_i, M_{i+1}-2(M_{i+1}-r_i), r_i]$，$M_i$是第i层最大dilation rate 一个可行方案[1,2,5]： deeplabV2: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs 动机 atrous convolution：control the resolution atrous spatial pyramid pooling (ASPP) ：multiple sampling rates fully connected Conditional Random Field (CRF) 论点 three challenges in the application of DCNNs to semantic image segmentation reduced feature resolution：max-pooling and downsampling (‘striding’) —&gt; atrous convolution existence of objects at multiple scales：multi input scale —&gt; ASPP reduced localization accuracy due to DCNN invariance：skip-layers —&gt; CRF improvements compared to its first version better segment objects at multiple scales ResNet replaces VGG16 a more comprehensive experimental evaluation on models &amp; dataset related works jointly learning of the DCNN and CRF to form an end-to-end trainable feed-forward network while in our work still a 2 stage process use a series of atrous convolutional layers with increasing rates to aggregate multiscale context while in our structure using parallel instead of serial 方法 atrous convolution 在下采样以后的特征图上，运行普通卷积，相当于在原图上运行上采样的filter 1-D示意图上可以看出，两者感受野相同 同时能保持high resolution while both the number of filter parameters and the number of operations per position stay constant 把backbone中下采样的层(pooling/conv)中的stride改成1，然后将接下来的conv层都改成2-dilated conv：could allow us to compute feature responses at the original image resolution efficiency/accuracy trade-off：using atrous convolution to increase the resolution by a factor of 4 followed by fast bilinear interpolation by a factor of 8 to the original image resolution Bilinear interpolation is sufficient in this setting because the class score maps are quite smooth unlike FCN Atrous convolution offers easily control of the field-of-view and finds the best trade-off between accurate localization (small field-of-view) and context assimilation (large field-of-view)：大感受野，抽象融合上下文，大感受野，low-level局部信息准确 实现：（1）根据定义，给filter上采样，插0；（2）给feature map下采样得到k*k个reduced resolution maps，然后run orgin conv，组合位移结果 ASPP multi input scale： run parallel DCNN branches that share the same parameters fuse by taking at each position the maximum response across scales computing spatial pyramid pooling run multiple parallel filters with different rates multi-scale features are further processed in separate branches：fc7&amp;fc8 fuse：sum fusion CRF：keep the same as V1 deeplabV3: Rethinking Atrous Convolution for Semantic Image Segmentation 动机 for segmenting objects at multiple scales employ atrous convolution in cascade or in parallel with multiple atrous rates augment ASPP with image-level features encoding global context and further boost performance without DenseCRF 论点 our proposed module consists of atrous convolution with various rates and batch normalization layers modules in cascade or in parallel：when applying a 3*3 atrous convolution with an extremely large rate, it fails to capture long range information due to image boundary effects 方法 Atrous Convolution for each location $i$ on the output $y$ and a filter $w$, an $r$-rate atrous convolution is applied over the input feature map $x$： y[i] = \sum_k x[i+rk]w[k] in cascade duplicate several copies of the last ResNet block (block4) extra block5, block6, block7 as replicas of block4 multi-rates ASPP we include batch normalization within ASPP as the sampling rate becomes larger, the number of valid filter weights becomes smaller (beyond boundary) to incorporate global context information：we adopt image-level features by GAP on the last feature map of the model GAP —&gt; 1*1*256 conv —&gt; BN —&gt; bilinearly upsample fusion: concatenated + 1*1 conv seg：final 1*1*n_classes conv training details large crop size required to make sure the large atrous rates effective upsample the output: it is important to keep the groundtruths intact and instead upsample the final logits 结论 output stride=8 好过16，但是运算速度慢了几倍 deeplabV3+: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation 动机 spatial pyramid pooling module captures rich contextual information encode-decoder structure captures sharp object boundaries combine the above two methods propose a simple yet effective decoder module explore Xception backbone 论点 even though rich semantic information is encoded through ASPP, detailed information related to object boundaries is missing due to striding operations atrous convolution could alleviate but suffer the computational balance while encoder-decoder models lend themselves to faster computation (since no features are dilated) in the encoder path and gradually recover sharp object boundaries in the decoder path 所谓encoder-decoder structure，就是通过encoder和decoder之间的短连接来将不同尺度的特征集成起来，增加这样的shortcut，同时增大网络的下采样率（encoder path上不使用空洞卷积，因此为了达到同样的感受野，得增加pooling，然后保留最底端的ASPP block），既减少了计算，又enrich了local border这种细节特征 applying the atrous separable convolution to both the ASPP and decoder modules：最后又引入可分离卷积，进一步提升计算效率 方法 atrous separable convolution significantly reduces the computation complexity while maintaining similar (or better) performance DeepLabv3 as encoder output_stride=16/8：remove the striding of the last 1/2 blocks atrous convolution：apply atrous convolution to the blocks without striding ASPP：run 1x1 conv in the end to set the output channel to 256 proposed decoder naive decoder：bilinearly upsampled by 16 proposed：first bilinearly upsampled by 4, then concatenated with the corresponding low-level features low-level features： apply 1x1 conv on the low-level features to reduce the number of channels to avoid outweigh the importance the last feature map in res2x residual block before striding combined features：apply 3x3 conv(2 layers, 256 channels) to obtain sharper segmentation results more shortcut：observed no significant improvement modified Xception backbone deeper all the max pooling operations are replaced with depthwise separable convolutions with striding DWconv-BN-ReLU-PWconv-BN-ReLU 实验 decoder effect on border f Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation]]></content>
      <tags>
        <tag>papers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RCNN系列]]></title>
    <url>%2F2020%2F01%2F08%2FRCNN%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[综述 papers [R-CNN] R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation [SPP] SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition [Fast R-CNN] Fast R-CNN: Fast Region-based Convolutional Network [Faster R-CNN] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks [Mask R-CNN] Mask R-CNN [FPN] FPN: Feature Pyramid Networks for Object Detection [Cascade R-CNN] Cascade R-CNN: Delving into High Quality Object Detection R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation 动机 localizing objects with a deep network and training a high-capacity model with only a small quantity of annotated detection data apply CNN to region proposals: R-CNN represents ‘Regions with CNN features’ supervised pre-training 论点 model as a regression problem: not fare well in practice build a sliding-window detector: have to maintain high spatial resolution what we do: our method gener- ates around 2000 category-independent region proposals for the input image, extracts a fixed-length feature vector from each proposal using a CNN, and then classifies each region with category-specific linear SVMs conventional solution to training a large CNN is ‘using unsupervised pre-training, followed by supervised fine-tuning’ what we do: ‘supervised pre-training on a large auxiliary dataset (ILSVRC), followed by domain specific fine-tuning on a small dataset (PASCAL)’ we also demonstrate: a simple bounding box regression method significantly reduces mislocalizations R-CNN operates on regions: it is natural to extend it to the task of semantic segmentation 要素 category-independent region proposals a large convolutional neural network that extracts a fixed-length feature vector from each region a set of class-specific linear SVMs 方法 Region proposals: we use selective search Feature extraction: we use Krizhevsky CNN, 227*227 RGB input, 5 convs, 2 fcs, 4096 output we first dilate the tight bounding box (padding=16) then warp the bounding box to the required size (各向异性缩放) Test-time detection: we score each extracted feature vector using the SVM trained for each class we apply a greedy non-maximum suppression (for each class independently) 对留下的这些框进行canny边缘检测，就可以得到bounding-box (then B-BoxRegression) Supervised pre-training: pre-trained the CNN on a large auxiliary dataset (ILSVRC 2012) with image-level annotations Domain-specific fine-tuning: continue SGD training of the CNN using only warped region proposals from VOC replace the 1000-way classification layer with a randomly initialized 21-way layer (20 VOC classes plus background) class label: all region proposals with ≥ 0.5 IoU overlap with a ground-truth box as positives, else negatives 1/10th of the initial pre-training rate uniformly sample 32 positive windows (over all classes) and 96 background windows to construct a mini-batch of size 128 Object category classifiers: considering a binary classifier for a specific class class label: take IoU overlap threshold &lt;0.3 as negatives, take only regions tightly enclosing the object as positives take the ground-truth bounding boxes for each class as positives unexplained: the positive and negative examples are defined differently in CNN fine-tuning versus SVM training CNN容易过拟合，需要大量的训练数据，所以在CNN训练阶段我们对Bounding box的位置限制条件限制的比较松(IOU只要大于0.5都被标注为正样本)，svm适用于少样本训练，所以对于训练样本数据的IOU要求比较严格，我们只有当bounding box把整个物体都包含进去了，我们才把它标注为物体类别。 it’s necessary to train detection classifiers rather than simply use outputs of the fine-tuned CNN 上一个回答其实同时也解释了CNN的head已经是一个分类器了，还要用SVM分类：按照上述正负样本定义，CNN softmax的输出比采用svm精度低。 分析 learned features: compute the units’ activations on a large set of held-out region proposals sort from the highest to low perform non-maximum suppression display the top-scoring regions Ablation studies: without fine-tuning: features from fc7 generalize worse than features from fc6, indicating that most of the CNN’s representational power comes from its convolutional layers with fine-tuning: The boost from fine-tuning is much larger for fc6 and fc7 than for pool5, suggests that pool features learned from ImageNet are general and that most of the improvement is gained from learning domain-specific non-linear classifiers on top of them Detection error analysis: more of our errors result from poor localization rather than confusion CNN features are much more discriminative than HOG Loose localization likely results from our use of bottom-up region proposals and the positional invariance learned from pre-training the CNN for whole-image classification(粗暴的IOU判定前背景，二值化label，无法体现定位好坏差异) Bounding box regression： a linear regression model use the pool5 features for a selective search region proposal as input 输出为xy方向的缩放和平移 训练样本：判定为本类的候选框中和真值重叠面积大于0.6的候选框 Semantic segmentation： three strategies for computing features: ‘full ‘ ignores the region’s shape, two regions with different shape might have very similar bounding boxes(信息不充分) ‘fg ‘ slightly outperforms full, indicating that the masked region shape provides a stronger signal ‘full+fg ‘ achieves the best, indicating that the context provided by the full features is highly informative even given the fg features(形状和context信息都重要) SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition 动机： propose a new pooling strategy, “spatial pyramid pooling” can generate a fixed-length representation regardless of image size/scale also robust to object deformations 论点： existing CNNs require a fixed-size input reduce accuracy for sub-images of an arbitrary size/scale (need cropping/warping) cropped region lost content, while warped content generates unwanted distortion overlooks the issues involving scales convolutional layers do not require a fixed image size, whle the fully-connected layers need to have fixed- size/length input by their definition by introducing the SPP layer between the last convolutional layer and the first fully-connected layer pools the features and generates fixed- length outputs Spatial pyramid pooling partitions the image into divisions from finer to coarser levels, and aggregates local features in them generates fixed- length output uses multi-level spatial bins(robust to object deformations ) can run at variable scales also allows varying sizes or scales during training: train the network with different input size at different epoch increases scale-invariance reduces over-fitting in object detection run the convolutional layers only once on the entire image then extract features by SPP-net on the feature maps speedup accuracy 方法： Convolutional Layers and Feature Maps the outputs of the convolutional layers are known as feature maps feature maps involve not only the strength of the responses(the strength of activation), but also their spatial positions(the reception field) The Spatial Pyramid Pooling Layer it can maintain spatial information by pooling in local spatial bins the spatial bins have sizes proportional to the image size(k-level: 1*1, 2*2, …, k*k) we can resize the input image to any scale, which is important for the accuracy the coarsest pyramid level has a single bin that covers the entire image, which is in fact a “global pooling” operation for a feature map of $a×a$, with a pyramid level of $n×n$ bins: the\ window\ size:\ win = ceiling(a/n)\\ the\ stride:\ str = floor(a/n) Training the Network Single-size training: fixed-size input (224×224) cropped from images, cropping for data augmentation Multi-size training: rather than cropping, we resize the aforementioned 224×224 region to 180×180, then we train two fixed-size networks that share parameters by altenate epoch 分析 50 bins vs. 30 bins: the gain of multi-level pooling is not simply due to more parameters, it is because the multi-level pooling is robust to the variance in object deformations and spatial layout multi-size vs. single-size: multi results are more or less better than the single-size version full vs. crop: shows the importance of maintaining the complete content SPP-NET FOR OBJECT DETECTION We extract the feature maps from the entire image only once we apply the spatial pyramid pooling on each candidate window of the feature maps These representations are provided to the fully-connected layers of the network SVM samples: We use the ground-truth windows to generate the positive samples, use the samples with IOU&lt;30% as the negative samples multi-scale feature extraction: We resize the image at {480, 576, 688, 864, 1200}, and compute the feature maps of conv5 for each scale. we choose a single scale s ∈ S such that the scaled candidate window has a number of pixels closest to 224×224. And we use the corresponding feature map to compute the feature for this window this is roughly equivalent to resizing the window to 224×224 fine-tuning: Since our features are pooled from the conv5 feature maps from windows of any sizes for simplicity we only fine-tune the fully-connected layers Mapping a Window to Feature Maps** we project the corner point of a window onto a pixel in the feature maps, such that this corner point in the image domain is closest to the center of the receptive field of that feature map pixel. ​ 确定原图上的两个角点（左上角和右下角），映射到 feature map上的两个对应点，使得映射点$(x^{‘}, y^{‘})$在原始图上感受野（上图绿色框）的中心点与$(x,y)$尽可能接近。 Fast R-CNN: Fast Region-based Convolutional Network 动机 improve training and testing speed increase detection accuracy 论点 current approaches train models in multi-stage pipelines that are slow and inelegant R-CNN &amp; SPPnet: CNN+SVM+bounding-box regression disk storage: features are written to disk SPPnet: can only fine-tuning the fc layers, limits the accuracy of very deep networks task complexity: numerous candidate proposals rough localization proposals must be refined We propose: a single-stage training algorithm multi-task: jointly learns to classify object proposals and refine their spatial locations 要素 input: an entire image and a set of object proposals convs a region of interest (RoI) pooling layer: extracts a fixed-length feature vector from the feature map fcs that finally branch into two sibling output layers multi-outputs: one produces softmax probability over K+1 classes one outputs four bounding-box regression offsets per class 方法 RoI pooling an RoI is a rectangular window inside a conv feature map, which can be defined by (r, c, h, w) the RoI pooling layer converts the features inside any valid RoI into a small feature map with a fixed size H × W it is a special case of SPPnet when there is only one pyramid level (pooling window size = h/H * w/W) Initializing from pre-trained networks the last max pooling layer is replaced by a RoI pooling layer the last fully connected layer and softmax is replaced by the wo sibling layers + respective head (softmax &amp; regressor) modified to take two inputs Fine-tuning for detection why SPPnet is unable to update weights below the spatial pyramid pooling layer: 原文提到feature vector来源于不同尺寸的图像——不是主要原因 feature vector在原图上的感受野通常很大（接近全图）——forward pass的计算量就很大 不同的图片forward pass的计算结果不能复用（when each training sample (i.e. RoI) comes from a different image, which is exactly how R-CNN and SPPnet networks are trained） We propose: takes advantage of feature sharing mini-batches are sampled hierarchically: N images and R/N RoIs from each image RoIs from the same image share computation and memory in the forward and backward passes jointly optimize the two tasks each RoI is labeled with a ground-truth class $u$ and a ground-truth bounding-box regression target $v$ the network outputs are K+1 probability $p=(p_0,…p_k)$ and K b-box regression offsets $t^k=(t_x^k, t_y^k, t_w^k,t_h^k)$ L(p, u, t^u, v) = L_{cls}(p,u) + \lambda[u>0]L_{loc}(t^u,v)\\ $L_{cls}$: L_{cls}(p,u) = -log p_u\\ $L_{loc}$: L_{loc}(t^u, v) = \sum_{i \in \{x,y,w,h\}}smooth_{L_1}(t^u_i - v_i)\\ smooth_{L_1}(x) = \begin{cases} 0.5x^2\ \ \ \ \ \ \ \ \ \ \ if |x|]]></content>
      <tags>
        <tag>目标检测，two-stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN Visualization系列]]></title>
    <url>%2F2020%2F01%2F03%2FCNN-Visualization%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[1. Visualizing and Understanding Convolutional Networks 动机 give insight into the internal operation and behavior of the complex models then one can design better models reveal which parts of the scene in image are important for classification explore the generalization ability of the model to other datasets 论点 most visualizing methods limited to the 1st layer where projections to pixel space are possible Our approach propose a method that could projects high level feature maps to the pixel space * some methods give some insight into invariances basing on a simple quadratic approximation * Our approach, by contrast, provides a non-parametric view of invariance * some methods associate patches that responsible for strong activations at higher layers * In our approach they are not just crops of input images, but rather top-down projections that reveal structures 方法 3.1 Deconvnet: use deconvnet to project the feature activations back to the input pixel space To examine a given convnet activation, we set all other activations in the layer to zero and pass the feature maps as input to the attached deconvnet layer Then successively (i) unpool, (ii) rectify and (iii) filter to reconstruct the activity of the layer beneath until the input pixel space is reached 【Unpooling】using switches 【Rectification】the convnet uses relu to ensure always positive, same for back projection 【Filtering】transposed conv Due to unpooling, the reconstruction obtained from a single activation resembles a small piece of the original input image 3.2 CNN model 3.3 visualization among layers for each layer, we take the top9 strongest activation across the validation data calculate the back projection separately alongside we provide the corresponding image patches 3.4 visualization during training randomly choose several strongest activation of a given feature map lower layers converge fast, higher layers conversely 3.5 visualizing the Feature Invariance 5 sample images being translated, rotated and scaled by varying degrees Small transformations have a dramatic effect in the first layer of the model(c2 &amp; c3对比) the network is stable to translations and scalings, but not invariant to rotation 3.6 architecture selection old architecture(stride4, filterSize11)：The first layer filters are a mix of extremely high and low frequency information, with little coverage of the mid frequencies. The 2nd layer visualization shows aliasing artifacts caused by the large stride 4 used in the 1st layer convolutions. (这点可以参考之前vnet中提到的，deconv导致的棋盘格伪影，大stride会更明显) smaller stride &amp; smaller filter(stride2, filterSize7)：more coverage of mid frequencies, no aliasing, no dead feature 3.7 对于物体的关键部分遮挡之后会极大的影响分类结果 第二个和第三个例子中分别是文字和人脸的响应更高，但是却不是关键部分。 理解 4.1 总的来说，网络学习到的特征，是具有辨别性的特征，通过可视化就可以看到我们提取到的特征忽视了背景，而是把关键的信息给提取出来了。从layer 1、layer 2学习到的特征基本上是颜色、边缘等低层特征；layer 3则开始稍微变得复杂，学习到的是纹理特征，比如上面的一些网格纹理；layer 4学习到的则是较多的类别信息，比如狗头；layer 5对应着更强的不变性，可以包含物体的整体信息。。 4.2 在网络迭代的过程中，特征图出现了sudden jumps。低层在训练的过程中基本没啥变化，比较容易收敛，高层的特征学习则变化很大。这解释了低层网络的从训练开始，基本上没有太大的变化，因为梯度弥散。高层网络刚开始几次的迭代，变化不是很大，但是到了40~50的迭代的时候，变化很大，因此我们以后在训练网络的时候，不要着急看结果，看结果需要保证网络收敛。 4.3 图像的平移、缩放、旋转，可以看出第一层中对于图像变化非常敏感，第7层就接近于线性变化。 2. Striving for Simplicity: The All Convolutional Net 动机 traditional pipeline: alternating convolution and max-pooling layers followed by a small number of fully connected layers questioning the necessity of different components in the pipeline, max-pooling layer to be specified to analyze the network we introduce a new variant of the “deconvolution approach” for visualizing features 论点 two major improving directions based on traditional pipeline using more complex activation functions building multiple conv modules we study the most simple architecture we could conceive a homogeneous network solely consisting of convolutional layers without the need for complicated activation functions, any response normalization or max-pooling reaches state of the art performance 方法 replace the pooling layers with standard convolutional layers with stride two the spatial dimensionality reduction performed by pooling makes covering larger parts of the input in higher layers possible which is crucial for achieving good performance with CNNs make use of small convolutional layers greatly reduce the number of parameters in a network and thus serve as a form of regularization if the topmost convolutional layer covers a portion of the image large enough to recognize its content then fully connected layers can also be replaced by simple 1-by-1 convolutions the overall architecture consists only of convolutional layers with rectified linear non-linearities and an averaging + softmax layer to produce predictions Strided-CNN-C: pooling is removed and the preceded conv stride is increase ConvPool-CNN-C: a dense conv is placed, to show the effect of increasing parameters All-CNN-C: max-pooling is replaced by conv when pooling is replaced by an additional convolution layer with stride 2, performance stabilizes and even improves small 3 × 3 convolutions stacked after each other seem to be enough to achieve the best performance guided backpropagation the paper above proposed ‘deconvnet’, which we observe that it does not always work well without max-pooling layers For higher layers of our network the method of Zeiler and Fergus fails to produce sharp, recognizable image structure Our architecture does not include max-pooling, thus we can ’deconvolve’ without switches, i.e. not conditioning on an input image In order to obtain a reconstruction conditioned on an input image from our network without pooling layers we to combine the simple backward pass and the deconvnet Interestingly, the very first layer of the network does not learn the usual Gabor filters, but higher layers do 3. Cam: Learning Deep Features for Discriminative Localization 动机 we found that CNNs actually behave as object detectors despite no supervision on the location this ability is lost when fully-connected layers are used for classification we found that the advantages of global average pooling layers are beyond simply acting as a regularizer it makes it easily to localize the discriminative image regions despite not being trained for them 论点 2.1 Weakly-supervised object localization previous methods are not trained end-to-end and require multiple forward passes Our approach is trained end-to-end and can localize objects in a single forward pass 2.2 Visualizing CNNs previous methods only analyze the convolutional layers, ignoring the fully connected thereby painting an incomplete picture of the full story we are able to understand our network from the beginning to the end 方法 3.1 Class Activation Mapping A class activation map for a particular category indicates the discriminative image regions used by the network to identify that category the network architecture: convs—-gap—-fc+softmax we can identify the importance of the image regions by projecting back the weights of the output layer on to the convolutional feature maps by simply upsampling the class activation map to the size of the input image we can identify the image regions most relevant to the particular category 3.2 Weakly-supervised Object Localization our technique does not adversely impact the classification performance when learning to localize we found that the localization ability of the networks improved when the last convolutional layer before GAP had a higher spatial resolution, thus we removed several convolutional layers from the origin networks overall we find that the classification performance is largely preserved for our GAP networks compared with the origin fc structure our CAM approach significantly outperforms the backpropagation approach on generating bounding box low mapping resolution prevents the network from obtaining accurate localizations 3.3 Visualizing Class-Specific Units the convolutional units of various layers of CNNs act as visual concept detec- tors, identifying low-level concepts like textures or mate- rials, to high-level concepts like objects or scenes Deeper into the network, the units become increasingly discriminative given the fully-connected layers in many networks, it can be difficult to identify the importance of different units for identifying different categories 4. Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization5. Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks6. 综述 GAP 首先回顾一下GAP，NiN中提出了GAP，主要为了解决全连接层参数过多，不易训练且容易过拟合等问题。 对大多数分类任务来说不会因为做了gap让特征变少而让模型性能下降。因为GAP层是一个非线性操作层，这C个特征相当于是从kxkxC经过非线性变化选择出来的强特征。 heatmap step1. 图像经过卷积网络后最后得到的特征图，在全连接层分类的权重（$w_{k,n}$）肯定不同， step2. 利用反向传播求出每张特征图的权重， step3. 用每张特征图乘以权重得到带权重的特征图，在第三维求均值，relu激活，归一化处理 relu只保留wx大于0的值——我们正响应是对当前类别有用的特征，负响应会拉低$\sum wx$，即会降低当前类别的置信度 如果没有relu，定位图谱显示的不仅仅是某一类的特征。而是所有类别的特征。 step4. 将特征图resize到原图尺寸，便于叠加显示 CAM CAM要求必须使用GAP层， CAM选择softmax层值最大的节点反向传播，求GAP层的梯度作为特征图的权重，每个GAP的节点对应一张特征图。 Grad-CAM Grad-CAM不需要限制模型结构， Grad-CAM选择softmax层值最大的节点反向传播，对最后一层卷积层求梯度，用每张特征图的梯度的均值作为该特征图的权重。]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NiN: network in network]]></title>
    <url>%2F2019%2F12%2F25%2FNiN-network-in-network%2F</url>
    <content type="text"><![CDATA[Network In Network 动机 enhance model discriminability(获得更好的特征描述)：propose mlpconv less prone to overfitting：propose global average pooling 论点 comparison 1: conventional CNN uses linear filter, which implicitly makes the assumption that the latent concepts are linearly separable. traditional CNN is stacking [linear filters+nonlinear activation/linear+maxpooling+nonlinear]：这里引出了一个激活函数和池化层先后顺序的问题，对于avg_poolling，两种操作得到的结果是不一样的，先接激活函数会丢失部分信息，所以应该先池化再激活，对于MAX_pooling，两种操作结果一样，但是先池化下采样，可以减少激活函数的计算量，总结就是先池化再激活。但是好多网络实际实现上都是relu紧跟着conv，后面接pooling，这样比较interpretable——cross feature map pooling mlpconv layer can be regarded as a highly nonlinear function(filter-fc-activation-fc-activation-fc-activation…) comparison 2: maxout network imposes the prior that instances of a latent concept lie within a convex set in the input space【QUESTION HERE】 mlpconv layer is a universal function approximator instead of a convex function approximator comparison 3: fully connected layers are prone to overfitting and heavily depend on dropout regularization global average pooling is more meaningful and interpretable, moreover it itself is a structural regularizer【QUESTION HERE】 方法 use mlpconv layer to replace conventional GLM(linear filters) use global average pooling to replace traditional fully connected layers the overall structure is a stack of mlpconv layers, on top of which lie the global average pooling and the objective cost layer Sub-sampling layers can be added in between the mlpconv as in CNN dropout is applied on the outputs of all but the last mlpconv layers for regularization another regularizer applied is weight decay 细节 preprocessing：global contrast normalization and ZCA whitening augmentation：translation and horizontal flipping GAP for conventional CNN：CNN+FC+DROPOUT &lt; CNN+GAP &lt; CNN+FC gap is effective as a regularizer slightly worse than the dropout regularizer result for some reason confidence maps explicitly enforce feature maps in the last mlpconv layer of NIN to be confidence maps of the categories by means of global average pooling：NiN将GAP的输出直接作为output layer，因此每一个类别对应的feature map可以近似认为是 confidence map。 the strongest activations appear roughly at the same region of the object in the original image：特征图上高响应区域基本与原图上目标区域对应。 this motivates the possibility of performing object detection via NIN architecture：实际中多层感知器使用1x1conv来实现，增加的多层感知器相当于是一个含参的池化层，通过对多个特征图进行含参池化，再传递到下一层继续含参池化，这种级联的跨通道的含参池化让网络有了更复杂的表征能力。 总结 mlpconv：stronger local reception unit gap：regularizer &amp; bring confidence maps]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[unet & vnet]]></title>
    <url>%2F2019%2F12%2F05%2Funet-vnet%2F</url>
    <content type="text"><![CDATA[U-NET: Convolutional Networks for Biomedical Image Segmentation 动机： train from very few images outperforms more precisely on segmentation tasks fast 要素： 编码：a contracting path to capture context 解码：a symmetric expanding path that enables precise localization 实现：pooling operators &amp; upsampling operators 论点： when we talk about deep convolutional networks： larger and deeper millions of parameters millions of training samples representative method：run a sliding-window and predict a pixel label based on its‘ patch drawbacks： calculating redundancy of overlapping patches big patch：more max-pooling layers that reduce the localization accuracy small patch：less involvement of context metioned but not further explained：cascade structure 方法： In order to localize, high resolution features from the contracting path are combined with the upsampled output. A successive convolution layer can then learn to assemble a more precise output based on this information. 理解：深层特征层感受野较大，带有全局信息，将其上采样用于提供localization information，而横向add过来特征层带有局部特征信息。两个3*3的conv block用于将两类信息整合，输出更精确的表达。 In the upsampling part we have also a large number of feature channels, which allow the network to propagate context information to higher resolution layers. 理解：应该是字面意思吧，为上采样的卷积层保留更多的特征通道，就相当于保留了更多的上下文信息。 we use excessive data augmentation. 细节： contracting path： typical CNN：blocks of [2 3*3 unpadded convs+ReLU+2*2 stride2 maxpooling] At each downsampling step we double the number of feature channels expansive path： upsampling： 2*2 up-conv that half the channels concatenation the corresponding cropped feature map from the contracting path 2 [3x3 conv+ReLU] final layer：use a 1*1 conv to map the feature vectors to class vectors train： prefer larger input size to larger batch size sgd with 0.99 momentum so that the previously seen samples dominate the optimization loss：softmax &amp; cross entropy unbalanced weight： pre-compute the weight map base on the frequency of pixels for a certain class add the weight for a certain element to force the learning emphasis：e.g. the small separation borders initialization：Gaussian distribution data augmentation： deformations “Drop-out layers at the end of the contracting path perform further implicit data augmentation” metrics：“warping error”, the “Rand error” and the “pixel error” for EM segmentation challenge and average IOU for ISBI cell tracking challenge prediction： 按照论文的模型结构，输入和输出的维度是不一样的——在valid padding的过程中有边缘信息损失。 那么如果我们想要预测黄框内的分割结果，需要输入一张更大的图（蓝框）作为输入，在图片边缘的时候，我们通过镜像的方式补全。 因果关系： 首先因为内存限制，输入的不是整张图，是图片patch， 为了保留上下文信息，使得预测更准确，我们给图片patch添加一圈border的上下文信息（实际感兴趣的是黄框区域） 在训练时，为了避免重叠引入的计算，卷积层使用了valid padding 因此在网络的输出层，输出尺寸才是我们真正关注的部分 如果训练样本尺寸不那么huge，完全可以全图输入，然后使用same padding，直接预测全图mask 总结： train from very few images —-&gt; data augmentation fast —-&gt; full convolution layers precise —-&gt; global? V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation 动机 entire 3D volume imbalance between the number of foreground and background voxels：dice coefficient limited data：apply random non-linear transformations and histogram matching fast and accurate 论点： early approaches based on patches local context challenging modailities efficiency issues fully convolutional networks 2D so far imbalance issue：the anatomy of interest occupies only a very small region of the scan thus predictions are strongly biased towards the background. re-weighting dice coefficient claims to be better that above 要素： a compression path a decompression path 方法： compression： add residual能够加速收敛 resolution is reduced by [2*2*2 conv with stride 2]相比于maxpooling节省了bp所需switch map的memory消耗 double the number of feature maps as we reduce their resolution PReLU decompression： horizontal connections：1) gather fine grained detail that would be otherwise lost in the compression path 2) improve the convergence time residual conv：blocks of [5*5*5 conv with stride 1] 提取特征继续增大感受野 up-conv：expands the spatial support of the lower resolution feature maps last layer：run [1*1*1conv with 2 channel+softmax] to obtain the voxelwise probabilistic segmentations of the foreground and background dice coefficient： [0,1] which we aim to maximise，assume $p_i$、$g_i$ belong to two binary volumes D = \frac{2\sum_i^N p_i g_i}{\sum_i^N p_i^2 + \sum_i^N g_i^2} train： input fix size 128 × 128 × 64 voxels and a spatial resolution of 1 × 1 × 1.5 millimeters each mini-batch contains 2 volumes online augmentation： randomly deformation vary the intensity distribution：随机选取样本的灰度分布作为当前训练样本的灰度分布 used a momentum of 0.99 and a initial learning rate of 0.0001 which decreases by one order of magnitude every 25K iterations metrics： Dice coefficient Hausdorff distance of the predicted delineation to the ground truth annotation the score obtained on the challenge dice loss &amp; focal loss CE &amp; BCE CE：categorical_crossentropy，针对所有类别计算，类别间互斥 CE(x) = -\sum_{i=1}^{n\_class}y_i log f_i(x) $x$是输入样本，$y_i$是第$i$个类别对应的真实标签，$f_i(x)$是对应的模型输出值。 对分类问题，$y_i$是one-hot，$f_i(x)$是个一维向量。最终得到一个数值。 BCE：binary_crossentropy，针对每个类别计算 BCE(x)_i = - [y_i log f_i(x) + (1-y_i)log(1-f_i(x))] $i$是类别编号，最终得到一个维度为$n_class$的向量。 再求类均值得到一个数值作为单个样本的loss。 BCE(x) = \frac{\sum_{i=1}^{n\_class}BCE_i(x)}{n\_class} batch loss：对batch中所有样本的loss求均值。 从公式上看，CE的输出通常是经过了softmax，softmax的某一个输出增大，必然导致其它类别的输出减小，因此在计算loss的时候关注正确类别的预测值是否被拉高即可。使用BCE的场景通常是使用sigmoid，类别间不会互相压制，因此既要考虑所属类别的预测概率够高，也要考虑不所属类别的预测概率足够低（这一项在softmax中被实现了故CE不需要这一项）。 场景： 二分类：只有一个输出节点，$f(x) \in (0,1)$，应该使用sigmoid+BCE作为最后的输出层配置。 单标签多分类：应该使用softmax+CE的方案，BCE也同样适用。 多标签多分类：multi-label每个标签的输出是相互独立的，因此常用配置是sigmoid+BCE。 对分割场景来说，输出的每一个channel对应一个类别的预测map，可以看成是多个channel间的单标签多分类（softmax+CE），也可以看成是每个独立通道类别map的二分类（sigmoid+BCE）。unet论文用了weighted的softmax+CE。vnet论文用了dice_loss。 re-weighting(WCE) 基于CE&amp;BCE，给了样本不同的权重。 unet论文中提到了基于pixel frequency为不同的类别创建了weight map。 一种实现：基于每个类别的weight map，在实现CE的时候改成加权平均即可。 另一种实现：基于每个样本的weight map，作为网络的附加输入，在实现CE的时候乘在loss map上。 focal loss 提出是在目标检测领域，用于解决正负样本比例严重失调的问题。 也是一种加权，但是相比较于re-weighting，困难样本的权重由网络自行推断出，通过添加$(\alpha)$和$(-)^\lambda$这一加权项： focal\_loss(x)_i = -[\alpha y_i (1-p_i)^\lambda log (p_i)+(1-\alpha)(1-y_i)p_i^\lambda log(1-p_i)] 对于类别间不均衡的情况（通常负样本远远多于正样本），$(\alpha)$项用于平衡正负样本权重。 对于类内困难样本的挖掘，$(-)^\lambda$项用于调整简单样本和困难样本的权重，预测概率更接近真实label的样本（简单样本）的权重会衰减更快，预测概率比较不准确的样本（苦难样本）的权重则更高些。 由于分割网络的输出的单通道／多通道的图片，直接使用focal loss会导致loss值很大。 ​ 1. 通常与其他loss加权组合使用 ​ 2. sum可以改成mean ​ 3.不建议在训练初期就加入，可在训练后期用于优化模型 ​ 4. 公式中含log计算，可能导致nan，要对log中的元素clip 12345678910111213def focal_loss(y_true, y_pred): gamma = 2. alpha = 0.25 # score = alpha * y_true * K.pow(1 - y_pred, gamma) * K.log(y_pred) + # this works when y_true==1 # (1 - alpha) * (1 - y_true) * K.pow(y_pred, gamma) * K.log(1 - y_pred) # this works when y_true==0 pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred)) pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred)) # avoid nan pt_1 = K.clip(pt_1, 1e-3, .999) pt_0 = K.clip(pt_0, 1e-3, .999) score = -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - \ K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0)) return score dice loss dice定义两个mask的相似程度： dice = \frac{2 * A \bigcap B}{|A|+|B|} = \frac{2 * TP}{2*TP + FN + FP} 分子是TP——只关注前景 分母可以是$|A|$（逐个元素相加），也可以是平方形式$|A|^2$ 梯度：“使用dice loss有时会不可信，原因是对于softmax或log loss其梯度简言之是p-t ，t为目标值，p为预测值。而dice loss 为 2t2 / (p+t)2 如果p，t过小会导致梯度变化剧烈，导致训练困难。” 【详细解释下】交叉熵loss：$L=-(1-|t-p|)log(1-|t-p|)$，求导得到$\frac{\partial L}{\partial p}=-log(1-|t-p|)$，其实就可以简化看作$t-p$，很显然这个梯度是有界的，因此使用交叉熵loss的优化过程比较稳定。而dice loss的两种形式（不平方&amp;平方）：$L=\frac{2pt}{p+t}\ or\ L=\frac{2pt}{p^2+t^2}$，求导以后分别是$\frac{\partial L}{\partial p} = \frac{t^2+2pt}{(p+t)^2} \ or\ \frac{3tp^2+t^3}{(p^2+t^2)^2}$计算结果比较复杂，pt都很小的情况下，梯度值可能很大，可能导致训练不稳定，loss曲线混乱。 vnet论文中的定义在分母上稍有不同（see below）。smoothing的好处： 避免分子除0 减少过拟合 12345678def dice_coef(y_true, y_pred): smooth = 1. intersection = K.sum(y_true * y_pred, axis=[1,2,3]) union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3]) return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0) def dice_coef_loss(y_true, y_pred): 1 - dice_coef(y_true, y_pred, smooth=1) iou loss dice loss衍生，intersection over union： iou = \frac{A \bigcap B}{A \bigcup B} 分母上比dice少了一个intersection。 “IOU loss的缺点同DICE loss，训练曲线可能并不可信，训练的过程也可能并不稳定，有时不如使用softmax loss等的曲线有直观性，通常而言softmax loss得到的loss下降曲线较为平滑。” boundary loss dice loss和iou loss是基于区域面积匹配度去学习，我们也可以使用边界匹配度去监督网络的学习。 只对边界上的像素进行评估，和GT的边界吻合则为0，不吻合的点，根据其距离边界的距离评估它的Loss。 Hausdorff distance 用于度量两个点集之间的相似程度，denote 点集$A\{a_1, a_2, …, a_p\}$，点集$B\{b_1, b_2, …, b_p\}$： HD(A, B) = max\{hd(A,B), hd(B,A)\}\\ hd(A,B) = max_{a \in A} min_{b in B} ||a-b||\\ hd(B,A) = max_{b \in B} min_{a in A} ||b-a|| 其中HD(A,B)是Hausdorff distance的基本形式，称为双向距离 hd(A,B)描述的是单向距离，首先找到点集A中每个点在点集B中距离最近的点作为匹配点，然后计算这些a-b-pair的距离的最大值。 HD(A,B)取单向距离中的最大值，描述了两个点集合的最大不匹配程度。 mix loss BCE + dice loss：在数据较为平衡的情况下有改善作用，但是在数据极度不均衡的情况下，交叉熵损失会在几个训练之后远小于Dice 损失，效果会损失。 focal loss + dice loss：数量级问题 MSE 关键点检测有时候也会采用分割框架，这时候ground truth是高斯map，dice是针对二值化mask的，这时候还可以用MSE。 ohnm online hard negative mining 困难样本挖掘 Tversky loss 一种加权的dice loss，dice loss会平等的权衡FP（精度，假阳）和FN（召回，假阴），但是医学图像中病灶数目远少于背景数量，很可能导致训练结果偏向高精度但是低召回率，Tversky loss控制loss更偏向FN： loss = 1-\frac{|PG|}{|PG|+\alpha|P\backslash G|+\beta|G\backslash P|}1234567891011def tversky_loss(y_true, y_pred): y_true_pos = K.flatten(y_true) y_pred_pos = K.flatten(y_pred) # TP true_pos = K.sum(y_true_pos * y_pred_pos) # FN false_neg = K.sum(y_true_pos * (1-y_pred_pos)) # FP false_pos = K.sum((1-y_true_pos) * y_pred_pos) alpha = 0.7 return 1 - (true_pos + K.epsilon())/(true_pos + alpha * false_neg + (1-alpha) * false_pos + K.epsilon()) Lovasz hinge &amp; Lovasz-Softmax loss IOU loss衍生，jaccard loss只适用于离散情况，而网络预测是连续值，如果不使用某个超参将神经元输出二值化，就不可导。blabla 不是很懂直接用吧：https://github.com/bermanmaxim/LovaszSoftmax 一些补充 改进： dropout、batch normalization：从论文上看，unet只在最深层卷积层后面添加了dropout layer，BN未表，而common sense用每一个conv层后面接BN层能够替换掉dropout并能获得性能提升的。 UpSampling2D、Conv2DTranspose：unet使用了上采样，vnet使用了deconv，但是“DeConv will produce image with checkerboard effect, which can be revised by upsample and conv”(Reference)。 valid padding、same padding：unet论文使用图像patch作为输入，特征提取时使用valid padding，损失边缘信息。 network blocks：unet用的conv block是两个一组的3*3conv，vnet稍微不同一点，可以尝试的block有ResNet／ResNext、DenseNet、DeepLab等。 pretrained encoder：feature extraction path使用一些现有的backbone，可以加载预训练权重(Reference)，加速训练，防止过拟合。 加入SE模块(Reference)：对每个通道的特征加权 attention mechanisms： 引用nn-Unet主要结构改进合集：“Just to provide some prominent examples: variations of encoder-decoder style architectures with skip connections, first introduced by the U-Net [12], include the introduction of residual connections [9], dense connections [6], at- tention mechanisms [10], additional loss layers [5], feature recalibration [13], and others [11]. 衍生： TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation nnU-Net: Breaking the Spell on Successful Medical Image Segmentation TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation 动机： neural network initialized with pre-trained weights usually shows better performance than those trained from scratch on a small dataset. 保留encoder-decoder的结构，同时充分利用迁移学习的优势 论点： load pretrained weights 用huge dataset做预训练 方法： 用vgg11替换原始的encoder，并load pre-trained weights on ImageNet： 最深层输入(maxpooling5)：use a single conv of 512 channels that serves as a bottleneck central part of the network upsampling换成了convTranspose loss function：IOU + BCE： L = BCE - log(IOU) inference：choose a threshold 0.3, all pixel values below which are set to be zero 结论： converge faster better IOU nnU-Net: Breaking the Spell on Successful Medical Image Segmentation 动机 many proposed methods fail to generalize: 对于分割任务，从unet出来之后的几年里，在网络结构上已经没有多少的突破了，结构修改越多，反而越容易过拟合 relies on just a simple U-Net architecture embedded in a robust training scheme automate necessary adaptations such as preprocessing, the exact patch size, batch size, and inference settings based on the properties of a given dataset: 更多的提升其实在于理解数据，针对数据采用适当的预处理和训练方法和技巧 论点 the diversity and individual peculiarities of imaging datasets make it difficult to generalize prominent modifications focus on architectural modifications, merely brushing over all the other hyperparameters we propose: 使用基础版unet：nnUNet（no-new-Net） a formalism for automatic adaptation to new datasets automatically designs and executes a network training pipeline without any manual fine-tuning 要素 a segmentation task: $f_{\theta}(X) = \hat Y$, in this paper we seek for a $g(X,Y)=\theta$. First we distinguish two type of hyperparameters: static params：in this case the network architecture and a robust training scheme dynamic params：those that need to be changed in dependence of $X$ and $Y$ Second we define g——a set of heuristics rules covering the entire process of the task: 预处理：resampling和normalization 训练：loss，optimizer设置、数据增广 推理：patch-based策略、test-time-augmentations集成和模型集成等 后处理：增强单连通域等 方法 Preprocessing Image Normalization： CT：$normed_intensity = (intensity - fg_mean) / fg_standard_deviation$, $fg$ for $[0.05,0.95]$ foreground intensity not CT：$normed_intensity = (intensity - mean) / standard_deviation $ Voxel Spacing： for each axis chooses the median as the target spacing image resampled with third order spline interpolation z-axis using nearest neighbor interpolation if ‘anisotropic spacing’ occurs mask resampled with third order spline interpolation Training Procedure Network Architecture： 3 independent model：a 2D U-Net, a 3D U-Net and a cascade of two 3D U-Net padded convolutions：to achieve identical output and input shapes instance normalization：“BN适用于判别模型，比如图片分类模型。因为BN注重对每个batch进行归一化，从而保证数据分布的一致性，而判别模型的结果正是取决于数据整体分布。但是BN对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布；IN适用于生成模型，比如图片风格迁移。因为图片生成的结果主要依赖于某个图像实例，所以对整个batch归一化不适合图像风格化，在风格迁移中使用Instance Normalization不仅可以加速模型收敛，并且可以保持每个图像实例之间的独立。” Leaky ReLUs Network Hyperparameters： sets the batch size, patch size and number of pooling operations for each axis based on the memory consumption large patch sizes are favored over large batch sizes pooling along each axis is done until the voxel size=4 start num of filters=30, double after each pooling If the selected patch size covers less than 25% of the voxels, train the 3D U-Net cascade on a downsampled version of the training data to keep sufficient context Network Training: five-fold cross-validation One epoch is defined as processing 250 batches loss = dice loss + cross-entropy loss Adam(lr=3e-4, decay=3e-5) lrReduce: EMA(train_loss), 30 epoch, factor=0.2 earlyStop: earning rate drops below 10 6 or 1000 epochs are exceeded data augmentation: elastic deformations, random scaling and random rotations as well as gamma augmentation($g(x,y)=f(x,y)^{gamma}$) keep transformations in 2D-plane if ‘anisotropic spacing’ occurs Inference sliding window with half the patch size: this increases the weight of the predictions close to the center relative to the borders ensemble: U-Net configurations (2D, 3D and cascade) furthermore uses the five models (five-fold cross-validation) Ablation studies 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation 动机 learns from sparsely/full annotated volumetric images (user annotates some slices) provides a dense 3D segmentation 要素 3D operations avoid bottlenecks and use batch normalization for faster convergence on-the-fly elastic deformation train from scratch 论点 neighboring slices show almost the same information many biomedical applications generalizes reasonably well because medical images comprises repetitive structures thus we suggest dense-volume-segmentation-network that only requires some annotated 2D slices for training scenarios manual annotated 一部分slice，然后训练网络实现dense seg 用一部分 sparsely annotated的dataset作为training set，然后训练的网络实现在新的数据集上dense seg 方法 Network Architecture compression：2*3x3x3 convs(+BN)+relu+2x2x2 maxpooling decompression：2x2x2 upconv+2*3x3x3 convs+relu head：1x1x1 conv concat shortcut connections 【QUESTION】avoid bottlenecks by doubling the number of channels already before max pooling 个人理解这个double channel是在跟原始的unet结构对比，原始unet每个stage的两个conv的filter num是一样的，然后进行max pooling会损失部分信息，但是分割任务本身是个dense prediction，所以增大channel来减少信息损失 但是不理解什么叫“avoid bottlenecks” 原文说是参考了《Rethinking the inception architecture for computer vision》大名鼎鼎的inception V3 可能对应的是“1. Avoid representational bottlenecks, especially early in the network.”，从输入到输出，要逐渐减少feature map的尺寸，同时要逐渐增加feature map的数量。 input：132x132x116 voxel tile output：44x44x28 BN：before each ReLU weighted softmax loss function：setting the weights of unlabeled pixels to zero makes it possible to learn from only the labelled ones and, hence, to generalize to the whole volume（是不是random set the loss zeros of some samples总能让网络更好的generalize？） Data manually annotated some orthogonal xy, xz, and yz slices annotation slices were sampled uniformly ran on down-sampled versions of the original resolution by factor of two labels：0: “inside the tubule”; 1: “tubule”; 2: “background”, and 3: “unlabeled”. Training rotation, scaling and gray value augmentation a smooth dense deformation：random vector, normal distribution, B-spline interpolation weighted cross-entropy loss：increase weights “inside the tubule”, reduce weights “background”, set zero “unlabeled” 2.5D-UNet: Automatic Segmentation of Vestibular Schwannoma from T2-Weighted MRI by Deep Spatial Attention with Hardness-Weighted Loss 专业术语 Vestibular Schwannoma(VS) tumors：前庭神经鞘瘤 through-plane resolution：层厚 isotropic resolution：各向同性 anisotropic resolutions：各向异性 动机 tumor的精确自动分割 challenge low contrast：hardness-weighted Dice loss functio small target region：attention module low through-plane resolution：2.5D 论点 segment small structures from large image contexts coarse-to-fine attention map Dice loss our method end-to-end supervision on the learning of attention map voxel-level hardness- weighted Dice loss function CNN 2D CNNs ignore inter-slice correlation 3D CNNs most applied to images with isotropic resolution requiring upsampling to balance the physical receptive field (in terms of mm rather than voxels)：memory rise our method high in-plane resolution &amp; low through-plane resolution 2.5D CNN combining 2D and 3D convolutions use inter-slice features more efficient than 3D CNNs 数据 T2-weighted MR images of 245 patients with VS tumor high in-plane resolution around 0.4 mm×0.4 mm，512x512 slice thickness and inter-slice spacing 1.5 mm，slice number 19 to 118 cropped cube size：100 mm×50 mm×50 mm 方法 architecture five levels：L1、L2 use 2D，L3、L4、L5 use 3D After the first two max-pooling layers that downsample the feature maps only in 2D, the feature maps in L3 and the followings have a near- isotropic 3D resolution. start channels：16 conv block：conv-BN-pReLU add a spatial attention module to each level of the decoder spatial attention module A spatial attention map can be seen as a single-channel image of attention coefficient input：feature map with channel $N_l$ conv1+ReLU： channel $N_l/2$ conv2+Sigmoid：channel 1，outputs the attention map multiplied the feature map with the attention map a residual connection explicit supervision multi-scale attention loss $L_{attention} = \frac{1}{L} \sum_{L} l(A_l, G_l^f)$ $A_l$是每一层的attention map，$G_l^f$是每一层是前景ground truth average-pool到当前resolution的mask Voxel-Level Hardness-Weighted Dice Loss automatic hard voxel weighting：$w_i = \lambda * abs(p_i - g_i) + (1-\lambda)$ $\lambda \in [0,1]$，controls the degree of hard voxel weighting hardness-weighted Dice loss (HDL) ： l(P,G) = 1.0 - \frac{1}{C}\sum_{C} \frac{2\sum_i w_i p_i g_i + \epsilon}{\sum_i w_i (p_i + g_i) + \epsilon} total loss： L = \frac{1}{L} \sum_{L} l(A_l, G_l^f) + l(P,G) Combining analysis of multi-parametric MR images into a convolutional neural network: Precise target delineation for vestibular schwannoma treatment planning只有摘要和一幅图 multi-parametric MR images：T1W、T2W、T1C two-pathway U-Net model kernel 3 × 3 × 1 and 1 × 1 × 3 respectively to extract the in-plane and through-plane features of the anisotropic MR images 结论 The proposed two-pathway U-Net model outperformed the single-pathway U-Net model when segmenting VS using anisotropic MR images. multi-inputs（T1、T2）outperforms single-inputs]]></content>
      <tags>
        <tag>paper, 语义分割</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yolo系列]]></title>
    <url>%2F2019%2F11%2F28%2Fyolo%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[综述 [yolov1] Yolov1: You Only Look Once: Unified, Real-Time Object Detection [yolov2] Yolov2: YOLO9000: Better, Faster, Stronger [yolov3] Yolov3: An Incremental Improvement [yolov4] YOLOv4: Optimal Speed and Accuracy of Object Detection [poly-yolo] POLY-YOLO: HIGHER SPEED, MORE PRECISE DETECTION AND INSTANCE SEGMENTATION FOR YOLOV3 [scaled-yolov4] Scaled-YOLOv4: Scaling Cross Stage Partial Network [yolov7] YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors 0. review review0121：关于yolo loss 之前看keras版的yolo loss，包含分类的bce，回归的l2/mse，以及confidence的回归loss，其中conf loss被建模成单纯的0-1分类问题，用bce来实现。 事实上原版的yolo loss中，objectness是iou（pred和gt的iou），从意义上，不仅指示当前格子有无目标，还对当前的box prediction做了评估 回传梯度 不回传梯度 iou是通过xywh计算的，也就是说基于iou的loss做梯度回传的时候要流经box head，scaled_yolov4中把这个梯度截断，只作为一个值，对confidence进行梯度回传， 梯度不截断也没有问题，相当于对xywh再回传一个iou的loss review1215：main features梳理 yolov1 yolov2 yolov3 giou anchor机制 匹配机制：用max_iou做1v1匹配，1 in 9 (all-level)， 回归机制：用sigmoid和exp包裹logits，作为xy和wh的偏移量 yolov4：bag-of-tricks，CSPDarknet53+PAN-SPP+yolohead，mosaic，ciou，syncBN，diou_nms anchor机制 匹配机制：不再是唯一匹配，可以跨level，只要满足iou thresh就作为positive anchor 回归机制 scaled-yolov4：fully CSP-ized，双向FPN，scaling up出一系列模型 CSP-block是为了scaling-up服务的，yolov3的结构一张卡batch size只能开到8，在OOM边缘试探，这篇文章将整个结构都CSP化 面向GPU的YOLOv4-large包含YOLOv4-P5, YOLOv4-P6, and YOLOv4-P7 anchor机制 匹配机制：相比较于yolov4进一步拓展，跨level且跨网格，只要长宽比满足阈值，都作为positive anchor，同时找到gt center最近的两个邻域网格，都作为正样本 回归机制：重新定义了回归机制，因为跨网格，偏移量的激活区间变成[-0.5,1.5] loss loss_box：giou loss_obj：bce loss_cls：bce 1. Yolov1: You Only Look Once: Unified, Real-Time Object Detection 动机: end-to-end: 2 stages —-&gt; 1 stage real-time 论点： past methods: complex pipelines, hard to optimize(trained separately) DPM use a sliding window and a classifier to evaluate an object at various locations R-CNN use region proposal and run classifier on the proposed boxes, then post-processing in this paper: you only look once at an image rebuild the framework as a single regression problem: single stands for you don’t have to run classifiers on each patch straight from image pixels to bounding box coordinates and class probabilities: straight stands for you obtain the bounding box and the classification results side by side, comparing to the previous serial pipeline advantages： fast &amp; twice the mean average precision of other real-time systems CNN sees the entire image thus encodes contextual information generalize better disadvantage: accuracy: “ it struggles to precisely localize some objects, especially small ones” 细节： grid： Our system divides the input image into an S × S grid. If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object. prediction： Each grid cell predicts B bounding boxes, confidence scores for these boxes , and C conditional class probabilities for each grid that is an S*S*(B*5+C) tensor We normalize the bounding box width and height by the image width and height so that they fall between 0 and 1. We parametrize the bounding box x and y coordinates to be offsets of a particular grid cell so they are also bounded between 0 and 1. at test time： We obtain the class-specific confidence for individual box by multiply the class probability and box confidence： Pr(Class_i | Object) * Pr(Object)* IOU^{truth}_{pred} = Pr(Class_i)* IOU^{truth}_{pred} network： the convolutional layers extract features from the image while the fully connected layers predict the probabilities and coordinates training： activation：use a linear activation function for the final layer and leaky rectified linear activation all the other layers optimization：use sum-squared error, however it does not perfectly align with the goal of maximizing average precision ​ * weights equally the localization error and classification error：$\lambda_{coord}$ ​ * weights equally the grid cells containing and not-containing objects：$\lambda_{noobj}$ ​ * weights equally the large boxes and small boxes：square roots the h&amp;w insteand of the straight h&amp;w loss：pick the box predictor has the highest current IOU with the ground truth per grid cell avoid overfitting：dropout &amp; data augmentation ​ * use dropout after the first connected layer, ​ * introduce random scaling and translations of up to 20% of the original image size for data augmentation ​ * randomly adjust the exposure and saturation of the image by up to a factor of 1.5 in the HSV color space for data augmentation inference： multiple detections：some objects locates near the border of multiple cells and can be well localized by multiple cells. Non-maximal suppression is proved critical, adding 2- 3% in mAP. Limitations： strong spatial constraints：decided by the settings of bounding boxes softmax classification：can only have one class for each grid “This spatial constraint lim- its the number of nearby objects that our model can pre- dict. Our model struggles with small objects that appear in groups, such as flocks of birds. “ “ It struggles to generalize to objects in new or unusual aspect ratios or configurations. “ coarse bounding box prediction：the architecture has multiple downsampling layers the loss function treats errors the same in small bounding boxes versus large bounding boxes： The same error has much greater effect on a small box’s IOU than a big box. “Our main source of error is incorrect localizations. “ Comparison： mAP among real-time detectors and Less Than Real-Time detectors：less mAP than fast-rcnn but much faster error analysis between yolo and fast-rcnn：greater localization error and less background false-positive combination analysis：[fast-rcnn+yolo] defeats [fast-rcnn+fast-rcnn] since YOLO makes different kinds of mistakes with fast-rcnn generalizability：RCNN degrades more because the Selective Search is tuned for natural images, change of dataset makes the proposals get worse. YOLO degrades less because it models the size and shape of objects, change of dataset varies less at object level but more at pixel level. 2. Yolov2: YOLO9000: Better, Faster, Stronger 动机： run at varying sizes：offering an easy tradeoff between speed and accuracy recognize a wide variety of objects ：jointly train on object detection and classification, so that the model can predict objects that aren’t labelled in detection data better performance but still fast 论点： Current object detection datasets are limited compared to classification datasets leverage the classification data to expand the scope of current detection system joint training algorithm making the object detectors working on both detection and classification data Better performance often hinges on larger networks or ensembling multiple models. However we want a more accurate detector that is still fast YOLOv1’s shortcomings more localization errors low recall 要素： better faster backbone stronger uses labeled detection images to learn to precisely localize objects uses classification images to increase its vocabulary and robustness 方法： better： batch normalization：convergence &amp; regularization add batch normalization on all of the convolutional layers remove dropout from the model high resolution classifier：pretrain a hi-res classifier first fine tune the classification network at the full 448 × 448 resolution for 10 epochs on ImageNet then fine tune the resulting network on detection convolutional with anchor boxes： YOLOv1通过网络最后的全连接层，直接预测每个grid上bounding box的坐标 而RPN基于先验框，使用最后一层卷积层，在特征图的各位置预测bounding box的offset和confidence “Predicting offsets instead of coordinates simplifies the problem and makes it easier for the network to learn” YOLOv2去掉了全连接层，也使用anchor box来回归bounding box eliminate one pooling layer to make the network output have higher resolution shrink the network input to 416416 to obtain an odd number so that there is a *single center cell in the feature map predict class and objectness for every anchor box(offset prediction) instead of nothing(direct location&amp;scale prediction) dimension clustering： what we want are priors that lead to good IOU scores, thus comes the distance metric： d(box, centroid) = 1 - IOU(box, centroid) direct location prediction： YOLOv1 encounter model instability issue for predicting the (x, y) locations for the box RPN also takes a long time to stabilize by predicting a (tx, ty) and obtain the (x, y) center coordinates indirectly because this formulation is unconstrained so any anchor box can end up at any point in the image： x = t_x * w_a - x_a\\ y = t_y * h_a - y_a 学习RPN：回归一个相对量，比盲猜回归一个绝对location（YOLOv1）更好学习 学习YOLOv1：基于cell的预测，将bounding box限定在有限区域，不是全图飞（RPN） YOLOv2对每个cell，基于5个prior anchor size，预测5个bounding box，每个bounding box具有5维： b_x = \sigma(t_x) + c_x\\ b_y = \sigma(t_y) + c_y\\ b_w = p_w e^{t_w}\\ b_h = p_h e^{t_h}\\ Pr(object)*IOU(b,object) = \sigma(t_o) $t_x\ \&amp;\ t_y$用于回归bounding box的位置，通过sigmoid激活函数被限定在0-1，通过上式能够间接得到bounding box的归一化位置（相对原图） $t_w\ \&amp;\ t_h$用于回归bounding box的尺度，输出应该不是0-1限定，$p_w\ \&amp;\ p_h$是先验框的归一化尺度，通过上式能够间接得到bounding box的归一化尺度（相对原图） $t_o$用于回归objectness，通过sigmoid限定在0-1之间，因为$Pr(object)\ \&amp;\ IOU(b,object)$都是0-1之间的值，IOU通过前面四个值能够求解，进而可以解耦objectness fine-grained features： motive：小物体的检测依赖更加细粒度的特征 cascade：Faster R-CNN and SSD both run their proposal networks at various feature maps in the network to get a range of resolutions 【QUESTION】YOLOv2 simply adds a passthrough layer from an earlier layer at 26 × 26 resolution： latter featuremap —-&gt; upsampling concatenate with early featuremap the detector runs on top of this expanded feature map predicts a $NN(3*(4+1+80))$ tensor for each scale multi-scale training： 模型本身不限定输入尺寸：model only uses convolutional and pooling layers thus it can be resized on the fly forces the network to learn to predict well across a variety of input dimensions the same network can predict detections at different resolutions loss：cited from the latter yolov3 paper use sum of squared error loss for box coordinate(x,y,w,h)：then the gradient is $y_{true} - y_{pred}$ use logistic regression for objectness score：which should be 1 if the bounding box prior overlaps a ground truth object by more than any other bounding box prior if a bounding box prior is not assigned to a ground truth object it incurs no loss(coordinate&amp;objectness) use binary cross-entropy loss for multilabel classification faster： darknet-19： YOLOv1中讨论过换VGG-16和YOLOv1使用的backbone对比，前者有map提升，但是耗时。 YOLOv2的新backbone，参数更少，而且相对于VGG16在ImageNet上精度更高。 training for classification： first train on ImageNet using 224*224 then fine-tuning on 448*448 training for detection： remove the last convolutional layer add on three 3 × 3 convolutional layers with 1024 filters each followed by a final 1×1 convolutional layer with the number of outputs we need for detection add a passthrough from the final 3 × 3 × 512 layer to the second to last convolutional layer so that our model can use fine grain features. stronger： jointly training：以后再填坑 构造标签树 classification sample用cls loss，detection sample用detect loss 预测正确的classification sample给一个.3 IOU的假设值用于计算objectness loss 3. Yolov3: An Incremental Improvement 动机： nothing like super interesting, just a bunch of small changes that make it better 方法： bounding box prediction： use anchor boxes and predicts offsets for each bounding box use sum of squared error loss for training predicts the objectness score for each bounding box using logistic regression one ground truth coresponds to one best box and one loss class prediction： use binary cross-entropy loss for multilabel classification 【NEW】prediction across scales： the detector：a few more convolutional layers following the feature map, the last of which predicts a 3-d(for 3 priors) tensor encoding bounding box, objectness, and class predictions expanded feature map：upsampling the deeper feature map by 2X and concatenating with the former features “With the new multi-scale predictions, YOLOv3 has better perfomance on small objects and comparatively worse performance on medium and larger size objects “ 【NEW】feature extractor： darknet-53 ! training：common skills 4. 一些补充 metrics：mAP 最早由PASCAL VOC提出，输出结果是一个ranked list，每一项包含框、confidence、class， yolov3提到了一个“COCOs weird average mean AP metric ” IoU：预测框与ground truth的交并比，也被称为Jaccard指数，我们通常用其来判断每个检测的正确性。PASCAL VOC数据集用0.5为阈值来判定预测框是True Positive还是False Positive，COCO数据集则建议对不同的IoU阈值进行计算。 置信度：通过改变置信度阈值，我们可以改变一个预测框是Positive还是 Negative。 precision &amp; recall：precision = TP ／(TP + FP)，recall = TP／(TP + FN)。图片中我们没有预测到的每个部分都是Negative，因此计算True Negatives比较难办。但是我们只需要计算False Negatives，即我们模型所漏检的物体。 AP：不同的置信度下会得到不同的precision-recall。为了得到precision-recall曲线，首先对模型预测结果进行排序，按照各个预测值置信度降序排列。给定不同的置信度阈值，就有不同的ranked output，Recall和Precision仅在高于该rank值的预测结果中计算。这里共选择11个不同的recall（[0, 0.1, …, 0.9, 1.0]），那么AP就定义为在这11个recall下precision的平均值，其可以表征整个precision-recall曲线（曲线下面积）。给定recall下的precision计算，是通过一种插值的方式： AP = \frac{1}{11}\sum_{r\in\{0,0.1,...,1.0\}}p_{interp}(r) \\ p_{interp}(r) = max_{\tilde r: \tilde r > r} p(\tilde r) mAP：此度量指标在信息检索和目标检测领域有不同的计算方式。对于目标检测，对于各个类别，分别按照上述方式计算AP，取所有类别的AP平均值就是mAP。 eval： yolo_head输出：box_xy是box的中心坐标，(0~1)相对值；box_wh是box的宽高，(0~1)相对值；box_confidence是框中物体置信度；box_class_probs是类别置信度； yolo_correct_boxes函数：能够将box中心的相对信息转换成[y_min,x_min,y_max,x_max]的绝对值 yolo_boxes_and_scores函数：输出网络预测的所有box yolo_eval函数：基于score_threshold、max_boxes两项过滤，类内NMS，得到最终输出 4. YOLOv4: Optimal Speed and Accuracy of Object Detection 动机 Practical testing the tricks of improving CNN some features work for certain problems/dataset exclusively applicable to the majority of models, tasks, and datasets only increase the training cost [bag-of-freebies] only increase the inference cost by a small amount but can significantly improve the accuracy [bag-of-specials] Optimal Speed and Accuracy 论点 head： predict classes and bounding boxes one-stage head YOLO, SSD, RetinaNet anchor-free：CenterNet, CornerNet, FCOS two-stage head R-CNN series anchor-free：RepPoints neck： collect feature maps from different stages FPN, PAN, BiFPN, NAS-FPN backbone： pre-trained on ImageNet VGG, ResNet, ResNeXt, DenseNet Bag of freebies data augmentation pixel-wise adjustments photometric distortions：brightness, contrast, hue, saturation, and noise geometric distortions：random scaling, cropping, flipping, and rotating object-wise cut： to image：CutOut to featuremaps：DropOut, DropConnect, DropBlock add：MixUp, CutMix, GAN data imbalance for classification two-stage：hard example mining one-stage：focal loss, soft label bounding box regression MSE-regression：treat [x,y,w,h] as independent variables IoU loss：consider the integrity &amp; scale invariant Bag of specials enlarging receptive field：improved SPP, ASPP, RFB introducing attention mechanism channel-wise attention：SE, increase the inference time by about 10% point-wise attention：Spatial Attention Module (SAM), does not affect the speed of inference strengthening feature integration channel-wise level：SFAM point-wise level：ASFF scale-wise level：BiFPN activation function：A good activation function can make the gradient more efficiently propagated post-processing：各种NMS 方法 choose a backbone —- CSPDarknet53 Higher input network size (resolution) – for detecting multiple small-sized objects More conv layers – for a higher receptive field to cover the increased size of input network More parameters – for greater capacity of a model to detect multiple objects of different sizes in a single image add the SPP block over the CSPDarknet53 significantly increases the receptive field separates out the most significant context features causes almost no re- duction of the network operation speed use PANet as the method of parameter aggregation Modified PAN replace shortcut connection of PAN to concatenation use YOLOv3 (anchor based) head encoding/decoding method变了！！！ 代码和issue里面有说明：https://github.com/WongKinYiu/ScaledYOLOv4/issues/90#，但是论文里没显式的说明 given pred offsets $t_x,t_y,t_w,t_h$ xy_offsets：for each grid centers b_x = sigmoid(t_x)*2 - 0.5 \\ b_y = sigmoid(t_y)*2 - 0.5 wh_ratios：for each grid anchors b_w = (sigmoid(t_w)*2)^2 - 0.5 \\ b_h = (sigmoid(t_h)*2)^2 - 0.5 Mosaic data augmentation mixes 4 training images allows detection of objects outside their normal context reduces the need for a large mini-batch size Self-Adversarial Training (SAT) data augmentation 1st stage alters images 2nd stage train on the modified images CmBN：a CBN modified version modified SAM：from spatial-wise attention to point- wise attention 这里的SAM for 《An Empirical Study of Spatial Attention Mechanisms in Deep Networks》 ，空间注意力机制 还有一篇SAM是《Sharpness-Aware Minimization for Efficiently Improving Generalization》，google的锐度感知最小化，用来提升模型泛化性能，注意区分 实验 Influence of different features on Classifier training Bluring和Swish没有提升 Influence of different features on Detector training IoU threshold, CmBN, Cosine annealing sheduler, CIOU有提升 POLY-YOLO: HIGHER SPEED, MORE PRECISE DETECTION AND INSTANCE SEGMENTATION FOR YOLOV3 动机 yoloV3’s weakness rewritten labels inefficient distribution of anchors light backbone： stairstep upsampling single scale output to extend instance segmentation detect size-independent polygons defined on a polar grid real-time processing 论点 yolov3 real-time low precision cmp with RetinaNet, EfficientDet low precision of the detection of big boxes rewriting of labels by each-other due to the coarse resolution this paper solution： 解决yolo精度问题：propose a brand-new feature decoder with a single ouput tensor that goes to head with higher resolution 多尺度特征融合：utilize stairstep upscaling 实例分割：bounding polygon within a poly grid instance segmentation two-stage：mask-rcnn one-stage： top-down：segmenting this object within a bounding box bottom-up：start with clustering pixels direct methods：既不需要bounding box也不需要clustered pixels，PolarMask cmp with PolarMask size-independent：尺度，大小目标都能检测 dynamic number of vertices：多边形定点可变 yolov3 issues rewriting of labels： 两个目标如果落在同一个格子里，在一个尺度上ground truth label只会保留一个box 对越小的特征图，grid越大，这个问题越严重 imbalanced distribution of anchors across output scales anchor如果选的不合理，会导致特征图尺度和anchor尺度不匹配 most of the boxes will be captured by the middle output layer and the two other layers will be underused 如上面车的case，大多数车的框很小，聚类出的给level0和level1的anchor shape还是很小，但是level0是稀疏grid 一方面，grid shape和anchor shape不匹配 一方面，label rewriten问题会升级 反过来，如果dense grid上预测大目标，会受到感受野的制约 一种解决方案是基于感受野首先对gt box分成三组，然后分别聚类，然后9选1 yolov3原文：YOLOv3 has relatively high $AP_{small}$ performance. However, it has comparatively worse performance on medium and larger size objects. More investigation is needed to get to the bottom of this. 小目标performance更好，大目标worse，主要是就是因为coarse grid上存在label rewriten问题，存在部分gt box被抑制掉了。 方法 architecture single output higher resolution：stride4 handle all the anchors at once cross-scale fusion hypercolumn technique：add operation stairstep interpolation：x2 x2 … SE-blocks reduced the number of convolutional filters to 75% in the feature extraction phase bounding polygons extend the box tuple：$b_i=\{b_i^{x^1},b_i^{y^1},b_i^{x^2},b_i^{y^2},V_i\}$ The center of a bounding box is used as the origin polygon tuple：$v_{i,j}=\{\alpha_{i,j},\beta_{i,j},\gamma_{i,j}\}$ polar coordinate：distance &amp; oriented angle，相对距离（相对anchor box的对角线），相对角度（norm到[0,1]） polar cell：一定角度的扇形区域 内，如果sector内没有定点，conf=0 general shape： 不同尺度，形状相同的object，在polar coord下表示是一样的 distance*anchor box的对角线，转换成绝对尺度 bounding box的两个对角预测，负责尺度估计，polygon只负责预测形状 sharing values should make the learning easier mix loss output：a*(4+1+3*n_vmax) box center loss：bce box wh loss：l2 loss conf loss：bce with ignore mask cls loss：bce polygon loss：$\gamma(log(\frac{\alpha}{anchor^d})-\hat a)^2 + \gammabce(\beta,\hat{beta})+bce(\gamma, \hat \gamma)$ auxiliary task learning： 任务间相互boost converge faster Scaled-YOLOv4: Scaling Cross Stage Partial Network 动机 model scaling method redesign yolov4 and propose yolov4-CSP develop scaled yolov4 yolov4-tiny yolov4-large 没什么技术细节，就是网络结构大更新 论点 common technique changes depth &amp; width of the backbone recently there are NAS model scaling input size、width、depth对网络计算量呈现square, linear, and square increase 改成CSP版本以后，能够减少参数量、计算量，提高acc，缩短inference time 检测的准确性高度依赖reception field，RF随着depth线性增长，随着stride倍数增长，所以一般先组合调节input size和stage，然后再根据算力调整depth和width 方法 backbone：CSPDarknet53 neck：CSP-PAN，减少40%计算量，SPP yoloV4-tiny yoloV4-large：P456 补充细节 box regression YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors 精度 YOLOv7打败了所有real-time detectors：GPU V100上30以上FPS的模型，56.8% AP YOLOv7-E6打败了transformer-based (SWIN- L Cascade-Mask R-CNN)/conv-based (ConvNeXt-XL Cascade-Mask R-CNN)的两阶段模型 论点 结构上 CPU real-time detectors基本是基于MobileNet、ShuffleNet、GhostNet GPU real-time detectors则大多数用ResNet、DarkNet、CSPNet strategy Real-time object detectors 大多数基于YOLO / FCOS faster and stronger back effective feature integration accurate detection head robust loss a more efficient label assignment method a more efficient training method 本文关注4/5/6要素 Model re-parameterization merge multiple computational modules into one at inference stage 其实是一种模型/module层面的ensemble 模型层面 ema k-fold ensemble module层面 线形层的合并 需要设计可合并的module（如非线性操作放在branch外面） Model scaling 大多数NAS方法不会考虑各种factor的correlation darknet之类的网络实际上是compound scaling的，网络加深的同时内部block的branch就进行了同步的加宽，效果上来讲并不是孤立的增加了depth main contribution 主要focus在training method上 优化训练进程 增加了training cost，但是不影响inference性能 所以叫trainable bag-of-freebies two issues &amp; solutions model re-parameterization dynamic label assignment propose extent &amp; compound scaling 降低40%参数量 &amp; 50%计算量 faster &amp; higher acc Architecture Extended efficient layer aggregation networks 考虑要素 number of parameters the amount of computation the computational density 提出了Extended-ELAN group conv：相同计算量下，可以扩大channel（expand cardinality） shuffle &amp; merge：combine the features of different groups，持续增强学习能力同时不伤害original gradient path【不太理解为啥有这个作用】 Model scaling for concatenation-based models 大多数讨论scaleup的网络结构，在加深时，网络内部的block的输入/输出维度不会变化，但是本文这种concatenation-based architecture，在加深时因为分支变多了，宽度也变了，因此并不能孤立探讨一个scaleup factor compound scaling 在加深computation block的深度的同时，它的宽度也变了 因此也要同步调整其他部分的宽度 从而maintain the optimal structure Trainable bag-of-freebies Planned re-parameterized convolution：RepConvN 受RepConv的启发，RepVGG的效果是好的，但是直接复用在resnet/densenet结构上会显著掉点 recall RepConv：inference阶段的一个3x3conv在训练阶段其实是2/3个分支（identity+1x1+3x3） 本文实验发现直接复用RepConv，它的id path会destroy the residual in ResNet/DenseNet，所以本文改良的RepConv去掉了id path（RepConvN） Coarse for auxiliary and fine for lead loss：label assigner deep supervision：对中间层添加辅助分支进行loss guidance 保留作为最终输出的branch叫做lead head 只在训练阶段做辅助的branch叫auxiliary head label assignment soft label：如用pred box和gt box的IoU作为objectness的标签，是随着网络预测结果调节标签的 one issue：如何给每个预测头分配label，计算loss？ 各算各的 用lead prediction来assign both：因为lead head的学习能力更强，生成的标签更representative一些 this paper one step further 用lead prediction来assign both 同时生成的是coarse-to-fine的hierarchical label，to fit不同的feature level fine label还是上面那个lead head计算的soft label coarse label则allow more grids as positives，constraints放的更低了，因为让学习能力不够强的层提前学习过于精准的label，会导致后面的层学习能力恶化，但是如果前期encourage more，后期只需过滤掉低质量框就好了 具体实现是通过put restrictions in the decoder so that the extra coarse positive grids cannot produce soft label properly，这个fine-coarse的assign机制是在训练过程中online实现的 Other trainable bag-of-freebies BN：conv-bn-activation，测试阶段合并线形计算单元 Implicit knowledge in YOLOR：测试阶段也能合并 EMA：老ensemble了]]></content>
      <tags>
        <tag>目标检测，one-stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[triplet-center-loss论文]]></title>
    <url>%2F2019%2F11%2F13%2Ftriplet-center-loss%E8%AE%BA%E6%96%87%2F</url>
    <content type="text"><![CDATA[0. before reading结合： triplet loss：考虑类间关系，但计算复杂度高，困难样本难挖掘 center loss：考虑类内关系 TCL：同时增加类内数据的紧实度（compactness）和类间的分离度（separability） 三元组只考虑样本、所属类中心、最近邻类的中心。避免了建立triplets的复杂度和mining hard samples的难度。 title：Triplet-Center Loss for Multi-View 3D Object Retrieval 动机：deep metric learning the learned features using softmax loss are not discriminative enough in nature although samples of the two classes are separated by the decision boundary elaborately, there exists significant intra-class variations QUESTION1：so what? how does this affect the current task? 动机描述不充分。 QUESTION2：在二维平面上overlap不代表在高维空间中overlap，这种illustration究竟是否有意义。 ANSWER for above：高维空间可分，投影到二维平面不一定可分，但是反过来，二维平面上高度可分，映射会高维空间数据仍旧是高度可分的。只能说，后者能够确保不同类别数据离散性更好，不能说明前者数据离散性不好（如果定义了高维距离，也可以说明）。 应用场景：3D object retrieval 要素： learns a center for each class requires that the distances between samples and centers from the same class are smaller than those from different classes, in this way the samples are pulled closer to the corresponding center and meanwhile pushed away from the different centers both the inter-class separability and the intra-class variations are considered 论点： Compared with triplet loss, TCL avoids the complex construction of triplets and hard sample mining mechanism. Compared with center loss, TCL not only considers to reduce the intra-class variations. QUESTION：what about the comparison with [softmax loss + center loss]? ANSWER for above：center-loss is actually representing for the joint loss [softmax loss + center loss]. ‘’Since the class centers are updated at each iteration based on a mini-batch instead of the whole dataset, which can be very unstable, it has to be under the joint supervision of softmax loss during training. ‘’ 本文做法： the proposed TCL is used as the supervision loss the softmax loss could be also combined in as an addition 细节： TCL： L_{tc} = \sum_{i=1}^Mmax(D(f_i, c_{y^i}) + m - min_{j\neq y^i}D(f_i, c_j), 0) 前半部分是center-loss，类内欧几里得距离，后半部分是每个样本和与其最近的negative center之间的距离。 ‘Unlike center loss, TCL can be used independently from softmax loss. However… ‘ 作者解释说，因为center layer是随机初始化出来的，而且是batch updating，因此开始阶段会比较tricky，’while softmax loss could serve as a good guider for seeking better class centers ‘ 调参中提到’m is fixed to 5’，说明本文对feature vector没有做normalization（相比之下facenet做了归一化，限定所有embedding分布在高维球面上）。 衡量指标：AUC和MAP，这是一个retrieval任务，最终需要的是embedding，给定Query，召回top matches。 reviews： 个人理解： softmax分类器旨在数据可分，对于分类边界、feature vector的空间意义不存在一个具象的描述。deep metric learning能够引入这种具象的、图像学的意义，在此基础上，探讨distance、center才有意义。 就封闭类数据（类别有限且已知）分类来讲，分类边界有无图像学描述其实意义不大。已知的数据分布尽可能discriminative的主要意义是针对未知类别，我们希望给到模型一个未知数据时，它能够检测出来，而不是划入某个已知类（softmax）。 TCL的最大贡献应该是想到用center替代样本来进行metric judgement，改善triplet-loss复杂计算量这一问题，后者实际训起来太难了，没有感情的GPU吞噬机器。 XXX： 能够引入这种具象的、图像学的意义，在此基础上，我们探讨distance、center才有意义。 ​]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dicomReader]]></title>
    <url>%2F2019%2F11%2F11%2FdicomReader%2F</url>
    <content type="text"><![CDATA[read a dcm file 1234import SimpleITK as sitkimage = sitk.ReadImage(dcm_file)image_arr = sitk.GetArrayFromImage(image) read a dcm series 12345678910series_IDs = sitk.ImageSeriesReader.GetGDCMSeriesIDs(series_path)nb_series = len(series_IDs)print(nb_series)# 默认获取第一个序列的所有切片路径dicom_names = sitk.ImageSeriesReader.GetGDCMSeriesFileNames(file_path)series_reader = sitk.ImageSeriesReader()series_reader.SetFileNames(dicom_names)image3D = series_reader.Execute() read a dcm case 123456series_IDs = sitk.ImageSeriesReader.GetGDCMSeriesIDs(case_path)for series_id in series_IDs: dicom_names = sitk.ImageSeriesReader.GetGDCMSeriesFileNames(case_path, series_id) series_reader = sitk.ImageSeriesReader() series_reader.SetFileNames(dicom_names) image3D = series_reader.Execute() read tag 12# 首先得到image对象Image_type = image.GetMetaData("0008|0008") if image.HasMetaData("0008|0008") else 'Nan' 发现一种序列，每张图的尺寸不同，这样执行series_reader的时候会报错，因为series_reader会依照第一层的图像尺寸申请空间，所以要么异常要么逐张读。 reference: http://itk-users.7.n7.nabble.com/ITK-users-Reader-InvalidRequestedRegionError-td38608.html]]></content>
      <tags>
        <tag>SimpleITK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++ tricks in engineering]]></title>
    <url>%2F2019%2F11%2F06%2Fc-tricks-in-engineering%2F</url>
    <content type="text"><![CDATA[数组传参 工程化被坑了好多回！ C/C++ 传递数组，虽然传递的是首地址地址，但是参数到了函数内就成了普通指针。 所以试图在调用函数中求取所传递数组的长度是行不通的。 vector传参 传值—&gt;拷贝构造，传引用／指针—&gt;不发生拷贝构造。 实际工程化中遇到的问题是，构建了一个vector\ imgs对象，传入函数以后，在函数内部创建空间cv::Mat img，然后将img push进vector。在函数外读取该vector的时候发现其内部没值。 要点：1. 要传引用，2. push clone：imgs.push_back(img) 另外，vector可以作为函数返回值。]]></content>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图像算法综述]]></title>
    <url>%2F2019%2F10%2F31%2F%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[类别 按照任务类型：度量学习（metric learning）和描述子学习（image descriptor learning） 按照网络结构：pairwise的siamese结构、triplet的three branch结构、以及引入尺度信息的central-surround结构 按照网络输出：特征向量（feature embedding）和单个概率值（pairwise similarity） 按照损失函数：对比损失函数、交叉熵损失函数、triplet loss、hinge loss等，此外损失函数可以带有隐式的困难样本挖掘，例如pn-net中的softpn等，也可以是显示的困难挖掘。 Plain网络 主要是说AlexNet／VGG-Net，后者更常用一些。 Plain网络的设计主要遵循以下几个准则： （1）输出特征图尺寸相同的层使用相同数量的滤波器。 （2）如果特征图尺寸减半，那么滤波器数量就加倍，从而保证每层的时间复杂度相同（这是为啥？？）。 名词 感受野：卷积神经网络每一层输出的特征图上的像素点在原始图像上映射区域的大小。通俗的说，就是输入图像对这一层输出的神经元的影响有多大。 感受野计算：由当前层向前推，需要的参数是kernel size和stride。 N\_RF = kernel\_size + (cur\_RF-1)*stride 其中$cur_RF$是当前层（start from 1），$N_RF$、$kernel_size$、$stride$是上一层参数。 有效感受野：并不是感受野内所有像素对输出向量的贡献相同，在很多情况下感受野区域内像素的影响分布是高斯，有效感受野仅占理论感受野的一部分，且高斯分布从中心到边缘快速衰减。 感受野大小： 小感受野：local，位置信息更准确 大感受野：global，语义信息更丰富 inception module：下图为其中一种。 意义：增加网络深度和宽度的同时，减少参数。结构中嵌入了多尺度信息，集成了多种不同感受野上的特征。 building block：左边这种，红色框框里面是一个block。 几个相同的building block堆叠为一层conv。在第一个building Block块中，输出特征图的尺寸下降一半（第一个卷积stride=2），剩余的building Block块输入输出尺寸是一样的。 bottleneck：右边这种，蓝色框框block。字面意思，瓶颈，形容输入输出维度差距较大。 第一个1*1负责降低维度，第二个1*1负责恢复维度，3*3层就处在一个输入／输出维度较小的瓶颈。 左右两种结构时间复杂度相似。 &lt;img src=&quot;图像算法综述/block.png&quot; width=&quot;30%;&quot; /&gt; &lt;img src=&quot;图像算法综述/ImageNet.png&quot; width=&quot;110%;&quot; /&gt; top-1和top-5：top-1就是预测概率最大的类别，top-5则取最后预测概率的前五个，只要其中包含正确类别则认为预测正确。 使用top-5主要是因为ImageNet中很多图片中其实是包含多个物体的。 accuracy、error rate、F1-score、sensitivity、specificity、precision、recall accuracy：总体准确率 precision：从结果角度，单一类别准确率 recall：从输入角度，预测类别真实为1的准确率 P-R曲线：选用不同阈值，precision-recall围成的曲线 AP：平均精度，P-R曲线围住的面积 F1-score：对于某个分类，综合了Precision和Recall的一个判断指标，因为选用不同阈值，precision-recall会随之变化，F1-score用于选出最佳阈值。 sensitivity：=recall specificity：预测类别真实为0的准确率 reference：https://zhuanlan.zhihu.com/p/33273532 trade-off： FLOPS：每秒浮点运算次数是每秒所执行的浮点运算次数的简称，被用来估算电脑效能。 ROC、AUC、MAP： ROC：TPR和FPR围成的曲线 AUC：ROC围住的面积 mAP：所有类别AP的平均值 梯度弥散： “底层先收敛、高层再收敛”： 特征图：卷积层通过线性滤波器进行线性卷积运算，然后再接个非线性激活函数，最终生成特征图。 TTA test time augmentation：测试时增强，为原始图像造出多个不同版本，包括不同区域裁剪和更改缩放程度等，并将它们输入到模型中；然后对多个版本进行计算得到平均输出，作为图像的最终输出分数。 pooling mode: full mode：从filter和image刚开始相交开始卷积 same mode：当filter的中心和image的角重合时开始卷积，如果stride=1，那么输入输出尺寸相同 valid mode：当filter完全在image里面时开始卷积 空间不变性： 平移不变性：不管输入如何平移，系统产生完全相同的响应，比如图像分类任务，图像中的目标不管被移动到图片的哪个位置，得到的结果（标签）应该是相同的 平移同变性（translation equivariance）：系统在不同位置的工作原理相同，但它的响应随着目标位置的变化而变化，比如实例分割任务，目标如果被平移了，那么输出的实例掩码也相应变化 局部连接：每个神经元没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息 权值共享：对于这个图像上的所有位置，我们都能使用同样的学习特征 池化：通过消除非极大值，降低了上层的计算复杂度。最大池化返回感受野中的最大值，如果最大值被移动了，但是仍然在这个感受野中，那么池化层也仍然会输出相同的最大值。 卷积和池化这两种操作共同提供了一些平移不变性，即使图像被平移，卷积保证仍然能检测到它的特征，池化则尽可能地保持一致的表达。 同理，所谓的CNN的尺度、旋转不变性，也是由于pooling操作，引入的微小形变的鲁棒性。 模型大小与参数量：float32是4个字节，因此模型大小字节数=参数量×4 训练技巧 迁移学习：当数据集太小，无法用来训练一个足够好的神经网络，可以选择fine-tune一些预训练网络。使用时修改最后几层，降低学习率。 keras中一些预训练权重下载地址：https://github.com/fchollet/deep-learning-models/releases/ K-fold交叉验证： 我们不能将全部数据集用于训练——这样就没有数据来测试模型性能了 将数据集分割为training set 和 test set，衡量结果取决于数据集划分，training set和全集之间存在bias，不同test下结果variety很大 交叉验证Cross-Validation： 极端情况LOOCV：全集N，每次取一个做test，其他做train，重复N次，得到N个模型，并计算N个test做平均 K-fold：全集切分成k份，每次取一个做test，其他做train，重复k次～ 实验显示LOOCV和10-foldCV的结果很相近，后者计算成本明显减小 Bias-Variance Trade-Off：K越大，train set越接近全集，bias越小，但是每个train set之间相关性越大，而这种大相关性会导致最终的test error具有更大的Variance 分割 实例分割&amp;语义分割 instance segmentation：标记实例和语义, 不仅要分割出人这个类, 而且要分割出这个人是谁, 也就是具体的实例 semantic segmentation：只标记语义, 也就是说只分割出人这个类来]]></content>
      <tags>
        <tag>CV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Segmentation]]></title>
    <url>%2F2019%2F08%2F22%2FSegmentation%2F</url>
    <content type="text"><![CDATA[idea: CT图一般是单通道灰度图像，假如我将128张CT图堆叠在一起（即128通道的图像），然后用2D卷积（会考虑通道数128），这样和直接用3D卷积会有结果上的差别吗？ 3d网络可以结合图像层间信息，能够保证隔层图像Mask之间的一个变化连续性，效果会比2d好。层间距大的图像，在预处理中会有插值。 3d网络因为显存的限制，一种处理方式是裁成3d patch作为输入，导致其感受野有限，通常只能专注于细节和局部特征，适合作为第二级网络用于对细节做精优化。一种处理方式是降采样，分割精度下降。 2.5D网络。]]></content>
  </entry>
  <entry>
    <title><![CDATA[keras note]]></title>
    <url>%2F2019%2F08%2F14%2Fkeras-note%2F</url>
    <content type="text"><![CDATA[1. keras Lambda自定义层官方文档：将任意表达式(function)封装为 Layer 对象。1keras.layers.Lambda(function, output_shape=None, mask=None, arguments=None) function: 需要封装的函数。 将输入张量作为第一个参数。 output_shape: 预期的函数输出尺寸。(使用 TensorFlow 时，可自动推理得到) arguments: 可选的需要传递给函数的关键字参数。以字典形式传入。 几个栗子： 1.1 最简：使用匿名函数123model.add(Lambda(lambda x: x ** 2))x = Lambda(lambda image: K.image.resize_images(image, (target_size, target_size)))(inpt) 其中，lambda是python的匿名函数，后面的[xx: xxxx]用来描述函数的表达形式，lambda xx: xxxx整体作为Lambda函数的function参数。 1.2 中级：通过字典传参，封装自定义函数，实现数据切片123456789101112131415from keras.layers import Input, Lambda, Dense, Activation, Reshape, concatenatefrom keras.utils import plot_modelfrom keras.models import Modeldef slice(x, index): return x[:, :, index]a = Input(shape=(4,2))x1 = Lambda(slice,output_shape=(4,1),arguments=&#123;'index':0&#125;)(a)x2 = Lambda(slice,output_shape=(4,1),arguments=&#123;'index':1&#125;)(a)x1 = Reshape((4,1,1))(x1)x2 = Reshape((4,1,1))(x2)output = concatenate([x1,x2])model = Model(a, output)plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True) 模型结构图如下： 1.3 高级：自定义损失函数 step 1. 把y_true定义为一个输入 step 2. 把loss写成一个层，作为网络的最终输出 step 3. 在compile的时候，将loss设置为y_pred 123456789101112# yolov3 train.py create_model:model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss', arguments=&#123;'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5&#125;)( [*model_body.output, *y_true])model = Model([model_body.input, *y_true], model_loss)model.compile(optimizer=Adam(lr=1e-3), loss=&#123;'yolo_loss': lambda y_true, y_pred: y_pred&#125;)# yolov3 model.py yolo_lossdef yolo_loss(args, anchors, num_classes, ignore_thresh=.5, print_loss=False): ... return loss 2. keras 自定义loss补充1.3: 也可以不定义为网络层，直接调用自定义loss函数参数： y_true: 真实标签，Theano/Tensorflow 张量。 y_pred: 预测值。和 y_true 相同尺寸的 Theano/TensorFlow 张量。1234def mycrossentropy(y_true, y_pred, e=0.1): return (1-e)*K.categorical_crossentropy(y_pred,y_true) + e*K.categorical_crossentropy(y_pred, K.ones_like(y_pred)/num_classes)model.compile(optimizer='rmsprop',loss=mycrossentropy, metrics=['accuracy']) 带参数的自定义loss： 有时候我们计算loss的时候不只要用到y_true和y_pred，还想引入一些参数，但是keras限定构造loss函数时只能接收(y_true, y_pred)两个参数，如何优雅的传入参数？ 优雅的解决方案如下： 1234567891011# build modelmodel = my_model()# define loss funcmodel_loss = dice_loss(smooth=1e-5, thresh=0.5)model.compile(loss=model_loss)# 实现def dice_loss(smooth, thresh): def dice(y_true, y_pred): return 1-dice_coef(y_true, y_pred, smooth, thresh) return dice 3. keras 自定义metricsmodel.compile里面除了loss还有一个metrics，用于模型性能评估参数： y_true: 真实标签，Theano/Tensorflow 张量。 y_pred: 预测值。和 y_true 相同尺寸的 Theano/TensorFlow 张量。12345678910111213141516def precision(y_true, y_pred): # Calculates the precision true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1))) precision = true_positives / (predicted_positives + K.epsilon()) return precisiondef recall(y_true, y_pred): # Calculates the recall true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) possible_positives = K.sum(K.round(K.clip(y_true, 0, 1))) recall = true_positives / (possible_positives + K.epsilon()) return recallmodel.compile(optimizer='rmsprop',loss=mycrossentropy, metrics=['accuracy', recall, precision]) 4. keras 自定义Layer源代码：https://github.com/keras-team/keras/blob/master/keras/engine/base_layer.py 自定义layer继承keras的Layer类，需要实现三个方法： build(input_shape)：定义权重，调用add_weight来创建层的权重矩阵，其中有参数trainable声明该参数的权重是否为可训练权重，若trainable==True，会执行self._trainable_weights.append(weight)将该权重加入到可训练权重的lst中。 call(x)：编写层逻辑 compute_output_shape(input_shape)：定义张量形状的变化逻辑 get_config：返回一个字典，获取当前层的参数信息 看了keras一些层的实现，keras中层（如conv、depthwise conv）的call函数基本都是通过调用tf.backend中的方法来实现 4.1 栗子：CenterLossLayer 123456789101112131415161718192021222324252627282930class CenterLossLayer(Layer): def __init__(self, alpha=0.5, **kwargs): # alpha：center update的学习率 super().__init__(**kwargs) self.alpha = alpha def build(self, input_shape): # add_weight：为该层创建一个可训练／不可训练的权重 self.centers = self.add_weight(name='centers', shape=(10, 2), initializer='uniform', trainable=False) # 一定要在最后调用它 super().build(input_shape) def call(self, x, mask=None): # x[0] is Nx2, x[1] is Nx10 onehot, self.centers is 10x2 delta_centers = K.dot(K.transpose(x[1]), (K.dot(x[1], self.centers) - x[0])) # 10x2 center_counts = K.sum(K.transpose(x[1]), axis=1, keepdims=True) + 1 # 10x1 delta_centers /= center_counts new_centers = self.centers - self.alpha * delta_centers # add_update：更新层内参数(build中定义的参数)的方法 self.add_update((self.centers, new_centers), x) self.result = x[0] - K.dot(x[1], self.centers) self.result = K.sum(self.result ** 2, axis=1, keepdims=True) #/ K.dot(x[1], center_counts) return self.result # Nx1 def compute_output_shape(self, input_shape): return K.int_shape(self.result) 有一些自定义层，有时候会不实现compute_output_shape和get_config 在call方法中，输出tensor如果发生了shape的变化，keras layer是不会自动推导出输出shape的，所以要显示的自定义compute_output_shape 不管定不定义get_config方法，都可以使用load_weights方法加载保存的权重 但是如果要使用load_model方法载入包含自定义层的model，必须要显示自定义get_config方法，否则keras 无法获知 Linear 的配置参数！ 在 __init__ 的最后加上 **kwargs 参数，并用 **kwargs 参数初始化父类。 实现上述的 get_config 方法，返回自定义的参数配置和默认的参数配置 4.2 补充1.3 &amp; 2: 自定义损失函数除了可以用Lambda层，也可以定义Layer层，这是个没有权重的自定义Layer。 123456789101112131415161718# 官方示例：Custom loss layerclass CustomVariationalLayer(Layer): def __init__(self, **kwargs): self.is_placeholder = True super(CustomVariationalLayer, self).__init__(**kwargs) def vae_loss(self, x, x_decoded_mean): xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)#Square Loss kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)# KL-Divergence Loss return K.mean(xent_loss + kl_loss) def call(self, inputs): x = inputs[0] x_decoded_mean = inputs[1] loss = self.vae_loss(x, x_decoded_mean) self.add_loss(loss, inputs=inputs) # We won't actually use the output. return x 4.3 补充 call方法的完整参数：call(self, inputs, args, *kwargs) 其中inputs就是层输入，tensor/tensors 除此之外还有两个reserved keyword arguments：training&amp;mask，一个用于bn/dropout这种train/test计算有区别的flag，一个用于RNNlayers约束时序相关关系 args和*kwargs是预留为了以后扩展更多输入参数的 4.4 更加flexible的自定义层：https://keras.io/guides/making_new_layers_and_models_via_subclassing/ 我们可以将trainable variable直接定义在__init__()里面，省略build，直接call： 12345678910111213141516171819202122232425262728293031# use tf.Variableclass Linear(Layer): def __init__(self, units=32, input_dim=32): super(Linear, self).__init__() w_init = tf.random_normal_initializer() self.w = tf.Variable( initial_value=w_init(shape=(input_dim, units), dtype="float32"), trainable=True, ) b_init = tf.zeros_initializer() self.b = tf.Variable( initial_value=b_init(shape=(units,), dtype="float32"), trainable=True ) def call(self, inputs): return tf.matmul(inputs, self.w) + self.b # use self.add_weight方法class Linear(Layer): def __init__(self, units=32, input_dim=32): super(Linear, self).__init__() self.w = self.add_weight( shape=(input_dim, units), initializer="random_normal", trainable=True ) self.b = self.add_weight(shape=(units,), initializer="zeros", trainable=True) def call(self, inputs): return tf.matmul(inputs, self.w) + self.b ！！！我实验下来发现对tf1，第一种写法没法创建权重，第二种才能，可能要到tf2才行 build中add_weight实际上也是调用tf.Variable创建一个varaible 4.5 Layer内部也可以创建a Layer instance作为其attribute——torch style 也是放在__init__()里面，如learnable positional embedding 123456789101112131415161718192021class MLPBlock(Layer): def __init__(self): super(MLPBlock, self).__init__() self.linear_1 = Linear(32) self.linear_2 = Linear(32) self.linear_3 = Linear(1) def call(self, inputs): x = self.linear_1(inputs) x = tf.nn.relu(x) x = self.linear_2(x) x = tf.nn.relu(x) return self.linear_3(x)mlp = MLPBlock()y = mlp(tf.ones(shape=(3, 64))) # The first call to the `mlp` will create the weightsprint("weights:", len(mlp.weights))print("trainable weights:", len(mlp.trainable_weights))！！！tf2才支持，别瞎搞 ！！！需要注意的是：用躲避build的方法创建的层，在层被fisrt call的时候才会创建权重，而不是在模型创建阶段 5. keras Generator本质上就是python的生成器，每次返回一个batch的样本及标签自定义generator的时候要写成死循环（while true），因为model.fit_generator()在使用在个函数的时候，并不会在每一个epoch之后重新调用，那么如果这时候generator自己结束了就会有问题。栗子是我为mixup写的generator：没有显示的while True是因为创建keras自带的generator的时候已经是死循环了（for永不跳出） 123456789101112131415161718192021222324252627def Datagen_mixup(data_path, img_size, batch_size, is_train=True, mix_prop=0.8, alpha=1.0): if is_train: datagen = ImageDataGenerator() else: datagen = ImageDataGenerator() # using keras库函数 generator = datagen.flow_from_directory(data_path, target_size=(img_size, img_size), batch_size=batch_size, color_mode="grayscale", shuffle=True) for x,y in generator: # a batch of &lt;img, label&gt; if alpha &gt; 0: lam = np.random.beta(alpha, alpha) else: lam = 1 idx = [i for i in range(x.shape[0])] random.shuffle(idx) mixed_x = lam*x + (1-lam)*x[idx] mixed_y = lam*y + (1-lam)*y[idx] n_origin = int(batch_size * mix_prop) gen_x = np.vstack(x[:n_origin], mixed_x[:(batch_size-n_origin)]) gen_y = np.vstack(y[:n_origin], mixed_y[:(batch_size-n_origin)]) yield gen_x, gen_y 【多进程】fit_generator中有一个参数use_multiprocessing，默认设置为false，因为‘using a generator with use_multiprocessing=True and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence’ class’ 如果设置多进程use_multiprocessing，代码会把你的数据复制几份，分给不同的workers进行输入，这显然不是我们希望的，我们希望一份数据直接平均分给多个workers帮忙输入，这样才是最快的。而Sequence数据类能完美解决这个问题。 keras.utils.Sequence()： 每一个 Sequence 必须实现 __getitem__ 和 __len__ 方法 __getitem__ 方法应该范围一个完整的批次 如果你想在迭代之间修改你的数据集，你可以实现 on_epoch_end（会在每个迭代之间被隐式调用）\ github上有issue反映on_epoch_end不会没调用，解决方案：在__len__方法中显示自行调用 直接看栗子： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import kerasimport mathimport osimport cv2import numpy as npfrom keras.applications import ResNet50from keras.optimizers import SGDclass DataGenerator(keras.utils.Sequence): def __init__(self, data, batch_size=1, shuffle=True): self.batch_size = batch_size self.data = data self.indexes = np.arange(len(self.data)) self.shuffle = shuffle def __len__(self): # 计算每一个epoch的迭代次数 return math.ceil(len(self.data) / float(self.batch_size)) def __getitem__(self, index): # 生成每个batch数据 batch_indices = self.indexes[index*self.batch_size:(index+1)*self.batch_size] batch_data = [self.data[k] for k in batch_indices] x_batch, y_batch = self.data_generation(batch_data) return x_batch, y_batch def on_epoch_end(self): if self.shuffle == True: np.random.shuffle(self.indexes) def data_generation(self, batch_data): images = [] labels = [] # 生成数据 for i, data in enumerate(batch_data): image = cv2.imread(data, 0) image = cv2.resize(image, dsize=(64,64), interpolation=cv2.INTER_LINEAR) if np.max(image)&gt;1: image = image / 255. image = np.expand_dims(image, axis=-1) images.append(image) if 'd0' in data: labels.append([1,0]) else: labels.append([0,1]) return np.array(images), np.array(labels)if __name__ == '__main__': # data data_dir = "/Users/amber/dataset/mnist" data_lst = [] for file in os.listdir(data_dir+"/d0")[:]: data_lst.append(os.path.join(data_dir, "d0", file)) for file in os.listdir(data_dir+"/d1")[:]: data_lst.append(os.path.join(data_dir, "d1", file)) training_generator = DataGenerator(data_lst, batch_size=128) # model model = ResNet50(input_shape=(64,64,1),weights=None, classes=2) model.compile(optimizer=SGD(1e-3), loss='categorical_crossentropy', metrics=['accuracy']) model.fit_generator(training_generator, epochs=50,max_queue_size=200,workers=2) 经验值： workers：2/3 max_queue_size：默认10，具体基于GPU处于空闲状态适量调节 【附加】实验中还发现一个问题，最开始定义了一个sequential model，然后在调用fit_generator一直报错：model not compile，但是显然model是compile过了的，网上查到的解释：‘Sequential model works with model.fit but not with model.fit_generator’ 6. 多GPU多GPU运行分为两种情况： * 数据并行 * 设备并行 6.1 数据并行 数据并行将目标模型在多个GPU上各复制一份，使用每个复制品处理数据集的不同部分。 一个栗子：写tripleNet模型时，取了batch=4，总共15类，那么三元组总共有$(4/2)^2*15=60$个，训练用了224的图像，单张GPU内存会溢出，因此需要单机多卡数据并行。 ​ step1. 在模型定义中，用multi_gpu_model封一层，需要在model.compile之前。 12345678910111213141516171819202122232425262728from keras.util import multi_gpu_modeldef triple_model(input_shape=(512,512,1), n_classes=10, multi_gpu=False): anchor_input = Input(shape=input_shape) positive_input = Input(shape=input_shape) negative_input = Input(shape=input_shape) sharedCNN = base_model(input_shape) encoded_anchor = sharedCNN(anchor_input) encoded_positive = sharedCNN(positive_input) encoded_negative = sharedCNN(negative_input) # class branch x = Dense(n_classses, activation='softmax')(encoded_anchor) # distance branch encoded_anchor = Activation('sigmoid')(encoded_anchor) encoded_positive = Activation('sigmoid')(encoded_positive) encoded_negative = Activation('sigmoid')(encoded_negative) merged = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=-1, name='tripleLossLayer') model = Model(inputs=[anchor_input,positive_input,negative_input], outputs=[x, merged]) if multi_gpu: model = multi_gpu_model(model, GPU_COUNT) model.compile(optimizer=SGD, loss=[cls_loss, triplet_loss], metrics=[cls_acc]) return model ​ step2. 在定义checkpoint时，要用ParallelModelCheckpoint封一层，初始化参数的model要传原始模型。 1234567891011121314151617181920212223from keras.callbacks import ModelCheckpointclass ParallelModelCheckpoint(ModelCheckpoint): def __init__(self,single_model,multi_model, filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1): self.single_model = single_model self.multi_model = multi_model super(ParallelModelCheckpoint,self).__init__(filepath, monitor, verbose,save_best_only, save_weights_only,mode, period) def set_model(self, model): self.single_model.optimizer = self.multi_model.optimizer super(ParallelModelCheckpoint,self).set_model(self.single_model) def on_epoch_end(self, epoch, logs=None): # save optimizer weights self.single_model.optimizer = self.multi_model.optimizer super(ParallelModelCheckpoint, self).on_epoch_end(epoch, logs)model = triple_model(multi_gpu=True)single_model = triple_model(multi_gpu=False)filepath = "./tripleNet_&#123;epoch:02d&#125;_val_loss_&#123;val_loss:.3f&#125;.h5"check_point = ParallelModelCheckpoint(single_model, filepath) ​ step3. 在保存权重时，通过cpu模型来保存。 1234567# 实例化基础模型，这样定义模型权重会存储在CPU内存中with tf.device('/cpu:0'): model = Resnet50(input_shape=(512,512,3), classes=4, weights=None) parallel_model = multi_gpu_model(model, GPU_COUNT)parallel_model.fit(x,y, epochs=20, batch_size=32)model.save('model.h5') ​ 【attention】同理，在load权重时，也是load单模型的权重，再调用multi_gpu_model将模型复制到多个GPU上： 12345model = Model(inputs=[anchor_input,positive_input,negative_input], outputs=[x, merged])if multi_gpu: if os.path.exists(weight_pt): model.load_weights(weight_pt) model = multi_gpu_model(model, GPU_COUNT) ​ step4. 如果保存成了多GPU权重，可以如下代码解封： 123456model = EfficientV2(input_shape=380, n_classes=2)model = multi_gpu_model(model, 2)model.load_weigts('multi.h5')single_model = model.layers[-2]single_model.save_weights('single.h5') 【ATTENTION】实验中发现一个问题：在有些case中，我们使用了自定义loss作为网络的输出，此时网络的输出是个标量，但是在调用multi_gpu_model这个方法时，具体实现在multi_gpu_utils.py中，最后一个步骤要merge几个device的输出，通过axis=0的concat实现，网络输出是标量的话就会报错——list assignment index out of range。 尝试的解决方案是改成相加： 1234567# Merge outputs under expected scope.with tf.device('/cpu:0' if cpu_merge else '/gpu:%d' % target_gpu_ids[0]): merged = [] for name, outputs in zip(output_names, all_outputs): merged.append(Lambda(lambda x: K.sum(x))(outputs)) # merged.append(concatenate(outputs, axis=0, name=name)) return Model(model.inputs, merged) 【ATTENTION++】网络的输出不能是标量！！永远会隐藏保留一个batch dim，之前是写错了！！ model loss是一个标量 作为输出层的loss是保留batch dim的！！ 6.2 设备并行 设备并行适用于多分支结构，一个分支用一个GPU。通过使用TensorFlow device scopes实现。 栗子： 12345678910111213141516# Model where a shared LSTM is used to encode two different sequences in parallelinput_a = keras.Input(shape=(140, 256))input_b = keras.Input(shape=(140, 256))shared_lstm = keras.layers.LSTM(64)# Process the first sequence on one GPUwith tf.device_scope('/gpu:0'): encoded_a = shared_lstm(tweet_a)# Process the next sequence on another GPUwith tf.device_scope('/gpu:1'): encoded_b = shared_lstm(tweet_b)# Concatenate results on CPUwith tf.device_scope('/cpu:0'): merged_vector = keras.layers.concatenate([encoded_a, encoded_b],axis=-1) 7. 库函数讲解7.1 BatchNormalization(axis=-1) 用于在每个batch上将前一层的激活值重新规范化，即使得其输出数据的均值接近0，其标准差接近1。 常用参数axis：指定要规范化的轴，通常为特征轴，如在“channels_first”的data format下，axis=1，反之axis=-1。 7.2 LSTM 参数： units：输出维度（最后一维），标准输入NxTxD，N for batch，T for time-step，D for vector-dimension。 activation：激活函数 recurrent_activation：用于循环时间步的激活函数 dropout：在 0 和 1 之间的浮点数。 单元的丢弃比例，用于输入的线性转换 recurrent_dropout：在 0 和 1 之间的浮点数。 单元的丢弃比例，用于循环层状态的线性转换 return_sequences: 布尔值，默认False。是返回输出序列中的最后一个输出，还是全部序列的输出。即many-to-one还是many-to-many，简单来讲，当我们需要时序输出（many-to-many）的时候，就set True。 return_state: 布尔值，默认False。除了输出之外是否返回最后一个状态（cell值） 1234567891011121314151617181920# return_sequencesinputs1 = Input(tensor=(1，3, 1))lstm1 = LSTM(1, return_sequences=True)(inputs1)'''输出结果为[[[-0.02243521][-0.06210149][-0.11457888]]]表示每个time-step，LSTM cell的输出'''# return_statelstm1, state_h, state_c = LSTM(1, return_state=True)(inputs1)'''输出结果为[array([[ 0.10951342]], dtype=float32), array([[ 0.10951342]], dtype=float32), array([[ 0.24143776]], dtype=float32)] list中依次为网络输出，最后一个time-step的LSTM cell的输出值和cell值''' 7.2.5 TimeDistributed 顺便再说下TimeDistributed，当我们使用many-to-many模型，最后一层LSTM的输出维度为k，而我们想要的最终输出维度为n，那么就需要引入Dense层，对于时序模型，我们要对每一个time-step引入dense层，这实质上是多个Dense操作，那么我们就可以用TimeDistributed来包裹Dense层来实现。 123model = Sequential()model.add(LSTM(3, input_shape=(length, 1), return_sequences=True))model.add(TimeDistributed(Dense(1))) 官方文档：这个封装器将一个层应用于输入的每个时间片。 当该层作为第一层时，应显式说明input_shape TimeDistributed可以应用于任意层，如Conv3D： 1234567891011121314151617# 例如我的crnn modeldef crnn(input_shape, cnn, n_classes=24): inpt = Input(input_shape) x = TimeDistributed(cnn, input_shape=input_shape)(inpt) x = LSTM(128, return_sequences=True)(x) x = LSTM(256, return_sequences=True)(x) x = TimeDistributed(Dense(n_classes))(x) model = Model(inpt, x) model.summary() return model crnn_model = crnn((24,128,128,128,2), cnn_model) 7.3 Embedding 用于将稀疏编码映射为固定尺寸的密集表示。 输入形如（samples，sequence_length）的2D张量，输出形如(samples, sequence_length, output_dim)的3D张量。 参数： input_dim：字典长度，即输入数据最大下标+1 output_dim： input_length： 栗子： 123456789101112# centerloss branchlambda_c = 1input_ = Input(shape=(1,))centers = Embedding(10,2)(input_) # (None, 1, 2)# 这里的输入是0-9的枚举（dim=10），然后映射成一个簇心intra_loss = Lambda(lambda x:K.sum(K.square(x[0]-x[1][:,0]),1,keepdims=True))([out1,centers])model_center_loss = Model([inputs,input_],[out2,intra_loss])model_center_loss.compile(optimizer="sgd", loss=["categorical_crossentropy",lambda y_true,y_pred:y_pred], loss_weights=[1,lambda_c/2.], metrics=["acc"])model_center_loss.summary() 7.4 plot_model 12from keras.utils import plot_modelplot_model(model, to_file=&apos;model.png&apos;, show_shapes=False, show_layer_names=True) 7.5 K.function 获取模型某层的输出，一种方法是创建一个新的模型，使它的输出是目标层，然后调用predict。 123456model = ... # the original modelnew_model = Model(input=model.input, output=model.get_layer('my_layer').output)intermediate_output = new_model.predict(input_data） 也可以创建一个函数来实现：keras.backend.function(inputs, outputs, updates=None) 123456# 这是写center-loss时写的栗子：func = K.function(inputs=[model.input[0]], outputs=[model.get_layer('out1').output]) # model.input[0]: one input of the multi-input modeltest_features = func([x_test])[0] 7.6 K.gradients(y,x) 求y关于x的导数，y和x可以是张量／张量列表。返回张量列表，列表长度同x列表，列表中元素shape同x列表中元素。 对于$y=[y_1, y_2], x=[x_1, x_2, x_3]$，有返回值$[grad_1, grad_2, grad_3]$，真实的计算过程为： grad_1 = \frac{\partial y_1}{\partial x_1} + \frac{\partial y_2}{\partial x_1} \\ grad_2 = \frac{\partial y_1}{\partial x_2} + \frac{\partial y_2}{\partial x_2} \\ grad_3 = \frac{\partial y_1}{\partial x_3} + \frac{\partial y_2}{\partial x_3}7.7 ModelCheckpoint、ReduceLROnPlateau、EarlyStopping、LearningRateScheduler、Tensorboard 模型检查点ModelCheckpoint 12ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1) filepath可以由 epoch 的值和 logs 的键来填充，如weights.{epoch:02d}-{val_loss:.2f}.hdf5。 moniter：被监测的数据 mode：在 auto 模式中，方向会自动从被监测的数据的名字(不靠谱🤷‍♀️)中判断出来。 学习率衰减ReduceLROnPlateau 学习率的方案相对简单，要么在验证集的损失或准确率开始稳定时调低学习率，要么在固定间隔上调低学习率。 1ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0) 当学习停止时，模型总是会受益于降低 2-10 倍的学习速率。 moniter：被监测的数据 factor：新的学习速率 = 学习速率 * factor patience：被监测数据没有进步的训练轮数，在这之后训练速率会被降低。 更复杂的学习率变化模式定义LearningRateScheduler 前提是只需要用到默认参数是epoch 12345678910111213141516# 首先定义一个变化模式def warmup_scheduler(epoch, mode='power_decay'): lr_base = 1e-5 lr_stable = 1e-4 lr = lr_base * math.pow(10, epoch) if lr&gt;lr_stable: return lr_stable else: return lr # 然后调用LearningRateScheduler方法wrapper这个schedulerscheduler = LearningRateScheduler(warmup_scheduler)# 在使用的时候放在callbacks的list里面，在每个epoch结束触发callbacks = [checkpoint, reduce_lr, scheduler, early_stopping] 更更复杂的学习率变化模式定义可以直接继承Callback：https://kexue.fm/archives/5765 当我们需要传入更丰富的自定义参数/需要进行by step的参数更新等，可以直接继承Callback，进行更自由的自定义 123456789101112131415161718192021222324252627282930313233# 以余弦退火算法为例：class CosineAnnealingScheduler(Callback): """Cosine annealing scheduler. """ def __init__(self, epochs, scale=1.6, shift=0, verbose=0): super(CosineAnnealingScheduler, self).__init__() self.epochs = epochs self.scale = scale self.shift = shift self.verbose = verbose def on_epoch_begin(self, epoch, logs=None): if epoch&lt;=6: # linearly increase from 0 to 1.6 in first 5 epochs lr = 1.6 / 5 * (epoch+1) else: # cosine annealing lr = self.shift + self.scale * (1 + math.cos(math.pi * (epoch+1-5) / self.epochs)) / 2 K.set_value(self.model.optimizer.lr, lr) if self.verbose &gt; 0: print('\nEpoch %05d: CosineAnnealingScheduler setting learning rate to %s.' % (epoch+1, lr)) def on_epoch_end(self, epoch, logs=None): logs = logs or &#123;&#125; logs['lr'] = K.get_value(self.model.optimizer.lr) # 调用lrscheduler = CosineAnnealingScheduler(epochs=2, verbose=1)callbacks = [checkpoint, lrscheduler]model.fit(..., callbacks=callbacks) 有一些计算指标，不好写成张量形式，也可以放在Callback器里面，想咋写就咋写 说白了就是on_epoch_end里面的数据是array，而不是tensor，比较好写 12345678910111213141516171819202122232425262728from keras.callbacks import Callback# 定义Callback器，计算验证集的acc，并保存最优模型class Evaluate(Callback): def __init__(self): self.accs = [] self.highest = 0. def on_epoch_end(self, epoch, logs=None): ###### 自由发挥区域 pred = model.predict(x_test) acc = np.mean(pred.argmax(axis=1) == y_test) ######## self.accs.append(acc) if acc &gt;= self.highest: # 保存最优模型权重 self.highest = acc model.save_weights('best_model.weights') print('acc: %s, highest: %s' % (acc, self.highest))evaluator = Evaluate()model.fit(x_train, y_train, epochs=10, callbacks=[evaluator]) Callback类共支持六种在不同阶段的执行函数： on_epoch_begin：warmup on_epoch_end：metrics on_batch_begin on_batch_end on_train_begin on_train_end 提前停止训练EarlyStopping 1EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False) moniter：被监测的数据 patience：被监测数据没有进步的训练轮数，在这之后训练速率会被降低。 min_delta：在被监测的数据中被认为是提升的最小变化，小于 min_delta 的绝对变化会被认为没有提升。 baseline: 要监控的数量的基准值。 以上这四个都是继承自keras.callbacks() 可视化工具TensorBoard 这个回调函数为 Tensorboard 编写一个日志， 这样你可以可视化测试和训练的标准评估的动态图像， 也可以可视化模型中不同层的激活值直方图。 1TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch') 实际使用时关注第一个参数log_dir就好，查看时通过命令行启动： 1tensorboard --logdir=/full_path_to_your_logs 这几个回调函数，通通在训练时（model.fit / fit_generator）放在callbacks关键字里面。 7.8 反卷积 Conv2DTranspose 三个核心的参数filtes、kernel_size、strides、padding=’valid’ filtes：输出通道数 strides：步长 kernel_size：一般需要通过上面两项计算得到 反卷积运算和正向卷积运算保持一致，即： (output\_shape - kernel\_size) / stride + 1 = input\_shape1234# fcn example: current feature map x (,32,32,32), input_shape (512,512,2), output_shape (,512,512,1)strides = 2kernel_size = input_shape[0] - (x.get_shape().as_list()[1] - 1)*stridesy = Conv2DTranspose(1, kernel_size, padding='valid', strides=strides) 7.9 K.shape &amp; K.int_shape &amp; tensor._keras_shape tensor._keras_shape等价于K.int_shape：张量的shape，返回值是个tuple K.shape：返回值是个tensor，tensor是个一维向量，其中每一个元素可以用[i]来访问，是个标量tensor 两个方法的主要区别是：前者返回值是个常量，只能表征语句执行时刻（如构建图）tensor的状态，后者返回值是个变量，wrapper的方法可以看成一个节点，在graph的作用域内始终有效，在构建图的时候可以是None，在实际流入数据流的时候传值就行，如batch_size！！！ 1234567891011import keras.backend as Kfrom keras.layers import Inputx = Input((22,22,1))print(K.shape(x))# Tensor("Shape:0", shape=(4,), dtype=int32)print(K.shape(x)[0])# Tensor("strided_slice:0", shape=(), dtype=int32)print(K.int_shape(x))# (None, 22, 22, 1) 7.10 binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0) from_logits：logits表示网络的直接输出——没经过sigmoid或者softmax的概率化，默认情况下，我们认为y_pred是已经处理过的概率分布 8. 衍生：一些tf函数8.1 tf.where(condition, x=None, y=None,name=None) 两种用法： 如果x，y为空，返回值是满足condition元素的索引，每个索引占一行。 如果x，y不为空，那么condition、x、y 和返回值相同维度，condition为True的位置替换x中对应元素，condition为False的位置替换y中对应元素。 关于索引indices： condition的shape的dim，就是每一行索引vector的shape，例： 1234567891011import tensorflow as tfimport numpy as npcondition1 = np.array([[True,False,False],[False,True,True]])print(condition1.shape) # (2,3)with tf.compat.v1.Session() as sess: print(sess.run(tf.where(condition1)))# [[0 0]# [1 1]# [1 2]]# condition是2x3的arr，也就是dim=2，那么索引vector的shape就是2，纵轴的shape是满足cond的数量 索引通常与tf.gather和tf.gather_nd搭配使用： tf.gather(params,indices,axis=0.name=None)：tf.gather只能接受1-D的索引，axis用来指定轴，一个索引取回对应维度的一个向量 tf.gather_nd(params,indices)：tf.gather_nd可以接受多维的索引，如果索引的dim小于params的dim，则从axis=0开始索引，后面的取全部。 tf.batch_gather(params,indices,name=None)：对张量的批量索引 8.2 tf.Print() 相当于一个节点，定义了数据的流入和流出。 一个error：在模型定义中，直接调用： 1234intra_distance = tf.Print(intra_distance, [intra_distance], message='Debug info: ', summarize=10) 会报错：AttributeError: ‘Tensor’ object has no attribute ‘_keras_history’ 参考：https://stackoverflow.com/questions/56096399/creating-model-throws-attributeerror-tensor-object-has-no-attribute-keras You cannot use backend functions directly in Keras tensors, every operation in these tensors must be a layer. You need to wrap each custom operation in a Lambda layer and provide the appropriate inputs to the layer. 之前一直没注意到这个问题，凡是调用了tf.XXX的operation，都要wrapper在Lambda层里。 改写： 123456789101112131415# wrapper functiondef debug(args): intra_distance, min_inter_distance = args intra_distance = tf.Print(intra_distance, [intra_distance], message='Debug info: ', summarize=10) min_inter_distance = tf.Print(min_inter_distance, [min_inter_distance], message='Debug info: ', summarize=10) return [intra_distance, min_inter_distance]# 模型内intra_distance, min_inter_distance = Lambda(debug)([intra_distance, min_inter_distance]) 【夹带私货】tf.Print同时也可以打印wrapper function内的中间变量，都放在列表里面就可以了。 8.3 tf.while_loop(cond, body, init_value) tensorflow中实现循环的语句 终止条件cond：是一个函数 循环体body：是一个函数 init_value：是一个list，保存循环相关参数 cond、body的参数是要与init_value列表中变量一一对应的 body返回值的格式要与init_value变量一致（tensor形状保持不变） 若非要变怎么办（有时候我们希望在while_loop的过程中，维护一个list）？动态数组TensorArray／高级参数shape_invariants 8.3.1 动态数组 12345# 定义b_boxes = tf.TensorArray(K.dtype(boxes), size=1, dynamic_size=True, clear_after_read=False)# 写入指定位置b_boxes = b_boxes.write(b, boxes_) ​ tensor array变量中一个位置只能写入一次 8.3.2 shape_invariants ​ reference stackoverflow 12345678910i = tf.constant(0)l = tf.Variable([])def body(i, l): temp = tf.gather(array,i) l = tf.concat([l, [temp]], 0) return i+1, lindex, list_vals = tf.while_loop(cond, body, [i, l], shape_invariants=[i.get_shape(), tf.TensorShape([None])]) ​ 在while_loop中显示地指定参数的shape，上面的例子用了tf.TensorShape([None])令其自动推断，而不是固定检查，因此可以解决变化长度列表。 一个完整的栗子：第一次见while_loop，在yolo_loss里面 基于batch维度做遍历 loop结束后将动态数据stack起来，重获batch dim 12345678910111213# Find ignore mask, iterate over each of batch.# extract the elements on the mask which has iou &lt; ignore_threshignore_mask = tf.TensorArray(K.dtype(y_true[0]), size=1, dynamic_size=True) # 动态size数组object_mask_bool = K.cast(object_mask, 'bool')def loop_body(b, ignore_mask): true_box = tf.boolean_mask(y_true[l][b,...,0:4], object_mask_bool[b,...,0]) # (H,W,3,5) iou = box_iou(pred_box[b], true_box) # (H,W,3,1) best_iou = K.max(iou, axis=-1) ignore_mask = ignore_mask.write(b, K.cast(best_iou&lt;ignore_thresh, K.dtype(true_box))) return b+1, ignore_mask_, ignore_mask = K.control_flow_ops.while_loop(lambda b,*args: b&lt;m, loop_body, [0, ignore_mask])ignore_mask = ignore_mask.stack()ignore_mask = K.expand_dims(ignore_mask, -1) # （N,H,W,3,1） 8.4 tf.image.non_max_suppression() 非最大值抑制：贪婪算法，按scores由大到小排序，选定第一个，依次对之后的框求iou，删除那些和选定框iou大于阈值的box。 12345# 返回是被选中边框在参数boxes中的下标位置selected_indices=tf.image.non_max_suppression(boxes, scores, max_output_size, iou_threshold=0.5, name=None)# 根据indices获取边框selected_boxes=tf.gather(boxes,selected_indices) boxes：2-D的float类型的，大小为[num_boxes,4]的张量 scores：1-D的float类型的，大小为[num_boxes]，对应的每一个box的一个score max_output_size：标量整数Tensor，输出框的最大数量 iou_threshold：浮点数，IOU阈值 selected_indices：1-D的整数张量，大小为[M]，留下来的边框下标，M小于等于max_output_size 【拓展】还有Multi-class version of NMS——tf.multiclass_non_max_suppression() 8.5 限制GPU用量 linux下查看GPU使用情况，1秒刷新一次： 1watch -n 1 nvidia-smi 指定显卡号 12import osos.environ["CUDA_VISIBLE_DEVICES"] = "2" 限制GPU用量 1234567891011121314import tensorflow as tfimport keras.backend as K# 设置百分比config = tf.ConfigProto()config.gpu_options.per_process_gpu_memory_fraction = 0.3session = tf.Session(config=config)K.set_session(session)# 设置动态申请config = tf.ConfigProto() config.gpu_options.allow_growth=True #不全部占满显存, 按需分配session = tf.Session(config=config)K.set_session(session) 8.6 tf.boolean_mask() tf.boolean_mask(tensor,mask,name=’boolean_mask’,axis=None) 其中，tensor是N维度的，mask是K维度的，$K \leq N$ axis表示mask的起始维度，被mask的维度只保留mask为True的数据，同时这部分数据flatten成一维，最终tensor的维度是N-K+1 栗子：yolov3里面，把特征图上有object的grid提取出来： 123# y_trues: [b,h,w,a,4]# conf_gt: [b,h,w,a,1]true_box = tf.boolean_mask(y_trues[i][b,...,0:4], conf_gt[b,...,0]) 8.7 tf.clip 梯度阶段，用来在optmizer之前修正梯度，主要有以下4个方法： — tf.clip_by_value — tf.clip_by_norm — tf.clip_by_global_norm — tf.clip_by_average_norm tf.clip_by_value(t, clip_value_min, clip_value_max, name=None) 输入single tensor：t t中任意大于或者小于相应阈值的value都被截断 然后返回这个截断后的t tf.clip_by_norm(t, clip_norm, axes=None, name=None) 输入single tensor：t 首先计算它的l2norm 如果l2norm(t)&gt;clip_norm，计算t * clip_norm / l2norm(t)，否则不改变t 不指定axes默认计算all dimension的l2 norm（得到一个标量），指定的话计算对应维度（对应维度的dim变为1） 返回这个修正后的t tf.clip_by_global_norm(t_list, clip_norm, use_norm=None, name=None) 输入是一组tensors：t_list，当list里面只有一个元素的时候，就退化成了tf.clip_by_norm 首先计算global norm：sqrt(sum([l2norm(t)**2 for t in t_list]))，所有梯度的l2norm的平方和的平方根 对每个t，如果l2norm(t)&gt;global_norm，计算t * clip_norm / l2norm(t)，否则不改变t 返回修正后的t_list，以及global_norm tf.clip_by_average_norm(t, clip_norm, name=None) 输入single tensor：t 首先计算它的平均L2范数：l2norm_avg，【这是个啥？】 如果l2norm_avg(t)&gt;clip_norm，计算t * clip_norm / l2norm_avg(t)，否则不改变t 返回这个修正后的t 9. keras自定义优化器optimizer9.1 关于梯度的优化器公共参数，用于梯度裁剪 clipnorm：对所有梯度进行downscale，使得梯度vector中l2范数最大为1（g * 1 / max(1, l2_norm)） clipvalue：对绝对值进行上下限截断 9.2 keras的Optimizier对象 keras的官方代码有optimizier_v1和optimizier_v2两版，分别面向tf1和tf2，v1的看起来简洁一些 self.updates &amp; self.weights self.updates：stores the variables that will be updated with every batch that is processed by the model in training 用来保存与模型训练相关的参数（iterations、params、moments、accumulators，etc） symbolic graph variable，通过K.update_add方法说明图的operation self.weights：the functions that save and load optimizers will save and load this property 用来保存与优化器相关的参数 model.save()方法中涉及include_optimizer=False，决定优化器的保存和重载 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class Optimizer(object): # - 抽象类，所有真实的优化器继承自Optimizer对象 # - 提供两个用于梯度截断的公共参数 def __init__(self, **kwargs): allowed_kwargs = &#123;'clipnorm', 'clipvalue'&#125; for k in kwargs: if k not in allowed_kwargs: raise TypeError('Unexpected keyword argument passed to optimizer: ' + str(k)) # checks that clipnorm &gt;= 0 and clipvalue &gt;= 0 if kwargs[k] &lt; 0: raise ValueError('Expected &#123;&#125; &gt;= 0, received: &#123;&#125;'.format(k, kwargs[k])) self.__dict__.update(kwargs) self.updates = [] # 计算更新的参数 self.weights = [] # 优化器带来的权重，在get_updates以后才有元素，在保存模型时会被保存 # Set this to False, indicating `apply_gradients` does not take the # `experimental_aggregate_gradients` argument. _HAS_AGGREGATE_GRAD = False def _create_all_weights(self, params): # 声明除了grads以外用于梯度更新的参数，创建内存空间，在get_updates方法中使用 raise NotImplementedError def get_updates(self, loss, params): # 定义梯度更新的计算方法, 更新self.updates raise NotImplementedError def get_config(self): # config里面是优化器相关的参数，默认只有两个梯度截断的参数，需要根据实际优化器添加（lr、decay ...） config = &#123;&#125; if hasattr(self, 'clipnorm'): config['clipnorm'] = self.clipnorm if hasattr(self, 'clipvalue'): config['clipvalue'] = self.clipvalue return config def get_gradients(self, loss, params): # 计算梯度值，并在有必要时进行梯度截断 grads = K.gradients(loss, params) if any(g is None for g in grads): raise ValueError('An operation has `None` for gradient. ' 'Please make sure that all of your ops have a ' 'gradient defined (i.e. are differentiable). ' 'Common ops without gradient: ' 'K.argmax, K.round, K.eval.') if hasattr(self, 'clipnorm'): grads = [tf.clip_by_norm(g, self.clipnorm) for g in grads] if hasattr(self, 'clipvalue'): grads = [ tf.clip_by_value(g, -self.clipvalue, self.clipvalue) for g in grads ] return grads def set_weights(self, weights): # 给optimizer的weights用一系列np array赋值 # 没看到有调用，省略code： K.batch_set_value() def get_weights(self): # 获取weights的np array值 # 没看到有调用，省略code： K.batch_get_value() @classmethod def from_config(cls, config): return cls(**config) 9.3 实例化一个优化器 based on keras.Optimizer对象 主要需要重写get_updates和get_config方法 get_updates用来定义梯度更新的计算方法 get_config用来定义实例用到的参数 以SoftSGD为例： 每隔一定的batch才更新一次参数，不更新梯度的step梯度不清空，执行累加，从而实现batchsize的变相扩大 建议搭配间隔更新参数的BN层来使用，否则BN还是基于小batchsize来更新均值和方差 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class SoftSGD(Optimizer): # [new arg] steps_per_update: how many batch to update gradient def __init__(self, lr=0.01, momentum=0., decay=0., nesterov=False, steps_per_update=2, **kwargs): super(SoftSGD, self).__init__(**kwargs) with K.name_scope(self.__class__.__name__): self.iterations = K.variable(0, dtype='int64', name='iterations') self.lr = K.variable(lr, name='lr') self.steps_per_update = steps_per_update # 多少batch才更新一次 self.momentum = K.variable(momentum, name='momentum') self.decay = K.variable(decay, name='decay') self.initial_decay = decay self.nesterov = nesterov def get_updates(self, loss, params): # learning rate decay lr = self.lr if self.initial_decay &gt; 0: lr = lr * (1. / (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay)))) shapes = [K.int_shape(p) for p in params] sum_grads = [K.zeros(shape) for shape in shapes] # 平均梯度，用来梯度下降 grads = self.get_gradients(loss, params) # 当前batch梯度 self.updates = [K.update_add(self.iterations, 1)] self.weights = [self.iterations] + sum_grads for p, g, sg in zip(params, grads, sum_grads): # momentum 梯度下降 v = self.momentum * sg / float(self.steps_per_update) - lr * g # velocity if self.nesterov: new_p = p + self.momentum * v - lr * sg / float(self.steps_per_update) else: new_p = p + v # 如果有约束，对参数加上约束 if getattr(p, 'constraint', None) is not None: new_p = p.constraint(new_p) # 满足条件才更新参数 cond = K.equal(self.iterations % self.steps_per_update, 0) self.updates.append(K.switch(cond, K.update(p, new_p), p)) self.updates.append(K.switch(cond, K.update(sg, g), K.update(sg, sg + g))) return self.updates def get_config(self): config = &#123;'lr': float(K.get_value(self.lr)), 'steps_per_update': self.steps_per_update, 'momentum': float(K.get_value(self.momentum)), 'decay': float(K.get_value(self.decay)), 'nesterov': self.nesterov &#125; base_config = super(SoftSGD, self).get_config() return dict(list(base_config.items()) + list(config.items())) 10. keras自定义激活函数activation10.1 定义激活函数 123def gelu(x): cdf = 0.5 * (1.0 + tf.erf(x / tf.sqrt(2.0))) return x*cdf 10.2 使用自定义激活函数 使用Activation方法 1x = Activation(gelu)(x) 不能整合进带有activation参数的层（如Conv2D），因为Conv基类的get_config()方法从keras.activations里面读取相应的激活函数，其中带参数的激活函数如PReLU（Advanced activations）、以及自定义的激活函数都不在这个字典中，否则会报错： AttributeError: ‘Activation’ object has no attribute ‘name‘ 10.3 checkpoint issue 网上还有另一种写法： 1234567891011from keras.layers import Activationfrom keras.utils.generic_utils import get_custom_objectsdef gelu(x): cdf = 0.5 * (1.0 + tf.erf(x / tf.sqrt(2.0))) return x*cdf get_custom_objects().update(&#123;'gelu': Activation(gelu)&#125;)# 后面可以通过名字调用激活函数x = Activation('gelu')(x) 这种写法在使用ModelCheckpoints方法保存权重时会报错： AttributeError: ‘Activation’ object has no attribute ‘name‘ 看log发现当使用名字代表激活层的时候，在保存模型的时候，又会有一个get_config()函数从keras.activations中查表 11. keras自定义正则化器regularizers11.1 使用封装好的regularizers keras的正则化器没有global的一键添加方法，要layer-wise为每一层添加 keras的层share 3 common参数接口： kernel_regularizer bias_regularizer activity_regularizer 可选用的正则化器 keras.regularizers.l1(0.01) keras.regularizers.l2(0.01) keras.regularizers.l1_l2(l1=0.01, l2=0.01) 使用 12345678layer = tf.keras.layers.Dense(5, kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l1(0.01), activity_regularizer=tf.keras.regularizers.l2(0.01))tensor = tf.ones(shape=(5, 5)) * 2.0out = layer(tensor)# The kernel regularization term is 0.25# The activity regularization term (after dividing by the batch size) is 5print(tf.math.reduce_sum(layer.losses)) # 5.25 (= 5 + 0.25) 11.2 custom regularizer 一般不会自定义这个东西，硬要custom的话，两种方式 简单版，接口参数是weight_matrix，无额外参数，层直接调用 1234def my_regularizer(x): return 1e-3 * tf.reduce_sum(tf.square(x))layer = tf.keras.layers.Dense(5, kernel_initializer='ones', kernel_regularizer=my_regularizer) 子类继承版，可以加额外参数，需要补充get_config方法，支持读写权重时的串行化 123456789101112class MyRegularizer(regularizers.Regularizer): def __init__(self, strength): self.strength = strength def __call__(self, x): return self.strength * tf.reduce_sum(tf.square(x)) def get_config(self): return &#123;'strength': self.strength&#125;layer = tf.keras.layers.Dense(5, kernel_initializer='ones', kernel_regularizer=MyRegularizer(0.01)) 11.3 强行global 每层加起来太烦了，批量加的实质也是逐层加，只不过写成循环 核心是layer的add_loss方法 12345678model = keras.applications.ResNet50(include_top=True, weights='imagenet')alpha = 0.00002 # weight decay coefficientfor layer in model.layers: if isinstance(layer, keras.layers.Conv2D) or isinstance(layer, keras.layers.Dense): layer.add_loss(lambda: keras.regularizers.l2(alpha)(layer.kernel)) if hasattr(layer, 'bias_regularizer') and layer.use_bias: layer.add_loss(lambda: keras.regularizers.l2(alpha)(layer.bias)) 12. keras查看梯度&amp;权重12.1 easiest way 查看梯度最简单的方法：通过K.gradients方法定义一个求梯度的func，然后给定输入，得到梯度（CAM就是这么干的） 查看权重最简单的方法：存在h5文件，然后花式h5py解析 12.2 dig deeper 一个思路：将梯度保存在optimizer的self.weights中，并在model.save得到的模型中解析 13. keras实现权重滑动平均13.1 why EMA on weights [reference1][https://www.jiqizhixin.com/articles/2019-05-07-18]：权重滑动平均是提供训练稳定性的有效方法，要么在优化器里面实现，要么外嵌在训练代码里 [reference2][https://cloud.tencent.com/developer/article/1636781]：这里面举的例子很清晰了，就是为了权重每个step前后变化不大 权重EMA的计算方式有点类似于BN的running mean&amp;var： 在训练阶段：它不改变每个training step的优化方向，而是从initial weights开始，另外维护一组shadow weights，用每次的updating weights来进行滑动更新 在inference阶段，我们要用shadow weights来替换当前权重文件保存的weights（current step下计算的新权重） 如果要继续训练，要将替换的权重在换回来，因为【EMA不影响模型的优化轨迹】 13.2 who uses EMA 很多GAN的论文都用了EMA， 还有NLP阅读理解模型QANet， 还有Google的efficientNet、resnet_rs 13.3 how to implement outside 12345678910111213141516171819202122232425262728293031class ExponentialMovingAverage: """对模型权重进行指数滑动平均。 用法：在model.compile之后、第一次训练之前使用； 先初始化对象，然后执行inject方法。 """ def __init__(self, model, momentum=0.9999): self.momentum = momentum self.model = model self.ema_weights = [K.zeros(K.shape(w)) for w in model.weights] def inject(self): """添加更新算子到model.metrics_updates。 """ self.initialize() for w1, w2 in zip(self.ema_weights, self.model.weights): op = K.moving_average_update(w1, w2, self.momentum) self.model.metrics_updates.append(op) def initialize(self): """ema_weights初始化跟原模型初始化一致。 """ self.old_weights = K.batch_get_value(self.model.weights) K.batch_set_value(zip(self.ema_weights, self.old_weights)) def apply_ema_weights(self): """备份原模型权重，然后将平均权重应用到模型上去。 """ self.old_weights = K.batch_get_value(self.model.weights) ema_weights = K.batch_get_value(self.ema_weights) K.batch_set_value(zip(self.model.weights, ema_weights)) def reset_old_weights(self): """恢复模型到旧权重。 """ K.batch_set_value(zip(self.model.weights, self.old_weights)) then train 1234EMAer = ExponentialMovingAverage(model) # 在模型compile之后执行EMAer.inject() # 在模型compile之后执行model.fit(x_train, y_train) # 训练模型 then inference 12345MAer.apply_ema_weights() # 将EMA的权重应用到模型中model.predict(x_test) # 进行预测、验证、保存等操作EMAer.reset_old_weights() # 继续训练之前，要恢复模型旧权重。还是那句话，EMA不影响模型的优化轨迹。model.fit(x_train, y_train) # 继续训练 14. keras的Model类继承14.1 定义模型的方式 Sequential：最简单，但是不能表示复杂拓扑结构 函数式 API：和Sequential用法基本一致，输入张量和输出张量用于定义 tf.keras.Model实例 模型子类化：引入于 Keras 2.2.0 keras源代码定义在：https://github.com/keras-team/keras/blob/master/keras/engine/training.py 14.2 模型子类化overview 既可以用来定义一个model，也可以用来定义一个复杂的网络层，为实现复杂模型提供更大的灵活性 有点类似于torch的语法 网络层定义在 __init__(self, ...) 中：跟torch语法的主要区别在于层不能复用，torch同一个层在forward中每调用一次能够创建一个实例，keras每个层应该是在init中声明并创建，所以不能复用 前向传播在 call(self, inputs) 中，这里面也可以添加loss compute_output_shape计算模型输出的形状 和keras自定义层的语法也很相似 build(input_shape)：主要区别就在于build，因为自定义层有build，显式声明了数据流的shape，能够构造出静态图 call(x)： compute_output_shape(input_shape)： 【以下方法和属性不适用于类继承模型】，所以还是推荐优先使用函数式 API model.inputs &amp; model.outputs model.to_yaml() &amp; model.to_json() model.get_config() &amp; model.save()：！！！只能save_weights！！！ 14.3 栗子 12345678910111213141516171819202122232425262728293031import kerasimport numpy as npclass SimpleMLP(keras.Model): def __init__(self, num_classes=10): super(SimpleMLP, self).__init__(name='mlp') self.num_classes = num_classes self.dense1 = keras.layers.Dense(32, activation='relu') self.dense2 = keras.layers.Dense(num_classes, activation='softmax') self.dp = keras.layers.Dropout(0.5) self.bn = keras.layers.BatchNormalization(axis=-1) def call(self, inputs, training=None, mask=None): x = self.dense1(inputs) x = self.dp(x) x = self.bn(x, training=training) return self.dense2(x) def compute_output_shape(self, input_shape): batch, dim = input_shape return (batch, self.num_classes)model = SimpleMLP()model.compile('adam', loss='categorical_crossentropy')x = np.random.uniform(0,1,(32,100))y = np.random.randint(0, 2, (32,10))model.fit(x, y)model.summary() 可以看到，类继承模型是没有指明input_shape的，所以也就不存在静态图，要在有真正数据流以后，model才被build，才能够调用summay方法，查看图结构 第二个是，call方法的默认参数：def call(self, inputs, training=None, mask=None)， 子类继承模型不支持显式的多输入定义，所有的输入构成inputs 需要手工管理training参数，bn/dropout等在train/inference mode下计算不一样的情况，要显式传入training参数 mask在构建Attention机制或者序列模型时会使用到，如果previous layer生成了掩码（embedding的mask_zero参数为True），前两种构建模型的方法中，mask会自动传入当前层的call方法中 14.4 get layer output 有时候我们用Model类去定义一个层，以便获得更自由的表达能力，但是在inference阶段用get_layer(xxx).output去获取这个层的输出的时候会报错：multi-node… get_output_at… 说这个层含有多个输出节点，需要用get_output_at来声明 get_output_at(0)是当前模型level的节点 get_output_at(1)是当前模型作为层对象的输出节点 这俩本质是一样的，但是Model类就是会创建这样的副本，这可能就是目前keras复现版本的显存占用要高于pytorch版的原因 15. low-level training &amp; evaluation loops15.1 keras的Model类提供了build-in的train/eval方法 * fit() * evaluate() * predict() * reference: https://keras.io/api/models/model_training_apis/ * reference: https://keras.io/guides/training_with_built_in_methods/ 15.2 如果你想修改模型的训练过程，但仍旧通过fit()方法进行训练，Model类中提供了train_step()可以继承和重载 reference: https://keras.io/guides/customizing_what_happens_in_fit/ Model类中有一个train_step()方法，fit每个batch的时候都会调用一次 在重写这个train_step()方法时 传入参数data：取决于fit()方法传入的参数形式，tuple(x,y) / tf.data.Dataset forward pass：self(model) 计算loss：self.compiled_loss 计算梯度：tf.GradientTape() 更新权重：self.optimizer 更新metrics：self.compiled_metrics 返回值a dictionary mapping metric names 栗子🌰 12345678910111213141516171819202122232425262728293031323334class CustomModel(keras.Model): def train_step(self, data): # Unpack the data x, y = data with tf.GradientTape() as tape: y_pred = self(x, training=True) # Forward pass # Compute the loss value # (the loss function is configured in `compile()`) loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses) # Compute gradients trainable_vars = self.trainable_variables gradients = tape.gradient(loss, trainable_vars) # Update weights self.optimizer.apply_gradients(zip(gradients, trainable_vars)) # Update metrics (includes the metric that tracks the loss) self.compiled_metrics.update_state(y, y_pred) # Return a dict mapping metric names to current value return &#123;m.name: m.result() for m in self.metrics&#125;import numpy as np# Construct and compile an instance of CustomModelinputs = keras.Input(shape=(32,))outputs = keras.layers.Dense(1)(inputs)model = CustomModel(inputs, outputs)model.compile(optimizer="adam", loss="mse", metrics=["mae"])# Just use `fit` as usualx = np.random.random((1000, 32))y = np.random.random((1000, 1))model.fit(x, y, epochs=3) * 注意到这里面我们调用了self.compiled_loss和self.compiled_metrics，这就是在调用compile()方法的时候传入的loss和metrics参数 get lower loss和metrics也可以不传，直接在CustomModel里面声明和定义 声明：重载metrics()方法，创建metric instances，用于计算loss和metrics，把他们放在这里模型会在fit()/evaluate()方法的每个epoch起始阶段调用reset_states()方法，确保loss和metrics的states都是per epoch的，而不是avg from the beginning 更新：调用update_state()方法更新他们的状态参数，调用result()方法拿到他们的current value 12345678910111213141516171819202122232425262728293031323334353637383940414243loss_tracker = keras.metrics.Mean(name="loss")mae_metric = keras.metrics.MeanAbsoluteError(name="mae")class CustomModel(keras.Model): def train_step(self, data): x, y = data with tf.GradientTape() as tape: y_pred = self(x, training=True) # Forward pass # Compute our own loss loss = keras.losses.mean_squared_error(y, y_pred) # Compute gradients trainable_vars = self.trainable_variables gradients = tape.gradient(loss, trainable_vars) # Update weights self.optimizer.apply_gradients(zip(gradients, trainable_vars)) # Compute our own metrics loss_tracker.update_state(loss) mae_metric.update_state(y, y_pred) return &#123;"loss": loss_tracker.result(), "mae": mae_metric.result()&#125; @property def metrics(self): # We list our `Metric` objects here so that `reset_states()` can be called automatically per epoch return [loss_tracker, mae_metric]# Construct an instance of CustomModelinputs = keras.Input(shape=(32,))outputs = keras.layers.Dense(1)(inputs)model = CustomModel(inputs, outputs)# We don't passs a loss or metrics here.model.compile(optimizer="adam")# Just use `fit` as usual -- you can use callbacks, etc.x = np.random.random((1000, 32))y = np.random.random((1000, 1))model.fit(x, y, epochs=5) 相对应地，也可以定制model.evaluate()的计算过程——override test_step()方法 传入参数data：取决于fit()方法传入的参数形式，tuple(x,y) / tf.data.Dataset forward pass：self(model) 计算loss：self.compiled_loss 计算metrics：self.compiled_metrics 返回值a dictionary mapping metric names 12345678910111213141516171819202122232425class CustomModel(keras.Model): def test_step(self, data): # Unpack the data x, y = data # Compute predictions y_pred = self(x, training=False) # Updates the metrics tracking the loss self.compiled_loss(y, y_pred, regularization_losses=self.losses) # Update the metrics. self.compiled_metrics.update_state(y, y_pred) # Return a dict mapping metric names to current value. # Note that it will include the loss (tracked in self.metrics). return &#123;m.name: m.result() for m in self.metrics&#125;# Construct an instance of CustomModelinputs = keras.Input(shape=(32,))outputs = keras.layers.Dense(1)(inputs)model = CustomModel(inputs, outputs)model.compile(loss="mse", metrics=["mae"])# Evaluate with our custom test_stepx = np.random.random((1000, 32))y = np.random.random((1000, 1))model.evaluate(x, y) 15.3 实现完整的train loops reference: https://keras.io/guides/writing_a_training_loop_from_scratch/ a train loop a for loop：iter for each epoch a for loop：iter over the dataset * open a `GradientTape()` scope：tensorflow的梯度API，用于给定loss计算梯度 * Inside this scope：forward pass，compute loss * Outside the scope：retrieve the gradients * use optimizer to update the gradients：`optimizer.apply_gradients`，使用计算得到的梯度来更新对应的variable 栗子🌰 12345678910111213141516171819202122232425262728293031323334353637383940# modelmodel = keras.Model(inputs=inputs, outputs=outputs)optimizer = keras.optimizers.SGD(learning_rate=1e-3)loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)# datatrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)# iter epochsepochs = 2for epoch in range(epochs): print("\nStart of epoch %d" % (epoch,)) # iter batches for step, (x_batch_train, y_batch_train) in enumerate(train_dataset): # Open a GradientTape to record the operations run # during the forward pass, which enables auto-differentiation. with tf.GradientTape() as tape: # Run the forward pass of the layer. logits = model(x_batch_train, training=True) # Logits for this minibatch # Compute the loss value for this minibatch. loss_value = loss_fn(y_batch_train, logits) # automatically retrieve the gradients of the trainable variables with respect to the loss grads = tape.gradient(loss_value, model.trainable_weights) # Run one step of gradient descent by updating the value of the variables to minimize the loss. optimizer.apply_gradients(zip(grads, model.trainable_weights)) # Log every 200 batches. if step % 200 == 0: print( "Training loss (for one batch) at step %d: %.4f" % (step, float(loss_value)) ) print("Seen so far: %s samples" % ((step + 1) * 64)) 16. keras自定义初始化initializers16.1 基类：keras.initializers.Initializer()，用于给层参数kernel_initializer和bias_initializer传入初始化方法 内置初始化器： keras.initializers.Zeros()：全0 keras.initializers.Ones()：全1 keras.initializers.Constant(value=0)：全赋值为指定常量 keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)：正态分布 keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)：均匀分布 keras.initializers.VarianceScaling(scale=1.0, mode=’fan_in’, distribution=’normal’, seed=None)：根据层属性决定分布的参数，如果distribution=’normal’，那么正态分布的stddev=sqrt(scale/n)，如果distribution=”uniform”，那么均匀分布的limit=sqrt(3 * scale/n)，n决定于mode，可以是层输入单元数/输出单元数/两者平均 keras.initializers.Orthogonal(gain=1.0, seed=None)：随机正交矩阵 keras.initializers.Identity(gain=1.0)：单位矩阵 keras.initializers.glorot_normal(seed=None)： Xavier 正态分布，0为中心，stddev=sqrt(2 / (fan_in + fan_out)) keras.initializers.glorot_uniform(seed=None)： Xavier 均匀分布，limit=sqrt(6 / (fan_in + fan_out)) keras.initializers.he_normal(seed=None)：Kaiming正态分布，0为中心，stddev=sqrt(2 / fan_in) keras.initializers.he_uniform(seed=None)：Kaiming均匀分布，limit=sqrt(6/fan_in) 传入： 123456789101112131415161718# 用名字字符串model.add(Dense(64,kernel_initializer='random_uniform', bias_initializer='zeros'))# 用初始化器的实例model.add(Dense(64,kernel_initializer=keras.initializers.Constant(value=3.0), bias_initializer=keras.initializers.RandomNormal(mean=0.,stddev=.05)))# use config_dict，这个是在efficientDet源码里面看到的DENSE_KERNEL_INITIALIZER = &#123; 'class_name': 'VarianceScaling', 'config': &#123; 'scale': 1. / 3., 'mode': 'fan_out', 'distribution': 'uniform' &#125;&#125;x = Dense(n_classes, activation='softmax', kernel_initializer=DENSE_KERNEL_INITIALIZER)(x) 16.2 自定义初始化器： 必须使用参数shape和dtype 123456from keras import backend as Kdef my_init(shape, dtype=None): return K.random_normal(shape, dtype=dtype)model.add(Dense(64, kernel_initializer=my_init)) 17. keras的model.save() &amp; model.save_weights()17.1 首先明确包含内容 model.save() 模型结构，能够基于这个信息rebuild模型 模型权重 训练设置（loss，metric，optmizer），也就是compile的信息 优化器状态，to exactly resume the training state model.save_weights() 只有模型权重 17.2 一些特殊情况 使用类继承模型时，因为没有model.get_config方法，所以没法调用model.save()，只能save_weights，如果同时也想保存优化器状态，需要手动保存： Optimizer类的get_weights()方法：Returns the current weights of the optimizer as a list of Numpy arrays Optimizer类的set_weights()方法：Pass a list of Numpy arrays to set the new state of the optimizer 18. keras logger18.1 主要有如下方式 * 原生：fit /fit_generator 都会返回一个history(Callback)对象，这里面记录了loss &amp; metrics (train &amp; val) * nohup log：保存打印日志，print什么就记录什么 * Logger：同上 18.2 查看 123456789history = model.fit_generator(train_gen, steps_per_epoch=32, epochs=3, validation_data=val_gen, validation_steps=8,)print(history.history)print(history.epoch)print(history.history['val_loss']) 18.3 保存：就是保存字典 123456import picklewith open('xxx_hist.pkl', 'wb') as f: pickle.dump(history.history, f)with open('xxx_hist.pkl', 'rb') as f: history = pickle.load(f) 19. keras 分层学习率19.1 包装层，添加层参数 19.2 写在optmizer里面]]></content>
      <tags>
        <tag>keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv库函数]]></title>
    <url>%2F2019%2F08%2F11%2Fopencv%E5%BA%93%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[1. imshow有时候imshow的图片会显示的和原图不一样，要查看read进来的数据格式，imshow会根据读入的数据格式自动进行归一化，映射到0-255。 如果image是默认的8-bit unsigned（0-255），不做处理。 如果image是16-bit unsigned（0-65535）或者32-bit integer（？？贼大），像素值除以256，[0,255*256]归一化到[0，255]。 如果image是32-bit float，像素值乘以255，[0,1]归一化到[0，255]。 2. imwrite通常imwrite把所有数据都强制转换成uchar（0-255）。]]></content>
  </entry>
  <entry>
    <title><![CDATA[beautifulsoup saving file]]></title>
    <url>%2F2019%2F06%2F11%2Fbeautifulsoup-saving-file%2F</url>
    <content type="text"><![CDATA[最近用bs4处理xml文件，遇到了一个在爬虫时候从未思考过的问题—— 修正从xml文件中解析出的文件树，并将changes保存到原来的xml文件中。 我一直在beautifulsoup的手册中去寻找库函数，实际只需要简单的文件读写操作： 123456789from bs4 import BeautifulSoupsoup = BeautifulSoup(open('test.xml'), 'xml')add = BeautifulSoup("&lt;a&gt;Foo&lt;/a&gt;", 'xml')soup.orderlist.append(add)print(soup.prettify())f = open('test.xml', 'w')f.write(str(soup))f.close() 附一个简单xml文件用来实验： 1234567891011121314&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;orderlist&gt;&lt;order&gt;&lt;customer&gt;姓名1&lt;/customer&gt;&lt;phone&gt;电话1&lt;/phone&gt;&lt;address&gt;地址1&lt;/address&gt;&lt;count&gt;点餐次数1&lt;/count&gt;&lt;/order&gt;&lt;order&gt;&lt;customer&gt;姓名2&lt;/customer&gt;&lt;phone&gt;电话2&lt;/phone&gt;&lt;address&gt;地址2&lt;/address&gt;&lt;count&gt;点餐次数2&lt;/count&gt;&lt;/order&gt;]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh visualization]]></title>
    <url>%2F2018%2F12%2F29%2Fssh-visualization%2F</url>
    <content type="text"><![CDATA[ssh boocax@192.168.1.100 密码：robot123 echo $ROS_MASTER_URI 查看端口号11311 小车端： export ROS_MASTER_URI=http://192.168.1.100:11311 export ROS_IP=192.168.1.100 虚拟机端： export ROS_MASTER_URI=http://192.168.1.100:11311 export ROS_IP=172.16.128.142 # nav远程开启三个终端（代码重构以前）： move_base: roslaunch teleop_twist_joy real_nav.launch mapserver: rosrun map_server map_server catkin_ws2/src/patrol/map/p1.yaml amcl: roslaunch patrol real_loc.launch 本地可视化：rviz／rqt_graph / rosservice call /rostopic pub 全局定位： 1rosservice call /global_localization "&#123;&#125;" 设置导航目标点： 12345// 相对base_link坐标系rostopic pub -1 /navigation_simple/goal geometry_msgs/PoseStamped '&#123; header: &#123; frame_id: "base_link" &#125;, pose: &#123; position: &#123; x: 0.5, y: 0.0, z: 0 &#125;, orientation: &#123; x: 0, y: 0, z: 0, w: 1 &#125; &#125; &#125;'// 相对map坐标系rostopic pub -1 /navigation_simple/goal geometry_msgs/PoseStamped '&#123; header: &#123; frame_id: "map" &#125;, pose: &#123; position: &#123; x: 5, y: 0.0, z: 0 &#125;, orientation: &#123; x: 0, y: 0, z: 0, w: 1 &#125; &#125; &#125;' 注意-1，否则循环发布。 # 往回备份： 1scp -r boocax@192.168.1.100:/home/boocax/catkin_ws2019 bkp/]]></content>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Occupancy Grid Map]]></title>
    <url>%2F2018%2F12%2F04%2FOccupancy-Grid-Map%2F</url>
    <content type="text"><![CDATA[to be completed… Inverse Sensor Model Incremental Updating]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[object tracking]]></title>
    <url>%2F2018%2F12%2F03%2Fobject-tracking%2F</url>
    <content type="text"><![CDATA[范围限定：过滤掉较远范围的点云数据 聚类：K-Means／欧式聚类，因为前者需要设定K，故使用后者。 D(p_i, p_{i+1}) = \sqrt{r_i^2 + r_{i+1}^2 - 2r_ir_{i+1}cos(\varphi_{i+1} - \varphi_i)}如果连续扫描点之间的距离小于一个阈值$D_t$，那么这两个点被认为属于同一个对象。这个阈值是根据当前参考点的距离动态调整的。 D_t = D_0 + a*r_i*sin(\Delta \varphi) 运动目标特征提取：（中心坐标，长／宽／半径，反射强度） 由上一时刻的位置速度设置ROI： 基于局部匹配：通过相似度计算选取响应值最高的目标 基于分类器：动态目标已知（人腿），采集正负样本，构造分类器， 卡尔曼滤波：]]></content>
      <tags>
        <tag>extensions for slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib的colormap]]></title>
    <url>%2F2018%2F11%2F29%2Fmatplotlib%E7%9A%84colormap%2F</url>
    <content type="text"><![CDATA[用plt的imshow画图，总是找不到心仪的colorbar，可以自定义： 在原有cmap基础上自定义： 12345colorbar = plt.get_cmap('Greys')(range(180))cm = LinearSegmentedColormap.from_list(name="grey_cm", colors=colorbar)plt.register_cmap(cmap=cm)plt.imshow(map2d.data, cmap='grey_cm') define一个新的cmap： 123456def colormap(): colors = ['#FFFFFF', '#9ff113', '#5fbb44', '#f5f329', '#e50b32'] return colors.ListedColormap(colors, 'my_cmap')my_cmap = colormap()plt.imshow(map2d.data, cmap=my_cmap)]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mpv-video-cutter]]></title>
    <url>%2F2018%2F11%2F26%2Fmpv-video-cutter%2F</url>
    <content type="text"><![CDATA[mpv的小插件，能够一键（三键）剪辑。 工程地址：https://github.com/rushmj/mpv-video-cutter step1：把c_concat.sh和cutter.lua两个文件复制到~/.config/mpv/scripts/目录下。 step2：给c_concat.sh脚本添加执行权限。 step3：用命令行打开文件，c-c-o在原目录下生成剪辑文件。]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[problems with ROS]]></title>
    <url>%2F2018%2F11%2F22%2Fproblems-with-ROS%2F</url>
    <content type="text"><![CDATA[[WARN] Detected jump back in time of 5.51266s. Clearing TF buffer. 手动建图的时候，时不时的就跳出来这个，然后小车跳变到初始位置，而且还是根据TF buffer回溯回去的，真高级。。。 排查原因发现竟然是忘记运行roscore了，mmp。 [rosrun] Couldn&#39;t find executable named patrol.py below /home/carrol/catkin_ws/src/patrol 原因如提示，python是脚本执行，要添加可执行权限。 error: ‘array’ is not a member of ‘std’ 编译导航包时反复出现这个错误，因为cmake版本比较低（2.8），不会自动找c++11，解决办法在对应package的cmake文件中添加c++声明：add_definitions(-std=c++11) 同样的错误catkin_make时重复出现，我还以为问题没解决： 删除build文件夹中对应包，再进行catkin_make。如果删除了某个包，还要删除devel文件夹再编译。 cmake warning conflicts with Anaconda： 编译到最后会卡死，错误具体啥意思我也没弄明白，粗暴解决了，将系统环境变量里面的anaconda path暂时屏蔽，首先查看环境变量：echo $PATH，然后返回结果： /home/[username]/anaconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games 然后在当前命令行执行：export PATH=&quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games&quot; c++: internal compiler error: Killed (program cc1plus) 虚拟机内存不足。 undefined error with CONSOLE_BRIDGE_logError/CONSOLE_BRIDGE_logWarn 安装并编译console_bridge包，注意build instructions： 12345git clone git://github.com/ros/console_bridge.gitcd console_bridgecmake .makesudo make install there are no arguments to ‘logDebug’ that depend on a template parameter, so a declaration of ‘logDebug’ must be available [-fpermissive] 参考（Reference），还是上面的问题， console_bridge的API变了，将logDebug改成CONSOLE_BRIDGE_logDebug就行了。 running environment相关包的缺失和安装： 在官网查找相关包和依赖，然后执行： 12345# installsudo dpkg -i 软件包名.deb# uninstallsudo apt-get remove 软件包名称]]></content>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[amcl]]></title>
    <url>%2F2018%2F11%2F16%2Famcl%2F</url>
    <content type="text"><![CDATA[滤波：机器人从已知点$x_0$开始运动，里程计误差逐渐累积，位置不确定性将越来越大（$x_1, x_2$）。因此需要借助外部环境信息对自己进行定位，于是引入测量值$d$，计算出当前位置$x_2^{‘}$，再结合预测值$x_2$，得到一个矫正位置$x_2^{‘’}$，使其不确定性降到最小。 贝叶斯滤波：$p(x|z) = \frac{p(z|x)p(x)}{p(z)}$先验：$p(x_t|u_t, x_{t-1})$，通过预测方程得到 似然：$p(z_t| x_t)$，通过测量方程得到 后验：$p(x_t|z_t)$，通过贝叶斯方程得到 对于一般的非线性、非高斯系统，很难通过上述方法得到后验概率的解析解。 蒙特卡洛采样：假设能从一个目标分布$p(x)$获得一系列样本$x_1, x2, …, x_N$，那么就能利用这些样本去估计这个分布的某些函数的期望值。 E(f(x)) = \int_a^{b}f(x)p(x)dx \approx\frac{f(x_1) + f(x_2) + ... + f(x_N)}{N}蒙特卡洛采样的核心思想就是用均值来代替积分。 假设可以从后验概率中采样到N个样本，那么后验概率可以表示为： \hat p(x_t|z_{1:t}) = \frac{1}{N} \sum_{i=1}^{N}\delta(x_n - x_n^{i}) \approx p(x_t|z_{1:t})粒子滤波： \begin{align} E(f(x)) & \approx \int f(x_n) \hat p(x_t|z_{1:t}) dx \nonumber\\ & = \frac{1}{N}\sum_N f(x_n) \delta(x_n - x_n^{i})\nonumber\\ & = \frac{1}{N}\sum_N f(x_n^{i}) \nonumber \end{align}用采样粒子（服从后验概率）的状态值直接平均作为期望值，这就是粒子滤波。 MCL：蒙特卡洛定位／粒子滤波定位 Randomly generate a bunch of particles Predict next state of the particles Update the weighting of the particles based on the measurement. Resample：Discard highly improbable particle and replace them with copies of the more probable particles. This leads to a new particle set with uniform importance weights, but with an increased number of particles near the three likely places. Compute the weighted mean and covariance of the set of particles to get a state estimate. 权值退化：如果任由粒子权值增长，只有少数粒子的权值较大，其余粒子的权值可以忽略不计，变成无效粒子，因此需要引入重采样。采用$N_{eff}$衡量粒子权值的退化程度。 N_{eff} \approx \hat{N_{eff}} = \frac{1}{\sum_N (w_k^{i})^2}粒子多样性：通常我们会舍弃权值较小的粒子，代之以权值较大的粒子。这样会导致权值小的粒子逐渐绝种，粒子群多样性减弱，从而不足以近似表征后验密度。 重要性采样实际上后验概率并不知道，谈何采样（$x_n^i$）。我们可以从一个已知的分布$q(x|z)$里来采样，间接得到滤波值。 \begin{align} E(f(x_k))& = \int f(x) \frac{p(x|z)}{q(x|z)} q(x|z) dx \nonumber\\ & = \frac{E_{q(x|z)}W_k(x_k)f(x_k)}{E_{q(x|z)}W_k(x_k)}\nonumber\\ & \approx \frac{\frac{1}{N} \sum_{i=1}^N W_k({x_k^{i})f(x_k^{i})}}{\frac{1}{N} \sum_{i=1}^N W_k({x_k^{i}})}\nonumber\\ & = \sum_N \hat W_k(x_k^i)f(x_k^i) \nonumber \end{align}相比较于原始的均值表示，变成了加权平均值。不同粒子拥有了不同的权重。 \hat W_k(x_k^i) = \frac{W_k(x_k^i)}{\sum_N W_k(x_k^i)}\\ W_k (x_k) \propto \frac{p(x_k|z_{1:k})}{q(x_k|z_{1:k})}已知的$q$分布叫做重要性概率密度函数。 递推算法：序贯重要性采样 \{x_k^i, w_k^i\} = SIS(\{x_{k-1}, w_{k-1}\})_{i=1}^N, y_k)首先假设重要性分布$q(x|z)$满足： q(x_k | x_{0:k-1}, y_{1:k}) = q(x_k|x_{k-1}, y_k)即只和前一时刻的状态$x_{k-1}$和测量$y_k$有关。于是有： w_k^i \approx w_{k-1}^i \frac{p(y_k|x_k^i)p(x_k^i|x_{k-1}^i)}{q(x_k^i|x_{k-1}^i, y_k)}伪代码： 1234567For i=1:N (1)采样：式1 (2)权值更新：式2End For权值归一化加权平均得到粒子滤波值，也就是当前状态的估计值重采样 重采样既然权重小的那些粒子不起作用了，那就不要了。为了保持粒子数目不变，就要补充新粒子，最简单的办法就是复制权重大的粒子。用$x_k^i$表示k时刻的粒子，$x_k^j$表示重采样以后的粒子，那么： \tilde p (x_k|y_{1:k}) = \sum_N \frac{1}{N}\delta(x_k - x_k^j) = \sum_N \frac{n_i}{N}\delta(x_k - x_i^j)总的来说，新粒子按照权重比例来补充，算法流程为： 12345678计算概率累积和wcum(N)用[0,1]之间的均匀分布随机采样N个值u(N)for i in 1:N: k = 1 while u(i)&lt;wcum(k): k += 1 end while resample(i) = k SIR滤波器（Sampling Importance Resampling Filter ）选取特定的重要性概率密度函数： q(x_k^i|x_{k-1}^i, y_k) = p(x_k^i|x_{k-1}^i)于是权重更新公式可以简化： w_k^i \propto w_{k-1}^i \frac{p(z|x)p(x|x_{k-1})}{q(x|x_{k-1})}由于重采样以后，粒子分布更新，权值统一为$\frac{1}{N}$，于是权重更新公式进一步简化： w_k^i \propto p(z_k|x_k^i)根据测量方程可知，上面这个概率就是以真实测量值为均值，以噪声方差为方差的高斯分布。 此算法中的采样，并没有加入测量$z_k$，只凭先验知识$p(x_k|x_{k-1})$，虽然简单易用，但是存在效率不高和对奇异点(outliers)敏感的问题。 AMCLMCL算法能够用于全局定位，但是无法从机器人绑架或全局定位失败中恢复过来，因为随着位置被获取，其他地方的不正确粒子会逐渐消失。稳定状态下，粒子只“生存”在一个单一的姿态附近，如果这个姿态恰好不正确（在重采样步骤中可能意外的丢弃所有正确位姿附近的粒子），算法就无法恢复。 AMCL就是为了解决上述问题：结合了自适应（Augmented_MCL）和库尔贝克-莱不勒散度采样（KLD_Sampling_MCL） Augmented_MCL：在机器人遭到绑架的时候，它会在发现粒子们的平均分数突然降低了，这意味着正确的粒子在某次迭代中被抛弃了，此时会随机的全局注入粒子（injection of random particles）。 KLD_Sampling_MCL：动态调整粒子数，当机器人定位差不多得到了的时候，粒子都集中在一块了，就没必要维持这么多的粒子了——在栅格地图中，看粒子占了多少栅格。占得多，说明粒子很分散，在每次迭代重采样的时候，允许粒子数量的上限高一些。占得少，说明粒子都已经集中了，那就将上限设低。 算法流程上看，augmented_MCL算法最显著的区别就是引入了四个参数用于失效恢复： $w_{slow}$：长期似然平均估计 $w_{fast}$：短期似然平均估计 $\alpha_{slow}$：长期指数滤波器衰减率 $\alpha_{fast}$：短期指数滤波器衰减率 失效恢复的核心思想是：测量似然的一个突然衰减（短期似然劣于长期似然）象征着粒子质量的下降，这将引起随机采样数目的增加。 $w_{avg}$计算了粒子的平均权重，当粒子质量下降时，平均权重随之下降，$w_{slow}、w_{fast}$也会随之下降，但是显然$w_{fast}$下降的速度要快于$w_{slow}$——这由衰减率决定，因此随机概率$p = 1 - \frac{w_{fast}}{w_{slow}}$会增大，随机粒子数目增加。而当粒子质量提高时，粒子短期权重要好于长期，随机概率小于0，不生成随机粒子。 ROS amcl参数解析Augmented_MCL： &lt;param name=&quot;recovery_alpha_slow&quot; value=&quot;0.0&quot;/&gt;：默认0（MCL），我的0.001。 &lt;param name=&quot;recovery_alpha_fast&quot; value=&quot;0.0&quot;/&gt;：默认0（MCL），我的0.8。 在rviz里通过2D Pose Estimate按钮移动机器人来触发，机器人位置突变后要过一会儿才注入随机粒子，因为概率是渐变的。 KLD： &lt;param name=&quot;kld_z&quot; value=&quot;0.99&quot;/&gt;： KLD采样以概率$1-\delta（kld_z）$确定样本数。 &lt;param name=&quot;kld_err&quot; value=&quot;0.05&quot;/&gt;： 真实的后验与基于采样的近似之间的误差。 动态障碍物：环境中的动态物体总是会获得比静态障碍物更短的读数，因此可以根据这样的不对称性去除异常值。 静态障碍物应该服从稳定的高斯分布，以距离传感器的真实距离为均值。 扫描到动态目标的beam则服从衰减分布，$-\eta e ^{-\lambda z}$。 laser_model_type：使用beam model时会用到四个混合权重参数z_hit，z_short，z_max和z_rand，使用likelihood_field model时使用两个z_hit和z_rand。 laser_z_hit：default=0.95，以真实值为均值的噪声高斯分布 laser_z_rand：defualt=0.05，随机测量权重，均匀分布 laser_z_short：default=0.1，意外对象权重，衰减分布 laser_z_max：default=0.05，测量失败权重，0/1分布 初始位姿： 可以在rviz里通过2D Pose Estimate按钮设定（rviz会发布initialPose话题）。 或者写在launch文件中： 123456&lt;param name="initial_pose_x" value="0.0"/&gt;&lt;param name="initial_pose_y" value="0.0"/&gt;&lt;param name="initial_pose_a" value="0.0"/&gt; &lt;param name="initial_cov_xx" value="0.25"/&gt;&lt;param name="initial_cov_yy" value="0.25"/&gt;&lt;param name="initial_cov_aa" value="(pi/12)*(pi/12)"/&gt; 调用全局定位服务： 1rosservice call /global_localization "&#123;&#125;" 位姿随机初始化，粒子洒满地图： transform_tolerance： 默认是0.1seconds，官方定义是Time with which to post-date the transform that is published, to indicate that this transform is valid into the future. tf变换发布推迟的时间，意思是tf变换在未来这段时间内是可用的。 【存疑】我个人理解tf的更新频率应该越快越准确，launch文件中最开始设定为0.5，但是实际上机器人移动速度调快时，会报错Costmap2DROS transform timeout...Could not get robot pose, cancelling reconfiguration，然后我调整为1.5就不报错了。 目前设定为1.0，仿真里观测不出差异。]]></content>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[recovery behavior]]></title>
    <url>%2F2018%2F11%2F16%2Frecovery-behavior%2F</url>
    <content type="text"><![CDATA[navigation stack的move_base包中一个插件。DWA的速度空间中如果没有可行的采样点，那么机器人get stuck，触发recovery行为。 recovery行为的实质是clear out space——试图搞清楚自己的处境： 首先机器人会清扫地图——conservative reset 然后原地旋转360度，刷新处境——clearing rotation 如果还是导航失败，机器人会更加激进的清扫地图——aggressive reset 然后原地旋转360度，刷新处境——clearing rotation 如果仍旧失败——mission impossible 源代码在move_base.cpp里面，继承了nav_core的接口，设置在move_base_params.yaml配置文件中。 nav_core的recovery_behavior.h封装了RecoveryBehavior类。 move_base中创建了名为”clear_costmap_recovery/ClearCostmapRecovery”、”rotate_recovery/RotateRecovery”、”clear_costmap_recovery/ClearCostmapRecovery”的默认对象。 move_base的主程序是一个状态机，case CLEARING就调用RecoveryBehavior的runBehavior()。]]></content>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dynamic window approach]]></title>
    <url>%2F2018%2F11%2F15%2Fdynamic-window-approach%2F</url>
    <content type="text"><![CDATA[动态窗口： 窗口框的是速度空间的采样点$(v_t, w_t)$，一对$(v_t, w_t)$就代表一段轨迹，轨迹通过机器人底盘的运动学建模得到。 12345678910// base_local_planner的simple_trajectory_generator.cpp// 直线模型double dt = (current_time - last_time).toSec();double delta_x = (vx * cos(theta) - vy * sin(theta)) * dt;double delta_y = (vx * sin(theta) + vy * cos(theta)) * dt;double delta_th = vth * dt;x += delta_x;y += delta_y;theta += delta_th; 窗口的选择： 速度限制 (V, W) = \{v \in[v_{min}, v_{max}], w \in [w_{min}, w_{max}] \} 加速度限制 (V, W) = \left\{ \begin{array} & v \in[v_c - \dot{v}*\Delta t, v_c + \dot{v}*\Delta t], \\ w \in [w_c - \dot{w}*\Delta t, w_c + \dot{w}*\Delta t] \end{array} \right\} 障碍物制动限制 (V, W) = \left\{ v \leq \sqrt{2*dist(v,w)*\dot{v}}, w \leq \sqrt{2*dist(v,w)*\dot{w}} \right\}$dist(v,w)$表示采样点$(v, w)$对应轨迹上离障碍物最近的距离。 确定窗口后进行采样，可以得到一系列轨迹： 轨迹的选择： 原始论文中采用评价函数： G(v,w) = \sigma [\alpha * heading(v, w) + \beta * dist(v,w) + \gamma * velocity(v,w)] 方位角评价函数：采用当前采样点设定下，达到模拟轨迹末端时机器人的朝向角与目标点朝向角的差距。 空隙评价：当前模拟轨迹上与最近障碍物之间的距离。 速度评价：采样点速度与最大速度的差距。 上述评价函数要进行归一化。 算法流程： 123456789101112131415161718BEGIN DWA(robotPose,robotGoal,robotModel) laserscan = readScanner() allowable_v = generateWindow(robotV, robotModel) allowable_w = generateWindow(robotW, robotModel) for each v in allowable_v for each w in allowable_w dist = find_dist(v,w,laserscan,robotModel) breakDist = calculateBreakingDistance(v) if (dist &gt; breakDist) //can stop in time heading = hDiff(robotPose,goalPose, v,w) clearance = (dist-breakDist)/(dmax - breakDist) cost = costFunction(heading,clearance, abs(desired_v - v)) if (cost &gt; optimal) best_v = v best_w = w optimal = cost set robot trajectory to best_v, best_wEND]]></content>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A-star algorithm]]></title>
    <url>%2F2018%2F11%2F14%2FA-star-algorithm%2F</url>
    <content type="text"><![CDATA[有目的性地寻找最佳路径，首先定义一个损失函数，表示节点消耗： f = g+h$g$表示起点到当前节点的已知消耗 $h$表示对当前节点到终点消耗的猜测，估价函数有多种形式——启发式探索的核心 算法流程： 123456789101112131415161718192021初始化openList初始化closeList将起点放入openListwhile openList非空： 找到开启列表上f最小的节点，记为q 找到q周围的子节点p，记其父节点为q for 每一个子节点p： if p是终点： 算法终止 p.g = q.g + qp p.h = h(p, terminate) p.f = p.g + p.h if p已经在开启列表中且保存的f值小于当前计算值||p在关闭列表中： 跳过该节点 else： if p已经在开启列表中： 修改该节点的信息（父节点、fgh） else： 将该节点加入openList 将q放入closeList 算法性能在细节上的优化：http://theory.stanford.edu/~amitp/GameProgramming/ 序言：路径搜索算法的前世今生 Dijkstra算法：从初始节点开始向外扩展，直到到达目标节点。算法保证能找到从初始点到目标点的最短路径。 最佳优先搜索BFS算法：算法能够评估任意节点到目标点的代价，并优先选择离目标最近的结点。启发式算法，比Dijkstra算法运行快得多，但是不能保证路径最短。 如下面这种情况： 因为BFS是基于贪心策略的，它只关注到尽可能向着目标点移动，而不考虑已花费的代价。Dijkstra算法则正相反，它会确保每一步都是最优的，但是为此要遍历周围全部的节点。 A*算法：将两种路径搜索算法的思想结合起来，考虑两个极端及其中间的情况： 如果$h(n)$是0，只有$g(n)$起作用，那么算法演变为Dijkstra算法。 如果$h(n)$能够始终满足“比当前节点移动到目标节点的实际代价小”，那么算法保证能够找到最短路径。（$h(n)$越小，算法扩展的节点数就越多） 如果$h(n)$能够精确等于“当前节点移动到目标节点的实际代价”，那么算法将会仅仅扩展最优路径。而不扩展其他节点，算法运行非常快。 如果$h(n)$有时会“比当前节点移动到目标节点的实际代价大”，那么此时算法不能保证最短路径了。 如果$g(n)$比$h(n)$小的多，只有$h(n)$起作用，那么算法演变为BFS算法。 估价函数Heuristic function $h(a, b)$ 估价函数的选择可以follow以下的instructions： square grid that allows 4 directions：use Manhattan distance (L1) dx = abs(a.x - b.x)\\ dy = abs(a.y - b.y)\\ dis = D*(dx+dy) square grid that allows 8 directions：use Diagonal distance (L∞) dx = abs(a.x - b.x)\\ dy = abs(a.y - b.y)\\ dis = D*(dx+dy) + (D2 - 2*D)*min(dx, dy)当$D = D2 =1$时，$dis = dx + dy -min(dx, dy) = max(dx, dy)$，这个距离称为切比雪夫距离。 当$D = 1, D2 =\sqrt 2$时，这个距离称为octile distance。 square grid that allows any direcitons：use Euclidean distance (L2) dx = abs(a.x - b.x)\\ dy = abs(a.y - b.y)\\ dis = D*\sqrt{dx*dx + dy*dy} If A* is finding paths on the grid but you are allowing movement not on the grid, you may want to consider other representations of the map ​ 欧几里得距离并不适用于栅格地图，因为这会导致代价函数g和估价函数的不匹配（代价函数并不是连续的）。 由于欧几里得距离引入了开根号计算，一些算法会直接用$dis = D(dxdx + dydy)$来代替，*不建议！，会引入尺度问题，$f = g + h$，其中代价函数会逐渐增长，估价函数则逐渐减小，平方会导致两个函数的变化速率不match，使得估价函数的权重过大，导致BFS。 code： 12345678910111213public function manhattanHeuristic(a:Object, b:Object):Number &#123; return graph.distance(a, b) + simpleCost(a, b) - 1;&#125;public function simpleCost(a:Object, b:Object):Number &#123; var c:Number = costs[graph.nodeToString(b)]; if (isNaN(c)) &#123; return 1; &#125; else &#123; return c; &#125;&#125;// simpleCost限定为小于等于1的数 此时$h(a,b) = dis(a,b)+c-1 \leq h^{*}(a,b)$，此时能够找到最优解。]]></content>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[branch and bound]]></title>
    <url>%2F2018%2F11%2F13%2Fbranch-and-bound%2F</url>
    <content type="text"><![CDATA[一个栗子：整数规划问题欲求$max\ z = 5x_1 + 8x_2$ \left\{ \begin{align} & x_1 + x_2 \leq 6 \nonumber\\ & 5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\ & x_1,x_2为整数 \nonumber \end{align} \right.根据方程组可以绘制下图： 于是可以得到实数空间上的最优解：$x_1 = 2.25, x_2 = 3.75, z_0 = 41.25$。——松弛问题 由于存在整数限定条件： 最优解$0 \leq z^{*} \leq 41$，且必为整数 x_2的最优解不在3和4之间，因为限定为整数 一、分支 于是问题可以拆分为：$max\ z = 5x_1 + 8x_2$ p1\left\{ \begin{align} & x_1 + x_2 \leq 6 \nonumber\\ & 5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\ & x_2\leq 3 \nonumber \end{align} \right.\\ p2\left\{ \begin{align} & x_1 + x_2 \leq 6 \nonumber\\ & 5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\ & x_2\geq 4 \nonumber \end{align} \right.\\问题拆分的实质是将$x_2$在3和4之间的小数部分划掉了，将可行域拆分成$x_2 \leq 3$ 和$x_3 \geq 4$，但是没有排除任何整数可行解。——分支 二、定界 子问题$p1$的最优解为：$x_1 = 3, x_2=3, z^{*}=39$ 子问题$p2$的最优解为：$x_1 = 1.8, x_2=4, z^{*}=41$ 也就是说，子问题$p1$的整个参数空间上，能够取得的最优解为39，子问题$p2$上则为41，显然最优解应该位于子问题$p2$所在的参数空间中，且$39\leq z^{} \leq41$。——*定界 三、迭代 对$p2$参数空间再分支，参数$x_1$可以拆分为$x_1 \leq 1$和$x_1 \geq 2$： p3\left\{ \begin{align} & x_1 + x_2 \leq 6 \nonumber\\ & 5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\ & x_1\leq 1 \nonumber\\ & x_2\geq 4 \nonumber \end{align} \right.\\ p4\left\{ \begin{align} & x_1 + x_2 \leq 6 \nonumber\\ & 5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\ & x_1\geq 2 \nonumber \\ & x_2\geq 4 \nonumber \end{align} \right.\\四、总结 分支定界算法的总体流程如下： 先求解相应的松弛问题，得到最优解，检查其是否符合原问题约束，若符合则为最优解，否则进行下一步。 定界，取各分支中目标函数最大的作为上界$U_z$，取其余分支中目标函数中最大的作为下界$L_z$。$L_z \leq z^{*} \leq U_z$。 分支，否则选择一个不符合原问题条件的变量，构建子问题。 对各分支，有序地，进行步骤1。 在求解对应的松弛问题时，通常会有以下几种情况： 松弛问题没有可行解，那么原问题也没有可行解。 松弛问题的最优解也满足原问题约束，那么该解也是原问题的最优解，算法终止。 松弛问题的最优解小于现有下界，那么应该对该子问题进行剪枝。 五、DFS 对一颗搜索树，不用计算每一层全部节点的score（BFS），我们会维护一个优先队列，其中按照score的大小存放节点，然后选择score最大的节点（the most promising child）进行分支。]]></content>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[trifles with arduino]]></title>
    <url>%2F2018%2F10%2F30%2Ftrifles-with-arduino%2F</url>
    <content type="text"><![CDATA[系统总体的通信架构如下： 底盘驱动板Arduino负责接收上层的运动控制指令，并驱动电机，两块板子通过串口进行通信。 ROS提供了一个ros_arduino_bridge功能包集，它包括了Arduino库（ROSArduinoBridge）以及一系列用来控制Arduino-based robot的ROS功能包，这个功能包可以实现读取Twist控制信息，以及发布里程计信息等任务，封装了Raspberry Pi和Arduino之间的底层通信。 Arduino库（ROSArduinoBridge）位于ros_arduino_firmware/src/libraries/路径下，里面是一些arduino脚本和头文件，将这个文件夹复制到我们Arduino IDE的SKETCHBOOK_PATH下，然后在Arduino IDE中就可以直接打开这个sketch项目。 ROSArduinoBridge文件下是一些配置选项，另外commands.h文件中给出了一些可用的串口控制指令，如电机控制指令： 1m 20 20 // move the robot forward at 20 encoder ticks per second]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[skid-steer drive]]></title>
    <url>%2F2018%2F10%2F29%2Fskid-steer-drive%2F</url>
    <content type="text"><![CDATA[实验中使用了两种类型的底盘，基于差速驱动的2WD底盘和基于滑动转向的4WD底盘。两种驱动方式原理相似，也有其显著的区别。 相同点： 两种底盘都没有显示的转动机制，采用差速驱动的方式通过以不同的方向或速度驱动两边轮子来实现方向控制。 差速驱动的运动形式通常有一下几种类型： 第一种是原地旋转，左右轮的速度大小相等，方向相反，这样相当于绕着底盘的形心原地打转。 第二种是沿着某个方向直线行走，此时左右轮速度相同。 第三种是沿着某条曲线前行或后退，此时左右轮速度方向相同，大小不同。 第四种是旋转转弯，此时左右轮速度方向相反。 两种机构共同的优势是：没有显示的转向机构，极大地简化了运动学模型。 而两种机构共同的缺点是：由于两侧的轮子是由独立电机分别驱动的，直线运动要求两侧的轮子以相同速度转动，这将很难完成。 不同点： 差速驱动底盘通常是由一个两轮系统，每个轮子都带有独立的执行机构（直流电机），以及一个无驱动轮（可以是脚轮或者万向滚珠）组成，机器人的运动矢量是每个独立车轮运动的总和。 滑动转向底盘通常被用在履带车上，比如坦克和推土机，也被用于某些四轮六轮机构上，相比较于两轮差速底盘，滑动转向的主要区别在于： 优势：滑动转向使用了两个额外的驱动轮代替了差速驱动的脚轮，增大了牵引力。 劣势：引入了滑动，在对里程计要求高的场景中，滑动是一个致命的缺陷，因为这会对编码器造成负面影响，滑动的轮子不会跟踪机器人的确切运动。 运动学分析： 对于差速驱动机构，移动机器人航向角变化了多少角度，它就绕其运动轨迹的圆心旋转了多少角度。这句话很好验证，我们让机器人做圆周运动，从起点出发绕圆心一圈回到起点处，在这过程中机器人累计的航向角为360度，同时它也确实绕轨迹圆心运动了360度。 机器人的速度是指两个相邻的控制时刻之间的速度，因此小车的行驶轨迹可以分解为连续的圆弧片段，对于每一段圆弧，根据阿克曼转向几何原理，在小车转向时，为保证行驶稳定性，两侧轮胎都近似围绕一个中心点旋转。即整个小车底盘都围绕一个中心点旋转，已知小车中心的线速度（上层算法给定），此时小车底盘的运动学模型如下图： 参数说明： $\alpha_1$是小车前左轮和后左轮的转角。 $\alpha_2$是小车前右轮和后右轮的转角。 $2L$是左右轮距离。 $2K$是前后轮距离。 $w$是小车转轴的角速度。 $v$是小车几何中心的线速度。 $v1, v2, v3, v4$是四个车轮的速度。 $i$是电机的减速比。 $r$是车轮半径。 首先可以得到各车轮速度和角速度的关系： V_1 = w * R_1 = w * \frac{K}{sin\alpha_1}\\ V_2 = w * R_2 = w * \frac{K}{sin\alpha_2}\\ V_3 = V_1 = w * \frac{K}{sin\alpha_1}\\ V_4 = V_2 = w * \frac{K}{sin\alpha_2}\\其中车轮沿着转动方向（$y$方向）的速度由电机提供，切向速度由地面摩擦提供，车轮沿着$y$方向的速度为： R =\frac{v}{w}\\ V_{1y} = V_1 * cos\alpha_1 = w * \frac{K}{tan \alpha_1} = w(R-L)\\ V_{2y} = V_2 * cos\alpha_2 = w * \frac{K}{tan \alpha_2} = w(R+L)\\ V_{3y} = V_{1y} = w(R-L)\\ V_{4y} = V_{2y} = w(R+L)\\那么电机的角速度为： w_n= \frac{V_{ny}*i}{r}, n = 1,2,3,4\\相应电机的转速（by rpm）为： n = \frac{w_n*60}{2\pi}整理成矩阵表达式为： \begin{bmatrix} w_1\\ w_2\\ w_3\\ w_4 \end{bmatrix}= \begin{bmatrix} 1 & - L\\ 1 & L\\ 1 & - L\\ 1 & L \end{bmatrix} \begin{bmatrix} v\\ w \end{bmatrix}该表达式反映了机器人关键点速度与主动轮转速之间的关系。给定小车底盘电机转速就可以求出机器人关键点的速度，并由此得到机器人上任意一点的速度（如激光雷达的安装位置的速度），上层算法给出的关键点速度控制信号也可以由此转化成电机的控制量。 里程计模型 ／ 机器人定位方法 坐标变换模型： 在一个较短的时间间隔$\Delta t$内，假定机器人左右轮的移动距离分别是$\Delta l$和$\Delta r$，那么在机器人坐标系下：机器人中心沿着机器人坐标系的$x$轴方向前进的距离为$\Delta u = (\Delta l + \Delta r)/2$，$y$轴方向前进的距离为$\Delta v = 0$，转过的角度为$\Delta \varphi = (\Delta l - \Delta r)/b$。机器人坐标系到世界坐标系的旋转变换矩阵为$R(\phi)$。 那么转换到世界坐标系下机器人的运动增量为： \begin{bmatrix} \Delta x\\ \Delta y\\ \Delta \phi \end{bmatrix} = \begin{bmatrix} R(\phi) & 0\\ 0 & 1 \end{bmatrix} \begin{bmatrix} \Delta u\\ \Delta v\\ \Delta \varphi \end{bmatrix} = \begin{bmatrix} cos\phi & sin\phi & 0\\ -sin\phi & cos\phi & 0\\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} (\Delta l + \Delta r)/2\\ 0\\ (\Delta l - \Delta r)/b \end{bmatrix}世界坐标系下机器人位姿更新为： \begin{bmatrix} x_t\\ y_t\\ \phi_t \end{bmatrix} = \begin{bmatrix} cos\phi_{t-1}(\Delta l + \Delta r)/2\\ sin\phi_{t-1}(\Delta l + \Delta r)/2\\ (\Delta l - \Delta r)/b \end{bmatrix} + \begin{bmatrix} x_{t-1}\\ y_{t-1}\\ \phi_{t-1} \end{bmatrix}beside from测量误差，利用坐标变换模型去推算里程计信息是引入了模型误差的——在时间间隔$\Delta t$内，为了简化计算，机器人坐标系相对世界坐标系的旋转变换矩阵被假定为起始值$R(\phi)$。在转向运动比较多的情况下，里程计信息会迅速恶化。 圆弧模型： 将极小时间间隔内小车运动的轨迹看作是一段圆弧，那么就可以确定该时刻的转动中心$C_{t-1}$，及内侧轮的转动半径为$R_{t-1}$，根据几何关系： \left\{ \begin{align} &\Delta l = (b + R)\Delta \varphi\\ &\Delta r = R \Delta \varphi \end{align} \right.解得： \left\{ \begin{align} &R = \frac{b\Delta r}{\Delta l - \Delta r}\\ &\Delta \varphi = \frac{\Delta l - \Delta r }{b} \end{align} \right.由三角相似得： \frac{Rsin(\Delta \varphi/2)}{D/2} = \frac{R}{R + b/2}解得弦$D$的长度为： D = [b(\Delta l + \Delta r)/(\Delta l-\Delta r)]sin[(\Delta l - \Delta r)/2b]弦$D$与世界坐标系$x$轴正向的夹角为$\theta = \phi - \Delta \varphi/2$，那么机器人在世界坐标系下的位姿增量为： \left\{ \begin{align} &\Delta x_{t-1} = D_{t-1} cos\theta_{t-1}\\ &\Delta y_{t-1} = D_{t-1} sin\theta_{t-1}\\ &\Delta \varphi = (\Delta l - \Delta r )/b \end{align} \right.使用圆弧模型对里程计增量进行推算，完全依照几何关系来计算，计算过程中没有近似，能够有效控制误差累积。 概率模型： 不是单纯的基于里程计的估计，而是结合其他传感器的测量值对里程计进行矫正，详见滤波算法。 scan match模型： 同样也不是单纯的基于里程计的估计，详见karto scanMatch。与滤波的区别在于，返回的不是概率分布，而是一个代表最佳估计的值。]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scan matcher]]></title>
    <url>%2F2018%2F10%2F26%2Fscan-matcher%2F</url>
    <content type="text"><![CDATA[目前开源算法中采取的scanMatching方法主要是以下四种： Gmapping：ICP（simple Gradient Descent） Hector：Guass-Newton（multi-resolution map） karto：Real-time CSM（multi-resolution + 三维窗口遍历寻优） cartographer：Fast CSM（multi-resolution + branch and bound） scanMatcher主要涉及两个评价函数，一个score用于优化调整粒子pose作为参考，一个likelihoodAndScore用于更新粒子权重： s(x, z, m) = \sum_i s(x, z^i, m)\\ s(x, z^i, m) = e^{d^2/ \sigma}粒子权重根据地图的匹配度更新： laser\ frame \to map\ frame: \hat{z}^i = x \oplus z^i\\ map\ cell: (x,\ y)^T\\ d^2 = (\hat{z}^i - (x,y)^T)^T (\hat{z}^i - (x,y)^T)]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[navigation stack]]></title>
    <url>%2F2018%2F10%2F23%2Fnavigation-stack%2F</url>
    <content type="text"><![CDATA[part0现实中想要移动平台移动到指定地点，机器人运动控制系统架构包括了如下几个层次： 最底层是机器人的底盘控制部分，驱动器接收的是机器人的期望速度（Twist），将速度解算为左右轮的期望速度，并根据期望速度对左右轮分别进行PID驱控速，输出电机的转速。 这部分ROS社区已经有针对Arduino封装好的Package——rosserial_arduino。 中间层是通信层，电脑端发布速度指令给平台，同时接收平台发布的当前速度，然后发布/odom topic，让其他节点订阅。 最上层是决策层，也就是导航规划层，goal、localization（matching&amp;里程计）、path planner以及最终输出速度指令，这一部分都在navigation stack里面。 part1 packagesnavigation stack是ROS提供的导航方案，内部集成了很多个package，模块之间完全解耦，可以个性化选择提供的方法。 官网上给出了导航栈宏观的结构描述： move_base中主要包含三个部分，global_plan、local_plan以及recovery behavior。对应的插件有： global_plan：global_planner（实现了dijkstra和A*算法），carrot_planner，navfn local_plan：base_local_planner（实现了Trajectory Rollout和DWA算法），dwa_local_planner recovery：clear_costmap_recovery，move_slow_and_clear，rotate_recovery nav_core是一个接口插件，包含了以上插件基类的头文件，move_base中的方法都是在其规则上扩展的。 两个灰色的插件map_server和amcl表示可选可不选： 可以使用meta package提供的map_server节点来进行代价地图管理，也可以使用其他节点（例如直接使用gmapping的输出）。 可以使用meta package提供的amcl节点来进行自定位，也可以使用其他算法包（例如ROS里面还有一个robot_pose_ekf节点）。 costmap_2d将不同传感器的输入处理成统一的栅格地图格式。以层的概念来组织图层，用户可以根据需要自己配置（通过Social Costmap Layer、Range Sensor Layer等开源插件），默认的层有： static_layer：静态地图层，（通过订阅map_server的/map主题）来生成。 obstacle_layer：障碍地图层，根据动态的传感器信息来生成。 inflation_layer：膨胀层，将前两个图层的信息综合进行缓冲区扩展。 voxel_grid是三维代价地图。 fake_localization用来做定位仿真，内含/base_pose_ground_truth话题。 part2 params move_base_params.yaml： planner_frequency：全局规划的执行频率，如果设置为0.0则全局规划器仅在接受到新目标点或者局部规划器报告路径堵塞时才会重新执行。 global_planner_params.yaml： default_tolerance：当设置的目的地被占据时，以该参数为半径的范围内选取最近的点作为新目标点。 dwa_local_planner_params.yaml： latch_xy_goal_tolerance：如果设置为true，达到xy_goal_tolerance以内机器人就会原地旋转，即使会转出容错圈外。 sim_granularity：间隔尺寸，轨迹上采样点步长。 scaling_speed：启动机器人底盘的速度。 global_costmap_params.yaml： raytrace_range：实时清除代价地图上障碍物的最大范围，清除的是obstacle_layer的数据。 part3 topics move_base &amp; move_base_simple： 12345678ros::NodeHandle action_nh("move_base");action_goal_pub_ = action_nh.advertise&lt;move_base_msgs::MoveBaseActionGoal&gt;("goal", 1);//we'll provide a mechanism for some people to send goals as PoseStamped messages over a topic//they won't get any useful information back about its status, but this is useful for tools//like nav_view and rvizros::NodeHandle simple_nh("move_base_simple");goal_sub_ = simple_nh.subscribe&lt;geometry_msgs::PoseStamped&gt;("goal", 1, boost::bind(&amp;MoveBase::goalCB, this, _1)); 之前查看节点图的时候发现这两个节点都提供goal，一直没想通两者的关系，发现代码注释里面有，move_base继承了actionlib，有状态反馈（详见wiki 1.1.2 ActionAPI），move_base_simple就是一个publisher（topic可以来自rviz／cmd line）。 /result 记录了Goal reached /feedback 记录了每个时刻机器人的位姿 /status 记录了任务进程（goal accepted、failed、aborting） /cancel 没echo出信息，应该与上层对接 【定点巡航】另外，定点巡航的时候将global_path的buffer设置为n就可以显示多条路径了。 DWAPlanner的global_plan &amp; local_plan： local_plan就是DWA算法每个时刻计算的最优预期路径。global_plan是整个局部代价地图上的路径，它是全局路径的crop，因为局部动态环境不会影响全局路径，我们只研究落在localmap以内这一段路径是否需要矫正。]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Autolabor]]></title>
    <url>%2F2018%2F10%2F15%2Fautolabor%2F</url>
    <content type="text"><![CDATA[今天发现了一个支持二次开发的开源方案，说白了就是把karto、acml、navigation stack等几个ROS开源包整合的比较漂亮，代码结构值得借鉴。 执行keyboard_control之前要先执行脚本，添加键盘。 老AS包里面没有keyboard control，可以执行teleop_twist_keyboard包。autolabor_fake是虚拟小车的driver，建模了电机、odom相关信息，订阅cmd_vel信息。控制真实小车时这个节点要替换。 base是对机器人底盘的仿真，launch文件的默认Fixed Frame是base_link，想要控制小车运动可以将Frame切成real_map或odom。 todolist: 命令行里面控制量的显示不太好看，可以尝试在源文件里面优化，添加交互提示。 stage是对场景的仿真，场景由map_server读取，launch以后就能查看当前场景地图。 rostopic里面有一个initialpose信息，由rviz发布。rostopic list里面好多topic都是rviz发布的，在displays栏目里面取消勾选就不会发布了。 object是对障碍物的仿真，调用stage，添加interactivemarker，然后选择Interact工具，理论上地图上应该出现障碍物，但是我没找到。。。状态显示waiting for tf info。 修正：添加的marker要在topic里面选择，不要在type栏下。然后地图上放好障碍物以后要右键apply。 lidar是对雷达点云的仿真，launch中给了一个lidar和map的静态tf，实际使用中应该给lidar和base_link的。 todolist: map和real_map给的有点混乱，明天会统一一下。 老AS包create_map仿真过程中，由于场景提供mapserver和slam算法同时publish了/map这个topic，要进行区分，在launch文件里面对其中一个进行remap： 123&lt;node pkg="map_server" type="map_server" name="map_server" args="$(find simulation_launch)/map/MG_map.yaml"&gt; &lt;remap from="/map" to="real_map" /&gt;&lt;/node&gt; 这时rostopic里面就会出现real_map这个话题，两个地图能够同时显示。 代码解析首先是simulation包： autolabor_description没啥好说的，urdf文件里面定义了一个robot，整个机器人被渲染成了一个base_link，没有子节点，懒。 autolabor_fake包是底盘驱动，提供了一个autolabor_fake_node节点，其订阅类型为geometry_msgs/Twist的话题cmd_vel，信息来源可以是joystick／keyboard（tele_op_xxx）／cmd line（rostopic pub xxx）。其发布类型为nav_msgs/Odometry的话题odom。同时该节点还会将odom frame到base_link frame的transform信息提供给tf node，用来tf_broadcast。 lidar_simulation包自身提供了两个节点lidar_simulation和obstacle_simulation。 3.1 LidarSimulation::getPose这个函数中有一段代码开始比较困惑： 1234567// ROS_INFO("roll and pitch and yaw and esp :%lf %lf %lf %lf", roll, pitch, yaw, esp); //esp=0.000001,r&amp;p=0.000000if (pow(roll,2) + pow(pitch,2) &gt; esp)&#123; start_angle = yaw + max_angle_; reverse = -1.0;&#125;else&#123; // default situation: start_angle = yaw + min_angle_; reverse = 1.0; 通常情况下，我们都使用右手坐标系，二维平面下，global_frame_到lidar_frame_的坐标变换transform欧拉角形式下的r和p角应该始终是$0.0$，yaw代表了激光雷达$x$轴的变换，加上min_angle_就切换成了激光光束的初始发射角度start_angle。如果坐标系定义反了，r&amp;p就应该有值，这时因为坐标轴定义反过来了，激光光束的初始发射角度就变成了从正方向上的max_angle_开始的。 3.2 LidarSimulation::updateMap这个函数值得注意，这是一个service client，用来调用地图更新，在当前功能包的默认launch文件中，只加载了一次地图，没有体现出它的作用。当执行建图任务时，因为map frame和odom frame会不断进行矫正，建图包就会call这个request来实时更新地图。 3.3 该功能包下还自定义了一个obstacle service，提供obstacle_simulation节点来更新障碍物信息。这里的障碍物是指手动添加的障碍物（interactiveMarker），launch文件中可以定义其形状顶点。 ​ 其中的ObstacleSimulation::pnpoly函数用来判断某点是否落在多边形内，之前刷算法时有考虑过这个问题，这里给出的解法不知道是不是最优的，just for record： 1234567891011bool ObstacleSimulation::pnpoly(geometry_msgs::Polygon&amp; footprint, float&amp; x, float&amp; y)&#123; int i,j; bool c = false; for (i=0, j=footprint.points.size()-1; i&lt;footprint.points.size(); j = i++)&#123; if ( ( (footprint.points.at(i).y &gt; y) != (footprint.points.at(j).y &gt; y) ) &amp;&amp; (x &lt; (footprint.points.at(j).x-footprint.points.at(i).x) * (y-footprint.points.at(i).y) / (footprint.points.at(j).y - footprint.points.at(i).y) + footprint.points.at(i).x) )&#123; c = !c; &#125; &#125; return c;&#125; 这里面的for，循环条件遍历的是多边形的每一条边，如$(3,0), (0,1), (1,2), (2, 3)$这样。判定的是给定点是否落在给定边的内侧，这里所谓的内侧是以给定边的起始节点为原点，给定线段的顺时针方向。 3.4 obstacle的具体操作定义在static_map中，这里面出现了世界坐标系World，值得注意的是，栅格地图的原点在地图的一角，栅格的位置用整型来表示，而世界坐标系中栅格的位置由其中心来表示，两者相差$0.5$个resolution。lidar_simulation里面创建了一个static_map对象map_，以及回调函数LidarSimulation::obstacleHandleServer。 3.5 lidar_simulation功能包中的这两个节点：lidar_simulation是map级的，obstacle_simulation是obstacle级的。 接下来看simulation_launch包： 这个包里面没有源代码，只提供了几个launch文件，用来仿真几种不同的情况： sim_move_simulation.launch就是简单的底盘控制，控制小车在给定地图上运动，同时可视化雷达点云信息。 create_map_simulation.launch用来建图，在底盘控制的基础上，启动了建图功能包。发布默认名字为/map的topic，命令行执行map_saver保存。 move_base_simulation.launch用来导航，在底盘控制的基础上，启动了导航套件acml&amp;move_base，这时小车的底盘控制节点autolabor_fake_node订阅的cmd_vel信息不再来自teleop_keyboard，而是来自move_base的规划结果。 最后是move_base_sim这个功能包： 是用作真实底盘控制的（目测就是对ROS开源的move_base包的二次封装，貌似删了一些不用的插件），先skip，接下来我会直接解析ROS导航套件。]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sublime无法安装插件]]></title>
    <url>%2F2018%2F10%2F12%2Fsublime%E6%97%A0%E6%B3%95%E5%AE%89%E8%A3%85%E6%8F%92%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Package Control的配置文件中添加： 123456789101112131415161718"downloader_precedence":&#123; "linux": [ "curl", "urllib", "wget" ], "osx": [ "curl", "urllib" ], "windows": [ "wininet" ]&#125;, OSX和ubuntu下亲测均有效。]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VMware扩容]]></title>
    <url>%2F2018%2F10%2F10%2FVMware%E6%89%A9%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[之前创建虚拟机时不知道需要多大空间，给了20G，装了ROS相关套件之后磁盘使用率到了90%。 然后在硬盘设置里面调整到了60G，我以为这样就可以了。。。。 too naive。。。还是要手动配置一下，需要安装工具Gparted： 1sudo apt-get install gparted 然后先抹掉extended和swap两个分区，然后就可以resize主分区了，然后在重新创建那两个分区就好了。 Attention：磁盘只能扩展，不能变小，因此建议逐渐扩展。]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSM, Correlative Scan Matching]]></title>
    <url>%2F2018%2F10%2F08%2FCSM%2F</url>
    <content type="text"><![CDATA[SLAM前端主要解决两个问题，一是从不同时刻的传感器输入中识别出同一个地图特征，二是计算每个当前时刻机器人相对该特征的位姿。 vSLAM能够轻松解决前者，LidarSLAM解决后者无压力。本文讨论激光SLAM前端问题1——特征点匹配。目前两大常用思路：scan2scan——ICP，scan2map——CSM。 basic的CSM算法思路如下：从前一帧机器人位姿开始，寻找最优刚性变换，使雷达点在栅格地图中的位置尽量对应于占据度为1的栅格。 为了保持定位信息原有的数位精度，使用双线性插值方法来获取雷达点的地图值(占据度)，而不是使用其所在栅格的地图值来直接对应： $Pm$是雷达点，$P_{00}, P_{01}, P_{10}, P_{11}$是雷达点邻近的四个栅格中心点。于是得到地图值： \begin{align} M(P_m)& \approx \frac{y-y_0}{y_1-y_0}M(I_0) +\frac{y_1-y}{y_1-y_0}M(I_1)\\ & =\frac{y-y_0}{y_1-y_0} \left ( \frac{x-x_0}{x_1-x_0}M(P_{11}) + \frac{x_1-x}{x_1-x_0}M(P_{01})\right)\\ & +\frac{y_1-y}{y_1-y_0}\left ( \frac{x-x_0}{x_1-x_0}M(P_{10}) + \frac{x_1-x}{x_1-x_0}M(P_{00})\right) \end{align}雷达点所在位置的地图值变化梯度： \triangledown M(P_m) = \left [\frac{\delta M(P_m)}{\delta x}, \frac{\delta M(P_m)}{\delta y} \right]\\ \frac{\delta M(P_m)}{\delta x} =\frac{y-y_0}{(y_1-y_0)(x_1-x_0)}(M(P_{11})- M(P_{01})) +\frac{y_1-y}{(y_1-y_0)(x_1-x_0)}(M(P_{10})- M(P_{00}))\\ \frac{\delta M(P_m)}{\delta y} =\frac{x-x_0}{(y_1-y_0)(x_1-x_0)}(M(P_{11})- M(P_{10})) +\frac{x_1-x}{(y_1-y_0)(x_1-x_0)}(M(P_{01})- M(P_{00}))\\记当前时刻的已有地图$M$，当前帧共输入$n$个雷达点$S_1, …, S_n$，其对应位置的占据度为$M(S_k)$，最优变换定义为$\xi = (\Delta x, \Delta y, \psi)^T$，则最优问题的最小二乘描述为： \xi = argmin_{\xi} \sum_{k=1}^n[1-M(S_k(\xi))]^2scan2map的鲁棒性更强，但是实时性上打了折扣。对此主要有两个改进措施：一是局部搜索，二是分辨率金字塔。 一、局部搜索 实际计算中会选定一个搜索区间，通常只在上一时刻地图位置的附近，对其中包含的全部可能位姿进行评分。 上式（最小二乘表达式）只包含了雷达端点，考虑到传感器的噪点影响，局部极值影响大。 基于模版的匹配：雷达端点及射线所在栅格构成的多边形区域，以此作为局部地图，进行map2map匹配。减少局部极值的影响，提高计算代价，同时考虑到动态目标，会引入新的局部极值。 LM算法：迭代的方式求解最小二乘的最优解。 分支界定算法：基于广度优先搜索的算法，通过对解空间搜索树的分支进行扩展和剪枝，不断调整搜索方向，加快找到全局最优解的速度。界定核心：若当前分支的下界$C_{branch}$小于解空间上界$C_{HB}$，则进行拓展，否则进行裁剪，直至到达叶子结点，即找到最小代价解。 二、多分辨率金字塔 两帧雷达点云的相似区域并不会影响匹配的最终结果，但会参与计算，导致搜索效率降低，需要更多的迭代次数达到收敛。 当地图分辨率较低时，部分地图信息会被忽略，这种高、低分辨率下的差异，有助于对地图中的相似场景进行区分。实际使用中，首先将初始位姿对应的雷达点云与最上层（粗分辨率）的地图进行匹配，计算出当前分辨率下的位姿，并作为初始值进入次一级地图进行匹配，以此类推。]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[状态估计]]></title>
    <url>%2F2018%2F06%2F30%2F%E7%8A%B6%E6%80%81%E4%BC%B0%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[1. 概述 经典的SLAM模型由一个运动方程和一个观测方程构成： \left\{ \begin{split} & x_k = f(x_{k-1}, u_k) + w_k\\ & z_{k,j} = h(y_i, x_k) + v_{kj} \end{split} \right.其中，$x_k$表示相机的位姿，可以用变换矩阵或李代数来表示，$y_i$表示路标，也就是图像中的特征点，$w_k$和$v_{kj}$表示噪声项，假设满足零均值的高斯分布。 我们希望通过带噪声的观测数据$z$和输入数据$u$推断位姿$x$和地图$y$的概率分布。主要采用两大类方法，一类是滤波方法：基于当前状态来估计下一状态，忽略历史；一类是非线性优化方法，使用所有时刻的数据估计新状态的最优分布。 滤波方法主要分为扩展卡尔曼滤波和粒子滤波两大类。 非线性优化根据实现细节的不同主要分为滑动窗口法和Pose Graph法。 2. 非线性优化 非线性优化基于历史，同时也作用于历史，因此把所有待估计的变量放在一个状态变量中： x = \{x_1, ..., x_N, y_1, ..., y_M\}在已知观测数据$z$和输入数据$u$的条件下，对机器人的状态估计： P(x | z,u)先忽略测量运动的传感器，仅考虑测量方程，根据贝叶斯法则： P(x|z) = \frac{P(z|x)P(x)}{P(z)} \varpropto P(z|x)P(x) 先验概率$P(x)$：先验的概念最好理解，就是一个事件的概率分布。 似然概率$P(z|x)$：已知事件的概率分布，事件中某状态的概率。 后验概率$P(x|z)$：在给定数据条件下，不确定性的条件分布。 $Posterior \varpropto Likelihood * Prior$ 求解后验分布比较困难，但是求一个状态最优估计（使得后验概率最大化）是可行的： x^*_{MAP} = argmaxP(x|z) = argmaxP(z|x)P(x) = argmaxP(z|x)因为先验概率不知道，所以问题直接转成为求解极大似然估计，问题中的未知数是$x$，直观意义就是：寻找一个最优的状态分布，使其最可能产生当前观测到的数据。 假设了噪声项$v_{kj} \thicksim N(0, Q_{k,j})$，所以极大似然概率也服从一个高斯分布： P(z_{kj}|x_k, y_j) = N(h (y_j, x_i), Q)求高斯分布的最值通常取负对数处理，最大化变成求最小化： P(x) = \frac{1}{\sqrt{(2\pi)^Ndet(\Sigma)}} exp\bigg(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\bigg)\\ -ln(P(x) ) = \frac{1}{2}ln\big((2\pi)^Ndet(\Sigma)\big) + \frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)对数第一项与$x$无关，第二项等价于噪声的平方项。因此可以得到优化问题的目标函数： e_{k} = x_k - f(x_{k-1}, u_k)\\ e_{kj} = z_{kj} - h(x_k, y_j)\\ J(x) = \Sigma e_k^TR^{-1}_ke_{k} + \Sigma e_{kj}^TQ^{-1}_ke_{kj}\\ x* = argmin_x J(x)以上的最小二乘问题可以采用各种各样的梯度下降法求解最优解（参考图优化）。 Bundle Ajustment 优化问题最终可以表示成$H\Delta x = g$的形式，其对角线上的两个矩阵为稀疏矩阵，且右下角的矩阵维度往往远大于左上角（因为特征点的数目远大于位姿节点）： H = \begin{bmatrix} B &E\\ E^T & C \end{bmatrix}一个有效的求解方式称为Schur消元，也叫边缘化Marginalization，主要思路如下：首先求解$C$矩阵的逆矩阵，然后对$H$矩阵进行消元，目标是消去其右上角的$E$矩阵，这样就能够先独立求解相机参数$\Delta x_c$，再利用求得的解来求landmarks参数$\Delta x_p$： \begin{bmatrix} I &-EC^{-1}\\ 0 & I \end{bmatrix} \begin{bmatrix} B &E\\ E^T & C \end{bmatrix} \begin{bmatrix} \Delta x_c \\ \Delta x_p \end{bmatrix} = \begin{bmatrix} I &-EC^{-1}\\ 0 & I \end{bmatrix} \begin{bmatrix} v \\ w \end{bmatrix} \\ \begin{bmatrix} B - EC^{-1}E^T & 0\\ E^T & C \end{bmatrix}\begin{bmatrix} \Delta x_c \\ \Delta x_p \end{bmatrix} = \begin{bmatrix} v - EC^{-1}w\\ w \end{bmatrix}因此可以解得$\Delta x_c$： \begin{bmatrix} B - EC^{-1}E^T \end{bmatrix} \Delta x_c = v - EC^{-1}w 这个矩阵称为消元之后的$S$矩阵，它的维度与相机参数的维度一致 $S$矩阵的意义是两个相机变量之间是否存在着共同观测点 $S$矩阵的稀疏性由实际数据情况决定，因此只能通过普通的矩阵分解的方式来求解 核函数 当误差很大时，二范数增长的很快，为了防止其过大掩盖掉其他的边，可以将其替换成增长没那么快的函数，使得整个优化结果更为稳健，因此又叫鲁棒核函数，常用的核有Huber核、Cauchy核、Tukey核等，Huber核的定义如下： H(e) = \left\{ \begin{split} & \frac{1}{2} e ^2, \ \ \ |e| \leq \delta\\ & \delta(|e| - \frac{1}{2}\delta), \ \ else \end{split} \right. 3. 卡尔曼滤波滤波思路基于一个重要的假设：一阶马尔可夫性——k时刻状态只与k-1时刻状态有关，整理成两个要素如下： $x_{k-1}$ contains the whole history $x_k = f(x_{k-1}, u_k, z_k)$ 在这里我们只需要维护一个状态量$x_k$，并对它不断进行迭代更新，moreover，如果状态量服从高斯分布，我们只需要维护状态量的均值和方差即可（进一步简化）。 首先考虑一个线性系统： \left\{ \begin{split} & x_k = A_k x_{k-1}+u_k + w_k\\ & z_k = C_k x_k + v_k \end{split} \right.\\ w_k \thicksim N(0, R), v_k \thicksim N(0, Q)卡尔曼滤波器的第一步预测，通过运动方程确定$x_k$的先验分布，注意用不同的上标区分不同的概率分布：尖帽子$\hat x_k$表示后验，平帽子$\bar x_k$表示先验： P(\bar x_k) = N(\bar x_k, \bar P_k)\\ \bar x_k = A_k \hat x_{k-1} + u_k\\ \bar P_k = A_k\hat P_{k-1}A_k^T+R第二步为观测，通过分析实际观测值，计算在某状态下应该产生怎样的分布： P(z_k|x_k) = N(C_kx_k, Q)第三步为更新，根据第一节中的贝叶斯法则，得到$x_k$的后验分布： (\hat x_k, \hat P_k) = N(C_kx_k, Q)N(\bar x_k, \bar P_k)\\ K = \bar P_k C_k^T(C_k\bar P_k C_k^T + Q_k)^{-1}\\ \hat x_k = \bar x_k + K(z_k-C_k\bar x_k)\\ \hat P_k = (I - KC_k)\bar P_k整体的流程图如下： 具体过程本节中不做展开，详情可以参考卡尔曼滤波。高斯分布经过线性变换仍然服从高斯分布，因此整个过程没有发生任何的近似，因此可以说卡尔曼滤波器构成了线性系统的最优无偏估计。 下面考虑非线性系统： SLAM中不管是三维还是平面刚体运动，因为都引入了旋转，因此其运动方程和观测方程都是非线性函数。一个高斯分布，经过非线性变换，通常就不再服从高斯分布，因此对于非线性系统，必须采取一定的近似，将一个非高斯分布近似成高斯分布。 通常的做法是，将k时刻的运动方程和观测方程在$\hat x_{k-1}$，$\hat P_{k-1}$处做一阶泰勒展开，得到两个雅可比矩阵： F = \frac{\partial f}{\partial x_{k-1}}\bigg|_{\hat x_{k-1}}\\ H = \frac{\partial h}{\partial x_k}\bigg|_{\hat x_k}中间量卡尔曼增益$K_k$： \bar P_k = F\hat P_{k-1}F^T + R_k\\ K_k = \bar P_k H^T(H \bar P_k H^T + Q_k)^{-1}后验概率： \hat x_k = \bar x_k + K_k(z_k - h(\bar x_k))\\ \hat P_k = (I - K_k H)\bar P_k 对于SLAM这种非线性的情况，EKF给出的是单次线性近似下的最大后验估计（MAP）。 4. EKF VS Graph-Optimization 马尔可夫性抛弃了更久之前的状态，优化方法则运用了更多的信息。 非线性误差：两种方法都使用了线性化近似，EKF只在$x_{k-1}$处做了一次线性化，图优化法则在每一次迭代更新时都对新的状态点做泰勒展开，其线性化的模型更接近原始非线性模型。 存储：EKF维护的是状态的均值和方差，存储量与状态维度成平方增长，图优化存储的是每个状态点的位姿，存储线性增长。]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优化库：Ceres & g2o]]></title>
    <url>%2F2018%2F06%2F28%2F%E4%BC%98%E5%8C%96%E5%BA%93%EF%BC%9Ag2o-Ceres%2F</url>
    <content type="text"><![CDATA[Ceres使用Ceres求解非线性优化问题，主要分为三个部分： 第一部分：构建代价函数Cost_Functor 12345678// 定义一个实例化时才知道的类型Ttemplate &lt;typename T&gt;// 运算符()的重载，用来得到残差fibool operator()(const T* const x, T* residual) const &#123; residual[0] = T(10.0) - x[0]; return true; &#125; 第二部分：构建最小二乘问题problem 12345Problem problem;// 使用自动求导，第一个1是输出维度（残差项），第二个1是输入维度（优化项）CostFunction* cost_function = new AutoDiffCostFunction&lt;CostFunctor, 1, 1&gt;(new CostFunctor);// 添加误差项，NULL表示不使用核函数，x是优化项problem.AddResidualBlock(cost_function, NULL, &amp;x); 第三部分：求解器参数配置Solver 1234567Solver::Options options;options.linear_solver_type = ceres::DENSE_QR; //配置增量方程的解法，稠密的QR分解options.minimizer_progress_to_stdout = true;//输出到coutSolver::Summary summary;//优化信息Solve(options, &amp;problem, &amp;summary);//求解cout &lt;&lt; summary.BriefReport() &lt;&lt; "\n";//输出优化的简要信息 使用核函数：数据中往往存在离群点，离群点会对寻优结果造成影响，这时可以使用一些损失核函数来对离群点的影响加以消除，Ceres库中提供的核函数主要有：TrivialLoss 、HuberLoss、 SoftLOneLoss 、 CauchyLoss。 12// 使用核函数problem.AddResidualBlock(cost_function, new CauchyLoss(0.5, &amp;x); g2o用g2o优化库来进行优化的步骤如下： 定义节点和边的类型，通常在默认的基础类型上做修改 定义顶点，顶点的基类为g2o::BaseVertex&lt;优化变量维度，数据类型&gt; 1class CurveFittingVertex: public g2o::BaseVertex&lt;3, Eigen::Vector3d&gt; 顶点的更新函数oplusImpl：定义增量加法，因为优化变量和增量之间并不一定是线性叠加的关系，如位姿变换。 定义边， 本例中的边为一元边，基类为g2o::BaseUnaryEdge&lt;观测值维度，数据类型，连接顶点类型&gt; 1class CurveFittingEdge: public g2o::BaseUnaryEdge&lt;1, double , CurveFittingVertex&gt; 误差项计算函数computeError：计算预测值和观测值的误差。估计值是基于当前对优化变量的estimate计算出的，观测值是直接获取的，如本例中的y值。 构建图模型 123456789101112131415// vertexg2o::VertexSE3Expmap* pose = new g2o::VertexSE3Expmap(); // camera posepose-&gt;setId( index );pose-&gt;setEstimate( expression );optimizer.addVertex ( pose );// edgeg2o::EdgeProjectXYZ2UV* edge = new g2o::EdgeProjectXYZ2UV();edge-&gt;setId ( index );edge-&gt;setVertex ( 0, point );edge-&gt;setVertex ( 1, pose );edge-&gt;setMeasurement ( Eigen::Vector2d ( p.x, p.y ) ); // 导入观测值edge-&gt;setParameterId ( 0,0 );edge-&gt;setInformation ( Eigen::Matrix2d::Identity() ); // 设置信息矩阵optimizer.addEdge ( edge ); 信息矩阵edge-&gt;setInformation(信息矩阵)：因为最终的优化函数是$\sum e_i^T \Sigma^{-1}e_i$，是误差项和信息矩阵乘积的形式。 优化器配置 矩阵块Block 优化算法solver 图模型optimizer 执行优化]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[直接法]]></title>
    <url>%2F2018%2F06%2F16%2F%E7%9B%B4%E6%8E%A5%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前面说完了 PnP，趁热打铁更新直接法，因为两者的思路基本一致，主要的差别在于PnP中利用的是特征点的重投影误差——匹配点在query帧像素平面上的实际位置和估计位置的误差，直接法不提取特征点，而是采用像素亮度误差。 1. 直接法的推导以第一个相机为参考系，第二个相机的运动参数为$R, t, \xi$，对某个空间点$P$： p_1 = \begin{bmatrix} u_1\\ v_1\\ 1 \end{bmatrix} = \frac{1}{Z_1}KP\\ p_2 = \begin{bmatrix} u_2\\ v_2\\ 1 \end{bmatrix} = \frac{1}{Z_2}K(RP+t) = \frac{1}{Z_2}Kexp(\xi^{\wedge})P两个像素点的亮度误差： e = I_1(p_1) - I_2(p_2)目标函数： min_{\xi} J(\xi) = \sum_{i=1}^N||e_i^Te_i||^2_2误差函数关于优化变量的导数： \begin{split} e(\xi + \delta \xi)& = I_1\big(\frac{1}{Z_1}KP\big) - I_2\big(\frac{1}{Z_2}K(exp(\delta \xi^{\wedge})exp(\xi^{\wedge})P\big)\\ & \approx I_1\big(\frac{1}{Z_1}KP\big) - I_2\big(\frac{1}{Z_2}K(1+\delta \xi^{\wedge})exp(\xi^{\wedge})P\big)\\ & = I_1\big(\frac{1}{Z_1}KP\big) - I_2\big(\frac{1}{Z_2}Kexp(\xi^{\wedge})P\big) - I_2\big(\frac{1}{Z_2}K\delta \xi^{\wedge} exp(\xi^{\wedge})P \big)\\ & = e(\xi) - I_2\big(\frac{1}{Z_2}K\delta \xi^{\wedge} exp(\xi^{\wedge})P \big) \end{split}上面的扰动相关项中，记： q = \delta \xi^{\wedge} exp(\xi^{\wedge})P\\ u = \frac{1}{Z_2}Kq\\误差函数线性化： e(\xi + \delta \xi) = e(\xi) - I_2(u)\\ \therefore \frac{e(\xi + \delta \xi)}{\partial \delta \xi} = \frac{-I_2(u)}{\partial \delta \xi}\\ e(\xi + \delta \xi) =e(\xi) - (\frac{\partial I_2}{\partial u}\frac{\partial u}{\partial q}\frac{\partial q}{\partial \delta \xi})\delta \xi$q$表示扰动分量在第二相机坐标系下的坐标（回顾关于$R$的微分方程：$\dot R(t) = \phi_0^{\wedge}R(t)$），因此$u$的意义为像素坐标，$\frac{\partial I_2}{\partial u}$的物理意义为像素梯度，$\frac{\partial u}{\partial q}$的物理意义为像素坐标关于三维点的导数（参考针孔相机模型），$\frac{\partial q}{\partial \delta \xi}$的物理意义为三维点关于扰动的导数（参考李代数）。 2. 直接法分类根据P的来源，直接法分为三类： P来自于稀疏关键点——稀疏直接法 P来自部分像素，只使用带有梯度的像素点——半稠密直接法 P为所有像素——稠密直接法 3. 代码实现主要关注Edge类里面重定义的增量更新函数linearizeOplus()里面Jacobian矩阵的写法。 \begin{split} J & = -\frac{\partial I_2}{\partial u}\frac{\partial u}{\partial q}\frac{\partial q}{\partial \delta \xi}\\ & = -\frac{\partial I_2}{\partial u}\frac{\partial u}{\partial \delta \xi}\\ \end{split} 前一项是$u$处的像素梯度，使用数值导数： 1234// jacobian from I to u (1*2)Eigen::Matrix&lt;double, 1, 2&gt; jacobian_pixel_uv;jacobian_pixel_uv ( 0,0 ) = ( getPixelValue ( u+1,v )-getPixelValue ( u-1,v ) ) /2;jacobian_pixel_uv ( 0,1 ) = ( getPixelValue ( u,v+1 )-getPixelValue ( u,v-1 ) ) /2; getPixelValue这个函数涉及到一个双线性插值，因为上面的二维坐标uv是通过相机投影变换得到的，是浮点形式，而像素值是离散的整数值，为了更精细地表示像素亮度，要对图像进行进行插值。 线性插值：已知数据$(x_0, y_0)$和$(x_1, y_1)$，要计算$[x_0, x_1]$区间内任一$x$对应的$y$值： \frac{y - y_0}{x-x_0} = \frac{y_1-y_0}{x_1-x_0}\\ \therefore y = \frac{x_1 -x}{x_1 - x_0}y_0 + \frac{x-x_0}{x_1-x_0}y_1 双线性插值：本质上就是在两个方向上做线性插值： 首先是x方向： f(R_1) = \frac{x_2 - x}{x_2-x_1}f(Q_{11}) + \frac{x - x_1}{x_2-x_1}f(Q_{21}), \ where\ R_1 = (x, y_1)\\ f(R_2) = \frac{x_2 - x}{x_2-x_1}f(Q_{12}) + \frac{x - x_1}{x_2-x_1}f(Q_{22}), \ where\ R_2 = (x, y_2)然后y方向： f(P) = \frac{y_2 - y}{y_2 - y_1}f(R_1) + \frac{y-y_1}{y_2-y_1}f(R_2)综合起来就是： f(x,y) = \frac{f(Q_{11})}{(x_2-x_1)(y_2-y_1)}(x_2-x)(y_2-y) + \frac{f(Q_{21})}{(x_2-x_1)(y_2-y_1)}(x-x_1)(y_2-y) \\ + \frac{f(Q_{12})}{(x_2-x_1)(y_2-y_1)}(x_2-x)(y-y_1) + \frac{f(Q_{22})}{(x_2-x_1)(y_2-y_1)}(x-x_1)(y-y_1) 1234567891011inline float getPixelValue ( float x, float y )&#123; uchar* data = &amp; image_-&gt;data[ int ( y ) * image_-&gt;step + int ( x ) ]; float xx = x - floor ( x ); float yy = y - floor ( y ); return float ( ( 1-xx ) * ( 1-yy ) * data[0] + xx* ( 1-yy ) * data[1] + ( 1-xx ) *yy*data[ image_-&gt;step ] + xx*yy*data[image_-&gt;step+1] ); 后两项都是与相机参数和三维点坐标有关，可以合并，同时注意g2o中对SE3的定义平移和旋转和本文设定是反过来的。 \xi = \begin{bmatrix} \rho\\ \phi \end{bmatrix}\\ \frac{\partial u}{\partial \delta \xi}= \begin{bmatrix} \frac{f_x}{Z} & 0 & -\frac{f_xX}{Z^2} & |& -\frac{f_xXY}{Z^2} & f_x + \frac{f_xX^2}{Z^2} & -\frac{f_xY}{Z}\\ 0 & \frac{f_y}{Z} & -\frac{f_yY}{Z^2}& | & - f_y - \frac{f_xY^2}{Z^2} & \frac{f_yXY}{Z^2} & \frac{f_yX}{Z}\\ \end{bmatrix}12345678910111213141516// jacobian from u to xi (2*6)Eigen::Matrix&lt;double, 2, 6&gt; jacobian_uv_ksai;// xi = [\phi, \pho]jacobian_uv_ksai ( 0,0 ) = - x*y*invz_2 *fx_;jacobian_uv_ksai ( 0,1 ) = ( 1+ ( x*x*invz_2 ) ) *fx_;jacobian_uv_ksai ( 0,2 ) = - y*invz *fx_;jacobian_uv_ksai ( 0,3 ) = invz *fx_;jacobian_uv_ksai ( 0,4 ) = 0;jacobian_uv_ksai ( 0,5 ) = -x*invz_2 *fx_;jacobian_uv_ksai ( 1,0 ) = - ( 1+y*y*invz_2 ) *fy_;jacobian_uv_ksai ( 1,1 ) = x*y*invz_2 *fy_;jacobian_uv_ksai ( 1,2 ) = x*invz *fy_;jacobian_uv_ksai ( 1,3 ) = 0;jacobian_uv_ksai ( 1,4 ) = invz *fy_;jacobian_uv_ksai ( 1,5 ) = -y*invz_2 *fy_;]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenCV定制源码编译]]></title>
    <url>%2F2018%2F06%2F12%2FOpenCV%E5%AE%9A%E5%88%B6%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[1. cmake选项 测试单元可以关掉：BUILD_DOCS，BUILD_EXAMPLES，BUILD_XXX_TESTS，BUILD_opencv_ts(一些单元测试代码)，BUILD_PACKAGE (CPACK_BINARY_XXX，CPACK_SOURCE_XXX)，INSTALL_XXX 减少引入体积：打开世界模块开关BUILD_opencv_world(暂时没开，因为编译之后发现找不到要引用的头文件了)，打开BUILD_SHARED_LIBS 关掉音视频处理相关模块：BUILD_opencv_video，BUILD_opencv_videoio，BUILD_opencv_videostab，WITH_1394，WITH_GSTREAMER_XXX 关闭GPU相关模块：WITH_OPENCL，WITH_CUDA 打开TBB模块：隐式的并行计算程序，底层依赖于操作系统的多线程库，BUILD_TBB 打开viz模块：WITH_VTK，BUILD_opencv_viz 暂时没开启Java相关模块：ant，就没brew过这个包 以上reference from 博客1，博客2 2. extra modules with opencv3.0, SURF/SIFT and some other things have been moved to a seperate opencv_contrib repo. 一部分模块被独立到了opencv_contrib这个包，首先clone到本地，然后在cmake选项里面找到OPENCV_EXTRA_MODULES_PATH，填好。 2. 其他说明另外之前brew install的opencv包一定要卸载掉，不要乱link，否则INCLUDE和LIBS的路径都会出问题，手动修改cmake文件不要太酸爽。]]></content>
      <tags>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PnP, Perspective-n-Point]]></title>
    <url>%2F2018%2F06%2F07%2FPnP-Perspective-n-Point%2F</url>
    <content type="text"><![CDATA[1. 概述 PnP是求解3D到2D点对运动的方法，它描述了当知道n个3D空间点及其投影位置时，如何估计相机的位姿变换。最少只需要3个点对就可以估计相机的运动。 该方法使用的条件是，参考点为世界坐标系下的特征点，其空间位置已知，并且知道query帧中对应参考点的像素坐标。 PnP问题的求解方法有很多种： 直接线性变换 P3P 非线性优化方法——Bundle Ajustment 本节着眼于BA求解方法，其他方法暂时不做展开。 2. Bundle Ajustment利用优化求解的思路是：最小化重投影误差——期望计算query帧的相机位姿$R, t$，它的李代数为$\xi$，空间特征点的坐标为$P_i = [X_i, Y_i, Z_i]^T$，其在query帧上的像素坐标为$u_i = [u_i, v_i]^T$，那么理论上： s_i u_i = K exp(\xi^{\wedge})P_i构建成最小二乘问题就是：寻找最优的相机位姿$\xi$，使得误差最小化： \xi^* = argmin_{\xi} \frac{1}{2}\sum_{i=1}^{n}||u_i -\frac{1}{s_i}Kexp(\xi^{\wedge})P_i||^2_2在使用优化库来求解之前，还有一个问题——每个误差项$e_i = u_i -\frac{1}{s_i}Kexp(\xi^{\wedge})P_i$的导数$J_i$。 回忆图优化中讲过的，优化问题最终转化成为矩阵的线性求解$H\Delta x = g$，其中矩阵$H$是由单个误差项一阶展开$e(x+\Delta x) = e(x) + J\Delta x$中的雅可比矩阵$J_i$ 构成的稀疏对称阵。 误差项是一个二维向量（像素坐标差），优化变量是一个六维向量（空间位姿李代数），因此$J$是一个2*6的矩阵。 \frac{\partial e}{\partial \delta \xi} = \lim_{\delta \xi \rightarrow0} \frac{\partial e}{\partial P^{'}}\frac{\partial P^{'}}{\partial \delta \xi}其中$P^{‘}$是特征点转换到相机坐标系下的空间坐标： su = KP^{'}\\ u = f_x \frac{X^{'}}{Z^{'}} + c_x\\ v = f_y \frac{X^{'}}{Z^{'}} + c_y因此误差项导数的第一项为： \frac{\partial e}{\partial P^{'}} = - \begin{bmatrix} \frac{\partial u}{\partial X^{'}} & \frac{\partial u}{\partial Y^{'}} & \frac{\partial u}{\partial Z^{'}}\\ \frac{\partial v}{\partial X^{'}} & \frac{\partial v}{\partial Y^{'}} & \frac{\partial v}{\partial Z^{'}} \end{bmatrix} =- \begin{bmatrix} \frac{f_x}{Z^{'}} & 0 & -\frac{f_xX^{'}}{Z^{'2}}\\ 0 & \frac{f_y}{Z^{'}} & -\frac{f_yY^{'}}{Z^{'2}}\\ \end{bmatrix}误差项的第二项为变换后的点关于李代数的导数，参考李代数节： \frac{\partial TP}{\partial \delta \xi} = \begin{bmatrix} I & -P^{'\wedge}\\ 0^T & 0^T \end{bmatrix}其中$P^{‘\wedge}$是$P^{‘}$的反对称阵： P^{'\wedge} = \begin{bmatrix} 0 & -Z^{'} & Y^{'}\\ Z^{'} & 0 & -X^{'}\\ -Y^{'} & X^{'} & 0 \end{bmatrix}因此得到完整的雅可比矩阵： \frac{\partial e}{\partial \delta \xi} =\frac{\partial e}{\partial P^{'}} \begin{bmatrix} I & P^{'\wedge} \end{bmatrix}=- \begin{bmatrix} \frac{f_x}{Z^{'}} & 0 & -\frac{f_xX^{'}}{Z^{'2}} & |& -\frac{f_xX^{'}Y^{'}}{Z^{'2}} & f_x + \frac{f_xX^{'2}}{Z^{'2}} & -\frac{f_xY^{'}}{Z^{'}}\\ 0 & \frac{f_y}{Z^{'}} & -\frac{f_yY^{'}}{Z^{'2}}& | & - f_y - \frac{f_xY^{'2}}{Z^{'2}} & \frac{f_yX^{'}Y^{'}}{Z^{'2}} & \frac{f_yX^{'}}{Z^{'}}\\ \end{bmatrix}除了优化相机位姿以外，还可以同时优化特征点的空间位置$P$： \frac{\partial e}{\partial P} = \frac{\partial e}{\partial P^{'}} \frac{\partial P^{'}}{\partial P}其中的第二项为： P^{'} = exp(\xi^{\wedge})P = RP +t\\ \therefore \frac{\partial P^{'}}{\partial P} = R^T3. 优化库使用构建图模型： 优化变量1：节点1：query相机位姿（六维向量李代数） 优化变量2：节点2：特征点空间位置（三维向量坐标描述） 预测值：边n：根据当前estimate的优化量，投影到投影平面的像素坐标$z_i = h(\xi, P_i)$ 观测值：能够直接读出的，query帧上对应特征点的实际投影坐标$u_i$ g2o中已经提供了相近的基类（在g2o/types/sba/types_six_dof_expmap.h中）：李代数位姿节点VertexSE3Expmap、空间点位置节点VertexSBAPointXYZ、投影方程边EdgeProjectXYZ2UV。 边类里面要关注linearizeOplus函数，这个函数描述的是非线性函数进行线性化的过程中，求导的解析解（当然也可以使用数值解），最后函数给出的是每个节点的导数矩阵（$\frac{\partial e}{\partial \delta \xi} $和$\frac{\partial e}{\partial P_i}$） 。这点是Ceres库和g2o库的一点主要差别：Ceres都是使用自动的数值导数，g2o要自己求导。 1234567891011void EdgeProjectXYZ2UV::linearizeOplus()&#123; VertexSE3Expmap * vj = static_cast&lt;VertexSE3Expmap*&gt;(_vertices[1]); VertexSBAPointXYZ* vi = static_cast&lt;VertexSBAPointXYZ*&gt;(_vertices[0]); ... ... _jacobianOplusXi = -1./z * tmp * T.rotation().toRotationMatrix(); _jacobianOplusXj(0,0) = x*y/z^2 * cam-&gt;focal_length; ...&#125; 4. OpenCV内置函数basic： 12345void cv::solvePnP ( pts3d, pts2d, K, Mat(), r, t, false );Mat r, t, R;cv::solvePnP ( pts3d, pts2d, K, Mat(), r, t, false );cv::Rodrigues ( r, R ); // 旋转向量和旋转矩阵的转换 advanced： 1234567891011121314bool cv::solvePnPRansac( InputArray objectPoints, // 3D空间坐标 vector&lt;cv::Point3f&gt; pts3d InputArray imagePoints, // 2D像素坐标 vector&lt;cv::Point2f&gt; pts2d InputArray cameraMatrix, // 相机内部参数矩阵 K InputArray distCoeffs, // 畸变系数向量 cv::Mat() OutputArray rvec, // 旋转向量 OutputArray tvec, // 平移向量 bool useExtrinsicGuess = false, // If true, the function uses the provided rvec and tvec values as initial int iterationsCount = 100, // Number of iterations float reprojectionError = 8.0,// 重投影误差最大值 double confidence = 0.99, OutputArray inliers = noArray(), // Output vector that contains indices of inliers in objectPoints and imagePoints . int flags = SOLVEPNP_ITERATIVE // method for solving PnP) Ransac：考虑到我们提供的匹配里面存在误匹配的情况，OpenCV采用“随机采样一致性算法”（Random Sample Consensus），从现有匹配中随机取一部分用来估计运动（PnP的解析解法最少只需要三个点就能计算相对位姿），正确的匹配结果都是近似的，从而剔除误匹配。 inlier：内点，函数最终给出的匹配可信的点。 RANSAC只采用少数几个随机点来计算PnP，容易受到噪声影响。工程上通常使用RANSAC的解作为初值，再使用非线性优化方法求解最优值。 12345678910111213// Ransac粗匹配cv::solvePnPRansac( pts3d, pts2d, K, Mat(), rvec, tvec, false, 100, 4.0, 0.99, inliers );cv::Rodrigues ( rvec, R ); Eigen::Matrix3d rotation_matrix = R.at&lt;double&gt;;T_c_r_estimated_ = SE3d( SO3d(rotation_matrix), Vector3d( tvec.at&lt;double&gt;(0,0), tvec.at&lt;double&gt;(1,0), tvec.at&lt;double&gt;(2,0)) ); // BA局部优化g2o::VertexSE3Expmap* pose new g2o::VertexSE3Expmap();...pose-&gt;setEstimate(g2o::SE3Quat( T_c_r_estimated_.rotationMatrix(), T_c_r_estimated_.translation()));]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[karto key concepts]]></title>
    <url>%2F2018%2F06%2F05%2Fkarto-key-concepts%2F</url>
    <content type="text"><![CDATA[基于极其标准的图优化SLAM框架来实现： 提出了采用稀疏点调整（SPA）的方法来高效求解位姿图优化问题，针对本算法的文献为《Efficient Sparse Pose Adjustment for 2D Mapping》。 scan matching部分的参考文献为《Real-time correlative scan matching》，M3RSM的前身！ key concepts： keyScan：机器人运动一定的距离或角度（关键帧），储存在sensorManager，无地图缓存。 look-up table查找表：查找表的意义就是相比于暴力匹配，不需要每次都重新计算每个激光数据信息，相同角度不同位置的激光数据信息只需要被索引一次。 response响应值：将查找表以一定的位移投到子图上，总共有n个点被查找表击中（hit），击中的每个点得分不同（score），累加得分并除以可以达到的最高分。 response = \frac{\sum_{i=0}^n goal_i}{goalmax} 协方差：文献专门用了一节计算协方差，但是没看到用在哪，是为了后面求误差做准备吗？？？ addScans添加顶点和边：边是误差值，添加的边约束来自两部分， （1）link to running scans，距当前帧一定范围内的激光数据链（RunningScan chain）。 （2）link to other near chains，从当前节点开始广度优先遍历一定距离范围内所有节点，依据当前id从sensorManager中分别递增和递减寻找一定范围内的chain（不一定直接相连）。 回环检测：操作与添加边约束类似，位姿图上要去除那些和当前节点的时间相邻的节点。 （1）找到一定距离范围内（near）和相连（adjacent）的节点添加进nearLinkedScans。 （2）MapperGraph::FindPossibleLoopClosure：从sensorManager中从前到后，依据序号挑选与当前节点在一定距离范围内，且不在nearLinkedScans中的candidate。返回潜在chain。其中涉及两个参数： LoopSearchMaximumDistance：candidateScan与当前scan的距离必须在可容许的距离内。 LoopMatchMinimumChainSize：chain中的节点数必须不小于限定值。 （3）MapperGraph::TryCloseLoop：scan2map匹配，当response和covariance达到一定要求认为闭环检测到，得到correct pose（也就是认为candidateScan的pose才是当前帧的实际pose）。 （4）add link to loop，构成全局闭环。 （5）触发correctPose，进行spa优化。 代码随手记： ROS上面提供三个开源包：nav2d_karto, open_karto, slam_karto。 ROS Wiki上这么描述nav2d_karto这个package：Graph-based Simultaneous Localization and Mapping module. Includes OpenKarto GraphSLAM library by “SRI International”. open_karto：开源的karto包，实现底层的kartoslam slam_karto：ros层，应用层的kartoslam接口 The LaserRangeFinder contains parameters for physical laser sensor used by the mapper for scan matching Also contains information about the maximum range of the sensor and provides a threshold for limiting the range of readings. The optimal value for the range threshold depends on the angular resolution of the scan and the desired map resolution. resolution：0.25 &amp; 0.5 &amp; 1 degree number of range readings (beams)：（maximumAngle - minimumAngle）／angularResolution + 1 GridStates：0 for Unknown，100 for Occupied， 255 for Free。 flipY：最开始机器人应该处在世界坐标系的原点，传感器坐标系与机器人baselink存在一个坐标变换，原始的传感器坐标系位置应该与地图坐标系重合，这就是world和grid之间的offset。flip是啥呢？？ LookupArray[index]：Create lookup tables for point readings at varying angles in grid. This is to speed up finding best angle/position for a localized range scan MapperGraph：花式构造位姿图 CorrelationGrid：Implementation of a correlation grid used for scan matching Region of Interest ROI： smear：The point readings are smeared by this value in X and Y to create a smoother response. 个人理解这句话是说点容易生成突变，用以点为中心的一小片区域平滑一点。 ScanMatch：返回响应值response 前端匹配调用m_pSequentialScanMatcher-&gt;MatchScan 闭环检测调用m_pLoopScanMatcher-&gt;MatchScan 两个函数继承于ScanMatcher::MatchScan： 12345678kt_double ScanMatcher::MatchScan( LocalizedRangeScan* pScan, const LocalizedRangeScanVector&amp; rBaseScans, Pose2&amp; rMean, Matrix3&amp; rCovariance, kt_bool doPenalize, kt_bool doRefineMatch) // default is True, 决定是否做精匹配// @return: strength of response (best response) 其中会调用ScanMatcher::CorrelateScan方法。ScanMatcher::CorrelateScan方法中调用ScanMatcher::GetResponse方法计算响应值。 123kt_double ScanMatcher::GetResponse( kt_int32u angleIndex, kt_int32s gridPositionIndex) const GetResponse的核心在kt_int8u* pByte和const LookupArray* pOffsets两个数据结构： 前者是在correlationGrid范围内的real sensed占据情况。 后者是lookup-table中（已知地图）读取的栅格占据情况，只包含占据的栅格，key是angular。 计算response只要看地图上的占据点是否在观测中是否也是占据的： 123456789101112for (kt_int32u i = 0; i &lt; pOffsets-&gt;GetSize(); i++)&#123; // ignore points that fall off the grid kt_int32s pointGridIndex = gridPositionIndex + pAngleIndexPointer[i]; if (!math::IsUpTo(pointGridIndex, m_pCorrelationGrid-&gt;GetDataSize()) || pAngleIndexPointer[i] == INVALID_SCAN) &#123; continue; &#125; // uses index offsets to efficiently find location of point in the grid response += pByte[pAngleIndexPointer[i]];&#125; 最终的response要normalize： 123// normalize responseresponse /= (nPoints * GridStates_Occupied); // GridStates_Occupied = 100,assert(fabs(response) &lt;= 1.0); karto只在闭环的时候触发后端优化CorrectPoses()，ScanSolver的实现在Samples路径下的SpaSolver，调用了现有的BA求解器sba(A Generic Sparse Bundle Adjustment C/C++ Package Based on the Levenberg-Marquardt Algorithm)。 参数&amp;优化方向 闭环中candidate数量的调整： 减小LoopSearchMaximumDistance，进入candidate范围的节点数据减少 减小LoopMatchMinimumChainSize，用来计算优化的candidate数量减少 增大minimum_travel_distance和minimum_travel_heading，这样总体的节点数减少 Map_update_interval：发布地图的间隔，其主要过程是遍历当前所有节点数据，对每个栅格的占有状态进行判定，生成栅格地图。 ScanBufferSize和ScanBufferMaximumScanDistance：控制buffer也就是chain的大小。chain不能太大也不能太小，太小会造成前端误差累积，太大会导致构建闭环的节点数不足。推荐值是ScanBufferMaximumScanDistance／minimum_travel_distance。 位姿纠正中： CorrelationSearchSpaceDimension：The size of the search grid CorrelationSearchSpaceResolution：The size of the correlation grid 回环检测中： LoopSearchMaximumDistance：闭环检测的搜索距离，数值越大能越早发现闭环，也能容忍更大的偏离误差。 LoopMatchMinimumResponseCoarse和LoopMatchMinimumResponseFine：粗匹配和精匹配的响应阈值，与闭环中candidate数量相关。阈值过低会导致candidate迅速被填满，真正好的点还没找到。阈值过高会导致回环失败（一直找不到回环点），地图上出现重影。 CPU Usage算法资源占用的主要压力来源： 地图更新 回环检测 SPA优化]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode]]></title>
    <url>%2F2018%2F06%2F04%2Fleetcode%2F</url>
    <content type="text"><![CDATA[lc79挂一道很猥琐的题，二维网格中搜索单词，同一单元格不能重复使用： 12345678910board =[ [&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;E&apos;], [&apos;S&apos;,&apos;F&apos;,&apos;C&apos;,&apos;S&apos;], [&apos;A&apos;,&apos;D&apos;,&apos;E&apos;,&apos;E&apos;]]给定 word = &quot;ABCCED&quot;, 返回 true.给定 word = &quot;SEE&quot;, 返回 true.给定 word = &quot;ABCB&quot;, 返回 false. 没啥好算法，就是DFS，但是坑在于visited的存储，python数组默认浅拷贝，递归传进去再回到上一层网格状态就变了，之前一贯的做法就是新开一块内存空间，传新的数组进去，然而这次超时了，因为测试用例的二维数组尺寸贼大，终于有机会正视这个问题，并获取正确的打开方式： 尺寸贼大的二维数组，每次只需要修改一个值，重新划空间拷贝再修改时间复杂度瞬间增大O(m*n)倍，很明显传原来的数组进去比较合适。 但是深层递归会修改传进去的参数，因此在每次递归之前先创建一个tmp，记录修改行为，递归函数进行完以后，再根据记录恢复原来的参数，保证本层参数不变。 123456789101112131415161718def search_tail(board, word, h, w): size = len(word) char = word[0] height, width = len(board), len(board[0]) exist = False if h - 1 &gt;= 0 and board[h-1][w] == char: if size == 1: return True else: tmp = board[h-1][w] board[h-1][w] = 'INF' exist = search_tail(board, word[1:], h-1, w) if exist: return True board[h-1][w] = tmp if h + 1 &lt; height and board[h+1][w] == char: ... ... 然后针对本题还有一个骚操作，很多人专门创建一个visited表来记录搜索路径，但是因为本题的二维数组限定存储字母，所以任意一个非字母都可以作为标志位，美滋滋又省下一个O(m*n)。 lc81实名diss这道题，一看是旋转排序数组直接做了，然后才发现测试样例里面出现了左右边界重合的情况，然后仔细再审题才发现这行小字： 这是搜索旋转排序数组的延伸题目，本题中nums可能包含重复元素 不包含重复元素的情况下代码实现如下： 12345678910111213141516171819202122232425class Solution: def search(self, nums, target): size = len(nums) start = 0 end = size - 1 while start &lt;= end: mid = (start + end) // 2 if nums[mid] == target: return True if nums[mid] &lt;= nums[end]: if target &lt; nums[mid] or target &gt; nums[end]: end = mid - 1 else: start = mid + 1 else: if target &gt; nums[mid] or target &lt; nums[start]: start = mid + 1 else: end = mid - 1 return False 因为数组中现在存在重复的元素，因此有一个特殊的情况：左右边界值相同，并且nums[mid]值与边界值也相同，这时nums[mid]可能位于两段数组的任意一边。因此要独立讨论一下这个情况： 1234567891011121314151617181920212223242526272829class Solution: def search(self, nums, target): size = len(nums) start = 0 end = size - 1 while start &lt;= end: mid = (start + end) // 2 if nums[mid] == target: return True if nums[mid] &lt; nums[end]: if target &lt; nums[mid] or target &gt; nums[end]: end = mid - 1 else: start = mid + 1 elif nums[mid] &gt; nums[end]: if target &gt; nums[mid] or target &lt; nums[start]: start = mid + 1 else: end = mid - 1 # nums[mid] = nums[end]的情况 else: end -= 1 return False 测试用时50ms，因为边界重复的循环没有有效地二分数组，但是思路贼简单啊。 lc9596和95都是二叉搜索树，先做的96，求树的结构有几种，没注意结点大小关系，用动态规划来做，$dp[i] = dp[0]dp[i-1] + … + dp[i-1]dp[0]$。注意递归调用的时间复杂度，自底向上来算： 1234567891011def numTrees(self, n): res = [1, 1] if n &lt; 2: return res[n] res += [0]*(n-1) for i in range(2, n+1): for j in range(i): res[i] += res[j]*res[i-1-j] return res[n] 95要列出结构了，才发现什么是二叉搜索树来着——左子树的结点值均小于根节点，右子树的结点值均大于根节点。按照惯例，求数量用DP，求枚举则用DFS： 遍历每一个数字$i$作为根节点，那么$[1, 2, …, i-1]$构成其左子树，$[i+1, i+2, …, n]$构成其右子树。 123456789101112131415161718192021def generateTrees(self, n): if n == 0: return [] return self.dfs(1,n)def dfs(self, b, e): if b &gt; e: return [None] res = [] for i in range(b, e+1): # set as the root node leftTrees = self.dfs(b, i-1) rightTrees = self.dfs(i+1, e) for left in leftTrees: for right in rightTrees: root = TreeNode(i) root.left = left root.right = right res.append(root) return res lc28实现 strStr() 函数。给你两个字符串 haystack 和 needle ，请你在 haystack 字符串中找出 needle 字符串出现的第一个位置（下标从 0 开始）。如果不存在，则返回 -1 。 KMP算法，两个难点： O(m)计算needle中每个子串 needle[:k]的最长前缀 O(n)计算无回溯匹配needle 【先分析第一个： needle的长度为m，那么维护一个等长的前缀数组next，next[0]=0，首字母没有真正意义的前/后缀 对于子串needle[:i]，首先查看子串needle[:i-1]的前缀长度，j=next[i-1] 如果needle[j]=needle[i]，next[i]=j+1 如果不相等，我们目前有needle[:i-1]的前/后缀，needle[0:j]和needle[i-j:i]，显然needle[:i]的前缀只能是needle[0:j]的前缀子串，最长为needle[0:j-1]，同时它还得是needle[i-j:i]的后缀子串，说白了就是在子串needle[0:j]中要最大匹配前/后缀：迭代j=next[next[i-1]-1]，如果子串没有共享前/后缀就是0 12345678910next = [0] * len(needle)for i in range(1,len(needle)): j = next[i-1] while j&gt;0 and needle[i]!=needle[j]: j = next[j-1] if needle[i]==needle[j]: next[i] = j+1print('next', next) 【再分析查找阶段，对于给定字符串haystack，暴力解法是从头开始匹配，遇到mismatch就回溯到起始点下一位重新开始匹配，这样的时间复杂度是O(nm)，之所以能够做到无回溯匹配，就是因为有了前缀数组，当我们匹配到needle的一半遇到mismatch了，对于已经匹配上的子串，不需要回溯到起始位置，而是可以从共享前/后缀开始匹配， 123456789needle_idx = 0for i in range(len(haystack)): while needle_idx and haystack[i]!=needle[needle_idx]: needle_idx = next[max(0,needle_idx-1)] if haystack[i]==needle[needle_idx]: needle_idx += 1 if needle_idx&gt;=len(needle): return i-len(needle)+1return -1]]></content>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kd-tree]]></title>
    <url>%2F2018%2F06%2F01%2Fkd-tree%2F</url>
    <content type="text"><![CDATA[Reference：Bentley J L. Multidimensional Binary Search Trees Used for Associative Searching[J]. Communications of the Acm, 1975, 18(9):509-517. 前面更新basic ICP的时候留了一个坑——最近邻的求法。线性扫描？手动挥手。 1. 先说说距离吧1.1 欧式距离 d(x,y) = \sqrt{(x_1 - y_1)^2 + (x_2-y_2)^2 + ... + (x_n-y_n)^2} = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}1.2 标准化欧式距离 首先将数据各维度分量都标准化到均值为0，方差为1，再求欧式距离： X_{stand} = \frac{X - \mu}{\sigma}简单推导后可以发现，标准化欧式距离实际上就是一种加权欧式距离： d(x,y) = \sqrt{\sum_{i=1}^n (\frac{x_i-y_i}{s_i})^2}1.3 马氏距离（Mahalanobis Distance） D(X_i,X_j) = \sqrt{(X_i - X_j)S^{-1}(X_i-X_j)}其中$S$为协方差矩阵$Cov$： Cov(X,Y) = E\{[X-E(X)][Y-E(Y)]\}若协方差矩阵是单位阵（样本各维度之间独立同分布），那公式就变成欧式距离了，若协方差矩阵是对角阵，那公式就变成标准化欧式距离了。 1.4 相似度 相似度也是距离的一种表征方式，距离越相近，相似度越高。 欧式距离相似度：将欧式距离限定在0 1之间变化 相似度 = \frac{1}{1+欧式距离} 余弦相似度：-1到1之间变化 cos\theta = \frac{A\cdot B}{||A||\ ||B||} 皮尔逊相关系数：-1到1之间变化 1return numpy.corrcoef(A, B) 2. KD树 k-NN算法 推荐系统 SIFT特征匹配 ICP迭代最近点 最近在做的M3RCM中的堆结构（这个有点牵强，因为不是二叉树） 总之以上这些基于匹配／比较的目的而进行的数据库查找／图像检索，本质上都可以归结为通过距离函数在高维矢量之间的相似性检索问题。 一维向量有二分法查找，对应地高维空间有树形结构便于快速检索。利用树形结构可以省去对大部分数据点的搜索，从而减少检索的计算量。 KD树是一种二叉树，通过不断地用垂直于某坐标轴的超平面将k维空间切分构造而成。 2.1 构造树 递归创建节点：节点信息包含切分坐标轴和切分点，从而确定超平面，将当前空间切分为左右两个子空间，递归直到当前子空间内没有实例为止。 123456class Node: def __init__(self, point, axis): self.value = point self.axis = axis self.left = None self.right = None 为了使得构造出的KD树尽可能平衡（高效分割空间）： 选择坐标轴：简单点的方式是循环交替选择坐标轴，复杂点的做法是选择当前方差最大的轴作为切分轴。 选择切分点：取选定坐标轴上数据的中值作为切分点。 注意：KD树的构造旨在高效分割空间，其叶子节点并非是最近邻搜索等应用场景的最优解。 12345678910def kdTree(points, depth): if len(points) == 0: return None axis = depth % len(points[0]) points.sort(key=lambda x: x[axis]) cut_idx = centreValue(points) node = Node(points[cut_idx], axis) node.left = kdTree(points[:cut_idx], depth+1) node.right = kdTree(points[cut_idx+1:], depth+1) return node 对于包含n个实例的k维数据来说，构造KD树的时间复杂度为O(k*n*log n)。 2.2 新增节点 递归实现：从根节点开始做比较，大于则插入左子树，小于则插入右子树。直到达到叶子节点，并创建新的叶子节点。 2.3 删除节点 将待删除的节点的所有子节点组成一个集合，重新构建KD子树，替换待删除节点。 2.4 最近邻搜索 搜索最近邻算法主要分为两部分：首先是深度优先遍历，直到遇到叶子节点，生成搜索路径。 12345678910111213141516171819202122232425def searchNearest(node, target): # input: node: root node of the tree # target: list # output: nearest: list # dist: distance between target and nearest if node == None: return None # 生成搜索路径 search_path = deque() nearest = node print("search path: ") while node: print(node.value) search_path.append(node) # if Dist(nearest.value, target) &gt; Dist(node.value, target): # nearest.value = node.value # minDist = Dist(node.value, target) axis = node.axis if target[axis] &gt; node.value[axis]: node = node.right else: node = node.left ... ... 然后是回溯查找，如果目标点和当前最近点构成的球形区域与其上溯节点相交，那么就有一种潜在的可能——上溯节点的另一个子空间的实例可能位于当前这个球形区域内，因此要进行一次判断。 1234567891011121314151617181920212223242526def searchNearest(node, target): # input: node: root node of the tree # target: list # output: nearest: list # dist: distance between target and nearest ... ... # 回溯 print("\nsearch backwards: ") nearest = search_path.pop() minDist = Dist(nearest.value, target) while search_path: node = search_path.pop() print(node.value) if node.axis: axis = node.axis if minDist &gt; Dist1(node.value[axis], target[axis]): if target[axis] &gt; node.value[axis]: search_path.append(node.left) else: search_path.append(node.right) if Dist(target, nearest.value) &gt; Dist(node.value, target): nearest = node minDist = Dist(node.value, target) return nearest.value, minDist 两个参考点： 123samples = [(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)]target = (2.1, 3.1)target = (2, 4.5) KD树搜索的核心就是：当查询点的邻域与分割超平面两侧空间交割时，需要查找另一侧子空间！！！算法平均复杂度O(N logN)。实际时间复杂度与实例分布情况有关，$t_{worst} = O(kN^{1-\frac{1}{k}})$，通常要求数据规模达到$N \geq 2^D$才能达到高效的搜索。 3. 改进算法：BBF算法回溯是由查询路径决定的，因此一种算法改进思路就是将查询路径上的结点排序，回溯检查总是从优先级最高的树节点开始——Best-Bin-First BBF算法。该算法能确保优先检索包含最邻近点可能性较高的空间。 优先队列：优先级取决于它们离查询点的距离，距离越近，优先级越高，回溯的时候优先遍历。 对回溯可能需要路过的结点加入队列：切分的时候，把未选中的那个兄弟结点加入到队列中。]]></content>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Real-Time loop Loop Closure in 2D Lidar SLAM 论文笔记]]></title>
    <url>%2F2018%2F05%2F27%2FReal-Time-loop-Loop-Closure-in-2D-Lidar-SLAM%2F</url>
    <content type="text"><![CDATA[文章的核心思想在于解决loop closure问题。 全局地图由一系列的submap构成，每个submap则由一系列的位姿节点及对应的scan数据构成。 文章的重点在第四部分和第五部分： 第四部分：local 2d slam，将scan与当前submap的匹配问题转化成一个最小二乘优化问题，由ceres来求解。参考文献《Many-to-Many Multi-Resolution Scan Matching 》 第五部分：closing loop，采用SPA进行后端loop closure，提出一种并行的scan与finished submaps匹配的方法BBS，大幅提高精度和速度。参考文献《Efficient Sparse Pose Adjustment for 2D Mapping(SPA)》、《Real-Time Correlative Scan Matching(BBS)》]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三维刚体运动 & 李代数]]></title>
    <url>%2F2018%2F05%2F12%2F%E4%B8%89%E7%BB%B4%E5%88%9A%E4%BD%93%E8%BF%90%E5%8A%A8-%E6%9D%8E%E4%BB%A3%E6%95%B0%2F</url>
    <content type="text"><![CDATA[0. 向量 坐标：首先确定一个坐标系，也就确定了一组基$(e_1, e_2, e_3)$，那么向量$a$的坐标为： a = [e_1, e_2, e_3] \begin{bmatrix} a_1\\ a_2\\ a_3 \end{bmatrix} = a_1e_1 + a_2e_2 + a_3e_3 内积：对向量$a, b \in R^3$，其内积为： a \cdot b = a^Tb = \Sigma a_ib_i = |a||b|cos内积可以描述向量间的投影关系。 外积： a \times b = \begin{vmatrix} i & j & k\\ a_1 & a_2 & a_3\\ b_1 & b_2 & b_3 \end{vmatrix}= \begin{bmatrix} 0 & -a_3 & a_2\\ a_3 & 0 & -a_1\\ -a_2 & a_1 & 0 \end{bmatrix}b=a^{\wedge}b外积的方向垂直与这两个向量，大小为$|a||b|sin$。 外积可以表示向量的旋转，向量$a$到$b$的旋转向量，外积的方向是旋转向量的方向，大小由夹角决定。 1. 旋转矩阵R与变换矩阵T 通常设置固定的世界坐标系$O_w$和运动的相机坐标系$O_c$，相机运动是刚体运动，两个坐标系之间的变换称为欧式变换。 旋转矩阵$R$：可以描述相机的旋转 坐标系旋转前后同一个向量的坐标变换关系： a =\begin{bmatrix} a_1\\ a_2\\ a_3 \end{bmatrix}= \begin{bmatrix} e_1^T\\ e_2^T\\ e_3^T \end{bmatrix} \begin{bmatrix} e_1^{'} & e_2^{'}& e_3^{'} \end{bmatrix} \begin{bmatrix} a_1^{'}\\ a_2^{'}\\ a_3^{'} \end{bmatrix}= Ra^{'}不难验证旋转矩阵是行列式为1的正交矩阵，因此可以把旋转矩阵的集合特殊正交群定义如下： SO(n) = \{R \in R^{n*n} | RR^T=I, det(R)=1\}相反的旋转： a = Ra^{'}\\ a^{'} = R^{-1}a = R^Ta 欧式变换：包括旋转和平移 a^{'} = Ra + t 齐次坐标：射影几何的概念，每个分量同乘一个非零常数仍然表示同一个点： \tilde{x} = [x,y,z,w]^T=[x/w, y/w, z/w, 1]^T 齐次变换矩阵$T$：使得欧式变换仍旧保持线性关系： \begin{bmatrix} a^{'}\\ 1 \end{bmatrix}= \begin{bmatrix} R & t\\ 0 &1 \end{bmatrix} \begin{bmatrix} a\\ 1 \end{bmatrix} =T \begin{bmatrix} a\\ 1 \end{bmatrix}变换矩阵的集合特殊欧式群： SE(3) = \left\{T= \begin{bmatrix} R & t\\ 0 & 1 \end{bmatrix} \in R^{4*4} | R\in SO(3), t \in R^3 \right\} 2. 旋转向量 Axis-Angle一个旋转只有3个自由度，旋转矩阵R要用9的参数来描述，显然是冗余的。一种紧凑的方式——任何旋转都可以用一个旋转轴$n$和一个旋转角$\theta$来刻画： R = cos\theta I + (1-cos\theta)nn^T + sin\theta n^{\wedge}\\ \theta = arccos(\frac{tr(R)-1}{2})\\旋转轴上的向量在旋转后不发生改变，因此有： Rn = n转轴$n$是旋转矩阵$R$的特征值1对应的特征向量，可以由此来计算转轴$n$。 3. 欧拉角 rpy把旋转分解到3个轴上，rpy角的旋转顺序是ZYX： 首先绕物体的Z轴旋转，得到偏航角yaw 然后绕旋转之后的Y轴旋转，得到俯仰角pitch 绕旋转之后的X轴旋转，得到滚转角roll 万向锁问题：在俯仰角为$\pm 90^{\circ}$时，第一次和第三次旋转使用同一根轴，丢了自由度——奇异性问题。 4. 四元数 q四元数是一种扩展的负数，由一个实部和三个虚部组成，可以把三个虚部脑补成空间中的三根轴： q = q_0 + q_1i + q_2j + q_3k\\ \left\{ \begin{split} & i^2 = j^2=k^2=-1\\ & ij = k, ji = -k\\ & jk = i, kj = -i\\ & ki=j, ik=-j \end{split} \right. 乘以$i$对应着绕$i$轴旋转$180^{\circ}$ 任意的旋转可以由两个互为相反数的四元数表示 与旋转向量$n = [n_x, n_y, n_z]^T, \theta$转换关系： q = [cos\frac{\theta}{2}, n_xsin\frac{\theta}{2}, n_ysin\frac{\theta}{2}, n_zsin\frac{\theta}{2}]^T\\ \left\{ \begin{split} & \theta = 2arccos q_0\\ & [n_x, n_y, n_z]^T = [q_1, q_2, q_3]^T/sin\frac{\theta}{2} \end{split} \right. 与旋转矩阵$R$的关系： R = \begin{bmatrix} 1-2q_2^2 - 2q_3^2 & 2q_1q_2-2q_0q_3 & 2q_1q_3+2q_0q_2\\ 2q_1q_2+2q_0q_3 & 1-2q_1^2 - 2q_3^2 & 2q_2q_3-2q_0q_1\\ 2q_1q_3-2q_0q_2 & 2q_2q_3+2q_0q_1 & 1-2q_1^2 - 2q_2^2 \end{bmatrix}\\ q_0 = \frac{\sqrt{tr(R)+1}}{2}, q_1 = \frac{R_{23}-R_{32}}{4q_0}, q_2 = \frac{R_{31}-R_{13}}{4q_0}, q_3 = \frac{R_{12}-R_{21}}{4q_0} 表示旋转： 空间中点$p = [x, y,z]^T\in R^3$，已知旋转轴角$n,\theta$，旋转之后点坐标变为$p^{‘}$，如果用旋转矩阵描述： p^{'} = Rp四元数$q = [cos\frac{\theta}{2}, nsin\frac{\theta}{2}]$，那么旋转后的点$p^{‘}$可以表示为： p^{'} = qpq^{-1} 5. 李群上面提到了旋转矩阵构成的特殊正交群$SO(3)$和由变换矩阵构成的特殊欧式群$SE(3)$： SO(n) = \left\{R \in R^{n*n} | RR^T=I, det(R)=1\right\} \\ SE(3) = \left\{T= \begin{bmatrix} R & t\\ 0 & 1 \end{bmatrix} \in R^{4*4} | R\in SO(3), t \in R^3 \right\} $SO(n)$和$SE(n)$对加法不封闭，对乘法是封闭的。 群是一种集合$A$加上一种运算$\ \cdot \ $的代数结构，记作$G = (A, \ \cdot \ )$，群内元素满足封闭性、结合律、幺元、可逆四个条件。 李群是指具有连续性质的群。刚体在空间中能够连续地运动，因此$SO(n)$和$SE(n)$是李群。 6. 李代数6.1 引入 对任意旋转矩阵$R$，都满足$RR^T=I$。把它写成关于时间的函数$R(t)$有： R(t)R(t)^T = I对等式两边求导： \dot R(t)R(t)^T + R(t)\dot R(t)^T=0\\ \dot R(t)R(t)^T =-\big(\dot R(t)R(t)^T \big)^T可以看出$\dot R(t)R(t)^T $是一个反对称阵，对任意一个反对称阵，都可以找到一个与之对应的向量： a^{\wedge} = A, A^{\vee}=a于是可以找到一个三维向量$\phi(t) \in R^3$与之对应： \dot R(t)R(t)^T = \phi(t)^{\wedge}\\ \dot R(t) = \phi(t)^{\wedge}R(t)可以看到，每对旋转矩阵求一次导数，只需左乘一个反对称阵$\phi(t)^{\wedge}$即可。 求解上面的微分方程，可以得到$R(t) = exp(\phi^{\wedge}t)$。也就是说$\phi$描述了$R$在局部的导数关系。 6.2 李代数 每个李群都有与之对应的李代数。李代数描述了李群的局部性质。 李代数由一个集合$V$，一个数域$F$，和一个二元运算李括号$[,]$组成，记作$( V, F, [,])$。李代数的元素满足封闭性、双线性、自反性、雅可比等价四条性质。 上一节的$\phi$就是$SO(3)$对应的李代数$so(3)$，两者的关系由指数映射给定： R = exp(\phi^{\wedge})\\ so(3) = \left\{ \phi \in R^3, \Phi = \phi^{\wedge} \in R^{3*3}\right\} $SE(3)$对应的李代数$se(3)$位于$R^6$空间中： se(3) = \left\{ \xi = \begin{bmatrix} \rho\\ \phi \end{bmatrix} \in R^6, \rho \in so(3), \xi^{\wedge} = \begin{bmatrix} \phi^{\wedge} & \rho\\ 0^T & 0 \end{bmatrix} \in R^{4*4} \right\} 指数映射 由于$\phi$是一个三维向量，因此可以写作$\theta a$的形式，$a$是一个单位向量，因此具有以下性质： a^{\wedge}a^{\wedge} = aa^T-I\\ a^{\wedge}a^{\wedge}a^{\wedge} = -a^{\wedge}对$so(3)$李代数的指数映射做泰勒展开，可以得到： \begin{split} &R= exp(\phi^{\wedge}) = exp(\theta a^{\wedge})=\Sigma_{n=0}^{\infty}\frac{1}{n!} (\theta a^{\wedge})^n\\ & =cos\theta I + (1-cos\theta)aa^T+sin\theta a^{\wedge} \end{split}可以看到$so(3)$实际上就是旋转向量组成的空间，指数映射即是罗德里格斯公式。 指数映射是一个满射，每个$SO(3)$中的元素，都可以找到至少一个$so(3)$元素与之对应（$\theta + 2k\pi$）。 $se(3)$上的指数映射为： T = exp(\xi^{\wedge}) = \begin{bmatrix} R & J\rho\\ 0 &1 \end{bmatrix}\\ J = \frac{sin\theta}{\theta}I + (1-\frac{sin\theta}{\theta})aa^T + \frac{1-cos\theta}{\theta}a^{\wedge} 6.3 李代数求导 两个李代数指数映射乘积的完整形式由BCH公式给出： ln(exp(A)exp(B)) = A+B + \frac{1}{2}[A, B] + \frac{1}{12}[A,[A,B]] + ... 对$ln(exp(\phi_1^{\wedge})exp(\phi_2^{\wedge}))^{\vee}$，当$\phi_1$或$\phi_2$为小量时，BCH公式给出了线性近似表达： ln(exp(\phi_1^{\wedge})exp(\phi_2^{\wedge}))^{\vee} = \left\{ \begin{split} J_l(\phi_2)^{-1}\phi_1 + \phi_2\ \ \ \ \ \ \phi_1为小量\\ J_r(\phi_1)^{-1}\phi_2 + \phi_1\ \ \ \ \ \ \phi_2为小量\\ \end{split} \right.BCH近似雅可比$J_l$就是上一节的$J$： J_l = J = \frac{sin\theta}{\theta}I + (1-\frac{sin\theta}{\theta})aa^T + \frac{1-cos\theta}{\theta}a^{\wedge}\\ J_l^{-1} = \frac{\theta}{2}cot\frac{\theta}{2}I + (1-\frac{\theta}{2}cot\frac{\theta}{2})aa^T - \frac{\theta}{2}a^{\wedge}\\ J_r(\phi) = J_l(-\phi)由以上公式说明了李群乘法和李代数加法的近似转换关系。 在$SO(3)、SE(3)$上没有良好定义的加法，而李代数由向量组成，有良好的加法运算。因此在计算位姿的导数时，通常使用李代数解决，李代数求导的两种思路： 李代数求导$\delta \phi$：用李代数表示姿态，然后转化成对李代数求导$\phi + \delta \phi$ \begin{split} &\frac{\partial (Rp)}{\partial R} = \frac{\partial(exp(\phi^{\wedge})p)}{\partial \phi}\\ &= lim \frac{exp((\phi+\delta\phi)^{\wedge})p-exp(\phi^{\wedge})p}{\partial \phi}\\ &=-(Rp)^{\wedge}J_l \end{split} 扰动模型$\Delta R$：对$R$进行扰动，然后对扰动求导$\Delta R R$ \begin{split} &\frac{\partial (Rp)}{\partial R} = lim \frac{exp(\varphi^{\wedge})exp(\phi^{\wedge})p-exp(\phi^{\wedge})p}{\partial \varphi}= -(Rp)^{\wedge}\\ & \frac{\partial Tp}{\partial \delta \xi} = \begin{bmatrix} I & -(Rp+t)^{\wedge}\\ 0 & 0 \end{bmatrix} = (Tp)^{\odot} \end{split}]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++ for record]]></title>
    <url>%2F2018%2F05%2F10%2Fc-for-record%2F</url>
    <content type="text"><![CDATA[最近开始着手写slam代码，看一些常用库源码的时候发现各种力不从心，一些c++11的骚操作竟然没见过，是时候完整撸一发c++ primer祭天了。 iostream 标准输入：cin 标准输出：cout、cerr、clog 123456#include &lt;iostream&gt;using namespace std;int v1=0, v2=0;cin &gt;&gt; v1 &gt;&gt; v2;cout &lt;&lt; v1+v2 &lt;&lt; endl;cerr &lt;&lt; "This is nonsense." &lt;&lt; endl; &lt;&lt; 和 &gt;&gt; 的方向表示了数据流的走向，也就是赋值的方向。cerr用来输出错误信息。 控制流 while：每次执行循环之前先检查循环条件 do while：先执行循环体后检查条件 123456while (condition) statement do statementwhile (condition); for：每次执行循环之前先检查循环条件，执行循环之后执行表达式 1234567for (init-statement; condition; expression)&#123; statemnt&#125;// 范围for语句for (declaration : expression) statement switch： case label：case标签必须是整形常量表达式 如果某个case标签匹配成功，会往后顺序执行所有case分支，直到结尾或者遇到break default标签 123456switch(ch)&#123; case 'a': case 'b': case 'c': ++cnt; break;&#125; break：负责终止离他最近的while、do while、for或switch语句。 continue：负责终止离他最近的while、do while、for循环的当前迭代。 goto：无条件跳转到同一函数内的某个带标签语句。 labeled statement: label: statement 异常 throw：引发异常，后面紧随一个异常类型，终止当前函数，将控制权转移给能够处理该异常的代码。 123#include &lt;stdexcept&gt;// runtime_error 标准库异常类型throw runtime_error("Data must refer to same name"); try：处理异常，后面紧随一套catch子句用来处理异常。 1234567try&#123; program statements&#125; catch (exception-declaration) &#123; handler-statements&#125; catch (exception-declaration) &#123; handler-statements&#125; ... try语句块内声明的变量在块外无法访问，即使是catch语句。 catch一旦完成，程序跳转到最后一个catch子句之后的语句。 类 类型 &amp; 对象（实例），变量 &amp; 行为（方法）。 存在类内默认初始化 类通常被定义在头文件中，头文件名字应与类的名字保持一致 头文件通常包含只能被定义一次的实体，如类、const等。 头文件保护符#ifndef系列，创建预处理变量，防止多次包含。 构造函数初始值列表：冒号以及冒号和花括号之间的代码 列表只说明用于初始化成员的值，而不限定初始化的具体顺序。 成员的初始化顺序与它们在类定义中的出现顺序一致。 12345678910// 为类成员初始化Sales_data(const string &amp;s, unsigned n, double p) : bookNo(s), units_sold(n), revenue(p*n) &#123;&#125; // 区别于赋值Sales_data(const string &amp;s, unsigned n, double p)&#123; bookNo = s; ...&#125; 接口与封装： 定义在private说明符之后的成员只能被类内成员函数访问，封装了类的实现细节。 定义在public说明符之后的成员可以在整个程序内被访问，定义类的接口。 class和struct的区别：成员访问权限 struct：定义在第一个说明符之前的成员是public class：定义在第一个说明符之前的成员是private 友元：允许其他类或函数访问它的非公有成员，在类内添加以friend关键字开始的友元声明。 友元的声明仅仅指定了访问权限，而非一个通常意义上的函数声明。 12345678class Sales_data &#123;// 友元声明friend Sales_data add(const Sales_data&amp;, const Sales_data&amp;);// 非公有成员private: string bookNo; double revenue = 0.0;&#125;; 静态成员static：与类本身相关联，不属于任何一个对象，因此不是在创建类对象的时候被定义的，因此通常在类的外部定义和初始化，在类内部添加以static关键字开始的静态成员声明。 内置类型 内存中的一个地址对应一个字节 unsigned类型表示大于等于0的数（$[0, 2^{n}-1]$），被赋给一个超出表示范围的数时，自动取余，作为循环条件时当心进入无限循环 signed类型正负值范围平衡（$[-2^{n-1}, 2^{n-1}-1]$），被赋给一个超出表示范围的数时，结果未定义 字符型char，单引号，一个字节 字符串型，双引号，常量字符数组，结尾隐含空字符 ‘\0’ nullptr = 0（传统NULL包含在cstdlib头文件内） 变量 列表初始化，花括号 123456// 拷贝初始化int x=0;int x=&#123;0&#125;;// 直接初始化int x&#123;0&#125;;int x(0); 变量声明extern，源于分离式编译机制，一个变量只能被定义一次，可以声明多次 作用域，嵌套作用域 &amp; 内部重定义 复合类型 引用，typename &amp;declaration，浅拷贝，绑定一个对象，引用不是对象 指针，typename *declaration，存放对象地址 1234int a;int *p, *q=a;p = &amp;a;p = q; 取地址符&amp; 123int *p = a;int *p = &amp;a;// a---&gt;对象 &amp;a---&gt;地址 解引用符* 123456int a;int *p;*p ---&gt; undefinedp = &amp;a;*p = 10;// p---&gt;指针 *p---&gt;对象 void* 指针，可以指向任意类型的对象，但是不能进行对象操作 const限定符 参与编译预处理 要实现多个文件共享，必须在const变量定义之前加上extern关键字 1234// defineextern const int bufferSize = fcn();// declareextern const int bufferSize; 允许任意表达式作为初始值（允许隐式类型转换） 常量引用，允许非常量赋值，实际引用一个内存中的“临时值” 指向常量的指针，允许非常量赋值，但是不能通过该指针修改对象 常量指针，指针始终指向同一个对象 常量表达式constexpr，表达式在编译过程中就能得到计算结果 处理类型 类型别名typedef &amp; using 1234567// 传统typedef double base;typedef base *p; // p是double指针base a;p p1=&amp;a;// c++11using base = double; auto类型说明符，让编译器分析表达式所属类型并为变量赋值 12// 一条类型声明语句中所有变量的类型必须保持一致auto i=0, *p=&amp;i; decltype类型指示符，仅分析表达式返回类型，不做赋值（因此不做实际计算） 1decltype(f()) a=x; string 读取，&gt;&gt;不读取空白，遇到空白符停止，getline保留空白符，遇到换行符停止。 字符串字面值不是string对象，而是C风格字符串，c_str()成员函数能够将string对象转化成C风格字符串 遍历，范围for语句，每次迭代declare的变量会被初始化为expression的下一个元素 123456789for (declaration : expression) statementstring str("some string");// 赋值for (auto c: str) cout &lt;&lt; c &lt;&lt; endl;// 引用for (auto &amp;c: str) c = toupper(c); size()返回的类型是string::size_type，通常用auto vector 类模版，相同类型对象的集合，声明时必须提供元素类型vector&lt;int&gt; 添加元素push_back() 迭代器 所有标准库容器都支持迭代器，只有少数支持下标访问 begin()返回指向第一个元素的迭代器，end()返回尾后元素的迭代器 cbegin()和cend()操作类似，返回值是const_iterator，不能修改对象 迭代器的类型是container::iterator和container::const_iterator，通常用auto 解引用迭代器得到对象 箭头运算符-&gt;，结合解引用+成员访问两个操作 迭代器失效：容器改变容量 数组 大小固定，编译的时候维度应该已知，因此必须是常量表达式 不能用做拷贝和赋值 表达式 左值和右值 ​ C语言中，左值指的是既能出现在等号左边也能出现在等号右边的变量或表达式，通常来说就是有名字的变量，而右值只能出现在等号右侧，通常就是一些没有名字也取不到地址的中间结果。 继承到C++中归纳来讲就是：当一个对象被用作右值的时候，用的是对象的值（内容），当被用作左值的时候，用的是对象的身份（在内存中的位置）。 求值顺序 有四种运算符明确规定了求值顺序，逻辑与（&amp;&amp;）、逻辑或（||）、条件（?:）、逗号（,）运算符。 12int i = 0;cout &lt;&lt; i &lt;&lt; ++i &lt;&lt; endl; 前置版本和后置版本的递增递减 用于复合运算中时， 前置版本首先修改对象，然后将对象本身作为左值返回。 后置版本将对象原始值的副本作为右值返回。 位运算 整形提升，char8-&gt;int32 添0，越界丢弃 逗号运算符：含有两个运算对象，首先对左表达式求值，然后将求值结果丢弃掉，最右边的表达式的值将作为整个逗号表达式的值。本质上，逗号的作用是导致一些列运算被顺序执行。 12// 分别对逗号表达式内对象赋值，然后返回最右cnt的值var = (count=19, incr=10, cnt++) 函数 局部静态对象static：首次调用时被初始化，直到程序终止才被销毁。 123456int f()&#123; // 只初始化一次，函数调用结束以后这个值仍有效 static cnt = 0; return ++cnt;&#125; 参数传递：如果形参被声明为引用类型，它将绑定到对应的实参上（传引用调用），否则将实参的值拷贝后赋给形参（传值调用）。 含有可变形参的函数 所有实参类型相同，可以使用initializer_list模版类型的形参，列表中元素是const。 12initializer_list&lt;T&gt; lst;initializer_list&lt;T&gt; lst&#123;a, b, c, ...&#125;; 编写可变参数模版 省略符形参：对应的实参无需类型检查 1234// 带部分形参类型void foo(parm_list, ...);void foo(...); 内联函数incline：避免函数调用开销 调试帮助 NDEBUG预处理变量：用于关闭调试状态，assert将跳过不执行。 assert (expr) 预处理宏：如果表达式为假，assert输出信息并终止程序。 预处理名字由预处理而非编译器管理，因此可以直接使用名字而无须提供using声明。 static_cast和dynamic_cast强制类型转换 static_cast \ (expression)：暴力类型转换，不运行类型检查。 dynamic_cast\ (expression)：运行类型检查，下行转换安全。 new &amp; delete：new [] 要和 delete []对应上。 c++的oop特性（private public…）只在编译时刻有意义。同一类的对象可以互相访问私有成员。 firend：注意方向是give acess to，授权friend访问自己的private。编译时刻检查。 composition：组合，用一系列对象构造对象。 inheritance：继承，用一些类来构造新的类。 1234class A;class B : public A&#123; ....&#125;; 构造：子类构造的时候要先构造父类，析构的时候反过来，先析构子类。 重名：name hiding，special for c++。 ​ protected：designed for sub class。子类可以直接访问。其他类看不到。 overload：参数表必须不同，否则编译器无法识别。 default argument：defaults must be added from right to left。must be declared in .h files。发生在编译时刻。 inline：不用真正调用函数，而是直接插入汇编代码段。tradeoff between space and time consuming。区别于宏，宏是没有类型检查的。 const declare a variable：是变量，而不是常数 变量的生存期限 Automatic：local variables，存活在{}函数定义里面 dynamic：controlled by special instructions，比如main里面实例化的变量/类/内存，最后delete/free它 thread： static：程序运行时间内有效 存储 stack：快，小，用于Automatic heap：慢，大，用于dynamic 指针 比引用灵活：可以在程序存活期间改指别的目标 用于支持底层的各类不连续存储：dynamic memory各种链表 智能指针 std::unique_pointer：每个object只能有一个unique pointer，pointer和target的生存时间相同 std::shared_pointer：每个object可以有多个，只要pointer存在target就存在 std::weak_pointer：同上 c++11里面是shared_ptr、unique_ptr 以及 weak_ptr nullptr：可以当false用，free memory，所以不能解引用，*p会segmentation fault const T*：自由指针 T const*：用read-only way指向value T* const：常数指针，不能改指 T const* const：即不能改指，也不能改数 this指针 available inside member functions：类/函数等{}里面 返回object的地址 this-&gt;能够访问成员 *this表示object本身 unique java里面有garbage collector，扫描那种没有指针的内存地址，然后销毁，但是不及时 c++里面关键字类型可以自动回收 unique_ptr保存在stack里面，但是它创建的对象在heap里面，unique_ptr失效的时候heap里的object也销毁掉 move可以给object切换指针]]></content>
      <tags>
        <tag>basic, c++ primer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cmake for record]]></title>
    <url>%2F2018%2F05%2F08%2Fcmake%2F</url>
    <content type="text"><![CDATA[0. 变量变量使用 ${ } 的方式取值，但是在if控制语句中直接使用变量名。 1. projectproject ( project_name [CXX] [C] [Java] ) 用来指定工程名称和工程语言（可省略），指令隐式定义了projectname_BINARY_DIR和projectname_SOURCE_DIR两个变量（写在cmake_cache里面），指的是编译发生的当前目录。 2. setset ( VAR [VALUE] ) 用来显式定义变量，如set (SRC_LIST main.c t1.c t2.c) 。（竟然不用套括号？） 3. messagemessage ( [SEND_ERROR | STATUS | FATAL_ERROR] “message to display” VAR ) 用来向终端输出用户定义的信息。 4. add_executableadd_executable ( executable_filename [source_filename] ) 生成名字为executable_filename的可执行文件，相关的源文件 [source_filename] 可以是一个源文件列表。 5. 清理构建结果make clean 对构建出的可执行文件进行清理。 6. 外部构建1234mkdir buildcd buildcmake ..make 所有编译动作发生在编译目录，对原有工程没有任何影响。 7. add_subdirectoryadd_subdirectory ( source_dir [binary_dir] [EXCLUDE_FROM_ALL] ) 向当前工程目录添加存放源文件的子目录source_dir，并指定存放中间二进制文件和目标二进制文件的位置binary_dir。指令隐式修改 EXECUTABLE_OUTPUT_PATH 和 LIBRARY_OUTPUT_PATH 两个变量。 8. 更加像一个工程 创建工程根目录，创建CMakeLists.txt。 1234567# 指定最低编译版本cmake_minimum_required(VERSION 3.7)# 指定工程名字PROJECT(HELLO)# 测试类打印信息MESSAGE(STATUS "This is BINARY dir " $&#123;HELLO_BINARY_DIR&#125;)MESSAGE(STATUS "This is SOURCE dir " $&#123;HELLO_SOURCE_DIR&#125;) 添加子目录src，用来存放源文件，为子目录创建CMakeLists.txt。 123# 在根目录CMakeLists.txt中添加子目录声明add_subdirectory(src bin)# 编译产生的中间文件以及目标文件将保存在编译文件夹的bin子目录下 12345# 编写当前子目录的CMakeLists.txtadd_executable(hello main.c)# 修改最终生成的可执行文件以及库的路径，这两个指令要追随对应的add_executable()和add_library()指令set(EXECUTABLE_OUTPUT_PATH $&#123;PROJECT_BINARY_PATH&#125;/bin)set(LIBRARY_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;/lib) 添加子目录build，作为外部编译文件夹（ ${PROJECT_BINARY_DIR} ），存放编译的过程和目标文件。 123cd buildcmake ..make 添加子目录doc，用来存放工程文档hello.txt。 添加文本文件README，COPYRIGHT。 添加runhello.sh脚本，用来调用可执行文件hello。 9. 打包安装 在根目录的CMakeList.txt中添加安装信息 123456# 安装COPYRIGHT/README到&lt;prefix&gt;/share/doc/cmake/t2INSTALL(FILES COPYRIGHT README DESTINATION share/doc/cmake/t2)# 安装runhello.sh到&lt;prefix&gt;/binINSTALL(PROGRAMS runhello.sh DESTINATION bin)# 安装工程文档到&lt;prefix&gt;/share/doc/cmake/t2INSTALL(DIRECTORY doc/ DESTINATION share/doc/cmake/t2) 在子目录的CMakeList.txt中添加安装信息 1234# 安装脚本要调用的可执行文件hello到&lt;prefix&gt;/bin，# 注意install(targets)指令也要追随对应add_executable()和add_library()指令的路径INSTALL(TARGETS hello RUNTIME DESTINATION bin) 安装程序包 12345678910cd build# 在cmake命令中指明安装目录的前缀&lt;prefix&gt;# CMAKE_INSTALL_PREFIX 默认是/usr/localcmake -DCMAKE_INSTALL_PREFIX=/Users/carrol/tmp ..makemake install# 查看目标文件夹j tmptree -a 10. add_libraryadd_library ( name [SHARED | STATIC | MODULE] [source_filename] ) 生成名字为libname.X的库文件。 SHARED，动态库，libname.dylib STATIC，静态库，libname.a 设置目标动态库和静态库同名 set_target_properties 12345# 设置目标动静态库同名add_library(hello SHARED hello.c)add_library(hello_static hello.c)set_target_properties(hello_static PROPERTIES OUTPUT_NAME hello) 防止构建中清理同名文件 set_target_properties cmake在构建一个target时，会尝试清理掉其他使用这个名字的库——在构建libhello.a时会清理掉libhello.dylib。 我实际操作时候会保留两个库文件，但是在作为第三方被引用的时候会报错： dyld: Library not loaded: libhello.dylib Reason: image not found 1234SET_TARGET_PROPERTIES(hello PROPERTIES CLEAN_DIRECT_OUTPUT 1)SET_TARGET_PROPERTIES(hello_static PROPERTIES CLEAN_DIRECT_OUTPUT 1) 设置动态版本号 set_target_properties 1234# 设置动态库版本号set_target_properties(hello PROPERTIES VERSION 1.2 SOVERSION 1) 编译文件夹下生成了libhello.1.2.dylib、libhello.1.dylib、libhello.dylib三个动态库文件，只有一个是真的，另外两个是替身。 安装共享库和头文件 修改库的源文件夹下的CMakeLIsts.txt 123456# 库文件install(TARGETS hello hello_static ARCHIVE DESTINATION lib //静态库 LIBRARY DESTINATION lib) //动态库# 头文件install(FILES hello.h DESTINATION include/hello) 11. include_directoriesinclude_directories( dir1 dir2 … ) 用来向工程添加多个特定的头文件搜索路径 12. link_directories &amp; target_link_librarieslink_directories( dir1 dir2 … ) 添加非标准的共享库搜索路径 target_link_libraries( target lib1 lib2 … ) 用来为目标target添加需要链接的共享库，target可以是一个可执行文件，也可以是一个库文件。 查看生成目标的库依赖情况 12345# 生成的目标可执行文件为main# for OSXotool -L main# for linuxldd main 只能列出动态库。 13. 常用变量PROJECT_BINARY_DIR：编译发生的目录 PROJECT_SOURCE_DIR：工程顶层目录 CMAKE_CURRENT_SOURCE_DIR：当前CMakeLists.txt所在目录 CMAKE_MODULE_PATH：自定义的cmake模块所在路径 LIBRARY_OUTPUT_PATH：重定义目标库文件存放目录 EXECUTABLE_OUTPUT_PATH：重定义目标可执行文件存放目录 14. findNAME.cmake模块 在工程目录中创建cmake文件夹，并创建FindHELLO.cmake模块： 123456789101112131415# 示例FIND_PATH(HELLO_INCLUDE_DIR hello.h /usr/local/include/hello)FIND_LIBRARY(HELLO_LIBRARY hello /usr/local/lib)IF (HELLO_INCLUDE_DIR AND HELLO_LIBRARY) SET(HELLO_FOUND TRUE)ENDIF (HELLO_INCLUDE_DIR AND HELLO_LIBRARY)IF (HELLO_FOUND) IF (NOT HELLO_FIND_QUIETLY) MESSAGE(STATUS "Found Hello: $&#123;HELLO_LIBRARY&#125;") ENDIF (NOT HELLO_FIND_QUIETLY)ELSE (HELLO_FOUND) IF (HELLO_FIND_REQUIRED) MESSAGE(FATAL_ERROR "Could not find hello library") ENDIF (HELLO_FIND_REQUIRED)ENDIF (HELLO_FOUND) 在主目录CMakeLists.txt中添加cmake模块所在路径： 12# 为find_package()指令成功执行set(CMAKE_MODULE_PATH $&#123;PROJECT_SOURCE_DIR&#125;/cmake) 然后就可以在源文件CMakeLists.txt中调用 find_package： find_package ( name [QUIET] [REQUIRED] ) 用来调用预定义在CMAKE_MODULE_PATH下的Find\.cmake模块。 每一个模块都会定义以下几个变量： NAME_FOUND NAME_INCLUDE_DIR or NAME_INCLUDES NAME_LIBRARY or NAME_LIBRARIES 根据指令后面的参数还会有以下变量： NAME_FIND_QUIETLY，如果指定了QUIET参数，就不会执行如下语句： 1MESSAGE(STATUS "Found Hello: $&#123;NAME_LIBRARY&#125;") NAME_FIND_REQUIRED，如果指定了REQUIRED参数，就是指这个共享库是工程必须的，如果找不到，工程就不能编译，对应地会执行如下语句： 1MESSAGE(FATAL_ERROR "Could not find NAME library") 可以通过\_FOUND判断模块是否被找到，并执行不同的操作（如添加非标准路径、输出错误信息等）。 15. find_指令 find_path find_path ( VAR name1 path1 path2 … ) VAR变量代表包含name1文件的路径——路径。 find_library find_library ( VAR name1 path1 path2 …) VAR变量包含找到的库的全路径，包括库文件名——路径下的所有文件。]]></content>
      <tags>
        <tag>basic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ICP, Iterative Closest Points]]></title>
    <url>%2F2018%2F05%2F05%2FICP-Iterative-Closest-Points%2F</url>
    <content type="text"><![CDATA[1 基本实现数据点云配准，最经典的方法就是ICP迭代最近点法。 最近点：欧几里得意义上距离最近的点。 迭代：迭代目标是通过不断更新运动参数，使得两个点云的重叠部分充分吻合。 ICP的求解分为两种方式： 利用线性代数求解（SVD），在给定了匹配的情况下，最小二乘问题实际上具有解析解。 利用非线性优化方式求解，类似于BA方法，适用于匹配未知的情况。 2 SVD方法求解算法推导如下： 首先将点云文件进行粗匹配，如ORB特征点匹配。 从点集$P={\overrightarrow{p_1}, \overrightarrow{p_2}, …, \overrightarrow{p_n}}$中随机选取指定数量的点$\{\overrightarrow{p_t}\}$作为参考点，参考点的数量决定了ICP算法的计算效率和配准精度。 在另一个点集$Q={\overrightarrow{q_1}, \overrightarrow{q_2}, …, \overrightarrow{q_m}}$是待匹配的点query points，那么想要找到一个欧式变换$R, t$，使得$\forall i, p_i = Rq_i + t$。 求解欧式变换$T^k$，使得$E^k=\Sigma| \overrightarrow{p_t} - T^k \overrightarrow{q_t}|^2$最小化。 将空间变换分解为旋转和平移两部分，首先定义两个点云的质心： \overrightarrow{p} = \frac{1}{n} \Sigma \overrightarrow{p_t}, \ \ \overrightarrow{q} = \frac{1}{n} \Sigma \overrightarrow{q_t}，质心,\ 用于描述平移\\ \overrightarrow p_i = \overrightarrow{p_t} - \overrightarrow{p}, \ \ \overrightarrow q_i = \overrightarrow{q_t} - \overrightarrow p，中心化点云,\ 用于描述旋转\\于是有目标函数： \begin{split} E^k & = \Sigma|\overrightarrow{p_t} - T^k \overrightarrow{q_t}|^2 = \Sigma|(p+p_i) -T (q+q_i)|^2\\ & = \Sigma|(p+p_i) -R (q+q_i) -t|^2\\ & = \Sigma |(p_i - Rq_i) + (p - Rq -t)|^2\\ & = \Sigma( |p_i - Rq_i|^2 + |p - Rq -t|^2)\\ & = \Sigma( |p_i - Rq_i|^2\\ J &= \frac{1}{2} \sum e_i = \frac{1}{2} E^k \end{split}对目标函数展开，而且已知旋转矩阵是正交阵，$R^TR=I​$，所以目标函数的前两项都与$R​$无关： R^* = argmin_R J = \frac{1}{2}\sum p_i^Tp_i + q_i^TR^TRq_i - 2p_i^TRq_i只有最后一项与$R$有关，于是得到关于$R$的目标函数： J(R) = \sum_{unrelated} -\ p_i^TRq_i = \sum - \ tr(Rq_ip_i^T) = -tr(R\sum_{i=1}^nq_ip_i^T)然后通过SVD奇异值分解求解上述问题的最优$R$，首先定义$W = \sum_1^n pq^T$，当$W$满秩时： W = \sum_{i=1}^{n} \overrightarrow{p_i}*\overrightarrow{q_i^T} = U \begin{bmatrix} \sigma1 & 0 & 0 \\ 0 & \sigma2 & 0 \\ 0 & 0 & \sigma3 \end{bmatrix} V^T\\ R = UV^T然后间接得到平移$t$： t = {p} - R{q} 代码实现如下： 1234567891011121314151617181920212223242526272829303132333435363738void pose_estimation_3d3d( const vector&lt;Point3f&gt;&amp; pts1, // point cloud 1 const vector&lt;Point3f&gt;&amp; pts2, // point cloud 2 Mat&amp; R, Mat&amp; t, Eigen::Matrix3d&amp; R_, Eigen::Vector3d&amp; t_ )&#123; Point3f p1, p2; // center of Mass int N = pts1.size(); for(int i=0; i&lt;N; i++) &#123; p1 += pts1[i]; p2 += pts2[i]; &#125; p1 /= N; p2 /= N; vector&lt;Point3f&gt; q1(N), q2(N); // remove the COM for(int i=0; i&lt;N; i++) &#123; q1[i] = pts1[i] - p1; q2[i] = pts2[i] - p2; &#125; Eigen::Matrix3d W = Eigen::Matrix3d::Zero(); // calculate W matrix for(int i=0; i&lt;N; i++) &#123; W += Eigen::Vector3d(q1[i].x, q1[i].y, q1[i].z) * Eigen::Vector3d(q2[i].x, q2[i].y, q2[i].z).transpose(); &#125; // SVD decomposition Eigen::JacobiSVD&lt;Eigen::Matrix3d&gt; svd(W, Eigen::ComputeFullU|Eigen::ComputeFullV); // SVD Eigen::Matrix3d U = svd.matrixU(); Eigen::Matrix3d V = svd.matrixV(); // calculate R,t R_ = U * V.transpose(); t_ = Eigen::Vector3d(p1.x, p1.y, p1.z) - R_ * Eigen::Vector3d(p2.x, p2.y, p2.z); 3 非线性优化方法另一种方式是通过迭代的方式来寻找最优值，误差项的表示与上一节相同，用李代数来表达位姿，旋转和平移不用再解耦表示，目标函数为： \xi^* = argmin \frac{1}{2}\sum_{i=1}^n ||p_i - exp(\xi ^{\wedge})q_i||_2^2单个误差项关于位姿的导数可以使用李代数扰动模型来描述： \frac{\partial e}{\partial \delta \xi} = (e)^{\odot} = (p_i - exp(\xi^{\wedge})q_i)^{\odot}其中$p_i$作为参考点，对扰动的导数为0，因此： \frac{\partial e}{\partial \delta \xi} = - (exp(\xi^{\wedge})q_i)^{\odot}将最小二乘问题进行图描述：优化变量为李代数表达的位姿$\xi$，因此图中只有一个节点，误差项为一元边（从当前节点指向当前节点），对误差项做线性展开： e_i(\xi + \delta \xi) = e(\xi) + J(\xi)\delta \xi其中的雅可比矩阵也就是上面说的，单个误差项关于位姿的一阶导数。 4算法优化 删除点云数据采集中产生的噪声及异常值。 查找最近点的过程采用KD-Tree数据结构，减少时间复杂度。]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CLion for record]]></title>
    <url>%2F2018%2F05%2F03%2Fclion%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1 cmake详见cmake for record。 2 简单配置主要就是keymap很不适应，基本上删除了大部分editing的配置，因为可以用vim。剩下的修改主要延续sublime和OSX的习惯。 2.1 搜索全局搜索：cmd + F 剩下的交给vim。 2.2 导航search for file：cmd + O search for class：opt + cmd + O search for symbol：shift + cmd + O go to line：cmd + G back：ctrl + cmd + left forward：ctrl + cmd + right 剩下的交给vim。 2.3 注释代码块注释：shift + cmd + ／ 2.4 智能提示看见小灯泡就：opt + enter 2.5 run &amp; buildrun：cmd + R build：cmd + B 2.6 代码生成insert：cmd + J 最近在熟悉Eigen库，经常要打印东西，加了一个split模版快速分割代码片段。 generate：cmd + N 还有一些vim与ide冲突的键，可以手动选择是服从ide还是vim。]]></content>
      <tags>
        <tag>ide</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Graph-based Optimization]]></title>
    <url>%2F2018%2F05%2F02%2Fgraph-based-optimization%2F</url>
    <content type="text"><![CDATA[1 综述基于图优化的slam主要分为以下三个部分： 前端：基于传感器数据建图，匹配相邻帧，添加节点和边（raw graph），节点表示机器人的位姿，边表示节点之间的位姿联系。位姿信息可以来自里程计计算，可以来自ICP激光点云匹配，也可以来自闭环检测反馈。 后端：优化图，基于历史信息的约束，调整新加入的机器人位姿顶点使其尽量满足边的约束（optimized graph）。 宏观的闭环检测：根据闭环信息优化矫正整个拓扑图。 这里面涉及到了两个优化，一个是后端局部优化，一个是全局闭环优化，两者计算的思路是一样的。 2 优化2.1 全局闭环优化，用于矫正整个拓扑图前端后端完成的事情是探索并创建新的节点，获得新的测量值，添加新的位姿关系方程： \begin{eqnarray} \begin{split} & x_0 + z_{01} = x_1\\ & x_1 + z_{12} = x_2\\ & ...\\ & x_{k-1} + z_{k-1, k} = x_{k}\\ \end{split} \end{eqnarray}而全局闭环检测添加已知节点之间的位姿约束关系： \begin{equation} x_i + z_{i j} = x_j, \ \ \ \ i,j\in [0, k] \end{equation}再添加一个初始条件（不是必须的，但是后面实验表明固定一个顶点比不固定效果要好——相当于有一个明确可信的基准）： x_0 = 0 以上线性方程组中，闭环检测部分的方程中的两个结点都在前面出现过，因此不增加矩阵的秩，因此最终要求解包含$k$个方程$k+1$个未知数的线性方程组。 闭环的关键性：如果没有闭环条件，方程组$Ax=b$左右两边秩是相等的——有唯一解，添加了闭环条件以后，相当于方程组左侧$A$的秩不变，但是右侧$b$的秩则增加了，$rank(A) &lt; rank(A, b)$——没有解析解，只有最优。 实际上状态$\textbf x$是一个包含夹角$\theta$的向量$[x, y, \theta]$，实际相对位姿的计算并非简单的线性叠加： \textbf x \oplus \Delta \textbf x = \begin{pmatrix} x + \Delta x cos\theta - \Delta y sin \theta \\ y + \Delta x sin\theta + \Delta y cos \theta \\ normAngle(\theta + \Delta \theta) \end{pmatrix}举个栗子： 机器人从起始位置$x_0=0$处出发，里程计测得它向前移动了1m，到达$x_1$，接着测得它向后移动了0.8m，到达$x_2$，这时通过闭环检测，发现他回到了起始位置。 首先根据给出信息构建图： x_0 + 1 = x_1\\ x_1 - 0.8 = x_2然后根据闭环条件添加约束： x_2 = x_0补充初始条件： x_0 = 0使用最小二乘法求上述方程组的最优解，首先构建残差平方和函数： \begin{eqnarray} \begin{split} & f_1 = x_0 = 0\\ & f_2 = x_1 - x_0 - 1 = 0\\ & f_3 = x_2 - x_1 + 0.8 = 0\\ & f_4 = x_2 - x_0 = 0 \end{split} \end{eqnarray} c = \Sigma_{i=1}^4 f_i^2 = x_0^2 + (x_1-x_0-1)^2 + (x_2-x_1+0.8)^2 + (x_2-x_0)^2然后对每个参数求偏导： \frac{\partial c}{\partial x_1} = -x_0 + 2x_1-x_2 -1.8=0\\ \frac{\partial c}{\partial x_2} = -x_0 - x_1 +2x_2 + 0.8 = 0解得$x_1 = 0.93, x_2 = 0.07$，可以看到闭环矫正了所有节点的位姿，优化了整个拓扑图。 2.2 后端局部优化，用于矫正局部地图再举个栗子： 机器人从起始位置$x_0=0$处出发，并观测到其正前方2m处有一个路标$l_0$，里程计测得它向前移动了1m，到达$x_1$，这时观测到路标在其正前方0.8m处。 首先根据前端信息建图 raw graph（这样建图明显是存在累积误差的）： x_0 + 1 = x_1然后添加闭环约束： x_1 + 0.8 = l_0\\ x_0 + 2 = l_0初始条件： x_0 = 0构建残差平方和： c = \Sigma_{i=1}^4 f_i^2 = x_0^2 + (x_1-x_0-1)^2 +(l_0-x_0-2)^2 + (l_0-x_1-0.8)^2求偏导求解：$x_1 = 1.07, l_0 = 1.93$，可以看到后端是对前端新添加进来的节点位姿做了矫正，消除部分测量误差。 这里面涉及到两种传感器信息——里程计和激光雷达，两种传感器的精度是有差别的，我们对其的信任程度也应该不同，反映到公式中就是要为不同传感器信息赋予不同的权重。假设编码器信息更准确，那么： c = \Sigma_{i=1}^4 f_i^2 = x_0^2 + \textbf{10}(x_1-x_0-1)^2 +(l_0-x_0-2)^2 + (l_0-x_1-0.8)^2调整权重之后解得：$x_1 = 1.01, l_0 = 1.9$，可以看到计算结果会向着更信任的传感器的测量结果靠近。 2.3 严格推导2.3.1 信息矩阵（误差权重矩阵) 图优化问题转化为最小二乘问题，首先是带权重的残差平方和函数的一般形式： F(x) = \Sigma_{i,j} e(x_i, x_j, z_{i,j})^T\Omega_{i,j}e(x_i, x_j, z_{i,j}) x^{*} = argminF(x)其中的$\Omega_{i,j}$项就是上文提到的误差权重矩阵，它的正式名字叫信息矩阵。 传感器的测量值，可以看作是以真值为中心的多元高斯分布： f_x(x_1, x_2, ..., x_k) = \frac{1}{\sqrt{}(2\pi)^k|\Sigma|}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))协方差矩阵$\Sigma$对角线上的值表示每一维对应的方差，该方差值越大表示该维度不确定性越大，对应的信息权重应该越小。实际上拓扑图上每条边对应的信息矩阵就是对应测量协方差矩阵的逆。 2.3.2 非线性 上文已经提到，位姿变化非线性——非线性最小二乘问题，要采用迭代法求解。迭代法需要有一个好的初始假设值，然后在这个值附近增量式迭代寻找最优解。 f(x) = \Sigma e^T\Omega e\\ 最小二乘问题目标函数：min \frac{1}{2}||f(x)||_2^2首先要将非线性函数转化成关于增量$\Delta x$的线性函数——泰勒展开，根据具体的展开形式又分为： 一阶、二阶梯度法 直接对目标函数在$x$附近进行泰勒展开： ||f(x+\Delta x)||_2^2 \approx ||f(x)||_2^2 +J(x) \Delta x = \frac{1}{2} \Delta x^T H \Delta x一阶梯度法（最速下降法）：只保留一阶梯度，并引入步长$\lambda$： \Delta x^* = -\lambda J^T(x)二阶梯度法（牛顿法）：保留一阶和二阶梯度信息 J^T(x)+H\Delta x^*=0最速下降法过于贪心，容易走出锯齿路线，增加迭代次数。牛顿法需要计算目标函数的二阶导数（Hessian矩阵），计算困难。 高斯牛顿法 对$f(x)$而不是目标函数$f(x)^2$在$x$附近进行一阶泰勒展开： f(x+\Delta x) \approx f(x) + J(x) \Delta x对应每一个误差函数$e_{ij}$： \begin{split} & e_{ij}(x_i+\Delta x_i, x_j+\Delta x_j) = e_{i,j}(x+\Delta x) \\ &\approx e_{ij} +\frac{\partial e_{ij}}{\partial x}\Delta x = e_{ij} + J_{ij}\Delta x \end{split}​ 其中$J_{ij}$为初始值附近的雅可比矩阵（定义见卡尔曼滤波）。 带入目标函数得到近似二阶展开： \begin{split} & F_{ij}(x+\Delta x) = e_{ij}(x+\Delta x)^T \Omega_{ij}e_{ij}(x+\Delta x)\\ & \approx (e_{ij} + J_{ij}\Delta x)^T \Omega_{ij}(e_{ij} + J_{ij}\Delta x)\\ & = \underbrace{e_{ij}^T\Omega_{ij}e_{ij}}_{c_{ij}} + 2\underbrace{e_{ij}^T\Omega_{ij}J_{ij}}_{b_{ij}^T}\Delta x + \Delta x^T \underbrace{J_{ij}^T\Omega_{ij}J_{ij}}_{H_{ij}}\Delta x\\ & = c_{ij} + 2b_{ij}^T\Delta x + \Delta x^T H_{ij}\Delta x \end{split}求解增量$\Delta x$： 2b + 2H\Delta x^* = 0\\ 高斯牛顿方程：H\Delta x^* = -b\\对比牛顿法可见，高斯牛顿法用$J^TJ$作为二阶Hessian矩阵的近似，简化了计算。 上述算法要求近似$H$矩阵是正定且可逆的，实际数据很难满足，因而在使用高斯牛顿算法时可能出现$H$为奇异矩阵或病态的情况，增量稳定性较差，导致算法不收敛。 图形上来思考，就是近似后的梯度方向不再是梯度变化最快的方向，可能引起不稳定。 列文伯格—马夸尔特法 为$\Delta x$添加一个信赖区域，不让它因为过大而使得近似$f(x+\Delta x) = f(x) + J(x)\Delta x$不准确。 \rho = \frac{f(x+\Delta x) - f(x)}{J(x) \Delta x}可以看到如果$\rho$接近1，说明近似比较好。如果$\rho$比较大，说明实际减小的值远大于估计减小的值，需要放大近似范围，反之你懂的。 ||D\Delta x_k||_2^2 \leq \mu将每次迭代得到的$\Delta x$限定在一个半径为信赖区域的椭球中，根据$\rho$的大小修改信赖区域。于是问题转化成为了带不等式约束的优化问题： min_\Delta x \frac{1}{2}||f(x) + J(x)\Delta x||^2_2, \ \ s.t. ||D \Delta x ||^2 = \mu用拉格朗日乘子转化成无约束问题： min_\Delta x \frac{1}{2}||f(x) + J(x)\Delta x||^2_2 + \frac{\lambda}{2}||D \Delta x ||^2展开后得到如下形式： (H + \lambda D^TD)\Delta x^* = -b通常把$D$取值为单位阵$I$，得到更简化形式： (H + \lambda I)\Delta x^* = -b当$\lambda$较小时，$H$占主要地位，说明二次近似模型较好，LM算法更接近高斯牛顿法。当$\lambda$较大时，$\lambda I$占主要地位，LM算法更接近一阶梯度法。修正了线性方程组矩阵的病态问题，比高斯牛顿法更加健壮，但是收敛速度也更慢。 图形上思考，LM算法修正了高斯牛顿法得到的梯度，以此固定一个搜索区域，在区域内寻找最优。 2.3.3 稀疏矩阵 对于误差函数$e_{ij}$，它只和$e_i$和$e_j$有关，因此它的雅可比矩阵有如下结构（行数是$x$的维度，列数是拓扑图中节点映射关系的数目）： J_{ij} = \begin{bmatrix} 0 & ... & 0 & \underbrace{\frac{\partial e_{i}}{\partial x_i}}_{A_{ij}} & 0 & ... & \underbrace{\frac{\partial e_{j}}{\partial x_j}}_{B_{ij}} & 0 & ... & 0 \end{bmatrix}相应地$b_{ij}$是一个包含很多0的列向量： \begin{split} b_{ij}^T& = e_{ij}^T \Omega_{ij} J_{ij}\\ &= e_{ij}^T\Omega_{ij} (0 ... A_{ij}...B_{ij}...0)\\ &=(0...e_{ij}^T\Omega_{ij}A_{ij}...e_{ij}^T\Omega_{ij}B_{ij}...0) \end{split}$b = \Sigma_{ij} b_{ij}$： $H_{ij}$是一个包含很多0的对称阵： \begin{split} H_{ij}& = J_{ij}^T \Omega_{ij}J_{ij} \\ & = \begin{pmatrix} ...\\ A_{ij}^T\\ ...\\ B_{ij}^T\\ ... \end{pmatrix} \Omega_{ij} \begin{pmatrix} ... & A_{ij} & ... & B_{ij} & ... \end{pmatrix}\\ & = \begin{pmatrix} & \\ & A_{ij}^T\Omega{ij}A_{ij} & A_{ij}^T\Omega_{ij}B_{ij} & \\ & \\ & B{ij}^T\Omega_{ij}A_{ij} & B_{ij}^T\Omega_{ij}B_{ij}& \\ & \end{pmatrix} \end{split}$H=\Sigma_{ij}H_{ij}$： 梳理一下计算流程：$e_{ij} \to J_{ij} \to A_{ij}, B_{ij} \to b_{ij}, H_{ij} \to b, H \to \Delta x^* \to x$ 2.3.4 误差函数 前面定义过位姿的非线性叠加，显然位姿误差也不是简单的线性加减关系： e_{ij}(x_i, x_j) = t2v(Z_{ij}^{-1}(X_i^{-1}.X_j))其中的$Z_{ij}$、$X_i$、$X_j$都是矩阵形式。$X_i^{-1}X_j$表示节点j到节点i之间的位姿差异$\hat Z_{ij}$，假设这个转移矩阵形式如下： \hat Z_{ij} = \begin{bmatrix} R_{2*2} & t_{2*1} \\ 0 & 1 \end{bmatrix}假设测量值$Z_{ij}$形式如下： Z_{ij} = \begin{bmatrix} R^{'} & t^{'}\\ 0 & 1 \end{bmatrix}分块矩阵的求逆过程如下： \begin{split} \begin{bmatrix} R & t\\ 0 & 1 \end{bmatrix}^{-1}& = \begin{bmatrix} \begin{bmatrix} I & t\\ 0 & 1 \end{bmatrix} \begin{bmatrix} R & 0\\ 0 & 1 \end{bmatrix} \end{bmatrix}^{-1}= \begin{bmatrix} R & 0\\ 0 & 1 \end{bmatrix}^{-1} \begin{bmatrix} I & t\\ 0 & 1 \end{bmatrix}^{-1}\\ & =\begin{bmatrix} R^T & 0\\ 0 & 1 \end{bmatrix} \begin{bmatrix} I & -t\\ 0 & 1 \end{bmatrix}= \begin{bmatrix} R^T & -R^Tt\\ 0 & 1 \end{bmatrix} \end{split}所以误差$e_{ij}$计算如下： E_{ij} = Z_{ij}^{-1}\hat Z_{ij} = \begin{bmatrix} R^{'} & t^{'}\\ 0 & 1 \end{bmatrix}^{-1} \begin{bmatrix} R & t\\ 0 & 1 \end{bmatrix}= \begin{bmatrix} R^{'T} & -R^{'T}t^{'}\\ 0 & 1 \end{bmatrix} \begin{bmatrix} R & t\\ 0 & 1 \end{bmatrix}= \begin{bmatrix} R^{'T}R & R^{'T}(t-t^{'})\\ 0 & 1 \end{bmatrix} e_{ij} = t2v(E_{ij})= \begin{bmatrix} R_z(t_{\hat z} - t_z)_{2*1}\\ \theta_\hat z - \theta_z \end{bmatrix}_{3*1}= \begin{bmatrix} R_z(x_{\hat z} - x_z)\\ R_z(y_{\hat z} - y_z)\\ \theta_\hat z - \theta_z \end{bmatrix}= \begin{bmatrix} R_z[R_i(x_{j} - x_i) - x_z]\\ R_z[R_i(y_{j} - y_{i}) - y_z]\\ \theta_j - \theta_i - \theta_z \end{bmatrix}求解雅可比矩阵$J_{ij}$： A_{ij} = \begin{bmatrix} \frac{\partial e_1}{\partial x_i} & \frac{\partial e_1}{\partial y_i} & \frac{\partial e_1}{\partial \theta_i}\\ \frac{\partial e_2}{\partial x_i} & \frac{\partial e_2}{\partial y_i} & \frac{\partial e_2}{\partial \theta_i}\\ \frac{\partial e_3}{\partial x_i} & \frac{\partial e_3}{\partial y_i} & \frac{\partial e_3}{\partial \theta_i}\\ \end{bmatrix}= \begin{bmatrix} -R_z^TR_i^T & 0 & R_z^T\frac{\partial R_I^T}{\partial \theta_i}(x_j-x_i)\\ 0 & -R_z^TR_i^T & R_z^T\frac{\partial R_I^T}{\partial \theta_i}(y_j-y_i)\\ 0 & 0 & -1 \end{bmatrix} B_{ij} = \begin{bmatrix} R_z^TR_i^T & 0\\ 0 & 1 \end{bmatrix}累加$b$和$H$矩阵： b_{[i]} += A_{ij}\Omega_{ij}e_{ij}\\ b_{[j]} += B_{ij}^T\Omega_{ij}e_{ij}\\ H_{[ii]} += A_{ij}^T\Omega_{ij}A_{ij}\\ H_{[ij]} += A_{ij}^T\Omega_{ij}B_{ij}\\ H_{[ji]} += B_{ij}^T\Omega_{ij}A_{ij}\\ H_{[jj]} += B_{ij}^T\Omega_{ij}B_{ij}]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关联分析]]></title>
    <url>%2F2018%2F04%2F29%2F%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[1 引言频繁项集：集合，${a, b, c}$ 关联规则：映射，$a\to b$ 支持度：针对某个频繁项集，$support(频繁项集a) = \frac{freq(频繁项集a)}{freq(所有项集)}$ 可信度：衡量某条关联规则，$confidence(a\to b) = \frac{support(a|b)}{support(a)}$ 对于包含N个元素的数据集，可能的集合有$2^N - 1$种，暴力遍历显然药丸，因此引入Apriori原理。 Apriori原理：减少可能的项集，避免指数增长。 backwards：如果某个项集是频繁的，那么它的所有子集也是频繁的。 forwards：如果一个项集是非频繁项集，那么它的所有超集也是非频繁的。 2 Apriori算法1def apriori(dataSet, minSupport=0.5): 算法思路：从单个项集开始检查，去掉那些不满足最小支持度的项集，然后对剩下的集合进行组合，得到包含两个元素的项集，重复扫描，然后将剩余项集组合成包含三个元素的集合，依次类推，直到所有项集都被去掉。 为啥最后会得到空集：因为包含所有元素的项集一定不是频繁项集，否则根据Apriori原理，它的全部子集都是频繁项集。 如何从包含k个元素的项集集合生成包含k+1个元素的项集集合：从k个元素的项集到k+1个元素项集的扩充，只允许有一个元素的不同，算法中为了避免重复结果，只对前k-1个元素相同的两个项集求并集。 代码实现过程中发现了几个知识记录一下： map函数的返回值：python2下直接返回列表，python3下返回的是迭代器： 12345map(frozenset, C1)# 返回 &lt;map object at 0x101e78940&gt; list(map(frozenset, C1))# 返回 list[frozenset1(), frozenset2(), ...] 字典的update方法： 12# 将dict2的键值添加到dict1中，在涉及迭代操作时可以省略传递中间值dict1.update(dict2) set &amp; frozenset：set无排序且不重复，并且可变，因此unhashable。frozenset不可变，可以用作字典的key。 3 关联规则对一个包含k个元素的频繁项集，其中可能的关联规则有： C_N^1 + C_N^2 + ... + C_N^{N-1}暴力遍历肯定又药丸，因此延续Apriori的思路，关联规则也有一条类似的属性： 如果某条规则的前件不满足最小可信度要求，那么它的所有子集也不满足最小可信度要求。 对应的，如果某条规则的后件不满足最小可信度要求，那么它的所有超集也不满足。 算法思路：对每个至少包含两个元素的频繁项集，从后部只包含一个元素的规则开始，对这些规则进行测试，接下来对所有剩余规则的后件进行组合，得到包含两个元素的后件（对应的补集就是前件），依次类推，直到测试完所有可能的后件。 为啥只检查前后件互补的规则：因为一个频繁项集的所有子集也都是频繁项集，所以一个频繁项集中不互补的规则将是该频繁项集的某个子集的互补规则。 4 FP-growth算法Apriori算法避免了暴力遍历子项集的指数式增长，但是对每一个新生成的频繁项集，都要扫描整个数据集，当数据集很大时，这种抛物线式增长的时间复杂度也不太令人满意。 FP-growth算法借助一种称为FP树的数据结构存储数据，来抽象原始数据集： 项集以路径的方式存储在树中 相似项之间相连接成链表 一个元素项可以在FP树中出现多次 FP树存储的是元素的出现频率 项集完全不同时，树才会分叉，否则会有复用路径 ​算法思路：首先遍历一遍原始数据集，记录元素的出现频率，去掉不满足最小支持度的元素。然后再遍历一遍剩下的集合元素，构建FP树。然后就可以通过FP树挖掘频繁项集。 构建FP树：依次遍历每一个项集，首先将其中的非频繁项移除，并按照元素出现频率对过滤后的元素进行重排序。对过滤、排序后的集合，如果树中已存在现有元素，则增加现有元素的计数值，如果不存在，则向树中添加一个分支，新增节点的同时还要更新链表元素。主要就涉及两个数据结构： 12345678910111213# 自定义节点数据结构class treeNode: def __init__(self, nameValue, numOccur, parentNode): self.name self.count self.nodeLink # 链表信息，指向下一个相似项 self.parent self.children # 用于存储元素frequency以及链接相似项的字典数据结构freq = &#123;&#125;freq[node_name] = [frequency, node1, node2, ...] 因为集合中元素的出现频率可能相等，因此过滤排序的结果不唯一，生成的树结构也会有差异。 第一次遍历删除非频繁元素时，发现字典在迭代过程中不能删除item，我转化成list暴力解决了，不知道有没有什么优雅的方式。 123456del freq[item]# 返回 RuntimeError: dictionary changed size during iterationfor item in list(freq.keys()): if freq[item] &lt; minSupport: del(freq[item]) 挖掘频繁项集：首先创建条件模式基，然后利用条件模式基，构建条件FP树。 1 条件模式基：以所查找元素项为结尾的前缀路径集合，并且每条前缀路径都与起始元素项的计数值相关联。（这里面用到了前面定义的parent和nodeLink属性） 2 构造条件FP树：与构造树的过程相同，使用的dataSet换成了条件模式基而已，函数参数count就是预留彩蛋。这样得到的就是指定频繁项的条件FP树。 12345def updateTree(cond_set, myTree, freq_header, count):# cond_set: 一条path# myTree: 根节点# freq_header: dict[node_name] = [frequency, head_node]# count: path对应的count 构造的条件FP树过滤掉了条件模式基中的一些元素：这些元素本身是频繁项，但是与指定元素组合的集合不是频繁的。 相应地，条件树中剩余元素与指定频繁项组合的集合是频繁的。 3 迭代：从生成的条件FP树中，可以得到更复杂的频繁项。求解复杂频繁项的条件模式基，进而生成对应的条件FP树，就能得到更复杂的频繁项，依次类推进行迭代，直到FP树为空。 ​]]></content>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sublime注册码被无限次移除]]></title>
    <url>%2F2018%2F04%2F25%2Fsublime%E6%B3%A8%E5%86%8C%E7%A0%81%E8%A2%AB%E6%97%A0%E9%99%90%E6%AC%A1%E7%A7%BB%E9%99%A4%2F</url>
    <content type="text"><![CDATA[最近不知道sublime3抽什么风，突然开始验证注册码了，输入一个valid code分分钟给你移除。 收藏一个解决办法，有效性待验证： 1234# add the following to your host file(/private/etc/hosts)127.0.0.1 license.sublimehq.com127.0.0.1 45.55.255.55127.0.0.1 45.55.41.223]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VTK编译报错no override found for vtkpolydatamapper]]></title>
    <url>%2F2018%2F04%2F12%2FVTK%E7%BC%96%E8%AF%91%E6%8A%A5%E9%94%99no-override-found-for-vtkpolydatamapper%2F</url>
    <content type="text"><![CDATA[报错原因是通过IDE编译而不是直接通过cmake，因此要添加如下代码段： 123#include "vtkAutoInit.h" VTK_MODULE_INIT(vtkRenderingOpenGL2); VTK_MODULE_INIT(vtkInteractionStyle); 先记录解决办法，more details 留到以后。 基础测试： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include "vtkAutoInit.h"VTK_MODULE_INIT(vtkRenderingOpenGL2); // VTK was built with vtkRenderingOpenGL2VTK_MODULE_INIT(vtkInteractionStyle);#include &lt;vtkSphereSource.h&gt;#include &lt;vtkPolyData.h&gt;#include &lt;vtkSmartPointer.h&gt;#include &lt;vtkPolyDataMapper.h&gt;#include &lt;vtkActor.h&gt;#include &lt;vtkRenderWindow.h&gt;#include &lt;vtkRenderer.h&gt;#include &lt;vtkRenderWindowInteractor.h&gt;int main(int, char *[])&#123; // Create a sphere vtkSmartPointer&lt;vtkSphereSource&gt; sphereSource = vtkSmartPointer&lt;vtkSphereSource&gt;::New(); sphereSource-&gt;SetCenter(0.0, 0.0, 0.0); sphereSource-&gt;SetRadius(5.0); //mapper vtkSmartPointer&lt;vtkPolyDataMapper&gt; mapper = vtkSmartPointer&lt;vtkPolyDataMapper&gt;::New(); mapper-&gt;SetInputConnection(sphereSource-&gt;GetOutputPort()); //actor vtkSmartPointer&lt;vtkActor&gt; actor = vtkSmartPointer&lt;vtkActor&gt;::New(); actor-&gt;SetMapper(mapper); //renderer ,renderWindow, renderWindowInteractor. vtkSmartPointer&lt;vtkRenderer&gt; renderer = vtkSmartPointer&lt;vtkRenderer&gt;::New(); vtkSmartPointer&lt;vtkRenderWindow&gt; renderWindow = vtkSmartPointer&lt;vtkRenderWindow&gt;::New(); renderWindow-&gt;AddRenderer(renderer); vtkSmartPointer&lt;vtkRenderWindowInteractor&gt; renderWindowInteractor = vtkSmartPointer&lt;vtkRenderWindowInteractor&gt;::New(); renderWindowInteractor-&gt;SetRenderWindow(renderWindow); renderer-&gt;AddActor(actor); renderer-&gt;SetBackground(.3, .6, .3); // Background color green renderWindow-&gt;Render(); renderWindowInteractor-&gt;Start(); return EXIT_SUCCESS;&#125;]]></content>
      <tags>
        <tag>basic</tag>
      </tags>
  </entry>
</search>
