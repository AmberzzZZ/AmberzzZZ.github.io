<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>re-labeling</title>
      <link href="/2021/05/27/re-labeling/"/>
      <url>/2021/05/27/re-labeling/</url>
      <content type="html"><![CDATA[<h2 id="Re-labeling-ImageNet-from-Single-to-Multi-Labels-from-Global-to-Localized-Labels"><a href="#Re-labeling-ImageNet-from-Single-to-Multi-Labels-from-Global-to-Localized-Labels" class="headerlink" title="Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels"></a>Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels</h2><ol><li><p>动机</p><ul><li>label noise<ul><li>single-label benchmark</li><li>but contains multiple classes in one sample</li><li>a random crop may contain an entirely different object from the gt label</li><li>exhaustive multi-label annotations per image is too cost</li></ul></li><li>mismatch<ul><li>researches refine the validation set with multi-labels</li><li>propose new multi-label evaluation metrics</li><li>但是造成数据集的mismatch</li></ul></li><li>we propose<ul><li>re-label</li><li>use a strong image classifier trained on extra source of data to generate the multi-labels</li><li>use pixel-wise multi-label predictions before GAP：addtional location-specific supervision</li><li>then trained on re-labeled samples</li><li>further boost with CutMix</li></ul></li><li>from single to multi-labels：多标签</li><li><p>from global to localized：dense prediction map</p><p><img src="/2021/05/27/re-labeling/re-labeling.png" width="40%;"></p></li></ul></li><li><p>论点</p><ul><li>single-label<ul><li>和multi-label validation set的mismatch</li><li>random crop augmentation加剧了问题</li><li>除了多目标还有前背景，只有23%的random crops IOU&gt;0.5</li></ul></li><li>ideally label<ul><li>the full set of classes——multi-label</li><li>where each objects——localized label</li><li>results in a dense pixel labeling $L\in \{0,1\}^{HWC}$</li></ul></li><li><p>we propose a re-labeling strategy</p><ul><li>ReLabel <ul><li>strong classifier</li><li>external training data</li><li>generate feature map predictions</li></ul></li><li>LabelPooling<ul><li>with dense labels &amp; random crop</li><li>pooling the label scores from crop region</li></ul></li></ul></li><li><p>evaluations</p><ul><li>baseline r50：77.5%</li><li>r50 + ReLabel：78.9%</li><li>r50 + ReLabel + CutMix：80.2%</li></ul></li><li><p>【QUESTION】同样是引入外部数据实现无痛长点，与noisy student的区别/好坏？？？</p><ul><li><p>目前论文提到的就只有efficiency，ReLabel是one-time cost的，知识蒸馏是iterative&amp;on-the-fly的</p><p><img src="/2021/05/27/re-labeling/differences.png" width="40%;"></p></li></ul></li></ul></li><li><p>方法</p><ul><li><p>Re-labeling</p><ul><li><p>super annotator</p><ul><li>state-of-the-art classifier</li><li>trained on super large dataset</li><li>fine-tuned on ImageNet</li><li>and predict ImageNet labels</li></ul></li><li><p>we use open-source trained weights as annotators</p><ul><li>though trained with single-label supervision</li><li>still tend to make multi-label predictions</li><li>EfficientNet-L2</li><li>input size 475</li><li>feature map size 15x15x5504</li><li><p>output dense label size 15x15x1000</p><p><img src="/2021/05/27/re-labeling/relabel.png" width="60%;"></p></li></ul></li><li><p>location-specific labels</p><ul><li>remove GAP heads</li><li>add a 1x1 conv</li><li>说白了就是一个fcn</li><li>original classifier的fc层权重与新添加的1x1 conv层的权重是一样的</li><li><p>label的每个channel对应了一个类别的heatmap，可以看到disjointly located at each object’s position</p><p><img src="/2021/05/27/re-labeling/labelmap.png" width="80%;"></p></li></ul></li></ul></li><li><p>LabelPooling</p><p>  <img src="/2021/05/27/re-labeling/labelpooling.png" width="40%;"></p><ul><li>loads the pre-computed label map</li><li>region pooling (RoIAlign) on the label map</li><li>GAP + softmax to get multi-label vector</li><li>train a classifier with the multi-label vector</li><li><p>uses CE</p><p><img src="/2021/05/27/re-labeling/algorithm.png" width="90%;"></p></li></ul></li><li><p>choices</p><ul><li><p>space consumption</p><ul><li>主要是存储label map的空间</li><li>store only top-5 predictions per image：10GB</li></ul></li><li><p>time consumption</p><ul><li>主要是说生成label map的one-shot-inference time和labelPooling引入的额外计算时间</li><li>relabeling：10-GPU hours</li><li>labelPooling：0.5% additiona training time</li><li>more efficient than KD</li></ul></li><li><p>annotators</p><ul><li><p>标注工具哪家强：目前看下来eff-L2的supervision效果最强</p><p><img src="/2021/05/27/re-labeling/annotators.png" width="40%;"></p></li><li><p>supervision confidence</p><ul><li>随着image crop与前景物体的IOU增大，confidence逐渐增加</li><li><p>说明supervision provides some uncertainty when low IOU</p><p><img src="/2021/05/27/re-labeling/supervision.png" width="45%;"></p></li></ul></li></ul></li></ul></li></ul></li><li><p>实验</p></li></ol>]]></content>
      
      
        <tags>
            
            <tag> pretaining </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>mlp系列</title>
      <link href="/2021/05/27/mlp%E7%B3%BB%E5%88%97/"/>
      <url>/2021/05/27/mlp%E7%B3%BB%E5%88%97/</url>
      <content type="html"><![CDATA[<p>[papers]</p><ul><li>[MLP-Mixer] MLP-Mixer: An all-MLP Architecture for Vision，Google</li><li>[ResMLP] ResMLP: Feedforward networks for image classification with data-efficient training，Facebook</li></ul><h2 id="MLP-Mixer-An-all-MLP-Architecture-for-Vision"><a href="#MLP-Mixer-An-all-MLP-Architecture-for-Vision" class="headerlink" title="MLP-Mixer: An all-MLP Architecture for Vision"></a>MLP-Mixer: An all-MLP Architecture for Vision</h2><ol><li><p>动机</p><ul><li>image classification task</li><li>neither of [CNN, attention] are necessary</li><li>our proposed MLP-Mixer<ul><li>仅包含multi-layer-perceptrons</li><li>independently to image patches</li><li>repeated applied across either spatial locations or feature channels</li><li>two types<ul><li>applied independently to image patches</li><li>applied across patches</li></ul></li></ul></li></ul></li><li><p>方法</p><ul><li><p>overview</p><p>  <img src="/2021/05/27/mlp系列/mixer.png" width="70%;"></p><ul><li>输入是token sequences<ul><li>non-overlapping image patches</li><li>linear projected to dimension C</li></ul></li><li>Mixer Layer<ul><li>maintain the input dimension</li><li>channel-mixing MLP<ul><li>operate on each token independently</li><li>可以看作是1x1的conv</li></ul></li><li>token-mixing MLP<ul><li>operate on each channel independently</li><li>take each spatial vectors (hxw)x1 as inputs</li><li>可以看作是一个global depth-wise conv，s1，same pad，kernel size是(h,w)</li></ul></li></ul></li><li>最后对token embedding做GAP，提取sequence vec，然后进行类别预测</li></ul></li><li><p>idea behind Mixer</p><ul><li>clearly separate the per-location operations &amp; cross-location operations </li><li>CNN是同时进行这俩的</li><li>transformer的MSA同时进行这俩，MLP只进行per-location operations</li></ul></li><li><p>Mixer Layer</p><ul><li><p>two MLP blocks</p></li><li><p>given input $X\in R^{SC}$，S for spatial dim，C for channel dim</p><p>  <img src="/2021/05/27/mlp系列/blocks.png" width="70%;"></p></li><li><p>先是token-mixing MLP</p><ul><li>acts on S dim</li><li>maps $R^S$ to $R^S$</li><li>share across C-axis</li><li>LN-FC-GELU-FC-residual</li></ul></li><li><p>然后是channel-mixing MLP</p><ul><li>acts on C dim</li><li>maps $R^C$ to $R^C$</li><li>share across S-axis</li><li>LN-FC-GELU-FC-residual</li></ul></li><li><p>fixed width，更接近transformer/RNN，而不是CNN那种金字塔结构</p></li><li><p>不使用position embeddings</p><ul><li>the token-mixing MLPs are sensitive to the order of the input tokens</li><li>may learn to represent locations</li></ul></li></ul></li></ul></li><li><p>实验</p></li></ol><h2 id="ResMLP-Feedforward-networks-for-image-classification-with-data-efficient-training"><a href="#ResMLP-Feedforward-networks-for-image-classification-with-data-efficient-training" class="headerlink" title="ResMLP: Feedforward networks for image classification with data-efficient training"></a>ResMLP: Feedforward networks for image classification with data-efficient training</h2><ol><li><p>动机</p><ul><li>entirely build upon MLP</li><li>alternates from a simple residual network<ul><li>a linear layer to interact with image patches</li><li>a two-layer FFN to interact independently with each patch</li><li>affine transform替代LN是一个特别之处</li></ul></li><li>trained with modern strategy<ul><li>heavy data-augmentation </li><li>optionally distillation</li></ul></li><li>show good performace on ImageNet classification</li></ul></li><li><p>论点</p><ul><li>strongly inspired by ViT but simpler<ul><li>没有attention层，只有fc层+gelu</li><li>没有norm层，因为much more stable to train，但是用了affine transformation</li></ul></li></ul></li><li><p>方法</p><ul><li><p>overview</p><p>  <img src="/2021/05/27/mlp系列/ResMLP.png" width="70%;"></p><ul><li>takes flattened patches as inputs<ul><li>typically N=16：16x16</li></ul></li><li>linear project the patches into embeddings<ul><li>form $N^2$ d-dim embeddings</li></ul></li><li>ResMLP Layer<ul><li>main the dim throughout $[N^2,d]$</li><li>a simple linear layer<ul><li>interaction between the patches</li><li>applied to all channels independently</li><li>类似depth-wise conv with global kernel的东西，线性！！</li></ul></li><li>a two-layer-mlp<ul><li>fc-GELU-fc</li><li>independently applied to all patches</li><li>非线性！！</li></ul></li></ul></li><li>average pooled $[d-dim]$ + linear classifier $cls-dim$</li></ul></li><li><p>Residual Multi-Layer Perceptron Layer</p><ul><li>a linear layer + a FFN layer</li><li>each layer is paralleled with a skip-connection</li><li><p>没用LN，但是用了learnable affine transformation</p><ul><li>$Aff_{\alpha, \beta} (x) = Diag(\alpha) x + \beta$</li><li>rescale and shifts the input component-wise：对每个patch，分别做affine变换</li><li>在推理阶段可以与上一层线性层合并：no cost</li><li>用了两次<ul><li>第一个用在main path上用来替代LN：初值为identity transform(1,0)</li><li>第二个在residual path里面，down scale to boost，用一个small value初始化</li></ul></li></ul></li><li><p>given input： $d\times N^2$ matrix $X$</p><p>  <img src="/2021/05/27/mlp系列/mlp1.png" width="50%;"></p><ul><li>affine在d-dim上做</li><li>第一个Linear layer在$N^2-dim$上做：参数量$N^2 \times N^2$</li><li><p>第二、三个Linear layer在$d-dim$上做：参数量$d \times 4d$ &amp; $4d \times d$</p><p><img src="/2021/05/27/mlp系列/mlp2.png" width="50%;"></p></li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> mlp </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>torch-note</title>
      <link href="/2021/05/24/torch-note/"/>
      <url>/2021/05/24/torch-note/</url>
      <content type="html"><![CDATA[<ol><li><p>torch.cuda.amp</p><p> 自动混合精度：FloatTensor &amp; HalfTensor</p></li><li><p>torch.jit.script</p><p> 将模型从纯Python程序转换为能够独立于Python运行的TorchScript程序</p></li><li><p>torch.nn.DataParallel</p></li><li><p>torch.flatten(input, start_dim=0, end_dim=-1)</p><p> 展开start_dim到end_dim之间的dim成一维</p></li><li></li></ol>]]></content>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>LV-ViT</title>
      <link href="/2021/05/21/LV-ViT/"/>
      <url>/2021/05/21/LV-ViT/</url>
      <content type="html"><![CDATA[<p>[LV-ViT 2021] Token Labeling: Training an 85.4% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNet，新加坡国立&amp;字节，主体结构还是ViT，deeper+narrower+multi-layer-cnn-patch-projection+auxiliary label&amp;loss</p><p>同等参数量下，能够达到与CNN相当的分类精度</p><ul><li>26M——84.4% ImageNet top1 acc</li><li>56M——85.4% ImageNet top1 acc</li><li>150M——86.2% ImageNet top1 acc</li></ul><p><img src="/2021/05/21/LV-ViT/recent.png" width="70%;"></p><p>ImageNet &amp; ImageNet-1k：The ImageNet dataset consists of more than 14M images, divided into approximately 22k different labels/classes. However the <strong>ImageNet challenge is conducted on just 1k high-level categories</strong> (probably because 22k is just too much)</p><h2 id="Token-Labeling-Training-an-85-4-Top-1-Accuracy-Vision-Transformer-with-56M-Parameters-on-ImageNet"><a href="#Token-Labeling-Training-an-85-4-Top-1-Accuracy-Vision-Transformer-with-56M-Parameters-on-ImageNet" class="headerlink" title="Token Labeling: Training an 85.4% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNet"></a>Token Labeling: Training an 85.4% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNet</h2><ol><li><p>动机</p><ul><li>develop a bag of training techniques on vision transformers</li><li>slightly tune the structure</li><li>introduce token labeling——a new training objective</li><li>ImageNet classificaiton task</li></ul></li><li><p>论点</p><ul><li>former ViTs<ul><li>主要问题就是需要大数据集pretrain，不然精度上不去</li><li>然后模型也比较大，need huge computation resources</li><li>DeiT和T2T-ViT探索了data augmentation/引入additional token，能够在有限的数据集上拉精度</li></ul></li><li>our work<ul><li>rely on purely ImageNet-1k data</li><li>rethink the way of performing patch embedding</li><li>introduce inductive bias</li><li>we add a token labeling objective loss beside cls token predition</li><li>provide practical advice on adjusting vision transformer structures</li></ul></li></ul></li><li><p>方法</p><ul><li><p>overview &amp; comparison</p><ul><li>主体结构不变，就是增加了两项</li><li>a MixToken method</li><li><p>a token labeling objective</p><p><img src="/2021/05/21/LV-ViT/cmp.png" width="75%;"></p></li></ul></li><li><p>review the vision transformer</p><ul><li>patch embedding<ul><li>将固定尺寸的图片转换成patch sequence，例如224x224的图片，patch size=16，那就是14x14个small patches</li><li>将每个patch(16x16x3=768-dim) linear project成一个token(embedding-dim)</li><li>concat a class token，构成全部的input tokens</li></ul></li><li>position encoding<ul><li>added to input tokens</li><li>fixed sinusoidal / learnable</li></ul></li><li>multi-head self-attention<ul><li>用来建立long-range dependency</li><li>multi-heads：所有attention heads的输出在channel-dim上concat，然后linear project回单个head的channel-dim</li></ul></li><li>feed-forward layers<ul><li>fc1-activation-fc2</li></ul></li><li>score predition layer<ul><li>只用了cls token对应的输出embedding，其他的discard</li></ul></li></ul></li><li><p>training techniques</p><ul><li><p>network depth</p><ul><li>add more transformer blocks</li><li>同时decrease the hidden dim of FFN</li></ul></li><li><p>explicit inductive bias</p><ul><li>CNN逐步扩大感受野，擅长提取局部特征，具有天然的平移不变性等</li><li>transformer被发现failed to capture the low-level and local structures</li><li>we use convolutions with a smaller stride to provide an overlapped information for each nearby tokens</li><li>在patch embedding的时候不是independent crop，而是有overlap</li><li>然后用多层conv，逐步扩大感受野，smaller kernel size同时降低了计算量</li></ul></li><li><p>rethinking residual connection</p><ul><li><p>给残差分支add a smaller ratio $\alpha$</p><p>  <img src="/2021/05/21/LV-ViT/residual.png" width="40%;"></p></li><li><p>enhance the residual connection since less information will go to the residual branch</p></li><li><p>improve the generalization ability</p></li></ul></li><li><p>re-labeling</p><ul><li>label is not always accurate after cropping</li><li><p>situations are worse on smaller images</p></li><li><p>re-assign each image with a K-dim score map，在1k类数据集上K=1000</p></li><li>cheap operation compared to teacher-student</li><li>这个label是针对whole image的label，是通过另一个预训练模型获取</li></ul></li><li><p>token-labeling</p><ul><li>based on the dense score map provided by re-labeling，we can assign each patch an individual label</li><li>auxiliary token labeling loss<ul><li>每个token都对应了一个K-dim score map</li><li>可以计算一个ce</li></ul></li><li>given<ul><li>outputs of the transformer $[X^{cls}, X^1, …, X^N]$</li><li>K-dim score map $[y^1, y^2, …, y^N]$ </li><li>whole image label $y^{cls}$</li></ul></li><li>loss<ul><li>auxiliary token labeling loss：$L_{aux} = \frac{1}{N} \sum_1^N CE(X^i, y^i)$</li><li>cls loss：$L_{cls} = CE(X^{cls}, y^{cls})$</li><li>total loss：$L_{total} = L_{cls}+\beta L_{aux}$，$\beta=0.5$</li></ul></li></ul></li><li><p>MixToken</p><ul><li>从Mixup&amp;CutMix启发来的</li><li>为了确保each token have clear content，我们基于token embedding进行mixup</li><li>given<ul><li>token sequence $T_1=[t^1_1, t^2_1, …, t^N_1]$ &amp; $T_2=[t^1_2, t^2_2, …, t^N_2]$</li><li>token labels $y_1=[y^1_1, y^2_1, …, y^N_1]$ &amp; $Y_2=[y^1_2, y^2_2, …, y^N_2]$</li><li>binary mask M</li></ul></li><li>MixToken<ul><li>mixed token sequence：$\hat T = T_1 \odot M + T_2 \odot (1-M)$</li><li>mixed labels：$\hat Y = Y_1 \odot M + Y_2 \odot (1-M)$</li><li>mixed cls label：$\hat {Y^{cls}} = \overline M y_1^{cls} + (1-\overline M) y_2^{cls}$，$\overline M$ is the average of $M$</li></ul></li></ul></li></ul></li></ul></li><li><p>实验</p><ul><li><p>training details</p><ul><li>AdamW</li><li>linear lr scaling：larger when use token labeling</li><li>weight decay</li><li><p>dropout：hurts small models，use Stochastic Depth instead</p><p><img src="/2021/05/21/LV-ViT/hyper.png" width="50%;"></p></li></ul></li><li><p>Training Technique Analysis</p><ul><li><p>more convs in patch embedding</p><p>  <img src="/2021/05/21/LV-ViT/convs.png" width="50%;"></p></li><li><p>enhanced residual</p><ul><li><p>smaller scaling factor</p><ul><li>the weight get larger gradients in residual branch</li><li>more information can be preserved in main branch</li><li>better performance</li><li>faster convergence</li></ul><p><img src="/2021/05/21/LV-ViT/enhanced residual.png" width="50%;"></p></li></ul></li><li><p>re-labeling</p><ul><li>use NFNet-F6 to re-label the ImageNet dataset and obtain the 1000-dimensional score map for each image</li><li>NFNet-F6 is trained from scratch</li><li>given input 576x576，获得的score map是18x18x1000（s32）</li><li>store the top5 probs for each position to save storage</li></ul></li><li><p>MixToken</p><ul><li>比baseline的CutMix method要好</li><li><p>同时看到token labeling比relabeling要好</p><p><img src="/2021/05/21/LV-ViT/MixToken.png" width="50%;"></p></li></ul></li><li><p>token labeling</p><ul><li>relabeling是在whole image上</li><li>token labeling是进一步地，在token level添加label和loss</li></ul></li><li><p>augmentation techniques</p><ul><li><p>发现MixUp会hurt</p><p><img src="/2021/05/21/LV-ViT/aug.png" width="50%;"></p></li></ul></li><li><p>Model Scaling</p><ul><li><p>越大越好</p><p><img src="/2021/05/21/LV-ViT/scaling.png" width="50%;"></p></li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> visual transformer </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>memory bank</title>
      <link href="/2021/05/19/memory-bank/"/>
      <url>/2021/05/19/memory-bank/</url>
      <content type="html"><![CDATA[<ul><li>2018年的paper</li><li>official code：<a href="https://github.com/zhirongw/lemniscate.pytorch" target="_blank" rel="noopener">https://github.com/zhirongw/lemniscate.pytorch</a></li><li>memory bank</li><li>NCE</li></ul><h2 id="Unsupervised-Feature-Learning-via-Non-Parametric-Instance-Discrimination"><a href="#Unsupervised-Feature-Learning-via-Non-Parametric-Instance-Discrimination" class="headerlink" title="Unsupervised Feature Learning via Non-Parametric Instance Discrimination"></a>Unsupervised Feature Learning via Non-Parametric Instance Discrimination</h2><ol><li><p>动机</p><ul><li>unsupervised learning<ul><li>can we learn good feature representation that captures apparent similarity among instances instead of classes</li><li>formulate a non-parametric classification problem at instance-level</li><li>use noise contrastive estimation</li></ul></li><li>our non-parametric model<ul><li>highly compact：128-d feature per image，only 600MB storage in total</li><li>enable fast nearest neighbour retrieval</li></ul></li><li>【QUESTION】无类别标签，单靠similarity，最终的分类模型是如何建立的？</li><li>verified on<ul><li>ImageNet 1K classification</li><li>semi-supervised learning</li><li>object detection tasks</li></ul></li></ul></li><li><p>论点</p><ul><li>observations<ul><li>ImageNet top-5 err远比top-1 err小</li><li>second highest responding class is more likely to be visually related</li><li>说明模型隐式地学到了similarity</li><li>apparent similarity is learned not from se- mantic annotations, but from the visual data themselves</li></ul></li><li>将class-wise supervision推到一个极限<ul><li>就变成了instance-level</li><li>类别数变成了the whole training set：softmax to many more classes becomes infeasible<ul><li>approximate the full softmax distribution with noise-contrastive estimation(NCE)</li><li>use a proximal regularization to stablize the learning process</li></ul></li></ul></li><li>train &amp; test<ul><li>通常的做法是learned representations加一个线性分类器</li><li>e.g. SVM：但是train和test的feature space是不一致的</li><li>我们用了KNN：same metric space </li></ul></li></ul></li><li><p>方法</p><ul><li><p>overview</p><ul><li>to learn a embedding function $f_{\theta}$</li><li>distance metric $d_{\theta}(x,y)  = ||f_{\theta}(x)-f_{\theta}(y)||$</li><li>to map visually similar images closer</li><li><p>instance-level：to distinct between instances</p><p><img src="/2021/05/19/memory-bank/pipeline.png" width="70%;"></p></li></ul></li><li><p>Non-Parametric Softmax Classifier</p><ul><li><p>common parametric classifier</p><ul><li>given网络预测的N-dim representation $v=f_{\theta}(x)$</li><li>要预测C-classes的概率，需要一个$W^{NC}$的projection：$P(i|v) = \frac{exp (W^T_iv)}{\sum exp (W^T_jv)}$</li></ul></li><li><p>Non-Parametric version</p><ul><li>enforce $||v||=1$ via L2 norm</li><li>replace $W^T$ with $v^T$</li><li>then the probability：$P(i|v) = \frac{exp (v^T_iv/\tau)}{\sum exp (v^T_jv / \tau)}$</li><li>temperature param $\tau$：controls the concentration level of the distribution </li><li><p>the goal is to minimize the negative log-likelihood</p></li><li><p>意义：L2 norm将所有的representation映射到了一个128-d unit sphere上面，$v_i^T v_j$度量了两个projection vec的similarity，我们希望同类的vec尽可能重合，不同类的vec尽可能正交</p><ul><li>class weights $W$ are not generalized to new classes</li><li>but feature representations $V$ does</li></ul></li></ul></li><li><p>memory bank</p><ul><li>因为是instance level，C-classes对应整个training set，也就是说${v_i}$ for all the images are needed for loss</li><li>Let $V={v_i}$ 表示memory bank，初始为unit random vectors</li><li>every learning iterations<ul><li>$f_\theta$ is optimized by SGD</li><li>输入$x_i$所对应的$f_i$更新到$v_i$上</li><li>也就是只有mini-batch中包含的样本，在这一个step，更新projection vec</li></ul></li></ul></li></ul></li><li><p>Noise-Contrastive Estimation</p><ul><li><p>non-parametric softmax的计算量随着样本量线性增长，millions level样本量的情况下，计算太heavy了</p></li><li><p>we use NCE to approximate the full softmax</p></li><li><p>assume</p><ul><li>noise samples的uniform distribution：$P_n =\frac{1}{n}$</li><li>noise samples are $m$ times frequent than data samples</li></ul></li><li><p>那么sample $i$ matches vec $v$的后验概率是：$h(i,v)=\frac{P(i|v)}{P(i|v)+mP_n}$</p><ul><li>approximated training object is to minimize the negative log-likelihood of $h(i,v)$</li><li><img src="/2021/05/19/memory-bank/NCE.png" width="40%;"></li></ul></li><li><p>normalizing constant $Z$的近似</p><ul><li><p><img src="/2021/05/19/memory-bank/norm.png" width="40%;"></p></li><li><p>主要就是分母这个$Z_i$的计算比较heavy，我们用Monte Carlo采样来近似：</p><p>  <img src="/2021/05/19/memory-bank/monte.png" width="40%;"></p></li><li><p>${j_k}$ is a random subset of indices：随机抽了memory bank的一个子集来approx全集的分母，实验发现取batch size大小的子集就可以，m=4096</p></li></ul></li></ul></li><li><p>Proximal Regularization</p><ul><li><p>the learning process oscillates a lot</p><ul><li>we have one instance per class</li><li>during each training epoch each class is only visited once</li></ul></li><li><p>we introduce an additional term</p><ul><li>overall workflow：在每一个iteration t，feature representation是$v_i^t=f_{\theta}(x_i)$，而memory bank里面的representations来自上一个iteration step $V={v^{t-1}}$，我们从memory bank里面采样，并计算NCE loss，然后bp更新网络权重，然后将这一轮fp的representations update到memory bank的指定样本上，然后下一轮</li><li>可以发现，在初始random阶段，梯度更新会比较快而且不稳定</li><li><p>我们给positive sample的loss上额外加了一个$\lambda ||v_i^t-v_i^{t-1}||^2_2$，有点类似weight decay那种东西，开始阶段l2 loss会占主导，引导网络收敛</p></li><li><p><img src="/2021/05/19/memory-bank/proximal.png" width="50%;"></p></li><li><p>stabilize</p></li><li>speed up convergence</li><li>improve the learned representations </li></ul></li></ul></li><li><p>Weighted k-Nearest Neighbor Classifier</p><ul><li>a test time，先计算feature representation，然后跟memory bank的vectors分别计算cosine similarity $s_i=cos(v_i, f)$，选出topk neighbours $N_k$，然后进行weighted voting</li><li>weighted voting：<ul><li>对每个class c，计算它在topk neighbours的total weight，$w_c =\sum_{i \in N_k} \alpha_i 1(c_i=c)$</li><li>$\alpha_i = exp(s_i/\tau)$</li></ul></li><li>k = 200</li><li>$\tau = 0.07$</li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> Unsupervised Learning, NCE </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>MoCo系列</title>
      <link href="/2021/04/30/MoCo%E7%B3%BB%E5%88%97/"/>
      <url>/2021/04/30/MoCo%E7%B3%BB%E5%88%97/</url>
      <content type="html"><![CDATA[<p>papers：</p><p>[2019 MoCo v1] Momentum Contrast for Unsupervised Visual Representation Learning，kaiming</p><p>[2020 SimCLR] A Simple Framework for Contrastive Learning of Visual Representations，Google Brain，混进来是因为它improve based on MoCo v1，而MoCo v2/v3又都是基于它改进</p><p>[2020 MoCo v2] Improved Baselines with Momentum Contrastive Learning，kaiming</p><p>[2021 MoCo v3] An Empirical Study of Training Self-Supervised Visual Transformers，kaiming</p><h2 id="preview-自监督学习-Self-supervised-Learning"><a href="#preview-自监督学习-Self-supervised-Learning" class="headerlink" title="preview: 自监督学习 Self-supervised Learning"></a>preview: 自监督学习 Self-supervised Learning</h2><ol><li><p>reference：<a href="https://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html" target="_blank" rel="noopener">https://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html</a></p></li><li><p>overview</p><p><img src="/2021/04/30/MoCo系列/无监督.png" width="40%;"></p><ul><li>就是无监督</li><li>针对的痛点（有监督训练模型）<ul><li>标注成本高</li><li>迁移性差</li></ul></li><li>会基于数据特点，设置Pretext tasks（最常见的任务就是生成/重建），构造Pesdeo Labels来训练网络</li><li>通常模型用来作为其他学习任务的预训练模型</li><li>被认为是用来学习图像的通用视觉表示</li></ul></li><li><p>methods</p><ul><li><p>从结构上区分主要就是两大类方法</p><ul><li>生成式：通过encoder-decoder结构还原输入，监督信号是输入输出尽可能相似<ul><li>重建任务开销大</li><li>没有建立直接的语义学习</li><li>外加GAN的判别器使得任务更加复杂难训</li></ul></li><li>判别式：输入两张图片，通过encoder编码，监督信号是判断两张图是否相似，判别式模型也叫Contrastive Learning</li></ul><p><img src="/2021/04/30/MoCo系列/self-supervised.png" width="70%;"></p></li><li><p>从Pretext tasks上划分主要分为三类</p><ul><li>基于上下文（Context based） ：如bert的MLM，在句子/图片中随机扣掉一部分，然后推动模型基于上下文/语义信息预测这部分/相对位置关系</li><li>基于时序（Temporal Based）：如bert的NSP，视频/语音，利用相邻帧的相似性，构建不同排序的序列，判断B是否是A的下一句/是否相邻帧</li><li>基于对比（Contrastive Based）：比较正负样本，最大化相似度的loss在这里面被叫做InfoNCE</li></ul></li></ul></li><li><p>memory-bank</p><ul><li>Contrastive Based方法最常见的方式是在一个batch中构建正负样本进行对比学习<ul><li>end-to-end</li><li>每个mini-batch中的图像增强前后的两张图片互为正样本</li><li>字典大小就是minibatch大小</li></ul></li><li>memory bank包含数据集中所有样本编码后特征<ul><li>随机采样一部分作为keys</li><li>每个迭代只更新被采样的样本编码</li><li>因为样本编码来自不同的training step，一致性差</li></ul></li><li><p>MoCo</p><ul><li>动态编码库：out-of-date的编码出列</li><li>momentum update：一致性提升</li></ul><p><img src="/2021/04/30/MoCo系列/memory-bank.png" width="60%;"></p></li></ul></li><li><p>InfoNCE</p><ul><li><p>deep mind在CPC(Contrastive Predictive Coding)提出，论文以后有机会再展开</p><ul><li>unsupervised</li><li>encoder：encode x into latent space representations z，resnet blocks</li><li>autoregressive model：summarize each time-step set of {z} into a context representation c，GRUs</li><li><p>probabilistic contrastive loss</p><ul><li>Noise-Contrastive Estimation</li><li>Importance Sampling</li></ul><p><img src="/2021/04/30/MoCo系列/CPC.png" width="60%;"></p></li></ul></li><li><p>训练目标是输入数据x和context vector c之间的mutual information</p><ul><li>每次从$p(x_{t+k}|c_t)$中采样一个正样本：正样本是这个序列接下来预测的东西，和c的相似性肯定要高于不想干的token</li><li>从$p(x_{t+k})$中采样N-1个负样本：负样本是别的序列里面随机采样的东西</li><li><p>目标是让正样本与context相关性高，负样本低</p><p><img src="/2021/04/30/MoCo系列/InfoNCE.png" width="30%;"></p></li></ul></li></ul></li></ol><h2 id="MoCo-v1-Momentum-Contrast-for-Unsupervised-Visual-Representation-Learning"><a href="#MoCo-v1-Momentum-Contrast-for-Unsupervised-Visual-Representation-Learning" class="headerlink" title="MoCo v1: Momentum Contrast for Unsupervised Visual Representation Learning"></a>MoCo v1: Momentum Contrast for Unsupervised Visual Representation Learning</h2><ol><li><p>动机</p><ul><li>unsupervised visual representation learning </li><li>contrastive learning </li><li><p>dynamic dictionary</p><ul><li>large</li><li>consisitent</li></ul></li><li><p>verified on</p><ul><li>7 down-stream tasks</li><li>ImageNet classification</li><li>VOC &amp; COCO det/seg</li></ul></li></ul></li><li><p>论点</p><ul><li><p>Unsupervised representation learning </p><ul><li>highly successful in NLP，in CV supervised is still the main-stream</li><li>两个核心<ul><li>pretext tasks</li><li>loss functions</li></ul></li><li>loss functions<ul><li>生成式方法的loss是基于prediction和一个fix target来计算的</li><li>contrastive-based的key target则是vary on-the-fly during training</li><li>Adversarial losses没展开</li></ul></li><li>pretext tasks<ul><li>tasks involving recover：auto-encoder</li><li>task involving pseudo-labels：通常有个exemplar/anchor，然后计算contrastive loss </li></ul></li><li>contrastive learning VS pretext tasks<ul><li>大量pretext tasks可以通过设计一些contrastive loss来实现</li></ul></li></ul></li><li><p>recent approaches using contrastive loss</p><ul><li>dynamic dictionaries<ul><li>由keys组成：sampled from data &amp; represented by an encoder </li></ul></li><li>train the encoder to perform dictionary look-up<ul><li>given an encoded query</li><li>similar to its matching key and dissimilar to others</li></ul></li></ul></li><li><p>desirable dictionary </p><ul><li>large：better sample</li><li>consistent：training target consistent</li></ul></li><li><p>MoCo：Momentum Contrast</p><ul><li>queue</li><li>每个it step的mini-batch的编码入库</li><li>the oldest are dequeued</li><li><p>EMA：</p><ul><li>a slowly progressing key encoder</li><li>momentum-based moving average of the query encoder</li></ul><p><img src="/2021/04/30/MoCo系列/MoCo.png" width="50%;"></p></li><li><p>similar的定义：q &amp; k are from the same image</p></li></ul></li></ul></li><li><p>方法</p><ul><li><p>contrastive learning</p><ul><li>a encoded query $q$</li><li>a set of encoded samples $\{k_0, k_1, …\}$</li><li>assume：there is a single key $k_+$ in the dictionary that $q$ matches</li><li>similarity measurement：dot product</li><li>InfoNCE：<ul><li>$L_q = -log \frac{exp(qk_+/\tau)}{\sum_0^K exp(qk/\tau)}$</li><li>1 positive &amp; K negtive samples</li><li>本质上是个softmax-based classifier，尝试将$q$分类成$k_+$</li></ul></li><li>unsupervised workflow<ul><li>with a encoder network $f_q$ &amp; $f_k$</li><li>thus we have query &amp; sample representation $q=f_q(x^q)$ &amp; $k=f_k(x^k)$</li><li>inputs $x$ can be images/patches/context(patches set)</li><li>$f_q$ &amp; $f_k$ can be identical/partially shared/different</li></ul></li></ul></li><li><p>momentum contrast</p><ul><li><p>dictionary as a key</p><ul><li>the dictionary always represents a sampled subset of all data</li><li>the current mini-batch入列</li><li>the oldest mini-batch出列</li></ul></li><li><p>momentum update</p><ul><li><p>large dictionary没法对keys进行back-propagation：因为sample太多了</p></li><li><p>only $f_q$ are updated by back-propagation：mini-batch</p></li><li><p>naive solution：copy $f_q$的参数给$f_k$，yields poor results，因为key encoder参数变化太频繁了，representation inconsistent issue</p></li><li><p>momentum update：$f_k = mf_k + (1-m)f_q$，$m=0.999$</p></li><li><p>三种更新方式对比</p><p>  <img src="/2021/04/30/MoCo系列/key encoder.png" width="80%;"></p><ul><li>第一种end-to-end method：<ul><li>use samples in current mini-batch as the dictionary</li><li>keys are consistently encoded</li><li>dictionary size is limited</li></ul></li><li>第二种memory bank<ul><li>A memory bank consists of the representations of all samples in the dataset</li><li>the dictionary for each mini-batch is randomly sampled from the memory bank，不进行bp，thus enables large dictionary</li><li>key representation is updated when it was last seen：inconsistent</li><li>有些也用momentum update，但是是用在representation上，而不是encoder参数</li></ul></li></ul></li></ul></li><li><p>pretext task</p><ul><li>define positive pair：if the query and the key come from the same image</li><li>我们从图上take two random views under random augmentation to form a positive pair</li><li>然后用各自的encoder编码成q &amp; k</li><li>每一对计算similarity：pos similarity</li><li>然后再计算input queries和dictionary的similarity：neg similarity</li><li>计算ce，update $f_q$</li><li>用$f_q$ update $f_k$</li><li>把k加入dictionary队列</li><li><p>把最早的mini-batch出列</p><p><img src="/2021/04/30/MoCo系列/MoCo code.png" width="50%;"></p></li><li><p>技术细节</p><ul><li>resnet：last fc dim=128，L2 norm</li><li>temperature $\tau=0.07$</li><li>augmentation<ul><li>random resize + random(224,224) crop</li><li>random color jittering</li><li>random horizontal flip</li><li>random grayscale conversion</li></ul></li><li>shuffling BN<ul><li>实验发现使用resnet里面的BN会导致不好的结果：猜测是intra-batch communication引导模型学习了一种cheating的low-loss solution</li><li>具体做法是给$f_k$的输入mini-batch先shuffle the order，然后进行fp，然后再shuffle back，这样$f_q$和$f_k$的BN计算的mini-batch的statics就是不同的</li></ul></li></ul></li></ul></li></ul></li></ul></li><li><p>实验</p></li></ol><h2 id="SimCLR-A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations"><a href="#SimCLR-A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations" class="headerlink" title="SimCLR: A Simple Framework for Contrastive Learning of Visual Representations"></a>SimCLR: A Simple Framework for Contrastive Learning of Visual Representations</h2><ol><li><p>动机</p><ul><li>simplify recently proposed contrastive self-supervised learning algorithms </li><li>systematically study the major components <ul><li>data augmentations</li><li>learnable unlinear prediction head</li><li>larger batch size and more training steps</li></ul></li><li><p>outperform previous self-supervised &amp; semi-supervised learning methods on ImageNet</p><p><img src="/2021/04/30/MoCo系列/acc.png" width="40%;"></p></li></ul></li><li><p>论点</p><ul><li><p>discriminative approaches based on contrastive learning</p><ul><li>maximizing agreement between differently augmented views of the same data sample </li><li>via a contrastive loss in the latent space</li></ul></li><li><p>major components &amp; conclusions</p><ul><li>数据增强很重要，unsupervised比supervised benefits more</li><li>引入的learnable nonlinear transformation提升了representation quality</li><li>contrastive cross entropy loss受益于normalized embedding和adjusted temperature parameter</li><li>larger batch size and more training steps很重要，unsupervised比supervised benefits more</li></ul></li></ul></li><li><p>方法</p><ul><li><p>common framework</p><p>  <img src="/2021/04/30/MoCo系列/framework.png" width="40%;"></p><ul><li><p>4 major components</p><ul><li>随机数据增强<ul><li>results in two views of the same sample，构成positive pair</li><li>crop + resize back + color distortions + gaussian blur</li></ul></li><li>base encoder<ul><li>用啥都行，本文用了resnet including the GAP</li></ul></li><li>a projection head<ul><li>将representation dim映射到the space where contrastive loss is applied（given 1 pos pair &amp; N neg pair，就是N+1 dim）</li><li>之前有方法直接用linear projection</li><li>我们用了带一个hidden layer的MLP：fc-bn-relu-fc</li></ul></li><li>a contrastive loss</li></ul></li><li><p>overall workflow</p><ul><li>random sample a minibatch of N</li><li>random augmentation results in 2N data points</li><li>对每个样本来讲，有1个positive pair，其余2(N-1)个data points都是negative samples</li><li>set cosine similarity $sim(u,v)=u^Tv/|u||v|$</li><li>given positive pair $(i,j)$ then the loss is $l_{i,j} = -log \frac{exp(s_{i,j}/\tau)}{\sum_{k\neq i}^{2N} exp(s_{i,k}/\tau)}$</li><li>对每个positive pair都计算，包括$(i,j)$ 和$(j,i)$，叫那个symmetrized loss</li><li><p>update encoder</p><p><img src="/2021/04/30/MoCo系列/SimCLR.png" width="40%;"></p></li></ul></li></ul></li><li><p>training with large batch size</p><ul><li>batch 8192，negatives 16382</li><li>大batch时，linear learning rate scaling可能不稳定，所以用了LARS optmizer</li><li>global BN，aggregate BN mean &amp; variance over all devices</li><li>TPU</li></ul></li></ul></li></ol><h2 id="MoCo-v2-Improved-Baselines-with-Momentum-Contrastive-Learning"><a href="#MoCo-v2-Improved-Baselines-with-Momentum-Contrastive-Learning" class="headerlink" title="MoCo v2: Improved Baselines with Momentum Contrastive Learning"></a>MoCo v2: Improved Baselines with Momentum Contrastive Learning</h2><ol><li><p>动机</p><ul><li>still working on contrastive unsupervised learning</li><li>simple modifications on MoCo<ul><li>introduce two effective SimCLR’s designs：</li><li>an MLP head</li><li>more data augmentation</li><li>requires smaller batch size than SimCLR，making it possible to run on GPU</li></ul></li><li>verified on<ul><li>ImageNet classification </li><li>VOC detection</li></ul></li></ul></li><li><p>论点</p><ul><li>MoCo &amp; SimCLR <ul><li>contrastive unsupervised learning frameworks</li><li>MoCo v1 shows promising</li><li>SimCLR further reduce the gap</li><li>we found two design imrpovements in SimCLR 在两个方法中都work，而且用在MoCo中shows better transfer learning results<ul><li>an MLP projection head</li><li>stronger data augmentation</li></ul></li><li>同时MoCo framework相比较于SimCLR ，远不需要large training batches<ul><li>SimCLR based on end-to-end mechanism，需要比较大的batch size，来提供足够多的negative pair</li><li>MoCo则用了动态队列，所以不限制batch size</li></ul></li></ul></li><li>SimCLR <ul><li>improves the end-to-end method</li><li>larger batch：to provide more negative samples</li><li>output layer：replace fc with a MLP head</li><li>stronger data augmentation</li></ul></li><li><p>MoCo</p><ul><li>a large number of negative samples are readily available</li><li>所以就把后两项引入进来了</li></ul><p><img src="/2021/04/30/MoCo系列/optimization.png" width="50%;"></p></li></ul></li><li><p>方法</p><ul><li><p>MLP head</p><ul><li>2-layer MLP(hidden dim=2048, ReLU)</li><li>仅影响unsupervised training，有监督transfer learning的时候换头</li><li><p>temperature param调整：从default 0.07 调整成optimal value 0.2</p><p><img src="/2021/04/30/MoCo系列/temperature.png" width="50%;"></p></li></ul></li><li><p>augmentation</p><ul><li>add blur</li><li>SimCLR还用了stronger color distortion：we found stronger color distortion in SimCLR hurts in our MoCo，所以没加</li></ul></li></ul></li><li><p>实验</p><ul><li><p>ablation</p><ul><li>MLP：在分类任务上的提升比检测大</li><li><p>augmentation：在检测上的提升比分类大</p><p><img src="/2021/04/30/MoCo系列/ablation.png" width="50%;"></p></li></ul></li><li><p>comparison</p><p>  <img src="/2021/04/30/MoCo系列/comparison.png" width="50%;"></p><ul><li>large batches are not necessary for good acc：SimCLR longer training那个版本精度更高</li><li>end-to-end的方法肯定more costly in memory and time：因为要bp两个encoder</li></ul></li></ul></li></ol><h2 id="MoCo-v3-An-Empirical-Study-of-Training-Self-Supervised-Visual-Transformers"><a href="#MoCo-v3-An-Empirical-Study-of-Training-Self-Supervised-Visual-Transformers" class="headerlink" title="MoCo v3: An Empirical Study of Training Self-Supervised Visual Transformers"></a>MoCo v3: An Empirical Study of Training Self-Supervised Visual Transformers</h2><ol><li><p>动机</p><ul><li>self-supervised frameworks that based on Siamese network, including MoCo</li><li>ViT：study the fundamental components for training self-supervised ViT</li><li>MoCo v3：an incremental improvement of MoCo v1/2，striking for a better balance of simplicity &amp; accuracy &amp; scalability</li><li>instability is a major issue</li><li>scaling up ViT models<ul><li>ViT-Large</li><li>ViT-Huge</li></ul></li></ul></li><li><p>论点</p><ul><li>we go back to the basics and investigate the fundamental components of training deep neural networks<ul><li>batch size</li><li>learning rate</li><li>optmizer</li></ul></li><li>instability <ul><li>instability is a major issue that impacts self-supervised ViT training</li><li>but may not result in catastrophic failure，只会导致精度损失</li><li>所以称之为hidden degradation</li><li>use a simple trick to improve stability：freeze the patch projection layer in ViT</li><li>and observes increasement in acc</li></ul></li><li>NLP里面基于masked auto-encoding的framework效果要比基于contrastvie的framework好，图像正好反过来</li></ul></li><li><p>方法</p><ul><li><p>MoCo v3</p><ul><li>take two crops for each image under random augmentation</li><li>encoded by two encoders $f_q$ &amp; $f_k$ into vectors $q$ &amp; $k$</li><li>we use the keys that naturally co-exist in the same batch<ul><li>abandon the memory queue：因为发现batch size足够大（4096）的时候，memory queue就没啥acc gain了</li><li>回归到batch-based sample pair</li></ul></li><li>但是encoder k仍旧不回传梯度，还是基于encoder q进行动量更新</li><li><p>symmetrized loss：</p><ul><li>$ctr(q_1, k_2) + ctr(q_2,k_1)$</li><li>InfoNCE</li><li>temperature</li><li>两个crops分别计算ctr</li></ul><p><img src="/2021/04/30/MoCo系列/MoCo v3.png" width="50%;"></p></li></ul></li><li><p>encoder </p><ul><li>encoder $f_q$<ul><li>a backbone</li><li>a projection head</li><li>an extra prediction head</li></ul></li><li>encoder $f_k$<ul><li>a backbone</li><li>a projection head</li></ul></li><li>encoder $f_k$ is updated by the moving average of $f_q$，excluding the prediction head</li></ul></li><li><p>baseline acc</p><p>  <img src="/2021/04/30/MoCo系列/v3 base.png" width="50%;"></p><ul><li>basic settings，主要变动就是两个：<ul><li>dynamic queue换成large batch</li><li>encoder $f_q$的extra prediction head</li></ul></li></ul></li><li><p>use ViT</p><ul><li><p>直接用ViT替换resnet back met instability issue</p></li><li><p>batch size</p><ul><li><p>ViT里面的一个观点就是，model本身比较heavy，所以large batch is desirable</p></li><li><p>实验发现</p><ul><li>a batch of 1k &amp; 2k produces reasonably smooth curves：In this regime, the larger batch improves accuracy thanks to more negative samples</li><li>a batch of 4k 有明显的untable dips：</li><li><p>a batch of 6k has worse failure patterns：我们解读为在跳水点，training is partially restarted and jumps out of the current local optimum</p><p><img src="/2021/04/30/MoCo系列/batch.png" width="50%;"></p></li></ul></li></ul></li><li><p>learning rate</p><ul><li>lr较小，training比较稳定，但是容易欠拟合</li><li>lr过大，会导致unstable，也会影响acc</li><li><p>总体来说精度还是决定于stability</p><p><img src="/2021/04/30/MoCo系列/lr.png" width="50%;"></p></li></ul></li><li><p>optimizer</p><ul><li>default adamW，batch size 4096</li><li>有些方法用了LARS &amp; LAMB for large-batch training</li><li><p>LAMB </p><ul><li>sensitive to lr</li><li>optmal lr achieves slightly better accuracy than AdamW</li><li>但是lr一旦过大，acc极速drop</li><li>但是training curves still smooth，虽然中间过程有drop：我们解读为LAMB can avoid sudden change in the gradients，但是避免不了negative compact，还是会累加</li></ul><p><img src="/2021/04/30/MoCo系列/lamb.png" width="45%;"></p></li></ul></li><li><p>a trick for improving stability</p><ul><li>we found a spike in gradient causes a dip in the training curve</li><li>we also observe that gradient spikes happen earlier in the first layer (patch projection)</li><li><p>所以尝试freezing the patch projection layer during training，也就是一个random的patch projection layer</p><ul><li>This stability benefits the final accuracy</li><li>The improvement is bigger for a larger lr</li><li>在别的ViT-back-framework上也有效（SimCLR、BYOL）</li></ul><p><img src="/2021/04/30/MoCo系列/random.png" width="45%;"></p></li></ul></li><li><p>we also tried BN，WN，gradient clip</p><ul><li>BN/WN does not improve</li><li>gradient clip在threshold足够小的时候有用，推到极限就是freezing了</li></ul></li></ul></li><li><p>implementation details</p><ul><li>AdamW</li><li>batch size 4096</li><li>lr：warmup 40 eps then cosine decay</li></ul></li><li><p>MLP heads</p><ul><li>projection head：3-layers，4096-BN-ReLU-4096-BN-ReLU-256</li><li>prediction head：2-layers，4096-BN-ReLU-256</li></ul></li><li><p>loss</p><ul><li>ctr里面有个scale的参数，$2\tau$</li><li>makes it less sensitive to $\tau$ value</li><li>$\tau=0.2$</li></ul></li><li><p>ViT architecture</p><p>  <img src="/2021/04/30/MoCo系列/ViT.png" width="45%;"></p><ul><li>跟原论文保持一致</li><li>输入是224x244的image，划分成16x16/14x14的patch sequence，project成256d/196d的embedding</li><li>加上sine-cosine-2D的PE</li><li>再concat一个cls token</li><li>经过一系列transformer blocks</li><li>The class token after the last block (and after the final LayerNorm) is treated as the output of the backbone，and is the input to the MLP heads</li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> self-supervised learning, transformer, contrastive loss </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>optimizers优化器</title>
      <link href="/2021/03/15/optimizers%E4%BC%98%E5%8C%96%E5%99%A8/"/>
      <url>/2021/03/15/optimizers%E4%BC%98%E5%8C%96%E5%99%A8/</url>
      <content type="html"><![CDATA[<h2 id="0-overview"><a href="#0-overview" class="headerlink" title="0. overview"></a>0. overview</h2><p>keywords：SGD, moment, Nesterov, adaptive, ADAM, Weight decay</p><ol><li>优化问题Optimization<ul><li>to minimize目标函数</li><li>grandient decent<ul><li>gradient<ul><li>numerical：数值法，approx，slow</li><li>analytical：解析法，exact，fast</li></ul></li><li>Stochastic<ul><li>用minibatch的梯度来approximate全集</li><li>$\theta_{k+1} = \theta_k - v_{t+1}(x_i,y_i)$</li></ul></li><li>classic optimizers：SGD，Momentum，Nesterov‘s momentum</li><li>adaptive optimizers：AdaGrad，Adadelta，RMSProp，Adam</li></ul></li><li>Newton</li></ul></li></ol><ul><li>modern optimizers for large-batch<pre><code>  * AdamW</code></pre><ul><li>LARS</li><li>LAMB</li></ul></li></ul><ol><li><p>common updating steps</p><p> for current step t：</p><p> step1：计算直接梯度，$g_t = \nabla f(w_t)$</p><p> step2：计算一阶动量和二阶动量，$m_t \&amp; V_t$</p><p> step3：计算当前时刻的下降梯度，$\eta_t = \alpha m_t/\sqrt {V_t}$</p><p> step4：参数更新，$w_{t+1} = w_t - \eta_t$</p><ul><li>各种优化算法的主要差别在step1和step2上</li></ul></li><li><p>滑动平均/指数加权平均/moving average/EMA</p><ul><li>局部均值，与一段时间内的历史相关</li><li>$v_t = \beta v_{t-1}+(1-\beta)\theta_t$，大致等于过去$1/(1-\beta)$个时刻的$\theta$的平均值，但是在起始点附近偏差较大</li><li>$v_{tbiased} = \frac{v_t}{1-\beta^t}$，做了bias correction</li><li>t越大，越不需要修正，两个滑动均值的结果越接近</li><li><p>优缺点：不用保存历史，但是近似</p><p><img src="/2021/03/15/optimizers优化器/moving average.png" width="50%;"></p></li></ul></li><li><p>SGD</p><ul><li>SGD没有动量的概念，$m_t=g_t$，$V_t=I^2$，$w_{t+1} = w_t - \alpha g_t$</li></ul></li></ol><ul><li>仅依赖当前计算的梯度<ul><li>缺点：下降速度慢，可能陷在local optima上持续震荡</li></ul></li></ul><ol><li><p>SGDW (with weight decay)</p><ul><li>在权重更新的同时进行权重衰减</li><li>$w_{t+1} = (1-\lambda)w_t - \alpha g_t$</li><li>在SGD form的优化器中weight decay等价于在loss上L2 regularization</li><li>但是在adaptive form的优化器中是不等价的！！因为historical func（ERM）中regularizer和gradient一起被downscale了，因此not as much as they would get regularized in SGDW</li></ul></li><li><p>SGD with Momentum</p><ul><li>引入一阶动量，$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$，使用滑动均值，抑制震荡</li><li>梯度下降的主要方向是此前累积的下降方向，略微向当前时刻的方向调整</li></ul></li><li><p>SGD with Nesterov Acceleration</p><ul><li>look ahead SGD-momentum</li><li>在local minima的时候，四周没有下降的方向，但是如果走一步再看，可能就会找到优化方向</li><li>先跟着累积动量走一步，求梯度：$g_t = \nabla f(w_t-\alpha m_{t-1}/\sqrt {V_{t-1}})$</li><li>用这个点的梯度方向来计算滑动平均，并更新梯度</li></ul></li><li><p>Adagrad</p><ul><li>引入二阶动量，开启“自适应学习率”，$V_t = \sum_0^t g_k^2$，度量历史更新频率</li><li>对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些</li><li>$\eta_t = \alpha m_t / \sqrt{V_t}$，本质上为每个参数，对学习率分别rescale</li><li>缺点：二阶动量单调递增，导致学习率单调衰减，可能会使得训练过程提前结束</li></ul></li><li><p>AdaDelta/RMSProp</p><ul><li>参考momentum，对二阶动量也计算滑动平均，$V_t = \beta_2 V_{t-1} + (1-\beta_2)g_t^2$</li><li>避免了二阶动量持续累积、导致训练过程提前结束</li></ul></li><li><p>Adam</p><ul><li>集大成者：把一阶动量和二阶动量都用起来，Adaptive Momentum<ul><li>SGD-M在SGD基础上增加了一阶动量</li><li>AdaGrad和AdaDelta在SGD基础上增加了二阶动量</li></ul></li><li>一阶动量滑动平均：$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$</li><li>二阶动量滑动平均：$V_t = \beta_2 V_{t-1} + (1-\beta_2)g_t^2$</li></ul></li><li><p>Nadam</p><ul><li>look ahead Adam</li><li>把Nesterov的one step try加上：$g_t = \nabla f(w_t-\alpha m_{t-1}/\sqrt {V_{t-1}})$</li><li>再Adam更新两个动量</li></ul></li><li><p>经验超参</p><ul><li>$momentum=0.9$</li><li>$\beta_1=0.9$</li><li>$\beta_2=0.999$</li><li>$m_0 = 0$</li><li>$V_0 = 0$</li><li>上面的图上可以看出，初期的$m_t$和$V_t$会无限接近于0，此时可以进行误差修正：$factor=\frac{1}{1-\beta^t}$</li></ul></li><li><p>AdamW</p><ul><li>在adaptive methods中，解耦weight-decay和loss-based gradient在ERM过程中的绑定downscale的关系</li><li>实质就是将导数项后移</li></ul><p><img src="/2021/03/15/optimizers优化器/AdamW.png" width="80%;"></p></li></ol>]]></content>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>regnet</title>
      <link href="/2021/03/11/regnet/"/>
      <url>/2021/03/11/regnet/</url>
      <content type="html"><![CDATA[<h2 id="RegNet-Designing-Network-Design-Spaces"><a href="#RegNet-Designing-Network-Design-Spaces" class="headerlink" title="RegNet: Designing Network Design Spaces"></a>RegNet: Designing Network Design Spaces</h2><ol><li><p>动机</p><ul><li>study the network design principles</li><li>design RegNet </li><li>outperforms efficientNet and 5x faster<ul><li>top1 error：20.1 （eff-b5：21.5）</li><li>larger batch size</li><li>1/4 的 train/test latency</li></ul></li></ul></li><li><p>论点</p><ul><li>manual network design<ul><li>AlexNet, ResNet family, DenseNet, MobileNet</li><li>focus on discovering new design choices that improve acc </li></ul></li><li>the recent popular approach NAS<ul><li>search the best in a fixed search space of possible networks</li><li>limitations：generalize to new settings，lack of interpretability  </li></ul></li><li>network scaling<ul><li>上面两个focus on 找出一个basenet for a specific regime</li><li>scaling rules aims at tuning the optimal network in any target regime </li></ul></li><li>comparing networks<ul><li>the reliable comparison metric to guide the design process</li></ul></li><li>our method<ul><li>combines the disadvantages of manual design and NAS</li><li>first AnyNet</li><li>then RegNet</li></ul></li></ul></li><li><p>方法</p></li></ol>]]></content>
      
      
    </entry>
    
    <entry>
      <title>mongodb</title>
      <link href="/2021/03/09/mongodb/"/>
      <url>/2021/03/09/mongodb/</url>
      <content type="html"><![CDATA[<ol><li><p>download：<a href="https://www.mongodb.com/try/download/enterprise" target="_blank" rel="noopener">https://www.mongodb.com/try/download/enterprise</a></p></li><li><p>install</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 将解压以后的文件夹放在/usr/<span class="built_in">local</span>下</span></span><br><span class="line">sudo mv mongodb-osx-x86_64-4.0.9/ /usr/local/</span><br><span class="line">sudo ln -s mongodb-macos-x86_64-4.4.4 mongodb</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ENV PATH</span></span><br><span class="line">export PATH=/usr/local/mongodb/bin:$PATH</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建日志及数据存放的目录</span></span><br><span class="line">sudo mkdir -p /usr/local/var/mongodb</span><br><span class="line">sudo mkdir -p /usr/local/var/log/mongodb</span><br><span class="line">sudo chown [amber] /usr/local/var/mongodb</span><br><span class="line">sudo chown [amber] /usr/local/var/log/mongodb</span><br></pre></td></tr></table></figure></li><li><p>configuration</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 后台启动</span></span><br><span class="line">mongod --dbpath /usr/local/var/mongodb --logpath /usr/local/var/log/mongodb/mongo.log --fork</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 控制台启动</span></span><br><span class="line">mongod --config /usr/local/etc/mongod.conf</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看状态</span></span><br><span class="line">ps aux | grep -v grep | grep mongod</span><br></pre></td></tr></table></figure></li><li><p>run</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在db环境下启动一个终端</span></span><br><span class="line">cd /usr/local/mongodb/bin </span><br><span class="line">./mongo</span><br></pre></td></tr></table></figure></li><li><p>original settings</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 显示所有数据的列表</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> show dbs</span></span><br><span class="line">admin   0.000GB</span><br><span class="line">config  0.000GB</span><br><span class="line">local   0.000GB</span><br><span class="line"><span class="meta">#</span><span class="bash"> 三个系统保留的特殊数据库</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 连接/创建一个指定的数据库</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> use <span class="built_in">local</span></span></span><br><span class="line">switched to db local</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示当前数据库, 如果没use默认为<span class="built_in">test</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db</span></span><br><span class="line">test</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 【！！重要】关闭服务</span></span><br><span class="line">之前服务器被kill -9强制关闭，数据库丢失了</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> use admin</span></span><br><span class="line">switched to db admin</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.shutdownServer()</span></span><br><span class="line">server should be down...</span><br></pre></td></tr></table></figure></li><li><p>concepts</p><p> <img src="/2021/03/09/mongodb/concepts.png" width="90%;"></p></li><li><p>文档document</p><p> 一组key-value对，如上面左图中的一行记录，如上面右图中的一个dict</p></li><li><p>集合collection</p><p> 一张表，如上面左图和上面右图</p></li><li><p>主键primary key</p><p> 唯一主键，ObjectId类型，自定生成，有标准格式</p></li><li><p>常用命令</p><p>10.1 创建/删除/重命名db</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 切换至数据库test1</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> use test1</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 插入一条doc, db.COLLECTION_NAME.insert(document)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> db要包含至少一条文档，才能在show dbs的时候显示（才真正创建）</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.sheet1.insert(&#123;<span class="string">'name'</span>: img0&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示当前已有数据库</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> show dbs</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除指定数据库</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> use test1</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.dropDatabase()</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 旧版本(before4.0)重命名：先拷贝一份，在删除旧的</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.copyDatabase(<span class="string">'OLDNAME'</span>, <span class="string">'NEWNAME'</span>);</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> use old_name</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.dropDatabase()</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 新版本重命名：dump&amp;restore，这个东西在mongodb tools里面，要另外下载，可执行文件放在bin下</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> mongodump   <span class="comment"># 将所有数据库导出到bin/dump/以每个db名字命名的文件夹下</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> mongodump -h dbhost -d dbname -o dbdirectory</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -h: 服务器地址:端口号</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -d: 需要备份的数据库</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -o: 存放位置（需要已存在）</span></span><br><span class="line">mongodump -d test -o tmp/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在恢复备份数据库的时候换个名字：mongorestore -h dbhost -d dbname path</span></span><br><span class="line">mongorestore -d test_bkp tmp/test</span><br><span class="line"><span class="meta">#</span><span class="bash"> 这时候可以看到一个新增了一个叫test_bkp的db</span></span><br></pre></td></tr></table></figure><p>10.2 创建/删除/重命名collection</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建：db.createCollection(name, options)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.createCollection(<span class="string">'case2img'</span>)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示已有tables</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> show collections</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 不用显示创建，在db insert的时候会自动创建集合</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.sheet2.insert(&#123;<span class="string">"name"</span> : <span class="string">"img2"</span>&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除：db.COLLECTION_NAME.drop()</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.sheet2.drop()</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 重命名：db.COLLECTION_NAME.renameCollection(<span class="string">'NEWNAME'</span>)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.sheet2.renameCollection(<span class="string">'sheet3'</span>)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 复制：db.COLLECTION_NAME.aggregate(&#123;<span class="variable">$out</span>: <span class="string">'NEWNAME'</span>&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.sheet2.aggregate(&#123; <span class="variable">$out</span> : <span class="string">"sheet3"</span> &#125;)</span></span><br></pre></td></tr></table></figure><p>10.3 插入/显示/更新/删除document</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 插入</span></span><br><span class="line">db.COLLECTION_NAME.insert(document)</span><br><span class="line">db.COLLECTION_NAME.save(document)</span><br><span class="line">db.COLLECTION_NAME.insertOne()</span><br><span class="line">db.COLLECTION_NAME.insertMany()</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示已有doc</span></span><br><span class="line">db.COLLECTION_NAME.find()</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 更新doc的部分内容</span></span><br><span class="line">db.COLLECTION_NAME.update(</span><br><span class="line">   &lt;query&gt;,   # 查询条件</span><br><span class="line">   &lt;update&gt;,  # 更新操作</span><br><span class="line">   &#123;</span><br><span class="line">     upsert: &lt;boolean&gt;,     # if true 如果不存在则插入</span><br><span class="line">     multi: &lt;boolean&gt;,      # find fist/all match</span><br><span class="line">     writeConcern: &lt;document&gt;</span><br><span class="line">   &#125;</span><br><span class="line">)</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.insert(&#123;<span class="string">"case"</span>: <span class="string">"s0"</span>, <span class="string">"name"</span>: <span class="string">"img0"</span>&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.insert(&#123;<span class="string">"case"</span>: <span class="string">"s1"</span>, <span class="string">"name"</span>: <span class="string">"img1"</span>&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find()</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.update(&#123;<span class="string">'case'</span>: <span class="string">'s1'</span>&#125;, &#123;<span class="variable">$set</span>: &#123;<span class="string">'case'</span>: <span class="string">'s2'</span>, <span class="string">'name'</span>: <span class="string">'img2'</span>&#125;&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find()</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 给doc的某个key重命名</span></span><br><span class="line">db.COLLECTION_NAME.updateMany(</span><br><span class="line">&#123;&#125;,</span><br><span class="line">&#123;'$rename': &#123;"old_key": "new_key"&#125;&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 更新整条文档by object_id</span></span><br><span class="line">db.COLLECTION_NAME.save(</span><br><span class="line">   &lt;document&gt;,</span><br><span class="line">   &#123;</span><br><span class="line">     writeConcern: &lt;document&gt;</span><br><span class="line">   &#125;</span><br><span class="line">)</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.save(&#123;<span class="string">"_id"</span>: ObjectId(<span class="string">"60474e4b77e21bad9bd4655a"</span>), <span class="string">"case"</span>:<span class="string">"s3"</span>, <span class="string">"name"</span>:<span class="string">"img3"</span>&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除满足条件的doc</span></span><br><span class="line">db.COLLECTION_NAME.remove(</span><br><span class="line">   &lt;query&gt;,</span><br><span class="line">   &#123;</span><br><span class="line">     justOne: &lt;boolean&gt;,   # find fist/all match</span><br><span class="line">     writeConcern: &lt;document&gt;</span><br><span class="line">   &#125;</span><br><span class="line">)</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.remove(&#123;<span class="string">"case"</span>: <span class="string">"s0"</span>&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除所有doc</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.remove(&#123;&#125;)</span></span><br></pre></td></tr></table></figure><p>10.4 简单查询find</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.insert(&#123;<span class="string">"case"</span>: <span class="string">"s0"</span>, <span class="string">"name"</span>: <span class="string">"img0"</span>&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.insert(&#123;<span class="string">"case"</span>: <span class="string">"s1"</span>, <span class="string">"name"</span>: <span class="string">"img1"</span>&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.insert(&#123;<span class="string">"case"</span>: <span class="string">"s2"</span>, <span class="string">"name"</span>: <span class="string">"img2"</span>&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.insert(&#123;<span class="string">"case"</span>: <span class="string">"s2"</span>, <span class="string">"name"</span>: <span class="string">"img3"</span>&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查询表中的doc：db.COLLECTION_NAME.find(&#123;query&#125;, &#123;KEY:1&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find(&#123;<span class="string">'case'</span>: s2&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find(&#123;<span class="string">'case'</span>: <span class="string">'s1'</span>&#125;, &#123;<span class="string">"name"</span>:1&#125;)   <span class="comment"># projection的value在对应的key-value是list的时候有意义</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 格式化显示查询结果：db.COLLECTION_NAME.find(&#123;query&#125;, &#123;KEY:1&#125;).pretty()</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find(&#123;<span class="string">'case'</span>: s2&#125;).pretty()</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 读取指定数量的数据记录：db.COLLECTION_NAME.find(&#123;query&#125;, &#123;KEY:1&#125;).<span class="built_in">limit</span>(NUMBER)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find(&#123;<span class="string">'case'</span>: &#123;<span class="variable">$type</span>: <span class="string">'string'</span>&#125;&#125;).<span class="built_in">limit</span>(1)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 跳过指定数量的数据：db.COLLECTION_NAME.find(&#123;query&#125;, &#123;KEY:1&#125;).skip(NUMBER)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find(&#123;<span class="string">'case'</span>: &#123;<span class="variable">$type</span>: <span class="string">'string'</span>&#125;&#125;).skip(1)</span></span><br></pre></td></tr></table></figure><p>10.5 条件操作符</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">(&gt;</span><span class="bash">) 大于 - <span class="variable">$gt</span></span></span><br><span class="line">(&lt;) 小于 - $lt</span><br><span class="line"><span class="meta">(&gt;</span><span class="bash">=) 大于等于 - <span class="variable">$gte</span></span></span><br><span class="line">(&lt;= ) 小于等于 - $lte</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.update(&#123;<span class="string">'case'</span>:<span class="string">'s1'</span>&#125;, &#123;<span class="variable">$set</span>: &#123;<span class="string">"name"</span>:<span class="string">'img1'</span>, <span class="string">'size'</span>:100&#125;&#125;)</span></span><br><span class="line">WriteResult(&#123; "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 &#125;)</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.update(&#123;<span class="string">'case'</span>:<span class="string">'s2'</span>&#125;, &#123;<span class="variable">$set</span>: &#123;<span class="string">"name"</span>:<span class="string">'img2'</span>, <span class="string">'size'</span>:200&#125;&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查询size&gt;150的doc</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find(&#123;<span class="string">'size'</span>: &#123;<span class="variable">$gt</span>: 150&#125;&#125;)</span></span><br></pre></td></tr></table></figure><p>10.6 数据类型操作符</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">type(KEY)等于 - $type</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 比较对象可以是字符串/对应的reflect NUM</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find(&#123;<span class="string">'case'</span>: &#123;<span class="variable">$type</span>: <span class="string">'string'</span>&#125;&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find(&#123;<span class="string">'case'</span>: &#123;<span class="variable">$type</span>: <span class="string">'0'</span>&#125;&#125;)</span></span><br></pre></td></tr></table></figure><p>10.7 排序find().sort</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 通过指定字段&amp;指定升序/降序来对数据排序：db.COLLECTION_NAME.find().sort(&#123;KEY:1/-1&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find().sort(&#123;<span class="string">'name'</span>:1&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> skip(), limilt(), sort()三个放在一起执行的时候，执行的顺序是先 sort(), 然后是 skip()，最后是显示的 <span class="built_in">limit</span>()。</span></span><br></pre></td></tr></table></figure><p>10.8 索引</p><p>skip</p><p>10.9 聚合aggregate</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 用于统计 db.COLLECTION_NAME.aggregate(AGGREGATE_OPERATION)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> by group</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.aggregate([&#123;<span class="variable">$group</span>: &#123;_id: <span class="string">'$case'</span>, img_num:&#123;<span class="variable">$sum</span>:1&#125;&#125;&#125;])</span></span><br><span class="line">group by key value 'case'</span><br><span class="line">count number of items in each group</span><br><span class="line">refer to the number as img_num</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.aggregate([&#123;<span class="variable">$group</span>: &#123;_id: <span class="string">'$case'</span>, img_num:&#123;<span class="variable">$sum</span>:<span class="string">'$size'</span>&#125;&#125;&#125;])</span></span><br><span class="line">计算每一个group内，size值的总和</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> by match</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.aggregate([&#123;<span class="variable">$match</span>: &#123;<span class="string">'size'</span>: &#123;<span class="variable">$gt</span>:150&#125;&#125;&#125;, </span></span><br><span class="line"> &#123;$group:&#123;_id: null, totalsize: &#123;$sum: '$size'&#125;&#125;&#125;])</span><br><span class="line">类似shell的管道，match用来筛选条件，符合条件的送入下一步统计</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.aggregate([&#123;<span class="variable">$skip</span>: 4&#125;, </span></span><br><span class="line"> &#123;$group:&#123;_id: null, totalsize: &#123;$sum: '$size'&#125;&#125;&#125;])</span><br></pre></td></tr></table></figure></li><li><p>快速统计distinct</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db.case2img.distinct(TAG_NAME)</span><br><span class="line"><span class="meta">#</span><span class="bash"> 注意如果distinct的内容太长，超过16M，会报distinct too big的error，推荐用聚合来做统计</span></span><br></pre></td></tr></table></figure></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">12. pymongo</span><br><span class="line"></span><br><span class="line">   用python代码来操作数据库</span><br><span class="line"></span><br><span class="line">   先安装：pip install pymongo</span><br><span class="line"></span><br><span class="line">   11.1 连接client</span><br><span class="line"></span><br><span class="line">   ```python</span><br><span class="line">   from pymongo import MongoClient</span><br><span class="line">   Client = MongoClient()</span><br></pre></td></tr></table></figure><p>   11.2 获取数据库</p>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db = Client.DB_NAME</span><br><span class="line">db = Client[<span class="string">'DB_NAME'</span>]</span><br></pre></td></tr></table></figure><p>   11.3 获取collection</p>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">collection = db.COLLECTION_NAME</span><br><span class="line">collection = db[<span class="string">'COLLECTION_NAME'</span>]</span><br></pre></td></tr></table></figure><p>   11.4 插入doc</p>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># insert one</span></span><br><span class="line">document1 = &#123;<span class="string">'x'</span>:<span class="number">1</span>&#125;</span><br><span class="line">document2 = &#123;<span class="string">'x'</span>:<span class="number">2</span>&#125;</span><br><span class="line">post_1 = collection.insert_one(document1).inserted_id</span><br><span class="line">post_2 = collection.insert_one(document2).inserted_id</span><br><span class="line">print(post_1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># insert many</span></span><br><span class="line">new_document = [&#123;<span class="string">'x'</span>:<span class="number">1</span>&#125;,&#123;<span class="string">'x'</span>:<span class="number">2</span>&#125;]</span><br><span class="line"><span class="comment"># new_document = [document1,document2]  注意doc是神拷贝，只能作为一条doc被插入一次</span></span><br><span class="line">result = collection.insert_many(new_document).inserted_ids</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><p>   11.5 查找</p>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bson.objectid <span class="keyword">import</span> ObjectId</span><br><span class="line"></span><br><span class="line"><span class="comment"># find one 返回一条doc</span></span><br><span class="line">result = collection.find_one()</span><br><span class="line">result = collection.find_one(&#123;<span class="string">'case'</span>: <span class="string">'s0'</span>&#125;)</span><br><span class="line">result = collection.find_one(&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'604752f277e21bad9bd46560'</span>)&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># find 返回一个迭代器</span></span><br><span class="line"><span class="keyword">for</span> _, item <span class="keyword">in</span> enumerate(collection.find()):</span><br><span class="line">    print(item)</span><br></pre></td></tr></table></figure><p>   11.6 更新</p>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># update one</span></span><br><span class="line">collection.update_one(&#123;<span class="string">'case'</span>:<span class="string">'s1'</span>&#125;,&#123;<span class="string">'$set'</span>:&#123;<span class="string">'size'</span>:<span class="number">300</span>&#125;&#125;)</span><br><span class="line">collection.update_one(&#123;<span class="string">'case'</span>:<span class="string">'s1'</span>&#125;,&#123;<span class="string">'$push'</span>:&#123;<span class="string">'add'</span>:<span class="number">1</span>&#125;&#125;)    <span class="comment"># 追加数组内容</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># update many</span></span><br><span class="line">collection.update_many(&#123;<span class="string">'case'</span>:<span class="string">'s1'</span>&#125;,&#123;<span class="string">'$set'</span>:&#123;<span class="string">'size'</span>:<span class="number">300</span>&#125;&#125;)</span><br></pre></td></tr></table></figure><p>   11.7 删除</p>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在mongo shell里面是remove方法，在pymongo里面被deprecated成delete方法</span></span><br><span class="line">collection.delete_one(&#123;<span class="string">"case"</span>: <span class="string">"s2"</span>&#125;)</span><br><span class="line">collection.delete_many(&#123;<span class="string">"case"</span>: <span class="string">"s1"</span>&#125;)</span><br></pre></td></tr></table></figure><p>   11.8 统计</p>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计数：count方法已经被重构</span></span><br><span class="line">print(collection.count_documents(&#123;<span class="string">'case'</span>:<span class="string">'s0'</span>&#125;))</span><br><span class="line"></span><br><span class="line"><span class="comment"># unique：distinct方法</span></span><br><span class="line">print(collection.distinct(<span class="string">'case'</span>))</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> 数据库，NoSQL </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>docker</title>
      <link href="/2021/03/04/docker/"/>
      <url>/2021/03/04/docker/</url>
      <content type="html"><![CDATA[<ol><li><p>shartup</p><ul><li><p>部署方案</p><ul><li><p>古早年代</p><p>  <img src="/2021/03/04/docker/old.png" width="50%;"></p></li><li><p>虚拟机</p><p>  <img src="/2021/03/04/docker/vm.png" width="50%;"></p></li><li><p>docker</p><p>  <img src="/2021/03/04/docker/docker.png" width="50%;"></p></li></ul></li><li><p>image镜像 &amp; container容器 &amp; registry仓库</p><ul><li>镜像：相当于是一个 root 文件系统，提供容器运行时所需的程序、库、资源、配置等</li><li>容器：镜像运行时的实体，可以被创建、启动、停止、删除、暂停等</li><li>仓库：用来保存镜像<ul><li>官方仓库：docker hub：<a href="https://hub.docker.com/r/floydhub/tensorflow/tags?page=1&amp;ordering=last_updated" target="_blank" rel="noopener">https://hub.docker.com/r/floydhub/tensorflow/tags?page=1&amp;ordering=last_updated</a></li></ul></li></ul></li></ul></li><li><p>常用命令</p><ul><li>拉镜像<ul><li>docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签]</li><li>地址可以是官方地址，也可以是第三方（如Harbor）</li><li>仓库名由作者名和软件名组成（如zhangruiming/skin）</li><li>标签用来指定某个版本的image，省略则默认latest</li></ul></li><li>列出所有镜像<ul><li>docker  images</li></ul></li><li>删除镜像<ul><li>docker rmi [-f] [镜像id]</li><li>删除镜像之前要kill/rm所有使用该镜像的container：docker rm [容器id]</li></ul></li><li>运行镜像并创建一个容器<ul><li>docker run [-it] [仓库名] [命令]</li><li>选项 -it：为容器配置一个交互终端</li><li>选项 -d：后台运行容器，并返回容器ID（不直接进入终端）</li><li>选项 —name=’xxx’：为容器指定一个名称</li><li>选项-v /host_dir:/container_dir：将主机上指定目录映射到容器的指定目录</li><li>[命令]参数必须要加，而且要是那种一直挂起的命令（/bin/bash），如果是ls/cd/直接不填，那么命令运行完容器就会停止运行，docker ps -a查看状态，发现都是Exited</li></ul></li><li>创建容器<ul><li>docker run</li></ul></li><li>查看所有容器<ul><li>docker ps</li></ul></li><li>启动一个已经停止的容器/停止正在运行的容器<ul><li>docker start [容器id]</li><li>docker stop [容器id]</li></ul></li><li>进入容器<ul><li>docker exec -it [容器id] [linux命令]</li></ul></li><li>删除容器<ul><li>docker rm [容器id]</li></ul></li><li>删除所有不活跃的容器<ul><li>docker container prune</li></ul></li></ul></li><li><p>案例</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 拉镜像</span></span><br><span class="line">docker pull 地址/仓库:标签</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示镜像</span></span><br><span class="line">docker images</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 运行指定镜像</span></span><br><span class="line">docker run -itd --name='test' 地址/仓库:标签</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看运行的容器</span></span><br><span class="line">docker ps</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入容器</span></span><br><span class="line">docker exec -it 容器id或name /bin/bash</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 一顿操作完退出容器</span></span><br><span class="line">exit</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将修改后的容器保存为镜像</span></span><br><span class="line">docker commit 容器id或name 新镜像名字</span><br><span class="line">docker images可以看到这个镜像了</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 保存镜像到本地</span></span><br><span class="line">docker save -o tf_torch.rar  tf_torch</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 还原镜像</span></span><br><span class="line">docker load --input tf_torch.tar</span><br></pre></td></tr></table></figure><ol><li>dockerfile</li></ol></li></ol>]]></content>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>layer norm</title>
      <link href="/2021/03/02/layer-norm/"/>
      <url>/2021/03/02/layer-norm/</url>
      <content type="html"><![CDATA[<h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><ol><li>papers</li></ol><p>[batch norm 2015] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift，inceptionV2，Google Team，归一化层的始祖，加速训练&amp;正则，BN被后辈追着打的主要痛点：approximation by mini-batch，test phase frozen</p><p>[layer norm 2016] Layer Normalization，Toronto+Google，针对BN不适用small batch和RNN的问题，主要用于RNN，在CNN上不好，在test的时候也是active的，因为mean&amp;variance由于当前数据决定，有负责rescale和reshift的layer params</p><p>[weight norm 2016] Weight normalization: A simple reparameterization to accelerate training of deep neural networks，OpenAI，</p><p>[cosine norm 2017] Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks，中科院，</p><p>[instance norm 2017] Instance Normalization: The Missing Ingredient for Fast Stylization，高校report，针对风格迁移，IN在test的时候也是active的，而不是freeze的，单纯的instance-independent norm，没有layer params</p><p>[group norm 2018] Group Normalization，FAIR Kaiming，针对BN在small batch上性能下降的问题，提出batch-independent的</p><p>[weight standardization 2019] Weight Standardization，Johns Hopkins，</p><p>[batch-channel normalization &amp; weight standardization 2020] BCN&amp;WS: Micro-Batch Training with Batch-Channel Normalization and Weight Standardization，Johns Hopkins，</p><ol><li><p>why Normalization</p><ul><li><p>独立同分布：independent and identically distribute</p></li><li><p>白化：whitening（[PCA whitening][<a href="http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/]）" target="_blank" rel="noopener">http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/]）</a></p><ul><li>去除特征之间的相关性</li><li>使所有特征具有相同的均值和方差</li></ul></li><li><p>样本分布变化：Internal Covariate Shift</p><ul><li>对于神经网络的各层输入，由于stacking internel byproduct，每层的分布显然各不相同，但是对于某个特定的样本输入，他们所指示的label是不变的</li><li><p>即源空间和目标空间的条件概率是一致的，但是边缘概率是不同的</p><script type="math/tex; mode=display">P_s(Y|X=x) = P_t(Y|X=x) \\P_s(X) \neq P_t(X)</script></li><li><p>每个神经元的数据不再是独立同分布，网络需要不断适应新的分布，上层神经元容易饱和：网络训练又慢又不稳定</p></li></ul></li></ul></li><li><p>how to Normalization</p><ul><li><p>preparation</p><ul><li>unit：一个神经元（一个op），输入[b,N,C_in]，输出[b,N,1]</li><li>layer：一层的神经元（一系列op，$W\in R^{M*N}$），在channel-dim上concat当前层所有unit的输出[b,N,C_out]</li><li>dims<ul><li>b：batch dimension</li><li>N：spatial dimension，1/2/3-dims</li><li>C：channel dimension</li></ul></li><li>unified representation：本质上都是对数据在规范化<ul><li>$h = f(g*\frac{x-\mu}{\sigma}+b)$：先归一化，在rescale &amp; reshift</li><li>$\mu$ &amp; $\sigma$：compute from上一层的特征值</li><li>$g$ &amp; $b$：learnable params基于当前层</li><li>$f$：neurons’ weighting operation</li><li>各方法的主要区别在于mean &amp; variance的计算维度</li></ul></li></ul></li><li><p>对数据</p><ul><li>BN：以一层每个神经元的输出为单位，即每个channel的mean&amp;var相互独立</li><li>LN：以一层所有神经元的输出为单位，即每个sample的mean&amp;var相互独立</li><li>IN：以每个sample在每个神经元的输出为单位，每个sample在每个channel的mean&amp;var都相互独立</li><li>GN：以每个sample在一组神经元的输出为单位，一组包含一个神经元的时候变成IN，一组包含一层所有神经元的时候就是LN</li><li><p>示意图：</p><p><img src="/2021/03/02/layer-norm/methods.png" width="70%;"></p></li></ul></li><li><p>对权重</p><ul><li>WN：将权重分解为单位向量和一个固定标量，相当于神经元的任意输入vec点乘了一个单位vec（downscale），再rescale，进一步地相当于没有做shift和reshift的数据normalization</li><li>WS：对权重做全套（归一化再recale），比WN多了shift，“zero-center is the key” </li></ul></li><li><p>对op</p><ul><li><p>CosN：</p><ul><li><p>将线性变换op替换成cos op：$f_w(x) = cos <w,x> = \frac{w \cdot x}{|w||x|}$</w,x></p></li><li><p>数学本质上又退化成了只有downscale的变换，表征能力不足</p></li></ul></li></ul></li></ul></li></ol><h2 id="Whitening白化"><a href="#Whitening白化" class="headerlink" title="Whitening白化"></a>Whitening白化</h2><ol><li><p>purpose</p><ul><li>images的adjacent pixel values are highly correlated，thus redundant</li><li>linearly move the origin distribution，making the inputs share the same mean &amp; variance</li></ul></li><li><p>method</p><ul><li><p>首先进行PCA预处理，去掉correlation</p><ul><li><p>mean on sample（注意不是mean on image）</p><script type="math/tex; mode=display">  \overline x = \frac{1}{N}\sum_{i=1}^N x_i\\  x^{'} = x - \overline x</script></li><li><p>协方差矩阵</p><script type="math/tex; mode=display">  X \in R^{d*N}\\  S = \frac{1}{N}XX^T</script></li><li><p>奇异值分解svd(S)</p><script type="math/tex; mode=display">  S = U \Sigma V^T</script><ul><li>$\Sigma$为对角矩阵，对角上的元素为奇异值</li><li>$U=[u_1,u_2,…u_N]$中是奇异值对应的正交向量</li></ul></li><li><p>投影变换</p><script type="math/tex; mode=display">  X^{'} = U_p^T X</script><ul><li>取投影矩阵$U_p$ from $U$，$U_p \in R^{N*d}$表示将数据空间从N维投影到$U_p$所在的d维空间上</li></ul></li><li><p>recover（投影逆变换）</p><script type="math/tex; mode=display">  X^{''} = U_p^T X^{'}</script></li></ul></li></ul></li></ol><pre><code>        * 取投影矩阵$U_r=U_p^T$，就是将 数据空间从d维空间再投影回N维空间上* PCA白化：    * 对PCA投影后的新坐标，做归一化处理：基于特征值进行缩放        $$        X_{PCAwhite} = \Sigma^{-\frac{1}{2}}X^{&#39;} =  \Sigma^{-\frac{1}{2}}U^TX        $$    * $X_{PCAwhite}$的协方差矩阵$S_{PCAwhite} = I$，因此是去了correlation的* ZCA白化：在上一步做完之后，再把它变换到原始空间，所以ZCA白化后的特征图更接近原始数据    * 对PCA白化后的数据，再做一步recover        $$        X_{ZCAwhite} = U X_{PCAwhite}        $$    * 协方差矩阵仍旧是I，合法白化</code></pre><h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><ol><li><p>动机</p><ul><li><p>BN reduces training time</p><ul><li>compute by each neuron</li><li>require moving average</li><li>depend on mini-batch size</li><li>how to apply to recurrent neural nets</li></ul></li><li><p>propose layer norm</p><ul><li>[unlike BN] compute by each layer</li><li>[like BN] with adaptive bias &amp; gain</li><li>[unlike BN] perform the same computation at training &amp; test time</li><li>[unlike BN] straightforward to apply to recurrent nets</li><li>work well for RNNs</li></ul></li></ul></li><li><p>论点</p><ul><li>BN<ul><li>reduce training time &amp; serves as regularizer</li><li>require moving average：introduce dependencies between training cases </li><li>the approxmation of mean &amp; variance expectations constraints on the size of a mini-batch</li></ul></li><li>intuition<ul><li>norm layer提升训练速度的核心是限制神经元输入输出的变化幅度，稳定梯度</li><li>只要控制数据分布，就能保持训练速度</li></ul></li></ul></li><li><p>方法</p><ul><li>compute over all hidden units in the same layer</li><li>different training cases have different normalization terms</li><li>没啥好说的，就是在channel维度计算norm</li><li>further的GN把channel维度分组做norm，IN在直接每个特征计算norm</li><li>gain &amp; bias<ul><li>也是在对应维度：(hwd)c-dim</li><li><a href="https://tobiaslee.top/2019/11/21/understanding-layernorm/" target="_blank" rel="noopener">https://tobiaslee.top/2019/11/21/understanding-layernorm/</a></li><li>后续有实验发现，去掉两个learnable rescale params反而提点</li><li>考虑是在training set上的过拟合</li></ul></li></ul></li><li><p>实验</p><ul><li>RNN上有用</li><li>CNN上比没有norm layer好，但是没有BN好：因为channel是特征维度，特征维度之间有明显的有用/没用，不能简单的norm</li></ul></li></ol><h2 id="Weight-Normalization-A-Simple-Reparameterization-to-Accelerate-Training-of-Deep-Neural-Networks"><a href="#Weight-Normalization-A-Simple-Reparameterization-to-Accelerate-Training-of-Deep-Neural-Networks" class="headerlink" title="Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks"></a>Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks</h2><ol><li>动机<ul><li>reparameterizing the weights<ul><li>decouple length &amp; direction</li><li>no dependency between samples which suits well for<ul><li>recurrent</li><li>reinforcement</li><li>generative</li></ul></li></ul></li><li>no additional memory and computation</li><li>testified on<ul><li>MLP with CIFAR</li><li>generative model VAE &amp; DRAW</li><li>reinforcement DQN</li></ul></li></ul></li><li>论点<ul><li>a neuron：<ul><li>get inputs from former layers(neurons)</li><li>weighted sum over the inputs</li><li>add a bias</li><li>elementwise nonlinear transformation</li><li>batch outputs：one value per sample</li></ul></li><li>intuition of normalization：<ul><li>give gradients that are more like whitened natural gradients</li><li>BN：make the outputs of each neuron服从std norm</li><li>our WN：<ul><li>inspired by BN</li><li>does not share BN’s across-sample property</li><li>no addition memory and tiny addition computation</li></ul></li><li></li></ul></li></ul></li></ol><h2 id="Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization"><a href="#Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization" class="headerlink" title="Instance Normalization: The Missing Ingredient for Fast Stylization"></a>Instance Normalization: The Missing Ingredient for Fast Stylization</h2><ol><li><p>动机</p><ul><li>stylization：针对风格迁移网络</li><li>with a small change：swapping BN with IN</li><li>achieve qualitative improvement</li></ul></li><li><p>论点</p><ul><li>stylized image <ul><li>a content image + a style image</li><li>both style and content statistics are obtained from a pretrained CNN for image classification</li><li>methods<ul><li>optimization-based：iterative thus computationally inefficient</li><li>generator-based：single pass but never as good as </li></ul></li></ul></li><li>our work<ul><li>revisit the feed-forward  method</li><li>replace BN in the generator with IN</li><li>keep them at test time as opposed to freeze</li></ul></li></ul></li><li><p>方法</p><ul><li><p>formulation</p><ul><li>given a fixed stype image $x_0$</li><li>given a set of content images $x_t, t= 1,2,…,n$</li><li>given a pre-trained CNN</li><li>with a variable z controlling the generation of stylization results</li><li>compute the stylied image g($x_t$, z)</li><li>compare the statistics：$min_g \frac{1}{n} \sum^n_{t=1} L(x_0, x_t, g(x_t, z))$</li><li>comparing target：the contrast of the stylized image is similar to the constrast of the style image</li></ul></li><li><p>observations</p><ul><li>the more training examples, the poorer the qualitive results</li><li>the result of stylization still depent on the constrast of the content image</li></ul></li><li><p>intuition </p><ul><li>风格迁移本质上就是将style image的contrast用在content image的：也就是rescale content image的contrast</li><li><p>constrast是per sample的：$\frac{pixel}{\sum pixels\ on\ the\ map}$</p></li><li><p>BN在norm的时候将batch samples搅合在了一起</p></li></ul></li><li><p>IN</p><ul><li>instance-specfic normalization</li><li><p>also known as contrast normalization</p></li><li><p>就是per image做标准化，没有trainable/frozen params，在test phase也一样用</p><p>  <img src="/2021/03/02/layer-norm/IN.png" width="80%;"></p></li></ul></li></ul></li></ol><h2 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a>Group Normalization</h2><ol><li><p>动机</p><ul><li>for small batch size</li><li>do normalization in channel groups</li><li>batch-independent</li><li>behaves stably over different batch sizes</li><li><p>approach BN’s accuracy </p><p><img src="/2021/03/02/layer-norm/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/group-normalization/error.png" width="40%;"></p></li></ul></li><li><p>论点</p><ul><li>BN<ul><li>requires sufficiently large batch size (e.g. 32)</li><li>Mask R-CNN frameworks use a batch size of 1 or 2 images because of higher resolution, where BN is “frozen” by transforming to a linear layer </li><li>synchronized BN 、BR</li></ul></li><li>LN &amp; IN<ul><li>effective for training sequential models or generative models </li><li>but have limited success in visual recognition </li><li>GN能转换成LN／IN</li></ul></li><li>WN<ul><li>normalize the filter weights, instead of operating on features</li></ul></li></ul></li><li><p>方法</p><ul><li><p>group</p><ul><li>it is not necessary to think of deep neural network features as unstructured vectors <ul><li>第一层卷积核通常存在一组对称的filter，这样就能捕获到相似特征</li><li>这些特征对应的channel can be normalized together</li></ul></li></ul></li><li><p>normalization</p><ul><li><p>transform the feature x：$\hat x_i = \frac{1}{\sigma}(x_i-\mu_i)$</p></li><li><p>the mean and the standard deviation：</p><script type="math/tex; mode=display">  \mu_i=\frac{1}{m}\sum_{k\in S_i}x_k\\  \sigma_i=\sqrt {\frac{1}{m}\sum_{k\in S_i}(x_k-\mu_i)^2+\epsilon}</script></li><li><p>the set $S_i$</p><ul><li>BN：<ul><li>$S_i=\{k|k_C = i_C\}$</li><li>pixels sharing the same channel index are normalized together </li><li>for each channel, BN computes μ and σ along the (N, H, W) axes </li></ul></li><li>LN<ul><li>$S_i=\{k|k_N = i_N\}$</li><li>pixels sharing the same batch index (per sample) are normalized together </li><li>LN computes μ and σ along the (C,H,W) axes for each sample </li></ul></li><li>IN<ul><li>$S_i=\{k|k_N = i_N, k_C=i_C\}$</li><li>pixels sharing the same batch index and the same channel index are normalized together </li><li>LN computes μ and σ along the (H,W) axes for each sample </li></ul></li><li>GN<ul><li>$S_i=\{k|k_N = i_N, [\frac{k_C}{C/G}]=[\frac{i_C}{C/G}]\}$</li><li>computes μ and σ along the (H, W ) axes and along a group of C/G channels </li></ul></li></ul></li><li><p>linear transform  </p><ul><li>to keep representational ability </li><li><strong>per channel</strong></li><li>scale and shift：$y_i = \gamma \hat x_i + \beta$</li></ul><p><img src="/2021/03/02/layer-norm/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/group-normalization/gn.png" width="80%;"></p></li></ul></li><li><p>relation</p><ul><li>to LN<ul><li>LN assumes <em>all</em> channels in a layer make “similar contributions” </li><li>which is less valid with the presence of convolutions </li><li>GN improved representational power over LN</li></ul></li><li>to IN<ul><li>IN can only rely on the spatial dimension for computing the mean and variance </li><li>it misses the opportunity of exploiting the channel dependence</li><li>【QUESTION】BN也没考虑通道间的联系啊，但是计算mean和variance时跨了sample</li></ul></li></ul></li><li><p>implementation</p><ul><li>reshape</li><li>learnable $\gamma \&amp; \beta$</li><li>computable mean &amp; var</li></ul><p><img src="/2021/03/02/layer-norm/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/group-normalization/code.png" width="50%;"></p></li></ul></li><li><p>实验</p><ul><li>GN相比于BN，training error更低，但是val error略高于BN<ul><li>GN is effective for easing optimization</li><li>loses some regularization ability </li><li>it is possible that GN combined with a suitable regularizer will improve results </li></ul></li><li>选取不同的group数，所有的group&gt;1均好于group=1（LN）</li><li>选取不同的channel数（C／G），所有的channel&gt;1均好于channel=1（IN）</li><li>Object Detection  <ul><li>frozen：因为higher resolution，batch size通常设置为2/GPU，这时的BN frozen成一个线性层$y=\gamma(x-\mu)/\sigma+beta$，其中的$\mu$和$sigma$是load了pre-trained model中保存的值，并且frozen掉，不再更新</li><li>denote as BN*</li><li>replace BN* with GN during fine-tuning </li><li>use a weight decay of 0 for the γ and β parameters </li></ul></li></ul></li></ol><h2 id="WS-Weight-Standardization"><a href="#WS-Weight-Standardization" class="headerlink" title="WS: Weight Standardization"></a>WS: Weight Standardization</h2><ol><li><p>动机</p><ul><li>accelerate training</li><li>micro-batch：<ul><li>以BN with large-batch为基准</li><li>目前BN with micro-batch及其他normalization methods都不能match这个baseline</li></ul></li><li>operates on weights instead of activations</li><li>效果<ul><li>match or outperform BN</li><li>smooth the loss</li></ul></li></ul></li><li><p>论点</p><ul><li><p>two facts</p><ul><li>BN的performance gain与reduction of internal covariate shift没什么关系</li><li>BN使得optimization landscape significantly smoother</li><li>因此our target is to find another technique<ul><li>achieves smooth landscape</li><li>work with micro-batch</li></ul></li></ul></li><li><p>normalization methods </p><ul><li>focus on activations<ul><li>不展开</li></ul></li><li><p>focus on weights</p><ul><li>WN：just length-direction decoupling</li></ul><p><img src="/2021/03/02/layer-norm/methods2.png" width="40%;"></p></li></ul></li></ul></li><li><p>方法</p><ul><li><p>Lipschitz constants </p><ul><li>BN reduces the Lipschitz constants of the loss function</li><li>makes the gradient more Lipschitz</li><li>BN considers the Lipschitz constants with respect to activations，not the weights that the optimizer is directly optimizing</li></ul></li><li><p>our inspiration</p><ul><li>standardize the weights也同样能够smooth the landscape</li><li>更直接</li><li>smoothing effects on activations and weights是可以累积的，因为是线性运算</li></ul></li><li><p>Weight Standardization </p><ul><li>reparameterize the original weights $W$<ul><li>对卷积层的权重参数做变换，no bias</li><li>$W \in R^{O * I}$</li><li>$O=C_{out}$</li><li>$I=C_{in}*kernel_size$</li></ul></li><li>optimize the loss on $\hat W$</li><li>compute mean &amp; var on I-dim</li><li><p>只做标准化，无需affine，因为默认后续还要接一个normalization layer对神经元进行refine</p><p><img src="/2021/03/02/layer-norm/WS.png" width="40%;"></p></li></ul></li><li><p>WS normalizes gradients </p><ul><li><p>拆解：</p><ul><li>eq5：$W$ to $\dot W$，减均值，zero-centered</li><li>eq6：$\dot W$ to $\hat W$，除方差，one-varianced</li><li>eq8：$\delta \hat W$由前一步的梯度normalize得到</li><li>eq9：$\delta \dot W$也由前一步的梯度normalize</li><li><p>最终用于梯度更新的梯度是zero-centered</p><p><img src="/2021/03/02/layer-norm/gradients.png" width="40%;"></p></li></ul></li></ul></li><li><p>WS smooths landscape </p><ul><li>判定是否smooth就看Lipschitz constant的大小</li><li>eq5和eq6都能reduce the Lipschitz constant</li><li>其中eq5 makes the major improvements </li><li>eq6 slightly improves，因为计算量不大，所以保留</li></ul></li></ul></li><li><p>实验</p><ul><li><p>ImageNet</p><ul><li>BN的batchsize是64，其余都是1，其余的梯度更新iterations改成64——使得参数更新次数同步</li><li>所有的normalization methods加上WS都有提升</li><li>裸的normalization methods里面batchsize1的GN最好，所以选用GN+WS做进一步实验</li><li><p>GN+WS+AF：加上conv weight的affine会harm</p><p><img src="/2021/03/02/layer-norm/imageNet.png" width="40%;"></p></li></ul></li></ul></li><li><p>code</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># official release</span></span><br><span class="line"><span class="comment"># 放在WSConv2D子类的call里面</span></span><br><span class="line">kernel_mean = tf.math.reduce_mean(kernel, axis=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], keepdims=<span class="keyword">True</span>, name=<span class="string">'kernel_mean'</span>)</span><br><span class="line">kernel = kernel - kernel_mean</span><br><span class="line">kernel_std = tf.keras.backend.std(kernel, axis=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], keepdims=<span class="keyword">True</span>)</span><br><span class="line">kernel = kernel / (kernel_std + <span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> CNN, layer, normalization </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>NFNet</title>
      <link href="/2021/02/22/NFNet/"/>
      <url>/2021/02/22/NFNet/</url>
      <content type="html"><![CDATA[<h2 id="NFNet-High-Performance-Large-Scale-Image-Recognition-Without-Normalization"><a href="#NFNet-High-Performance-Large-Scale-Image-Recognition-Without-Normalization" class="headerlink" title="NFNet: High-Performance Large-Scale Image Recognition Without Normalization"></a>NFNet: High-Performance Large-Scale Image Recognition Without Normalization</h2><ol><li><p>动机</p><ul><li>NF：<ul><li>normalization-free</li><li>aims to match the test acc of batch-normalized networks<ul><li>attain new SOTA 86.5%</li><li>pre-training + fine-tuning上也表现更好89.2%</li></ul></li></ul></li><li>batch normalization<ul><li>不是完美解决方案</li><li>depends on batch size</li></ul></li><li>non-normalized networks<ul><li>accuracy</li><li>instabilities：develop adaptive gradient clipping</li></ul></li></ul></li><li><p>论点</p><ul><li>vast majority models<ul><li>variants of deep residual + BN</li><li>allow deeper, stable and regularizing</li></ul></li><li>disadvantages of batch normalization<ul><li>computational expensive</li><li>introduces discrepancy between training &amp; testing models &amp; increase params</li><li>breaks the independence among samples</li></ul></li><li>methods seeks to replace BN<ul><li>alternative normalizers </li><li>study the origin benefits of BN</li><li>train deep ResNets without normalization layers  </li></ul></li><li>key theme when removing normalization<ul><li>suppress the scale of the residual branch</li><li>simplest way：apply a learnable scalar</li><li>recent work：suppress the branch at initialization &amp; apply Scaled Weight Standardization，能追上ResNet家族，但是没追上Eff家族</li></ul></li><li>our NFNets’ main contributions <ul><li>propose AGC：解决unstable问题，allow larger batch size and stronger augmentatons</li><li>NFNets家族刷新SOTA：又快又准</li><li>pretraining + finetuning的成绩也比batch normed models好</li></ul></li></ul></li><li><p>方法</p><ul><li><p>Understanding Batch Normalization </p><ul><li>four main benefits<ul><li>downscale the residual branch：从initialization就保证残差分支的scale比较小，使得网络has well-behaved gradients early in training，从而efficient optimization </li><li>eliminates mean-shift：ReLU是不对称的，stacking layers以后数据分布会累积偏移</li><li>regularizing effect：mini-batch作为subset对于全集是有偏的，这种noise可以看作是regularizer</li><li>allows efficient large-batch training：数据分布稳定所以loss变化稳定，同时大batch更接近真实分布，因此我们可以使用更大的learning rate，但是这个property仅在使用大batch size的时候有效</li></ul></li></ul></li><li><p>NF-ResNets </p><ul><li>recovering the benefits of BN：对residual branch进行scale和mean-shift</li></ul></li></ul><ul><li><p>residual block：$h_{i+1} = h_i + \alpha f_i (h_i/\beta_i)$</p></li><li><p>$\beta_i = Var(h_i)$：对输入进行标准化（方差为1），这是个expected value，不是算出来的，结构定死就定死了</p></li><li><p>Scaled Weight Standardization &amp; scaled activation</p><ul><li>比原版的WS多了一个$\sqrt N$的分母<ul><li>源码实现中比原版WS还多了learnable affine gain</li></ul></li><li>使得conv-relu以后输出还是标准分布</li></ul><p><img src="/2021/02/22/NFNet/weight.png" width="45%;"></p></li><li><p>$\alpha=0.2$：rescale</p></li><li><p>residual branch上，最终的输出为$\alpha*$标准分布，方差是$\alpha^2$</p></li><li><p>id path上，输出还是$h_{i}$，方差是$Var(h_i)$</p><ul><li><p>update这个block输出的方差为$Var(h_{i+1}) = Var(h_i)+\alpha^2$，来更新下一个block的 $\beta$</p></li><li><p>variance reset</p><ul><li>每个transition block以后，把variance重新设定为$1+\alpha^2$<ul><li>在接下来的non-transition block中，用上面的update公式更新expected std</li></ul></li></ul></li><li><p>再加上additional regularization（Dropout和Stochastic Depth两种正则手段），就满足了BN benefits的前三条</p><ul><li>在batch size较小的时候能够catch up甚至超越batch normalized models<ul><li>但是large batch size的时候perform worse</li></ul></li></ul></li><li><p>对于一个标准的conv-bn-relu，从workflow上看</p><ul><li>origin：input——一个free的conv weighting——BN（norm &amp; rescale）——activation<ul><li>NFNet：input——standard norm——normed weighting &amp; activation——rescale</li></ul></li></ul></li></ul></li></ul></li></ol><ul><li><p>Adaptive Gradient Clipping for Efficient Large-Batch Training</p><ul><li><p>梯度裁剪：</p><ul><li>clip by norm：用一个clipping threshold $\lambda$ 进行rescale，training stability was extremely sensitive to 超参的选择，settings（model depth, the batch size, or the learning rate）一变超参就要重新调</li></ul></li></ul></li><li><p>clip by value：用一个clipping value进行上下限截断</p></li><li><p>AGC</p><ul><li><p>given 某层的权重$W \in R^{N<em>M}$ 和 对应梯度$G \in R^{N</em>M}$</p></li><li><p>ratio $\frac{||G||_F}{||W||_F}$ 可以看作是梯度变化大小的measurement</p></li><li><p>所以我们直观地想到将这个ratio进行限幅：所谓的adaptive就是在梯度裁剪的时候不是对所有梯度一刀切，而是考虑其对应权重大小，从而进行更合理的调节</p><ul><li><p>但是实验中发现unit-wise的gradient norm要比layer-wise的好：每个unit就是每行，对于conv weights就是(hxwxCin)中的一个</p><p><img src="/2021/02/22/NFNet/clipping.png" width="45%;"></p></li></ul></li><li><p>scalar hyperparameter $\lambda$</p><pre><code>  * the optimal value may depend on the choice of optimizer, learning rate and batch size  * empirically we found $\lambda$ should be smaller for larger batches</code></pre><ul><li><p>ablations for AGC</p><p><img src="/2021/02/22/NFNet/agc.png" width="50%;"></p><ul><li>用pre-activation NF-ResNet-50 和 NF-ResNet-200 做实验，batch size选择从256到4096，学习率从0.1开始基于batch size线性增长，超参$\lambda$的取值见右图<ul><li>左图结论1：在batch size较小的情况下，NF-Nets能够追上甚至超越normed models的精度，但是batch size一大（2048）情况就恶化了，但是有AGC的NF-Nets则能够maintaining performance comparable or better than～～～</li><li>左图结论2：the benefits of using AGC are smaller when the batch size is small</li></ul></li></ul></li><li>右图结论1：超参$\lambda$的取值比较小的时候，我们对梯度的clipping更strong，这对于使用大batch size训练的稳定性来说非常重要</li></ul></li><li><p>whether or not AGC is beneficial for all layers</p><pre><code>      * it is always better to not clip the final linear layer   * 最开始的卷积不做梯度裁剪也能稳定训练</code></pre><ul><li>最终we apply AGC to every layer except for the final linear layer</li></ul></li><li><p>Normalizer-Free Architectures</p></li><li><p>begin with SE-ResNeXt-D model</p></li><li><p>about group width</p><pre><code>      * set group width to 128</code></pre><ul><li>the reduction in compute density means that 只减少了理论上的FLOPs，没有实际加速</li></ul></li><li><p>about stages</p><pre><code>      * R系列模型加深的时候是非线性增长，疯狂叠加stage3的block数，因为这一层resolution不大，channel也不是最多，兼顾了两侧计算量</code></pre><ul><li>我们给F0设置为[1,2,6,3]，然后在deeper variants中对每个stage的block数用一个scalar N线形增长</li></ul></li><li><p>about width</p><pre><code>      * 仍旧对stage3下手，[256,512,1536,1536]      * roughly preserves the training speed</code></pre><ul><li>一个论点：stage3 is the best place to add capacity，因为deeper enough同时have access to deeper levels同时又比最后一层有slightly higher resolution</li></ul></li><li><p>about block</p><pre><code>  * 实验发现最有用的操作是adding an additional 3 × 3 grouped conv after the first  * overview</code></pre><p>  <img src="/2021/02/22/NFNet/NFblock.png" width="50%;"></p></li><li><p>about scaling variants</p><pre><code>      * eff系列采用的是R、W、D一起增长，因为eff的block比较轻量</code></pre><ul><li><p>但是对R系列来说，只增长D和R就够了</p></li><li><p>补充细节</p><pre><code>  * 在inference阶段使用比训练阶段slightly higher resolution</code></pre><ul><li>随着模型加大increase the regularization strength：<ul><li>scale the drop rate of Dropout</li><li>调整stochastic depth rate和weight decay则not effective</li></ul></li><li><p>se-block的scale乘个2</p></li><li><p>SGD params:</p><ul><li>Nesterov=True, momentum=0.9, clipnorm=0.01</li><li>lr：<ul><li>先warmup再余弦退火：increase from 0 to 1.6 over 5 epochs, then decay to zero with cosine annealing</li><li>余弦退火cosine annealing<ul><li></li></ul></li></ul></li></ul><p><img src="/2021/02/22/NFNet/NFNets.png" width="45%;"></p></li><li><p>summary</p><ul><li>总结来说，就是拿来一个SE-ResNeXt-D</li><li>先做结构上的调整，modified width and depth patterns以及a second spatial convolution，还有drop rate，resolution</li><li>再做对梯度的调整：除了最后一个线形分类层以外，全用AGC，$\lambda=0.01$</li></ul></li><li><p>最后是训练上的trick：strong regularization and data augmentation</p><p>  <img src="/2021/02/22/NFNet/ablation.png" width="45%;"></p></li></ul></li><li><p>detailed view of NFBlocks</p><ul><li>transition block：有下采样的block<ul><li>残差branch上，bottleneck的narrow ratio是0.5</li><li>每个stage的3x3 conv的group width永远是128，而group数目是在随着block width变的</li><li>skip path接在 $\beta$ downscaling 之后</li><li>skip path上是avg pooling + 1x1 conv</li></ul></li><li><p>non-transition block：无下采样的block</p><ul><li>bottleneck-ratio仍旧是0.5</li><li>3x3conv的group width仍旧是128</li><li>skip path接在$\beta$ downscaling 之前</li><li>skip path就是id</li></ul><p><img src="/2021/02/22/NFNet/NFblocks.png" width="45%;"></p></li></ul></li></ul></li></ul></li></ul><ol><li><p>实验</p><ul><li></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> CNN, classification </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>repVGG</title>
      <link href="/2021/02/09/repVGG/"/>
      <url>/2021/02/09/repVGG/</url>
      <content type="html"><![CDATA[<h2 id="RepVGG-Making-VGG-style-ConvNets-Great-Again"><a href="#RepVGG-Making-VGG-style-ConvNets-Great-Again" class="headerlink" title="RepVGG: Making VGG-style ConvNets Great Again"></a>RepVGG: Making VGG-style ConvNets Great Again</h2><ol><li><p>动机</p><ul><li>plain ConvNets <ul><li>simply efficient but poor performance</li></ul></li><li>propose a CNN architecture RepVGG<ul><li>能够decouple为training-time和inference-time两个结构</li><li>通过structure re-paramterization technique</li><li>inference-time architecture has a VGG-like plain body</li></ul></li><li>faster<ul><li>83% faster than ResNet-50 or 101% faster than ResNet-101</li></ul></li><li>accuracy-speed trade-off<ul><li>reaches over 80% top-1 accuracy</li><li>outperforms ResNets by a large margin</li></ul></li><li>verify on classification &amp; semantic segmentation tasks</li></ul></li><li><p>论点</p><ul><li><p>well-designed CNN architectures </p><ul><li>Inception，ResNet，DenseNet，NAS models</li><li>deliver higher accuracy</li><li>drawbacks <ul><li>multi-branch designs：slow down inference and reduce memory utilization，对高并行化的设备不友好</li><li>some components：depthwise &amp; channel shuffle，increase memory access cost </li></ul></li><li>MAC(memory access cost) constitutes a large time usage in groupwise convolution：我的groupconv实现里cardinality维度上计算不并行</li><li>FLOPs并不能precisely reflect actual speed，一些结构看似比old fashioned VGG/resnet的FLOPs少，但实际并没有快</li></ul></li><li><p>multi-branch</p><ul><li>通常multi-branch model要比plain model表现好</li><li>因为makes the model an implicit ensemble of numerous shallower models</li><li>so that avoids gradient vanishing</li><li>benefits are all for training</li><li>drawbacks are undesired for inference</li></ul></li><li><p>the proposed RepVGG</p><ul><li>advantages<ul><li>plain architecture：no branches</li><li>3x3 conv &amp; ReLU组成</li><li>没有过重的人工设计痕迹</li></ul></li><li>training time use identity &amp; 1x1 conv branches</li><li><p>at inference time</p><ul><li>identity 可以看做degraded 1x1 conv</li><li>1x1 conv 可以看做degraded 3x3 conv</li><li>最终整个conv-bn branches能够整合成一个3x3 conv</li><li>inference-time model只包含conv和ReLU：没有max pooling！！</li><li>fewer memory units：分支会占内存，直到分支计算结束，plain结构的memory则是immediately released</li></ul><p><img src="/2021/02/09/repVGG/memory.png" width="40%;"></p></li></ul></li></ul></li><li><p>方法</p><p> <img src="/2021/02/09/repVGG/repvgg.png" width="45%;"></p><ul><li><p>training-time</p><ul><li>ResNet-like block<ul><li>id + 1x1 conv + 3x3 conv multi-branches</li><li>use BN in each branch</li><li>with n blocks, the model can be interpreted as an ensemble of $3^n$ models</li><li>stride2的block应该没有id path吧？？</li></ul></li><li>simply stack serveral blocks to construct the training model</li></ul></li><li><p>inference-time</p><ul><li><p>re-param</p><ul><li>inference-time BN也是一个线性计算</li><li>两个1x1 conv都可以转换成中通的3x3 kernel，有权/无权</li><li><p>要求各branch has the same strides &amp; padding pixel要对齐</p><p><img src="/2021/02/09/repVGG/reparam.png" width="40%;"></p></li></ul></li><li><p>architectural specification </p><ul><li>variety：depth and width</li><li>does not use maxpooling：只有一种operator：3x3 conv+relu</li><li>head：GAP + fc / task specific</li><li>5 stages<ul><li>第一个stage处理high resolution，stride2</li><li>第五个stage shall have more channels，所以只用一层，save parameters</li><li>给倒数第二个stage最多层，考虑params和computation的balance</li></ul></li><li>RepVGG-A：[1,2,4,14,1]，用来compete against轻量和中量级model</li><li><p>RepVGG-B：deeper in s2,3,4，[1,4,6,16,1]，用来compete against high-performance ones</p><p><img src="/2021/02/09/repVGG/architecture.png" width="45%;"></p></li><li><p>basic width：[64, 128, 256, 512]  </p><ul><li>width multiplier a &amp; b</li><li>a控制前4个stage宽度，b控制最后一个stage</li><li>[64a, 128a, 256a, 512b]  </li><li><p>第一个stage的宽度只接受变小不接受变大，因为大resolution影响计算量，min(64,64a)</p><p><img src="/2021/02/09/repVGG/variety.png" width="45%;"></p></li></ul></li><li><p>further reduce params &amp; computation</p><ul><li>groupwise 3x3 conv</li><li>跳着层换：从第三开始，第三、第五、</li><li>number of groups：1，2，4 globally </li></ul></li></ul></li></ul></li></ul></li><li><p>实验</p><ul><li><p>分支的作用</p><p>  <img src="/2021/02/09/repVGG/branch.png" width="45%;"></p></li><li><p>结构上的微调</p><ul><li>id path去掉BN</li><li>把所有的BN移动到add的后面</li><li><p>每个path加上relu</p><p><img src="/2021/02/09/repVGG/variants.png" width="45%;"></p></li></ul></li><li><p>ImageNet分类任务上对标其他模型</p><ul><li><p>simple augmentation </p><p>  <img src="/2021/02/09/repVGG/simple.png" width="45%;"></p></li><li><p>strong：Autoaugment, label smoothing and mixup </p><p>  <img src="/2021/02/09/repVGG/strong.png" width="45%;"></p></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>transform in CNN</title>
      <link href="/2021/02/03/transform-in-CNN/"/>
      <url>/2021/02/03/transform-in-CNN/</url>
      <content type="html"><![CDATA[<h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><ol><li>几何变换<ul><li>STN：<ul><li>普通的CNN能够隐式的学习一定的平移、旋转不变性，让网络能够适应这种变换：降采样结构本身能够使得网络对变换不敏感</li><li>从数据角度出发，我们还会引入各种augmentation，强化网络对变化的不变能力</li><li>deepMind为网络设计了一个显式的变换模块来学习各种变化，将distorted的输入变换回去，让网络学习更简单的东西</li><li>参数量：就是变换矩阵的参数，通常是2x3的纺射变化矩阵，也就是6个参数</li></ul></li><li>deformable conv：<ul><li>based on STN</li><li>针对分类和检测分别提出deformable convolution和deformable RoI pooling：</li><li>感觉deformable RoI pooling和guiding anchor里面的feature adaption是一个东西</li><li>参数量：regular kernel params 3x3 + deformable offsets 3x3x2</li><li>what’s new？<ul><li>个人认为引入更多的参数引入的变化</li><li>首先STN是从output到input的映射，使用变换矩阵M通常只能表示depictable transformation，且全图只有1个transformation</li><li>其次STN的sampling kernel也是预定义的算法，对kernel内的所有pixel使用相同的变化，也就是1个weight factor</li><li>deformable conv是从input到output的映射，映射可以是任意的transformation，且3x3x2的参数最多可以包含3x3种transformation</li><li>sampling kernel对kernel内的每个点，也可以有不同的权重，也就是3x3个weight factor</li></ul></li></ul></li><li>还有啥跟形变相关的</li></ul></li><li>attention机制<ul><li>spatial attention：STN，sSE</li><li>channel attention：SENet</li><li>同时使用空间attention和通道attention机制：CBAM</li></ul></li></ol><h2 id="STN-Spatial-Transformer-Networks"><a href="#STN-Spatial-Transformer-Networks" class="headerlink" title="STN: Spatial Transformer Networks"></a>STN: Spatial Transformer Networks</h2><ol><li><p>动机</p><ul><li>传统卷积：lack the ability of spacially invariant</li><li>propose a new learnable module<ul><li>can be inserted into CNN</li><li>spatially manipulate the data</li><li>without any extra supervision</li><li>models learn to be invariant to transformations</li></ul></li></ul></li><li><p>论点</p><ul><li>spatially invariant<ul><li>the ability of being invariant to large transformations of the input data</li></ul></li><li>max-pooling<ul><li>在一定程度上spatially invariant</li><li>因为receptive fields are fixed and local and small</li><li>必须叠加到比较深层的时候才能实现，intermediate feature layers对large transformations不太行</li><li>是一种pre-defined mechanism，跟sample无关</li></ul></li><li>spatial transformation module<ul><li>conditioned on individual samples</li><li>dynamic mechanism</li><li>produce a transformation and perform it on the entire feature map </li></ul></li><li>task场景<ul><li>distorted digits分类：对输入做tranform能够simplify后面的分类任务</li><li>co-localisation：</li><li>spatial attention</li></ul></li><li>related work<ul><li>生成器用来生成transformed images，从而判别器能够学习分类任务from transformation supervision</li><li>一些methods试图从网络结构、feature extractors的角度的获得invariant representations，while STN aims to achieve this by manipulating the data</li><li>manipulating the data通常就是基于attention mechanism，crop涉及differentiable问题</li></ul></li></ul></li><li><p>方法</p><ul><li><p>formulation </p><ul><li>localisation network：predict transform parameters</li><li>grid generator：基于predicted params生成sampling grid</li><li><p>sampler：element-multiply</p><p><img src="/2021/02/03/transform-in-CNN/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/transform-in-CNN/spacial transformer.png" width="80%;"></p></li></ul></li><li><p>localisation network</p><ul><li>input feature map $U \in R^{h<em>w</em>c}$</li><li>same transformation is applied to each channel</li><li>generate parameters of transformation $\theta$：1-d vector</li><li>fc / conv + final regression layer</li></ul></li><li><p>parameterised sampling grid</p><ul><li><p>sampling kernel</p></li><li><p>applied by pixel</p></li><li><p>general affine transformation：cropping，translation，rotation，scale，skew</p></li><li><p>ouput map上任意一点一定来自变换前的某一点，反之不一定，input map上某一点可能是bg，被crop掉了，所以pointwise transformation写成反过来的：</p><p>  <img src="/2021/02/03/transform-in-CNN/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/transform-in-CNN/pointwise transformation.png" width="50%;"></p><p>  <img src="/2021/02/03/transform-in-CNN/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/transform-in-CNN/transformation.png" width="70%;"></p></li><li><p>target points构成的点集就是sampling points on the input feature map</p></li></ul></li><li><p>differentiable image sampling </p><ul><li><p>通过上一步的矩阵transformation，得到input map上需要保留的source point set</p></li><li><p>对点集中每一点apply kernel</p></li><li><p>通用的插值表达式：</p><p>  <img src="/2021/02/03/transform-in-CNN/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/transform-in-CNN/interpolation.png" width="70%;"></p></li><li><p>最近邻kernel是个pulse函数</p><p>  <img src="/2021/02/03/transform-in-CNN/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/transform-in-CNN/nearest.png" width="70%;"></p></li><li><p>bilinear kernel是个distance&gt;1的全mute掉，分段可导</p><p>  <img src="/2021/02/03/transform-in-CNN/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/transform-in-CNN/bilinear.png" width="70%;"></p></li></ul></li><li><p>STN：Spatial Transformer Networks </p><ul><li>把spatial transformer嵌进CNN去：learn how to actively transform the features to help minimize the overall cost</li><li>computationally fast</li><li>几种用法<ul><li>feed the output of the localization network $\theta$ to the rest of the network：因为transform参数explicitly encodes目标的位置姿态信息</li><li>place multiple spatial transformers at increasing depth：串行能够让深层的transformer学习更抽象的变换</li><li>place multiple spatial transformers in parallel：并行的变换使得每个变换针对不同的object</li></ul></li></ul></li></ul></li><li><p>实验</p><p> <img src="/2021/02/03/transform-in-CNN/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/transform-in-CNN/error.png" width="70%;"></p><ul><li>R、RTS、P、E：distortion ahead</li><li>aff、proj、TPS：transformer predefined<ul><li>aff：给定角度？？</li><li>TPS：薄板样条插值</li></ul></li></ul></li></ol><h2 id="Deformable-Convolutional-Networks"><a href="#Deformable-Convolutional-Networks" class="headerlink" title="Deformable Convolutional Networks"></a>Deformable Convolutional Networks</h2><ol><li><p>动机</p><ul><li>CNN：fixed geometric structures</li><li>enhance the transformation modeling capability<ul><li>deformable convolution</li><li>deformable RoI pooling</li></ul></li><li>without additional supervision</li><li>share similiar spirit with STN</li></ul></li><li><p>论点</p><ul><li><p>to accommodate geometric variations </p><ul><li>data augmentation is limited to model large, unknown transformations </li><li>fixed receptive fields is undesirable for high level CNN layers that encode the semantics </li><li>使用大量增广的数据，枚举不全，而且收敛慢，所需网络参数量大</li><li>对于提取语义特征的高层网络来讲，固定的感受野对不同目标不友好</li></ul></li><li><p>introduce two new modules </p><ul><li>deformable convolution<ul><li>learning offsets for each kernel via additional convolutional layers </li></ul></li><li><p>deformable RoI pooling</p><ul><li>learning offset for each bin partition of the previous RoI pooling </li></ul><p><img src="/2021/02/03/transform-in-CNN/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/transform-in-CNN/deform.png" width="40%;"></p></li></ul></li></ul></li><li><p>方法</p><ul><li><p>overview</p><ul><li>operate on the 2D spatial domain </li><li>remains the same across the channel dimension </li></ul></li><li><p>deformable convolution</p><ul><li>正常的卷积：<ul><li>$y(p_0) = \sum w(p_n)*x(p_0 + p_n)$</li><li>$p_n \in R\{(-1,-1),(-1,0),…, (0,0), (1,1)\}$</li></ul></li><li>deformable conv：with offsets $\Delta p_n$<ul><li>$y(p_0) = \sum w(p_n)*x(p_0 + p_n + \Delta p_n)$</li><li>offset value is typically fractional </li><li>bilinear interpolation：<ul><li>$x(p) = \sum_q G(q,p)x(q)$</li><li>其中$G(q,p)$是条件：$G(q,p)=max(0, 1-|q_x-p_x|)*max(0, 1-|q_y-p_y|)$</li><li>只计算和offset点距离小于1个单位的邻近点</li></ul></li></ul></li><li>实现<ul><li><img src="/2021/02/03/transform-in-CNN/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/transform-in-CNN/conv.png" width="40%;"></li><li>offsets conv和特征提取conv是一样的kernel：same spatial resolution and dilation（N个position）</li><li>the channel dimension 2N：因为是x和y两个方向的offset</li></ul></li></ul></li><li><p>deformable RoI pooling</p><ul><li><p>RoI pooling converts an input feature map of arbitrary size into fixed size features </p></li><li><p>常规的RoI pooling</p><ul><li>divides ROI into k*k bins and for each bin：$y(i,j) = \sum_{p \in bin(i,j)} x(p_0+p)/n_{ij}$</li><li>对feature map上划分到每个bin里面所有的点</li></ul></li><li><p>deformable RoI pooling：with offsets $\Delta p_{ij}$</p><ul><li>$y(i,j) = \sum_{p \in bin(i,j)} x(p_0+p+\Delta p_{ij})/n_{ij}$</li><li>scaled normalized offsets：$\Delta p_{ij} = \gamma \Delta p_{ij} (w,h) $</li><li>normalized offset value is fractional </li><li>bilinear interpolation on the pooled map as above</li></ul></li><li><p>实现</p><ul><li><img src="/2021/02/03/transform-in-CNN/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/transform-in-CNN/pooling.png" width="40%;"></li><li>fc layer：k*k*2个element（sigmoid？）</li></ul></li><li><p>position sensitive RoI Pooling</p><ul><li>fully convolutional  </li><li>input feature map先通过卷积扩展成k*k*(C+1)通道</li><li><p>对每个C+1(包含k<em>k个feature map)，conv出全图的offset(2\</em>k*k个)</p><p><img src="/2021/02/03/transform-in-CNN/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/transform-in-CNN/pspooling.png" width="40%;"></p></li></ul></li></ul></li><li><p>deformable convNets</p><ul><li>initialized with zero weights</li><li>learning rates are set to $\beta$ times of the learning rate for the existing layers <ul><li>$\beta=1.0$ for conv</li><li>$\beta=0.01$ for fc</li></ul></li><li>feature extraction<ul><li>back：ResNet-101 &amp; Aligned-Inception-ResNet </li><li>withoutTop：A randomly initialized 1x1 conv is added at last to reduce the channel dimension to 1024 </li><li>last block<ul><li>stride is changed from 2 to 1</li><li>the dilation of all the convolution filters with kernel size&gt;1 is changed from 1 to 2</li></ul></li><li>Optionally last block<ul><li>use deformable conv in res5a,b,c</li></ul></li></ul></li><li>segmentation and detection<ul><li>deeplab predicts 1x1 score maps</li><li>Category-Aware RPN run region proposal with specific class</li><li>modified faster R-CNN：add ROI pooling at last conv</li><li>optional faster R-CNN：use deformable ROI pooling</li><li>R-FCN：state-of-the-art detector</li><li>optional R-FCN：use deformable ROI pooling</li></ul></li></ul><p><img src="/2021/02/03/transform-in-CNN/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/transform-in-CNN/Illustration.png" width="80%;"></p></li></ul></li><li><p>实验</p><ul><li><p>Accuracy steadily improves when more deformable convolution layers are used：使用越多层deform conv越好，经验取了3</p><p>  <img src="/2021/02/03/transform-in-CNN/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/transform-in-CNN/table1.png" width="80%;"></p></li><li><p>the learned offsets are highly adaptive to the image content：大目标的间距大，因为reception field大，consistent in different layers </p><p>  <img src="/2021/02/03/transform-in-CNN/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/transform-in-CNN/table2.png" width="45%;"></p></li><li><p>atrous convolution also improves：default networks have too small receptive fields，但是dilation需要手调到最优</p></li><li><p>using deformable RoI pooling alone already produces noticeable performance gains, using both obtains significant accuracy improvements</p><p>  <img src="/2021/02/03/transform-in-CNN/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/transform-in-CNN/table3.png" width="80%;"></p></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 几何变换 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>spineNet</title>
      <link href="/2021/01/28/spineNet/"/>
      <url>/2021/01/28/spineNet/</url>
      <content type="html"><![CDATA[<h2 id="SpineNet-Learning-Scale-Permuted-Backbone-for-Recognition-and-Localization"><a href="#SpineNet-Learning-Scale-Permuted-Backbone-for-Recognition-and-Localization" class="headerlink" title="SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization"></a>SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</h2><ol><li><p>动机</p><ul><li>object detection task<ul><li>requiring simultaneous recognition and localization</li><li>solely encoder performs not well</li><li>while encoder-decoder architectures are ineffective</li></ul></li><li><p>propose SpineNet</p><ul><li>scale-permuted intermediate features</li><li>cross-scale connections</li><li>searched by NAS on detection COCO</li><li>can transfer to classification tasks</li><li><p>在轻量和重量back的一阶段网络中都涨点领先</p><p><img src="/2021/01/28/spineNet/scale-permuted.png" width="40%;"></p></li></ul></li></ul></li><li><p>论点</p><ul><li>scale-decreasing backbone <ul><li>throws away the spatial information by down-sampling</li><li>challenging to recover</li><li>接一个轻量的FPN：</li></ul></li><li>scale-permuted model <ul><li>scales of features can increase/decrease anytime：retain the spacial information </li><li>connections go across scales：multi-scale fusion</li><li>searched by NAS</li><li>是一个完整的FPN，不是encoder-decoder那种可分的形式</li><li>directly connect to classification and bounding box regression subnets</li><li>base on ResNet50<ul><li>use bottleneck feature blocks</li><li>two inputs for each feature blocks</li><li>roughly the same computation</li></ul></li></ul></li></ul></li><li><p>方法</p><ul><li><p>formulation</p><ul><li>overall architecture<ul><li>stem：scale-decreased architecture </li><li>scale-permuted network</li><li>blocks in the stem network can be candidate inputs for the following scale-permuted network </li></ul></li><li>scale-permuted network<ul><li>building blocks：$B_k$</li><li>feature level：$L_3 - L_7$</li><li>output features：1x1 conv，$P_3 - P_7$</li></ul></li></ul></li><li><p>search space</p><ul><li><p>scale-permuted network：</p><ul><li>block只能从前往后connect</li><li>based on resNet blocks</li><li>channel 256 for $L_5, L_6, L_7$</li></ul></li><li><p>cross-scale connections：</p><ul><li><p>two input connections for each block</p></li><li><p>from lower ordering block / stem</p></li><li><p>resampling</p><ul><li>narrow factor  $\alpha$：1x1 conv</li><li>上采样：interpolation</li><li>下采样：3x3 s2 conv</li><li><p>element-wise add</p><p><img src="/2021/01/28/spineNet/resampling.png" width="50%;"></p></li></ul></li></ul></li><li><p>block adjustment</p><ul><li>intermediate blocks can adjust its scale level &amp; type</li><li>level from {-1, 0, 1, 2}</li><li>select from bottleneck / residual block</li></ul></li></ul></li><li><p>family of models</p><ul><li>R[N] - SP[M]：N feature layers in stem &amp; M feature layers in scale-permuted layers</li><li>gradually shift from stem to SP</li><li><p>with size decreasing </p><p><img src="/2021/01/28/spineNet/shifting.png" width="45%;"></p><p><img src="/2021/01/28/spineNet/family.png" width="75%;"></p></li></ul></li><li><p>spineNet family</p><ul><li>basic：spineNet-49</li><li>spineNet-49S：channel数scaled down by 0.65</li><li>spineNet-96：double the number of blocks</li><li>spineNet-143：repeat 3 times，fusion narrow factor $\alpha=1$</li><li>spineNet-190：repeat 4 times，fusion narrow factor $\alpha=1$，channel数scaled up by 1.3</li></ul></li></ul></li><li><p>实验</p><ul><li><p>在mid/heavy量级上，比resnet-family-FPN涨出两个点</p><p>  <img src="/2021/01/28/spineNet/heavy.png" width="45%;"></p></li><li><p>在light量级上，比mobileNet-family-FPN涨出一个点</p><p>  <img src="/2021/01/28/spineNet/light.png" width="45%;"></p></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> backbone </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>guided anchoring</title>
      <link href="/2021/01/27/guided-anchoring/"/>
      <url>/2021/01/27/guided-anchoring/</url>
      <content type="html"><![CDATA[<p>原作者知乎reference：<a href="https://zhuanlan.zhihu.com/p/55854246" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/55854246</a></p><ul><li>不完全是anchor-free，因为还是有decision grid to choose from的，应该说是adaptive anchor instead of hand-picked</li><li>为了特征和adaptive anchor对齐，引入deformable conv</li></ul><h2 id="Region-Proposal-by-Guided-Anchoring"><a href="#Region-Proposal-by-Guided-Anchoring" class="headerlink" title="Region Proposal by Guided Anchoring"></a>Region Proposal by Guided Anchoring</h2><ol><li><p>动机</p><ul><li>most methods<ul><li>predefined anchors</li><li>do a uniformed dense prediction </li></ul></li><li>our method<ul><li>use sematic features to guide the anchoring</li><li>anchor size也是网络预测参数，compute from feature map</li><li>arbitrary aspect ratios </li></ul></li><li>feature inconsistency<ul><li>不同的anchor loc都是对应feature map上某一个点</li><li>变化的anchor size和固定的位置向量之间存在inconsistency</li><li>引入feature adaption module</li></ul></li><li>use high-quality proposals<ul><li>GA-RPN提升了proposal的质量</li><li>因此我们对proposal进入stage2的条件更严格</li></ul></li><li>adopt in Fast R-CNN, Faster R-CNN and RetinaNet均涨点<ul><li>RPN提升显著：9.1</li><li>MAP也有涨点：1.2-2.7</li></ul></li><li>还可以boosting trained models<ul><li>boosting a two-stage detector by a fine-tuning schedule</li></ul></li></ul></li><li><p>论点</p><ul><li><p>alignment &amp; consistency</p><ul><li><p>我们用feature map的pixels作为anchor representations，那么anchor centers必须跟feature pixels保持align</p></li><li><p>不同pixel的reception field必须跟对应的anchor size保持匹配</p></li><li>previous sliding window scheme对每个pixel都做一样的操作，用同样一组anchor，因此是align和consist的</li><li>previous progressly refining scheme对anchor的位置大小做了refinement，ignore the alignment &amp; consistency issue，是不对的！！</li></ul></li><li><p>disadvantage of predefined anchors</p><ul><li>hard hyperparams</li><li>huge pos/neg imbalance &amp; computation </li></ul></li><li><p>we propose GA-RPN</p><ul><li>learnable anchor shapes to mitigate the hand-picked issue</li><li>feature adaptation to solve the consistency issue</li><li>key concerns in this paper<ul><li>learnable anchors</li><li>joint anchor distribution </li><li>alignment &amp; consistency</li><li>high-quality proposals </li></ul></li></ul></li></ul></li><li><p>方法</p><ul><li><p>formulation</p><ul><li>$p(x,y,w,h|I) = p(x,y|I)p(w,h|x,y,I)$</li><li>将问题解耦成位置和尺寸的预测，首先anchor的loc服从full image的均匀分布，anchor的size建立在loc存在的基础上</li><li>two branches for loc &amp; shape prediction<ul><li>loc：binary classification，hxwx1</li><li>shape：location-dependent shapes，hxwx2</li><li>anchors：loc probabilities above a certain threshold &amp; correponding ‘most probable’ anchor shape</li></ul></li><li>multi-scale<ul><li>the anchor generation parameters are shared</li></ul></li><li><p>feature adaptation module</p><ul><li>adapts the feature according to the anchor shape </li></ul><p><img src="/2021/01/27/guided-anchoring/guided anchoring.png" width="80%;"></p></li></ul></li><li><p>anchor location prediction</p><ul><li>indicates the probability of an object’s center  </li><li>一层卷积：1x1 conv，channel1，sigmoid</li><li>transform back：each grid(i,j) corresponds to coords ((i+0.5)*s, (j+0.5)*s) on the origin map</li><li>filter out 90% of the regions</li><li>thus replace the ensuing conv layers by masked convs</li><li>groud truth<ul><li>binary label map </li><li>each level：center region &amp; ignore region &amp; outside region，基于object center的方框<ul><li>$\sigma_1=0.2，\sigma_2=0.5$：region box的长宽系数</li><li>？？？用centerNet的heatmap会不会更好？？？</li></ul></li></ul></li><li>focal loss $L_{loc}$</li></ul></li><li>anchor shape prediction<ul><li>predicts the best shape for each location</li><li>best shape：a shape that lead to best iou with the nearest gt box</li><li>一层卷积：1x1 conv，channel2，[-1,1]</li><li>transform layer：transform direct [-1,1] outputs to real box shape<ul><li>$w = \sigma <em> s </em> e^{dw}$</li><li>$h = \sigma <em> s </em> e^{dh}$</li><li>s：stride</li><li>$\sigma$：经验参数，8 in experiments</li></ul></li><li>set 9 pairs of (w,h) as RetinaNet，calculate the IoU of these sampled anchors with gt，take the max as target value</li><li>bounded iou loss：$L_{shape} = L_1(1-min(\frac{w}{w_g}, \frac{w_g}{w})) + L_1(1-min(\frac{h}{h_g}, \frac{h_g}{h}))$ </li></ul></li><li>feature adaptation<ul><li>intuition：the feature corresponding to different size of anchor shapes应该encode different content region</li><li>inputs：feature map &amp; anchor shape</li><li>location-dependent transformation：3x3 deformable conv</li><li>deformable conv的offset是anchor shape得到的</li><li>outputs：adapted features</li></ul></li><li>with adapted features<ul><li>then perform further classification and bounding-box regression </li></ul></li><li>training<ul><li>jointly optimize：$L = \lambda_1 L_{loc} + \lambda_2 L_{shape} + L_{cls} + L_{reg}$</li><li>$\lambda_1=0.2，\lambda_2=0.5$</li><li>each level of feature map should only target objects of a specific scale range：但是ASFF论文主张说这种arrange by scale的模式会引入前背景inconsistency？？</li></ul></li><li>High-quality Proposals <ul><li>set a higher positive/negative threshold </li><li>use fewer samples  </li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 目标检测，one/two-stage </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ASFF</title>
      <link href="/2021/01/25/ASFF/"/>
      <url>/2021/01/25/ASFF/</url>
      <content type="html"><![CDATA[<h2 id="Learning-Spatial-Fusion-for-Single-Shot-Object-Detection"><a href="#Learning-Spatial-Fusion-for-Single-Shot-Object-Detection" class="headerlink" title="Learning Spatial Fusion for Single-Shot Object Detection"></a>Learning Spatial Fusion for Single-Shot Object Detection</h2><ol><li><p>动机</p><ul><li>inconsistency when fuse across different feature scales</li><li>propose ASFF<ul><li>suppress the inconsistency </li><li>spatially filter conflictive information：想法应该跟SSE-block类似</li></ul></li><li><p>build on yolov3</p><ul><li>introduce a bag of tricks</li><li>anchor-free pipeline</li></ul><p><img src="/2021/01/25/ASFF/ASFF.png" width="70%;"></p></li></ul></li><li>论点<ul><li>ssd is one of the first to generate pyramidal feature representations<ul><li>deeper layers reuse the formers</li><li>bottom-up path</li><li>small instances suffers low acc because containing insufficient semanic info</li></ul></li><li>FPN use top-down path<ul><li>shares rich semantics at all levels </li><li>improvement：more strengthening feature fusion </li></ul></li><li>在使用FPN时，通常不同scale的目标绑定到不同的level上面<ul><li>inconsistency：其他level的feature map对应位置的信息则为背景</li><li>some methods set ignore region in adjacent features</li></ul></li></ul></li><li><p>方法</p><ul><li>introduce advanced techniques  <ul><li>mixup</li><li>cosine learning rate schedule </li><li>sync-bn</li><li>an anchor-free branch to run jointly with anchor-based ones</li><li>L1 loss + IoU loss</li></ul></li><li>fusion<ul><li>全联接而非adjacent merge：三个level的fuse map都来自三个level的feature map</li><li>上采样：<ul><li>1x1 conv：对齐channel</li><li>upsamp with interpolation</li></ul></li><li>下采样：<ul><li>s2：3x3 s2 conv</li><li>s4：maxpooling + 3x3 s2 conv</li></ul></li><li>adaptive fusion<ul><li>pixel level的reweight</li><li>shared across channels：hxwx1</li><li>对来自三个level的feature map，resolution对齐以后，分别1x1conv，channel 1</li><li>norm the weights：softmax</li><li>为啥能suppress inconsistency：三个level的像素点，只激活一个另外两个是0的情况是绝对不harm的，相当于上面ignore那个方法拓展成adaptive</li></ul></li></ul></li><li>training<ul><li>apply mixup on the classification pretraining of D53</li><li>turn off mixup augmentation for the last 30 epochs. </li></ul></li><li>inference<ul><li>the detection header at each level first predicts the shape of anchors？？？这个不太懂</li></ul></li><li>ASFF &amp; ASFF*<ul><li>enhanced version of ASFF by integrating other lightweight modules </li><li>dropblock &amp; RFB</li></ul></li></ul></li><li><p>实现</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ASFF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, level, activate, rfb=False, vis=False)</span>:</span></span><br><span class="line">        super(ASFF, self).__init__()</span><br><span class="line">        self.level = level</span><br><span class="line">        self.dim = [<span class="number">512</span>, <span class="number">256</span>, <span class="number">128</span>]</span><br><span class="line">        self.inter_dim = self.dim[self.level]</span><br><span class="line">        <span class="keyword">if</span> level == <span class="number">0</span>:</span><br><span class="line">            self.stride_level_1 = conv_bn(<span class="number">256</span>, self.inter_dim, kernel=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">            self.stride_level_2 = conv_bn(<span class="number">128</span>, self.inter_dim, kernel=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">            self.expand = conv_bn(self.inter_dim, <span class="number">512</span>, kernel=<span class="number">3</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">elif</span> level == <span class="number">1</span>:</span><br><span class="line">            self.compress_level_0 = conv_bn(<span class="number">512</span>, self.inter_dim, kernel=<span class="number">1</span>)</span><br><span class="line">            self.stride_level_2 = conv_bn(<span class="number">128</span>, self.inter_dim, kernel=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">            self.expand = conv_bn(self.inter_dim, <span class="number">256</span>, kernel=<span class="number">3</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">elif</span> level == <span class="number">2</span>:</span><br><span class="line">            self.compress_level_0 = conv_bn(<span class="number">512</span>, self.inter_dim, kernel=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">            self.compress_level_1= conv_bn(<span class="number">256</span>,self.inter_dim,kernel=<span class="number">1</span>,stride=<span class="number">1</span>)</span><br><span class="line">            self.expand = conv_bn(self.inter_dim, <span class="number">128</span>, kernel=<span class="number">3</span>, stride=<span class="number">1</span>)</span><br><span class="line">        compress_c = <span class="number">8</span> <span class="keyword">if</span> rfb <span class="keyword">else</span> <span class="number">16</span>  </span><br><span class="line">        self.weight_level_0 = conv_bn(self.inter_dim, compress_c, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        self.weight_level_1 = conv_bn(self.inter_dim, compress_c, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        self.weight_level_2 = conv_bn(self.inter_dim, compress_c, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        self.weight_levels = conv_bias(compress_c * <span class="number">3</span>, <span class="number">3</span>, kernel=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.vis = vis</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_level_0, x_level_1, x_level_2)</span>:</span></span><br><span class="line">      <span class="comment"># 跟论文描述一样：上采样先1x1conv对齐，再upinterp，下采样3x3 s2 conv</span></span><br><span class="line">        <span class="keyword">if</span> self.level == <span class="number">0</span>:</span><br><span class="line">            level_0_resized = x_level_0</span><br><span class="line">            level_1_resized = self.stride_level_1(x_level_1)</span><br><span class="line">            level_2_downsampled_inter = F.max_pool2d(x_level_2, <span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">            level_2_resized = self.stride_level_2(level_2_downsampled_inter)</span><br><span class="line">        <span class="keyword">elif</span> self.level == <span class="number">1</span>:</span><br><span class="line">            level_0_compressed = self.compress_level_0(x_level_0)</span><br><span class="line">            sh = torch.tensor(level_0_compressed.shape[<span class="number">-2</span>:])*<span class="number">2</span></span><br><span class="line">            level_0_resized = F.interpolate(level_0_compressed, tuple(sh), <span class="string">'nearest'</span>)</span><br><span class="line">            level_1_resized = x_level_1</span><br><span class="line">            level_2_resized = self.stride_level_2(x_level_2)</span><br><span class="line">        <span class="keyword">elif</span> self.level == <span class="number">2</span>:</span><br><span class="line">            level_0_compressed = self.compress_level_0(x_level_0)</span><br><span class="line">            sh = torch.tensor(level_0_compressed.shape[<span class="number">-2</span>:])*<span class="number">4</span></span><br><span class="line">            level_0_resized = F.interpolate(level_0_compressed, tuple(sh), <span class="string">'nearest'</span>)</span><br><span class="line">            level_1_compressed = self.compress_level_1(x_level_1)</span><br><span class="line">            sh = torch.tensor(level_1_compressed.shape[<span class="number">-2</span>:])*<span class="number">2</span></span><br><span class="line">            level_1_resized = F.interpolate(level_1_compressed, tuple(sh),<span class="string">'nearest'</span>)</span><br><span class="line">            level_2_resized = x_level_2</span><br><span class="line">        <span class="comment"># 这里得到的resized特征图不直接转换成一通道的weighting map，</span></span><br><span class="line">        <span class="comment"># 而是先1x1conv降维到8/16，然后concat，然后3x3生成3通道的weighting map</span></span><br><span class="line">        <span class="comment"># weighting map相当于一个prediction head，所以是conv_bias_softmax，无bn</span></span><br><span class="line">        level_0_weight_v = self.weight_level_0(level_0_resized)</span><br><span class="line">        level_1_weight_v = self.weight_level_1(level_1_resized)</span><br><span class="line">        level_2_weight_v = self.weight_level_2(level_2_resized)</span><br><span class="line">        levels_weight_v = torch.cat((level_0_weight_v, level_1_weight_v, level_2_weight_v), <span class="number">1</span>)</span><br><span class="line">        levels_weight = self.weight_levels(levels_weight_v)</span><br><span class="line">        levels_weight = F.softmax(levels_weight, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># reweighting</span></span><br><span class="line">        fused_out_reduced = level_0_resized * levels_weight[:, <span class="number">0</span>:<span class="number">1</span>, :, :] + \</span><br><span class="line">                            level_1_resized * levels_weight[:, <span class="number">1</span>:<span class="number">2</span>, :, :] + \</span><br><span class="line">                            level_2_resized * levels_weight[:, <span class="number">2</span>:, :, :]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3x3的conv，是特征图平滑</span></span><br><span class="line">        out = self.expand(fused_out_reduced)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.vis:</span><br><span class="line">            <span class="keyword">return</span> out, levels_weight, fused_out_reduced.sum(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 目标检测，yolov3 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>VoVNet</title>
      <link href="/2021/01/22/VoVNet/"/>
      <url>/2021/01/22/VoVNet/</url>
      <content type="html"><![CDATA[<h2 id="An-Energy-and-GPU-Computation-Efficient-Backbone-Network-for-Real-Time-Object-Detection"><a href="#An-Energy-and-GPU-Computation-Efficient-Backbone-Network-for-Real-Time-Object-Detection" class="headerlink" title="An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection"></a>An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection</h2><ol><li><p>动机</p><ul><li>denseNet<ul><li>dense path：diverse receptive fields</li><li>heavy memory cost &amp; low efficiency </li></ul></li><li>we propose a backbone<ul><li>preserve the benefit of concatenation</li><li>improve denseNet efficiency</li><li>VoVNet comprised of One-Shot Aggregation (OSA)</li></ul></li><li>apply to one/two stage object detection tasks<ul><li>outperforms denseNet &amp; resNet based ones</li><li>better small object detection performance</li></ul></li></ul></li><li><p>论点</p><ul><li>main difference between resNet &amp; denseNet<ul><li>aggregation：summation &amp; concatenation <ul><li>summation would washed out the early features</li><li>concatenation last as it preserves</li></ul></li></ul></li><li>GPU parallel computation <ul><li>computing utilization is maximized when operand tensor is larger </li><li>many 1x1 convs for reducing dimension</li><li>dense connections in intermediate layers are inducing the inefficiencies </li></ul></li><li><p>VoVNet </p><ul><li>hypothesize that the dense connections are redundant</li><li>OSA：aggregates intermediate features at once</li><li>test as object detection backbone：outperforms DenseNet &amp; ResNet with better energy efficiency and speed</li></ul></li><li><p>factors for efficiency</p><ul><li>FLOPS and model sizes are indirect metrics</li><li>energy per image and frame per second are more practical</li><li>MAC：<ul><li>memory accesses cost，$hw(c_i+c_o) + k^2 c_ic_o$</li><li>memory usage不止跟参数量有关，还跟特征图尺寸相关</li><li>MAC can be minimized when input channel size equals the output</li></ul></li><li>FLOPs/s<ul><li>splitting a large convolution operation into several fragmented smaller operations makes GPU computation inefficient as fewer computations are processed in parallel</li><li>所以depthwise/bottleneck理论上降低了计算量FLOP，但是从GPU并行的角度efficiency降低，并没有显著提速：cause more sequential computations</li><li>以时间为单位的FLOPs才是fair的</li></ul></li></ul></li></ul></li><li><p>方法</p><ul><li><p>hypothesize </p><ul><li>dense connection makes similar between neighbor layers</li><li>redundant</li></ul></li><li><p>OSA</p><ul><li>dense connection：former features concats in every following features</li><li><p>one-shot connection：former features concats once in the last feature</p><p><img src="/2021/01/22/VoVNet/OSA.png" width="40%;"></p></li><li><p>最开始跟dense block保持参数一致：一个block里面12个layers，channel20，发现深层特征contributes less，所以换成浅层，5个layers，channel43，发现有涨点：implies that building deep intermediate feature via dense connection is less effective than expected </p><p><img src="/2021/01/22/VoVNet/weight_mat.png" width="40%;"></p></li><li><p>in/out channel数相同</p><ul><li>much less MAC：<ul><li>denseNet40：3.7M</li><li>OSA：5layers，channel43，2.5M</li><li>对于higher resolution的detection任务impies more fast and energy efficient </li></ul></li><li>GPU efficiency <ul><li>不需要那好几十个1x1</li></ul></li></ul></li></ul></li><li><p>architecture</p><ul><li>stem：3个3x3conv</li><li>downsamp：s2的maxpooling</li><li>stages：increasing channels enables more rich semantic high-level information，better feature representation</li><li><p>deeper：makes more modules in stage3/4</p><p><img src="/2021/01/22/VoVNet/VoVNet.png" width="80%;"></p></li></ul></li></ul></li><li><p>实验</p><ul><li>one-stage：refineDet</li><li>two-stage：Mask-RCNN</li></ul></li></ol>]]></content>
      
      
    </entry>
    
    <entry>
      <title>GCN</title>
      <link href="/2021/01/18/GCN/"/>
      <url>/2021/01/18/GCN/</url>
      <content type="html"><![CDATA[<p>reference：<a href="https://mp.weixin.qq.com/s/SWQHgogAP164Kr082YkF4A" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/SWQHgogAP164Kr082YkF4A</a></p><ol><li><p>图</p><ul><li>$G = (V,E)$：节点 &amp; 边，连通图 &amp; 孤立点</li><li>邻接矩阵A：NxN，有向 &amp; 无向</li><li>度矩阵D：NxN对角矩阵，每个节点连接的节点</li><li>特征矩阵X：NxF，每个1-dim F是每个节点的特征向量</li></ul></li><li><p>特征学习</p><ul><li>可以类比CNN：对其领域（kernel）内特征进行线性变换（w加权），然后求和，然后激活函数</li><li>$H^{k+1} = f(H^{k},A) = \sigma(AH^{k}W^{k})$<ul><li>H：running updating 特征矩阵，NxFk</li><li>A：0-1邻接矩阵，NxN</li><li>W：权重，$F_k$x$F_{k+1}$</li></ul></li><li>权重所有节点共享</li><li>节点的邻接节点可以看做感受野</li><li>网络加深，感受野增大：节点的特征融合了更多节点的信息</li></ul></li><li><p>图卷积</p><ul><li><p>A中没有考虑自己的特征：添加自连接</p><ul><li>A = A + I</li></ul></li><li><p>加法规则对度大的节点，特征会越来越大：归一化</p><ul><li><p>使得邻接矩阵每行和为1：左乘度矩阵的逆</p></li><li><p>数学实质：求平均</p></li><li><p>one step further：不单对行做平均，对度较大的邻接节点也做punish</p><p>  <img src="/2021/01/18/GCN/graph-conv-layer.png" width="60%;"></p></li></ul></li><li><p>GCN网络</p><p>  <img src="/2021/01/18/GCN/gcn.png" width="60%;"></p></li></ul></li><li><p>实现</p><ul><li><p>weights：in x out，kaiming_uniform_initialize</p></li><li><p>bias：out，zero_initialize</p></li><li><p>activation：relu</p></li><li><p>A x H x W：左乘是系数矩阵乘法</p></li><li><p>邻接矩阵的结构从输入开始就不变了，和每层的特征矩阵一起作为输入，传入GCN</p></li><li><p>分类头：最后一层预测Nxn_class的特征向量，提取感兴趣节点F(n_class)，然后softmax，对其分类</p></li><li><p>归一化</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对称归一化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize_adj</span><span class="params">(adj)</span>:</span></span><br><span class="line">    <span class="string">"""compute L=D^-0.5 * (A+I) * D^-0.5"""</span></span><br><span class="line">    adj += sp.eye(adj.shape[<span class="number">0</span>])</span><br><span class="line">    degree = np.array(adj.sum(<span class="number">1</span>))</span><br><span class="line">    d_hat = sp.diags(np.power(degree, <span class="number">-0.5</span>).flatten())</span><br><span class="line">    norm_adj = d_hat.dot(adj).dot(d_hat)</span><br><span class="line">    <span class="keyword">return</span> norm_adj</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 均值归一化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize_adj</span><span class="params">(adj)</span>:</span></span><br><span class="line">    <span class="string">"""compute L=D^-1 * (A+I)"""</span></span><br><span class="line">    adj += sp.eye(adj.shape[<span class="number">0</span>])</span><br><span class="line">    degree = np.array(adj.sum(<span class="number">1</span>))</span><br><span class="line">    d_hat = sp.diags(np.power(degree, <span class="number">-1</span>).flatten())</span><br><span class="line">    norm_adj = d_hat.dot(adj)</span><br><span class="line">    <span class="keyword">return</span> norm_adj</span><br></pre></td></tr></table></figure></li></ul></li><li><p>应用场景</p></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 入门科普 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>transformers</title>
      <link href="/2021/01/18/transformers/"/>
      <url>/2021/01/18/transformers/</url>
      <content type="html"><![CDATA[<h2 id="startup"><a href="#startup" class="headerlink" title="startup"></a>startup</h2><p>reference1：<a href="https://mp.weixin.qq.com/s/Rm899vLhmZ5eCjuy6mW_HA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/Rm899vLhmZ5eCjuy6mW_HA</a></p><p>reference2：<a href="https://zhuanlan.zhihu.com/p/308301901" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/308301901</a></p><ol><li><p>NLP &amp; RNN</p><ul><li>文本涉及上下文关系</li><li>RNN时序串行，建立前后关系</li><li>缺点：对超长依赖关系失效，不好并行化</li></ul></li><li><p>NLP &amp; CNN</p><ul><li>文本是1维时间序列</li><li>1D CNN，并行计算</li><li>缺点：CNN擅长局部信息，卷积核尺寸和长距离依赖的balance</li></ul></li><li><p>NLP &amp; transformer</p><ul><li>对流入的每个单词，建立其对词库的权重映射，权重代表attention</li><li>自注意力机制</li><li><p>建立长距离依赖</p><p><img src="/2021/01/18/transformers/transformer.png" width="30%;"></p></li></ul></li><li><p>put in CV</p><ul><li>插入类似的自注意力层</li><li>完全抛弃卷积层，使用Transformers</li></ul></li><li><p>RNN &amp; LSTM &amp; GRU cell</p><ul><li><p>标准要素：输入x、输出y、隐层状态h</p></li><li><p>RNN</p><ul><li>RNN cell每次接收一个当前输入$x_t$，和前一步的隐层输出$h_{t-1}$，然后产生一个新的隐层状态$h_t$，也是当前的输出$y_t$</li><li>formulation：$y_t, h_t = f(x_t, h_{t-1})$</li><li>same parameters for each time step：同一个cell每个time step的权重共享</li><li><p>一个问题：梯度消失/爆炸</p><ul><li>考虑hidden states’ chain的简化形式：$h_t = \theta^t h_0$，一个sequence forward下去就是same weights multiplied over and over again</li><li>另外tanh也是会让神经元梯度消失/爆炸</li></ul><p><img src="/2021/01/18/transformers/RNN.png" width="40%;"></p></li></ul></li><li><p>LSTM</p><ul><li><p>key ingredient</p><ul><li>cell：增加了一条cell state workflow，优化梯度流</li><li>gate：通过门结构删选携带信息，优化长距离关联</li></ul><p><img src="/2021/01/18/transformers/LSTM.png" width="25%;"></p></li><li><p>可以看到LSTM的循环状态有两个：细胞状态$c_t$和隐层状态$h_t$，输出的$y_t$仍旧是$h_t$</p></li></ul></li><li><p>GRU</p><ul><li><p>LSTM的变体，仍旧是门结构，比LSTM结构简单，参数量小，据说更好训练</p><p><img src="/2021/01/18/transformers/GRU.png" width="25%;"></p></li></ul></li></ul></li><li><p>papers</p><p>[一个列了很多论文的主页] <a href="https://github.com/dk-liang/Awesome-Visual-Transformer" target="_blank" rel="noopener">https://github.com/dk-liang/Awesome-Visual-Transformer</a></p><p>[经典考古]</p><p>​    * [Seq2Seq 2014] Sequence to Sequence Learning with Neural Networks，Google，最早的encoder-decoder stacking LSTM用于机翻</p><p>​    * [self-attention/Transformer 2017] Transformer: Attention Is All You Need，Google，</p><p>​    * [bert 2019] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding，Google，NLP，输入single sentence/patched sentences，用Transformer encoder提取bidirectional cross sentence representation，用输出的第一个logit进行分类</p><p>[综述]</p><p>​    * [综述2020] Efficient Transformers: A Survey，Google，</p><p>​    * [综述2021] Transformers in Vision: A Survey，迪拜，</p><p>​    * [综述2021] A Survey on Visual Transformer，华为，</p><p>[classification]</p><p>​    * [ViT 2020] AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE，Google，分类任务，用transformer的encoder替换CNN再加分类头，每个feature patch作为一个input embedding，channel dim是vector dim，可以看到跟bert基本一样，就是input sequence换成patch，后续基于它的提升有DeiT、LV-ViT</p><p>​    * [BotNet 2021] Bottleneck Transformers for Visual Recognition，Google，将CNN backbone最后几个stage替换成MSA</p><p>​    * [CvT 2021] CvT: Introducing Convolutions to Vision Transformers，微软，</p><p>[detection]</p><p>​    * [DeTR 2020] DeTR: End-to-End Object Detection with Transformers，Facebook，目标检测，CNN+transformer(en-de)+预测头，每个feature pixel作为一个input embedding，channel dim是vector dim</p><p>​    * [Swin 2021] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows，微软</p><p>[segmentation]</p><p>​    * [SETR] Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers，复旦，水，感觉就是把FCN的back换成transformer</p><p>[Unet+Transformer]：</p><p>​    * [UNETR 2021] UNETR: Transformers for 3D Medical Image Segmentation，英伟达，直接使用transformer encoder做unet encoder</p><p>​    * [TransUNet 2021] TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation，encoder stream里面加transformer block</p><p>​    * [TransFuse 2021] TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation，大学，CNN feature和Transformer feature进行bifusion</p></li></ol><h2 id="Sequence-to-Sequence"><a href="#Sequence-to-Sequence" class="headerlink" title="Sequence to Sequence"></a>Sequence to Sequence</h2><ol><li><p>[a keras tutorial][<a href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" target="_blank" rel="noopener">https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html</a>]</p><ul><li><p>general case</p><ul><li>extract the information of the entire input sequence</li><li>then start generate the output sequence</li></ul></li><li><p>seq2seq model workflow</p><ul><li>a (stacking of) RNN layer acts as encoder<ul><li>processes the input sequence</li><li>returns its own internal state：不要RNN的outputs，只要internal states</li><li>encoder编码得到的东西叫Context Vector</li></ul></li><li>a (stacking of) RNN layer acts as decoder<ul><li>given previous characters of the target sequence</li><li>it is trained to predict the next characters of the target sequence</li><li>teacher forcing：<ul><li>输入是target sequence，训练目标是使模型输出offset by one timestep的target sequence</li><li>也可以不teacher forcing：直接把预测作为next step的输入</li></ul></li><li>Context Vector的同质性：每个step，decoder都读取一样的Context Vector作为initial_state</li></ul></li><li>when inference<ul><li>第一步获取input sequence的state vectors</li><li>repeat<ul><li>给decoder输入input states和out sequence(begin with a 起始符)</li><li>从prediction中拿到next character</li></ul></li><li>append the character to the output sequence</li><li>until：得到end character / hit the character limit</li></ul></li></ul></li><li><p>implementation</p><p>  <a href="https://github.com/AmberzzZZ/transformer/blob/master/seq2seq.py" target="_blank" rel="noopener">https://github.com/AmberzzZZ/transformer/blob/master/seq2seq.py</a></p></li></ul></li><li><p>one step further</p><ul><li>改进方向<ul><li>bi-directional RNN：粗暴反转序列，有效涨点</li><li>attention：本质是将encoder的输出Context Vector加权</li><li>ConvS2S：还没看</li></ul></li><li>主要都是针对RNN的缺陷提出</li></ul></li><li><p>动机</p><ul><li>present a general end-to-end sequence learning approach<ul><li>multi-layered LSTMs</li><li>encode the input seq to a fix-dim vector</li><li>decode the target seq from the fix-dim vector</li></ul></li><li>LSTM did not have difficulty on long sentences </li><li><p>reversing the order of the words improved performance</p><p><img src="/2021/01/18/transformers/seq2seq.png" width="65%;"></p></li></ul></li><li><p>方法</p><ul><li><p>standard  RNN</p><ul><li><p>given a sequence $(x_1, x_2, …, x_T)$</p></li><li><p>iterating：</p><script type="math/tex; mode=display">  h_t = sigm(W^{hx} x_t + W^{hh}h_{t-1})\\  y_t = W^{yh} h_t</script></li><li><p>如果输入、输出的长度事先已知且固定，一个RNN网络就能建模seq2seq model了</p></li><li><p>如果输入、输出的长度不同、并且服从一些更复杂的关系？就得用两个RNN网络，一个将input seq映射成fixed-sized vector，另一个将vector映射成output seq，but long-term-dependency issue</p></li></ul></li><li><p>LSTM</p><ul><li>LSTM是始终带着全部seq的信息的，如上图那样</li></ul></li><li><p>our actual model</p><ul><li>use two LSTMs：encoder-decoder能够增加参数量</li><li>an LSTM with four layers：deeper</li><li>input sequence倒序：真正的句首更接近trans的句首，makes it easy for SGD to establish communication</li></ul></li><li><p>training details</p><ul><li>LSTM：4 layers，1000 cells</li><li>word-embedding：1000-dim，(input vocab 160,000, output vocab 80,000)</li><li>naive softmax</li><li>uniform initialization：(-0.08, 0.08)</li><li>SGD，lr=0.7，half by every half epoch，total 7.5 epochs</li><li>gradient norm [10, 25]</li><li>all sentences in a minibatch are roughly of the same length</li></ul></li></ul></li></ol><h2 id="Transformer-Attention-Is-All-You-Need"><a href="#Transformer-Attention-Is-All-You-Need" class="headerlink" title="Transformer: Attention Is All You Need"></a>Transformer: Attention Is All You Need</h2><ol><li><p>动机</p><ul><li>sequence2sequence models<ul><li>encoder + decoder</li><li>RNN / CNN + an attention path</li></ul></li><li>we propose Transformer<ul><li>base solely on attention mechanisms</li><li>more parallelizable and less training time</li></ul></li></ul></li><li><p>论点</p><ul><li>sequence modeling<ul><li>主流：RNN，LSTM，gated<ul><li>align the positions to computing time steps</li><li>sequential本质阻碍并行化</li></ul></li><li>Attention mechanisms acts as a integral part<ul><li>in previous work used in conjunction with the RNN</li></ul></li><li>为了并行化<ul><li>some methods use CNN as basic building blocks</li><li>difficult to learn dependencies between distant positions</li></ul></li></ul></li><li>we propose Transformer<ul><li>rely entirely on an attention mechanism</li><li>draw global dependencies</li></ul></li><li>self-attention<ul><li>relating different positions of a single sequence</li><li>to generate a overall representation of the sequence</li></ul></li></ul></li><li><p>方法</p><ul><li><p>encoder-decoder </p><ul><li>encoder：doc2emb<ul><li>given an input sequence of symbol representation $(x_1, x_2, …, x_n)$</li><li>map to a sequence of continuous representations  $(z_1, z_2, …, z_n)$，(embeddings)</li></ul></li><li>decoder：hidden layers<ul><li>given embeddings z</li><li>generate an output sequence  $(y_1, y_2, …, y_m)$ one element at a time</li><li>the previous generated symbols are served as additional input when computing the current time step</li></ul></li></ul></li><li><p>Transformer Architecture</p><ul><li><p>Transformer use </p><ul><li>for both encoder and decoder</li><li><p>stacked self-attention and point-wise fully-connected layers</p><p><img src="/2021/01/18/transformers/architecture.png" width="40%;"></p></li></ul></li><li><p>encoder</p><ul><li>N=6 identical layers</li><li>each layer has 2 sub-layers<ul><li>multi-head self-attention mechanism</li><li>postision-wise fully connected layer</li></ul></li><li>residual<ul><li>for two sub-layers independently</li><li>add &amp; layer norm</li></ul></li><li>d=512</li></ul></li><li><p>decoder</p><ul><li>N=6 identical layers</li><li>3 sub-layers<ul><li>[new] masked multi-head self-attention：combine了先验知识，output embedding只能基于在它之前的time-step的embedding计算</li><li>multi-head self-attention mechanism</li><li>postision-wise fully connected layer</li></ul></li><li>residual</li></ul></li><li><p>attention</p><ul><li>reference：<a href="https://bbs.cvmart.net/articles/4032" target="_blank" rel="noopener">https://bbs.cvmart.net/articles/4032</a></li><li>step1：project embedding to query-key-value pairs<ul><li>$Q = W_Q^{d<em>d} A^{d</em>N}$</li><li>$K = W_K^{d<em>d} A^{d</em>N}$</li><li>$V = W_V^{d<em>d} A^{d</em>N}$</li></ul></li><li>step2：scaled dot-product attention<ul><li>$A^{N*N}=softmax(K^TQ/\sqrt{d})$</li><li>$B^{d<em>N} = V^{d</em>N}A^{N*N}$</li></ul></li><li>multi-head attention <ul><li>以上的step1&amp;step2操作performs a single attention function</li><li>事实上我们可以用多组projection得到多组$\{Q,K,V\}^h$，in parallel地执行attention运算，得到多组$\{B^{d*N}\}^h$</li><li>concat &amp; project<ul><li>concat in d-dim：$B\in R^{(d<em>h)</em>N}$</li><li>linear project：$out = W^{d<em>(d</em>h)} B$</li></ul></li><li>h=8</li><li>$d_{in}/h=64$：embedding的dim</li><li>$d_{out}=64$：query-key-value的dim</li></ul></li></ul></li><li><p>positional encoding</p><ul><li><p>数学本质是一个hand-crafted的映射矩阵$W^P$和one-hot的编码向量$p$：</p><script type="math/tex; mode=display">  \left[ \begin{array}{ccc}  a\\  e  \end{array}   \right ]  =  [W^I, W^P]   \left[ \begin{array}{ccc}  x\\  p  \end{array}   \right ]</script></li><li><p>用PE表示e</p><p>  <img src="/2021/01/18/transformers/PE.png" width="35%;"></p><ul><li>pos是sequence x上的position</li><li>2i和2i+1是embedding a上的idx</li></ul></li></ul></li><li><p>point-wise feed-forward network</p><ul><li>fc-ReLU-fc</li><li>dim_fc=2048</li><li>dim_in &amp; dim_out = 512</li></ul></li></ul></li><li><p>运行过程</p><ul><li><p>encoder是可以并行计算的</p><ul><li>输入是sequence embedding和positional embedding：$A\in R^{d*N}$</li><li>经过repeated blocks</li><li>输出是另外一个sequence：$B\in R^{d*N}$</li><li>self-attention：Q、K、V是一个东西</li><li>encoder的本质就是在解析自注意力：</li><li>并行的全局两两比较，一步到位<ul><li>RNN要by step</li></ul></li><li>CNN要stack layers</li></ul></li></ul></li></ul></li></ol><ul><li><p>decoder是在训练阶段是可以并行的，在inference阶段by step</p><ul><li><p>输入是encoder的输出和上一个time-step decoder的输出embedding</p></li><li><p>输出是当前time-step对应position的输出词的概率</p></li><li><p>第一个attention layer是out embedding的self-attention：要实现像RNN一样依次解码出来，每个time step要用到上一个位置的输出作为输入——masking</p><ul><li>given输入sequence是\ I have a cat，5个元素</li><li>那么mask就是$R^{5*5}$的下三角矩阵</li><li><p>输入embedding经过transformation变成Q、K、V三个矩阵</p></li><li><p>仍旧是$A=K^TQ$计算attention</p></li><li><p>这里有一些attention是非法的：位置靠前的query只能用到比他位置更靠前的query，因此要乘上mask矩阵：$A=M A$</p></li><li><p>softmax：$A=softmax(A)$</p></li><li><p>scale：$B = VA$</p></li><li><p>concat &amp; projection</p><p><img src="/2021/01/18/transformers/mask.png" width="60%;"></p><ul><li>第二个attention layer是in &amp; out sequence的注意力，其key和value来自encoder，query来自上一个decoder block的输出</li></ul></li></ul><p><img src="/2021/01/18/transformers/flow.jpeg" width="45%;"></p></li></ul></li></ul><ol><li><p>why self-attention</p><ul><li>衡量维度<ul><li>total computational complexity per layer</li><li>amount of computation that can be parallelized</li><li>path-length between long-range dependencies</li></ul></li><li>given input sequence with length N &amp; dim $d_{in}$，output sequence with dim $d_{out}$<ul><li>RNN need N sequencial operations of $W\in R^{d_{in} * d_{out}}$</li><li>CNN need N/k stacking layers of $d_{in}<em>d_{out}$ sequence operations of $W\in R^{k</em>k}$，generally是RNN的k倍</li></ul></li></ul></li><li><p>training</p><ul><li>optimizer：$Adam(lr, \beta_1=0.9, \beta_2=0.98, \epsilon=10^{-9})$</li><li>lrschedule：warmup by 4000 steps，then decay</li><li><p>dropout</p><ul><li>residual dropout：就是stochastic depth</li><li>dropout to the sum of embeddings &amp; PE for both encoder and decoder</li><li>drop_rate = 0.1</li></ul></li><li><p>label smoothing：smooth_factor = 0.1</p></li></ul></li><li><p>实验</p><ul><li>A：vary the number of attention heads，发现多了少了都hurts</li><li>B：reduce the dim of attention key，发现hurts</li><li>C &amp; D：大模型+dropout helps</li><li>E：learnable &amp; sincos PE：nearly identical</li><li><p>最后是big model的参数</p><p><img src="/2021/01/18/transformers/transformer_arch.png" width="65%;"></p></li></ul></li></ol><h2 id="BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding"><a href="#BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding" class="headerlink" title="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"></a>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</h2><ol><li><p>动机</p><ul><li>BERT：Bidirectional Encoder Representations from Transformers<ul><li>Bidirectional</li><li>Encoder</li><li>Representations</li><li>Transformers</li></ul></li><li>workflow<ul><li>pretrain bidirectional representations from unlabeled text</li><li>tune with one additional output layer to obtain the model</li></ul></li><li>SOTA<ul><li>GLUE score 80.5%</li></ul></li></ul></li><li><p>论点</p><ul><li>pretraining is effective in NLP tasks<ul><li>feature-based method：use task-specfic architectures，仅使用pretrained model的特征</li><li>fine-tuining method：直接fine-tune预训练模型</li><li>两种方法在预训练阶段训练目标一致：use unidirectional language models to learn general language representations</li><li>reduce the need for many heavily-engineered task- specific architectures</li></ul></li><li>current methods’ limitations<ul><li>unidirectional：<ul><li>limit the choice of architectures</li><li>事实上token的上下文都很重要，不能只看上文</li></ul></li><li>简单的concat两个independent的L2R和R2L模型（biRNN）<ul><li>independent</li><li>shallow concat</li></ul></li></ul></li><li>BERT<ul><li>masked language model：在一个sequence中预测被遮挡的词</li><li>next sentence prediction：trains text-pair representations</li></ul></li></ul></li><li><p>方法</p><ul><li><p>two steps</p><ul><li>pre-training<ul><li>unlabeled data</li><li>different pretraining tasks</li></ul></li><li>fine-tuning<ul><li>labeled data of the downstream tasks</li><li>fine-tune all the params</li></ul></li><li><p>两个阶段的模型，只有输出层不同</p><ul><li>例如问答模型</li><li>pretraining阶段，输入是两个sentence，输入的起始有一个CLS symbol，两个句子的分隔有一个SEP symbol</li><li>fine-tuning阶段，输入分别是问和答，【输出是啥？】</li></ul><p><img src="/2021/01/18/transformers/bert.png" width="65%;"></p></li></ul></li><li><p>architecture</p><ul><li><p>multi-layer bidirectional Transformer encoder</p><ul><li>number of transfomer blocks L</li><li>hidden size H</li><li>number of self-attention heads A</li><li>FFN dim 4H</li></ul></li><li><p>Bert base：L=12，H=768，A=12</p></li><li><p>Bert large：L=24，H=1024，A=16</p></li><li><p>input/output representations</p><ul><li>a single sentence / two packed up sentence：<ul><li>拼接的sentence用特殊token SEP衔接</li><li>segment embedding：同时add a learned embedding to every token indicating who it belongs</li></ul></li><li>use WordPiece embeddings with 30000 token vocabulary</li><li>输入sequence的第一个token永远是一个特殊符号CLS，它对应的final state输出作为sentence整体的representation，用于分类任务</li><li><p>overall网络的input representation是通过将token embeddings拼接上上特殊符号，加上SE和PE得到</p><p><img src="/2021/01/18/transformers/bert input.png" width="75%;"></p></li></ul></li></ul></li><li><p>pre-training</p><ul><li>two unsupervised tasks<ul><li>Masked LM (MLM)<ul><li>mask some percentage of the input tokens at random：15%<ul><li>80%的概率用MASK token替换</li><li>10%的概率用random token替换</li><li>10%的概率unchanged</li></ul></li><li>then predict those masked tokens</li><li>the final hidden states corresponding to the masked tokens are fed into a softmax</li><li>相比较于传统的left2right/right2left/concat模型<ul><li>既有前文又有后文</li><li>只预测masked token，而不是全句预测</li></ul></li></ul></li><li>Next Sentence Prediction (NSP)<ul><li>对于relationship between sentences：<ul><li>例如question&amp;answer，句子推断</li><li>not direatly captured by language modeling，模型直观学习的是token relationship</li></ul></li><li>binarized next sentence prediction task<ul><li>选取sentence A&amp;B：<ul><li>50%的概率是真的上下文（IsNext）</li><li>50%的概率是random（NotNext）</li></ul></li><li>构成了一个二分类问题：仍旧用CLS token对应的hidden state C来预测</li></ul></li></ul></li></ul></li></ul></li><li><p>fine-tuning</p><ul><li>BERT兼容many downstream tasks：single text or text pairs</li><li>直接组好输入，end-to-end fine-tuning就行</li><li>输出还是用CLS token对应的hidden state C来预测，接分类头</li></ul></li></ul></li></ol><h2 id="A-Survey-on-Visual-Transformer"><a href="#A-Survey-on-Visual-Transformer" class="headerlink" title="A Survey on Visual Transformer"></a>A Survey on Visual Transformer</h2><ol><li><p>动机</p><ul><li>provide a comprehensive overview of the recent advances in visual transformers </li><li>discuss the potential directions for further improvement</li><li><p>develop timeline</p><p><img src="/2021/01/18/transformers/timeline.png" width="75%;"></p></li></ul></li><li><p>按照应用场景分类</p><ul><li>backbone：分类</li><li>high/mid-level vision：通常是语义相关的，检测/分割/姿态估计</li><li>low-level vision：对图像本身进行操作，超分/图像生成，目前应用较少</li><li><p>video processing</p><p><img src="/2021/01/18/transformers/category.png" width="85%;"></p></li></ul></li><li><p>revisiting transformer</p><ul><li><p>key-concepts：sentence、embedding、positional encoding、encoder、decoder、self-attention layer、encoder-decoder attention layer、multi-head attention、feed-forward neural network</p><p><img src="/2021/01/18/transformers/revisit transformer.png" width="45%;"></p></li><li><p>self-attention layer</p><ul><li>input vector is transformed into 3 vectors<ul><li>input vector is embedding+PE(pos,i)：pos是word在sequence中的位置，i是PE-element在embedding vec中的位置</li><li>query vec q</li><li>key vec k</li><li>value vec v</li><li>$d_q = d_k = d_v = d_{model} = 512$</li></ul></li><li>then calculate：$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$</li><li>encoder-decoder attention layer<ul><li>K和V是从encoder中拿到</li><li>Q是从前一层拿到</li><li>计算是相似的</li></ul></li></ul></li><li>multi-head attention<ul><li>一个attention是一个softmax，对应了一对强相关，同时抑制了其他word的相关性</li><li>考虑一个词往往与几个词强相关，这就需要多个attention</li><li>multi-head：different QKV matrices are used for different heads</li><li>given a input vector，the number of heads h<ul><li>先产生h个<q,k,v> pairs</q,k,v></li><li>$d_q=d_k=d_v=d_{model}/h=64$</li><li>这h个pair，分别计算attention vector，得到h个[b,d]的context vector</li><li>concat along-d-axis and linear projection to final [b,d] vector</li></ul></li></ul></li><li>residual &amp; layer-norm：layer-norm在residual-add以后</li><li>feed-forward network<ul><li>fc-GeLU-fc</li><li>$d_h=2048$</li></ul></li><li>final-layer in decoder<ul><li>dense+softmax</li><li>$d_{words}=$ number of words in the vocabulary </li></ul></li><li>when applied in CV tasks<ul><li>most transformers adopt the original transformer’s encoder module</li><li>used as a feature selector</li><li>相比较于CNN，能够capture long-distance characteristics，derive global information</li><li>相比较于RNN，能够并行计算</li></ul></li><li>计算量<ul><li>首先是三个线性层：线性时间复杂度O(n)，计算量与$d_{model}$成正比</li><li>然后是self-attention层：QKV矩阵乘法运算，平方时间复杂度O(n^2)</li><li>multi-head的话，还有一个线性层：平方时间复杂度O(n^2)</li></ul></li></ul></li><li><p>revisiting transformers for NLP</p><ul><li>最早期的RNN + attention：rnn的sequential本质影响了长距离/并行化/大模型</li><li><p>transformer的solely attention结构：解决以上问题，促进了large pre-trained models (PTMs) for NLP</p></li><li><p>BERT and its variants </p><ul><li>are a series of PTMs built on the multi-layer transformer encoder architecture </li><li>pre-trained<ul><li>Masked language modeling </li><li>Next sentence prediction </li></ul></li><li>fine-tuned<ul><li>add an output layer</li></ul></li></ul></li><li>Generative Pre-trained Transformer models (GPT)<ul><li>are another type of PTMs based on the transformer decoder architecture</li><li>masked self-attention mechanisms </li><li>pre-trained<ul><li>与BERT最大的不同是有向性</li></ul></li></ul></li></ul></li><li><p>visual transformer</p><ul><li><p>【category1】: backbone for image classification</p><ul><li>transformer的输入是tokens，在NLP里是embedding形式的分词序列，在CV里就是representing a certain semantic concept的visual token<ul><li>visual token可以来自CNN的feature</li><li>也可以直接来自image的小patch</li></ul></li><li><p>purely use transformer来做image classification任务的模型有iGPT、ViT、DeiT</p></li><li><p>iGPT</p><ul><li>pretraining stage + finetuning stage</li><li>pre-training stage<ul><li>self-supervised：自监督，所以结果较差</li><li>given an unlabeled dataset</li><li>train the model by minimizing the -log(density)，感觉是在force光栅排序正确</li></ul></li><li>fine-tuning stage<ul><li>average pool + fc + softmax</li><li>jointly train with L_gen &amp; L_CE</li></ul></li></ul></li><li>ViT<ul><li>pre-trained on large datasets<ul><li>standard transformer’s encoder + MLP head</li><li>treats all patches equally</li><li>有一个类似BERT class token的东西<ul><li>从训练的角度，gather knowledge of the entire class</li><li>inference的时候，只拿了这第一个logit用来做预测</li></ul></li></ul></li><li>fine-tuning<ul><li>换一个zero-initialized的MLP head</li><li>use higher resolution &amp; 插值pe</li></ul></li></ul></li><li>DeiT<ul><li>Data-efficient image transformer </li><li>better performance with <ul><li>a more cautious training strategy </li><li>and a token-based distillation</li></ul></li></ul></li></ul></li><li><p>【category2】: High/Mid-level Vision </p></li><li><p>【category3】: Low-level Vision </p></li><li><p>【category4】: Video Processing </p></li><li><p>efficient transformer：瘦身&amp;加速</p><ul><li>Pruning and Decomposition </li><li>Knowledge Distillation </li><li>Quantization </li><li><p>Compact Architecture Design </p><p><img src="/2021/01/18/transformers/efficient.png" width="45%;"></p></li></ul></li></ul></li></ol><h2 id="ViT-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE"><a href="#ViT-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE" class="headerlink" title="ViT: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"></a>ViT: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</h2><ol><li><p>动机</p><ul><li>attention in vision<ul><li>either in conjunction with CNN</li><li>or replace certain part of a CNN</li><li>overall都还是CNN-based</li></ul></li><li>use a pure transformer to sequence of image patches</li><li>verified on image classification tasks in supervised fashion</li></ul></li><li><p>论点</p><ul><li>transformer lack some inductive biases inherent to CNNs，所以在insufficient data上not generalize well</li><li>however large scale training trumps inductive bias，大数据集上ViT更好</li><li>naive application of self-attention<ul><li>建立pixel之间的两两关联：计算量太大了</li><li>需要approximation：local/改变size</li></ul></li><li>we use transformer<ul><li>wih global self-attention</li><li>to full-sized images</li></ul></li></ul></li><li><p>方法</p><p> <img src="/2021/01/18/transformers/ViT.png" width="65%;"></p><ul><li><p>input 1D-embedding sequence</p><ul><li>将image $x\in R^{HWC}$ 展开成patches $\{x_p \in R^{P^2C}\}$</li><li>thus sequence length $N=HW/P^2$</li><li>patch embedding：<ul><li>use a trainable linear projection</li><li>fixed dimension size through-all</li></ul></li><li>position embedding：<ul><li>add to patch embedding</li><li>standard learnable 1D position embedding</li></ul></li><li>prepended embedding：<ul><li>前置的learnable embedding $x_{class}$</li><li>similar to BERT’s class token</li></ul></li><li>以上三个embedding组合起来，作为输入sequence</li></ul></li><li><p>transformer encoder</p><ul><li>follow the original Transformer </li><li>交替的MSA和MLP</li><li>layer norm LN</li><li>residual</li><li><p>GELU</p><p><img src="/2021/01/18/transformers/ViT formulation.png" width="70%;"></p></li></ul></li><li><p>hybrid architecture</p><ul><li>input sequence也可以来源于CNN的feature maps</li><li>patch size可以是1x1</li></ul></li><li><p>classification head</p><ul><li>attached to $z_L^0$：是class token用来做预测</li><li>pre-training的时候是MLP</li><li>fine-tuning的时候换一个zero-initialized的single linear layer</li></ul></li><li><p>workflow</p><ul><li>typically先pre-train on large datasets</li><li>再fine-tune to downstream tasks</li><li>fine-tune的时候替换一个zero-initialized的新线性分类头</li><li>when feeding images with higher resolution<ul><li>keep the patch size</li><li>results in larger sequence length</li><li>这时候pre-trained PE就no longer meaningful了</li><li>we therefore perform 2D interpolation基于它在原图上的位置</li></ul></li></ul></li><li><p>training details</p><ul><li>Adam：$\beta_1=0.9，\beta_2=0.999$</li><li>batch size 4096</li><li>high weight decay 0.1</li><li>linear lr warmup &amp; decay</li></ul></li><li><p>fine-tuning details</p><ul><li>SGDM</li><li>cosine LR</li><li>no weight decay</li><li>【？？？？】average 0.9999</li></ul></li></ul></li></ol><h2 id="Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows"><a href="#Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows" class="headerlink" title="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"></a>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</h2><ol><li><p>动机</p><ul><li>use Transformer as visual tasks’ backbone</li><li>challenges of Transformer in vision domain<ul><li>large variations of scales of the visual entities</li><li>high resolution of pixels</li></ul></li><li>we propose hierarchical Transformer<ul><li>shifted windows</li><li>self-attention in local windows</li><li>cross-window connection</li></ul></li><li>verified on<ul><li>classification：ImageNet top1 acc 86.4</li><li>detection：COCO box-MAP 58.7</li><li>segmentation：ADE20K</li></ul></li></ul></li><li><p>论点</p><ul><li>when transfer Transformer’s high performance in NLP domain to CV domain<ul><li>differences between the two modalities <ul><li>scale：NLP里面，word tokens serves as the basic element，但是CV里面，patch的形态大小都是可变的，previous methods里面，都是统一设定固定大小的patch token</li><li>resolution：主要问题就是self-attention的计算复杂度，是image size的平方</li></ul></li><li>we propose Swin Transformer<ul><li>hierarchial feature maps</li><li>linear computatoinal complexity to image size</li></ul></li></ul></li><li>hierarchical<ul><li>start from small patches</li><li>merge in deeper layers</li><li>所以对不同尺度的特征patch进行了融合</li></ul></li><li><p>linear complexity</p><ul><li>compute self-attention locally in each window</li><li>每个window的number of patches是设定好的，window数是与image size成正比的</li><li>所以是线性</li></ul><p><img src="/2021/01/18/transformers/hierarchical.png" width="45%;"></p></li><li><p>shifted window approach</p><ul><li>跨层的window shift，建立起相邻window间的桥梁</li><li><p>【QUESTION】all query patches within a window share the same key set</p><p><img src="/2021/01/18/transformers/shifted window.png" width="45%;"></p></li></ul></li><li><p>previous attemptations of Transformer</p><ul><li>self-attention based backbone architectures<ul><li>将部分/全部conv layers替换成self-attention</li><li>模型主体架构还是ResNet</li><li>slightly better acc</li><li>larger latency caused by self-att</li></ul></li><li>self-attention complement CNNs<ul><li>作为additional block，给到backbone/head，提供长距离信息</li><li>有些检测/分割网络也开始用了transformer的encoder-decoder结构</li></ul></li><li>transformer-based vision backbones<ul><li>主要就是ViT及其衍生品</li><li>ViT requires large-scale training sets</li><li>DeiT introduces training strategies</li><li>但是还存在high resolution计算量的问题</li></ul></li></ul></li></ul></li><li><p>方法</p><ul><li><p>overview</p><p>  <img src="/2021/01/18/transformers/Swin.png" width="75%;"></p><ul><li>Swin-T：tiny version</li><li>第一步是patch partition：<ul><li>将RGB图切成non-overlapping patches</li><li>patches：token，basic element</li><li>feature dimention：with patch size 4x4，dim=4x4x3=48</li></ul></li><li>然后是linear embedding layer<ul><li>将raw feature re-projection到指定维度</li><li>指定维度C</li></ul></li><li>接下来是Swin Transformer blocks<ul><li>the number of tokens maintain</li></ul></li><li>patch merging layers负责reduce the number of tokens<ul><li>第一个patch merging layer concat 所有2x2的neighbor patches：4C-dim vec each</li><li>然后用了一个线性层re-projection</li><li>number of tokens（resolution）：（H/4*W/4）/4 = （H/8*W/8），跟常规的CNN一样变化的</li><li>token dims：2C</li><li>后面接上一个Transformer blocks</li><li>合起来叫stage2（stage3、stage4）</li></ul></li></ul></li><li><p>Swin Transformer blocks</p><ul><li><p>跟原始的Transformer block比，就是把原始的MSA替换成了window-based的MSA</p></li><li><p>原始的attention：global computation leads to quadratic complexity</p></li><li><p>window-based attention：</p><ul><li>attention的计算只发生在每个window内部</li><li>non-overlapping partition</li><li>很显然lacks connections across windows </li></ul></li><li><p>shifted window partitioning in successive blocks</p><ul><li><p>两个attention block</p></li><li><p>第一个用常规的window partitioning strategy：从左上角开始，take M=4，window size 4x4（一个window里面包含4x4个patch）</p></li><li><p>第二层的window，基于前一层，各平移M/2</p></li><li><p>introduce connections between neighbor non-overlapping windows in the previous layer</p></li><li><p>efficient computation</p><ul><li><p>shifted window会导致window尺寸不一致，不利于并行计算</p><p><img src="/2021/01/18/transformers/efficient-SW.png" width="55%;"></p></li></ul></li></ul></li><li><p>relative position bias</p><ul><li>我们在MxM的window内部计算local attention：也就是input sequence的time-step是$M^2$</li><li>Q、K、V $\in R ^ {M^2 d}$</li><li>$Attention(Q,K,V)=Softmax(QK^T/\sqrt{d}+B)V$</li><li>这个B作为local的position bias，在二维上，在每个轴上的变化范围[-M+1,M-1]</li><li>we parameterized a smaller-sized bias matrix $\hat B\in R ^{(2M-1)*(2M-1)}$</li><li>values in $B \in R ^ {M^2*M^2}$ are taken from $\hat B$</li><li>the learnt relative position bias可以用来initialize fine-tuned model</li></ul></li></ul></li><li><p>Architecture variants</p><ul><li><p>base model：Swin-B，参数量对标ViT-B</p></li><li><p>Swin-T：0.25x，对标ResNet-50 (DeiT-S) </p></li><li><p>Swin-S：0.5x，对标ResNet-101</p></li><li><p>Swin-L：2x</p></li><li><p>window size：M=7</p></li><li><p>query dim：d=32，（每个stage的input sequence dim逐渐x2，heads num逐渐x2）</p></li><li><p>MLP：expansion ratio=4</p></li><li><p>channel number C：第一个stage的embdding dim，（后续逐渐x2）</p></li><li><p>hypers：</p><p>  <img src="/2021/01/18/transformers/swins.png" width="40%;"></p></li><li><p>acc</p><p>  <img src="/2021/01/18/transformers/swinsacc.png" width="45%;"></p></li></ul></li></ul></li></ol><h2 id="DETR-End-to-End-Object-Detection-with-Transformers"><a href="#DETR-End-to-End-Object-Detection-with-Transformers" class="headerlink" title="DETR: End-to-End Object Detection with Transformers"></a>DETR: End-to-End Object Detection with Transformers</h2><ol><li><p>动机</p><ul><li>new task formulation：a direct set prediction problem</li><li>main gradients<ul><li>a set-based global loss</li><li>a transformer en-de architecture</li><li>remove the hand-designed componets like nms &amp; anchor</li></ul></li><li>acc &amp; run-time on par with Faster R-CNN on COCO<ul><li>significantly better performance on large objects</li><li>lower performances on small objects</li></ul></li></ul></li><li><p>论点</p><ul><li><p>modern detectors run object detection in an indirect way</p><ul><li>基于格子/anchor/proposals进行回归和分类</li><li>算法性能受制于nms机制、anchor设计、target-anchor的匹配机制</li></ul></li><li><p>end-to-end approach</p><p>  <img src="/2021/01/18/transformers/DETR.png" width="65%;"></p><ul><li>transformer的self-attention机制，explicitly model all pairwise interactions between elements：内含了去重（nms）的能力</li><li>bipartite matching：set loss function，将预测和gt的box一一匹配，run in parallel</li><li>DETR does not require any customized layers, thus can be reproduced easily</li><li>expand to segmentation task：a simple segmentation head trained on top of a pre-trained DETR</li></ul></li><li><p>set prediction：to predict a set of bounding boxes and the categories for each</p><ul><li>basic：multilabel classification</li><li>detection task has near-duplicates issues</li><li>set prediction是postprocessing-free的，它的global inference schemes能够avoid redundancy</li><li>usual loss：bipartite match</li></ul></li><li><p>object detection</p><ul><li>set-based loss<ul><li>modern detectors use non-unique assignment rules together with NMS</li><li>bipartite matching是target和pred一一对应</li></ul></li></ul></li></ul></li><li><p>方法</p><ul><li><p>overall</p><p>  <img src="/2021/01/18/transformers/DETR pipeline.png" width="65%;"></p><ul><li>three main components<ul><li>a CNN backbone</li><li>an encoder-decoder transformer</li><li>a simple FFN</li></ul></li></ul></li><li><p>backbone</p><ul><li>conventional r50</li><li>input：$[H_0, W_0, 3]$</li><li>output：$[H,W,C], H=\frac{H_0}{32}, W=\frac{W_0}{32}, C=2048$</li></ul></li><li><p>transformer encoder</p><ul><li>reduce channel dim to $d$：1x1 conv，$d=512$</li><li>collapse the spatial dimensions：feature sequence [d, HW]，每个spatial pixel作为一个feature</li><li>fixed positional encodings：<ul><li>added to the input of each attention layer</li><li>【QUESTION】加在K和Q上还是embedding上？</li></ul></li></ul></li><li><p>transformer decoder</p><ul><li>输入N个dim=d的embedding<ul><li>叫object queries：表示我们预测固定值N个目标</li><li>因为decoder也是permutation-invariant的（因为all shared），所以要输入N个不一样的embedding</li><li>learnt positional encodings</li><li>add them to the input of each attention layer</li></ul></li><li>decodes the N objects in parallel</li></ul></li><li><p>prediction FFN</p><ul><li>3 layer，ReLU，</li><li>box prediction：normalized center coords &amp; height &amp; width</li><li>class prediction：<ul><li>an additional class label $\varnothing$ 表示no object</li></ul></li></ul></li><li><p>auxiliary losses</p><ul><li>each decoder layer后面都接一个FFN prediction和Hungarian loss</li><li>shared FFN</li><li>an additional shared LN to norm the inputs of FFN</li><li>three components of the loss<ul><li>class loss：CE loss</li><li>box loss<ul><li>GIOU loss</li><li>L1 loss</li></ul></li></ul></li></ul></li><li><p>technical details</p><ul><li>AdamW：<ul><li>initial transformer lr=10e-4</li><li>initial backbone lr=10e-5</li><li>weight decay=10e-4</li></ul></li><li>Xavier init</li><li>imagenet-pretrained resnet weights with frozen batchnorm layers：r50 &amp; r101，DETR &amp; DETR-R101</li><li>a variant：<ul><li>increase feature resolution version</li><li>remove stage5’s stride and add a dilation</li><li>DETR-DC5 &amp; DETR-DC5-R101</li><li>improve performance for small objects</li><li>overall 2x computation increase</li></ul></li><li>augmentation<ul><li>resize input</li><li>random crop：with 0.5 prob then resize</li></ul></li><li>transformer default dropout 0.1</li><li>lr schedule<ul><li>300 epochs</li><li>drop by factor 10 after 200 epochs</li></ul></li><li>4 images per GPU，total batch 64</li></ul></li><li><p>for segmentation task：全景分割</p><ul><li>给decoder outputs加mask head</li><li>compute multi-head attention among<ul><li>decoder box predictions</li><li>encoder outputs</li></ul></li><li>generate M attention heatmaps per object</li><li>add a FPN styled CNN to recover resolution</li><li><p>pixel-wise argmax</p><p><img src="/2021/01/18/transformers/DETRseg.png" width="75%;"></p></li></ul></li></ul></li></ol><h2 id="UNETR-Transformers-for-3D-Medical-Image-Segmentation"><a href="#UNETR-Transformers-for-3D-Medical-Image-Segmentation" class="headerlink" title="UNETR: Transformers for 3D Medical Image Segmentation"></a>UNETR: Transformers for 3D Medical Image Segmentation</h2><ol><li><p>动机</p><ul><li>unet结构用于医学分割<ul><li>encoder learns global context</li><li>decoder utilize the representations to predict the semanic ouputs</li><li>the locality of CNN limits long-range spatial dependency</li></ul></li><li>our method<ul><li>use a pure transformer as the encoder</li><li>learn sequence representations of the input volume</li><li>global</li><li>multi-scale</li><li>encoder directly connects to decoder with skip connections</li></ul></li></ul></li><li><p>论点</p><ul><li>unet结构<ul><li>encoder用来提取全图特征</li><li>decoder用来recover</li><li>skip connections用来补充spatial information that is lost during downsampling</li><li>localized receptive fields：<ul><li>disadvantage in capturing multi-scale contextual information</li><li>如不同尺寸的脑肿瘤</li><li>缓和手段：atrous convs，still limited</li></ul></li></ul></li><li>transformer<ul><li>self-attention mechanism in NLP<ul><li>highlight the important features of word sequences</li><li>learn its long-range dependencies</li></ul></li><li>in ViT<ul><li>an image is represented as a patch embedding sequence</li></ul></li></ul></li><li>our method<ul><li>formulation<ul><li>1D seq2seq problem</li><li>use embedded patches</li></ul></li><li>the first completely transformer-based encoder</li></ul></li><li>other unet- transformer methods<ul><li>2D (ours 3D)</li><li>employ only in the bottleneck (ours pure transformer)</li><li>CNN &amp; transformer in separate streams and fuse</li></ul></li></ul></li><li><p>方法</p><ul><li><p>overview</p><p>  <img src="/2021/01/18/transformers/UNETR.png" width="75%;"></p></li><li><p>transformer encoder</p><ul><li>input：1D sequence of input embeddings</li><li>given 3D volume $x \in R^{HWDC}$</li><li>divide into flattened uniform non-overlapping patches $x\in R^{LCN^3}$<ul><li>$L=HWD/N^3$：the sequence length</li><li>$N^3$：patch dimension</li></ul></li><li>linear projection to K-dim $E \in R^{LCK}$：remain constant through transformer</li><li>1D learnable positional embedding $E_{pos} \in R^LD$</li><li>12 self-att blocks：MSA + MLP</li></ul></li><li>decoder &amp;skip connections<ul><li>选取encoder第{3,6,9,12}个block的输出</li><li>reshape back to 3D volume $[\frac{H}{N},\frac{W}{N},\frac{D}{N},C]$</li><li>consecutive 3x3x3 conv+BN+ReLU</li><li>bottleneck<ul><li>deconv by 2 to increase resolution</li><li>then concat with the previous resized feature</li><li>then jointly consecutive conv</li><li>then upsample with deconv…</li></ul></li><li>concat到原图resolution以后，consecutive conv以后，再1x1x1 conv+softmax</li></ul></li><li>loss<ul><li>dice loss<ul><li>dice：for each class channel，计算dice，然后求类平均</li><li>1-dice</li></ul></li><li>ce loss<ul><li>for each pixel，求bce，然后求所有pixel的平均</li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> transformer, self-attention </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>pre-training &amp; self-training</title>
      <link href="/2021/01/17/pre-training-self-training/"/>
      <url>/2021/01/17/pre-training-self-training/</url>
      <content type="html"><![CDATA[<p>[pre-training] Rethinking ImageNet Pre-training，He Kaiming，imageNet pre-training并没有真正helps acc，只是speedup，random initialization能够reach no worse的结果，前提是数据充足增强够猛，对小门小户还是没啥用，我们希望speedup</p><p>[pre-training &amp; self-training] Rethinking Pre-training and Self-training，Google Brain，提出task-specific的pseudo label要比pre-training中搞出来的各种标签要好，前提还是堆数据，对小门小户没啥用，low-data下还是pre-train保平安</p><p>总体上都是针对跨任务下，imageNet pre-training意义的探讨，</p><ul><li>分类问题还是可以继续pretrained</li><li>kaiming这个只是fact，没有现实指导意义</li><li>google这个one step further，提出了self-training在现实条件中可以一试</li></ul><h2 id="Rethinking-Pre-training-and-Self-training"><a href="#Rethinking-Pre-training-and-Self-training" class="headerlink" title="Rethinking Pre-training and Self-training"></a>Rethinking Pre-training and Self-training</h2><ol><li><p>动机</p><ul><li>given fact：ImageNet pre-training has limited impact on COCO object detection </li><li>investigate self-training to utilize the additional data</li></ul></li><li><p>论点</p><ul><li>common practice pre-training<ul><li>supervised pre-training<ul><li>首先要求数据有标签</li><li>pre-train the backbone on ImageNet as a classification task</li></ul></li><li>弱监督学习<ul><li>with pseudo/noisy label</li><li>kaiming：Exploring the limits of weakly supervised pretraining</li></ul></li><li>self-supervised pre-training<ul><li>无标签的海量数据</li><li>构造学习目标：autoencoder，contrastive，…</li><li><a href="https://zhuanlan.zhihu.com/p/108906502" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/108906502</a></li></ul></li></ul></li><li>self-training paradigm on COCO<ul><li>train an object detection model on COCO </li><li>generate pseudo labels on ImageNet</li><li>both labeled data are combined to train a new model</li><li>基本基于noisy student的方法</li></ul></li><li>observations<ul><li>with stronger data augmentation, pre-training hurts the accuracy, but helps in self-training</li><li>both supervised and self-supervised pre-training methods fails</li><li>the benefit of pre-training does not cancel out the gain by self-training</li><li>flexible about unlabeled data sources, model architectures and computer vision tasks</li></ul></li></ul></li><li><p>方法</p><ul><li>data augmentation<ul><li>vary the strength of data augmentation as 4 levels</li></ul></li><li><p>pre-training</p><ul><li>efficientNet-B7</li><li>AutoAugment weights &amp; noisy student weights</li></ul><p><img src="/2021/01/17/pre-training-self-training/notations.png" width="70%;"></p></li><li><p>self-training</p><ul><li>noisy student scheme</li><li>实验发现self-training with this standard loss function can be unstable </li><li>implement a loss normalization technique </li></ul></li><li>experimental settings<ul><li>object detection<ul><li>COCO dataset for supervised learning</li><li>unlabeled ImageNet and OpenImages dataset for self-training：score thresh 0.5 to generate pesudo labels</li><li>retinaNet &amp; spineNet</li><li>batch：half supervised half pesudo</li></ul></li><li>semantic segmentation<ul><li>PASCAL VOC 2012 for supervised learning</li><li>augmented PASCAL &amp; COCO &amp; ImageNet for self-training：score thresh 0.5 to generate pesudo masks &amp; multi-scale</li><li>NAS-FPN</li></ul></li></ul></li></ul></li><li><p>实验</p><ul><li><p>pre-training</p><ul><li>Pre-training hurts performance when stronger data augmentation is used：因为会sharpen数据差异？</li><li>More labeled data diminishes the value of pre-training：通常我们的实验数据fraction都比较小的相对imageNet，所以理论上不会harm？</li><li><p>self-supervised pre-training也会一样harm，在augment加强的时候</p><p><img src="/2021/01/17/pre-training-self-training/pre-training.png" width="60%;"></p></li></ul></li><li><p>self-training</p><ul><li><p>Self-training helps in high data/strong augmentation regimes, even when pre-training hurts：不同的augment level，self-training对最终结果都有加成</p><p><img src="/2021/01/17/pre-training-self-training/self1.png" width="60%;"></p></li><li><p>Self-training works across dataset sizes and is additive to pre-training：不同的数据量，也都有加成，但是low data regime下enjoys the biggest gain</p><p><img src="/2021/01/17/pre-training-self-training/self2.png" width="60%;"></p></li></ul></li><li><p>discussion</p><ul><li>weak performance of pre-training is that pre-training is not aware of the task of interest and can fail to adapt </li><li>jointly training also helps：address the mismatch between two dataset</li><li>noisy labeling is worse than targeted pseudo labeling</li></ul></li><li><p>总体结论：小样本量的时候，pre-training还是有加成的，再加上self-training进一步提升，样本多的时候就直接self-training</p></li></ul></li></ol><h2 id="Rethinking-ImageNet-Pre-training"><a href="#Rethinking-ImageNet-Pre-training" class="headerlink" title="Rethinking ImageNet Pre-training"></a>Rethinking ImageNet Pre-training</h2><ol><li>动机<ul><li>thinking random initialization &amp; pre-training</li><li>ImageNet pre-training<ul><li>speed up</li><li>but not necessarily improving</li></ul></li><li>random initialization<ul><li>can achieve no worse result</li><li>robust to data size, models, tasks and metrics</li></ul></li><li>rethink current paradigm of ‘pre- training and fine-tuning’</li></ul></li><li>论点<ul><li>no fundamental obstacle preventing us from training from scratch <ul><li>if use normalization techniques appropriately </li><li>if train sufficiently long </li></ul></li><li>pre-training<ul><li>speed up</li><li>when fine-tuning on small dataset new hyper-parameters must be selected to avoid overfitting</li><li>localization-sensitive task benefits limited from pre-training</li><li>aimed at communities that don’t have enough data or computational resources</li></ul></li></ul></li><li>方法<ul><li>normalization<ul><li>form<ul><li>normalized parameter initialization </li><li>normalization layers </li></ul></li><li>BN layers makes training from scratch difficult <ul><li>small batch size degrade the acc of BN</li><li>fine-tuning可以freeze BN</li><li>alternatives<ul><li>GN：对batch size不敏感</li><li>syncBN</li></ul></li></ul></li><li>with appropriately normalized initialization可以train from scratch VGG这种不用BN层的</li></ul></li><li>convergence<ul><li>pre-training model has learned low-level features that do not need to be re-learned during </li><li>random-initial training need more iterations to learn both low-level and semantic features</li></ul></li></ul></li><li>实验<ul><li>investigate maskRCNN<ul><li>替换BN：GN/sync-BN</li><li>learning rate：<ul><li>training longer for the first (large) learning rate is useful</li><li>but training for longer on small learning rates often leads to overfitting </li></ul></li></ul></li><li>10k COCO往上，train from scratch results能够catch up pretraining results，只要训的够久</li><li>1k和3.5k的COCO，converges show no worse，但是在验证集上差一些：strong overfitting due to lack of data</li><li>PASCAL的结果也差一点，因为instance和category都更少，not directly comparable to the same number of COCO images：fewer instances and categories has a similar negative impact as insufficient training data</li></ul></li></ol>]]></content>
      
      
    </entry>
    
    <entry>
      <title>long-tailed</title>
      <link href="/2021/01/11/long-tailed/"/>
      <url>/2021/01/11/long-tailed/</url>
      <content type="html"><![CDATA[<p>[bag of tricks] Bag of Tricks for Long-Tailed Visual Recognition with Deep Convolutional Neural Networks：结论就是两阶段，input mixup + CAM-based DRS + muted mixup fine-tuning组合使用最好</p><p>[balanced-meta softmax] Balanced Meta-Softmax for Long-Tailed Visual Recognition：商汤</p><p>[eql] Equalization Loss for Long-Tailed Object Recognition </p><p>[eql2] Equalization Loss v2:  A New Gradient Balance Approach for Long-tailed Object Detection </p><p>[Class Rectification Loss] Imbalanced Deep Learning by Minority Class Incremental Rectification：提出CRL使得模型能够识别分布稀疏的小类们的边界，以此避免大类主导的影响</p><h2 id="Bag-of-Tricks-for-Long-Tailed-Visual-Recognition-with-Deep-Convolutional-Neural-Networks"><a href="#Bag-of-Tricks-for-Long-Tailed-Visual-Recognition-with-Deep-Convolutional-Neural-Networks" class="headerlink" title="Bag of Tricks for Long-Tailed Visual Recognition with Deep Convolutional Neural Networks"></a>Bag of Tricks for Long-Tailed Visual Recognition with Deep Convolutional Neural Networks</h2><ol><li><p>动机</p><ul><li>to give a detailed experimental guideline of common tricks</li><li>to obtain the effective combinations of these tricks</li><li>propose a novel data augmentation approach</li></ul></li><li><p>论点</p><ul><li>long-tailed datasets <ul><li>poor accuray on the under-presented minority</li><li>long-tailed CIFAR：<ul><li>指数型衰减</li><li>imbalance factor：50/100</li><li>test set unchanged</li></ul></li><li>ImageNet-LT  <ul><li>sampling the origin set follow the pareto distribution</li><li>test set is balanced</li></ul></li><li>iNaturalist<ul><li>extremely imbalanced real world dataset</li><li>fine-grained problem</li></ul></li></ul></li><li>different learning paradigms <ul><li>metric learning </li><li>meta learning </li><li>knowledge transfer  </li><li>suffer from high sensitivity to hyper-parameters</li></ul></li><li>training tricks<ul><li>re-weighting</li><li>re-sample</li><li>mixup</li><li>two-stage training</li><li>different tricks might hurt each other</li><li>propose a novel data augmentation approach based on CAM：generate images with transferred foreground and unchanged background</li></ul></li></ul></li><li><p>方法</p><ul><li><p>start from baseline</p><p> <img src="/2021/01/11/long-tailed/baseline.png" width="70%;"></p></li><li><p>re-weighting</p><ul><li>baseline：CE</li><li>re-weighting methods：<ul><li>cost-sensitive CE：按照样本量线性加权$\frac{n_c}{n_{min}}$</li><li>focal loss：困难样本加权</li><li>class-balanced loss：<ul><li>effective number rather than 样本量$n_c$</li><li>hyperparameter $\beta$ and weighting factor：$\frac{1-\beta}{1-\beta^{n_c}}$</li></ul></li><li>在cifar10上有效，但是cifar100上就不好了<ul><li>directly application in training procedure is not a proper choice</li><li>especially when类别增多，imbalance加剧的时候</li></ul></li></ul></li><li><img src="/2021/01/11/long-tailed/reweighting.png" width="50%;"></li></ul></li><li><p>re-sampling</p><ul><li>re-sampling methods<ul><li>over-sampling：<ul><li>随机复制minority</li><li>might leads to overfitting</li></ul></li><li>under-sampling<ul><li>随机去掉一些majority</li><li>be preferable to over-sampling </li></ul></li><li>有规律地sampling<ul><li>大体都是imbalanced向着lighter imbalanced向着balanced推动</li></ul></li><li>artificial sampling methods<ul><li>create artificial samples </li><li>sample based on gradients and features </li><li>likely to introduce noisy data </li></ul></li></ul></li><li>观察到提升效果不明显</li><li><img src="/2021/01/11/long-tailed/resampling.png" width="50%;"></li></ul></li><li><p>mixup</p><ul><li>input mixup：input mixup can be further improved if we remove the mixup in last several epochs</li><li>manifold mixup：on only one layer </li><li><p>观察到两种mixup功效差不多，后面发现input mixup更好些</p><p><img src="/2021/01/11/long-tailed/mixup.png" width="50%;"></p><ul><li>input mixup去掉再finetuning几个epoch结果又提升，manifold则会变差</li></ul><p><img src="/2021/01/11/long-tailed/mixup1.png" width="50%;"></p></li></ul></li><li><p>two-stage training</p><ul><li>imbalanced training + balanced fine-tuning</li><li>vanilla training schedule on imbalanced data<ul><li>先学特征</li></ul></li><li>fine-tune on balanced subsets<ul><li>再调整recognition accuracy </li><li>deferred re-balancing by re-sampling (DRS)  ：propose CAM-based sampling</li><li>deferred re-balancing by re-weighting (DRW) </li></ul></li><li>proposed CAM-based sampling<ul><li>DRS only replicate or remove</li><li>for each sampled image, apply the trained model &amp; its ground truth label  to generate CAM</li><li>用heatmap的平均值作为阈值来区分前背景</li><li>对前景apply transformations <ul><li>horizontal flipping</li><li>translation</li><li>rotating</li><li>scaling</li></ul></li></ul></li><li>发现fine-tuning时候再resample比直接resample的结果好</li><li>proposed CAM-based sampling好于其他sampling，其中CAM-based balance- sampling最好</li><li><p>ImageTrans balance-sampling只做变换，不用CAM区分前背景，结果不如CAM-based，证明CAM有用</p><p><img src="/2021/01/11/long-tailed/DRS.png" width="50%;"></p></li><li><p>发现fine-tuning时候再reweight比直接reweight的结果好</p></li><li><p>其中CSCE（按照样本量线性加权）最好</p><p><img src="/2021/01/11/long-tailed/DRW.png" width="50%;"></p></li><li><p>整体来看DRS的结果稍微比DRW好一点</p></li></ul></li><li><p>trick combinations</p><ul><li>two-stage的CAM-based DRS略好于DRW，两个同时用不会further improve</li><li>再加上mixup的话，input比manifold好一些</li><li><p>结论就是：input mixup + CAM-based DRS + mute fine-tuning，apply the tricks incrementally</p><p><img src="/2021/01/11/long-tailed/bag.png" width="50%;"></p></li></ul></li></ul></li></ol><h2 id="Balanced-Meta-Softmax-for-Long-Tailed-Visual-Recognition"><a href="#Balanced-Meta-Softmax-for-Long-Tailed-Visual-Recognition" class="headerlink" title="Balanced Meta-Softmax for Long-Tailed Visual Recognition"></a>Balanced Meta-Softmax for Long-Tailed Visual Recognition</h2><ol><li><p>动机</p><ul><li>long-tailed：mismatch between training and testing distributions </li><li>softmax：biased gradient estimation under the long-tailed setup</li><li>propose <ul><li>Balanced Softmax：an elegant unbiased extension of Softmax</li><li>apply a complementary Meta Sampler：optimal sample rate</li></ul></li><li>classification &amp; segmentation</li></ul></li><li><p>论点</p><ul><li>raw baseline：a model that minimizes empirical risk on long-tailed training datasets often underperforms on a class-balanced test set</li><li>most methods use re-sampling or re-weighting<ul><li>to simulate a balanced dataset</li><li>may under-class the majority or have gradient issue</li></ul></li><li>meta-learning<ul><li>optimize the weight per sample</li><li>need a clean and unbiased dataset</li></ul></li><li>decoupled training<ul><li>就是上面一篇论文中的两阶段，第一阶段先学表征，第二阶段调整分布fine-tuning</li><li>not adequate for datasets with extremely high imbalance factor</li></ul></li><li>LDAM<ul><li>Label-Distribution-Aware Margin Loss </li><li>larger generalization error bound for minority</li><li>suit for binary classification</li></ul></li><li>we propose BALMS <ul><li>Balanced Meta-Softmax</li><li>theoretically equivalent with generalization error bound</li><li>for datasets with high imbalance factors should combine Meta Sampler  </li></ul></li></ul></li><li><p>方法</p><ul><li><p>balanced softmax</p><ul><li>biased：从贝叶斯条件概率公式看，standard softmax上默认了均匀采样的p(y)，在长尾分布的时候，就是有偏的</li><li><p>加权：</p><ul><li>加在softmax项里面</li><li>基于样本量线性加权</li></ul><p><img src="/2021/01/11/long-tailed/bsoft.png" width="40%;"></p></li><li><p>数学意义上：we need to focus on minimizing the training loss of the tail classes</p></li></ul></li><li><p>meta sampler</p><ul><li>resample和reweight直接combine可能会worsen performance</li><li>class balance resample可能有over-balance issue</li></ul></li><li><p>combination procedures</p><p>  <img src="/2021/01/11/long-tailed/meta.png" width="70%;"></p><ul><li>对当前分布，先计算balanced-softmax，保存一个梯度更新后的模型</li><li>计算这个临时模型在meta set上的CE，对分布embedding进行梯度更新：评估当前分布咋样，往一定方向矫正</li><li>对真正的模型，用最新的分布，计算balanced-softmax，进行梯度更新：用优化后的分布，引导模型学习</li></ul></li></ul></li><li><p>实验</p><ul><li>CE的结果呈现明显的长尾同分布趋势</li><li>CBS有缓解</li><li>BS更好</li><li>BS+CBS会over sample</li><li><p>BS+meta最好</p><p><img src="/2021/01/11/long-tailed/test.png" width="70%;"></p></li></ul></li></ol><h2 id="Imbalanced-Deep-Learning-by-Minority-Class-Incremental-Rectification"><a href="#Imbalanced-Deep-Learning-by-Minority-Class-Incremental-Rectification" class="headerlink" title="Imbalanced Deep Learning by Minority Class Incremental Rectification"></a>Imbalanced Deep Learning by Minority Class Incremental Rectification</h2><ol><li><p>动机</p><ul><li>significantly imbalanced training data </li><li>propose<ul><li>batch-wise incremental minority class rectification model  </li><li>Class Rectification Loss (CRL) </li></ul></li><li><p>bring benefits to both minority and majority class boundary learning </p><p><img src="/2021/01/11/long-tailed/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/data-aug/CRL.png" width="80%"></p></li></ul></li><li><p>论点</p><ul><li>Most methods produce learning bias towards the majority classes  <ul><li>to eliminate bias<ul><li>lifting the importance of minority classes：over-sampling can easily cause model overfitting，可能造成对小类别的过分关注，而对大类别不够重视，影响模型泛化能力</li><li>cost-sensitive learning：difficult to optimise </li><li>threshold-adjustment technique：given by experts </li></ul></li></ul></li><li>previous methods mainly investigate single-label binary-class with small imbalance ratio</li><li>real data <ul><li>large ratio：power-law distributions </li><li>Subtle appearance discrepancy </li></ul></li><li>hard sample mining<ul><li>hard negatives are more informative than easy negatives as they violate a model class boundary  </li><li>we only consider hard mining on the minority classes for efficiency </li><li>our batch-balancing hard mining strategy：eliminating exhaustive searching  </li></ul></li><li>LMLE <ul><li>唯一的竞品：考虑了data imbalance的细粒度分类</li><li>not end-to-end </li><li>global hard mining </li><li>computationally complex and expensive </li></ul></li></ul></li><li><p>方法</p><ul><li><p>CRL overview</p><ul><li>explicitly imposing structural discrimination of minority classes </li><li>batch-wise</li><li>operate on CE</li><li>forcus on minority class only：the conventional CE loss can already model the majority classes well </li></ul></li><li><p>limitations of CE</p><ul><li>CE treat the individual samples and classes as equally important </li><li>the learned model is suboptimal</li><li>boundaries are biased towards majority classes  </li></ul></li><li><p>profile the class distribution for each class</p><ul><li>hard mining</li><li><p>overview</p><p><img src="/2021/01/11/long-tailed/data-aug/CRL2.png" width="50%"></p></li></ul></li><li><p>minority class hard sample mining</p><ul><li><p>selectively “borrowing” majority class samples from class decision boundary </p></li><li><p>to minority class’s perspective：mining both hard-positive and hard-negative samples </p></li><li><p>define minority class：selected in each mini-batch</p></li><li><p>Incremental refinement：</p><ul><li>eliminates the LMLE’s drawback in assuming that local group structures of all classes can be estimated reliably by offline global clustering </li><li>mini-batch的data distribution和训练集不是完全一致的</li></ul></li><li><p>steps</p><ul><li><p>profile the minority and majority classes per label in each training mini-batch</p><ul><li>for each sample，for each class $j$，for each pred class $k$，we have $h^j=[h_1^j, …, h_k^j, …, h_{n_cls}^j]$</li><li>sort $h_k^j$ in descent order，define the minority classes for each class with $C_{min}^j = \sum_{k\in C_{min}^j}h_k^j \leq \rho * n_{bs}$，with $\rho=0.5$</li></ul></li><li><p>hard mining</p><ul><li><p>hardness</p><ul><li>score based：prediction score，class-level</li><li>feature based：feature distance，instance-level</li></ul></li><li><p>class-level，for class c</p><ul><li>hard-positives：same gt class，but low prediction</li><li>hard-negative：different gt class，with high prediction</li></ul></li><li><p>instance-level，for each sample in class c</p><ul><li>hard-positives：same gt class，large distance with current sample</li><li>hard-negative：different gt class，small distance with current sample</li></ul></li><li><p>top-k mining</p><ul><li>hard-positives：bottom-k scored on c/top-k distance on c</li><li>hard-negative：top-k scored on c/bottom-k distance on c</li></ul></li><li><p>score-based yields superior to distance-based</p><p>  <img src="/2021/01/11/long-tailed/data-aug/hard.png" width="50%"></p></li></ul></li></ul></li></ul></li><li><p>CRL</p><ul><li>final weighted loss：$L = \alpha L_{crl}+(1-\alpha)L_{ce}$，$\alpha=\eta\Omega_{imbalance}$</li><li>class imbalance measure $\Omega$：more weighting  is assigned to more imbalanced labels </li><li>form<ul><li>triplet loss：类内+类间</li><li>contrastive loss：类内</li><li>modelling the distribution relationship of positive and negative pairs：没看懂</li></ul></li></ul></li></ul></li><li><p>总结</p><p> 就是套用现有的metric learning，定义了一个变化的minority class，垃圾。</p><p> 说到底就是大数据——CE，小数据——metric learning。</p></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 长尾分布 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>refineDet</title>
      <link href="/2021/01/08/refineDet/"/>
      <url>/2021/01/08/refineDet/</url>
      <content type="html"><![CDATA[<p>和refineNet没有任何关系</p><h2 id="RefineDet-Single-Shot-Refinement-Neural-Network-for-Object-Detectio"><a href="#RefineDet-Single-Shot-Refinement-Neural-Network-for-Object-Detectio" class="headerlink" title="RefineDet: Single-Shot Refinement Neural Network for Object Detectio"></a>RefineDet: Single-Shot Refinement Neural Network for Object Detectio</h2><ol><li><p>动机</p><ul><li>inherit the merits of both two-stage and one-stage：accuracy and efficiency</li><li>single-shot</li><li>multi-task</li><li>refineDet<ul><li>anchor refinement module (ARM)  </li><li>object detection module (ODM)</li><li>transfer connection block (TCB) </li></ul></li></ul></li><li><p>论点</p><ul><li>three advantages that two-stage superior than one-stage<ul><li>RPN：handle class imbalance</li><li>two step regress：coarse to refine</li><li>two stage feature：RPN任务和regression任务有各自的feature</li></ul></li><li>模拟二阶段检测的RPN，把classifier任务中的大量阴性框先排掉，但不是以两个阶段的形式，而是multi-task并行</li><li>将一阶段检测的objectness和box regression任务解耦，两个任务通过transfer block连接</li><li>ARM<ul><li>remove negative anchors to reduce search space for the classifier </li><li>coarsely adjust the locations and sizes of anchors to provide better initialization for regression</li></ul></li><li>ODM <ul><li>further improve the regression </li><li>predict multi labels</li></ul></li><li><p>TCB </p><ul><li>transfer the features in the ARM to handle the more challenging tasks in the ODM </li></ul><p><img src="/2021/01/08/refineDet/refineDet.png" width="70%;"></p></li></ul></li><li><p>方法</p><ul><li><p>Transfer Connection Block</p><ul><li><p>没什么新的东西，上采样用了deconv，conv-relu，element-wise add</p><p><img src="/2021/01/08/refineDet/TCB.png" width="50%;"></p></li></ul></li><li><p>Two-Step Cascaded Regression </p><ul><li>fisrt step ARM prediction <ul><li>for each cell，for each predefined anchor boxes，predict 4 offsets and 2 scores</li><li>obtain refined anchor boxes </li></ul></li><li>second step ODM prediction<ul><li>with justified feature map，with refined anchor boxes </li><li>generate accurate boxes offset to refined boxes and multi-class scores，c+4</li></ul></li></ul></li><li><p>Negative Anchor Filtering </p><ul><li>reject well-classified negative anchors </li><li>if the negative confidence is larger than 0.99，discard it in training the ODM </li><li>ODM接收所有pred positive和hard negative</li></ul></li><li><p>Training and Inference details</p><ul><li>back：VGG16 &amp; resnet101<ul><li>fc6 &amp; fc7变成两个conv</li><li>different feature scales </li><li>L2 norm</li><li>two extra convolution layers and one extra residual block </li></ul></li><li>4 feature strides<ul><li>each level：1 scale &amp; 3 ratios</li><li>ensures that different scales of anchors have the same tiling density on the image</li></ul></li><li>matching<ul><li>每个GT box match一个score最高的anchor box</li><li>为每个anchor box找到最匹配的iou大于0.5的gt box</li><li>相当于把ignore那部分也作为正样本了</li></ul></li><li>Hard Negative Mining<ul><li>select negative anchor boxes with top loss values </li><li>n &amp; p ratio：3:1</li></ul></li><li>Loss Function<ul><li>ARM loss<ul><li>binary class：只计算正样本？？？</li><li>box：只计算正样本</li></ul></li><li>ODM loss<ul><li>pass the refined anchors with the negative confidence less than the threshold </li><li>multi-class：计算均衡的正负样本</li><li>box：只计算正样本</li></ul></li><li>正样本数为0的时候，loss均为0：纯阴性样本无效？？</li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CPNDet</title>
      <link href="/2021/01/05/CPNDet/"/>
      <url>/2021/01/05/CPNDet/</url>
      <content type="html"><![CDATA[<h2 id="Corner-Proposal-Network-for-Anchor-free-Two-stage-Object-Detection"><a href="#Corner-Proposal-Network-for-Anchor-free-Two-stage-Object-Detection" class="headerlink" title="Corner Proposal Network for Anchor-free, Two-stage Object Detection"></a>Corner Proposal Network for Anchor-free, Two-stage Object Detection</h2><ol><li><p>动机</p><ul><li>anchor-free</li><li>two-stage<ul><li>先找potential corner keypoints</li><li>classify each proposal</li></ul></li><li>corner-based方法：对于objects of various scales有效，在训练中避免产生过多的冗余false-positive proposals，但是在结果上会出现更多的fp</li><li>得到的是competitive results</li></ul></li><li><p>论点</p><ul><li>anchor-based methods对形状奇怪的目标容易漏检</li><li><p>anchor-free methods容易引入假阳caused by mistakely grouping</p><ul><li>thus an individual classifier is strongly required  </li></ul></li><li><p>Corner Proposal Network (CPN) </p><ul><li>use key-point detection in CornerNet</li><li>但是group阶段不再用embedding distance衡量，而是用a binary classifier </li><li>然后是multi-class classifier，operate on the survived objects</li><li>最后soft-NMS</li></ul></li><li></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 目标检测，two-stage，anchor-free </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>refineNet</title>
      <link href="/2021/01/05/refineNet/"/>
      <url>/2021/01/05/refineNet/</url>
      <content type="html"><![CDATA[<h2 id="RefineNet-Multi-Path-Refinement-Networks-for-High-Resolution-Semantic-Segmentation"><a href="#RefineNet-Multi-Path-Refinement-Networks-for-High-Resolution-Semantic-Segmentation" class="headerlink" title="RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation"></a>RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</h2><p>引用量1452，但是没有几篇技术博客？？</p><ol><li><p>动机</p><ul><li>语义分割<ul><li>dense classification on every single pixel</li></ul></li><li>refineNet<ul><li>long-range residual connections</li><li>chained residual pooling</li></ul></li></ul></li><li><p>论点</p><ul><li>pooling/conv stride：<ul><li>losing finer image structure</li><li>deconv is not able to recover the lost info</li></ul></li><li>atrous<ul><li>high reso：large computation</li><li>dilated conv：coarse sub-sampling of feature</li></ul></li><li>FCN<ul><li>fuse features from all levels </li><li>stage-wise rather than end-to-end???存疑</li></ul></li><li><p>this paper</p><ul><li>main idea：effectively exploit middle layer features </li><li>RefineNet<ul><li>fuse all level feature</li><li>residual connections with identity skip</li><li>chained residual pooling to capture background context：看描述感觉像inception downsamp</li><li>end-to-end</li><li>是整个分割网络中的一个component</li></ul></li></ul><p><img src="/2021/01/05/refineNet/dense.png" width="80%;"></p></li></ul></li><li><p>方法</p><p> <img src="/2021/01/05/refineNet/refineNet.png" width="70%;"></p><ul><li>backbone<ul><li>pretrained resnet</li><li>4 blocks：x4 - x32，each block：pool-residual</li><li>connection：每个输出连接一个RefineNet unit</li></ul></li><li>4-cascaded architecture<ul><li>final ouput：<ul><li>high-resolution feature maps</li><li>dense soft-max </li><li>bilinear interpolation to origin resolution</li></ul></li><li>cascade inputs<ul><li>output from backbone block</li><li>ouput from previous refineNet block</li></ul></li></ul></li><li>refineNet block<ul><li>adapt conv：<ul><li>to adapt the dimensionality and refine special task</li><li>BN layers are removed</li><li>channel 512 for R4，channel 256 for the rest</li></ul></li><li>fusion：<ul><li>先用conv to adapt dimension and recale the paths</li><li>然后upsamp</li><li>summation </li><li>如果single input：walk through and stay unchanged</li></ul></li><li>chained residual pooling：<ul><li>aims to capture background context from a large image region </li><li>chained：efficiently pool features with multiple window sizes  </li><li>pooling blocks：s1 maxpooling+conv</li><li>in practice用了两个pooling blocks</li><li>use one ReLU in the chained residual pooling block</li></ul></li><li>output conv：<ul><li>一个residual：to employ non-linearity</li><li>dimension remains unchanged</li><li>final level：two additional RCUs before the final softmax prediction </li></ul></li></ul></li><li>residual identity mappings <ul><li>a clean information path not block by any non-linearity：所有relu都在residual path里面</li><li>只有chained residual pooling模块起始时候有个ReLU：one single ReLU in each RefineNet block does not noticeably reduce the effectiveness of gradient flow </li><li>linear operations：<ul><li>within the fusion block </li><li>dimension reduction operations</li><li>upsamp operations</li></ul></li></ul></li></ul></li><li><p>其他结构</p><p> <img src="/2021/01/05/refineNet/vari.png" width="70%;"></p><ul><li>级联的就叫cascaded</li><li>一个block就叫single</li><li>多个input resolution就叫mult-scale</li></ul></li><li><p>实验</p><ul><li>4-cascaded works better than 1-cas &amp; 2-cas</li><li>2-scale works better than 1-scale</li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 语义分割 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>centerNet</title>
      <link href="/2020/12/29/centerNet/"/>
      <url>/2020/12/29/centerNet/</url>
      <content type="html"><![CDATA[<h2 id="centerNet-Objects-as-Points"><a href="#centerNet-Objects-as-Points" class="headerlink" title="centerNet: Objects as Points"></a>centerNet: Objects as Points</h2><ol><li><p>动机</p><ul><li><p>anchor-based</p><ul><li>exhaustive list of potential locations</li><li>wasteful, inefficient, requires additional post-processing</li></ul></li><li><p>our detector</p><ul><li>center：use keypoint estimation to find center points</li><li>other properties：regress</li></ul></li><li><p>tasks</p><ul><li>object detection</li><li>3d object detection</li><li><p>multi-person human pose estimation </p><p><img src="/2020/12/29/centerNet/tasks.png" width="40%;"></p></li></ul></li></ul></li><li><p>论点</p><ul><li>相比较于传统一阶段、二阶段检测<ul><li>anchor：<ul><li>box &amp; kp：一个是框，一个是击中格子</li><li>nms：take local peaks，no need of nms</li><li>larger resolution：hourglass架构，输出x4的heatmap，eliminates the need for multiple anchors </li></ul></li></ul></li><li>相比较于key point estimantion network<ul><li>them：require grouping stage</li><li>our：只定位一个center point，no need for group or post-processing</li></ul></li></ul></li><li><p>方法</p><ul><li><p>loss</p><ul><li><p>关键点loss</p><ul><li><p>center point关键点定义：每个目标的gt point只有一个，以它为中心，做object size-adaptive的高斯penalty reduction，overlap的地方取max</p></li><li><p>focal loss：基本与cornetNet一致</p><script type="math/tex; mode=display">  L_k = \frac{-1}{N}\sum_{x,y,c}  \begin{cases}  (1-\hat Y)^\alpha log(\hat Y), if Y=1\\  (1-Y)^\beta \hat Y^\alpha log(1-\hat Y), otherwise  \end{cases}</script><ul><li>$\alpha=2, \beta=4$</li><li>background points有penalty，根据gt的高斯衰减来的</li></ul></li></ul></li><li><p>offset loss</p><ul><li>只有两个通道(x_offset &amp; y_offset)：shared among categories</li><li>gt的offset是原始resolution/output stride向下取整得到</li><li>L1 loss</li></ul></li></ul></li><li><p>centerNet</p><ul><li><p>output</p><ul><li>第一个部分：中心点，[h,w,c]，binary mask for each category</li><li>第二个部分：offset，[h,w,2]，shared among</li><li>第三个部分：size，[h,w,2]，shared among<ul><li>L1 loss，use raw pixel coordinates</li></ul></li><li>overall<ul><li>C+4 channels，跟传统检测的formulation是一致的，只不过传统检测gt是基于anchor计算的相对值，本文直接回归绝对值</li><li>$L_{det} = L_k + \lambda_{size} L_{size} + \lambda_{off} L_{off}$</li><li>其他task的formulation看第一张图</li></ul></li></ul></li><li><p>inference workflow</p><ul><li>local peaks：<ul><li>for each category channel</li><li>all responses greater or equal to its 8-connected neighbors：3x3 max pooling</li><li>keep the top100</li></ul></li><li>generate bounding boxes<ul><li>组合offset &amp; size predictions</li><li>？？？？没有后处理了？？？假阳？？？？</li></ul></li></ul></li><li><p>encoder-decoder backbone：x4</p><ul><li>hourglass104<ul><li>stem：x4</li><li>modules：两个</li></ul></li><li>resnet18/101+deformable conv upsampling<ul><li>3x3 deformable conv, 256/128/64</li><li>bilinear interpolation</li></ul></li><li><p>DLA34+deformable conv upsampling</p><p><img src="/2020/12/29/centerNet/backs.jpg" width="80%;"></p></li></ul></li><li><p>heads</p><ul><li>independent heads</li><li>one 3x3 conv，256</li><li>1x1 conv for prediction</li></ul></li></ul></li></ul></li><li><p>总结</p><p> 个人感觉，centerNet和anchor-based的formulation其实是一样的，</p><ul><li>center的回归对标confidence的回归，区别在于高斯/[0,1]/[0,-1,1]</li><li>size的回归变成了raw pixel，不再基于anchor</li><li>hourglass结构就是fpn，级联的hourglass可以对标bi-fpn</li><li>多尺度变成了单一大resolution特征图，也可以用多尺度预测，需要加NMS</li></ul></li></ol><h2 id="cornet-centerNet-Keypoint-Triplets-for-Object-Detection"><a href="#cornet-centerNet-Keypoint-Triplets-for-Object-Detection" class="headerlink" title="cornet-centerNet: Keypoint Triplets for Object Detection"></a>cornet-centerNet: Keypoint Triplets for Object Detection</h2><ol><li><p>动机</p><ul><li><p>based on cornerNet</p></li><li><p>triplet</p><ul><li>corner keypoints：weak grouping ability cause false positives</li><li><p>correct predictions can be determined by checking the central parts</p><p><img src="/2020/12/29/centerNet/center.png" width="40%;"></p></li></ul></li><li><p>cascade corner pooling and center poolling</p></li></ul></li><li><p>论点</p><ul><li>whats new in CenterNet<ul><li>triplet inference workflow<ul><li>after a proposal is generated as a pair of corner keypoints</li><li>checking if there is a center keypoint of the same class </li></ul></li><li>center pooling<ul><li>for predicting center keypoints </li><li>by making the center keypoints on feature map having the max sum Hori+Verti responses</li></ul></li><li>cascade corner pooling<ul><li>equips the original corner pooling module with the ability of perceiving internal information </li><li>not only consider the boundary but also the internal directions </li></ul></li></ul></li><li>CornetNet痛点<ul><li>fp rate高</li><li>small object的fp rate尤其高</li><li>一个idea：cornerNet based RPN<ul><li>但是原生RPN都是复用的</li><li>计算效率？</li></ul></li></ul></li></ul></li><li><p>方法</p><ul><li><p>center pooling</p><ul><li>geometric centers &amp; semantic centers</li><li><p>center pooling能够有效地将语义信息最丰富的点（semantic centers）传达到物理中心点（geometric centers），也就是central region</p><p><img src="/2020/12/29/centerNet/central.png" width="40%;"></p></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 目标检测，anchor-free </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>equlization loss</title>
      <link href="/2020/12/21/equlization-loss/"/>
      <url>/2020/12/21/equlization-loss/</url>
      <content type="html"><![CDATA[]]></content>
      
      
    </entry>
    
    <entry>
      <title>megDet</title>
      <link href="/2020/12/18/megDet/"/>
      <url>/2020/12/18/megDet/</url>
      <content type="html"><![CDATA[<h2 id="MegDet-A-Large-Mini-Batch-Object-Detector"><a href="#MegDet-A-Large-Mini-Batch-Object-Detector" class="headerlink" title="MegDet: A Large Mini-Batch Object Detector"></a>MegDet: A Large Mini-Batch Object Detector</h2><ol><li><p>动机</p><ul><li>past methods mainly come from novel framework or loss design</li><li><p>this paper studies the mini-batch size</p><ul><li>enable training with a large mini-batch size</li><li>warmup learning rate policy</li><li>cross-gpu batch normalization</li></ul></li><li><p>faster &amp; better acc</p></li></ul></li><li><p>论点</p><ul><li><p>potential drawbacks with small mini-batch sizes</p><ul><li><p>long training time</p></li><li><p>inaccurate statistics for BN：previous methods use fixed statistics from ImageNet which is a sub-optimal trade-off</p></li><li><p>positive &amp; negative training examples are more likely imblanced</p></li><li><p>加大batch size以后，正负样本比例有提升，所以yolov3会先锁着back开大batchsize做warmup</p><p>  <img src="/2020/12/18/megDet/ratio.png" width="40%;"></p></li></ul></li><li><p>learning rate dilemma</p><ul><li>large min-batch size usually requires large learning rate </li><li>large learning rate is likely leading to convergence failure</li><li>a smaller learning rate often obtains inferior results </li></ul></li><li><p>solution of the paper</p><ul><li>linear scaling rule </li><li>warmup</li><li>Cross-GPU Batch Normalization (CGBN) </li></ul></li></ul></li><li><p>方法</p><ul><li><p>warmup</p><ul><li>set up the learning rate small enough at the be- ginning</li><li>then increase the learning rate with a constant speed after every iteration, until fixed</li></ul></li><li><p>Cross-GPU Batch Normalization </p><ul><li>两次同步</li><li><p>tensorpack里面有</p><p><img src="/2020/12/18/megDet/CGBN.png" width="40%;"></p><p><img src="/2020/12/18/megDet/Cross-GPU.png" width="40%;"></p></li></ul></li></ul></li><li><p>一次同步</p><ul><li><p>异步BN：batch size 较小时，每张卡计算得到的统计量可能与整体数据样本具有较大差异</p><p> <img src="/2020/12/18/megDet/unsync.png" width="40%;"></p></li><li><p>同步：</p></li><li><p>需要同步的是每张卡上计算的统计量，即BN层用到的均值$\mu$和方差$\sigma^2$</p></li><li><p>这样多卡训练结果才与单卡训练效果相当</p></li><li><p>两次同步：</p></li><li><p>第一次同步均值：计算全局均值</p></li><li><p>第二次同步方差：基于全局均值计算各自方差，再取平均</p></li><li><p>一次同步：</p><ul><li><p>核心在于方差的计算</p></li><li><p>首先均值：$\mu = \frac{1}{m} \sum_{i=1}^m x_i$</p><ul><li>然后是方差：<script type="math/tex; mode=display">  \sigma^2 = \frac{1}{m} \sum_{i=1}^m (x_i-\mu)^2 = \frac{1}{m} \sum_{i=1}^m x_i^2 - \mu^2\\  =\frac{1}{m} \sum_{i=1}^m x_i^2 - (\frac{1}{m} \sum_{i=1}^m x_i)^2</script></li></ul></li></ul></li></ul></li></ol><pre><code>  * 计算每张卡的$\sum x_i$和$\sum x_i^2$，就可以一次性算出总均值和总方差</code></pre>]]></content>
      
      
        <tags>
            
            <tag> 目标检测，large mini-batch </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>RFB</title>
      <link href="/2020/12/16/RFB/"/>
      <url>/2020/12/16/RFB/</url>
      <content type="html"><![CDATA[<h2 id="RFB-Receptive-Field-Block-Net-for-Accurate-and-Fast-Object-Detection"><a href="#RFB-Receptive-Field-Block-Net-for-Accurate-and-Fast-Object-Detection" class="headerlink" title="RFB: Receptive Field Block Net for Accurate and Fast Object Detection"></a>RFB: Receptive Field Block Net for Accurate and Fast Object Detection</h2><ol><li><p>动机</p><ul><li>RF block：Receptive Fields </li><li>strengthen the lightweight features using a hand-crafted mechanism：轻量，特征表达能力强</li><li>assemble RFB to the top of SSD </li></ul></li><li><p>论点</p><ul><li><p>lightweight </p><ul><li>enhance feature representation </li></ul></li><li><p>人类</p><ul><li>群智感受野（pRF）的大小是其视网膜图中偏心率的函数</li><li>感受野随着偏心率而增加</li><li>更靠近中心的区域在识别物体时拥有更高的比重或作用</li><li><p>大脑在对于小的空间变化不敏感</p><p><img src="/2020/12/16/RFB/human.png" width="60%;"></p></li></ul></li><li><p>fixed sampling grid (conv)</p><ul><li>probably induces some loss in the feature discriminability as well as robustness </li></ul></li><li><p>inception</p><ul><li>RFs of multiple sizes </li><li>but at the same center</li></ul></li><li><p>ASPP</p><ul><li>with different atrous rates </li><li>the resulting feature tends to be less distinctive </li></ul></li><li><p>Deformable CNN </p><ul><li>sampling grid is flexible</li><li><p>but all pixels in an RF contribute equally </p><p><img src="/2020/12/16/RFB/RF.png" width="60%;"></p></li></ul></li><li><p>RFB</p><ul><li>varying kernel sizes</li><li>applies dilated convolution layers to control their eccentricities </li><li>组合来模拟human visual system </li><li>concat</li><li><p>1x1 conv for fusion</p><p><img src="/2020/12/16/RFB/rfb.png" width="60%;"></p></li></ul></li><li><p>main contributions </p><ul><li>RFB module: enhance deep features of lightweight CNN networks</li><li>RFB Net: gain on SSD</li><li>assemble on MobileNet </li></ul></li></ul></li><li><p>方法</p><ul><li><p>Receptive Field Block </p><ul><li>类似inception的multi-branch</li><li><p>dilated pooling or convolution layer</p><p><img src="/2020/12/16/RFB/RFBlock.png" width="80%;"></p></li></ul></li><li><p>RFB Net </p><ul><li><p>SSD-base</p><p>  <img src="/2020/12/16/RFB/ssd.png" width="80%;"></p></li><li><p>头上有较大分辨率的特征图的conv层are replaced by the RFB module</p></li><li><p>特别头上的conv层就保留了，因为their feature maps are too small to apply filters with large kernels like 5 × 5</p><p><img src="/2020/12/16/RFB/rfbnet.png" width="70%;"></p></li><li><p>stride2 module：每个conv stride2，那id path得变成1x1 conv？</p></li></ul></li><li></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PANet</title>
      <link href="/2020/12/02/PANet/"/>
      <url>/2020/12/02/PANet/</url>
      <content type="html"><![CDATA[<h2 id="PANet-Path-Aggregation-Network-for-Instance-Segmentation"><a href="#PANet-Path-Aggregation-Network-for-Instance-Segmentation" class="headerlink" title="PANet: Path Aggregation Network for Instance Segmentation"></a>PANet: Path Aggregation Network for Instance Segmentation</h2><ol><li><p>动机</p><ul><li>boost the information flow</li><li>bottom-up path<ul><li>shorten information path </li><li>enhance  accurate localization </li></ul></li><li>adaptive feature pooling <ul><li>aggregate all levels</li><li>avoiding arbitrarily assigned results </li></ul></li><li>mask prediction head<ul><li>fcn + fc</li><li>captures different views, possess complementary properties </li></ul></li><li>subtle extra computational</li></ul></li><li><p>论点</p><ul><li>previous skills: fcn, fpn, residual, dense</li><li>findings<ul><li>高层特征类别准，底层特征定位准，但是高层和底层特征之间的path太长了，不利于双高</li><li>past proposals make predictions based on one level</li></ul></li><li>PANet <ul><li>bottom-up path<ul><li>shorten information path </li><li>enhance  accurate localization </li></ul></li><li>adaptive feature pooling <ul><li>aggregate all levels</li><li>avoiding arbitrarily assigned results </li></ul></li><li>mask prediction head<ul><li>fcn + fc</li><li>captures different views, possess complementary properties </li></ul></li></ul></li></ul></li><li><p>方法</p><ul><li><p>framework</p><p> <img src="/2020/12/02/PANet/PAN.png" width="80%;"></p><ul><li>b: bottom-up path</li><li>c: adaptive feature pooling </li><li>e: fusion mask branch</li></ul></li><li><p>bottom-up path</p><ul><li>fpn’s top-down path:<ul><li>to propagate strong semantical information</li><li>to ensure reasonable classification capability </li><li>long path: red line, 100+ layers</li></ul></li><li>bottom-up path:  <ul><li>enhances the localization capability  </li><li>short path: green line, less than 10 layers</li></ul></li><li><p>for each level $N_l$</p><ul><li>input: $N_{l+1}$ &amp; $P_l$</li><li>$N_{l+1}$ 3x3 conv &amp; $P_l$ id path - add - 3x3 conv</li><li>channel 256</li><li>ReLU after conv</li></ul><p><img src="/2020/12/02/PANet/bottom-up.png" width="50%;"></p></li></ul></li><li><p>adaptive feature pooling </p><ul><li><p>pool features from all levels, then fuse, then predict</p></li><li><p>steps</p><ul><li>map each proposal to all feature levels</li><li>roi align</li><li>go through one layer of the following sub-networks independently</li><li>fusion operation (element-wise max or sum) </li><li><p>例如，box branch是两个fc层，来自各个level的roi align之后的proposal features，先各自经过一个fc层，再share the following till the head，mask branch是4个conv层，来自各个level的roi align之后的proposal features，先各自经过一个conv层，再share the following till the head</p><p><img src="/2020/12/02/PANet/fusion.png" width="50%;"></p></li></ul></li><li><p>fusion mask branch</p><ul><li>fc layers are location sensitive </li><li>helpful to differentiate instances and recognize separate parts belonging to the same object </li><li>conv分支<ul><li>4个连续conv+1个deconv：3x3 conv，channel256，deconv factor=2</li><li>predict mask of each class：output channel n_classes </li></ul></li><li>fc分支<ul><li>from conv分支的conv3输出</li><li>2个连续conv，channel256，channel128</li><li>fc，dim=28x28，特征图尺寸，用于前背景分类</li></ul></li><li><p>final mask：add</p><p><img src="/2020/12/02/PANet/mask.png" width="50%;"></p></li></ul></li></ul></li></ul></li><li><p>实验</p><ul><li>heavier head<ul><li>4 consecutive 3x3 convs</li><li>shared among reg &amp; cls</li><li>在multi-task的情况下，对box的预测有效</li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 实例分割 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CSPNet</title>
      <link href="/2020/11/17/CSPNet/"/>
      <url>/2020/11/17/CSPNet/</url>
      <content type="html"><![CDATA[<h2 id="CSPNET-A-NEW-BACKBONE-THAT-CAN-ENHANCE-LEARNING-CAPABILITY-OF-CNN"><a href="#CSPNET-A-NEW-BACKBONE-THAT-CAN-ENHANCE-LEARNING-CAPABILITY-OF-CNN" class="headerlink" title="CSPNET: A NEW BACKBONE THAT CAN ENHANCE LEARNING CAPABILITY OF CNN"></a>CSPNET: A NEW BACKBONE THAT CAN ENHANCE LEARNING CAPABILITY OF CNN</h2><ol><li><p>动机</p><ul><li>propose a network from the respect of the variability of the gradients</li><li>reduces computations </li><li>superior accuracy while being lightweightening</li></ul></li><li><p>论点</p><ul><li><p>CNN architectures design </p><ul><li>ResNeXt：cardinality can be more effective than width and depth </li><li>DenseNet：reuse features </li><li>partial ResNet：high cardinality and sparse connection，the concept of gradient combination </li></ul></li><li><p>introduce Cross Stage Partial Network (CSPNet) </p><ul><li>strengthening learning ability of a CNN：sufficient accuracy while being lightweightening </li><li>removing computational bottlenecks：hoping evenly distribute the amount of computation at each layer in CNN </li><li><p>reducing memory costs：adopt cross-channel pooling during fpn</p><p><img src="/2020/11/17/CSPNet/CSP.png" width="90%;"></p></li></ul></li></ul></li><li><p>方法</p><ul><li><p>结构</p><ul><li>Partial Dense Block：节省一半计算</li><li>Partial Transition Layer：fusion last能够save computation同时精度不掉太多</li></ul><p><img src="/2020/11/17/CSPNet/fusion.png" width="90%;"></p><ul><li>论文说fusion first使得大量梯度得到重用，computation cost is significantly dropped，fusion last会损失部分梯度重用，但是精度损失也比较小(0.1)。</li><li>it is obvious that if one can effectively reduce the repeated gradient information, the learning ability of a network will be greatly improved. </li></ul><p><img src="/2020/11/17/CSPNet/acc.png" width="80%;"></p></li></ul></li></ol><ul><li><p>Apply CSPNet to Other Architectures </p><ul><li>因为只有一半的channel参与resnet block的计算，所以无需再引入bottleneck结构了</li><li><p>最后两个path的输出concat</p><p><img src="/2020/11/17/CSPNet/cspresnet.png" width="40%;"></p></li></ul></li><li><p>EFM</p><p>  <img src="/2020/11/17/CSPNet/ERM.png" width="80%;"></p></li><li><p>fusion</p><pre><code>  * 特征金字塔（FPN）：融合当前尺度和以前尺度的特征。  * 全局融合模型（GFM）：融合所有尺度的特征。  * 精确融合模型（EFM）：融合anchor尺寸上的特征。</code></pre><ul><li>EFM<ul><li>assembles features from the three scales：当前尺度&amp;相邻尺度</li><li>同时又加了一组bottom-up的融合</li><li>Maxout technique对特征映射进行压缩</li></ul></li></ul></li></ul><ol><li><p>结论</p><p> 从实验结果来看，</p><ul><li>分类问题中，使用CSPNet可以降低计算量，但是准确率提升很小；</li><li>在目标检测问题中，使用CSPNet作为Backbone带来的提升比较大，可以有效增强CNN的学习能力，同时也降低了计算量。本文所提出的EFM比GFM慢2fps，但AP和AP50分别显著提高了2.1%和2.4%。</li></ul></li></ol>]]></content>
      
      
    </entry>
    
    <entry>
      <title>nms</title>
      <link href="/2020/10/29/nms/"/>
      <url>/2020/10/29/nms/</url>
      <content type="html"><![CDATA[<p>Non-maximum suppression：非极大值抑制算法，本质是搜索局部极大值，抑制非极大值元素</p><p>[nms]：standard nms，当目标比较密集、存在遮挡时，漏检率高</p><p>[soft nms]：改变nms的hard threshold，用较低的分数替代0，提升recall</p><p>[softer nms]：引入box position confidence，通过后处理提高定位精度</p><p>[DIoU nms]：采用DIoU的计算方式替换IoU，因为DIoU的计算考虑到了两框中心点位置的信息，效果更优</p><p>[fast nms]：YOLOACT引入矩阵三角化，会比Traditional NMS抑制更多的框，性能略微下降</p><p>[cluster nms]：<a href="https://github.com/Zzh-tju/CIoU" target="_blank" rel="noopener">CIoU提出</a>，弥补Fast NMS的性能下降，运算效率比Fast NMS下降了一些</p><p>[mask nms]：mask iou计算有不可忽略的延迟，因此比box nms更耗时</p><p>[matrix nms]：SOLO将mask IoU并行化，比FAST-NMS还快，思路和FAST-NMS一样从上三角IoU矩阵出发，可能造成过多抑制。</p><p>[WBF]：加权框融合，Kaggle胸片异物比赛claim有用，速度慢，大概比标准NMS慢3倍，WBF实验中是在已经完成NMS的模型上进行的</p><ol><li>nms<ul><li>过滤+迭代+遍历+消除<ul><li>首先过滤掉大量置信度较低的框，大于confidence thresh的box保留</li><li>将所有框的得分排序，选中最高分的框</li><li>遍历其余的框，如果和当前最高分框的IOU大于一定阈值(nms thresh)，就将框删除(score=0)</li><li>从未处理的框中继续选一个得分最高的，重复上述过程</li></ul></li><li>when evaluation<ul><li>iou thresh：留下的box里面，与gt box的iou大于iou thresh的box作为正例，用于计算出AP和mAP，通过调整confidence thresh可以画出PR曲线</li></ul></li></ul></li></ol><ol><li><p>softnms</p><ul><li><p>基本流程还是nms的贪婪思路，过滤+迭代+遍历+衰减</p><p>  <img src="/2020/10/29/nms/softnms.png" width="40%;"></p></li><li><p>re-score function：high overlap decays more</p><ul><li>linear：<ul><li>for each $iou(M,b_i)&gt;th$， $s_i=s_i(1-iou)$</li><li>not continuous，sudden penalty </li></ul></li><li>gaussian：<ul><li>for all remaining detection boxes，$s_i=s_i e^{-\frac{iou(M,b_i)}{\sigma}}$</li></ul></li></ul></li><li><p>算法流程上未做优化，是针对精度的优化</p></li></ul></li></ol><ol><li><p>softer nms</p><ul><li>跟soft nms没关系</li><li>具有高分类置信度的边框其位置并不是最精准的</li><li>新增加了一个定位置信度的预测，使其服从高斯分布</li><li>infer阶段边框的标准差可以被看做边框的位置置信度，与分类置信度做加权平均，作为total score</li><li>算法流程上未做优化，完全是精度的优化</li></ul></li></ol><ol><li><p>DIoU nms</p><ul><li><p>也是为了解决hard nms在密集场景中漏检率高的问题</p></li><li><p>但是不同于soft nms的是，D的改进在iou计算上，而不是在score</p></li><li><p>diou的计算：$diou = iou-\frac{\rho^2(b_1, b_2)}{c^2}$</p><p>  <img src="/2020/10/29/nms/diou.png" width="60%;"></p></li><li><p>算法流程上未做优化，仍旧是精度的优化</p></li></ul></li></ol><ol><li><p>fast nms</p><ul><li><p>yoloact提出</p></li><li><p>主要效率提升在于用矩阵操作替换遍历，所有框同时被filter掉，而非依次遍历删除</p></li><li><p>iou上三角矩阵</p><ul><li>iou上三角矩阵的每一个元素都是行号小于列号</li><li>iou上三角矩阵的每一个行，对应一个bnd box，与其他所有score小于它的bnd box的iou</li><li>iou上三角矩阵的每一个列，对应一个bnd box，与其他所有score大于它的bnd box的iou</li><li>fast nms在iou矩阵每一列上求最大值，如果这个最大值大于iou thresh，说明当前列对应的bnd box，存在一个score大于它，且和它重叠度较高的bnd box，因此要把这个box过滤掉</li></ul></li><li><p>有精度损失</p><ul><li><p>场景：</p><p><img src="/2020/10/29/nms/fastnms1.jpg" width="60%;"></p></li><li><p>如果是hard nms的话，首先遍历b1的其他box，b2就被删除了，这是b3就不存在高重叠框了，b3就会被留下，但是在fast nms场景下，所有框被同时删除，因此b2、b3都没了。</p><p><img src="/2020/10/29/nms/fastnms2.png" width="60%;"></p></li></ul></li></ul></li></ol><ol><li><p>cluster nms</p><ul><li><p>针对fast nms性能下降的弥补</p></li><li><p>fast nms性能下降，主要问题在于过度抑制，并行操作无法及时消除high score框抹掉对后续low score框判断的影响</p></li><li><p>算法流程上，将fast nms的一次阈值操作，转换成少数几次的迭代操作，每次都是一个fast nms</p><p>  <img src="/2020/10/29/nms/clusternms.png" width="80%;"></p><ul><li>图中X表示iou矩阵，b表示nms阈值二值化以后的向量，也就是fast nms里面那个保留／抑制向量</li><li>每次迭代，算法将b展开成一个对角矩阵，然后左乘iou矩阵</li><li>直到出现某两次迭代后， b保持不变了，那么这就是最终的b</li></ul></li><li><p>cluster nms的迭代操作，其实就是在省略上一次Fast NMS迭代中被抑制的框对其他框的影响</p></li><li><p>数学归纳法证明，cluster nms的结果与hard nms完全一致，运算效率比fast nms下降了一些，但是比hard nms快得多</p></li><li><p>cluster nms的运算效率不与cluster数量有关，只与需要迭代次数最多的那一个cluster有关</p></li></ul></li></ol><ol><li><p>mask nms</p><ul><li>从检测框形状的角度拓展出来，包括但不限于mask nms、polygon nms以及inclined nms</li><li>iou的计算方式有一种是mmi：$mmi=max(\frac{I}{I_A}, \frac{I}{I_B})$</li></ul></li></ol><ol><li><p>matrix nms</p><ul><li><p>学习soft nms：decay factor</p></li><li><p>one step further：迭代改并行</p></li><li><p>对于某个object $m_j$的score进行penalty的时候考虑两部分影响</p><ul><li>迭代某个$m_i$时，对后续lower score的$m_j$的影响</li><li>一是正面影响$f(iou_{i,j})\ linear/guassian$：这个框保留，那么后续框都要基于与其的iou做decay</li><li>二是反向影响$f(iou_{*,i})=max_{\forall s_k&gt;s_i}f(iou_{k,i})$：如果这个框不保留，那么对于后续框来讲，应该消除这个框对其的decay，选最大值的意义是当前mask被抑制最有可能就是和他重叠度最大的那个mask干的（因为对应的正面影响1-iou最小）</li></ul></li></ul></li></ol><ul><li><p>final decay factor：$decay_j=min_{\forall s_i &gt; s_j}\frac{f(iou_{i,j})}{f(iou_{*,i})}$</p></li><li><p>算法流程</p><pre><code>  &lt;img src=&quot;nms/matrixnms.png&quot; width=&quot;50%;&quot; /&gt;</code></pre></li></ul><pre><code>* 按照原论文的实现，decay永远大于等于1，因为每一列的iou_cmax永远大于等于iou，从论文的思路来看，每个mask的decay是它之前所有mask的影响叠加在一起，所以应该是乘积而不是min：    <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 原论文实现</span></span><br><span class="line"><span class="keyword">if</span> method==<span class="string">'gaussian'</span>:</span><br><span class="line">    decay = np.exp(-(np.square(iou)-np.square(iou_cmax))/sigma)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    decay = (<span class="number">1</span>-iou)/(<span class="number">1</span>-iou_cmax)</span><br><span class="line">decay = np.min(decay, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 改进实现</span></span><br><span class="line"><span class="keyword">if</span> method==<span class="string">'gaussian'</span>:</span><br><span class="line">    decay = np.exp(-(np.sum(np.square(iou),axis=<span class="number">0</span>)-np.square(iou_cmax))/sigma)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    decay = np.prod(<span class="number">1</span>-iou)/(<span class="number">1</span>-iou_cmax)</span><br></pre></td></tr></table></figure></code></pre>]]></content>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>MimicDet</title>
      <link href="/2020/10/14/MimicDet/"/>
      <url>/2020/10/14/MimicDet/</url>
      <content type="html"><![CDATA[<p>[MimicDet] ResNeXt-101 backbone on the COCO: 46.1 mAP </p><h2 id="MimicDet-Bridging-the-Gap-Between-One-Stage-and-Two-Stage-Object-Detection"><a href="#MimicDet-Bridging-the-Gap-Between-One-Stage-and-Two-Stage-Object-Detection" class="headerlink" title="MimicDet: Bridging the Gap Between One-Stage and Two-Stage Object Detection"></a>MimicDet: Bridging the Gap Between One-Stage and Two-Stage Object Detection</h2><ol><li><p>动机</p><ul><li>mimic task：knowledge distillation </li><li>mimic the two-stage features <ul><li>a shared backbone </li><li>two heads for mimicking </li></ul></li><li>end-to-end training</li><li>specialized designs to facilitate mimicking <ul><li>dual-path mimicking</li><li>staggered feature pyramid </li></ul></li><li>reach two-stage accuracy </li></ul></li><li><p>论点</p><ul><li>one-stage detectors adopt a straightforward fully convolutional architecture </li><li>two-stage detectors use RPN + R-CNN</li><li>advantages of two-stage detectors <ul><li>avoid class imbalance </li><li>less proposals enables larger cls net and richer features </li><li>RoIAlign extracts location consistent feature -&gt; better represenation</li><li>regress the object location twice -&gt; better refined</li></ul></li><li>one-stage detectors’ imitation<ul><li>RefineDet：cascade detection flow  </li><li>AlignDet：RoIConv layer </li><li>still leaves a big gap </li></ul></li><li>network mimicking<ul><li>knowledge distillation </li><li>use a well-trained large teacher model to supervise </li><li>difference<ul><li>mimic in heads instead of backbones </li><li>teacher branch instead of model</li><li>trained jointly </li></ul></li></ul></li><li>this method<ul><li>not only mimic the structure design, but also imitate in the feature level</li><li>contains both one-stage detection head and two-stage detection head during training <ul><li>share the same backbone </li><li>two-stage detection head, called T-head </li><li>one-stage detection head, called S-head </li><li>similarity loss for matching feature：guided deformable conv layer </li><li>together with detection losses </li></ul></li><li>specialized designs <ul><li>decomposed detection heads</li><li>conduct mimicking in classification and regression branches individually</li><li>staggered feature pyramid  </li></ul></li></ul></li></ul></li><li><p>方法</p><ul><li><p>overview</p><p>  <img src="/2020/10/14/MimicDet/mimicdet.png" width="70%;"></p></li><li><p>back &amp; fpn</p><ul><li>RetinaNet fpn：with P6 &amp; P7</li><li>crucial modification：P2 ～ P7</li><li>staggered feature pyramid <ul><li>high-res set {P2 to P6}：for T-head &amp; accuray</li><li>low-res set {P3 to P7}：for S-head &amp; computation speed</li></ul></li></ul></li><li><p>refinement module</p><ul><li>filter out easy negatives：mitigate the class imbalance issue </li><li>adjust the location and size of pre-defined anchor boxes：anchor initialization  </li><li>module<ul><li>on top of the feature pyramid </li><li>one 3x3 conv</li><li>two sibling 1x1 convs<ul><li>binary classification：bce loss</li><li>bounding box regression：the same as Faster R-CNN，L1 loss</li></ul></li><li>top-ranked boxes transferred to T-head and S-head </li></ul></li><li>one anchor on each position：avoid feature sharing among proposals</li><li>assign the objects to feature pyramid according to their scale </li><li>positive area：0.3 times shrinking of gt boxes from center</li><li>positive sample：<ul><li>valid scale range：gt target belongs to this level</li><li>central point of anchor lies in the positive area</li></ul></li></ul></li><li><p>detection heads</p><ul><li>T-head<ul><li>heavy head </li><li>run on a sparse set of anchor boxes </li><li>use the staggered feature pyramid </li><li>generate 7x7 location-sensitive features for each anchor box </li><li>cls branch<ul><li>two 1024-d fc layers</li><li><strong>one 81-d fc layer</strong> + softmax：ce loss</li></ul></li><li>reg branch<ul><li>four 3x3 convs，ch256</li><li>flatten</li><li><strong>1024-d fc</strong></li><li>4-d fc：L1 loss</li></ul></li><li>mimicking target <ul><li>81-d classification logits </li><li>1024-d regression feature </li></ul></li></ul></li><li>S-head<ul><li>light-weight  </li><li>directly dense detection on fpn</li><li>【不太理解】introducing the refinement module will break the location consistency between the anchor box and its corresponding features：我的理解是refine以后的anchor和原始anchor对应的特征图misalign了，T-head用的是refined anchor，S-head用的是original grid，所以misalign</li><li>use deformable convolution to capture the misaligned feature <ul><li>deformation offset is computed by a micro-network </li><li>takes the regression output of the refinement module as input </li><li>three 1x1 convs，ch64/128／18(50)</li><li>3x3 Dconv for P3 and 5x5 for others，ch256</li></ul></li><li>two sibling 1x1 convs，ch1024<ul><li>cls branch：1x1 conv，ch80</li><li>reg branch：1x1 conv，ch4</li></ul></li></ul></li><li><img src="/2020/10/14/MimicDet/heads.png" width="70%;"></li></ul></li><li><p>head mimicking</p><ul><li>cosine similarity </li><li>cls logits &amp; refine params</li><li>To get the S-head feature of an adjusted anchor box <ul><li>trace back to its initial position </li><li>extract the pixel at that position in the feature map </li></ul></li><li>loss：$L_{mimic} = 1 - cosine(F_i^T, F_i^S)$</li></ul></li><li>multi-task training loss  <ul><li>$L = L_R + L_S + L_T + L_{mimic}$</li><li>$L_R$：refine module loss，bce+L1</li><li>$L_S$：S-head loss，ce+L1</li><li>$L_T$：T-head loss，ce+L1</li><li>$L_{mimic}$：mimic loss </li></ul></li><li>training details<ul><li>network：resnet50/101，resize image with shorter side 800</li><li>refinement module<ul><li>run NMS with 0.8 IoU threshold on anchor boxes </li><li>select top 2000 boxes </li></ul></li><li>T-head<ul><li>sample 128 boxes from proposal</li><li>p／n：1/3</li></ul></li><li>S-head<ul><li>hard mining：select 128 boxes with top loss value </li></ul></li></ul></li><li>inference<ul><li>take top 1000 boxes from refine module</li><li>NMS with 0.6 IoU threshold and 0.005 score threshold </li><li>【？？】finally top 100 scoring boxes：这块不太理解，最后应该不是结构化输出了啊，应该是一阶段检测头的re-refine输出啊</li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>metrics</title>
      <link href="/2020/10/09/metrics/"/>
      <url>/2020/10/09/metrics/</url>
      <content type="html"><![CDATA[<h2 id="分类指标"><a href="#分类指标" class="headerlink" title="分类指标"></a>分类指标</h2><ul><li>recall：召回率</li><li>precision：准确率</li><li>accuracy：正确率</li><li>F-Measure</li><li>sensitivity：灵敏度</li><li>specificity：特异度</li><li>TPR</li><li>FPR</li><li>ROC</li><li>AUC</li></ul><ol><li><p>混淆矩阵</p><p> |           |  gt is p   |   gt is n    |<br> | :———-: | :————: | :—————: |<br> | pred is p |     tp     | fp（假阳性） |<br> | pred is n | fn（漏检） |      tn      |</p><ul><li>注意区分fp和fn</li><li>fp：被错误地划分为正例的个数，即实际为负例但被分类器划分为正例的实例数</li><li>fn：被错误地划分为负例的个数，即实际为正例但被分类器划分为负例的实例数</li></ul></li><li><p>recall</p><ul><li>衡量查全率</li><li>对gt is p做统计</li><li>$recall = \frac{tp}{tp+fn}$</li></ul></li><li><p>precision</p><ul><li>衡量查准率</li><li>对pred is p做统计</li><li>$precision = \frac{tp}{tp+fp}$</li></ul></li><li><p>accuracy</p><ul><li>对的除以所有</li><li>$accuracy = \frac{tp+tn}{p+n}$</li></ul></li><li><p>sensitivity</p><ul><li>衡量分类器对正例的识别能力</li><li>对gt is p做统计</li><li>$sensitivity = \frac{tp}{p}=\frac{tp}{tp+fn}$</li></ul></li><li><p>specificity</p><ul><li>衡量分类器对负例的识别能力</li><li>对gt is n做统计</li><li>$specificity =\frac{tn}{n}= \frac{tn}{fp+tn}$</li></ul></li><li><p>F-measure</p><ul><li>综合考虑P和R，是Precision和Recall加权调和平均</li><li>$F = \frac{(a^2+1)<em>P</em>R}{a^2*P+R}$</li><li>$F_1 = \frac{2PR}{P+R}$</li></ul></li><li><p>TPR</p><ul><li>将正例分对的概率</li><li>对gt is t做统计</li><li>$TPR = \frac{tp}{tp+fn}$</li></ul></li><li><p>FPR</p><ul><li>将负例错分为正例的概率</li><li>对gt is n做统计</li><li>$FPR = \frac{fp}{fp+tn}$</li><li>FPR = 1 - 特异度</li></ul></li><li><p>ROC</p><ul><li>每个点的横坐标是FPR，纵坐标是TPR</li><li>描绘了分类器在TP（真正的正例）和FP（错误的正例）间的trade-off</li><li>通过变化阈值，得到不同的分类统计结果，连接这些点就形成ROC curve</li><li>曲线在对角线左上方，离得越远说明分类效果好</li><li>P/R和ROC是两个不同的评价指标和计算方式，一般情况下，检索用前者，分类、识别等用后者</li></ul></li><li><p>AUC</p><ul><li>AUC的值就是处于ROC curve下方的那部分面积的大小</li><li>通常，AUC的值介于0.5到1.0之间</li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 评价指标 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>metric learning系列</title>
      <link href="/2020/09/25/metric-learning%E7%B3%BB%E5%88%97/"/>
      <url>/2020/09/25/metric-learning%E7%B3%BB%E5%88%97/</url>
      <content type="html"><![CDATA[<p>参考：<a href="https://gombru.github.io/2019/04/03/ranking_loss/，博主实验下来觉得Triplet" target="_blank" rel="noopener">https://gombru.github.io/2019/04/03/ranking_loss/，博主实验下来觉得Triplet</a> Loss outperforms Cross-Entropy Loss</p><h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><ol><li><p>metric learning</p><ul><li>常规的cls loss系列(CE、BCE、MSE)的目标是predict a label</li></ul></li></ol><ul><li>metric loss系列的目标则是predict relative distances between inputs<ul><li>常用场景：人脸 &amp; fine-grained</li></ul></li></ul><ol><li><p>relation between samples</p><ul><li>first get the embedded representation</li><li>then compute the similarity score<ul><li>binary (similar / dissimilar)</li><li>regression (euclidian distance)</li></ul></li></ul></li><li><p>大类：不管叫啥，主体上就两类，二元组和三元组</p><ul><li><p>common target：拉近类内距离，拉大类间距离</p></li><li><p>pairs</p><ul><li>anchor + sample<ul><li>positive pairs：distance —&gt; 0</li><li>negative pairs：disctance &gt; a margin</li></ul></li><li><img src="/2020/09/25/metric-learning系列/pairs.png" width="50%;"></li></ul></li><li>triplets<ul><li>anchor + pos sample + neg sample</li><li>target：(dissimilar distance - similar distance) —&gt; a margin</li></ul></li><li><img src="/2020/09/25/metric-learning系列/triplet.png" width="50%;"></li></ul></li><li><p>papers</p><p>[siamese network] Signature Verification using a ‘Siamese’ Time Delay Neural Network：1993，lecun，孪生网络始祖，俩个子网络sharing weights，距离用的是cosine distance，loss直接优化距离，优化target是个定值cosine=1.0/-1.0</p><p>[contrastive loss] Dimensionality Reduction by Learning an Invariant Mapping：2006，lecun，contrastive loss始祖，研究的是高维特征向量向低维映射的非线性层，距离用的是euclidean distance，loss优化的是squared distance，优化target是0和m，similar pairs仍旧会被推向一个定点，没有解决论文声称的uniform distribution</p><p>[triplet-loss] Learning Fine-grained Image Similarity with Deep Ranking：2014，Google，用了三元组，提出了triplet-loss</p><p>[facenet] FaceNet: A Unified Embedding for Face Recognition and Clustering：2015，Google，用来识别人脸，用了三元组和triplet-loss，squared euclidean distance，优化目标是同类和异类pair之间的相对距离，困难样本（semi-hard &amp; hard）对收敛起作用（加速／local minima），triplet-loss考虑了类间的离散性，但没有考虑类内的紧凑性</p><p>[center-loss] A Discriminative Feature Learning Approach for Deep Face Recognition：2016，也是用在人脸任务上，优化目标是类内的绝对距离，而不是建模相对关系，center-loss直接优化的是类间的间凑性，类间的离散性靠的是softmax loss</p><p>[triplet-center-loss] Triplet-Center Loss for Multi-View 3D Object Retrieval：2018，东拼西凑水论文</p><p>[Hinge-loss] SVM margin</p><p>[circle-loss] Circle Loss: A Unified Perspective of Pair Similarity Optimization：2020CVPR，旷视，提出了cls loss和metric loss的统一形式$minimize(s_n - s_p+m)$，在此基础上提出circle loss作为优化目标$(\alpha_n s_n - \alpha_p s_p) = m$，在toy scenario下展示了分类边界和梯度的改善。</p><p>～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～</p><p>[Hierarchical Similarity] Learning Hierarchical Similarity Metrics：2012CVPR，</p><p>[Hierarchical Triplet Loss] Deep Metric Learning with Hierarchical Triplet Loss：2018ECCV，</p><ul><li>Hierarchical classicification应该单独做一个系列，tobeadded</li></ul></li><li><p>一些待明确的问题</p><ul><li>anchor怎么选：facenet中说明，每个mini-batch中每个类别必须都有</li><li>pairs怎么实现（困难的定义）：facenet中说明，hard distance sample in mini-batch</li><li>hingeloss &amp; SVM推导</li><li>常规使用？结合cls loss和metric loss还是只用metric loss：cls loss和metric loss本质上是一样的，都是希望同类样本输出一样，不同类样本输出不一样，只不过前者具有概率意义，后者具有距离意义。上面列出来的只有center loss是要跟cls loss结合起来用的，因为他只针对类内，不足以推动整个模型。</li></ul></li></ol><h2 id="Signature-Verification-using-a-‘Siamese’-Time-Delay-Neural-Network"><a href="#Signature-Verification-using-a-‘Siamese’-Time-Delay-Neural-Network" class="headerlink" title="Signature Verification using a ‘Siamese’ Time Delay Neural Network"></a>Signature Verification using a ‘Siamese’ Time Delay Neural Network</h2><ol><li>动机<ul><li>verification of written signatures </li><li>propose Siamese <ul><li>two identical sub-networks </li><li>joined at their outputs </li><li>measure the distance</li></ul></li><li>verification process<ul><li>a stored feature vector </li><li>a chosen threshold</li></ul></li></ul></li><li>方法<ul><li>network<ul><li>two inputs：extracting features</li><li>two sub-networks：share the same weights</li><li>one output：cosine of the angle between two feature vectors </li><li>target<ul><li>two real signatures：cosine=1.0</li><li>with one forgery：cosine=-0.9 and cosine=-1.0</li></ul></li><li>dataset<ul><li>50% genuine:genuine pairs</li><li>40% genuine:forgery pairs</li><li>10% genuine:zero-effort pairs</li></ul></li></ul></li></ul></li></ol><p><img src="/2020/09/25/metric-learning系列/siamese.png" width="40%;"></p><h2 id="Dimensionality-Reduction-by-Learning-an-Invariant-Mapping"><a href="#Dimensionality-Reduction-by-Learning-an-Invariant-Mapping" class="headerlink" title="Dimensionality Reduction by Learning an Invariant Mapping"></a>Dimensionality Reduction by Learning an Invariant Mapping</h2><ol><li><p>动机</p><ul><li>dimensionality reduction</li><li>propose Dimensionality Reduction by Learning an Invariant Mapping (DrLIM)<ul><li>globally co- herent non-linear function</li><li>relies solely on neighbor- hood relationships</li><li>invariant to certain transformations of the inputs</li></ul></li></ul></li><li><p>论点</p><ul><li>most existing dimensionality reduction techniques <ul><li>they do not produce a function (or a mapping) from input to manifold </li><li>new points with unknown relationships with training samples cannot be processed </li><li>they tend to cluster points in output space </li><li>a uniform distribution in the outer manifolds is desirable </li></ul></li><li>proposed DrLIM <ul><li>globally coherent non-linear function </li><li>neighborhood relationships that are independent from any distance metric</li><li>invariant to complicated non-linear trnasformations <ul><li>lighting changes</li><li>geometric distortions</li></ul></li><li>can be used to map new samples </li></ul></li><li>empoly contrastive loss <ul><li>neighbors are pulled together </li><li>non-neighbors are pushed apart</li></ul></li><li>energy based model <ul><li>euclidean distance  </li><li>approximates the “semantic similarity”of the inputs in input space</li></ul></li></ul></li><li><p>方法</p><ul><li><p>contrastive loss </p><ul><li><p>conventional loss sum over samples</p></li><li><p>contrastive loss sum over pairs $(X_1, X_2, Y)$</p><ul><li>similar pairs：$Y=0$</li><li>dissimilar：$Y=1$</li></ul></li><li><p>euclidean distance </p><ul><li><p>$L = (1-Y)\sum L_s ||G_w(X_1)-G_w(X_2)||_2 + Y\sum L_d ||G_w(X_1)-G_w(X_2)||_2$</p></li><li><p>$L_s$ should results in low values for similar pairs </p></li><li>$L_d$ should results in high values for dissimilar pairs</li><li>exact form：$L(W,Y,X_1,X_2) = (1-Y)\frac{1}{2}(D^2) + (Y)\frac{1}{2} \{max(0,m-D)\}^2$</li></ul><p><img src="/2020/09/25/metric-learning系列/contrastive loss.png" width="40%;"></p></li></ul></li><li><p>spring model analogy</p><ul><li>similar partial loss相当于给弹簧施加了一个恒定的力，向中心点挤压</li><li><p>dissimilar partial loss只对圈内的点施力，推出圈外就不管了</p><p><img src="/2020/09/25/metric-learning系列/spring.png" width="40%;"></p></li></ul></li></ul></li></ol><h2 id="FaceNet-A-Unified-Embedding-for-Face-Recognition-and-Clustering"><a href="#FaceNet-A-Unified-Embedding-for-Face-Recognition-and-Clustering" class="headerlink" title="FaceNet: A Unified Embedding for Face Recognition and Clustering"></a>FaceNet: A Unified Embedding for Face Recognition and Clustering</h2><ol><li><p>动机</p><ul><li><p>face tasks</p><ul><li>face verification: is this the same person</li><li>face recognition: who is the person</li><li>clustering: find common people among the faces</li></ul></li><li><p>learn a mapping</p><ul><li>compact Euclidean space</li><li><p>where the Euclidean distance directly correspond to face similarity</p><p><img src="/2020/09/25/metric-learning系列/face similarity.png" width="40%;"></p></li></ul></li></ul></li><li><p>论点</p><ul><li><p>traditionally training classification layer</p><ul><li><p>generalizes well to new faces？                      <strong>indirectness</strong></p></li><li><p>large dimension feature representation         <strong>inefficiency</strong></p></li><li><p>use siamese pairs</p><ul><li>the loss encourages all faces of one identity to project onto a single point</li></ul></li></ul></li><li><p>this paper</p><ul><li>employ triplet loss<ul><li>target：separate the positive pair from the negative by a distance margin </li><li>allows the faces of one identity to live on a manifold</li></ul></li><li>obtain face embedding<ul><li>l2 norm</li><li>a fixed d-dims hypersphere </li></ul></li><li>large dataset  <ul><li>to attain the appropriate invariances to pose, illumination, and other variational conditions</li></ul></li><li><p>architecture</p><ul><li>explore two different deep network</li></ul><p><img src="/2020/09/25/metric-learning系列/facenet.png" width="40%;"></p></li></ul></li></ul></li><li><p>方法</p><ul><li><p>input：三元组，consist of two matching face thumbnails and a non-matching one </p></li><li><p>ouput：特征描述，a compact 128-D embedding <strong>living on the fixed hypersphere $||f(x)||_2=1$</strong></p></li><li><p>triple-loss</p><ul><li>target：all anchor-pos distances are smaller than any anchor-neg distances with a least margin $\alpha$</li><li>$L = \sum_i^N [||f(x_i^a) - f(x_i^p)||_2^2 - ||f(x_i^a) - f(x_i^n)||_2^2 + \alpha]$</li><li>hard triplets </li></ul></li><li><p>hard samples</p><ul><li>$argmax_{x_i^p}||f(x_i^a) - f(x_i^p)||_2^2$</li><li>$argmin_{x_i^n}||f(x_i^a) - f(x_i^n)||_2^2$</li><li>infeasible to compute over the whole set：mislabelled and poorly imaged faces would dominate the hard positives and negatives </li><li>off-line：use recent checkpoint to compute on a subset</li><li>online：select in mini-batch</li></ul></li><li><p>mini-batch：</p><ul><li>每个类别都必须有正样本</li><li>负样本是randomly sampled </li></ul></li><li><p>hard sample</p><ul><li>use all anchor-positive pairs  </li><li>selecting the hard negatives <ul><li>hardest negatives can lead to bad local minima in early stage</li><li>先pick semi-hard：$||f(x_i^a) - f(x_i^p)||_2^2 &lt; ||f(x_i^a) - f(x_i^n)||_2^2$</li></ul></li></ul></li><li><p>network</p><ul><li>一种straight的网络，引入了1x1 conv先压缩通道</li><li>Inception models：20x fewer params，5x fewer FLOPS</li></ul></li><li><p>metric</p><ul><li>same／different是由a squared L2 distance决定</li><li>因此测试结果是d的函数</li><li>定义true accepts：圈内对的，$TA(d)=\{(i,j)\in P_{same}, with D(x_i,x_j)\leq d\}$</li><li><p>定义false accepts：圈内错的，$FA(d)=\{(i,j)\in P_{diff}, with D(x_i,x_j)\leq d\}$</p></li><li><p>定义validation rate：$VAL(d) = \frac{|TA(d)|}{|P_{same}|}$</p></li><li>定义false accept rate：$FAR(d) = \frac{|FA(d)|}{|P_{diff}|}$</li></ul></li></ul></li></ol><h2 id="A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition"><a href="#A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition" class="headerlink" title="A Discriminative Feature Learning Approach for Deep Face Recognition"></a>A Discriminative Feature Learning Approach for Deep Face Recognition</h2><ol><li><p>动机</p><ul><li>enhance the discriminationative power of the deeply learned features</li><li>joint supervision <ul><li>softmax loss</li><li>center loss</li></ul></li><li>two key learning objectives<ul><li>inter-class dispension </li><li>intra-class compactness </li></ul></li></ul></li><li><p>论点</p><ul><li>face recognition task requirement<ul><li>the learned features need to be not only separable but also discriminative</li><li>generalized enough for the new unseen samples</li></ul></li><li>the softmax loss only encourage the separability of features <ul><li>对分类边界、类内类间分布没有直接约束</li></ul></li><li>contrastive loss &amp; triplet loss <ul><li>training pairs or triplets dramatically grows </li><li>slow convergence and instability </li></ul></li><li>we propose<ul><li>learn a center </li><li>simultaneously update the center and optimize the distances </li><li>joint supervision<ul><li>softmax loss forces the deep features of different classes staying apart </li><li>center loss efficiently pulls the deep features of the same class to their centers </li><li>to be more discriminationative</li></ul></li><li>the inter-class features differences are enlarged </li><li>the intra-class features variations are reduced</li></ul></li></ul></li><li><p>方法</p><ul><li><p>softmax vis</p><ul><li>最后一层hidden layer使用两个神经元</li><li>so that we can directly plot</li><li><p>separable but still show significant intra-class variations</p><p><img src="/2020/09/25/metric-learning系列/softmax.png" width="50%;"></p></li></ul></li><li><p>center loss</p><ul><li><p>$L_c = \frac{1}{2} \sum_1^m ||x_i - c_{y_i}||_2^2$</p></li><li><p>update class center on mini-batch：</p><script type="math/tex; mode=display">  \frac{\partial L_c}{\partial x_i} = x_i - c_{y_i}\\  \Delta c_j = \frac{\sum_i^m \delta (y_i=j) * (c_j - x_i)}{1+\sum_i^m \delta (y_i=j)}</script></li><li><p>joint supervision：</p><script type="math/tex; mode=display">  L = L_{CE} + \lambda L_c</script><p><img src="/2020/09/25/metric-learning系列/joint supervision.png" width="50%;"></p></li></ul></li><li><p>discussion</p><ul><li>necessity of joint supervision <ul><li>solely softmax loss —-&gt; large intra-class variations </li><li>solely center loss —-&gt; features and centers will degraded to zeros </li></ul></li><li>compared to contrastive loss and triplet loss<ul><li>using pairs：suffer from dramatic data expansion </li><li>hard mining：complex recombination </li><li>optimizing target：<ul><li>center loss直接针对intra-class compactness，类内用距离来约束，类间用softmax来约束</li><li>contrastive loss也是直接优化绝对距离，类内&amp;类间都用距离来约束</li><li>triplet loss是建模相对关系，类内&amp;类间都用距离来约束</li></ul></li></ul></li></ul></li><li><p>architecture </p><ul><li>local convolution layer：当数据集在不同的区域有不同的特征分布时，适合用local-Conv，典型的例子就是人脸识别，一般人的面部都集中在图像的中央，因此我们希望当conv窗口滑过这块区域的时候，权重和其他边缘区域是不同的</li><li><p>参数量暴增：kernel_size <em> kernel_size </em> output_size <em> output_size </em> input_channel * output_channel</p><p><img src="/2020/09/25/metric-learning系列/centerlossnet.png" width="60%;"></p></li></ul></li></ul></li><li><p>实验</p><ul><li><p>hyperparam：$\lambda$ and $\alpha$</p><ul><li>fix $\alpha=0.5$ and vary $\lambda$ from 0-0.1</li><li>fix $\lambda=0.003$ and vary $\alpha$ from 0.01-1</li><li><p>结论是remains stable across a large range，没有给出最佳／建议</p><p><img src="/2020/09/25/metric-learning系列/hyper.png" width="50%;"></p></li></ul></li><li><p>我的实验</p><ul><li>加比不加训练慢得多</li><li>在Mnist上测试同样的epoch加比不加准确率低</li><li>之所以Center Loss是针对人脸识别的Loss是有原因的，个人认为<strong>人脸的中心性更强一些</strong>，也就是说一个人的所有脸取平均值之后的人脸我们还是可以辨识是不是这个人，所以Center Loss才能发挥作用</li></ul></li></ul></li></ol><h2 id="Circle-Loss-A-Unified-Perspective-of-Pair-Similarity-Optimization"><a href="#Circle-Loss-A-Unified-Perspective-of-Pair-Similarity-Optimization" class="headerlink" title="Circle Loss: A Unified Perspective of Pair Similarity Optimization"></a>Circle Loss: A Unified Perspective of Pair Similarity Optimization</h2><ol><li><p>动机</p><ul><li>pair similarity</li><li>circular decision boundary</li><li>unify cls-based &amp; metric-based data<ul><li>class-level labels  </li><li>pair-wise labels </li></ul></li></ul></li><li><p>论点</p><ul><li><p>there is no intrinsic difference between softmax loss &amp; metric loss</p><ul><li>minimize between-class similarity $s_n$</li><li>maximize within- class similarity $s_p$</li><li>reduce $s_n - s_p$</li></ul></li><li><p>short-commings</p><ul><li>lack of flexibility：$s_p$和$s_n$的优化速度可能不同，一个快收敛了一个还很差，这时候用同样的梯度去更新就非常inefficient and irrational，就左图来说，下面的点相对上面的点，$s_n$更小（更接近op），$s_p$更小（更远离op），vice versa，但是决策平面对三个点相对于$s_n$和$s_p$的梯度都是一样的（1和-1）。</li><li><p>ambiguous convergence status：用一个hard distance margin来描述decision boundary还不够discriminative，hard decision boundary上各点其实还是有差别的，假设存在一个optimum（$s_p=1 \ \&amp; \ s_n=0$），那么左图决策平面上两个点，相对optimum的意义明显不一样，决策平面应该是个围绕optimum的圆圈。</p><p><img src="/2020/09/25/metric-learning系列/circle loss.png" width="40%;"></p></li></ul></li><li><p>propose circle loss</p><ul><li>independent weighting factors：离optimum越远的penalty strength越大，这一项直接以距离为优化目标的loss都是满足的</li><li>different penalty strength：$s_p$和$s_n$ learn at different paces，类内加权，加权系数是learnable params</li><li>$(\alpha_n s_n - \alpha_p s_p) = m$：yielding a circle shape </li></ul></li></ul></li><li><p>方法</p><ul><li><p>核心：$(\alpha_n s_n - \alpha_p s_p) = m$</p></li><li><p>self-paced weighting</p><ul><li><p>given optimum $O_p$ and $O_n$，for each similarity score：</p><script type="math/tex; mode=display">  \begin{cases}  a_p^i = [O_p - s_p^i]_+ \\  a_n^j = [s_n^j - O_n]_+  \end{cases}</script></li><li><p>cut-off at zero </p></li><li><p>对于远离optimum的点梯度放大，接近optimum的点（快收敛）梯度缩小</p></li><li><p>softmax里面通常不会对同类样本间做这种rescaling的，因为它希望所有样本value都达到贼大</p></li><li><p>Circle loss abandons the interpretation of classifying a sample to its target class with a large probability </p></li></ul></li><li><p>margin</p><ul><li>adding a margin m reinforces the optimization </li><li>take toy scenario <ul><li>最终整理成：$(s_n-0)^2 + (s_p-1)^2 = 2m^2$</li><li>op target：$s_p &gt; 1-m$，$s_n &lt; m$</li><li>relaxation factor $m$：controls the radius of the decision boundary </li></ul></li></ul></li><li><p>unified perspective </p><ul><li>tranverse all the similarity pairs：$\{s_p^i\}^K$和$\{s_n^j\}^N$</li><li>to reduce $(s_n^j - s_p^i)$：$L_{uni}=log[1+\sum^K_i \sum^N_j exp(\lambda (s_n^j - s_p^i + m))]$</li><li>解耦（不会同时是$s_p$和$s_n$）：$L_{uni}=log[1+\sum^N_j exp(\lambda (s_n^j + m))\sum^K_i exp(\lambda (-s_p^i))]$</li><li>given class labels：<ul><li>we get $(N-1)$ between-class similarity scores and $(1)$ within-class similarity score </li><li>分母翻上去：$L = -log \frac{exp(\lambda (s_p-m))}{exp(\lambda (s_p-m)) + \sum^{N-1}_j exp(\lambda (s_n^j))}$</li><li>就是softmax</li></ul></li><li>given pair-wise labels：<ul><li>triplet loss with hard mining：find pairs with large $s_n$ and low $s_p$</li><li>use infinite：$L=lim_{\lambda \to \inf} \frac{1}{\lambda} L_{uni}$</li></ul></li></ul></li></ul></li><li><p>实验</p><ul><li>Face recognition <ul><li>noisy and long-tailed data：去噪并且去掉稀疏样本</li><li>resnet &amp; 512-d feature embeddings &amp; cosine distance </li><li>$\lambda=256$，$m=0.25$</li></ul></li><li>Person re-identification <ul><li>$\lambda=128$，$m=0.25$</li></ul></li><li>Fine-grained image retrieval <ul><li>车集和鸟集</li><li>bn-inception &amp; 512-d embeddings</li><li>P-K sampling</li><li>$\lambda=80$，$m=0.4$</li></ul></li><li>hyper-params<ul><li>the scale factor $\lambda$：<ul><li>determines the largest scale of each similarity score </li><li>Circle loss exhibits high robustness on $\lambda$</li><li>the other two becomes unstable with larger  $\lambda$</li><li>owing to the decay factor</li></ul></li><li>the relaxation factor m：<ul><li>determines the radius of the circular decision boundary </li><li>surpasses the best performance of the other two in full range</li><li>robustness </li></ul></li><li><img src="/2020/09/25/metric-learning系列/circle hyper.png" width="50%;"></li></ul></li></ul></li><li><p>inference</p><ul><li>对人脸类任务，通常用训练好的模型生成一个人脸标准底库，然后每次推理的时候得到测试数据的特征向量，并在标准底库中搜索相似度最高的特征，完成人脸识别过程。</li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 度量学习，loss &amp; network </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>efficient周边</title>
      <link href="/2020/09/23/efficient%E5%91%A8%E8%BE%B9/"/>
      <url>/2020/09/23/efficient%E5%91%A8%E8%BE%B9/</url>
      <content type="html"><![CDATA[<p>因为不是googlenet家族官方出品，所以放在外面</p><p>[EfficientFCN] EfficientFCN: Holistically-guided Decoding for Semantic Segmentation：商汤，主要针对upsampling是局部感受野，重建失真多，分割精度差的问题，提出了Holistically-guided Decoder (HGD) ，用来recover the high-resolution (OS=8) feature maps，想法上接近SCSE-block，数学表达上接近bilinear-CNN，性能提升主要归因于eff back吧。</p><h2 id="EfficientFCN-Holistically-guided-Decoding-for-Semantic-Segmentation"><a href="#EfficientFCN-Holistically-guided-Decoding-for-Semantic-Segmentation" class="headerlink" title="EfficientFCN: Holistically-guided Decoding for Semantic Segmentation"></a>EfficientFCN: Holistically-guided Decoding for Semantic Segmentation</h2><ol><li><p>动机</p><ul><li>Semantic Segmentation <ul><li>dilatedFCN：computational complexity </li><li>encoder-decoder：performance</li></ul></li><li>proposed EfficientFCN <ul><li>common back without dilated convolution </li><li>holistically-guided decoder </li></ul></li><li>balance performance and efficiency</li></ul></li><li><p>论点</p><ul><li>key elements for semantic segmentation<ul><li>high-resolution feature maps </li><li>pre-trained weights</li></ul></li><li>OS32 feature map：the fine-grained structural information is discarded </li><li>dilated convolution：no extra parameters introduced but equire high computational complexity and memory consumption </li><li>encoder-decoder based methods<ul><li>repeated upsampling + skip connection procedure<ul><li>upsampling</li><li>concat／add</li><li>successive convs</li></ul></li><li>Even with the skip connections, lower-level high-resolution feature maps cannot provide abstractive enough features for achieving high- performance segmentation </li><li>The  bilinear upsampling or deconvolution operations are conducted in a local manner(from a limited receptive filed)</li><li>improvements<ul><li>reweight：SE-block</li><li>scales each feature channel but maintains the original spatial size and structures：【scse block对spacial有加权啊】</li></ul></li></ul></li><li><p>propose EfficientFCN</p><ul><li>widely used classification model </li><li>Holistically-guided Decoder (HGD) <ul><li>take OS8, OS16, OS32 feature maps from backbone</li><li>OS8和OS16用来spatially guiding the feature upsampling process </li><li>OS32用来encode the global context然后基于guidance进行上采样</li><li>linear assembly at each high-resolution spatial location：感觉就是对上采样特征图做了加权</li></ul></li></ul><p><img src="/2020/09/23/efficient周边/FCN.png" width="50%;"></p></li></ul></li><li><p>方法</p><ul><li><p>Holistically-guided Decoder </p><ul><li>multi-scale feature fusion </li><li>holistic codebook generation<ul><li>from high-level feature maps </li><li>holistic codewords：without any spatial order </li></ul></li><li><p>codeword assembly </p><p><img src="/2020/09/23/efficient周边/hgd.png" width="70%;"></p></li></ul></li><li><p>multi-scale feature fusion </p><ul><li>we observe the fusion of multi-scale feature maps generally result in better performance </li><li>compress：separate 1x1 convs</li><li>bilinear downsamp／upsamp</li><li>concatenate</li><li>fused OS32 $m_{32}$ &amp; fused OS8 $m_8$</li></ul></li><li><p>holistic codebook generation</p><ul><li>from $m_{32}$</li><li>two separate 1x1 conv<ul><li>a codeword based map $B \in R^{1024<em>(H/32)</em>(W/32)}$：每个位置用一个1024-dim的vector来描述</li><li>n spatial weighting map $A\in R^{n<em>(H/32)</em>(W/32)}$：highlight 特征图上不同区域<ul><li>softmax norm in spatial-dim</li><li>$\widetilde A_i(x,y)=\frac{exp(A_i(x,y))}{\sum_{p,q} exp(A_i(p,q))}, i\in [0,n)$</li></ul></li></ul></li><li>codeword $c_i \in R^{1024}$<ul><li>global description for each weighting map </li><li>weighted average of B on all locations</li><li>$c_i = \sum_{p,q} \widetilde A_i(p,q) B(p,q)$</li><li>each codeword captures certain aspect of the global context </li></ul></li><li>orderless high-level global features $C \in R^{1024*n}$<ul><li>$C = [c_1, …, c_n]$</li></ul></li></ul></li><li><p>codeword assembly </p><ul><li>raw guidance map $G \in R^{1024<em>(H/8)</em>(W/8)}$：1x1 conv on $m_8$</li><li>fuse semantic-rich feature map $\overline B \in R^{1024}$：global average vector </li><li>novel guidance feature map $\overline G = G \oplus \overline B $：location-wise addition【？？？？】</li><li>linear assembly weights of the n codewords $W \in R^{n<em>(H/8)</em>(W/8)}$：1x1 conv on $\overline G$</li><li>holistically-guided upsampled feature $\tilde f_8 = W^T C$：reshape &amp; dot</li><li>final feature map $f_8$：concat $\tilde f_8$ and $G$</li></ul></li><li><p>final segmentation</p><ul><li>1x1 conv</li><li>further upsampling </li></ul></li></ul></li><li><p>实验</p><ul><li><p>numer of holistic codewords</p><ul><li>32-512：increase</li><li>512-1024：slight drop</li><li><p>we observe the number of codewords needed is approximately 4 times than the number of classes </p><p><img src="/2020/09/23/efficient周边/n.png" width="50%;"></p></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 语义分割 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>data aug</title>
      <link href="/2020/09/18/data-aug/"/>
      <url>/2020/09/18/data-aug/</url>
      <content type="html"><![CDATA[<p>[mixup] mixup: BEYOND EMPIRICAL RISK MINIMIZATION：对不同类别的样本，不仅可以作为数据增广手段，还可以用于semi-supervised learning（MixMatch）</p><p>[mixmatch] MixMatch: A Holistic Approach to Semi-Supervised Learning：针对半监督数据的数据增广</p><p>[mosaic] from YOLOv4</p><p>[AutoAugment] AutoAugment: Learning Augmentation Policies from Data：google</p><p>[RandAugment] RandAugment: Practical automated data augmentation with a reduced search space：google</p><h2 id="RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space"><a href="#RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space" class="headerlink" title="RandAugment: Practical automated data augmentation with a reduced search space"></a>RandAugment: Practical automated data augmentation with a reduced search space</h2><ol><li><p>动机</p><ul><li>AutoAugment<ul><li>separate search phase</li><li>run on a subset of a huge dataset</li><li>unable to adjust the regularization strength based on model or dataset size</li></ul></li><li><p>RandAugment</p><ul><li>significantly reduced search space</li><li>can be used uniformly across tasks and datasets</li><li>match or exceeds the previous val acc</li></ul><p><img src="/2020/09/18/data-aug/acc.png" width="50%"></p></li></ul></li><li><p>方法</p><ul><li><p>formulation</p><ul><li><p>always select a transformation with uniform prob $\frac{1}{K}$</p></li><li><p>given N transformations for an image：there are $K^N$ potential policies</p></li><li><p>fixied magnitude schedule M：we choose Constant，因为只要一个hyper</p><p>  <img src="/2020/09/18/data-aug/magnitude.png" width="50%"></p></li><li><p>run naive grid search</p></li></ul></li><li><p>疑问：这样每个op等概率，就不再data-specific了，也看不出自然图像更prefer color transformation这种结论了</p></li></ul></li></ol><h2 id="AutoAugment-Learning-Augmentation-Policies-from-Data"><a href="#AutoAugment-Learning-Augmentation-Policies-from-Data" class="headerlink" title="AutoAugment: Learning Augmentation Policies from Data"></a>AutoAugment: Learning Augmentation Policies from Data</h2><ol><li><p>动机</p><ul><li><p>search for data augmentation policies</p></li><li><p>propose AutoAugment</p><ul><li>create a search space composed of augmentation sub-policies<ul><li>one sub-policy is randomly choosed per image per mini-batch</li><li>a sub-policy consists of two base operations </li></ul></li><li><p>find the best policy：yields the highest val acc on the target dataset</p></li><li><p>the learned policy can transfer</p></li></ul></li></ul></li><li><p>论点</p><ul><li>data augmentation<ul><li>to teach a model about invariance</li><li>in data domain is easier than hardcoding it into model architecture</li><li>currently dataset-specific and often do not transfer：<ul><li>MNIST：elastic distortions, scale, translation, and rotation </li><li>CIFAR &amp; ImageNet：random cropping, image mirroring and color shifting / whitening </li><li>GAN：直接生成图像，没有归纳policy</li></ul></li></ul></li><li>we aim to automate the process of finding an effective data augmentation policy for a target dataset<ul><li>each policy：<ul><li>operations in certain order</li><li>probabilities after applying</li><li>magnitudes </li></ul></li><li>use reinforcement learning as the search algorithm</li></ul></li><li>contributions<ul><li>SOTA on CIFAR &amp; ImageNet &amp; SVHN </li><li>new insight on transfer learning：使用预训练权重没有显著提升的dataset上，使用同样的aug policies则会涨点</li></ul></li></ul></li><li><p>方法</p><ul><li><p>formulation</p><ul><li><p>search space of policies</p><ul><li>policy：a policy consists of 5 sub-policies </li><li>sub-policy：each sub-policy consisting of two image operations</li><li>operation：each operation is also associated with two hyperparameters<ul><li>probability：of applying the operation，uniformly discrete into 11 values</li><li>magnitude：of the operation，uniformly discrete into 10 values</li></ul></li><li>a mini-batch share the same chosen sub-policy</li></ul></li><li><p>operations：16 in total，mainly use PIL</p><p>  <img src="/2020/09/18/data-aug/operations.png" width="80%"></p><ul><li><a href="https://blog.csdn.net/u011583927/article/details/104724419有各种operation的可视化效果" target="_blank" rel="noopener">https://blog.csdn.net/u011583927/article/details/104724419有各种operation的可视化效果</a></li><li>shear是砍掉图像一个角的畸变</li><li>equalize是直方图均衡化</li><li>solarize是基于一定阈值的invert，高于阈值invert，低于阈值不变</li><li>posterize也是一种像素值截断操作</li><li>color是调整饱和度，mag&lt;1趋近灰度图</li><li>sharpness决定图像模糊/锐化</li><li>sample pairing：两张图加权求和，但是不改变标签</li></ul></li><li><p>searching goal</p><ul><li>with $(16<em>10</em>11)^2$ choices of sub-policies</li><li>we want 5</li></ul></li></ul></li><li><p>example</p><ul><li>一个sub-policy包含两个operation</li><li>每个operation有一定的possibility做/不做</li><li><p>每个operation有一定的magnitude决定做后的效果</p><p><img src="/2020/09/18/data-aug/example.png" width="60%"></p></li></ul></li></ul></li><li><p>结论</p><ul><li><p>On CIFAR-10, AutoAugment picks mostly color-based transformations </p></li><li><p>on ImageNet, AutoAugment focus on color-based transformations as well, besides geometric transformation and rotate is commonly used </p><ul><li><p>one of the best policy</p><p>  <img src="/2020/09/18/data-aug/imagenet.png" width="80%"></p></li><li><p>overall results</p><p>  <img src="/2020/09/18/data-aug/imagenet2.png" width="60%"></p></li></ul></li></ul></li></ol><h2 id="mixup-BEYOND-EMPIRICAL-RISK-MINIMIZATION"><a href="#mixup-BEYOND-EMPIRICAL-RISK-MINIMIZATION" class="headerlink" title="mixup: BEYOND EMPIRICAL RISK MINIMIZATION"></a>mixup: BEYOND EMPIRICAL RISK MINIMIZATION</h2><ol><li><p>动机</p><ul><li>classification task</li><li>memorization and sensitivity issue<ul><li>reduces the memorization of corrupt labels</li><li>increases the robustness to adversarial examples</li><li>improves the generalization </li><li>can be used to stabilize the training of GANs  </li></ul></li><li>propose convex combinations of pairs of examples and their labels</li></ul></li><li><p>论点</p><ul><li><p>ERM(Empirical Risk Minimization)：issue of generalization</p><ul><li>allows large neural networks to <em>memorize</em> (instead of generalize from) the training data even in the presence of strong regularization </li><li>neural networks change their predictions drastically when evaluated on examples just outside the training distribution </li></ul></li><li><p>VRM(Vicinal Risk Minimization)：introduce data augmentation</p><ul><li>e.g. define the vicinity of one image as the set of its horizontal reflections, slight rotations, and mild scalings </li><li>vicinity share the same class </li><li>does not model the vicinity relation across examples of different classes </li></ul></li><li>ERM中的training set并不是数据的真实分布，只是用有限数据来近似真实分布，memorization也会最小化training error，但是对training seg以外的sample就leads to undesirable behaviour </li><li>mixup就是VRM的一种，propose a generic vicinal distribution，补充vicinity relation across examples of different classes </li></ul></li><li><p>方法</p><ul><li><p>mixup</p><ul><li><p>constructs virtual training examples </p><script type="math/tex; mode=display">  x = \lambda x_i + (1-\lambda)x_j  \\  y = \lambda y_i + (1-\lambda)y_j</script></li><li><p>use two examples drawn at random：raw inputs &amp; raw one-hot labels</p></li><li><p>理论基础：linear interpolations of feature vectors should lead to linear interpolations of the associated targets </p></li><li><p>hyper-parameter $\alpha$</p><ul><li>$\lambda = np.random.beta(\alpha, \alpha)$</li></ul></li><li>controls the strength of interpolation  </li></ul></li><li><p>初步结论</p></li><li><p>three or more examples mixup does not provide further gain but more computation </p></li><li><p>interpolating only between inputs with equal label did not lead to the performance gains </p></li><li><p><strong>key elemets——two inputs with different label</strong></p></li><li><p>vis</p><ul><li><p>decision boundaries有了一个线性过渡</p><p>  <img src="/2020/09/18/data-aug/boundary.png" width="30%"></p></li><li><p>更准确 &amp; 梯度更小：error少所以loss小所以梯度小？？</p><p>  <img src="/2020/09/18/data-aug/gradient.png" width="60%"></p></li></ul></li></ul></li><li><p>实验</p><ul><li><p>初步分类实验</p><ul><li>$\alpha \in [0.1, 0.4]$ leads to improved performance，largers leads to underfitting</li><li>models with higher capacities and/or longer training runs are the ones to benefit the most from mixup</li></ul></li><li><p>memorization of corrupted labels</p><ul><li>将数据集中一部分label换成random noise </li><li>ERM直接过拟合，在corrupted sample上面training error最小，测试集上test error最大</li><li>dropout有效防止过拟合，但是mixup outperforms它</li><li><p>corrupted label多的情况下，dropout+mixup performs the best</p><p><img src="/2020/09/18/data-aug/memorization.png" width="60%"></p></li></ul></li><li><p>robustness to adversarial examples</p><ul><li>Adversarial examples are obtained by adding tiny (visually imperceptible) perturbations  </li><li>常规操作data augmentation：produce and train on adversarial examples </li><li>add significant computational：样本数量增多，梯度变化大</li><li>mixup results in a smaller loss and gradient norm：因为mixup生成的假样本“更合理一点”，梯度变化更小</li></ul></li><li><p>ablation study</p><ul><li><p>mixup is the best：绝对领先第二mix input + label smoothing</p></li><li><p>the effect of regularization </p><ul><li>ERM需要大weight decay，mixup需要小的——说明mixup本身的regularization effects更强</li><li>高层特征mixup需要更大的weight decay——随着层数加深regularization effects减弱</li><li><p>AC+RP最强</p></li><li><p>label smoothing和add Gaussian noise to inputs 相对比较弱</p></li><li>mix inputs only(SMOTE) shows no gain</li></ul></li></ul><p><img src="/2020/09/18/data-aug/ablation.png" width="60%"></p></li></ul></li></ol><h2 id="MixMatch-A-Holistic-Approach-to-Semi-Supervised-Learning"><a href="#MixMatch-A-Holistic-Approach-to-Semi-Supervised-Learning" class="headerlink" title="MixMatch: A Holistic Approach to Semi-Supervised Learning"></a>MixMatch: A Holistic Approach to Semi-Supervised Learning</h2><ol><li><p>动机</p><ul><li>semi-supervised learning</li><li>unify previous methods</li><li>proposed mixmatch<ul><li>guessing low-entropy labels </li><li>mixup labeled and unlabeled data </li></ul></li><li>useful for differentially private learning</li></ul></li><li><p>论点</p><ul><li>semi-supervised learning add a loss term computed on unlabeled data and encourages the model to generalize better to unseen data </li><li>the loss term<ul><li>entropy minimization：decision boundary应该尽可能远离数据簇，因此prediction on unlabeled data也应该是high confidence</li><li>consistency regularization：增强前后的unlabeled data输出分布一致</li><li>generic regularization：weight decay &amp; mixup</li></ul></li><li>MixMatch unified all above<ul><li>introduces a unified loss term for unlabeled data  </li></ul></li></ul></li><li><p>方法</p><ul><li><p>overview</p><ul><li>given：a batch of labeled examples $X$ and a batch of labeled examples $U$</li><li>augment+label guess：a batch of augmented labeled examples $X^{‘}$ and a batch of augmented labeled examples $U^{‘}$</li><li>compute：separate labeled and unlabeled loss terms $L_X$ and $L_U$</li><li><p>combine：weighted sum</p><p><img src="/2020/09/18/data-aug/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/data-aug/overview.png" width="50%;"></p></li></ul></li><li><p>MixMatch</p><ul><li><p>data augmentation</p><ul><li>常规augmentation</li><li>作用于每一个$x_b$和$u_b$</li><li>$u_b$做$K$次增强</li></ul></li><li><p>label guessing</p><ul><li>对增强的$K$个$u_b$分别预测，然后取平均</li><li>average class prediction </li></ul></li><li><p>sharpening</p><ul><li>reduce the entropy of the label distribution </li><li>拉高最大prediction，拉小其他的</li><li>$Sharpen (p, T)_i =\frac{p_i^{\frac{1}{T}}}{\sum^{N}_j p_j^{\frac{1}{T}}} $</li><li>$T$趋近于0的时候，processed label就接近one-hot了</li></ul></li><li><p>mixup</p><ul><li><p>slightly modified form of mixup to make the generated sample being more closer to the original</p><p><img src="/2020/09/18/data-aug/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/data-aug/mixup.png" width="50%;"></p></li></ul></li><li><p>loss function</p><ul><li>labeled loss：typical cross-entropy loss  </li></ul></li><li><p>unlabeled loss：<strong>squared L2，bounded and less sensitive to completely incorrect predictions</strong></p></li></ul></li><li><p>hyperparameters</p><ul><li><p>sharpening temperature $T$：fixed 0.5</p><ul><li>number of unlabeled augmentations $K$：fixed 2</li><li>MixUp Beta parameter $\alpha$：0.75 for start</li><li>unsupervised loss weight $\lambda_U$：100 for start</li></ul></li><li><p>Algorithm </p><p>  <img src="/2020/09/18/data-aug/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/data-aug/mixmatch.png" width="70%;"></p></li></ul></li></ul></li><li><p>实验</p></li></ol><p>[mosaic] from YOLOv4</p>]]></content>
      
      
        <tags>
            
            <tag> 数据增强，样本不平衡，半监督学习，度量学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hrnet</title>
      <link href="/2020/09/18/hrnet/"/>
      <url>/2020/09/18/hrnet/</url>
      <content type="html"><![CDATA[<ol><li><p>动机</p><ul><li>human pose estimation</li><li>high-resolution representations through<ul><li>existing methods recover high-res feature from the low</li><li>this methods maintain the high-res from start to the end</li><li>repeated multi-scale fusions</li></ul></li><li>more accurate and spatially more precise</li><li>estimate on the high-res output</li></ul></li><li><p>论点</p><ul><li>in parallel rather than in series：potentially spatially more precise</li><li>repeated multi- scale fusions：boost both high&amp;low representations，more accurate</li><li>pose estimation <ul><li>probabilistic graphical model </li><li>regression</li><li>heatmap</li></ul></li><li>High-to-low and low-to-high <ul><li>Symmetric high-to-low and low-to-high </li><li>Heavy high-to-low and light low-to-high </li><li>Heavy high-to-low with dilated convolutions and further lighter low-to-high </li></ul></li></ul></li><li><p>方法</p><ul><li><p>overview</p><p>  <img src="/2020/09/18/hrnet/hrnet.png" width="50%;"></p><ul><li>four stages </li><li>channels double when halve the res</li><li>1st stage<ul><li>4 residual units，bottleneck resblock</li><li>width=64</li><li>3x3 conv reducing width to C</li></ul></li><li>2、3、4 stages<ul><li>contain 1, 4, 3 exchange blocks respectively </li><li>exchange block<ul><li>conv：4 residual units，two 3x3 conv</li><li>exchange unit</li></ul></li></ul></li><li>width<ul><li>C：width of the high-resolution subnetworks in last three stages</li><li>other three parallel subnetworks <ul><li>HRNet-W32：64, 128, 256</li><li>HRNet-W48：96, 192, 384  </li></ul></li></ul></li></ul></li><li><p>parallel multi-resolution subnetworks</p><ul><li>for one stage，one produce features with same-res and a extra lower one</li><li><p>一个parallel代表一个scale</p><p><img src="/2020/09/18/hrnet/parallel.png" width="30%;"></p></li></ul></li><li><p>repeated multi-scale fusion</p><ul><li>exchange unit </li><li><p>multi-scale</p><ul><li>downsampling：3x3 s2 conv</li><li>upsampling：nearest neighbor+1x1 conv</li><li>identify connection </li><li>more factors：consecutive layers</li></ul><p><img src="/2020/09/18/hrnet/exchange unit.png" width="50%;"></p></li><li><p>exchange block</p><ul><li>3 parallel convolution units + an exchange unit </li><li><p>可以连续几个block</p><p><img src="/2020/09/18/hrnet/exchange block.png" width="45%;"></p></li></ul></li></ul></li><li><p>heatmap estimation</p><ul><li>from the last high-res exchange unit</li><li>mse</li><li>gt gassian map：std=1</li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 高分辨率，人体姿态估计 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>bilinear CNN</title>
      <link href="/2020/09/18/bilinear-CNN/"/>
      <url>/2020/09/18/bilinear-CNN/</url>
      <content type="html"><![CDATA[<p>17年的paper，引用量15，提出了网路结构，但是没分析为啥有效，垃圾</p><h2 id="Bilinear-CNNs-for-Fine-grained-Visual-Recognition"><a href="#Bilinear-CNNs-for-Fine-grained-Visual-Recognition" class="headerlink" title="Bilinear CNNs for Fine-grained Visual Recognition"></a>Bilinear CNNs for Fine-grained Visual Recognition</h2><ol><li><p>动机</p><ul><li>fine-grained classification </li><li>propose a pooled outer product of features derived from two CNNs <ul><li>2 CNNs</li><li>a bilinear layer</li><li>a pooling layer</li></ul></li><li>outperform existing models and fairly efficient </li><li>effective at other image classification tasks such as material, texture, and scene recognition </li></ul></li><li><p>论点</p><ul><li>fine-grained classification tasks require<ul><li>recognition of highly localized attributes of objects </li><li>while being invariant to their pose and location in the image </li></ul></li><li>previous techniques <ul><li>part-based models<ul><li>construct representations by localizing parts </li><li>more accurate but requires part annotations </li></ul></li><li>holistic models<ul><li>construct a representation of the entire image </li><li>texture descriptors：FV，SIFT </li></ul></li><li>STN：augment CNNs with parameterized image transformations </li><li>attention：use segmentation as a weakly-supervised manner  </li></ul></li><li>Our key insight is that several widely-used texture representations can be written as a pooled outer product of two suitably designed features <ul><li>several widely-used texture representations</li><li>two suitably designed features </li></ul></li><li>the bilinear features are highly redundant <ul><li>dimensionality reduction </li><li>trade-off between accuracy </li></ul></li><li><p>We also found that feature normalization and domain-specific fine-tuning offers additional benefits</p></li><li><p>combination</p><ul><li>concatenate：additional parameters to fuse</li><li>an outer product：no parameters </li><li>sum product：can achieve similar approximations  </li></ul></li><li>“two-stream” architectures <ul><li>one used to model two- factor variations such as “style” and “content” for images </li><li>in our case is to model two factor variations in location and appearance of parts：但并不是explicit modeling因为最终是个分类头</li><li>one used to analyze videos modeling the temporal aspect and the spatial aspect </li></ul></li><li>dimension reduction <ul><li>two 512-dim feature results in 512x512-dim</li><li>earlier work projects one feature to a lower-dimensional space, e.g. 64-dim—&gt;512x64-dim</li><li>we use compact bilinear pooling to generate low-dimensional embeddings (8-32x)</li></ul></li></ul></li><li><p>方法</p><ul><li><p>architecture</p><p>  <img src="/2020/09/18/bilinear-CNN/BCNN.png" width="50%;"></p><ul><li>input $(l,I)$：takes an image and a location，location generally contains position and scale </li><li>quadruple $B=(f_A, f_B, P, C)$</li><li>A、B两个CNN：conv+pooling layers，</li><li>P：pooling function<ul><li>combined A&amp;B outputs using the matrix outer product</li><li>average pooling</li></ul></li><li>C：logistic regression or linear SVM <ul><li>we found that linear models are effective on top of bilinear features</li></ul></li></ul></li><li><p>CNN</p><ul><li><p>independent／partial shared／fully shared</p><p><img src="/2020/09/18/bilinear-CNN/CNN.png" width="70%;"></p></li></ul></li><li><p>bilinear combination </p><ul><li><p>for each location</p></li><li><p>$bilinear(l,I,f_A,f_B)=f_A(l,I)^T f_B(l,I)$</p></li><li><p>pooling function combines bilinear features across all locations</p></li><li><p>$\Phi (I) = \sum_{l\in L} bilinear(l,I,f_A,f_B)$</p></li><li><p>same feature dimension K for A &amp; B，e.g. KxM &amp; KxN respectively，$\Phi(I)$ is size MxN</p></li><li><p>Normalization</p><ul><li>a signed square root：$y=sign(x)\sqrt {|x|}$</li><li><p>follow a l2 norm：$z = \frac{y}{||y||_2}$</p></li><li><p>improves performance in practice  </p></li></ul></li></ul></li><li><p>classification </p><ul><li>logistic regression or linear SVM  </li><li>we found that linear models are effective on top of bilinear features</li></ul></li><li><p>back propagation</p><ul><li><p>$\frac{dl}{dA}=B(\frac{dl}{dx})^T$，$\frac{dl}{dB}=A(\frac{dl}{dx})^T$</p></li><li><p><img src="/2020/09/18/bilinear-CNN/flow.png" width="50%;"></p></li></ul></li><li><p>Relation to classical texture representations：放在这一节撑篇幅？？</p><ul><li>texture representations can be defined by the choice of the local features, the encoding function, the pooling function, and the normalization function <ul><li>choice of local features：orderless aggregation with sum／max operation</li><li>encoding function：A non-linear encoding is typically applied to the local feature before aggregation</li><li>normalization：normalization of the aggregated feature is done to increase invariance </li></ul></li><li>end-to-end trainable</li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 细粒度，特征融合 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>label smoothing</title>
      <link href="/2020/09/14/label-smoothing/"/>
      <url>/2020/09/14/label-smoothing/</url>
      <content type="html"><![CDATA[<ol><li><p>动机</p><ul><li>to understand label smoothing<ul><li>improving generalization </li><li>improves model calibration </li><li>changes the representations learned by the penultimate layer of the network</li><li>effect on knowledge distillation of a student network</li></ul></li><li>soft targets：a hard target and the uniform distribution of other classes</li></ul></li><li><p>论点</p><ul><li>label smoothing implicitly calibrates the learned models  <ul><li>能让confidences更有解释性——more aligned with the accuracies of their predictions</li><li>label smoothing impairs distillation——teacher用了label smoothing，student会表现变差，this adverse effect results from loss of information in the digits</li></ul></li></ul></li><li><p>方法</p><ul><li><p>modeling</p><ul><li>penultimate layer：fc with activation<ul><li>$p_k = \frac{e^{wx}}{\sum e^{wx}}$</li></ul></li><li>outputs：loss<ul><li>$H(y,p)=\sum_{k=1}^K -y_klog(p_k)$</li></ul></li><li>hard targets：$y_k$ is 1 for the correct class and 0 for the rest</li><li>label smoothing：$y_k^{LS} = y_k(1-\alpha)+ \alpha /K$</li></ul></li><li><p>visualization schem </p><ul><li>将dimK activation vector投影到正交平面上，a dim2 vector per example</li><li><p>clusters are much tighter because label smoothing encourages that each example in training set to be equidistant from all the other class’s templates</p></li><li><p>3 classes shows triangle structure since ‘equidistant’</p></li><li>predictions‘ absolute values are much bigger without LM, representing over-confident</li><li>semantically similar classes are harder to separate，但是总体上cluster形态还是好一点</li><li>training without label smoothing there is continuous degree of change between two semantically similar classes，用了LM以后就观察不到了——相似class之间的语义相关性被破坏了，’erasure of information’</li><li>have similar accuracies despite qualitatively different clustering，对分类精度的提升不明显，但是从cluster形态上看更好看</li></ul></li><li><p>model calibration </p><ul><li><p>making the confidence of its predictions more accurately represent their accuracy </p></li><li><p>metric：expected calibration error (ECE) </p></li><li><p>reliability diagram </p><p>  <img src="/2020/09/14/label-smoothing/calibration.png" width="70%"></p></li><li><p>better calibration compared to the unscaled network </p></li><li><p>Despite trying to collapse the training examples to tiny clusters, these networks generalize and are calibrated：在训练集上的cluster分布非常紧凑，encourage每个样本都和其他类别的cluster保持相同的距离，但是在测试集上，样本的分布就比较松散了，不会限定在小小的一坨内，说明网络没有over-confident，representing the full range of confidences for each prediction </p></li></ul></li><li><p>knowledge distillation </p><ul><li><p>even when label smoothing improves the accuracy of the teacher network, teachers trained with label smoothing produce inferior student networks </p></li><li><p>As the representations collapse to small clusters of points, much of the information that could have helped distinguish examples is lost </p><p>  <img src="/2020/09/14/label-smoothing/knowledge.png" width="40%"></p></li><li><p>看training set的scatter，LM会倾向于将一类sample集中成为相似的表征，sample之间的差异性信息丢了：Therefore a teacher with better accuracy is not necessarily the one that distills better</p></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 分类，loss </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>noisy student</title>
      <link href="/2020/09/11/noisy-student/"/>
      <url>/2020/09/11/noisy-student/</url>
      <content type="html"><![CDATA[<h2 id="Self-training-with-Noisy-Student-improves-ImageNet-classification"><a href="#Self-training-with-Noisy-Student-improves-ImageNet-classification" class="headerlink" title="Self-training with Noisy Student improves ImageNet classification"></a>Self-training with Noisy Student improves ImageNet classification</h2><ol><li><p>动机</p><ul><li>semi-supervised learning（SSL）</li><li>semi-supervised approach when labeled data is abundant</li><li>use unlabeled images to improve SOTA model</li><li>improve self-training and distillation</li><li>accuracy and robustness  </li><li>better acc, mCE, mFR<ul><li>EfficientNet model on labeled images</li></ul></li><li>student<ul><li>even or larger student model</li><li>on labeled &amp; pseudo labeled images</li><li>noise, stochastic depth, data augmentation</li><li>generalizes better</li></ul></li><li>process iteration<ul><li>by putting back the student as the teacher</li></ul></li></ul></li><li><p>论点</p><ul><li>supervised learning which requires a large corpus of labeled images to work well </li><li>robustness<ul><li>noisy data：unlabeled images that do not belong to any category in ImageNet</li><li>large margins on much harder test sets </li></ul></li><li>training process<ul><li>teacher<ul><li>EfficientNet model on labeled images</li></ul></li><li>student<ul><li>even or larger student model</li><li>on labeled &amp; pseudo labeled images</li><li>noise, stochastic depth, data augmentation</li><li>generalizes better</li></ul></li><li>process iteration<ul><li>by putting back the student as the teacher</li></ul></li></ul></li><li>improve in two ways<ul><li>it makes the student larger：因为用了更多数据</li><li>noised student is forced to learn harder：因为label有pseudo labels，input有各类augmentation，网络有dropout／stochastic depth</li></ul></li><li>main difference compared with Knowledge Distillation <ul><li>use noise ——— KD do not use</li><li>use equal/larger student ——— KD use smaller student to learn faster</li></ul></li><li>think of as Knowledge Expansion<ul><li>giving the student model enough capacity and difficult environments</li><li>want the student to be better than the teacher </li></ul></li></ul></li><li><p>方法</p><ul><li>algorithm<ul><li>train teacher use labeled images</li><li>use teacher to inference unlabedled images, generating pseudo labels, soft/one-hot</li><li>train student model use labeled &amp; unlabeld images</li><li>make student the new teacher, jump to the inter step</li></ul></li><li>noise<ul><li>enforcing invariances：要求student网络能够对各种增强后的数据预测label一样，ensure consistency </li><li>required to mimic a more powerful ensemble model：teacher网络在inference阶段进行dropout和stochastic depth，behaves like an ensemble，whereas the student behaves like a single model，这就push student网络去学习一个更强大的模型</li></ul></li><li>other techniques<ul><li>data filtering  <ul><li>we filter images that the teacher model has low confidences </li><li>这部分data与training data的分布范围内</li></ul></li><li>data balancing <ul><li>duplicate images in classes where there are not enough images </li><li>take the images with the highest confidence when there are too many</li></ul></li></ul></li><li>soft／hard pseudo labels  <ul><li>both work</li><li>soft slightly better </li></ul></li></ul></li><li><p>实验</p><ul><li>dataset<ul><li>benchmarked dataset：ImageNet 2012 ILSVRC </li><li>unlabeled dataset：JFT</li><li>fillter &amp; balancing：<ul><li>use EfficientNet-B0</li><li>trained on ImageNet，inference over JFT</li><li>take images with confidence over 0.3</li><li>130M at most per class</li></ul></li></ul></li><li>models<ul><li>EfficientNet-L2<ul><li>further scale up EfficientNet-B7</li><li>wider &amp; deeper</li><li>lower resolution</li><li><img src="/2020/09/11/noisy-student/l2.png" width="50%"></li><li>train-test resolution discrepancy <ul><li>first perform normal training with a smaller resolution for 350 epochs</li><li>then finetune the model with a larger resolution for 1.5 epochs on unaugmented labeled images</li><li>shallow layers are fixed during finetuning</li></ul></li></ul></li><li>noise<ul><li>stochastic depth：stochastic depth 0.8 for the final layer and follow the linear decay rule for other layers </li><li>dropout：dropout 0.5 for the final layer</li><li>RandAugment：two random operations with magnitude set to 27</li></ul></li></ul></li><li><p>iterative training</p><ul><li>【teacher】first trained an EfficientNet-B7 on ImageNet</li><li>【student】then trained an EfficientNet-L2 with the unlabeled batch size set to 14 times the labeled batch size</li><li>【new teacher】trained a new EfficientNet-L2 </li><li>【new student】trained an EfficientNet-L2 with the unlabeled batch size set to 28 times the labeled batch size</li><li>【iteration】…</li><li><img src="/2020/09/11/noisy-student/iteration.png" width="50%"></li></ul></li><li><p>robustness test</p><ul><li>difficult images </li><li><p>common corruptions and perturbations  </p><p><img src="/2020/09/11/noisy-student/robustness.png" width="70%"></p></li><li><p>FGSM attack </p></li><li><p>metrics</p><ul><li>improves the top-1 accuracy </li><li><p>reduces mean corruption error (mCE) </p></li><li><p>reduces mean flip rate (mFR) </p></li></ul></li></ul></li><li><p>ablation study</p><ul><li>noisy<ul><li>如果不noise the student，当student model的预测和teacher预测的unlabeled数据完全一样的情况下，loss为0，不再学习，这样student就不能outperform teacher了</li><li>injecting noise to the student model enables the teacher and the student to make different predictions </li><li>The student performance consistently drops with noise function removed </li><li>removing noise leads to a smaller drop in training loss，说明noise的作用不是为了preventing overfitting，就是为了enhance model</li></ul></li><li>iteration<ul><li>iterative training is effective in producing increas- ingly better models </li><li>larger batch size ratio for latter iteration</li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> classification, semi-supervised, teacher-student </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>complement cross entropy</title>
      <link href="/2020/09/08/complement-cross-entropy/"/>
      <url>/2020/09/08/complement-cross-entropy/</url>
      <content type="html"><![CDATA[<ol><li>summary<ul><li>使用complement loss的主要动机是one-hot的label下，ce只关注拉高正样本概率，丧失掉了其他incorrect类别的信息</li><li>事实上对于incorrect类别，可以让其输出概率值分布的熵尽可能的大——也就是将这个分布尽可能推向均匀分布，让它们之间互相遏制从而凸显出ground truth的概率</li><li>但这是建立在“各个标签之间相互独立”这个假设上，如果类别间有hierarchical的关系／multi-label，就不行了。</li><li>在数学表达上，<ul><li>首先仍然是用ce作用于correct label，希望正样本概率gt_pred尽可能提高，接近真实值</li><li>然后是作用于incorrect label的cce，在除了正例pred possibility以外的几个概率上，计算交叉熵，希望这几个概率尽可能服从均匀分布，概率接近$\frac{1-gt_pred}{K-1}$</li><li>我感觉这就是label smoothing，主要区别就是cce上有个norm项，label smoothin在计算ce的时候，vector中每一个incorrect label的熵都与correct label等权重，cce对整个incorrect vector的权重与correct label等同，且可以调整。</li></ul></li></ul></li></ol><h2 id="Imbalanced-Image-Classification-with-Complement-Cross-Entropy"><a href="#Imbalanced-Image-Classification-with-Complement-Cross-Entropy" class="headerlink" title="Imbalanced Image Classification with Complement Cross Entropy"></a>Imbalanced Image Classification with Complement Cross Entropy</h2><ol><li><p>动机</p><ul><li>class-balanced datasets</li><li>motivated by COT(complement objective training)<ul><li>suppressing softmax probabilities on incorrect classes during training  </li></ul></li><li>propose cce<ul><li>keep ground truth probability overwhelm the other classes</li><li>neutralizing predicted probabilities on incorrect classes</li></ul></li></ul></li><li><p>论点</p><ul><li>class imbalace<ul><li>limits generalization </li><li>resample<ul><li>oversampling on minority classes </li><li>undersampling on majority classes  </li></ul></li><li>reweight<ul><li><strong>neglect the fact that samples on minority classes may have noise or false annotations</strong></li><li>might cause poor generalization </li></ul></li></ul></li><li>observed degradation in imbalanced datasets using CE<ul><li>cross entropy mostly ignores output scores on wrong classes</li><li>neutralizing predicted probabilities on incorrect classes helps improve accuracy of prediction for imbalanced image classification </li></ul></li></ul></li><li><p>方法</p><ul><li><p>complement entropy</p><ul><li>calculated on incorrect classes </li><li>N samples，K-dims class vector</li><li>$C(y,\hat y)=-\frac{1}{N}\sum_{i=1}^N\sum_{j=1,j \neq g}^K \frac{\hat y^j}{1-\hat y^g}log\frac{\hat y^j}{1-\hat y^g} $</li><li>the purpose is to encourage larger gap between ground truth and other classes —— when the incorrect classes obey normal distribution it reaches optimal</li><li><img src="/2020/09/08/complement-cross-entropy/complement.png" width="40%;"></li></ul></li><li><p>balanced complement entropy</p><ul><li>add balancing factor</li><li>$C^{‘}(y,\hat y) = \frac{1}{K-1}C(y,\hat y)$</li></ul></li><li><p>forming COT：</p><p>  <img src="/2020/09/08/complement-cross-entropy/COT.png" width="45%;"></p><ul><li>twice back-propagation per each iteration <ul><li>first cross entropy </li><li>second complement entropy</li></ul></li></ul></li><li><p>CCE (Complement Cross Entropy)</p><ul><li>add modulating factor：$\tilde C(y, \hat y) = \frac{\gamma}{K-1}C(y, \hat y)$，$\gamma=-1$</li><li>combination：CE+CCE</li></ul></li></ul></li></ol><ol><li><p>实验</p><ul><li><p>dataset：</p><ul><li>cifar</li><li>class-balanced originally</li><li>construct imbalanced variants with imbalance ratio $\frac{N_{min}}{N_{max}}$</li><li><img src="/2020/09/08/complement-cross-entropy/imbalance.png" width="40%;"></li></ul></li><li><p>test acc</p><ul><li>论文的实验结果都是在cifar上cce好于cot好于focal loss，在road上cce好于cot，没放fl</li><li>咱也不知道。。。</li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> loss </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>regression loss</title>
      <link href="/2020/09/07/regression-loss/"/>
      <url>/2020/09/07/regression-loss/</url>
      <content type="html"><![CDATA[<ol><li><p>损失函数用来评价模型预测值和真实值的不一样程度</p><p> 两系损失函数：</p><p> <img src="/2020/09/07/regression-loss/loss.png" width="30%;"></p></li></ol><ol><li><p>绝对值loss</p><ul><li>$L(Y,f(x))=|Y-f(x)|$</li><li><img src="/2020/09/07/regression-loss/L1.png" width="30%;"></li><li>平均绝对值损失，MAE，L1</li><li>对异常点有更好的鲁棒性</li><li>更新的梯度始终相同，对于很小的损失值，梯度也很大，不利于模型学习——手动衰减学习率</li></ul></li><li><p>平方差loss</p><ul><li>$L(Y, f(x)) = (Y-f(x))^2$</li><li><img src="/2020/09/07/regression-loss/L2.png" width="30%;"></li><li>均方误差损失，MSE，L2</li><li>因为取了平方，会赋予异常点更大的权重，会以牺牲其他样本的误差为代价，朝着减小异常点误差的方向更新，降低模型的整体性能</li></ul></li><li><p>Huber loss</p><ul><li>$L = \begin{cases} \frac{1}{2}(y-f(x))^2,\text{   for }|y-f(x)|&lt;\delta,\\ \delta |y-f(x)|-\frac{1}{2}\delta^2,  \text{  otherwise} \end{cases} $</li><li><img src="/2020/09/07/regression-loss/huber.png" width="30%;"></li><li>超参决定了对与异常点的定义，只对较小的异常值敏感</li></ul></li><li><p>对数loss</p><script type="math/tex; mode=display"> L(Y, P(Y|X)) = -log(P(Y|X))</script></li><li><p>cross-entropy loss</p><p> 二分类双边计算：</p><script type="math/tex; mode=display"> L = ylna + (1-y)ln(1-a)</script><p> 多分类单边计算：</p><script type="math/tex; mode=display"> L = ylna</script></li></ol><ol><li><p>指数loss</p><script type="math/tex; mode=display"> L(Y, f(x)) = exp[-yf(x)]</script></li><li><p>Hinge loss</p><script type="math/tex; mode=display"> L(Y, f(x)) = max(0, 1-yf(x))</script></li><li><p>perceptron loss</p><script type="math/tex; mode=display"> L(Y, f(x)) = max(0, -yf(x))</script></li><li><p>cross-entropy loss</p><p> 二分类双边计算：</p><script type="math/tex; mode=display"> L = ylna + (1-y)ln(1-a)</script><p> 多分类单边计算：</p><script type="math/tex; mode=display"> L = ylna</script></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 回归 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>pseudo-3d</title>
      <link href="/2020/09/02/pseudo-3d/"/>
      <url>/2020/09/02/pseudo-3d/</url>
      <content type="html"><![CDATA[<p>[3d resnet] Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition：真3d，for comparison，分类</p><p>[C3d] Learning Spatiotemporal Features with 3D Convolutional Networks：真3d，for comparison，分类</p><p>[Pseudo-3D resnet] Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks：伪3d，resblock，S和T花式连接，分类</p><p>[2.5d Unet] Automatic Segmentation of Vestibular Schwannoma from T2-Weighted MRI by Deep Spatial Attention with Hardness-Weighted Loss：patch输入，先2d后3d，针对各向异性，分割</p><p>[two-pathway U-Net] Combining analysis of multi-parametric MR images into a convolutional neural network: Precise target delineation for vestibular schwannoma treatment planning：patch输入，3d网络，xy和z平面分别conv &amp; concat，分割</p><p>[Projection-Based 2.5D U-net] Projection-Based 2.5D U-net Architecture for Fast Volumetric Segmentation：mip，2d网络，分割，重建</p><p>[New 2.5D Representation] A New 2.5D Representation for Lymph Node Detection using Random Sets of Deep Convolutional Neural Network Observations：横冠矢三个平面作为三个channel输入，2d网络，检测</p><h2 id="Learning-Spatio-Temporal-Representation-with-Pseudo-3D-Residual-Networks"><a href="#Learning-Spatio-Temporal-Representation-with-Pseudo-3D-Residual-Networks" class="headerlink" title="Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks"></a>Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks</h2><ol><li><p>动机</p><ul><li>spatio-temporal video</li><li>the development of a very deep 3D CNN from scratch results in expensive computational cost and memory demand</li><li>new framework<ul><li>1x3x3 &amp; 3x1x1</li><li>Pseudo-3D Residual Net which exploits all the variants of blocks</li></ul></li><li>outperforms 3D CNN and frame-based 2D CNN</li></ul></li><li><p>论点</p><ul><li>3d CNN的model size：making it extremely difficult to train a very deep model</li><li>fine-tuning 2d 好于 train from scrach 3d</li><li>RNN builds only the temporal connections on the high-level features，leaving the correlations in the low-level forms not fully exploited</li><li>we propose<ul><li>1x3x3 &amp; 3x1x1 in parallel or cascaded </li><li>其中的3x3 conv可以用2d conv来初始化</li><li>a family of bottleneck building blocks：enhance the structural diversity  </li></ul></li></ul></li><li><p>方法</p><ul><li><p>P3D Blocks  </p><ul><li>direct／indirect influence：S和T之间是串联还是并联</li><li><p>direct／indirect connected to the final output：S和T的输出是否直接与identity path相加</p><p><img src="/2020/09/02/pseudo-3d/P3D Blocks.png" width="40%"></p></li><li><p>bottleneck：</p><ul><li>头尾各接一个1x1x1的conv</li><li>头用来narrow channel，尾用来widen back</li><li><p>头有relu，尾没有relu</p><p><img src="/2020/09/02/pseudo-3d/bottlenecks.png" width="80%"></p></li></ul></li></ul></li><li><p>Pseudo-3D ResNet </p><ul><li>mixing blocks：循环ABC</li><li><p>better performance &amp; small increase in model size</p><p><img src="/2020/09/02/pseudo-3d/cmp.png" width="45%"></p></li><li><p>fine-tuning resnet50：</p><ul><li>randomly cropped 224x224</li><li>freeze all BN except for the first one</li><li>add an extra dropout layer with 0.9 dropout rate </li></ul></li><li>further fine-tuning P3D resnet：<ul><li>initialize with r50 in last step</li><li>randomly cropped 16x160x160</li><li>horizontally flipped </li><li>mini-batch as 128 frames </li></ul></li></ul></li><li><p>future work</p><ul><li>attention mechanism will be incorporated </li></ul></li></ul></li></ol><h2 id="Projection-Based-2-5D-U-net-Architecture-for-Fast-Volumetric-Segmentation"><a href="#Projection-Based-2-5D-U-net-Architecture-for-Fast-Volumetric-Segmentation" class="headerlink" title="Projection-Based 2.5D U-net Architecture for Fast Volumetric Segmentation"></a>Projection-Based 2.5D U-net Architecture for Fast Volumetric Segmentation</h2><ol><li><p>动机</p><ul><li><p>MIP：2D images containing information of the full 3D image </p></li><li><p>faster, less memory, accurate</p></li></ul></li><li><p>方法</p><ul><li><p>2d unet</p><ul><li>MIP：$\alpha=36$</li><li>3x3 conv, s2 pooling, transpose conv, concat, BN, relu, </li><li>filters：begin with 32, end with 512</li><li><p>dropout：0.5 in the deepest convolutional block and 0.2 in the second deepest blocks</p><p><img src="/2020/09/02/pseudo-3d/mra.png" width="50%"></p></li></ul></li><li><p>3d unet</p><ul><li>overfitting &amp; memory space</li><li>filters：begin with 4, end with 16</li><li>dropout：0.5 in the deepest convolutional block and 0.4 in the second deepest blocks</li></ul></li><li><p>Projection-Based 2.5D U-net </p><ul><li><p>2d slice：loss of connection </p></li><li><p>2d mip：disappointing results </p></li><li><p>2d volume：long training time</p></li><li><p>the proposed 2.5D U-net：</p><script type="math/tex; mode=display">  N(x) = T R_p F_p   \left[   \begin{matrix}     U M_{\alpha_1}(x) \\     ... \\     U M_{\alpha_p}(x)    \end{matrix}    \right]</script><ul><li><p>$M_{i}$：MIP，p=12</p></li><li><p>$U$：2d-Unet like above</p></li><li><p>$F_p$：learnable filtration，1x3 conv，for each projection，抑制重建伪影</p></li><li><p>$R_p$：reconstruction operator</p><p>  <img src="/2020/09/02/pseudo-3d/recon.png" width="40%"></p></li><li><p>$T$：fine-tuning operator，shift &amp; scale back to 0-1 mask</p></li></ul><p><img src="/2020/09/02/pseudo-3d/cmp2.png" width="60%"></p></li></ul></li></ul></li></ol><h2 id="Learning-Spatio-Temporal-Features-with-3D-Residual-Networks-for-Action-Recognition"><a href="#Learning-Spatio-Temporal-Features-with-3D-Residual-Networks-for-Action-Recognition" class="headerlink" title="Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition"></a>Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition</h2><ol><li><p>动机</p><ul><li>3D kernels tend to overfit</li><li>3D CNNs is relatively shallow</li><li>propose a 3D CNNs based on ResNets<ul><li>better performance</li><li>not overfit</li><li>deeper than C3D</li></ul></li></ul></li><li><p>论点</p><ul><li>two-stream architecture：consists of RGB and optical flow streams is often used to represent spatio-temporal information </li><li>3D CNNs：trained on relatively small video datasets performs worse than 2D CNNs pretrained on large datasets </li><li>Very deep 3D CNNs：not explored yet due to training difficulty  </li></ul></li><li><p>方法</p><ul><li><p>Network Architecture</p><ul><li>main difference：kernel dimensions</li><li>stem：stride2 for S，stride1 for T</li><li>resblock：conv_bn_relu&amp;conv + id</li><li>identity shortcuts：use zero-padding for increasing dimensions，to avoid increasing the number of parameters</li><li>stride2 conv：conv3_1、 conv4_1、 conv5_1</li><li>input clips：3x16x112x112</li><li><strong>large learning rate</strong> and batch size was important </li></ul><p><img src="/2020/09/02/pseudo-3d/3d res.png" width="50%"></p></li></ul></li><li><p>实验</p><ul><li>在小数据集上3d-r18不如C3D，overfit了：shallow architecture of the C3D and pretraining on the Sports-1M dataset prevent the C3D from overfitting </li><li>在大数据集上3d-r34好于C3D，同时C3D的val acc明显高于train acc——太shallow欠拟合了，r34则表现更好，而且不需要预训练</li><li>RGB-I3D achieved the best performance <ul><li>3d-r34是更deeper的</li><li>RGB-I3D用了更大的batch size：Large batch size is important to train good models with batch normalization </li><li>High resolutions：3x64x224x224</li></ul></li></ul></li></ol><h2 id="Learning-Spatiotemporal-Features-with-3D-Convolutional-Networks"><a href="#Learning-Spatiotemporal-Features-with-3D-Convolutional-Networks" class="headerlink" title="Learning Spatiotemporal Features with 3D Convolutional Networks"></a>Learning Spatiotemporal Features with 3D Convolutional Networks</h2><ol><li><p>动机</p><ul><li>generic </li><li>efficient </li><li>simple </li><li>3d ConvNet with 3x3x3 conv &amp; a simple linear classifier</li></ul></li><li><p>论点</p><ul><li>3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets</li><li>2D ConvNets lose temporal information of the input signal right after every convolution operation </li><li><p>2d conv在channel维度上权重都是一样的，相当于temporal dims上没有重要性特征提取</p><p><img src="/2020/09/02/pseudo-3d/2d3d.png" width="80%"></p></li></ul></li><li><p>方法</p><ul><li><p>basic network settings</p><ul><li>5 conv layers + 5 pooling layers + 2 fc layers + softmax</li><li>filters：[64，128，256，256，256]</li><li>fc dims：[2048，2048]</li><li>conv kernel：dx3x3</li><li>pooling kernel：2x2x2，s2 except for the first layer <ul><li>with the intention of not to merge the temporal signal too early </li><li>also to satisfy the clip length of 16 frames </li></ul></li></ul></li><li><p>varing settings</p><ul><li>temporal kernel depth<ul><li>homogeneous：depth-1/3/5/7 throughout</li><li>varying：increasing-3-3-5-5-7 &amp; decreasing-7- 5-5-3-3  </li></ul></li><li><p>depth-3 throughout performs the best</p><p><img src="/2020/09/02/pseudo-3d/depth.png" width="60%"></p></li><li><p>depth-1 is significantly worse  </p></li><li>We also verify that 3D ConvNet consistently performs better than 2D ConvNet <strong>on a large-scale internal dataset</strong></li></ul></li><li><p>C3D</p><ul><li>8 conv layers + 5 pooling layers + 2 fc layers + softmax</li><li>homogeneous：3x3x3 s1 conv thtoughout</li><li>pool1：1x2x2 kernel size &amp; stride，rest 2x2x2</li><li><p>fc dims：4096</p><p><img src="/2020/09/02/pseudo-3d/c3d.png" width="80%"></p></li></ul></li><li><p>C3D video descriptor：fc6 activations + L2-norm</p></li><li><p>deconvolution visualizing：</p><ul><li>conv5b feature maps</li><li>starts by focusing on appearance in the first few frames </li><li>tracks the salient motion in the subsequent frames </li></ul></li><li><p>compactness </p><ul><li>PCA</li><li>压缩到50-100dim不太损失acc</li><li><p>压缩到10dim仍旧是最高acc</p><p><img src="/2020/09/02/pseudo-3d/PCA.png" width="60%"></p></li><li><p>projected to 2-dimensional space using t-SNE </p><ul><li>C3D features are semantically separable compared to Imagenet  </li><li><p>quantitatively observe that C3D is better than Imagenet</p><p><img src="/2020/09/02/pseudo-3d/2dvec.png" width="60%"></p></li></ul></li></ul></li></ul></li><li><p>Action Similarity Labeling </p><ul><li>predicting action similarity  </li><li>extract C3D features: prob, fc7, fc6, pool5 for each clip </li><li>L2 normalization</li><li>compute the 12 different distances for each feature：48 in total</li><li>linear SVM is trained on these 48-dim feature vectors </li><li><p>C3D significantly outperforms the others</p><p><img src="/2020/09/02/pseudo-3d/similarity.png" width="50%"></p></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 3d CNN, 2.5d CNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SSD</title>
      <link href="/2020/08/13/SSD/"/>
      <url>/2020/08/13/SSD/</url>
      <content type="html"><![CDATA[<h2 id="SSD-Single-Shot-MultiBox-Detector"><a href="#SSD-Single-Shot-MultiBox-Detector" class="headerlink" title="SSD: Single Shot MultiBox Detector"></a>SSD: Single Shot MultiBox Detector</h2><ol><li><p>动机</p><ul><li>single network</li><li>speed &amp; accuracy </li><li>59 FPS / 74.3% mAP</li></ul></li><li><p>论点</p><ul><li><p>prev methods</p><ul><li>two-stage：生成稀疏的候选框，然后对候选框进行分类与回归</li><li>one-stage：均匀地在图片的不同位置，采用不同尺度和长宽比，进行密集抽样，然后利用CNN提取特征后直接进行分类与回归</li></ul></li><li><p>fundamental speed improvement </p><ul><li>eliminating bounding box proposals </li><li>eliminating feature resampling  </li></ul></li><li>other improvements <ul><li>small convolutional filter for bbox categories and offsets（针对yolov1的全连接层说）</li><li>separate predictors by aspect ratio</li><li>multiple scales</li><li>这些操作都不是原创</li></ul></li><li>The core of SSD is predicting category scores and box offsets for a fixed set of default bounding boxes using small convolutional filters applied to feature maps. </li></ul></li><li><p>方法</p><ul><li><p>Model</p><ul><li><p>Multi-scale feature maps for detection：采用了多尺度的特征图，逐渐用s2降维，大尺度特征图上有更多的单元，用来回归小物体</p></li><li><p>Convolutional predictors for detection：针对yolov1里面的fc层</p></li><li><p>Default boxes and aspect ratios：一个单元4种size的先验框，对每个先验框都预测一组4+(c+1)，其中的1可以看作背景类，也可以看做是有无目标的置信度，各用一个conv3x3的head</p></li><li><p>backbone</p><ul><li>参考：<a href="https://www.cnblogs.com/sddai/p/10206929.html" target="_blank" rel="noopener">https://www.cnblogs.com/sddai/p/10206929.html</a></li></ul></li><li>VGG16前四个conv block保留<ul><li>无dropout和fc</li><li>conv5的池化由2x2-s2变成3x3-s1</li><li>conv6和conv7是3x3x1024和1x1x1024的空洞卷积，输出19x19x1024</li><li>conv8是1x1x256和3x3x512 s2的conv，输出10x10x512</li><li>conv9都是1x1x128和3x3x256 s2的conv，输出5x5x256</li><li>conv10、conv11都是1x1x128和3x3x256 s1 p0的conv，输出3x3x256、1x1x256</li></ul></li></ul></li><li>Training <ul><li>Matching strategy：match default box和gt box<ul><li>首先为每一个gt box找到一个overlap最大的default box</li><li>然后找到所有与gt box的overlap大于0.5的default box</li><li>一个gt box可能对应多个default box</li><li>一个default box只能对应一个gt box（overlap最大的）</li></ul></li><li>Objective loss <ul><li>loc loss：smooth L1，offsets like Faster R-CNN</li><li>cls loss：softmax loss  </li><li>weighted sum：$L = \frac{1}{N} (L_{cls} + \alpha L_{loc})$，<ul><li>N is the number of matched default boxes</li><li>loss=0 when N=0</li></ul></li></ul></li><li>Choosing scales and aspect ratios for default boxes <ul><li>每个level的feature map感受野不同，default box的尺寸也不同</li><li>数量也不同，conv4、conv10和conv11是4个，conv7、conv8、conv9是6个</li><li>ratio：{1,2,3,1/2,1/3}，4个的没有3和1/3</li><li>L2 normalization for conv4：<ul><li>$y_i = \frac{x_i}{\sqrt{\sum_{k=1}^n x_k^2}}$</li><li>作用是将不同尺度的特征都归一化成模为1的向量</li><li>scale：可以是固定值，也可以是可学习参数</li><li>为啥只针对conv4？<a href="https://zhuanlan.zhihu.com/p/39399799" target="_blank" rel="noopener">作者的另一篇paper(ParseNet)中发现conv4和其他层特征的scale是不一样的</a></li></ul></li></ul></li><li>predictions<ul><li>all default boxes with different scales and aspect ratio from all locations of many feature maps</li><li>significant imbalance for positive/negative</li><li>Hard negative mining<ul><li>sort using the highest confidence loss</li><li>pick the top ones with n/p at most 3:1</li><li>faster optimization and a more stable training</li></ul></li></ul></li><li>Data augmentation <ul><li>sample a patch with specific IoU</li><li>resize</li></ul></li></ul></li></ul></li><li>性质<ul><li>much worse performance on smaller objects, increasing the input size can help improve</li><li>Data augmentation is crucial, resulting in a 8.8% mAP improvement</li><li>Atrous is faster, 保留pool5不变的话，the result is about the same while the speed is about 20% slower</li><li></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python多线程&amp;多进程</title>
      <link href="/2020/08/04/python%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
      <url>/2020/08/04/python%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E5%A4%9A%E8%BF%9B%E7%A8%8B/</url>
      <content type="html"><![CDATA[<p>Reference：</p><p><a href="https://www.cnblogs.com/kaituorensheng/p/4465768.html" target="_blank" rel="noopener">https://www.cnblogs.com/kaituorensheng/p/4465768.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/46368084" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46368084</a></p><p><a href="https://www.runoob.com/python3/python3-multithreading.html" target="_blank" rel="noopener">https://www.runoob.com/python3/python3-multithreading.html</a></p><ol><li><p>名词</p><ul><li><p>进程(process)和线程(thread)</p><ul><li>cpu在处理任务时，把时间分成若干个小时间段，这些时间段很小的，系统中有很多进程，每个进程中又包含很多线程，在同一时间段 内，电脑CPU只能处理一个线程，下一个时间段，可能又去执行别的线程了（时间片轮转，从而实现伪多任务），具体顺序取决于其调度逻辑</li><li>多核cpu可以实现真正的并行，同一个时刻每个cpu上都可以跑一个任务</li><li><p>多进程：每个进程分别执行指定任务，进程间互相独立，每个时刻并行的实际进程数取决于cpu数量</p></li><li><p>多线程：单个cpu同一时刻只能处理一个线程，一个任务可能由多个工人来完成，工人们相互协同，这则是多线程</p></li></ul></li><li><p>python的多进程：multiprocess模块</p></li><li><p>python的多线程：threading模块</p></li><li><p>每个进程在执行过程中拥有独立的内存单元，而一个进程的多个线程在执行过程中共享内存。</p></li></ul></li><li><p>多进程multiprocess</p><ul><li>母进程：当我们执行一个python脚本，if main下面实际运行的主体就是母进程</li><li>子进程：我们使用multiprocess显式创建的进程，都是子进程</li><li>join()方法：用来让母进程阻塞，等待所有子进程执行完成再结束</li></ul></li></ol><ul><li>使用multiprocess的多进程，可以通过process方法和pool方法<ul><li>process方法：适用进程较少时候，无法批量开启/关闭</li><li>pool方法：批量管理</li><li>参数：输入参数都差不多，第一个是要执行的函数方法target/func，第二个是输入参数args</li></ul></li></ul><p>  🌰Process方法：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">long_time_task</span><span class="params">(i)</span>:</span></span><br><span class="line">    print(<span class="string">'子进程: &#123;&#125; - 任务&#123;&#125;'</span>.format(os.getpid(), i))</span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line">    print(<span class="string">"结果: &#123;&#125;"</span>.format(<span class="number">8</span> ** <span class="number">20</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    print(<span class="string">'当前母进程: &#123;&#125;'</span>.format(os.getpid()))</span><br><span class="line">    start = time.time()</span><br><span class="line">    p1 = Process(target=long_time_task, args=(<span class="number">1</span>,))</span><br><span class="line">    p2 = Process(target=long_time_task, args=(<span class="number">2</span>,))</span><br><span class="line">    print(<span class="string">'等待所有子进程完成。'</span>)</span><br><span class="line">    p1.start()</span><br><span class="line">    p2.start()</span><br><span class="line">    p1.join()</span><br><span class="line">    p2.join()</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">"总共用时&#123;&#125;秒"</span>.format((end - start)))</span><br></pre></td></tr></table></figure><ul><li><p>process方法使用Process实例化一个进程对象，然后调用它的start方法开启进程</p><p>🌰Pool方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool, cpu_count</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">long_time_task</span><span class="params">(i)</span>:</span></span><br><span class="line">    print(<span class="string">'子进程: &#123;&#125; - 任务&#123;&#125;'</span>.format(os.getpid(), i))</span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line">    print(<span class="string">"结果: &#123;&#125;"</span>.format(<span class="number">8</span> ** <span class="number">20</span>))</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">True</span>     <span class="comment"># 用于演示pool适用于有返回值</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    print(<span class="string">"CPU内核数:&#123;&#125;"</span>.format(cpu_count()))        <span class="comment"># 4</span></span><br><span class="line">    print(<span class="string">'当前母进程: &#123;&#125;'</span>.format(os.getpid()))</span><br><span class="line">    start = time.time()</span><br><span class="line">    p = Pool(<span class="number">4</span>)</span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="comment"># p.apply_async(long_time_task, args=(i,))</span></span><br><span class="line">        results.append(p.apply_async(long_time_task, args=(i,)))</span><br><span class="line">    print(<span class="string">'等待所有子进程完成。'</span>)</span><br><span class="line">    p.close()</span><br><span class="line">    p.join()</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">"总共用时&#123;&#125;秒"</span>.format((send - start)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 查看返回值</span></span><br><span class="line">    <span class="keyword">for</span> res <span class="keyword">in</span> results:</span><br><span class="line">      print(res.get())</span><br></pre></td></tr></table></figure></li><li><p>apply_async(func, args=(), kwds={}, callback=None)：向进程池提交需要执行的函数及参数，各个进程采用非阻塞（异步）的调用方式，即每个子进程只管运行自己的，不管其它进程是否已经完成。</p></li><li>close()：关闭进程池（pool），不再接受新的任务。</li><li>join()：主进程阻塞等待子进程的退出， 调用join()之前必须先调用close()或terminate()方法，使其不再接受新的Process。</li></ul><ol><li><p>多线程threading</p><ul><li>python的多线程是伪多线程，因为主进程只有一个，所以只用了单核，只是通过碎片化进程、调度、全局锁等操作，cpu利用率提升了</li><li>所以我想并行处理百万量级的数据入库操作时，多进程的效率明显高于多线程</li><li><p>【问题】从我观察上看多线程基本就是串行？？</p><p>🌰threading</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">long_time_task</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'当子线程: &#123;&#125;'</span>.format(threading.current_thread().name))</span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line">    print(<span class="string">"结果: &#123;&#125;"</span>.format(<span class="number">8</span> ** <span class="number">20</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line">    print(<span class="string">'这是主线程：&#123;&#125;'</span>.format(threading.current_thread().name))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        t = threading.Thread(target=long_time_task, args=())</span><br><span class="line">        t.setDaemon(<span class="keyword">True</span>)</span><br><span class="line">        t.start()</span><br><span class="line">        t.join()</span><br><span class="line"></span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">"总共用时&#123;&#125;秒"</span>.format((end - start)))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment"># 继承&amp;有返回值的写法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">long_time_task</span><span class="params">(i)</span>:</span></span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">8</span>**<span class="number">20</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyThread</span><span class="params">(threading.Thread)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, func, args , name=<span class="string">''</span>, )</span>:</span></span><br><span class="line">        threading.Thread.__init__(self)</span><br><span class="line">        self.func = func</span><br><span class="line">        self.args = args</span><br><span class="line">        self.name = name</span><br><span class="line">        self.result = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'开始子进程&#123;&#125;'</span>.format(self.name))</span><br><span class="line">        self.result = self.func(self.args[<span class="number">0</span>],)</span><br><span class="line">        print(<span class="string">"结果: &#123;&#125;"</span>.format(self.result))</span><br><span class="line">        print(<span class="string">'结束子进程&#123;&#125;'</span>.format(self.name))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_result</span><span class="params">(self)</span>:</span></span><br><span class="line">        threading.Thread.join(self)  <span class="comment"># 等待线程执行完毕</span></span><br><span class="line">        <span class="keyword">return</span> self.result</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line">    threads = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">3</span>):</span><br><span class="line">        t = MyThread(long_time_task, (i,), str(i))</span><br><span class="line">        threads.append(t)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> threads:</span><br><span class="line">        t.start()</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> threads:</span><br><span class="line">        t.join()</span><br><span class="line"></span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">"总共用时&#123;&#125;秒"</span>.format((end - start)))</span><br></pre></td></tr></table></figure></li><li><p>join方法：等待所有进程执行完，主进程再执行完</p></li><li>setDaemon(True)：主线程执行完就退出</li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>IoU</title>
      <link href="/2020/08/03/IoU/"/>
      <url>/2020/08/03/IoU/</url>
      <content type="html"><![CDATA[<p>reference: <a href="https://bbs.cvmart.net/articles/1396" target="_blank" rel="noopener">https://bbs.cvmart.net/articles/1396</a></p><ol><li><p>IoU</p><p> IoU = Intersection / Union</p><p> $Loss_{IoU} = 1 - IoU$</p><ul><li>[0,1]</li><li>无法直接优化没有重叠的部分：如果两个框没有交集，IoU=0，没有梯度回传，无法进行学习训练</li><li>尺度不敏感</li><li><p>无法精确的反映两者的重合质量</p><p><img src="/2020/08/03/IoU/IoU.png" width="40%;"></p></li></ul></li><li><p>GIoU(Generalized Intersection over Union)</p><p> $GIoU = IoU - \frac{|A_c - U|}{|A_c|}$，$A_c$是包含两个框的最小外接框</p><p> $Loss_{GIoU} = 1 - GIoU$</p><p> <img src="/2020/08/03/IoU/GIoU.png" width="40%;"></p><ul><li>GIoU倾向于先增大bbox的大小来增大与GT的交集，然后通过IoU项引导最大化bbox的重叠区域</li><li>[-1,1]</li><li>能够关注到非重合区域</li><li>尺度不敏感</li><li>两个框为包含关系时，退化为IoU</li><li>如果之间用来替换mse，前期收敛会比较慢</li><li><strong>一般地，GIoU loss不能很好地收敛SOTA算法，反而造成不好的结果</strong></li></ul></li><li><p>DIoU (Distance-IoU)</p><p> $DIoU = IoU - \frac{d^2}{c^2}$，d是两个中心点间的欧式距离，c是两个框的最小外接框的对角线距离</p><p> $Loss_{DIoU} = 1 - DIoU$</p><p> <img src="/2020/08/03/IoU/DIoU.png" width="30%;"></p></li></ol><pre><code>* 直接最小化两个目标框的距离，收敛快得多* 能够关注到非重合区域* 对于包含关系的两个框，仍旧有距离损失，不会退化为IoU* 可以替换NMS中的IoU：原始的IoU仅考虑了重叠区域，对包含的情况没有很好的处理    $$    score = score\text{ if }IoU - dis(box_{max}, box)&gt;\epsilon \text{, else } 0    $$* 没有考虑形状（长宽比）</code></pre><ol><li><p>CIoU (Complete-IoU)</p><p> $CIoU = IoU - \frac{d^2}{c^2}-av$，在DIoU的基础上新增了惩罚项av，a是权重系数，v用来评价长宽比：</p><script type="math/tex; mode=display"> v = \frac{4}{\pi ^ 2}(arctan\frac{w^{gt}}{h^{gt}}-arctan\frac{w}{h})^2\\ a = \frac{v}{1-IoU+v}</script><p> $Loss_{CIoU} = 1 - CIoU$</p><ul><li><p>v的梯度中有$\frac{1}{w^2+h^2}$，长宽在[0,1]之间，可能很小，会导致梯度爆炸，用的时候</p><ul><li>clamp一下上下限</li><li><p>分母中的$w^2+h^2$替换成1</p><script type="math/tex; mode=display">\frac{\partial v }{\partial w} = \frac{8}{\pi ^ 2}(arctan\frac{w^{gt}}{h^{gt}}-arctan\frac{w}{h})\frac{h}{w^2+h^2}\\\frac{\partial v }{\partial w} = \frac{8}{\pi ^ 2}(arctan\frac{w^{gt}}{h^{gt}}-arctan\frac{w}{h})\frac{w}{w^2+h^2}</script></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 目标检测，loss </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>YOLOACT</title>
      <link href="/2020/07/17/YOLOACT/"/>
      <url>/2020/07/17/YOLOACT/</url>
      <content type="html"><![CDATA[<ul><li>[YOLACT] Real-time Instance Segmentation：33 FPS/30 mAP</li><li>[YOLACT++] Better Real-time Instance Segmentation：33.5 FPS/34.1 mAP</li></ul><h2 id="YOLACT-Real-time-Instance-Segmentation"><a href="#YOLACT-Real-time-Instance-Segmentation" class="headerlink" title="YOLACT: Real-time Instance Segmentation"></a>YOLACT: Real-time Instance Segmentation</h2><ol><li><p>动机</p><ul><li><p>create a real-time instance segmentation base on fast, one-stage detection model</p></li><li><p>forgoes an explicit localization step (e.g., feature repooling) </p><ul><li>doesn’t depend on repooling (RoI Pooling)</li><li>produces very high-quality masks</li></ul></li><li><p>set two parallel subtasks</p><ul><li>prototypes——conv</li><li>mask coefficients——fc</li><li>之后将模板mask和实例mask系数进行线性组合来获得实例的mask</li></ul></li></ul></li></ol><ul><li><p>‘prototypes’: vocabulary </p></li><li><p>fully-convolutional</p><ul><li>localization is still translation variant</li></ul></li><li><p>Fast NMS</p></li></ul><ol><li><p>论点</p><ul><li><p>State-of-the-art approaches to instance segmentation like Mask R- CNN and FCIS directly build off of advances in object detection like Faster R-CNNand R-FCN</p><ul><li>focus primarily on performance over speed </li><li>these methods “re-pool” features in some bounding box region  </li><li>inherently sequential therefore difficult to accelerate </li></ul></li><li><p>One-stage instance segmentation methods generate position sensitive maps </p><ul><li>still require repooling or other non-trivial computations </li></ul></li><li><p>prototypes </p><ul><li>related works use prototypes to represent features (Bag of Feature)</li><li>we use them to assemble masks for instance segmentation </li><li>we learn prototypes that are specific to each image, rather than global prototypes shared across the entire dataset</li></ul></li><li><p>Bag of Feature</p><ul><li><p>BOF假设图像相当于一个文本，图像中的不同局部区域或特征可以看作是构成图像的词汇(codebook)</p><p>  <img src="/2020/07/17/YOLOACT/BOF.png" width="60%"></p></li><li><p>所有的样本共享一份词汇本，针对每个图像，统计每个单词的频次，即可得到图片的特征向量</p><p><img src="/2020/07/17/YOLOACT/BOF1.png" width="50%"></p></li></ul></li></ul></li><li><p>方法</p><ul><li><p>parallel tasks</p><ul><li>The first branch uses an FCN to produce a set of image-sized “prototype masks” that do not depend on any one instance. </li><li>The second adds an extra head to the object detection branch to predict a vector of “mask coefficients” for each anchor that encode an instance’s rep- resentation in the prototype space.</li><li>linearly combining </li></ul></li><li><p>Rationale </p><ul><li>masks are spatially coherent：mash是空间相关的，相邻像素很可能是一类</li><li>卷积层能够利用到这种空间相关性，但是fc层不能</li><li>而one-stage检测器的检测头通常是fc层？？</li><li>making use of fc layers, which are good at producing semantic vectors</li><li>and conv layers, which are good at producing spatially coherent masks</li></ul></li><li><p>Prototype </p><ul><li>在backbone feature layer P3上接一个FCN<ul><li>taking protonet from deeper backbone features produces more robust masks</li><li>higher resolution prototypes result in both higher quality masks and better performance on smaller objects</li><li>upsample到x4的尺度to increase performance on small objects</li></ul></li><li><p>head包含k个channels</p><ul><li>梯度回传来源于最终的final assembled mask，不是当前这个头</li><li>unbounded：ReLU or no nonlinearity</li><li>We choose ReLU for more interpretable prototypes</li></ul><p><img src="/2020/07/17/YOLOACT/Protonet.png" width="40%"></p></li></ul></li><li><p>Mask Coefficients </p><ul><li>a third branch in parallel with detection heads</li><li><p>nonlinearity：要有正负，所以tanh </p><p><img src="/2020/07/17/YOLOACT/Coefficients.png" width="40%"></p></li></ul></li><li><p>Mask Assembly </p><ul><li>linear combination + sigmoid: $M=\sigma(PC^T)$</li><li>loss<ul><li>cls loss：w=1, 和ssd一样，c+1 softmax</li><li>box reg loss：w=1.5, 和ssd一样，smooth-L1</li><li>mask loss：w=6.125， BCE</li></ul></li><li>crop mask<ul><li>eval：用predict box去crop</li><li>train：用gt box去crop，同时还要给mask loss除以gt box的面积，to preserve small objects </li></ul></li></ul></li><li><p>Emergent Behavior </p><ul><li><p>不crop也能分割中大目标：</p><ul><li>YOLACT learns how to localize instances on its own via different activations in its prototypes</li><li>而不是靠定位结果 </li></ul></li><li><p>translation variant</p><ul><li>the consistent rim of padding in modern FCNs like ResNet gives the network the ability to tell how far away from the image’s edge a pixel is，所以用一张纯色的图能够看出kernel实际highlight的是哪部分特征</li><li>同一种kernel，同一种五角星，在画面不同位置，对应的响应值是不同的，说明fcn是能够提取物体位置这样的语义信息的</li><li><p>prototypes are compressible：</p><ul><li>增加模版数目反而不太有效，because predicting coefficients is difficult，</li><li>the network has to play a balancing act to produce the right coef- ficients, and adding more prototypes makes this harder, </li><li>We choose 32 for its mix of performance and speed</li></ul><p><img src="/2020/07/17/YOLOACT/Prototype.png" width="40%"></p></li></ul></li></ul></li><li><p>Network</p><ul><li>speed as well as feature richness</li><li>backbone参考RetinaNet，ResNet-101 + FPN <ul><li>550x550 input，resize</li><li>去掉P2，add P6&amp;P7</li><li>3 anchors per level，[1, 1/2, 2]</li><li>P3的anchor尺寸是24x24，接下来每层double the scale  </li><li>检测头：shared conv+parallel conv</li><li>OHEM </li></ul></li><li><p>single GPU：batch size 8 using ImageNet weights，no extra bn layers</p><p><img src="/2020/07/17/YOLOACT/YOLACT.png" width="80%"></p></li></ul></li><li><p>Fast NMS </p><ul><li>构造cxnxn的矩阵，c代表每个class</li><li>然后搞成上三角，求column-wise max</li><li>再IoU threshold</li><li>15.0 ms faster with a performance loss of 0.3 mAP</li></ul></li><li>Semantic Segmentation Loss <ul><li>using modules not executed at test time </li><li>P3上1x1 conv，sigmoid and c channels </li><li>w=1</li><li>+0.4 mAP boost</li></ul></li></ul></li></ol><h2 id="YOLACT-Better-Real-time-Instance-Segmentation"><a href="#YOLACT-Better-Real-time-Instance-Segmentation" class="headerlink" title="YOLACT++: Better Real-time Instance Segmentation"></a>YOLACT++: Better Real-time Instance Segmentation</h2>]]></content>
      
      
        <tags>
            
            <tag> 实例分割 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>cornerNet</title>
      <link href="/2020/07/17/cornerNet/"/>
      <url>/2020/07/17/cornerNet/</url>
      <content type="html"><![CDATA[<h2 id="CornerNet-Detecting-Objects-as-Paired-Keypoints"><a href="#CornerNet-Detecting-Objects-as-Paired-Keypoints" class="headerlink" title="CornerNet: Detecting Objects as Paired Keypoints"></a>CornerNet: Detecting Objects as Paired Keypoints</h2><ol><li><p>动机</p><ul><li>corner formulation <ul><li>top-left corner  </li><li>bottom-right corner </li></ul></li><li>anchor-free</li><li>corner pooling</li><li>no multi-scale</li></ul></li><li><p>论点</p><ul><li><p>anchor box drawbacks    </p><ul><li>huge set of anchors boxes to ensure sufficient overlap，cause huge imbalance</li><li>hyperparameters and design choices</li></ul></li><li><p>cornerNet</p><ul><li><p>detect and group</p><ul><li>heatmap to predict corners<ul><li>从数学表达上看，全图wh个tl corner，wh个bt corner，可以表达wwhh个框</li></ul></li><li>anchor-based，全图wh个中心点，9个anchor size，只能表达有限的框，且可能match不上</li><li>embeddings to group pairs of corners</li></ul><p><img src="/2020/07/17/cornerNet/cornerNet.png" width="80%;"></p></li><li><p>corner pooling</p><ul><li><p>better localize corners  which are usually out of the foreground</p><p><img src="/2020/07/17/cornerNet/cornerPooling.png" width="50%;"></p></li></ul></li><li><p>modifid hourglass architecture </p></li><li><p>add our novel variant of focal loss </p></li></ul></li></ul></li><li><p>方法</p><p> <img src="/2020/07/17/cornerNet/overview.png" width="75%;"></p><ul><li><p>two prediction modules</p><ul><li><p>heatmaps </p><ul><li><p>C channels, C for number of categories </p></li><li><p>binary mask</p></li><li><p>each corner has only one ground-truth positive</p></li><li><p>penalty the neighbored negatives within a radius that still hold high iou (0.3 iou)</p><ul><li>determine the radius </li><li>penalty reduction $=e^{-\frac{x^2+y^2}{2\sigma^2}}$</li></ul></li><li><p>variant focal loss</p><ul><li><script type="math/tex; mode=display">  L_{det} = \frac{-1}{N} \sum^C \sum^H \sum^W   \begin{cases}  (1-p_{i,j})^\alpha log(p_{i,j}), \ \ if y_{ij}=1\\  (1-y_{ij})^\beta (p_{i,j})^\alpha log(1-p_{i,j}), \ \ otherwise  \end{cases}</script></li><li><p>$\alpha=2, \beta=4$</p></li><li><p>N is the number of gts</p></li></ul></li></ul></li><li><p>embeddings</p><ul><li>associative embedding </li><li>use 1-dimension embedding</li><li>pull and push loss on gt positives <ul><li>$L_{pull} = \frac{1}{N} \sum^N [(e_{tk}-e_k)^2 + (e_{bk}-e_k)^2]$</li><li>$L_{push} = \frac{1}{N(N-1)} \sum_j^N\sum_{k\neq j}^N max(0, \Delta -|e_k-e_j|)$</li><li>$e_k$ is the average of $e_{tk}$ and $e{bk}$</li><li>$\Delta$ = 1</li></ul></li></ul></li><li><p>offsets</p><ul><li>从heatmap resolution remapping到origin resolution存在精度损失<script type="math/tex; mode=display">  o_k = （\frac{x_k}{n} - \lfloor \frac{x_k}{n} \rfloor， \frac{y_k}{n} - \lfloor \frac{y_k}{n} \rfloor）</script></li></ul></li></ul></li><li><p>greatly affect the IoU of small bounding boxes  </p></li><li><p>shared among all categories</p></li><li><p>smooth L1 loss on gt positives </p><pre><code>      $$      L_{off} = \frac{1}{N} \sum^N SmoothL1(o_k, \hat o_k)</code></pre><p>  $$</p></li><li><p>corner pooling</p><ul><li>top-left pooling layer：<pre><code>  * 从当前点(i,j)开始，  * 向下elementwise max所有feature vecor，得到$t_{i,j}$  * 向右elementwise max所有feature vecor，得到$l_{i,j}$  * 最后两个vector相加</code></pre><ul><li>bottom-right corner：向左向上</li></ul></li></ul><p><img src="/2020/07/17/cornerNet/prediction.png" width="80%;"></p></li><li><p>Hourglass Network </p><ul><li>hourglass modules<ul><li>series of convolution and max pooling layers </li><li>series of upsampling and convolution layers </li><li>skip layers </li></ul></li><li><p>multiple hourglass modules stacked：reprocess the features to capture higher-level information</p><p><img src="/2020/07/17/cornerNet/hourglass.png" width="40%;"></p></li><li><p>intermediate supervision</p><ul><li><p>常规的中继监督：</p><p>  <img src="/2020/07/17/cornerNet/intermediate.png" width="70%;"></p><p>  下一级hourglass module的输入包括三个部分</p><ul><li>前一级输入</li><li>前一级输出</li><li>中继监督的输出</li></ul></li><li><p>本文使用了中继监督，但是没把这个结果加回去</p><ul><li>hourglass2 input：1x1 conv-BN to both input and output of hourglass1 + add + relu</li></ul></li></ul></li></ul></li><li><p>Our backbone</p><ul><li>2 hourglasses</li><li>5 times downsamp with channels [256,384,384,384,512]</li><li>use stride2 conv instead of max-pooling</li><li>upsamp：2 residual modules + nearest neighbor upsampling</li><li>skip connection: 2 residual modules，add</li><li>mid connection: 4 residual modules</li><li>stem: 7x7 stride2, ch128 + residual stride2, ch256</li><li>hourglass2 input：1x1 conv-BN to both input and output of hourglass1 + add + relu</li></ul></li></ul></li><li><p>实验</p><ul><li>training details<ul><li>randomly initialized, no pretrained</li><li>bias：set the biases in the convolution layers that predict the corner heatmaps</li><li>input：511x511</li><li>output：128x128</li><li>apply PCA to the input image</li><li>full loss：$L = L_{det} + \alpha L_{pull} + \beta L_{push} + \gamma L_{off}$<ul><li>配对loss：$\alpha=\beta=0.1$</li><li>offset loss：$\gamma=1$</li></ul></li><li>batch size = 49 = 4+5x9</li></ul></li><li>test details<ul><li>NMS：3x3 max pooling on heatmaps</li><li>pick：top100 top-left corners &amp; top100 bottom-right corners</li><li>filter pairs：<ul><li>L1 distance greater than 0.5</li><li>from different categories </li></ul></li><li>fusion：combine the detections from the original and flipped images  + soft nms</li></ul></li><li>Ablation Study <ul><li>corner pooling is especially helpful for medium and large objects</li><li>penalty reduction especially benefits medium and large objects</li><li>CornerNet achieves a much higher AP at 0.9 IoU than other detectors：更有能力生成高质量框</li><li>error analysis：the main bottleneck is detecting corners</li></ul></li></ul></li></ol><h2 id="CornerNet-Lite-Efficient-Keypoint-Based-Object-Detection"><a href="#CornerNet-Lite-Efficient-Keypoint-Based-Object-Detection" class="headerlink" title="CornerNet-Lite: Efficient Keypoint-Based Object Detection"></a>CornerNet-Lite: Efficient Keypoint-Based Object Detection</h2><ol><li><p>动机</p><ul><li><p>keypoint-based methods</p><ul><li>detecting and grouping</li><li>accuary but with processing cost</li></ul></li><li><p>propose CornerNet-Lite </p><ul><li>CornerNet-Saccade：attention mechanism </li><li>CornerNet-Squeeze：a new compact backbone </li></ul></li><li><p>performance</p><p>  <img src="/2020/07/17/cornerNet/performance.png" width="60%;"></p></li></ul></li><li><p>论点</p><ul><li>main drawback of cornerNet<ul><li>inference speed</li><li>reducing the number of scales or the image resolution cause a large accuracy drop </li></ul></li><li>two orthogonal directions <ul><li>reduce the number of pixels to process：CornerNet-Saccade </li><li>reduce the amount of processing per pixel：</li></ul></li><li>CornerNet-Saccade <ul><li>downsized attention map </li><li>select a subset of crops to examine in high resolution </li><li>for off-line：AP of 43.2% at 190ms per image </li></ul></li><li>CornerNet-Squeeze  <ul><li>inspired by squeezeNet and mobileNet</li><li>1x1 convs</li><li>bottleneck layers</li><li>depth-wise separable convolution</li><li>for real-time：AP of 34.4% at 30ms </li></ul></li><li>combined??<ul><li>CornerNet-Squeeze-Saccade turns out slower and less accurate than CornerNet- Squeeze </li></ul></li><li>Saccades：扫视<ul><li>to generate interesting crops</li><li>RCNN系列：single-type &amp; single object</li><li>AutoFocus：add a branch调用faster-RCNN，thus multi-type &amp; mixed-objects，有single branch有multi branch</li><li>CornerNet-Saccade：<ul><li>single-type &amp; multi object</li><li>crops can be much smaller than number of objects</li></ul></li></ul></li></ul></li><li><p>方法</p><ul><li><p>CornerNet-Saccade </p><p>  <img src="/2020/07/17/cornerNet/Saccade.png" width="60%;"></p><ul><li><p>step1：obtain possible locations  </p><ul><li>downsize：two scales，255 &amp; 192，zero-padding</li><li>predicts 3 attention maps<ul><li>small object：longer side&lt;32 pixels</li><li>medium object：32-96</li><li>large object：&gt;96</li><li>so that we can control the zoom-in factor：zoom-in more for smaller objects</li><li>feature map：different scales from the upsampling layers</li><li>attention map：3x3 conv-relu + 1x1 conv-sigmoid</li><li>process locations where scores &gt; 0.3</li></ul></li></ul></li><li><p>step2：finer detection</p><ul><li>zoom-in scales：4，2，1 for small、medium、large objects</li><li>apply CornerNet-Saccade on the ROI<ul><li>255x255 window</li><li>centered at the location</li></ul></li></ul></li><li><p>step3：NMS</p><ul><li>soft-nms</li><li>remove the bounding boxes which touch the crop boundary</li></ul></li><li><p>CornerNet-Saccade uses the same network for attention maps and bounding boxes</p><ul><li>在第一步的时候，对一些大目标已经有了检测框</li><li>也要zoom-in，矫正一下</li></ul></li><li><p>efficiency</p><ul><li>regions/croped images都是processed in batch/parallel</li><li>resize/crop操作在GPU中实现</li><li><p>suppress redundant regions using a NMS-similar policy before prediction</p><p><img src="/2020/07/17/cornerNet/efficiency.png" width="60%;"></p></li></ul></li></ul></li><li><p>new hourglass backbone</p><ul><li>3 hourglass module，depth 54</li><li>downsize twice before hourglass modules</li><li>downsize 3 times in each module，with channels [384,384,512]</li><li>one residual in both encoding path &amp; skip connection</li><li>mid connection：one residual，with channels 512</li></ul></li><li><p>CornerNet-Squeeze </p><ul><li>to replace the heavy hourglass104</li><li>use fire module to replace residuals</li><li>downsizes 3 times before hourglass modules</li><li>downsize 4 times in each module</li><li>replace the 3x3 conv in prediction head with 1x1 conv</li><li><p>replace the nearest neighboor upsampling with 4x4 transpose conv</p><p><img src="/2020/07/17/cornerNet/fire.png" width="60%;"></p></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 目标检测，anchor-free </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SOLO</title>
      <link href="/2020/07/17/SOLO/"/>
      <url>/2020/07/17/SOLO/</url>
      <content type="html"><![CDATA[<p>[SOLO] SOLO: Segmenting Objects by Locations：字节，目前绝大多数方法实例分割的结构都是间接得到——检测框内语义分割／全图语义分割聚类，主要原因是formulation issue，很难把实例分割定义成一个结构化的问题</p><p>[SOLOv2] SOLOv2: Dynamic, Faster and Stronger：best 41.7% AP </p><h2 id="SOLO-Segmenting-Objects-by-Locations"><a href="#SOLO-Segmenting-Objects-by-Locations" class="headerlink" title="SOLO: Segmenting Objects by Locations"></a>SOLO: Segmenting Objects by Locations</h2><ol><li><p>动机</p><ul><li>challenging：arbitrary number of instances</li><li>form the task into a classification-solvable problem</li><li>direct &amp; end-to-end &amp; one-stage &amp; using mask annotations solely</li><li>on par accuracy with Mask R-CNN</li><li>outperforming recent single-shot instance segmenters</li></ul></li><li><p>论点</p><ul><li>formulating<ul><li>Objects in an image belong to a fixed set of semantic categories——semantic segmentation can be easily formulated as a dense per-pixel classification problem</li><li>the number of instances varies</li></ul></li><li>existing methods<ul><li>检测／聚类：step-wise and indirect</li><li>累积误差</li></ul></li><li>core idea <ul><li>in most cases two instances in an image either have different center locations or have different object sizes </li><li>location：<ul><li>think image as a divided grid of cells</li><li>an object instance is assigned to one of the grid cells as its center location category </li><li>encode center location categories as the channel axis </li></ul></li><li>size<ul><li>FPN</li><li>assign objects of different sizes to different levels of feature maps</li></ul></li><li>SOLO converts coordinate regression into classification by discrete quantization </li><li>One feat of doing so is the avoidance of heuristic coordination normalization and log-transformation typically used in detectors【？？？不懂这句话想表达啥】</li></ul></li></ul></li><li><p>方法</p><ul><li><p>problem formulation</p><ul><li>divided grids</li><li><p>simultaneous task</p><ul><li>category-aware prediction  </li><li>instance-aware mask generation</li></ul><p><img src="/2020/07/17/SOLO/solo.png" width="70%;"></p></li><li><p>category prediction</p><ul><li>predict instance for each grid：$S<em>S</em>C$</li><li>grid size：$S*S$</li><li>number of classes：$C$</li><li>based on the assumption that each cell must belong to one individual instance</li><li>C-dim vec indicates the class probability for each object instance in each grid</li></ul></li><li>mask prediction<ul><li>predict instance mask for each positive cell：$H<em>W</em>S^2$</li><li>the channel corresponding to the location</li><li>position sensitive：因为每个grid中分割的mask是要映射到对应的channel的，因此我们希望特征图是spatially variant<ul><li>让特征图spatially variant的最直接办法就是加一维spatially variant的信息</li><li>inspired by CoordConv：添加两个通道，normed_x和normed_y，[-1,1]</li><li>original feature tensor $H<em>W</em>D$ becomes $H<em>W</em>(D+2)$</li></ul></li></ul></li><li>final results<ul><li>gather category prediction &amp; mask prediction</li><li>NMS</li></ul></li></ul></li><li><p>network</p><ul><li>backbone：resnet</li><li>FCN：256-d</li><li><p>heads：weights are shared across different levels except for the last 1x1 conv</p><p><img src="/2020/07/17/SOLO/head.png" width="40%;"></p></li></ul></li><li><p>learning</p><ul><li>positive grid：falls into a center region<ul><li>mask：mask center $(c_x, c_y)$，mask size $(h,w)$</li><li>center region：$(c_x,c_y,\epsilon w, \epsilon h)$，set $\epsilon = 0.2$</li></ul></li><li>loss：$L = L_{cate} + \lambda L_{seg}$<ul><li>cate loss：focal loss</li><li>seg loss：dice，$L_{mask} = \frac{1}{N_{pos}}\sum_k 1_{p^<em>_{i,j}&gt;0} dice(m_k, m^</em>_k) $，带星号的是groud truth</li></ul></li></ul></li><li><p>inference</p><ul><li><p>use a confidence threshold of 0.1 to filter out low spacial predictions</p></li><li><p>use a threshold of 0.5 to binary the soft masks</p></li><li><p>select the top 500 scoring masks </p></li><li><p>NMS </p><ul><li>Only one instance will be activated at each grid</li><li><p>and one in- stance may be predicted by multiple adjacent mask channels </p><p><img src="/2020/07/17/SOLO/results.png" width="70%;"></p></li></ul></li><li><p>keep top 100</p></li></ul></li></ul></li><li><p>实验</p><ul><li><p>grid number</p><ul><li>适当增加有提升，主要提升还是在FPN</li><li><img src="/2020/07/17/SOLO/grid.png" width="40%;"></li></ul></li><li><p>fpn</p><ul><li>五个FPN pyramids </li><li><p>大特征图，小感受野，用来分配小目标，grid数量要增大</p></li><li><p><img src="/2020/07/17/SOLO/fpn.png" width="40%;"></p></li></ul></li><li><p>feature alignment</p><ul><li>在分类branch，$H<em>W$特征图要转换成$S</em>S$的特征图<ul><li>interpolation：bilinear interpolating </li><li>adaptive-pool：apply a 2D adaptive max-pool </li><li>region-grid- interpolation：对每个cell，采样多个点做双线性插值，然后取平均</li></ul></li><li>is no noticeable performance gap between these variants </li><li>（可能因为最终是分类任务</li></ul></li><li><p>head depth</p><ul><li>4-7有涨点</li><li>所以本文选了7</li><li><img src="/2020/07/17/SOLO/depth.png" width="40%;"></li></ul></li></ul></li><li><p>decoupled SOLO</p><ul><li><p>mask branch预测的channel数是$S^2$，其中大部分channel其实是没有贡献的，空占内存</p></li><li><p>prediction is somewhat redundant as in most cases the objects are located sparsely in the image </p></li><li><p>element-wise multiplication </p><p>  <img src="/2020/07/17/SOLO/decoupled.png" width="40%;"></p></li><li><p>实验下来</p><ul><li>achieves the same performance </li><li>efficient and equivalent variant  </li></ul></li></ul></li></ol><h2 id="SOLOv2-Dynamic-Faster-and-Stronger"><a href="#SOLOv2-Dynamic-Faster-and-Stronger" class="headerlink" title="SOLOv2: Dynamic, Faster and Stronger"></a>SOLOv2: Dynamic, Faster and Stronger</h2><ol><li><p>动机</p><ul><li>take one step further on the mask head<ul><li>dynamically learning the mask head</li><li>decoupled into mask kernel branch and mask feature branch</li></ul></li><li>propose Matrix NMS<ul><li>faster &amp; better results</li></ul></li><li>try object detection  and panoptic segmentation</li></ul></li><li><p>论点</p><ul><li>SOLO develop pure instance segmentation </li><li>instance segmentation <ul><li>requires instance-level and pixel-level predictions simultaneously </li><li>most existing instance segmentation methods build on the top of bounding boxes </li><li>SOLO develop pure instance segmentation </li></ul></li><li>SOLOv2 improve SOLO<ul><li>mask learning：dynamic scheme </li><li>mask NMS：parallel matrix operations，outperforms Fast NMS </li></ul></li><li>Dynamic Convolutions <ul><li>STN：adaptively transform feature maps conditioned on the input </li><li>Deformable Convolutional Networks：learn location</li></ul></li></ul></li><li><p>方法</p><ul><li><p>revisit SOLOv1</p><ul><li>redundant mask prediction</li><li>decouple</li><li><p>dynamic：dynamically pick the valid ones from predicted $s^2$ classifiers and perform the convolution </p><p><img src="/2020/07/17/SOLO/dynamic.png" width="80%;"></p></li></ul></li><li><p>SOLOv2</p><ul><li><p>dynamic mask segmentation head</p><ul><li>mask kernel branch</li><li>mask feature branch</li></ul></li><li><p>mask kernel branch</p><ul><li>prediction heads：4 convs + 1 final conv，shared across scale</li><li>no activation on the output</li><li>concat normalized coordinates in two additional input channels at start</li><li>ouputs D-dims kernel weights for each grid：e.g.  for 3x3 conv with E input channels, outputs $S<em>S</em>9E$</li></ul></li><li><p>mask feature branch</p><ul><li><p>predict instance-aware feature：$F \in R^{H<em>W</em>E}$</p></li><li><p>unified and high-resolution mask feature：只输出一个尺度的特征图，encoded x32 feature with coordinates info</p><ul><li>we feed normalized pixel coordinates to the deepest FPN level (at 1/32 scale)</li><li>repeated 【3x3 conv, group norm, ReLU, 2x bilinear upsampling】</li><li>element-wise sum</li><li><p>last layer：1x1 conv, group norm, ReLU</p><p><img src="/2020/07/17/SOLO/unified mask.png" width="50%;"></p></li></ul></li></ul></li><li><p>instance mask</p><ul><li>mask feature branch conved by the mask kernel branch：final conv $H<em>W</em>S^2$</li><li>mask NMS</li></ul></li><li><p>train</p><ul><li>loss：$L = L_{cate} + \lambda L_{seg}$<ul><li>cate loss：focal loss</li><li>seg loss：dice，$L_{mask} = \frac{1}{N_{pos}}\sum_k 1_{p^<em>_{i,j}&gt;0} dice(m_k, m^</em>_k) $，带星号的是groud truth</li></ul></li></ul></li><li><p>inference</p><ul><li>category score：first use a confidence threshold of 0.1 to filter out predictions with low confidence </li><li>mask branch：run convolution based on the filtered category map</li><li>sigmoid</li><li>use a threshold of 0.5 to convert predicted soft masks to binary masks </li><li>Matrix NMS</li></ul></li><li><p>Matrix NMS</p><ul><li>decremented functions  <ul><li>linear：$f(iou_{i,j}=1-iou_{i,j})$</li><li>gaussian：$f(iou_{i,j}=exp(-\frac{iou_{i,j}^2}{\sigma})$</li></ul></li><li>the most overlapped prediction for $m_i$：max iou<ul><li>$f(iou_{*,i}) = min_{s_k}f(iou_{k,i})$</li></ul></li><li>decay factor  <ul><li>$decay_i = min \frac{f(iou_{i,j})}{f(iou_{*,i})}$</li></ul></li><li><img src="/2020/07/17/SOLO/matrix nms.png" width="60%;"></li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 实例分割 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>polarMask</title>
      <link href="/2020/06/29/polarMask/"/>
      <url>/2020/06/29/polarMask/</url>
      <content type="html"><![CDATA[<h2 id="PolarMask-Single-Shot-Instance-Segmentation-with-Polar-Representation"><a href="#PolarMask-Single-Shot-Instance-Segmentation-with-Polar-Representation" class="headerlink" title="PolarMask: Single Shot Instance Segmentation with Polar Representation"></a>PolarMask: Single Shot Instance Segmentation with Polar Representation</h2><ol><li><p>动机</p><ul><li>instance segmentation</li><li>anchor-free</li><li>single-shot</li><li>modified on FCOS</li></ul></li><li><p>论点</p><ul><li>two-stage methods <ul><li>FCIS, Mask R-CNN</li><li>bounding box detection then semantic segmentation within each box</li></ul></li><li>single-shot method<ul><li>formulate the task as instance center classification and dense distance regression in a polar coordinate </li><li>FCOS can be regarded as a special case that the contours has only 4 directions</li></ul></li><li><p>this paper</p><ul><li>two parallel task：<ul><li>instance center classification</li><li>dense distance regression</li></ul></li><li>Polar IoU Loss can largely ease the optimization and considerably improve the accuary</li><li>Polar Centerness improves the original idea of “Centreness” in FCOS, leading to further performance boost</li></ul><p><img src="/2020/06/29/polarMask/PolarMask.png" width="80%;"></p></li></ul></li><li><p>方法</p><ul><li>architecture <ul><li>back &amp; fpn are the same as FCOS </li><li>model the instance mask as one center and n rays<ul><li>conclude that mass-center is more advantageous than box center</li><li>the angle interval is pre-fixed, thus only the length of the rays is to be regressed</li><li>positive samples：falls into 1.5xstrides of the area around the gt mass-center，that is 9-16 pixels around gt grid</li><li>distance regression<ul><li>如果一条射线上存在多个交点，取最长的</li><li>如果一条射线上没有交点，取最小值$\epsilon=10^{-6}$</li></ul></li></ul></li></ul></li><li>potential issuse of the mask regression branch<ul><li>dense regression task with such as 36 rays, may cause imbalance between regression loss and classification  loss</li><li>n rays are relevant and should be trained as a whole rather than a set of independent values—-&gt;iou loss</li></ul></li><li>inference<ul><li>multiply center-ness with classification to obtain final confidence scores, conf thresh=0.05</li><li>take top-1k predictions per fpn level</li><li>use the smallest bounding boxes to run NMS, nms thresh=0.5</li></ul></li><li>polar centerness<ul><li>to suppress low quality detected centers</li><li>$polar\ centerness=\sqrt{\frac{min(\{d_1,d_2, …, d_n\})}{max(\{d_1,d_2, …, d_n\})}}$</li><li>$d_{min}$和$d_{max}$越接近，说明中心点质量越好</li><li>Experiments show that Polar Centerness improves accuracy especially under stricter localization metrics, such as $AP_{75}$</li></ul></li><li>polar IoU loss<ul><li>polar IoU：$IoU=lim_{N\to\inf}\frac{\sum_{i=1}^N\frac{1}{2} d_{min}^2 \Delta \theta}{\sum_{i=1}^N\frac{1}{2} d_{max}^2 \Delta \theta}$</li><li>empirically observe that 去掉平方项效果更好：$polar\ IoU=\frac{\sum_{i=1}^n d_{min}}{\sum_{i=1}^n d_{max}}$</li><li>polar iou loss：bce of polar IoU，$-log(\frac{\sum_{i=1}^n d_{min}}{\sum_{i=1}^n d_{max}})$</li><li>advantage<ul><li>differentiable, enable bp</li><li>regards the regression targets as a whole</li><li>keep balance with classification loss </li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 实例分割，极坐标，one-stage </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>FCOS</title>
      <link href="/2020/06/23/FCOS/"/>
      <url>/2020/06/23/FCOS/</url>
      <content type="html"><![CDATA[<h2 id="FCOS-Fully-Convolutional-One-Stage-Object-Detection"><a href="#FCOS-Fully-Convolutional-One-Stage-Object-Detection" class="headerlink" title="FCOS: Fully Convolutional One-Stage Object Detection"></a>FCOS: Fully Convolutional One-Stage Object Detection</h2><ol><li><p>动机</p><ul><li>anchor free</li><li>proposal free</li><li>avoids the complicated computation related to anchor boxes<ul><li>calculating overlapping during training</li></ul></li><li>avoid all hyper-parameters related to anchor boxes<ul><li>size &amp; shape</li><li>positive／ignored／negative </li></ul></li><li>leverage as many foreground samples as possible</li></ul></li><li><p>论点</p><ul><li><p>anchor-based detectors</p><ul><li>detection performance is sensitive to anchor settings</li><li>encounter difficulties in cases with large shape variations</li><li>hamper the generalization ability of detectors </li><li>dense propose：the excessive number of negative samples aggravates the imbalance </li><li>involve complicated computation：such as calculating the IoU with gt boxes</li></ul></li><li><p>FCN-based detector</p><ul><li>predict a 4D vector plus a class category at each spatial location on a level of feature maps</li><li>do not work well when applied to overlapped bounding boxes</li><li><p><strong>with FPN this ambiguity can be largely eliminated</strong></p><p><img src="/2020/06/23/FCOS/fcn-based.png" width="40%;"></p></li></ul></li><li><p>anchor-free detector</p><ul><li>yolov1：only the points near the center are used，low recall  </li><li>CornerNet：complicated post-processing to match the pairs of corners</li><li>DenseBox：difficulty in handling overlapping bounding boxes </li></ul></li><li><p>this methos</p><ul><li>use FPN to deal with ambiguity</li><li>dense predict：<strong>use all points in a ground truth bounding box</strong> to predict the bounding box</li><li>introduce “center-ness” branch to predict the deviation of a pixel to the center of its corresponding bounding box</li><li>can be used as a RPN in two-stage detectors and can achieve significantly better performance</li></ul></li></ul></li><li><p>方法</p><ul><li><p>ground truth boxes，$B_i=(x_0, y_0, x_1, y_1, c)$，corners + cls</p></li><li><p>anchor-free：each location (x,y)，map into abs input image (xs+[s/2], ys+[s/2])</p></li></ul></li></ol><ul><li><p>positive sample：if a location falls into any ground-truth box</p></li><li><p>ambiguous sample：location falls into multiple gt boxes，<strong>choose the box with minimal area</strong></p></li><li><p>regression target：l t r b distance，location to the four sides</p><ul><li><p>cls branch</p><ul><li>C binary classifiers </li><li>C-dims vector p</li></ul></li><li>focal loss<ul><li>$\frac{1}{N_{pos}} \sum_{x,y}L_{cls}(p_{x,y}, c_{x,y}^*)$</li></ul></li><li><p>calculate on both positive/negative samples</p></li><li><p>box reg branch</p><ul><li>4-dims vector t</li></ul></li><li>IOU loss<ul><li>$\frac{1}{N_{pos}} \sum_{x,y}1_{\{c_{x,y}^<em>&gt;0\}}L_{reg}(t_{x,y}, t_{x,y}^</em>)$</li></ul></li><li>calculate on positive samples</li></ul></li><li><p>inference</p><ul><li><p>choose the location with p &gt; 0.05 as positive samples</p></li><li><p>two possible issues  </p><ul><li>large stride makes BPR low, which is actually not a problem in FCOS</li></ul></li><li><p>overlaps gt boxes cause ambiguity, which can be greatly resolved with multi-level prediction </p></li><li><p>FPN</p><ul><li>P3, P4, P5：1x1 conv from C3, C4, C5, top-down connections</li></ul></li><li><p>P6, P7: stride2 conv from P5, P6</p></li><li><p>limit the bbox regression for each level</p><ul><li>$m_i$：maximum distance for each level</li></ul></li><li>if a location’s gt bbox satifies：$max(l^<em>,t^</em>,r^<em>,b^</em>)&gt;m_i$ or $max(l^<em>,t^</em>,r^<em>,b^</em>)&lt;m_{i-1}$，it is set as a negative sample，not regress at current level<ul><li>objects with different sizes are assigned to different feature levels：largely alleviate一部分box overlapping问题</li></ul></li><li><p>for other overlapping cases：simply choose the gt box with minimal area</p></li><li><p>sharing heads between different feature levels </p></li><li><p>to regress different size range：use $exp(s_ix)$</p><ul><li>trainable scalar $s_i$</li></ul></li><li>slightly improve</li></ul></li><li><p>center-ness</p><ul><li><p>low-quality predicted bounding boxes are produced by locations far away from the center of an object</p><ul><li><p>predict the “center-ness” of a location </p></li><li><p>normalized distance </p><script type="math/tex; mode=display">  centerness^* = \sqrt {\frac{min(l^*,r^*)}{max(l^*,r^*)}* \frac{min(t^*,b^*)}{max(t^*,b^*)}}</script></li></ul></li><li><p>sqrt to slow down the decay</p></li><li><p>[0,1] use bce loss</p></li><li><p>when inference center-ness is mutiplied with the class score：can down-weight the scores of bounding boxes far from the center of an object, then filtered out by NMS</p><ul><li>an alternative of the center-ness：use of only the central portion of ground-truth bounding box as positive samples，实验证明两种方法结合效果最好</li></ul></li><li><p>architecture</p><ul><li>two minor differences from the standard RetinaNet<ul><li>use <strong>Group Normalization</strong> in the <strong>newly added convolutional layers</strong> except for the last prediction layers </li><li>use P5 instead of C5 to produce P6&amp;P7</li></ul></li></ul><p><img src="/2020/06/23/FCOS/architecture.png" width="70%;"></p><p>​    </p></li></ul></li></ul>]]></content>
      
      
        <tags>
            
            <tag> 目标检测，全卷积，one-stage，centerness，anchor free </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>FCIS</title>
      <link href="/2020/06/22/FCIS/"/>
      <url>/2020/06/22/FCIS/</url>
      <content type="html"><![CDATA[<h2 id="Fully-Convolutional-Instance-aware-Semantic-Segmentation"><a href="#Fully-Convolutional-Instance-aware-Semantic-Segmentation" class="headerlink" title="Fully Convolutional Instance-aware Semantic Segmentation"></a>Fully Convolutional Instance-aware Semantic Segmentation</h2><ol><li><p>动机</p><ul><li>instance segmentation：<ul><li>实例分割比起检测，需要得到目标更精确的边界信息</li><li>比起语义分割，需要区分不同的物体</li></ul></li><li>detects and segments simultanously</li><li>FCN + instance mask proposal</li></ul></li><li><p>论点</p><ul><li>FCNs do not work for the instance-aware semantic segmentation task <ul><li>convolution is translation invariant：权值共享，一个像素值对应一个响应值，与位置无关</li></ul></li><li>instance segmentation operates on region level <ul><li>the same pixel can have different semantics in different regions </li><li>Certain translation-variant property is required </li></ul></li><li>prevalent method<ul><li>step1: an FCN is applied on the whole image to generate shared feature maps </li><li>step2: a pooling layer warps each region of interest into fixed-size per-ROI feature maps </li><li>step3: use fc layers to convert the per-ROI feature maps to per-ROI masks </li><li><strong>the translation-variant property is introduced in the fc layer(s) in the last step</strong></li><li>drawbacks <ul><li>the ROI pooling step losses spatial details </li><li>the fc layers over-parametrize the task </li></ul></li></ul></li><li>InstanceFCN<ul><li>position-sensitive score maps</li><li>sliding windows </li><li>sub-tasks are separated and the solution is not end-to-end </li><li>blind to the object categories：前背景分割</li></ul></li><li><p>In this work</p><ul><li>extends InstanceFCN</li><li>end-to-end </li><li>fully convolutional </li><li>operates on box proposals instead of sliding windows</li><li>per-ROI computation does not involve any warping or resizing operations</li></ul><p><img src="/2020/06/22/FCIS/FCIS.png" width="60%"></p></li></ul></li><li><p>方法</p><ul><li><p>position-sensitive score map</p><ul><li>FCN<ul><li>predict a single score map</li><li>predict each pixel’s likelihood score of belonging to each category</li></ul></li><li>at instance level<ul><li>the same pixel can be foreground on one object but background on another </li><li>a single score map per-category is insufficient to distinguish these two cases</li></ul></li><li>a fully convolutional solution for instance mask proposal <ul><li>k x k evenly partitioned cells of object </li><li>thus obtain k x k position-sensitive score maps </li><li>Each score represents 当前像素在<strong>当前位置（score map在cells中的位置）</strong>上属于某个物体实例的似然得分</li><li>assembling (copy-paste) </li></ul></li></ul></li><li><p>jointly and simultaneously</p><ul><li>The same set of score maps are shared for the two sub-tasks </li><li>For each pixel in a ROI, there are two tasks:  <ul><li>detection：whether it belongs to an object bounding box </li><li>segmentation：whether it is inside an object instance’s boundary </li><li>separate：two 1x1 conv heads</li><li>fuse：inside and outside <ul><li>high inside score and low outside score：detection+, segmentation+</li><li>low inside score and high outside score：detection+, segmentation-</li><li>low inside score and low outside score：detection-, segmentation-</li><li>detection score<ul><li>average pooling over all pixels‘ likelihoods for each class</li><li>max(detection score) represent the object</li></ul></li><li>segmentation <ul><li>softmax(inside, outside) for each pixel to distinguish fg／bg</li></ul></li></ul></li></ul></li><li><p>All the per-ROI components are implemented through convs</p><ul><li>local weight sharing property：a regularization mechanism </li><li>without involving any feature warping, resizing or fc layers </li><li>the per-ROI computation cost is negligible</li></ul><p><img src="/2020/06/22/FCIS/fuse.png" width="70%"></p></li></ul></li><li><p>architecture </p><ul><li>ResNet back produce features with 2048 channels</li><li>a 1x1 conv reduces the dimension to 1024</li><li>x16 output stride：conv5 stride is decreased from 2 to 1, the dilation is increased from 1 to 2</li><li>head1：joint det conf &amp; segmentation<ul><li>1x1 conv，generates $2k^2(C+1)$ score maps</li><li>2 for inside／outside</li><li>$k^2$ for $k^2$个position</li><li>$(C+1)$ for fg／bg</li></ul></li><li>head2：bbox regression <ul><li>1x1 conv，$4k^2$ channels</li></ul></li><li>RPN to generate ROIs</li><li>inference<ul><li>300 ROIs </li><li>pass through the bbox regression obtaining another 300 ROIs </li><li>pass through joint head to obtain detection score&amp;fg mask for all categories </li><li>mask voting：每个ROI (with max det score) 只包含当前类别的前景，还要补上框内其他类别背景<ul><li>for current ROI, find all the ROIs (from the 600) with IoU scores higher than 0.5 </li><li>their fg masks are averaged per-pixel and weighted by the classification score </li></ul></li></ul></li><li><p>training</p><ul><li>ROI positive／negative：IoU&gt;0.5</li><li>loss<ul><li>softmax detection loss over C+1 categories</li><li>softmax segmentation loss over the gt fg mask, on positive ROIs</li><li>bbox regression loss, , on positive ROIs</li></ul></li><li>OHEM：among the 300 proposed ROIs on one image, 128 ROIs with the highest losses are selected to back-propagate their error gradients</li><li>RPN：<ul><li>9 anchors  </li><li>sharing feature between FCIS and RPN </li></ul></li></ul><p><img src="/2020/06/22/FCIS/architecture.png" width="70%"></p></li></ul></li></ul></li><li><p>实验</p><ul><li><p>metric：mAP</p></li><li><p>FCIS (translation invariant)：</p><ul><li>set k=1，achieve the worst mAP</li><li>indicating the position sensitive score map is vital for this method</li></ul></li><li><p>back</p><ul><li>50-101：increase</li><li>101-152：saturate</li></ul></li><li><p>tricks</p><p>  <img src="/2020/06/22/FCIS/tricks.png" width="40%"></p></li></ul></li></ol><pre><code>* r</code></pre>]]></content>
      
      
        <tags>
            
            <tag> 实例分割，全卷积，带位置信息的scoremap </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>天池脊柱MRI</title>
      <link href="/2020/06/11/%E5%A4%A9%E6%B1%A0%E8%84%8A%E6%9F%B1MRI/"/>
      <url>/2020/06/11/%E5%A4%A9%E6%B1%A0%E8%84%8A%E6%9F%B1MRI/</url>
      <content type="html"><![CDATA[<ol><li><p>MRI</p><p> T1加权相上面密度高的骨头会比较亮(就是高信号)，还有脂肪和甲状腺也是高信号，水份一般都是无信号，<br> T2加权相里水是高信号所以水比较亮，因为很多的病变有水肿，所以T2加权相通俗可以说是看病变(毕竟比较明显)，</p><p> 视觉直观上来看，T1看解剖，T2看病变</p><p> ——怎么fusion一个case（标注只有一张）</p></li><li><p>数据集</p><p> T1、T2矢状位，T2轴状位，</p><p> 关键点：基于T2矢状位的中间帧，</p><p> 标注范围：从胸12（T12）腰1（L1）间的椎间盘开始，到腰5（L5）骶1（S1）间的椎间盘结束</p><p> 类别：椎块有编号（T12到L5），间盘通过上下椎块的编号表示（T12-L1到L5-S1）</p><p> 病灶：</p><pre><code> * 椎块有两类：正常V1和退行性病变V2， * 椎间盘有7类：正常V1，退行性改变V2，弥漫性膨出，非对称性膨出，突出，脱出，疝出V7</code></pre><p> json结构：</p><ul><li>uid，dim，spacing等一些header info</li><li>annotation：<ul><li>slice：难道不是T2矢状位的中间帧吗？</li><li>point：关键点坐标，病灶类别，关键点类别</li></ul></li></ul></li><li><p>评估指标</p><ul><li><p>distance&lt;8mm</p></li><li><p>TP：多个命中取最近的，其余忽略</p></li><li>FP：假阳性，distance超出所有gt的8mm圈圈／落进圈圈但是类别错了</li><li>FN：假阴性，gt点没有被TP</li><li>precision：TP/(TP+FP)</li><li>recall：TP/(TP+FN)</li><li>AP</li><li>MAP</li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> competition </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>group normalization</title>
      <link href="/2020/06/08/group-normalization/"/>
      <url>/2020/06/08/group-normalization/</url>
      <content type="html"><![CDATA[<h2 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a>Group Normalization</h2><ol><li><p>动机</p><ul><li>for small batch size</li><li>do normalization in channel groups</li><li>batch-independent</li><li>behaves stably over different batch sizes</li><li>approach BN’s accuracy </li></ul><p><img src="/2020/06/08/group-normalization/error.png" width="40%;"></p></li><li><p>论点</p><ul><li>BN<ul><li>requires sufficiently large batch size (e.g. 32)</li><li>Mask R-CNN frameworks use a batch size of 1 or 2 images because of higher resolution, where BN is “frozen” by transforming to a linear layer </li><li>synchronized BN 、BR</li></ul></li><li>LN &amp; IN<ul><li>effective for training sequential models or generative models </li><li>but have limited success in visual recognition </li><li>GN能转换成LN／IN</li></ul></li><li>WN<ul><li>normalize the filter weights, instead of operating on features</li></ul></li></ul></li><li><p>方法</p><ul><li><p>group</p><ul><li>it is not necessary to think of deep neural network features as unstructured vectors <ul><li>第一层卷积核通常存在一组对称的filter，这样就能捕获到相似特征</li><li>这些特征对应的channel can be normalized together</li></ul></li></ul></li><li><p>normalization</p><ul><li><p>transform the feature x：$\hat x_i = \frac{1}{\sigma}(x_i-\mu_i)$</p></li><li><p>the mean and the standard deviation：</p><script type="math/tex; mode=display">  \mu_i=\frac{1}{m}\sum_{k\in S_i}x_k\\  \sigma_i=\sqrt {\frac{1}{m}\sum_{k\in S_i}(x_k-\mu_i)^2+\epsilon}</script></li><li><p>the set $S_i$</p><ul><li>BN：<ul><li>$S_i=\{k|k_C = i_C\}$</li><li>pixels sharing the same channel index are normalized together </li><li>for each channel, BN computes μ and σ along the (N, H, W) axes </li></ul></li><li>LN<ul><li>$S_i=\{k|k_N = i_N\}$</li><li>pixels sharing the same batch index (per sample) are normalized together </li><li>LN computes μ and σ along the (C,H,W) axes for each sample </li></ul></li><li>IN<ul><li>$S_i=\{k|k_N = i_N, k_C=i_C\}$</li><li>pixels sharing the same batch index and the same channel index are normalized together </li><li>LN computes μ and σ along the (H,W) axes for each sample </li></ul></li><li>GN<ul><li>$S_i=\{k|k_N = i_N, [\frac{k_C}{C/G}]=[\frac{i_C}{C/G}]\}$</li><li>computes μ and σ along the (H, W ) axes and along a group of C/G channels </li></ul></li></ul></li><li><p>linear transform  </p><ul><li>to keep representational ability </li><li><strong>per channel</strong></li><li>scale and shift：$y_i = \gamma \hat x_i + \beta$</li></ul><p><img src="/2020/06/08/group-normalization/gn.png" width="80%;"></p></li></ul></li><li><p>relation</p><ul><li>to LN<ul><li>LN assumes <em>all</em> channels in a layer make “similar contributions” </li><li>which is less valid with the presence of convolutions </li><li>GN improved representational power over LN</li></ul></li><li>to IN<ul><li>IN can only rely on the spatial dimension for computing the mean and variance </li><li>it misses the opportunity of exploiting the channel dependence</li><li>【QUESTION】BN也没考虑通道间的联系啊，但是计算mean和variance时跨了sample</li></ul></li></ul></li><li><p>implementation</p><ul><li>reshape</li><li>learnable $\gamma \&amp; \beta$</li><li>computable mean &amp; var</li></ul></li></ul><p><img src="/2020/06/08/group-normalization/code.png" width="50%;"></p></li><li><p>实验</p><ul><li>GN相比于BN，training error更低，但是val error略高于BN<ul><li>GN is effective for easing optimization</li><li>loses some regularization ability </li><li>it is possible that GN combined with a suitable regularizer will improve results </li></ul></li><li>选取不同的group数，所有的group&gt;1均好于group=1（LN）</li><li>选取不同的channel数（C／G），所有的channel&gt;1均好于channel=1（IN）</li><li>Object Detection  <ul><li>frozen：因为higher resolution，batch size通常设置为2/GPU，这时的BN frozen成一个线性层$y=\gamma(x-\mu)/\sigma+beta$，其中的$\mu$和$sigma$是load了pre-trained model中保存的值，并且frozen掉，不再更新</li><li>denote as BN*</li><li>replace BN* with GN during fine-tuning </li><li>use a weight decay of 0 for the γ and β parameters </li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>正则化</title>
      <link href="/2020/06/02/regularization/"/>
      <url>/2020/06/02/regularization/</url>
      <content type="html"><![CDATA[<h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><ol><li><p>正则</p><ul><li><p>正则化是用来解决神经网络过拟合的问题，通过降低模型的复杂性和约束权值，迫使神经网络学习可泛化的特征</p><ul><li>正则化可以定义为我们为了减少泛化误差而不是减少训练误差而对训练算法所做的任何改变<ul><li>对权重进行约束</li><li>对目标函数添加额外项（间接约束权值）：L1 &amp; L2正则</li><li>数据增强</li><li>降低网络复杂度：dropout，stochastic depth</li><li>early stopping</li></ul></li><li>我们在对网络进行正则化时不考虑网络的bias：正则表达式只是权值的表达式，不包含bias<ul><li>bias比weight具有更少的参数量</li><li>对bias进行正则化可能引入太多的方差，引入大量的欠拟合</li></ul></li></ul></li><li><p>L1 &amp; L2：</p><ul><li><p>要惩罚的是神经网络中每个神经元的权重大小</p></li><li><p>L2关注的是权重的平方和，是要网络中的权重接近0但不等于0，“权重衰减”</p><script type="math/tex; mode=display">  \frac{d}{dW}(\frac{\lambda}{2m}W^2) = \frac{\lambda}{m} W</script></li><li><p>L1关注的是权重的绝对值，权重可能被压缩成0，权重更新时每次减去的是一个常量</p><script type="math/tex; mode=display">  \frac{d}{dW}(\frac{\lambda}{m}W) = \frac{\lambda}{m} sgn(W)</script></li><li><p>L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0</p></li></ul></li><li><p>dropout</p><ul><li>每个epoch训练的模型都是随机的</li><li>在test的时候相当于ensemble多个模型</li></ul></li><li><p>权重共享</p></li><li><p>数据增强</p></li><li><p>隐式正则化：其出现的目的不是为了正则化，而正则化的效果是其副产品，包括early stopping，BN，随机梯度下降</p></li></ul></li><li><p>dropout &amp; drop connect（[Reference][<a href="https://zhuanlan.zhihu.com/p/108024434]）" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/108024434]）</a></p><ul><li><p>dropout：</p><ul><li>2012年Hinton提出，在模型训练时以概率p随机让隐层节点的输出变成0，暂时认为这些节点不是网络结构的一部分，但是会把它们的权重保留下来（不更新）。</li><li><p>标准dropout相当于在一层神经元之后再添加一个额外的层，这些神经元在训练期间以一定的概率将值设置为零，并在测试期间将它们乘以p。 </p><p><img src="/2020/06/02/regularization/dropout.jpg" width="70%"></p></li></ul></li><li><p>drop connect：</p><ul><li>不是随机的将隐层节点的输出变成0，而是将节点中的每个与其相连的输入权值以1-p的概率变成0。（一个是输出一个是输入）</li><li>训练阶段，对每个example／mini-batch, 每个epoch都随机sample一个mask矩阵</li><li><p>Dropconnect在测试期间采用了与标准dropout不同的方法。作者提出了dropconnect在每个神经元处的高斯近似，然后从这个高斯函数中抽取一个样本并传递给神经元激活函数。这使得dropconnect在测试时和训练时都是一种随机方法。</p><p><img src="/2020/06/02/regularization/dropconnect.png" width="40%"></p><p><img src="/2020/06/02/regularization/dropCinfer.jpg" width="80%"></p></li><li><p>伯努利分布：0-1分布</p></li></ul></li></ul></li><li><p>dropout &amp; drop connect 通常只作用于全连接层上：这俩是用来防止过多参数导致过拟合</p><ul><li><p>卷积层参数贼少，所以没必要，</p></li><li><p>针对卷积通道有spacial dropout：按照channel随机扔</p></li></ul></li><li><p>dropblock：是针对卷积层的正则化方法，相比较于dropout的random mute，能够更有效地remove掉部分语义信息，block size=1的时候退化成dropout</p><p><img src="/2020/06/02/regularization/dropblock.png" width="60%"></p></li><li><p>papers</p><p>[dropout] Improving neural networks by preventing co-adaptation of feature detectors，丢节点</p><p>[drop connect] Regularization of neural networks using dropconnect，丢weight path</p><p>[Stochastic Depth] Deep Networks with Stochastic Depth，丢layer</p><p>[DropBlock] A regularization method for convolutional networks</p></li><li><p>drop大法一句话汇总</p><ul><li>dropout：各维度完全随机扔</li><li>spacial dropout：按照channel随机扔</li><li>stochastic depth：按照res block随机扔</li><li>dropblock：在feature map上按照spacial块随机扔</li><li>cutout：在input map上按照spacial块随机扔</li><li>dropconnect：扔连接不扔神经元</li></ul></li></ol><h2 id="Deep-Networks-with-Stochastic-Depth"><a href="#Deep-Networks-with-Stochastic-Depth" class="headerlink" title="Deep Networks with Stochastic Depth"></a>Deep Networks with Stochastic Depth</h2><ol><li><p>动机</p><ul><li>propose a training procedure：stochastic depth，train short and test deep</li><li>for each mini-batch<ul><li>randomly drop a subset of layers  </li><li>and bypass them with the identity function</li></ul></li><li>short：reduces training time </li><li>reg：improves the test error </li><li>can increase the network depth </li></ul></li><li><p>论点</p><ul><li><p>deeper</p><ul><li>expressiveness </li><li>vanishing gradients </li><li>diminishing feature reuse  </li></ul></li><li><p>resnet </p><ul><li>skip connection</li><li><p>when输入输出channel数不match：redefine id(·) as a linear projection to reduce the dimensions </p><p><img src="/2020/06/02/regularization/resnet.png" width="60%"></p></li></ul></li><li><p>dropout</p><ul><li>Dropout reduces the effect known as “co- adaptation” of hidden nodes </li><li>Dropout loses effectiveness when used in combination with Batch Normalization </li></ul></li><li><p>our approach</p><ul><li>higher diversity </li><li>shorter instead of thinner </li><li>work with Batch Normalization</li></ul></li></ul></li><li><p>方法</p><ul><li><p>stochastic depth</p><ul><li>randomly dropping entire ResBlocks </li><li>$H_l = ReLU(b_l Res_l(H_{l-1}) + id(H_{l-1}))$</li></ul></li><li><p>survival probabilities </p><ul><li><p>$p_l = Pr(b_l=1)$</p></li><li><p>set uniformly / set following a linear decay rule </p></li><li><p>set $p_0=1, p_L=0.5$：</p><script type="math/tex; mode=display">  p_l = 1 - \frac{l}{L}(1-p_L)</script></li><li><p>intuition：the earlier layers extract low-level features that will be used by later layers and should therefore be more reliably present </p><p><img src="/2020/06/02/regularization/stochastic.png" width="60%"></p></li></ul></li><li><p>Expected network depth </p><ul><li>$E(L) \approx 3L/4$</li><li>approximately 25% of training time could be saved </li></ul></li><li><p>during testing</p><ul><li>all res path are active  </li><li>each res path is weighted by its survival probability</li><li>$H_l^{Test} = ReLU(b_l Res_l(H_{l-1}, W_l) + id(H_{l-1}))$</li><li>跟dropout一样</li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 正则化，dropout，dropconnect </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>RetinaNet</title>
      <link href="/2020/05/30/RetinaNet/"/>
      <url>/2020/05/30/RetinaNet/</url>
      <content type="html"><![CDATA[<ol><li>[det] RetinaNet: Focal Loss for Dense Object Detection</li><li>[det+instance seg] RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free </li><li>[det+semantic seg] Retina U-Net: Embarrassingly Simple Exploitation of Segmentation Supervision for Medical Object Detection </li></ol><h2 id="Focal-Loss-for-Dense-Object-Detection"><a href="#Focal-Loss-for-Dense-Object-Detection" class="headerlink" title="Focal Loss for Dense Object Detection"></a>Focal Loss for Dense Object Detection</h2><ol><li><p>动机</p><ul><li>dense prediction(one-stage detector)</li><li>focal loss：address the class imbalance problem</li><li>RetinaNet：design and train a simple dense detector </li></ul></li><li><p>论点</p><ul><li>accuracy trailed<ul><li>two-stage：classifier is applied to a sparse set of candidate</li><li>one-stage：dense sampling of possible object locations</li><li>the <strong>extreme foreground-background class imbalance</strong> encountered during training of dense detectors is the central cause</li></ul></li><li>loss<ul><li>standard cross entropy loss：down-weights the loss assigned to well-classified examples</li><li>proposed focal loss：focuses training on a sparse set of hard examples</li></ul></li><li>R-CNN系列two-stage framework <ul><li>proposal-driven </li><li>the first stage generates a sparse set of candidate object locations</li><li>the second stage classifies each candidate location as one of the foreground classes or as background</li><li>class imbalance：在stage1大部分背景被filter out了，stage2训练的时候强制固定前背景样本比例，再加上困难样本挖掘OHEM </li><li>faster：reducing input image resolution and the number of proposals </li><li>ever faster：one-stage</li></ul></li><li>one-stage detectors <ul><li>One stage detectors are applied over a regular, dense sampling of object locations, scales, and aspect ratios</li><li>dense：regularly sampling(contrast to selection)，基于grid以及anchor以及多尺度</li><li>the training procedure is still dominated by easily classified background examples</li><li>class imbalance：通常引入bootstrapping和hard example mining来优化</li></ul></li><li>Object Detectors<ul><li>Classic：sliding-window+classifier based on HOG，dense predict</li><li>Two-stage：selective Search+classifier based on CNN，shared network RPN</li><li>One-stage：‘anchors’ introduced by RPN，FPN</li></ul></li><li>loss<ul><li>Huber loss：down-weighting the loss of outliers (hard examples)</li><li>focal loss：down-weighting inliers (easy examples)  </li></ul></li></ul></li><li><p>方法</p><ul><li><p>focal loss</p><ul><li>CE：$CE(p_t)=-log(p_t)$<ul><li>even examples that are easily classified ($p_t&gt;0.5$) incur a loss with non-trivial magnitude</li><li>summed CE loss over a large number of easy examples can overwhelm the rare class</li></ul></li><li>WCE：$WCE(p_t)=-\alpha_t log(p_t)$<ul><li>balances the importance of positive/negative examples</li><li>does not differentiate between easy/hard examples </li></ul></li><li><p>FL：$FL(p_t)=-\alpha_t(1-p_t)^\gamma log(p_t)$</p><ul><li>as $\gamma$ increases the modulating factor is likewise increased </li><li>$\gamma=2$ works best in our experiments </li></ul><p><img src="/2020/05/30/RetinaNet/loss.png" width="40%;"></p><p>​    </p></li><li><p>two-stage detectors通常不会使用WCE或FL</p><ul><li>cascade stage会过滤掉大部分easy negatives </li><li>第二阶段训练会做biased minibatch sampling </li><li>Online Hard Example Mining (OHEM)<ul><li>construct minibatches using high-loss examples</li><li>scored by loss + nms</li><li>completely discards easy examples</li></ul></li></ul></li></ul></li><li><p>RetinaNet</p><ul><li>compose：backbone network + two task-specific subnetworks</li><li>backbone：convolutional feature map over the entire input image </li><li>subnet1：object classification </li><li><p>subnet2：bounding box regression </p><p><img src="/2020/05/30/RetinaNet/retinanet.png" width="60%;"></p></li><li><p>ResNet-FPN backbone</p><ul><li>rich, multi-scale feature pyramid，二阶段的RPN也用了FPN</li><li>each level can be used for detecting objects at a different scale</li><li>P3 - P7：8x - 128x downsamp</li><li>FPN channels：256</li></ul></li><li><p>anchors</p><ul><li>anchor ratios：{1:2, 1:1, 2:1}，长宽比</li><li>anchor scales：{$2^0$, $2^\frac{1}{3}$, $2^\frac{2}{3}$}，大小，同一个scale的anchor，面积相同，都是size*size，长宽通过ratio求得</li><li>anchor size per level：[32, 64, 128, 256, 512]，基本的正方形anchor的边长</li><li>total anchors per level：A=9</li><li>KA：each anchor is assigned a length K one-hot vector of classification targets </li><li>4A：and a 4-vector of box regression targets </li><li>anchors are assigned to ground-truth object boxes using an intersection-over-union (IoU) threshold of 0.5 </li><li>anchors are <strong>assigned background</strong> if their IoU is in [0, 0.4)</li><li>anchor is unassigned between [0.4, 0.5), which is ignored during training</li><li>each anchor is assigned to at most one object box </li><li><p>for each anchor</p><ul><li><p>classification targets：one-hot vector </p></li><li><p>box regression targets：each anchor和其对应的gt box的offset</p></li></ul></li></ul></li><li><p>rpn offset：中心点、宽、高</p><pre><code>  $$</code></pre><p>  t_x = (x - x_a) / w_a\\</p><pre><code>    t_y = (y - y_a) / h_a\\</code></pre><p>  t_w = log(w/ w_a)\\</p><pre><code>    t_h = log(h/ h_a)</code></pre><p>  $$</p></li><li><p>or omitted if there is no assignment</p></li><li><p>【QUESTION】所谓的anchor state {-1:ignore, 0:negative, 1:positive} 是针对cls loss来说的，相当于人为丢弃了一部分偏向中立的样本，这对分类效果有提升吗？？</p></li><li><p>classification subnet</p><ul><li><p>for each spatial position，for each anchor，predict one among K classes，one-hot</p></li><li><p>input：C channels feature map from FPN</p></li><li><p>structure：four 3x3 conv + ReLU，each with C filters </p></li><li><p>head：3x3 conv + sigmoid，with KA filters</p></li><li><p>share across levels</p></li></ul></li></ul></li><li><p>not share with box regression subnet </p></li><li><p>focal loss：</p><ul><li>sum over all ～100k anchors<pre><code>  * and normalized by the number of anchors assigned to a ground-truth box  * 因为是sum，所以要normailize，norm项用的是number of assigned anchors（这是包括了前背景？）  * vast majority of anchors are **easy negatives** and receive negligible loss values under the focal loss（确实包含背景框）  * $\alpha$：In general $alpha$ should be decreased slightly as $\gamma$ is increased </code></pre></li><li><p>strong effect on negatives：FL can effectively discount the effect of easy negatives, focusing all attention on the hard negative examples </p><p><img src="/2020/05/30/RetinaNet/bg.png" width="80%;"></p></li><li><p>box regression subnet </p><ul><li>class-agnostic bounding box regressor  </li></ul></li></ul></li><li>same structure：four 3x3 conv + ReLU，each with C filters <pre><code>  * head：4A linear outputs   * L1 loss</code></pre></li></ul></li></ol><ul><li><p>inference</p><ul><li><p>keep top 1k predictions per FPN level </p><pre><code>  * all levels are merged and non-maximum suppression with a threshold of 0.5 </code></pre><ul><li><p>train</p><ul><li>initialization：<ul><li>cls head bias initialization，encourage more foreground prediction at the start of training </li><li>prevents the large number of background anchors from generating a large, destabilizing loss</li></ul></li></ul></li></ul></li></ul></li><li><p>network design</p><ul><li><p>anchors</p><pre><code>      * one-stage detecors use fixed sampling grid to generate position      * use multiple ‘anchors’ at each spatial position to cover boxes of various scales and aspect ratios       * beyond 6-9 anchors did not shown further gains in AP  * speed/accuracy trade-off        * outperforms all previous methods      * bigger resolution bigger AP      * Retina-101-600与ResNet101-FRCNN的AP持平，但是比他快</code></pre><ul><li><p>gradient：</p><ul><li>梯度有界</li></ul></li></ul></li></ul></li><li><p>the derivative is small as soon as $x_t &gt; 0$</p><pre><code>      &lt;img src=&quot;RetinaNet/gradient.png&quot; width=&quot;70%;&quot; /&gt;</code></pre></li></ul><h2 id="RetinaMask-Learning-to-predict-masks-improves-state-of-the-art-single-shot-detection-for-free"><a href="#RetinaMask-Learning-to-predict-masks-improves-state-of-the-art-single-shot-detection-for-free" class="headerlink" title="RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free"></a>RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</h2><ol><li><p>动机</p><ul><li>improve single-shot detectors to the same level as current two-stage techniques</li><li>improve on RetinaNet<ul><li>integrating instance mask prediction</li><li>adaptive loss </li><li>additional hard examples</li><li>Group Normalization</li></ul></li><li><p>same computational cost as the original RetinaNet but more accurate：同样的参数量级比orgin RetinaNet准，整体的参数量级大于yolov3，acc快要接近二阶段的mask RCNN了</p><p><img src="/2020/05/30/RetinaNet/acc:time.png" width="40%;"></p></li></ul></li></ol><ol><li><p>论点</p><ul><li>part of improvements of two-stage detectors is due to architectures like Mask R-CNN that involves multiple prediction heads</li><li>additional segmentation task had only been added to two-stage detectors in the past</li><li>two-stage detectors have the cost of resampling(ROI-Align) issue：RPN之后要特征对齐</li><li>add addtional heads in training keeps the structure of the detector at test time unchanged </li><li>potential improvement directions<ul><li>data：OHEM</li><li>context：FPN</li><li>additional task：segmentation branch</li></ul></li><li>this paper’s contribution<ul><li>add a mask prediction branch</li><li>propose a new self-adjusting loss function</li><li>include more of positive samples—&gt;those with low overlap</li></ul></li></ul></li><li><p>方法</p><ul><li><p>best matching policy</p><ul><li>speical case：outlier gt box，跟所有的anchor iou都不大于0.5，永远不会被当作正样本</li><li>use best matching anchor with any nonzero overlap to replace the threshold</li></ul></li><li><p>self-adjusting Smooth L1 loss</p><ul><li><p>bbox regression</p></li><li><p>smooth L1：</p><ul><li>L1 loss is used beyond $\beta$ to <strong>avoid over-penalizing outliers</strong></li><li><p>the choice of control point is heuristic and is usually done by hyper parameter search </p><script type="math/tex; mode=display">f(x) = \begin{cases}0.5 \frac{x^2}{\beta} \text{,  if  } |x| < \beta \\|x| - 0.5\beta \text{,  otherwise  }\end{cases}</script></li></ul></li><li><p>self-adjusting control point</p><ul><li><p>running mean &amp; variance</p><script type="math/tex; mode=display">  \mu_B = \frac{1}{n}\sum_{i=1}^n |x_i|\\  \sigma_B^2 = \frac{1}{n}\sum_{i=1}^n(|x_i|-\mu_B)^2</script></li><li><p>minibatch update：m=0.9</p><script type="math/tex; mode=display">  \mu_R = \mu_R * m + \mu_B*(1-m)\\  \sigma_R^2 = \sigma_R^2*m+\sigma_B^2*(1-m)</script></li><li><p>control point：$[0, \hat \beta]$ clip to avoid unstable </p><script type="math/tex; mode=display">  \beta = max(0, min(\hat \beta, \mu_R-\sigma_R^2))</script></li></ul></li></ul></li></ul></li></ol><pre><code>* mask module    * detection predictions are treated as mask proposals     * extract the top N scored predictions     * distribute the mask proposals to sample features from the appropriate layers         $$        k = [k_0 + log_2 \sqrt{wh}/224]        $$        * $k_0=4$，如果size小于224\*224，proposal会被分配给P3，如果大于448\*448，proposal会被分配给P5        * using more feature layers shows no performance boost* architecture     * r50&amp;r101 back：freezing all of the Batch Nor- malization layers     * fpn feature channel：256    * classification branch        * 4 conv layers：conv3x3+relu，channel256        * head：conv3x3+sigmoid，channel n_anchors*n_classes    * regression branch        * 4 conv layers：conv3x3+relu，channel256        * head：conv3x3，channel n_anchors*4    * aggregate the boxes to the FPN layers     * ROI-Align yielding 14x14 resolution features     * mask head        * 4 conv layers：conv3x3        * a single transposed convolutional layer：convtranspose2d 2x2，to 28\*28 resolution        * prediction head：conv1x1    &lt;img src=&quot;RetinaNet/retinaMask.png&quot; width=&quot;70%;&quot; /&gt;* training    * min side &amp; max side：800&amp;1333    * limited GPU：reduce the batch size，increasing the number of training iterations and reducing the learning rate accordingly    * positive/ignore/negative：0.5，0.4    * focal loss for classification        * gaussian initialization        * $\alpha=0.25, \lambda=2.0$        * $FL=-\alpha_t(1-p_t)^\lambda log(p_t)$    * self-adjusting L1 loss for box regression        * limit running params：[0, 0.11]    * mask loss        * top-100 predicted boxes + ground truth boxes* inference    * box confidence threshold 0.05    * nms threshold 0.4    * use top-50 boxes for mask prediction</code></pre><h2 id="Retina-U-Net-Embarrassingly-Simple-Exploitation-of-Segmentation-Supervision-for-Medical-Object-Detection"><a href="#Retina-U-Net-Embarrassingly-Simple-Exploitation-of-Segmentation-Supervision-for-Medical-Object-Detection" class="headerlink" title="Retina U-Net: Embarrassingly Simple Exploitation of Segmentation Supervision for Medical Object Detection"></a>Retina U-Net: Embarrassingly Simple Exploitation of Segmentation Supervision for Medical Object Detection</h2><ol><li><p>动机</p><ul><li>localization<ul><li>pixel-level predict</li><li>ad-hoc heuristics when mapping back to object-level scores</li></ul></li><li>semantic segmentation<ul><li>auxiliary task</li><li>overall one-stage </li><li>leveraging available supervision signals</li></ul></li></ul></li><li><p>论点</p><ul><li>monitoring pixel-wise predictions are clinically required </li><li>medical annotations is commonly performed in pixel- wise  </li><li>full semantic supervision <ul><li>fully exploiting the available semantic segmentation signal results in significant performance gains  </li></ul></li><li>one-stage<ul><li>explicit scale variance enforced by the resampling operation in two-stage detectors is not helpful in the medical domain </li></ul></li><li>two-stage methods<ul><li>predict proposal-based segmentations </li><li>mask loss is only evaluated on cropped proposal：no context gradients</li><li>ROI-Align：not suggested in medical image</li><li>depends on the results of region proposal：serial vs parallel</li><li>gradients of the mask loss do not flow through the entire model</li></ul></li></ul></li><li><p>方法</p><ul><li><p>model</p><ul><li>back：<ul><li>ResNet50 </li></ul></li><li>fpn：<ul><li>shift p3-p6 to p2-p5</li><li>change sigmoid to softmax</li><li>3d head channels：64</li><li>anchor size：$\{P_2: 4^2, P_3: 8^2,, P_4: 16^2,, P_5: 32^2\}$</li><li>3d z-scale：{1，2，4，8}，考虑到z方向的low resolution</li></ul></li><li>segmentation supervision<ul><li>p0 &amp; p1</li><li>with skip connections </li><li>without detection heads</li><li>segmentation loss calculates on p0 logits</li><li>dice + ce</li></ul></li><li><p>h</p><p><img src="/2020/05/30/RetinaNet/retinaUnet.png" width="60%;"></p></li></ul></li><li><p>weighted box clustering</p><ul><li><p>patch crop</p></li><li><p>tiling strategies &amp; model ensembling causes multi predictions per location </p></li><li><p>nms选了一类中score最大的box，然后抑制所有与它同类的IoU大于一定阈值的box</p></li><li><p>weighted box作用于这一类所有的box，计算一个融合的结果</p><ul><li>coordinates confidence：$o_c = \frac{\sum c_i s_i w_i}{\sum s_i w_i}$</li><li><p>score confidence：$o_s = \frac{\sum s_i w_i}{\sum w_i + n_{missing * \overline w}}$</p></li><li><p>$w_i$：$w=f a p$</p><ul><li>overlap factor f：与highest scoring box的overlap</li><li>area factor a：higher weights to larger boxes，经验</li><li>patch center factor p：相对于patch center的正态分布</li></ul></li><li>score confidence的分母上有一个down-weight项$n_{missing}$：基于prior knowledge预期prediction的总数得到</li></ul></li><li><p>论文给的例子让我感觉好比nms的点</p><ul><li>一个cluster里面一类最终就留下一个框：解决nms一类大框包小框的情况</li><li>这个location上prediction明显少于prior knowledge的类别confidence会被显著拉低：解决一个位置出现大概率假阳框的情况</li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 目标检测，focalloss，实例分割，自适应smoothL1 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DCGAN</title>
      <link href="/2020/05/27/DCGAN/"/>
      <url>/2020/05/27/DCGAN/</url>
      <content type="html"><![CDATA[<h2 id="UNSUPERVISED-REPRESENTATION-LEARNING-WITH-DEEP-CONVOLUTIONAL-GENERATIVE-ADVERSARIAL-NETWORKS"><a href="#UNSUPERVISED-REPRESENTATION-LEARNING-WITH-DEEP-CONVOLUTIONAL-GENERATIVE-ADVERSARIAL-NETWORKS" class="headerlink" title="UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS"></a>UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS</h2><ol><li><p>动机</p><ul><li>unsupervised learning</li><li>learns a hierarchy of representations from object parts to scenes </li><li>used for novel tasks </li></ul></li><li><p>论点</p><ul><li>GAN<ul><li>Learning reusable feature representations from large unlabeled datasets </li><li>generator and discriminator networks can be later used as feature extractors for supervised tasks </li><li>unstable to train </li></ul></li><li>we<ul><li>propose a set of constraints on the architectural topology making it stable to train</li><li>use the trained discriminators for image classification tasks </li><li>visualize the filters </li><li>show that the generators have interesting vector arithmetic properties  </li></ul></li><li>unsupervised representation learning <ul><li>clustering, hierarchical clustering </li><li>auto-encoders learn good feature representations </li></ul></li><li>generative image models <ul><li>samples often suffer from being blurry, being noisy and incomprehensible </li><li>further use for supervised tasks</li></ul></li></ul></li><li><p>方法</p><ul><li><p>architecture </p><ul><li>all convolutional net：没有池化，用stride conv</li><li>eliminating fully connected layers：<ul><li>generator：输入是一个向量，reshape以后接的全是卷积层</li><li>discriminator：最后一层卷积出来直接flatten</li></ul></li><li>Batch Normalization <ul><li>generator输出层 &amp; discriminator输入层不加</li><li>resulted in sample oscillation and model instability </li></ul></li><li><p>ReLU</p><ul><li>generator输出层用Tanh </li><li>discriminator用leakyReLU</li></ul><p><img src="/2020/05/27/DCGAN/gen.png" width="60%;"></p></li></ul></li><li><p>train</p><ul><li>image preprocess：rescale to [-1,1]</li><li>LeakyReLU(0.2)</li><li>lr：2e-4</li><li>momentum term $\beta 1$：0.5, default 0.9</li></ul></li></ul></li><li><p>实验</p><ul><li>evaluate<ul><li>apply them as a feature extractor on supervised datasets</li><li>evaluate the performance of linear models on top of these features </li></ul></li><li>model<ul><li>use the discriminator’s convolutional features from all layers</li><li>maxpooling to 4x4 grids</li><li>flattened and concatenated to form a 28672 dimensional vector  </li><li>regularized linear L2-SVM </li><li>相比之下：the discriminator has many less feature maps, but larger total feature vector size </li></ul></li><li>visualizing<ul><li>walking in the latent space <ul><li>在vector Z上差值，生成图像可以观察到smooth transitions </li></ul></li><li>visualize the discriminator feature<ul><li>特征图可视化，能观察到床结构</li></ul></li><li>manipulate the generator representation<ul><li>generator learns <strong>specific object representations</strong> for major scene components  </li><li>use logistic regression to find feature maps related with window, drop the spatial locations on feature-maps</li><li>most result forgets to draw windows in the bedrooms, replacing them with other objects</li></ul></li></ul></li><li>vector arithmetic <ul><li>averaging the Z vector for three examplars </li><li>semantically obeyed the arithmetic </li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>densenet</title>
      <link href="/2020/05/27/densenet/"/>
      <url>/2020/05/27/densenet/</url>
      <content type="html"><![CDATA[<ol><li><p>动机</p><ul><li>embrace shorter connections</li><li>the feature-maps of all preceding layers are used as inputs</li><li>advantages<ul><li>alleviate vanishing-gradient</li><li>encourage feature reuse</li><li>reduce the number of parameters</li></ul></li></ul></li><li><p>论点</p><ul><li>Dense<ul><li>each layer obtains additional inputs from all preceding lay- ers and passes on its own feature-maps to all subsequent layers</li><li>feature reuse</li><li>combine features by concatenating：the summation in ResNet may impede the information flow in the network</li></ul></li><li>information preservation<ul><li>id shortcut/additive identity transformations</li></ul></li><li><p>fewer params</p><ul><li>DenseNet layers are very narrow </li><li>add only a small set of feature-maps to the “collective knowledge”</li></ul></li><li><p>gradients flow</p><ul><li>each layer has direct access to the gradients from the loss function  </li><li>have regularizing effect </li></ul><p><img src="/2020/05/27/densenet/densenet.png" width="45%;"></p></li></ul></li><li><p>方法</p><ul><li><p>architecture</p><p>  <img src="/2020/05/27/densenet/block.png" width="70%;"></p><ul><li>dense blocks<ul><li>concat</li><li>BN-ReLU-3x3 conv</li><li>$x_l = H_l([x_0, x_1, …, x_{l-1}])$</li></ul></li><li>transition layers<ul><li>change the size of feature-maps</li><li>BN-1x1 conv-2x2 avg pooling</li></ul></li><li><p>growth rate k</p><ul><li>$H_l$ produces feature- maps</li><li>narrow：<em>e.g.</em>, k = 12 </li><li>One can view the feature-maps as the global state of the network </li><li>The growth rate regulates <strong>how much new information</strong> each layer contributes to the global state</li></ul><p><img src="/2020/05/27/densenet/architectures.png" width="80%;"></p></li><li><p>bottleneck —- DenseNet-B </p><ul><li>in dense block stage</li><li>1x1 conv reduce dimension first</li><li>number of channels：4k </li></ul></li><li><p>compression —- DenseNet-C</p><ul><li>in transition stage</li><li>reduce the number of feature-maps </li><li>number of channels：$\theta k$</li></ul></li><li><p>structure configurations </p><ul><li>1st conv channels：第一层卷积通道数</li><li>number of dense blocks</li><li>L：dense block里面的layer数</li><li>k：growth rate</li><li>B：bottleneck 4k</li><li>C：compression 0.5k</li></ul></li></ul></li></ul></li><li><p>讨论</p><ul><li><p>concat replace sum：</p><ul><li>seemingly small modification lead to substantially different behaviors of the two network architectures </li><li>feature reuse：feature can be accessed anywhere</li><li>parameter efficient：同样参数量，test acc更高，同样acc，参数量更少</li><li><p>deep supervision：classifiers attached to every hidden layer</p></li><li><p>weight assign</p><ul><li>All layers spread their weights over multi inputs (include transition layers)</li><li>least weight are assigned to the transition layer, indicating that transition layers contain many redundant features, <strong>thus can be compressed</strong></li><li>overall there seems to be concentration towards final feature-maps, suggesting that more high-level features are produced late in the network</li></ul><p><img src="/2020/05/27/densenet/weight.png" width="50%;"></p></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>GANomaly</title>
      <link href="/2020/05/25/GANomaly/"/>
      <url>/2020/05/25/GANomaly/</url>
      <content type="html"><![CDATA[<h2 id="GANomaly-Semi-Supervised-Anomaly-Detection-via-Adversarial-Training"><a href="#GANomaly-Semi-Supervised-Anomaly-Detection-via-Adversarial-Training" class="headerlink" title="GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training"></a>GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training</h2><ol><li><p>动机</p><ul><li>Anomaly detection <ul><li>highly biased towards one class (normal) </li><li>insufficient sample size of the other class (abnormal)</li></ul></li><li>semi-supervised learning <ul><li>detecting the unknown/unseen anomaly case  </li><li>trained on normal samples </li><li>tested on normal and abnormal samples </li></ul></li><li>encoder-decoder-encoder  <ul><li>minimizing the distance between the images </li><li>and the latent vectors  </li><li>a larger distance metric </li></ul></li></ul></li><li><p>论点</p><ul><li>supervised approaches heavily depend on large, labeled datasets </li><li>Generative Adversarial Networks (GAN) have emerged as a leading methodology across both unsupervised and semi-supervised problems</li><li>reconstruction-based anomaly techniques<ul><li>Overall prior work strongly supports the hypothesis that the use of autoencoders and GAN </li></ul></li></ul></li><li><p>方法</p><ul><li>GAN<ul><li>unsupervised </li><li>to generate realistic images </li><li>compete<ul><li>generator tries to generate an image, decoder- alike network, map input to latent space</li><li>discriminator decides whether the generated image is a real or a fake, classical classification architecture, reading an input image, and determining its validity </li></ul></li></ul></li><li><p>Adversarial Auto-Encoders (AAE) </p><ul><li>encoder + decoder </li><li>reconstruction: maps the input to latent space and remaps back to input data space</li><li>train autoencoders with adversarial setting </li></ul></li><li><p>inverse mapping</p><ul><li>with the additional use of an encoder, a vanilla GAN network is capable of learning inverse mapping </li></ul></li><li><p>model</p><p>  <img src="/2020/05/25/GANomaly/GANomaly.png" width="50%;"></p><ul><li>learns both the normal data distribution and minimizes the output anomaly score </li><li>two encoder, one decoder, a discriminator</li><li>encoder<ul><li>convolutional layers followed by batch-norm and leaky ReLU() activation </li><li>compress to a vector z</li></ul></li><li>decoder<ul><li>convolutional transpose layers, ReLU() activation and batch-norm </li><li>a tanh layer at the end</li></ul></li><li>2nd encoder<ul><li>with the same architectural </li><li>but different parametrization</li></ul></li><li>discriminator <ul><li>DCGAN discriminator </li></ul></li><li>Adversarial Loss <ul><li>不是基于GAN的traditional 0/1 ouput</li><li>而是选了一个中间层，计算real／fake(reconstructed)的L2 distance</li></ul></li><li>Contextual Loss <ul><li>L1 yields less blurry results than L2</li><li>计算输入图像和重建图像的L1 distance</li></ul></li><li>Encoder Loss <ul><li>an additional encoder loss to minimize the distance of the bottleneck features </li><li>计算两个高维向量的L2 distance</li><li>在测试的时候用它来scoring the abnormality  </li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>resnets</title>
      <link href="/2020/05/23/resnets/"/>
      <url>/2020/05/23/resnets/</url>
      <content type="html"><![CDATA[<h2 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h2><ol><li><p>papers</p><p> [resnet] ResNet: Deep Residual Learning for Image Recognition </p><p> [resnext] ResNext: Aggregated Residual Transformations for Deep Neural Networks</p><p> [resnest] ResNeSt: Split-Attention Networks </p><p> [revisiting resnets] Revisiting ResNets: Improved Training and Scaling Strategies </p></li></ol><h2 id="ResNext-Aggregated-Residual-Transformations-for-Deep-Neural-Networks"><a href="#ResNext-Aggregated-Residual-Transformations-for-Deep-Neural-Networks" class="headerlink" title="ResNext: Aggregated Residual Transformations for Deep Neural Networks"></a>ResNext: Aggregated Residual Transformations for Deep Neural Networks</h2><ol><li><p>动机</p><ul><li>new network architecture</li><li>new building blocks with the same topology</li><li>propose cardinality<ul><li>increasing cardinality is able to improve classification accuracy</li><li>is more effective than going deeper or wider</li></ul></li><li>classification task</li></ul></li><li><p>论点</p><ul><li><p>VGG &amp; ResNets：</p><ul><li>stacking building blocks of the same topology</li><li>deeper</li><li>reduces the free choices of hyper-parameters </li></ul></li><li><p>Inception models </p><ul><li>split-transform-merge strategy</li><li>split：1x1conv spliting into a few lower-dimensional embeddings </li><li>transform：a set of specialized filters  </li><li><p>merge：concat</p></li><li><p>approach the representational power of large and dense layers, but at a considerably lower computational complexity</p></li><li>modules are customized stage-by-stage</li></ul></li><li><p>our architecture </p><ul><li>adopts VGG/ResNets’ repeating layers</li><li>adopts Inception‘s split-transform-merge strategy </li><li>aggregated by <strong>summation</strong></li><li><p>cardinality：the size of the set of transformations（split path数）</p><ul><li>多了1x1 conv的计算量</li><li>少了3x3 conv的计算量</li></ul><p><img src="/2020/05/23/resnets/rxblock.png" width="45%;"></p></li></ul></li></ul></li><li><p>要素</p><ul><li>Multi-branch convolutional blocks</li><li>Grouped convolutions：通道对齐，稀疏连接</li><li>Compressing convolutional networks</li><li>Ensembling </li></ul></li><li><p>方法</p><ul><li><p>architecture</p><ul><li>a template module</li><li>if producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes) </li><li><p>when the spatial map is downsampled by a factor of 2, the width of the blocks is multiplied by a factor of 2</p></li><li><p>grouped convolutions：第一个1x1和3x3conv的width要根据C进行split</p><p><img src="/2020/05/23/resnets/structure.png" width="40%;"></p></li><li><p>equivalent blocks</p><ul><li>BN after each conv</li><li>ReLU after each BN except the last of block</li><li>ReLU after add</li><li><p>r</p><p><img src="/2020/05/23/resnets/equiv.png" width="80%;"></p><p><img src="/2020/05/23/resnets/equiv2.png" width="50%;"></p></li></ul></li><li><p>Model Capacity </p><ul><li><p>improve accuracy when maintaining the model complexity and number of parameters </p></li><li><p>adjust the width of bottleneck, the according C to maintain capacity：C=1的时候退化成ResNet block</p><p>  <img src="/2020/05/23/resnets/bottled.png" width="45%;"></p></li></ul></li></ul></li></ul></li></ol><h2 id="ResNeSt-Split-Attention-Networks"><a href="#ResNeSt-Split-Attention-Networks" class="headerlink" title="ResNeSt: Split-Attention Networks"></a>ResNeSt: Split-Attention Networks</h2><ol><li><p>动机</p><ul><li>propose a modular Split-Attention block </li><li>enables attention across feature-map groups</li><li>preserve the overall ResNet structure for downstream applications such as object detection and semantic segmentation  </li><li>prove improvement on detection &amp; segmentation tasks</li></ul></li><li><p>论点</p><ul><li>ResNet<ul><li>simple and modular design </li><li>limited receptive-field size and lack of cross-channel interaction</li></ul></li><li>image classification networks have focused more on group or depth-wise convolution <ul><li>do not transfer well to other tasks </li><li>isolated representations cannot capture cross-channel relationships </li></ul></li><li>a versatile backbone <ul><li>improving performance across multiple tasks at the same time</li><li>a network with cross-channel representations is desirable </li><li>a Split-Attention block <ul><li>divides the feature-map into several groups (along the channel dimension)  </li><li>finer-grained subgroups or splits</li><li>weighted combination </li></ul></li></ul></li><li>featuremap attention mechanism：NiN’s 1x1 conv</li><li>Multi-path：GoogleNet </li><li>channel-attention mechanism：SE-Net</li><li><p>结构上，全局上看，模仿ResNext，引入cardinality和group conv，局部上看，每个group内部继续分组，然后模仿SK-Net，融合多个分支的split-attention，大group之间concat，而不是ResNext的add，再经1x1 conv调整维度，add id path</p><p><img src="/2020/05/23/resnets/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/resnets/rSblock.png" width="60%;"></p></li></ul></li><li><p>方法</p><ul><li><p>Split-Attention block </p><ul><li><p>enables feature-map attention across different feature-map groups</p></li><li><p>within a block：controlled by cardinality</p></li><li><p>within a cardinal group：introduce a new radix hyperparameter R indicating the number of splits </p></li><li><p>split-attention</p><ul><li>多个in-group branch的input输入进来</li><li>fusion：先做element-wise summation </li><li>channel-wise global contextual information：做global average pooling </li><li>降维：Dense-BN-ReLU</li><li>各分支Dense(the attention weight function)：学习各自的重要性权重</li><li>channel-wise soft attention：对全部的dense做softmax</li><li>加权：原始的各分支input与加权的dense做乘法</li><li>和：加权的各分支add</li><li><p>r=1：退化成SE-blockaverage pooling </p><p><img src="/2020/05/23/resnets/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/resnets/Split-Attention.png" width="35%;"></p></li></ul></li><li><p>shortcut connection</p><ul><li>for blocks with a strided convolution or combined convolution-with-pooling can be applied to the id</li></ul></li><li><p>concat</p></li><li><p>average pooling downsampling </p><ul><li>for dense prediction tasks：it becomes essential to preserve spatial information </li><li>former work tend to use strided 3x3 conv</li><li>we use an average pooling layer with 3x3 kernel</li><li>2x2 average pooling applied to strided shortcut connection before 1x1 conv</li></ul></li></ul></li></ul></li></ol><h2 id="Revisiting-ResNets-Improved-Training-and-Scaling-Strategies"><a href="#Revisiting-ResNets-Improved-Training-and-Scaling-Strategies" class="headerlink" title="Revisiting ResNets: Improved Training and Scaling Strategies"></a>Revisiting ResNets: Improved Training and Scaling Strategies</h2><ol><li><p>动机</p><ul><li><p>disentangle the three aspects</p><ul><li>model architecture</li><li>training methodology</li><li>scaling strategies</li></ul></li><li><p>improve ResNets to SOTA</p><ul><li>design a family of ResNet architectures, ResNet-RS </li><li>use improved training and scaling strategies </li><li>and combine minor architecture changes</li><li>在ImageNet上打败efficientNet</li><li><p>在半监督上打败efficientNet-noisystudent</p><p><img src="/2020/05/23/resnets/acc.png" width="45%;"></p></li></ul></li></ul></li><li><p>论点</p><ul><li>ImageNet上榜大法<ul><li>Architecture <ul><li>人工系列：AlexNet，VGG，ResNet，Inception，ResNeXt</li><li>NAS系列：NasNet-A，AmoebaNet-A，EfficientNet</li></ul></li><li>Training and Regularization Methods<ul><li>regularization methods<ul><li>dropout，label smoothing，stochastic depth，dropblock，data augmentation</li><li>significantly improve generalization when training more epochs</li></ul></li><li>training <ul><li>learning rate schedules</li></ul></li></ul></li><li>Scaling Strategies<ul><li>model dimension：width，depth，resolution</li><li>efficientNet提出的均衡增长，在本文中shows sub-optimal for both resnet and efficientNet</li></ul></li><li>Additional Training Data<ul><li>pretraining on larger dataset</li><li>semi-supervised</li></ul></li></ul></li><li>the performance of a vision model <ul><li>architecture：most research focus on</li><li>training methods and scaling strategy：less publicized but critical</li><li>unfair：使用modern training method的新架构与使用dated methods的老网络直接对比</li></ul></li><li>we focus on the impact of training methods and scaling strategies<ul><li>training methods：<ul><li>We survey the modern training and regularization techniques</li><li>发现引入其他正则方法的时候降低一点weight decay有好处</li></ul></li><li>scaling strategies：<ul><li>We offer new perspectives and practical advice on scaling</li><li>可能出现过拟合的时候就加depth，否则先加宽</li><li>resolution慢点增长，more slowly than prior works  </li><li>从acc图可以看到：我们的scaling strategies与网络结构的lightweight change正交，是additive的</li></ul></li></ul></li><li>re-scaled ResNets, ResNet-RS<ul><li>仅improve training &amp; scaling strategy就能大幅度涨点</li><li>combine minor architectural changes进一步涨点</li></ul></li></ul></li><li><p>方法</p><ul><li><p>architecture</p><ul><li>use ResNet with two widely used architecture changes</li><li>ResNet-D<ul><li>stem的7x7conv换成3个3x3conv</li><li>stem的maxpooling去掉，每个stage的首个3x3conv负责stride2</li><li>residual path上前两个卷积的stride互换（在3x3上下采样）</li><li>id path上的1x1 s2conv替换成2x2 s2的avg pooling+1x1conv</li></ul></li><li><p>SE in bottleneck</p><ul><li>use se-ratio of 0.25</li></ul><p><img src="/2020/05/23/resnets/ResNet-RS.png" width="75%;"></p></li></ul></li><li><p>training methods</p><ul><li>match the efficientNet setup<ul><li>train for 350 epochs</li><li>use cosine learning rate instead of exponential decay </li><li>RandAugment instead of AutoAugment </li><li>use Momentum optimizer instead of RMSProp </li></ul></li><li>regularization<ul><li>weight decay</li><li>label smoothing</li><li>dropout</li><li>stochastic depth</li></ul></li><li><p>data augmentation</p><ul><li>we use RandAugment</li><li>EfficientNet use AutoAugment which slightly outperforms RandAugment</li></ul><p><img src="/2020/05/23/resnets/training methods.png" width="75%;"></p></li><li><p>hyper：</p><ul><li>droprate</li><li>increase the regularization as the model size increase to limit overfitting</li><li>label smoothing = 0.1</li><li>weight decay = 4e-5</li><li><p><img src="/2020/05/23/resnets/hyper.png" width="75%;"></p></li></ul></li></ul></li><li><p>improved training methods</p><ul><li><p>additive study</p><ul><li>总体上看都是additive的</li><li>increase training epochs在添加regularization methods的前提下才不hurt，否则会overfitting</li><li><p>dropout在不降低weight decay的情况下会hurt</p><p><img src="/2020/05/23/resnets/additive.png" width="45%;"></p></li></ul></li><li><p>weight decay</p><ul><li>少量/没有regularization methods的情况下：大weight decay防止过拟合，1e-4</li><li>多/强regularization methods的情况下：适当减小weight decay能涨点，4e-5</li></ul></li></ul></li><li><p>improved scaling strategies</p><ul><li><p>search space</p><ul><li>width multiplier：[0.25, 0.5, 1.0, 1.5, 2.0]</li><li>depth：[26, 50, 101, 200, 300, 350, 400]</li><li>resolution：[128, 160, 224, 320, 448]</li><li>increase regularization as model size increase</li><li>observe 10/100/350 epoch regime</li></ul></li><li><p>we found that the best scaling strategies depends on training regime</p></li><li><p>strategy1：scale depth</p><ul><li>Depth scaling outperforms width scaling for longer epoch regimes </li><li>width scaling is preferable for shorter epoch regimes</li><li>scaling width可能会引起overfitting，有时候会hurt performance</li><li>depth scaling引入的参数量也比width小</li></ul></li><li><p>strategy2：slow resolution scaling</p><ul><li>efficientNets/resNeSt lead to very large images</li><li><p>our experiments：大可不必</p><p><img src="/2020/05/23/resnets/resolution.png" width="45%;"></p></li></ul></li></ul></li></ul></li><li><p>实验</p></li></ol>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Receptive Field</title>
      <link href="/2020/05/18/Receptive-Field/"/>
      <url>/2020/05/18/Receptive-Field/</url>
      <content type="html"><![CDATA[<h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><ul><li><p>感受野</p><p>  除了卷积和池化，其他层并不影响感受野大小</p><p>  感受野与卷积核尺寸kernel_size和步长stride有关</p><p>  递归计算：</p><script type="math/tex; mode=display">  N\_RF = kernel\_size + (cur\_RF-1)*stride</script><p>  其中$cur_RF$是当前层（start from 1），$kernel_size$、$stride$是当前层参数，$N_RF$是上一层的感受野。</p></li></ul><ul><li><p>感受野计算器</p><p>  <a href="https://fomoro.com/research/article/receptive-field-calculator" target="_blank" rel="noopener">https://fomoro.com/research/article/receptive-field-calculator</a></p></li></ul><h2 id="Understanding-the-Effective-Receptive-Field-in-Deep-Convolutional-Neural-Networks"><a href="#Understanding-the-Effective-Receptive-Field-in-Deep-Convolutional-Neural-Networks" class="headerlink" title="Understanding the Effective Receptive Field in Deep Convolutional Neural Networks"></a>Understanding the Effective Receptive Field in Deep Convolutional Neural Networks</h2><ol><li><p>动机</p><ul><li><p>effective receptive field </p></li><li><p>the effect of nonlinear activations, dropout, sub-sampling and skip connections on it </p></li></ul></li><li><p>论点</p><ul><li>it is critical for each output pixel to have a big receptive field, such that no important information is left out when making the prediction</li><li>deeper network：increase the receptive field size linearly  </li><li>Sub-sampling：increases the receptive field size multiplicatively</li><li>it is easy to see that pixels at the center of a receptive field have a much larger impact on an output：前向传播的时候，中间位置的像素点有更多条path通向output</li></ul></li><li><p>方法看不懂直接看结论</p><ul><li><p><strong>dropout</strong> does not change the Gaussian ERF shape </p></li><li><p><strong>Subsampling</strong> and dilated convolutions turn out to be effective ways to increase receptive field size quickly </p></li><li><p><strong>Skip-connections</strong> make ERFs smaller </p></li><li><p>ERFs are Gaussian distributed</p><ul><li>uniformly和随机初始化都是perfect Gaus- sian shapes </li><li><p>加上非线性激活函数以后是near Gaussian shapes </p><p><img src="/2020/05/18/Receptive-Field/gaussian.png" width="50%;"></p></li><li><p>with different nonlinearities </p><p><img src="/2020/05/18/Receptive-Field/activation.png" width="25%;"></p></li></ul></li><li><p>$\sqrt n$ absolute growth and $1/\sqrt n$ relative shrinkage：RF是随着layer线性增长的，ERF在log上0.56的斜率，约等于$\sqrt n$ </p><p>  <img src="/2020/05/18/Receptive-Field/growth.png" width="50%;"></p></li><li><p>Subsampling &amp; dilated convolution increases receptive field</p><ul><li>The reference baseline is a convnet with 15 dense convolution layers </li><li>Subsampling：replace 3 of the 15 convolutional layers with stride-2 convolution </li><li><p>dilated：replace them with dilated convolution with factor 2,4 and 8，rectangular ERF shape </p><p><img src="/2020/05/18/Receptive-Field/subsamp.png" width="25%;"></p></li></ul></li><li><p>evolves during training </p><ul><li>as the networks learns, the ERF gets bigger, and at the end of training is significantly larger than the initial ERF</li><li>classification<ul><li>32*32 cifar 10</li><li>theoretical receptive field of our network is actually 74 × 74 </li></ul></li><li>segmentation<ul><li>CamVid dataset  </li><li>the theoretical receptive field of the top convolutional layer units is quite big at 505 × 505</li></ul></li><li><p>实际的ERF都很小，都没到原图大小</p><p><img src="/2020/05/18/Receptive-Field/evolve.png" width="50%;"></p></li></ul></li><li><p>increase the effective receptive field  </p><ul><li>New Initialization：<ul><li>makes the weights at the center of the convolution kernel to have a smaller scale, and the weights on the outside to be larger</li><li>30% speed-up of training </li><li>其他效果不明显</li></ul></li><li>Architecturalchanges<ul><li>sparsely connect each unit to a larger area </li><li>dilated convolution or even not grid-like </li></ul></li></ul></li></ul></li></ol><p>g</p>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SqueezeNet</title>
      <link href="/2020/05/18/SqueezeNet/"/>
      <url>/2020/05/18/SqueezeNet/</url>
      <content type="html"><![CDATA[<h2 id="SQUEEZENET-ALEXNET-LEVEL-ACCURACY-WITH-50X-FEWER-PARAMETERS-AND-lt-0-5MB-MODEL-SIZE"><a href="#SQUEEZENET-ALEXNET-LEVEL-ACCURACY-WITH-50X-FEWER-PARAMETERS-AND-lt-0-5MB-MODEL-SIZE" class="headerlink" title="SQUEEZENET: ALEXNET-LEVEL ACCURACY WITH 50X FEWER PARAMETERS AND &lt;0.5MB MODEL SIZE"></a>SQUEEZENET: ALEXNET-LEVEL ACCURACY WITH 50X FEWER PARAMETERS AND &lt;0.5MB MODEL SIZE</h2><ol><li><p>动机</p><ul><li>Smaller CNN</li><li>achieve AlexNet-level accuracy</li><li>model compression </li></ul></li><li><p>论点</p><ul><li>model compression<ul><li>SVD</li><li>sparse matrix </li><li>quantization (to 8 bits or less)  </li></ul></li><li>CNN microarchitecture<ul><li>extensively 3x3 filters</li><li>1x1 filters  </li><li>higher level building blocks</li><li>bypass connections </li><li>automated designing approaches</li></ul></li><li>this paper eschew automated approaches  </li><li>propose and evaluate the SqueezeNet architecture with and without model compression</li><li>explore the impact of design choices </li></ul></li><li><p>方法</p><ul><li><p>architectural design strategy</p><ul><li>Replace 3x3 filters with 1x1 filters</li><li>Decrease the number of input channels to 3x3 filters （squeeze）</li><li>Downsample late in the network so that convolution layers have large activation maps：large activation maps (due to delayed downsampling) can lead to higher classification accuracy </li></ul></li><li><p>the fire module</p><ul><li>squeeze：1x1 convs</li><li>expand：mix of 1x1 and 3x3 convs, same padding</li><li>relu</li><li><p>concatenate</p><p><img src="/2020/05/18/SqueezeNet/fire.png" width="50%"></p></li></ul></li><li><p>the SqueezeNet</p><ul><li>a standalone convolution layer (conv1)</li><li>followed by 8 Fire modules (fire2-9)</li><li>ending with a final conv layer (conv10)</li><li>stride2 max-pooling after layers conv1, fire4, fire8, and conv10</li><li>dropout with a ratio of 50% is applied after the fire9 module</li><li><p>GAP</p><p><img src="/2020/05/18/SqueezeNet/squeezenet.png" width="60%"></p></li></ul><p><img src="/2020/05/18/SqueezeNet/dimension.png" width="60%"></p></li></ul></li><li><p>understand the impact</p><ul><li><p>each Fire module has three dimensional hyperparameters, to simplify：</p><ul><li>define $base_e$：the number of <em>expand</em> filters in the first Fire module </li><li>for layer i：$e_i=base_e + (incr_e*[\frac{i}{freq}])$</li><li>expand ratio $pct_{3x3}$：the percentage of 3x3 filters in expand layers</li><li>squeeze ratio $SR$：the number of filters in the squeeze layer／the number of filters in the expnad layer</li><li>normal setting：$base_e=128, incre_e=128, pct_{3x3}=0.5, freq=2, SR=0.125$</li></ul></li><li><p>SR</p><ul><li>increasing SR leads to higher accuracy and larger model size</li><li>Accuracy plateaus at 86.0% with SR=0.75 </li><li>further increasing provides no improvement  </li></ul></li><li><p>pct</p><ul><li>increasing pct leads to higher accuracy and larger model size</li><li>Accuracy plateaus at 85.6% with pct=50%</li><li>further increasing provides no improvement  </li></ul><p><img src="/2020/05/18/SqueezeNet/SR.png" width="60%"></p></li><li><p>bypass</p><ul><li>Vanilla </li><li>simple bypass：when in &amp; out channels have the same dimensions</li><li>complex bypass：includes a 1x1 convolution layer </li><li><strong>alleviate the representational bottleneck introduced by squeeze layers</strong></li><li>both yielded accuracy improvements</li><li>simple bypass enabled higher accuracy </li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hausdorff Distance</title>
      <link href="/2020/05/14/Hausdorff-Distance/"/>
      <url>/2020/05/14/Hausdorff-Distance/</url>
      <content type="html"><![CDATA[<h2 id="Reducing-the-Hausdorff-Distance-in-Medical-Image-Segmentation-with-Convolutional-Neural-Networks"><a href="#Reducing-the-Hausdorff-Distance-in-Medical-Image-Segmentation-with-Convolutional-Neural-Networks" class="headerlink" title="Reducing the Hausdorff Distance in Medical Image Segmentation with Convolutional Neural Networks"></a>Reducing the Hausdorff Distance in Medical Image Segmentation with Convolutional Neural Networks</h2><ol><li><p>动机</p><ul><li>novel loss function to reduce HD directly </li><li>propose three methods </li><li>2D&amp;3D，ultra &amp; MR &amp; CT</li><li>lead to approximately 18 − 45% reduction in HD without degrading other segmentation performance criteria </li></ul></li><li><p>论点</p><ul><li>HD is one of the most informative and useful criteria because it is an indicator of the largest segmentation error </li><li>current segmentation algorithms rarely aim at minimizing or reducing HD directly <ul><li>HD is determined <strong>solely</strong> by the largest error instead of the overall segmentation performance </li><li>HD‘s sensitivity to noise and outliers  —&gt; modified version</li><li>the optimization diffculty</li></ul></li><li>thus we propose an “HD- inspired” loss function </li></ul></li><li><p>方法</p><ul><li><p>denotations</p><ul><li>probability：$q$</li><li>binary mask：$\bar p$、$\bar q$</li><li>boundary：$\delta p$、$\delta q$</li><li><p>single hd：$hd(\bar p, \bar q)$、$hd(\bar q, \bar p)$</p><p><img src="/2020/05/14/Hausdorff-Distance/start.png" width="40%"></p></li></ul></li><li><p>based on distance transforms </p><ul><li><p>distance map $d_p$：define the distance map as the unsigned distance to the boundary $\delta p$</p><script type="math/tex; mode=display">  DT_X[i,j] = min_{[k,l]\in X}d([i,j], [k,l])</script><p>  距离场定义为：每个点到目标区域(X)的距离的最小值</p><p>  <img src="/2020/05/14/Hausdorff-Distance/dt.png" width="40%"></p></li><li><p>HD based on DT：</p><script type="math/tex; mode=display">  hd_{DT}(\delta p, \delta q) = max((\bar p \triangle \bar q)\circ d_p)\\  \bar p \triangle \bar q = |\bar p - \bar q|</script><ul><li>finally have：<script type="math/tex; mode=display">  HD_{DT}(\delta p, \delta q) = max(hd_{DT}(\delta p, \delta q), hd_{DT}(\delta q, \delta p))</script></li></ul></li><li><p>modified loss version of HD：</p><script type="math/tex; mode=display">  Loss_{DT}(q,p) = \frac{1}{|\Omega|}\sum_{\Omega}((p-q)^2\circ(d_p^{\alpha}+d_q^{\alpha}))</script><ul><li>penalizely focus on areas instead of single point</li><li>$\alpha$ determines how strongly we penalize larger errors</li><li>use possibility instead of thresholded value</li><li>use $(p-q)^2$ instead of $|p-q|$</li></ul></li><li><p>correlations</p><ul><li>$HD_{DT}$：Pearson correlation coefficient above 0.99</li><li><p>$Loss_{DT}$：Pearson correlation coefficient above 0.93</p><p><img src="/2020/05/14/Hausdorff-Distance/correlation.png" width="50%"></p></li></ul></li><li><p>drawback </p><ul><li><p><strong>high computational</strong> cost especially in 3D </p></li><li><p>$q$ changes along with training process thus $d_q$ changes while $d_p$ remains</p></li><li><p>modified one-sided HD (OS)：</p><script type="math/tex; mode=display">  Loss_{DT-OS}(q,p) = \frac{1}{|\Omega|}\sum_{\Omega}((p-q)^2\circ(d_p^{\alpha}))</script></li></ul></li></ul></li><li><p>HD using Morphological Operations</p><ul><li><p>morphological erosion：</p><script type="math/tex; mode=display">  S \ominus B = \{z\in \Omega | B(z) \subseteq S\}</script><p>  腐蚀操作定义为：在原始二值化图的前景区域，以每个像素为中心点，run structure element block B，如果B完全在原图内，则当前中心点在腐蚀后也是前景。</p></li><li><p>HD based on erosion：</p><script type="math/tex; mode=display">  HD_{ER}(\delta p, \delta q)=2r^*\\  where\ r^* = min_r \{(\bar p \triangle \bar q) \ominus B_r = \varnothing\}</script><ul><li>$HD_{ER}$ is a lower bound of the true value</li><li>can be computed more efficiently using convolutional operations </li></ul></li><li><p>modifid loss version：</p><script type="math/tex; mode=display">  Loss_{ER}(q,p) = \frac{1}{|\Omega|}\sum_k \sum_{\Omega}((p-q)^2 \ominus_k B)k^{\alpha}</script><ul><li>k successive erosions </li><li><strong>cross-shaped kernel</strong> whose elements sum to one followed by a <strong>soft thresholding</strong> at 0.50</li></ul></li><li><p>correlations</p><ul><li>$HD_{ER}$：Pearson correlation coefficient above 0.91</li><li>$Loss_{ER}$：Pearson correlation coefficient above 0.83</li></ul></li></ul></li><li><p>HD using circular-shaped convolutional kernel </p><ul><li><p>circular-shaped kernel</p><p>   <img src="/2020/05/14/Hausdorff-Distance/circle.png" width="40%"></p></li><li><p>HD based on circular-shaped kernel：</p><script type="math/tex; mode=display">  hd_{CV}(\delta p, \delta q)=max(r_1, r_2)\\  where \ r_1=max_r (max_{\Omega}f_h(\bar p ^C * B_r)\circ(\bar q \backslash \bar p))\\  where \ r_2=max_r (max_{\Omega}f_h(\bar p * B_r)\circ(\bar p \backslash \bar q))\\</script><ul><li>$\bar p^C$：complement 补集</li><li>$f_h$：hard thresholding setting all values below 1 to zero </li></ul></li><li><p>modified loss version：</p><script type="math/tex; mode=display">  Loss_{CV}(q,p)=\frac{1}{|\Omega|}\sum_{r\in R}r^{\alpha}\sum_{\Omega}[f_s(Br*\bar p^C)\circ f_{\bar q\backslash \bar p} + f_s(B_r * \bar p) \circ f_{\bar p \backslash \bar q}\\  +f_s(Br*\bar q^C)\circ f_{\bar p\backslash \bar q} + f_s(B_r * \bar q) \circ f_{\bar q \backslash \bar p}]</script><ul><li>soft thresholding</li><li><script type="math/tex; mode=display">f_{\bar p\backslash \bar q} = (p-q)^2*p</script></li></ul></li><li><p>correlations</p><ul><li>$HD_{CV}$：Pearson correlation coefficient above 0.99</li><li>$Loss_{CV}$：Pearson correlation coefficient above 0.88</li></ul></li><li><p>computation：</p><ul><li>kernel size<ul><li>$HD_{ER}$ is computed using small fixed convolutional kernels (of size 3)</li><li>$Loss_{CV}$ require applying filters of increasing size(we use a maximum kernel radius of 18 pixels in 2D and 9 voxels in 3D)</li></ul></li><li>steps<ul><li>choose R based on the expected range of segmentation errors </li><li>set R = {3, 6, . . . 18} for 2D images and R = {3,6,9} for 3D </li></ul></li></ul></li></ul></li><li><p>training</p><ul><li>standard Unet</li><li>augment our HD-based loss term with a DSC loss term for more stable training</li><li>reweight both loss after every epoch</li></ul></li></ul></li></ol><p>d</p>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SE block</title>
      <link href="/2020/04/30/SE-block/"/>
      <url>/2020/04/30/SE-block/</url>
      <content type="html"><![CDATA[<h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p>图像特征的提取能力是CNN的核心能力，而SE block可以起到为CNN校准采样的作用。</p><p>根据感受野理论，特征矩阵主要来自于样本的中央区域，处在边缘位置的酒瓶的图像特征很大概率会被pooling层抛弃掉。而SE block的加入就可以通过来调整特征矩阵，增强酒瓶特征的比重，提高它的识别概率。</p><p><img src="/2020/04/30/SE-block/bottle.png" width="25%"></p><ol><li>[SE-Net] Squeeze-and-Excitation Networks</li><li>[SC-SE] Concurrent Spatial and Channel ‘Squeeze &amp; Excitation’ in Fully Convolutional Networks </li><li>[CMPE-SE] Competitive Inner-Imaging Squeeze and Excitation for Residual Network</li></ol><h2 id="SENet-Squeeze-and-Excitation-Networks"><a href="#SENet-Squeeze-and-Excitation-Networks" class="headerlink" title="SENet: Squeeze-and-Excitation Networks"></a>SENet: Squeeze-and-Excitation Networks</h2><ol><li><p>动机</p><ul><li>prior research has investigated the spatial component to achieve more powerful representations </li><li>we focus on the channel relationship instead</li><li>SE-block：adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels</li><li>enhancing the representational power </li><li>in a computationally efficient manner</li></ul></li><li><p>论点</p><ul><li>stronger network：<ul><li>deeper</li><li>NiN-like bocks</li></ul></li><li>cross-channel correlations in prior work<ul><li>mapped as new combinations of features through 1x1 conv</li><li>concentrated on the objective of reducing model and computational complexity</li></ul></li><li>In contrast, we found this mechanism <ul><li>can ease the learning process</li><li>and significantly enhance the representational power of the network</li></ul></li><li>Attention<ul><li>Attention can be interpreted as a means of biasing the allocation of available computational resources towards the most informative components<ul><li>Some works provide interesting studies into the combined use of spatial and channel attention </li></ul></li></ul></li></ul></li><li><p>方法</p><ul><li><p>SE-block</p><ul><li><p>The channel relationships modelled by convolution are inherently implicit and local </p></li><li><p>we would like to provide it with access to global information</p></li><li><p>squeeze：using global average pooling </p></li><li><p>excitation：nonlinear &amp; non-mutually-exclusive using sigmoid</p><ul><li><p>bottleneck：a dimensionality-reduction layer $W_1$ with reduction ratio $r$ and ReLU and a dimensionality-increasing layer $W_2$</p></li><li><p>$s = F_{ex}(z,W) = \sigma (W_2 \delta(W_1 z))$</p><p><img src="/2020/04/30/SE-block/seblock.png" width="70%"></p></li></ul></li><li><p>integration</p><ul><li>insert after the non-linearity following each convolution</li><li>inception：take the transformation $F_{tr}$ to be an entire Inception module </li><li><p>residual：take the transformation $F_{tr}$ to be the non-identity branch of a residual module</p><p><img src="/2020/04/30/SE-block/integrate.png" width="70%"></p></li></ul></li></ul></li><li><p>model and computational complexity</p><ul><li>ResNet50 vs. SE-ResNet50：0.26% relative increase GFLOPs approaching ResNet10’s accuracy </li><li>the additional parameters result solely from the two FC layers, among which  the final stage FC claims the majority due to being performed across the greatest number of channels</li><li>the costly final stage of SE blocks could be removed at only a small cost in performance </li></ul></li><li><p>ablations</p><ul><li>FC<ul><li>removing the biases of the FC layers  in the excitation facilitates the modelling of channel dependencies </li></ul></li><li>reduction ratio<ul><li>performance is robust to a range of reduction ratios </li><li>In practice, using an identical ratio throughout a network may not be optimal due to the distinct roles performed by different layers </li></ul></li><li>squeeze<ul><li>global average pooling vs. global max pooling：average pooling slightly better </li></ul></li><li>excitation<ul><li>Sigmoid vs. ReLU vs. tanh：<ul><li>tanh：slightly worse</li><li>ReLU：dramatically worse   </li></ul></li></ul></li><li>stages<ul><li>each stages brings benefits </li><li>combination make even better</li></ul></li><li>integration strategy<ul><li>fairly robust to their location, provided that they are applied prior to branch aggregation</li><li>inside the residual unit：fewer channels, fewer parameters, comparable accuracy </li></ul></li></ul></li><li>primitive understanding <ul><li>squeeze<ul><li>the use of <strong>global information</strong> has a significant influence on the model performance</li></ul></li><li>excitation<ul><li>the distribution across different classes is very similar at the earlier layers  (general features)</li><li>the value of each channel becomes much more class-specific at greater depth</li><li>SE_5_2 exhibits an interesting tendency towards a saturated state in which most of the activations are close to one</li><li>SE_5_3 exhibits a similar pattern emerges over different classes, up to a modest change in scale</li><li>suggesting that SE_5_2 and SE_5_3 are less important than previous blocks in providing recalibration to the network (thus can be removed)</li></ul></li></ul></li></ul></li><li><p>APPENDIX</p><ul><li>在ImageNet上SOTA的模型是SENet-154，top1-err是18.68，被标记在了efficientNet论文的折线图上<ul><li>SE-ResNeXt-152（64x4d）<ul><li>input=(224,224)：top1-err是18.68</li><li>input=320/299：top1-err是17.28</li></ul></li><li>further difference<ul><li>each bottleneck building block的第一个1x1 convs的通道数减半</li><li>stem的第一个7x7conv换成了3个连续的3x3 conv</li><li>1x1的s2 conv换成了3x3的s2 conv</li><li>fc之前添加dropout layer</li><li>label smoothing</li><li>最后几个training epoch将BN层的参数冻住，保证训练和测试的参数一致</li><li>64 GPUs，batch size=2048（32 per GPU）</li><li>initial lr=1.0</li></ul></li></ul></li></ul></li></ol><h2 id="SC-SE-Concurrent-Spatial-and-Channel-‘Squeeze-amp-Excitation’-in-Fully-Convolutional-Networks"><a href="#SC-SE-Concurrent-Spatial-and-Channel-‘Squeeze-amp-Excitation’-in-Fully-Convolutional-Networks" class="headerlink" title="SC-SE: Concurrent Spatial and Channel ‘Squeeze &amp; Excitation’ in Fully Convolutional Networks"></a>SC-SE: Concurrent Spatial and Channel ‘Squeeze &amp; Excitation’ in Fully Convolutional Networks</h2><ol><li><p>动机</p><ul><li>image segmentation task 上面SE-Net提出来主要是针对分类</li><li>three variants of SE modules <ul><li>squeezing spatially and exciting channel-wise (cSE)</li><li>squeezing channel-wise and exciting spatially (sSE)</li><li>concurrent spatial and channel squeeze &amp; excitation (scSE) </li></ul></li><li>integrate within three different state-of-the- art F-CNNs (DenseNet, SD-Net, U-Net) </li></ul></li><li><p>论点</p><ul><li>F-CNNs have become the tool of choice for many image segmentation tasks </li><li>core：convolutions that capturing <strong>local spatial pattern along all input channels</strong> jointly </li><li>SE block <strong>factors out the spatial dependency</strong> by global average pooling to learn a channel specific descriptor (later refered to as cSE /channel-SE)</li><li>while for image segmentation, we hypothesize that the pixel-wise spatial information is more informative </li><li>thus we propose sSE(spatial SE) and scSE(spatial and channel SE)</li><li><p>can be seamlessly integrated by placing after every encoder and decoder block</p><p><img src="/2020/04/30/SE-block/scse.png" width="60%"></p></li></ul></li><li><p>方法</p><ul><li>cSE<ul><li>GAP：embeds the global spatial information into a vector </li><li>FC-ReLU-FC-Sigmoid：adaptively learns the importance  </li><li>recalibrate</li></ul></li><li>sSE<ul><li>1x1 conv：generating a projection tensor representing the linearly combined representation for all channels C for a spatial location (i,j)</li><li>Sigmoid：rescale </li><li>recalibrate </li></ul></li><li>scSE<ul><li>by element-wise addition </li><li>encourages the network to learn more meaningful feature maps ———- relevant both spatially and channel-wise</li></ul></li></ul></li><li><p>实验</p><ul><li><p>F-CNN architectures：</p><ul><li>4 encoder blocks, one bottleneck layer, 4 decoder blocks and a classification layer </li><li>class imbalance：median frequency balancing ce</li></ul></li><li><p>dice cmp：scSE &gt; sSE &gt; cSE &gt; vanilla</p></li><li><p>小区域类别的分割，观察到使用cSE可能会差于vanilla： might have got overlooked by only exciting the channels</p></li><li><p>定性分析：</p><ul><li>一些under segmented的地方，scSE improves with the inclusion </li><li><p>一些over segmented的地方，scSE rectified the result</p><p><img src="/2020/04/30/SE-block/qualitive.png" width="55%"></p></li></ul></li></ul></li></ol><h2 id="Competitive-Inner-Imaging-Squeeze-and-Excitation-for-Residual-Network"><a href="#Competitive-Inner-Imaging-Squeeze-and-Excitation-for-Residual-Network" class="headerlink" title="Competitive Inner-Imaging Squeeze and Excitation for Residual Network"></a>Competitive Inner-Imaging Squeeze and Excitation for Residual Network</h2><ol><li>动机<ul><li>for residual network</li><li>the residual architecture has been proved to be diverse and redundant</li><li>model the competition between residual and identity mappings</li><li>make the identity flow to control the complement of the residual feature maps</li></ul></li><li><p>论点</p><ul><li>For analysis of ResNet, with the increase in depth, the residual network exhibits a certain amount of redundancy </li><li><p>with the CMPE-SE mechanism, it makes residual mappings tend to provide more efficient supplementary for identity mappings</p><p><img src="/2020/04/30/SE-block/cmpese.png" width="55%"></p></li></ul></li><li><p>方法</p><ul><li><p>主要提出了三种变体：</p><p><img src="/2020/04/30/SE-block/cmpeses.png" width="75%"></p></li><li><p>第一个变体：</p><ul><li>两个分支id和res分别GAP出一个vector，然后fc reduct by ratio，然后concat，然后channel back</li><li>Implicitly, we can believe that the winning of the identity channels in this competition results in less weights of the residual channels </li></ul></li><li><p>第二个变体：</p><ul><li><p>两种方案</p><ul><li>2x1 convs：对上下相应位置的元素求avg</li><li>1x1 convs：对全部元素求avg，然后flatten</li></ul><p><img src="/2020/04/30/SE-block/sec.png" width="45%"></p></li></ul></li><li><p>第三个变体：</p><ul><li><p>两边的channel-wise vector叠起来，然后reshape成矩阵形式，然后3x3 conv，然后flatten</p><p><img src="/2020/04/30/SE-block/trd.png" width="45%"></p></li></ul></li></ul></li></ol><p>比较扯，不浪费时间分析了。</p>]]></content>
      
      
        <tags>
            
            <tag> papers </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>cv2&amp;numpy&amp;tobeadded</title>
      <link href="/2020/04/19/cv2-numpy-tobeadded/"/>
      <url>/2020/04/19/cv2-numpy-tobeadded/</url>
      <content type="html"><![CDATA[<ol><li>矩阵乘法<ul><li>np.dot(A,B)：真正的矩阵乘法</li><li>np.multiply(A,B) &amp; np重载的*：element-wise product，矩阵中对应元素相乘</li><li>cv的A.dot(B) &amp; cv重载的*：真正的矩阵乘法</li><li>cv的A.mul(B) ：element-wise product，矩阵中对应元素相乘</li></ul></li></ol><ol><li><p>图像旋转</p><p> 通过仿射矩阵<code>cv2.getRotationMatrix2D</code>和仿射变换函数<code>cv2.warpAffine</code>来实现</p><ul><li><p>src：输入图像</p></li><li><p>M：变换矩阵</p></li><li><p>dsize：输出图像的大小（基于图像<strong>原点</strong>裁剪）</p></li><li><p>flags：插值方法</p></li><li><p>borderMode：边界像素模式</p></li><li><p>borderValue：边界填充值，默认为0</p><p>cv2.getRotationMatrix2D(center, angle, scale)：返回一个2x3的变换矩阵</p></li><li><p>center：旋转中心</p></li><li>angle：旋转角度，<strong>正值是逆时针旋转</strong></li><li><p>scale：缩放因子</p><p>cv2.warpAffine(src, M, dsize[, dst[, flags[, borderMode[, borderValue]]]]))：返回变换后的图像</p></li><li><p>src：输入图像</p></li><li><p>M：变换矩阵</p></li><li><p>dsize：输出图像的大小（基于图像<strong>原点</strong>裁剪）</p></li><li><p>flags：插值方法</p></li><li><p>borderMode：边界像素模式</p></li><li><p>borderValue：边界填充值，默认为0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rotate_img</span><span class="params">(angle, img, interpolation=cv2.INTER_LINEAR, points=[])</span>:</span></span><br><span class="line">    h, w = img.shape</span><br><span class="line">    rotataMat = cv2.getRotationMatrix2D((w/<span class="number">2</span>, h/<span class="number">2</span>), math.degrees(angle), <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># rotate_img1: 输出图像尺寸不变，超出原图像部分被cut掉</span></span><br><span class="line">    rotate_img1 = cv2.warpAffine(img, rotataMat, dsize=(w, h), flags=interpolation, borderMode=cv2.BORDER_CONSTANT, borderValue=<span class="number">0</span>)</span><br><span class="line">   <span class="comment"># rotate_img2: 输出图像尺寸变大，保留超出原图像部分，新的坐标原点保证旋转中心仍旧位于图像中心</span></span><br><span class="line">    new_h = int(w*math.fabs(math.sin(angle)) + h*math.fabs(math.cos(angle)))</span><br><span class="line">    new_w = int(h*math.fabs(math.sin(angle)) + w*math.fabs(math.cos(angle)))</span><br><span class="line">    rotataMat[<span class="number">0</span>, <span class="number">2</span>] += (new_w - w) / <span class="number">2</span></span><br><span class="line">    rotataMat[<span class="number">1</span>, <span class="number">2</span>] += (new_h - h) / <span class="number">2</span></span><br><span class="line">    rotate_img2 = cv2.warpAffine(img, rotataMat, dsize=(new_w, new_h), flags=interpolation, borderMode=cv2.BORDER_CONSTANT, borderValue=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 坐标点的变换</span></span><br><span class="line">    rotated_points = []</span><br><span class="line">    <span class="keyword">for</span> point <span class="keyword">in</span> points:</span><br><span class="line">        point = rotataMat.dot([[point[<span class="number">0</span>]], [point[<span class="number">1</span>]], [<span class="number">1</span>]])</span><br><span class="line">        rotated_points.append((int(point[<span class="number">0</span>]), int(point[<span class="number">1</span>])))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> rotate_img2, rotated_points</span><br></pre></td></tr></table></figure><p>使用tips：</p></li><li><p>如果不修改仿射变换矩阵的平移参数，坐标原点的位置不发生改变</p></li><li><p>dsize指定的输出图像是从原点位置开始裁剪</p></li><li><p>坐标点的变换满足公式：</p><script type="math/tex; mode=display">  dst(x,y) = src(M_{11}x+M_{12}y+M_{13}, M_{21}x+M_{22}y+M_{23})</script></li></ul></li></ol><ol><li><p>np.meshgrid(*xi,**kwargs)</p><p> 这个函数神他妈坑，作用是Return coordinate matrices from coordinate vectors. Make N-D coordinate arrays for vectorized evaluations of N-D scalar/vector fields over N-D grids, given one-dimensional coordinate arrays x1, x2,…, xn. 但是尝试一下会发现：</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">0</span>,<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">y = np.arange(<span class="number">0</span>,<span class="number">20</span>,<span class="number">1</span>)</span><br><span class="line">z = np.arange(<span class="number">0</span>,<span class="number">30</span>,<span class="number">1</span>)</span><br><span class="line">x, y, z= np.meshgrid(x, y, z)</span><br><span class="line">print(x.shape)       <span class="comment"># (20, 10, 30)</span></span><br></pre></td></tr></table></figure><p> xy轴坐标是反过来的，这是因为optional args里面有一个indexing：</p><p> <strong>indexing</strong> : {‘xy’, ‘ij’}, Cartesian (‘xy’, default) or matrix (‘ij’) indexing of output.</p><p> 我们想要得到的坐标系和输入的轴一一对应，得指定参数<code>indexing=&#39;ij&#39;</code></p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">0</span>,<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">y = np.arange(<span class="number">0</span>,<span class="number">20</span>,<span class="number">1</span>)</span><br><span class="line">z = np.arange(<span class="number">0</span>,<span class="number">30</span>,<span class="number">1</span>)</span><br><span class="line">x, y, z= np.meshgrid(x, y, z, indexing=<span class="string">'ij'</span>)</span><br><span class="line">print(x.shape)      <span class="comment"># (10, 20, 30)</span></span><br></pre></td></tr></table></figure><p> 还有一个参数sparse，因为每根轴的坐标都是复制的，所以可以稀疏存储，此时函数返回值变化：</p><p> <strong>sparse</strong> : bool, If True a sparse grid is returned in order to conserve memory. Default is False.</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">0</span>,<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">y = np.arange(<span class="number">0</span>,<span class="number">20</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">xx, yy = np.meshgrid(x, y)</span><br><span class="line">print(xx)        <span class="comment"># a 20x10 list</span></span><br><span class="line"></span><br><span class="line">xx, yy = np.meshgrid(x, y, sparse=<span class="keyword">True</span>)</span><br><span class="line">print(xx)        <span class="comment"># a 1*10 list</span></span><br><span class="line">print(yy)        <span class="comment"># a 20*1 list</span></span><br><span class="line"><span class="comment"># 所以整体上还是个20*10的矩阵</span></span><br></pre></td></tr></table></figure><p> 二维可视化：</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">z = xx**<span class="number">2</span> + yy**<span class="number">2</span>            <span class="comment"># xx和yy既可以是dense convervation也可以是sparse convervation</span></span><br><span class="line">h = plt.contourf(x,y,z)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>np.tile(A,reps)</p><p> 这个函数挺有用的，把数组沿着指定维度复制，比stack、concat啥的都优雅，能自动创建新的维度</p><ul><li>A：array_like, The input array.</li><li>reps：array_like, The number of repetitions of <em>A</em> along each axis.</li></ul></li></ol><ol><li><p>np.reshape(a, newshape, order=’C’)</p><p> 这个函数贼常用，但是一般用于二维的时候没考虑重组顺序这件事</p><ul><li><p>order: {‘C’, ‘F’, ‘A’}, optional，简单理解，reshape的通用实现方式是先将真个array拉直，然后依次取数据填入指定维度，C是从最里面的维度开始拉直&amp;构造，F是从最外面的维度开始拉直&amp;构造，A for auto</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">6</span>)</span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># C-like index ordering</span></span><br><span class="line">np.reshape(a, (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fortran-like index ordering</span></span><br><span class="line">np.reshape(a, (<span class="number">2</span>, <span class="number">3</span>), order=<span class="string">'F'</span>)</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">4</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">1</span>, <span class="number">5</span>]])</span><br></pre></td></tr></table></figure></li><li><p>tf和keras里面也有reshape，是没有order参数的，默认是’C’</p></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>MobileNets</title>
      <link href="/2020/04/16/MobileNets/"/>
      <url>/2020/04/16/MobileNets/</url>
      <content type="html"><![CDATA[<h2 id="preview"><a href="#preview" class="headerlink" title="preview"></a>preview</h2><ol><li><p>动机</p><ul><li>计算力有限</li><li>模型压缩／使用小模型</li></ul></li><li><p>深度可分离卷积 Depthwise Separable Convolution</p><ul><li>将标准卷积拆分为两个操作：深度卷积(depthwise convolution) 和逐点卷积(pointwise convolution)</li><li>标准卷积：参数量k*k*input_channel*output_channel</li><li>深度卷积(depthwise convolution) ：针对每个输入通道采用不同的卷积核，参数量k*k*input_channel</li><li>逐点卷积(pointwise convolution)：就是普通的卷积，只不过其采用1x1的卷积核，参数量1*1*input_channel*output_channel</li><li><p>with BN and ReLU：</p><p><img src="/2020/04/16/MobileNets/deepwise block.png" width="40%"></p></li><li><p>DW没有改变通道数的能力，如果输入层的通道数很少，DW也只能在低维空间提特征，因此V2提出先对原始输入做expansion，用一个非线性PW升维，然后DW，然后再使用一个PW降维，值得注意的是，第二个PW不使用非线性激活函数，因为作者认为，relu作用在低维空间上会导致信息损失。</p></li></ul></li><li><p>进一步缩减计算量</p><ul><li>通道数缩减：宽度因子 alpha</li><li>分辨率缩减：分辨率因子rho</li></ul></li><li><p>papers</p><ul><li><p>[V1] MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications，主要贡献Depthwise Separable Convolution</p></li><li><p>[V2] MobileNetV2: Inverted Residuals and Linear Bottlenecks，主要贡献inverted residual with linear bottleneck</p></li><li>[V3] Searching for MobileNetV3，模型结构升级(inverted-res-block + SE-block)，通过NAS，而非手动设计</li></ul></li></ol><h2 id="MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications"><a href="#MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications" class="headerlink" title="MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"></a>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</h2><ol><li>动机<ul><li>efficient models：uses depthwise separable convolutions and two simple global hyper-parameters</li><li>resource and accuracy tradeoffs</li><li>a class of network architectures that allows a model developer to specifically choose a small network that matches the resource restrictions (latency, size) for their application </li></ul></li><li><p>论点：</p><ul><li>the general trend has been to make deeper and more complicated networks in order to achieve higher accuracy </li><li>not efficient on computationally limited platform</li><li>building small and efficient neural networks：either compressing pretrained networks or training small networks directly </li><li>Many papers on small networks focus only on size but do not consider speed</li><li>speed &amp; size 不完全对等</li><li>size：depthwise separable convolutions, bottleneck approaches, compressing pretrained networks, distillation </li></ul></li><li><p>方法</p><ul><li><p>depthwise separable convolutions </p><ul><li>a form of factorized convolutions：a standard conv splits into 2 layers</li><li>factorize the filtering and combination steps of standard conv</li><li>drastically reducing computation and model size to $\frac{1}{N} + \frac{1}{D_k^2}$</li><li>use both batchnorm and ReLU nonlinearities for both layers</li><li><p>MobileNet uses 3 × 3 depthwise separable convolutions which bring between 8 to 9 times less computation  </p><p><img src="/2020/04/16/MobileNets/deepwise conv.png" width="30%"></p></li></ul></li><li><p>MobileNet</p><ul><li>the first layer is a full convolution, the rest depthwise separable convolutions  </li><li>down sampling is handled with strided convolution</li><li>all layers are followed by a BN and ReLU nonlinearity</li><li>a final average pooling reduces the spatial resolution to 1 before the fully connected layer. </li><li><p>the final fully connected layer has no nonlinearity and feeds into a softmax layer for classification</p><p><img src="/2020/04/16/MobileNets/mobileNet structure.png" width="45%"></p></li></ul></li><li><p>training so few parameters</p><ul><li>RMSprop </li><li>less regularization and data augmentation techniques because small models have less trouble with overfitting</li><li>it was important to put very little or no weight decay (l2 regularization) </li><li>do not use side heads or label smoothing or image distortions  </li></ul></li><li><p>Width Multiplier: Thinner Models </p><ul><li>thin a network uniformly at each layer </li><li>the input channels $M$ and output channels $N$ becomes $\alpha M$ and $\alpha N$</li><li>$\alpha=1$：baseline MobileNet       $\alpha&lt;1$：reduced MobileNet</li><li>reduce the parameters roughly by $\alpha^2$</li></ul></li><li><p>Resolution Multiplier: Reduced Representation</p><ul><li>apply this to the input image</li><li>the input resolution of the network is typically 224, 192, 160 or 128</li><li>$\rho=1$：baseline MobileNet       $\rho&lt;1$：reduced MobileNet</li><li><p>reduce the parameters roughly by $\rho^2$</p><p><img src="/2020/04/16/MobileNets/param reduce.png" width="45%"></p></li></ul></li></ul></li><li><p>结论</p><ul><li>using depthwise separable convolutions compared to full convolutions only reduces accuracy by 1% on ImageNet but saving tremendously on mult-adds and parameters</li></ul><p><img src="/2020/04/16/MobileNets/cmp1.png" width="40%"></p><ul><li>at similar computation and number of parameters, thinner MobileNets is 3% better than making them shallower</li></ul><p><img src="/2020/04/16/MobileNets/cmp2.png" width="40%"></p><ul><li>trade-offs based on the two hyper-parameters</li></ul><p><img src="/2020/04/16/MobileNets/cmp3.png" width="40%"></p></li></ol><h2 id="MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks"><a href="#MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks" class="headerlink" title="MobileNetV2: Inverted Residuals and Linear Bottlenecks"></a>MobileNetV2: Inverted Residuals and Linear Bottlenecks</h2><ol><li><p>动机</p><ul><li>a new mobile architecture <ul><li>based on an inverted residual structure</li><li>remove non-linearities in the narrow layers in order to maintain representational power</li></ul></li><li>prove on multiple tasks<ul><li>object detection：SSDLite</li><li>semantic segmentation：Mobile DeepLabv3</li></ul></li></ul></li><li><p>方法</p><ul><li><p>Depthwise Separable Convolutions </p><ul><li>replace a full convolutional opera- tor with a factorized version </li><li>depthwise convolution, it performs lightweight filtering per input channel</li><li>pointwise convolution, computing linear combinations of the input channels</li></ul></li><li><p>Linear Bottlenecks </p><ul><li>ReLU results in information loss in lower dimension space</li><li><strong>expansion ratio</strong>：if we have lots of channels, information might still be preserved in the other channels</li><li><p>linear：bottleneck上面不包含非线性激活单元</p><p><img src="/2020/04/16/MobileNets/expansion.png" width="30%"></p></li></ul></li><li><p>Inverted residuals </p><ul><li>bottlenecks actually contain all the necessary information </li><li>expansion layer acts merely as an implementation detail that accompanies a non-linear transformation</li></ul><p><img src="/2020/04/16/MobileNets/residual.png" width="30%"></p><ul><li>parameter count：</li><li>basic building block is a bottleneck depth-separable convolution with residuals</li></ul></li></ul><p><img src="/2020/04/16/MobileNets/blockparam.png" width="40%"></p></li></ol><pre><code>* interpretation     * provides a natural separation between the input/output    * expansion：capacity    * layer transformation：expressiveness* MobileNetV2 model architecture    * initial filters：32    * ReLU6：use ReLU6 as the non-linearity because of its robustness when used with low-precision computation      * use constant expansion rate between 5 and 10 except the 1st：smaller network inclines smaller and larger larger    &lt;img src=&quot;MobileNets/MobileNetV2.png&quot; width=&quot;40%&quot; /&gt;    * comparison with other architectures    &lt;img src=&quot;MobileNets/cmpV2.png&quot; width=&quot;40%&quot; /&gt;</code></pre><ol><li><p>实验</p><ul><li><p>Object Detection  </p><ul><li>evaluate the performance as feature extractors</li><li>replace all the regular convolutions with separable convolutions <strong>in SSD prediction layers</strong>：backbone没有改动，只替换头部的卷积，降低计算量</li><li>achieves competitive accuracy with significantly fewer parameters and smaller computational complexity </li></ul></li><li><p>Semantic Segmentation </p><ul><li>build DeepLabv3 heads on top of the second last feature map of MobileNetV2  </li><li>DeepLabv3 heads are computationally expensive and removing the ASPP module significantly reduces the MAdds </li></ul></li><li><p>ablation</p><ul><li>inverted residual connections：shortcut connecting bottleneck perform better than shortcuts connecting the expanded layers 在少通道的特征上进行短连接</li><li><p>linear bottlenecks：linear bottlenecks improve performance, <strong>providing support that non-linearity destroys information in low-dimensional space</strong></p><p><img src="/2020/04/16/MobileNets/ablation.png" width="40%"></p></li></ul></li></ul></li></ol><h2 id="Searching-for-MobileNetV3"><a href="#Searching-for-MobileNetV3" class="headerlink" title="Searching for MobileNetV3"></a>Searching for MobileNetV3</h2><ol><li><p>动机</p><ul><li>automated search algorithms and network design work together</li><li>classification &amp; detection &amp; segmentation</li><li>a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP)</li><li>new efficient versions of nonlinearities  </li></ul></li><li><p>论点</p><ul><li>reducing<ul><li>the number of parameters </li><li>the number of operations (MAdds)  </li><li>inference latency</li></ul></li><li>related work<ul><li>SqueezeNet：1x1 convolutions </li><li>MobileNetV1：separable convolution  </li><li>MobileNetV2：inverted residuals </li><li>ShuffleNet：group convolutions  </li><li>CondenseNet：group convolutions  </li><li>ShiftNet：shift operation </li><li>MnasNet：MobileNetV2+SE-block，attention modules are placed after the depthwise filters in the expansion </li></ul></li></ul></li><li><p>方法</p><ul><li><p>base blocks </p><ul><li>combination of ideas from [MobileNetV1, MobileNetV2, MnasNet]</li><li>inverted-res-block + SE-block</li><li>swish nonlinearity  </li><li>hard sigmoid</li></ul><p><img src="/2020/04/16/MobileNets/IR+SE.png" width="40%"></p></li><li><p>Network Search </p><ul><li>use platform-aware NAS to search for the global network structures </li><li>use the NetAdapt algorithm to search per layer for the number of filters</li></ul></li><li><p>Network Improvements </p><ul><li><p>redesign the computionally-expensive layers at the beginning and the end of the network</p><ul><li>the last block of MobileNetV2’s inverted bottleneck structure </li><li>move this layer <strong>past</strong> the final average pooling：移动到GAP后面去，作用在1x1的featuremap上instead of 7x7，曲线救国</li></ul></li><li><p>a new nonlinearity, h-swish</p><ul><li><p>the initial set of filters are also expensive：usually start with 32 filters in a full 3x3 convolution to build initial filter banks for edge detection </p></li><li><p>reduce the number of filters to 16 and use the hard swish nonlinearity </p><script type="math/tex; mode=display">  swish\ [x]=x*\sigma(x)\\  h\_swish\ [x]=x\frac{ReLU6(x+3)}{6}</script><p><img src="/2020/04/16/MobileNets/swish.png" width="40%"></p></li><li><p>most of the benefits swish are realized by using them only in the deeper layers：只在后半段网络中用</p></li></ul></li><li><p>SE-block</p><ul><li>ratio：all to fixed to be 1/4 of the number of channels in expansion layer</li></ul></li></ul></li><li><p>MobileNetV3 architecture</p><p> <img src="/2020/04/16/MobileNets/mobileNetV3.png" width="40%"></p></li></ul></li><li><p>实验</p><ul><li><p>Detection </p><ul><li>use MobileNetV3 as replacement for the <strong>backbone feature extractor</strong> in SSDLite：改做backbone了</li><li>reduce the channel counts of C4&amp;C5’s block：因为MobileNetV3原本是被用来输出1000类的，transfer到90类的coco数据集上有些redundant</li></ul></li><li><p>Segmentation </p><ul><li><p>as network backbone</p></li><li><p>compare two segmentation heads</p><ul><li>R-ASPP：reduced design of the Atrous Spatial Pyramid Pooling module with only two branches </li><li><p>Lite R-ASPP：类SE-block的设计，大卷积核，大步长</p><p><img src="/2020/04/16/MobileNets/LR-ASPP.png" width="40%"></p></li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> papers </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>verseg</title>
      <link href="/2020/04/15/verseg/"/>
      <url>/2020/04/15/verseg/</url>
      <content type="html"><![CDATA[<ol><li><p>challenge</p><p> Large Scale Vertebrae Segmentation Challenge</p><ul><li>task1:Vertebra Labelling，关键点检测</li><li>task2:Vertebra Segmentation，多类别分割</li></ul></li><li><p>data</p><p><img src="/2020/04/15/verseg/data.png" width="80%"></p><ol><li>variation：数据affine轴不统一，尺寸不统一，扫描范围不统一，FOV区域不统一</li><li>nii的两大解析工具：nibabel库load data的xyz顺序与axcode的顺序一致，e.g.[‘R’,’A’,’S’]的orientation会得到xyz的array，而sitk的读取刚好反过来，sitk的arr会是zyx。我们之前在将dicom写入nii时，会指定一个不为np.eye(4)的affine，就是为了transpose这三个轴。</li></ol></li><li><p>model</p><ol><li><p>team paper \<vertebrae localization="" and="" segmentation="" with="" spatialconfiguration-net="" u-net\=""></vertebrae></p><ul><li><p>三阶段：第一阶段，due to large variation FOV of the dataset，粗分割定位脊柱位置，第二阶段，higher resolution多类别关键点定位center，获得each located vertebra，第三阶段，二类分割for each located vertebra。</p></li><li><p>keywords：1. uniform voxel spacing：不要随意resize，todo: trilinear interp；2. on-the-fly data augmentation：using SimpleITK</p></li><li><p>第一阶段：Spine Localization</p><ul><li>Unet</li><li><strong>regress</strong> the Gaussian <strong>heatmap</strong> of spinal centerline</li><li>L2-loss</li><li>uniform voxel spacing of 8mm</li><li><p>input shape：[64,64,128]，pad？</p><p><img src="/2020/04/15/verseg/stage1.png" width="50%"></p></li></ul></li><li><p>第二阶段：Vertebrae Localization</p><ul><li>SpatialConfiguration-Net</li><li>regress each located vertebra‘s heatmap in individual channel</li><li>resampling：bi/tricubic interpolation</li><li>norm：maxmin on the whole dataset</li><li>uniform voxel spacing of 2mm</li><li><p>input shape：[96,96,128]，z-axis random crop，xy-plane use ROI from stage1</p><p><img src="/2020/04/15/verseg/stage2.png" width="50%"></p><p><img src="/2020/04/15/verseg/SCN.png" width="60%"></p></li></ul></li><li><p>第三阶段：Vertebrae Segmentation</p><ul><li>Unet</li><li>binary segment the mask of each vertebrae</li><li>sigmoid ce-loss</li><li>uniform voxel spacing of 1mm</li><li><p>input shape：[128,128,96]，crop origin image &amp; heatmap image based on centroids</p><p><img src="/2020/04/15/verseg/stage3.png" width="50%"></p></li></ul></li></ul></li><li><p>reference paper\<btrfly net:="" vertebrae="" labelling="" with="" energy-="" based="" adversarial="" learning="" of="" local="" spine="" prior="" \=""></btrfly></p><ul><li>核心贡献：1.MIP：combines the information across reformations，3D to 2D，2. 基于判别器的训练机制：encodes local spine structure as an anatomical prior，加固椎块间类别&amp;位置的spacial information</li></ul><p><img src="/2020/04/15/verseg/btr overview.png" width="60%"></p><ul><li>MIP：<ul><li>localisation and identification rely on a large context </li><li>large receptive field</li><li>in full-body scans where spine is not spatially centred or is obstructed by the ribcage, such cases are handled with a pre-processing stage detecting the occluded spine  </li></ul></li><li><p>adversarial learning：</p><ul><li>FCN用于分割</li><li>AE用于评估分割的好坏</li><li>do not ‘pre-train’ it (the AE)</li><li>loss：an anatomically-inspired supervision instead of the usual binary adversarial supervision (vanilla GAN)</li></ul></li><li><p>先说FCN——Btrfly Network</p><ul><li><p>建模成回归问题，每个关键点对应一个通道的高斯heatmap，背景channel为$1-max_i (y_i)$</p></li><li><p>双输入双输出（sagittal &amp; coronal）</p></li><li><p>两个视角的feature map在网络深层做了融合，to learn their inter-dependency</p></li><li><p>Batch- normalisation is used after every convolution layer, along with 20% dropout in the fused layers of Btrfly</p></li><li><p>loss：l2 distance + weighted ce</p><script type="math/tex; mode=display">  L_{sag} = ||Y_{sag} - \hat{Y}_{sag}||^2 + \omega CE(softmax(Y_{sag}, softmax(\hat{Y}_{sag}))</script><p>  $\omega$ is the median frequency weighing map, boosting the learning of less frequent classes(ECB)</p><p><img src="/2020/04/15/verseg/btrfly.png" width="60%"></p></li></ul></li><li><p>再说判别器——Energy-based adversary for encoding prior</p><ul><li><p>fully-convolutional：its predictions across voxels are independent of each other owing to the spatial invariance of convolutions</p></li><li><p>to impose the anatomical prior of the spine’s shape onto the Btrfly net</p></li><li><p>look at $\hat{Y}_{sag}$ and $\hat{Y}_{cor}$ as a 3D volume and employ a 3D AE with a receptive field covering a part of the spine </p></li><li><p>$\hat{Y}_{sag}$ consists of Gaussians：less informative than an image, avoid using max-pooling by resorting to average pooling </p></li><li><p>employ spatially dilated convolution kernels</p></li><li><p>mission of AE：predict the l2 distance of input and its reconstruction, it learns to discriminate by predicting a low E for real annotations, while G learns to generate annotations that would trick D </p><script type="math/tex; mode=display">  L = D(Y_x) + max(0, m-D(Y_g))\\  L_G = D(Y_g) + L_{fcn}</script><p><img src="/2020/04/15/verseg/btr gan.png" width="60%"></p></li></ul></li><li><p>inference：</p><ul><li>The values below a threshold (T) are ignored in order to remove noisy predictions </li><li>用外积，$\hat{Y}=\hat{Y}_{sag}\otimes\hat{Y}_{cor}$</li><li>每个channel的最大值作为centroids</li></ul></li><li><p>experiments</p><ul><li>【IMPORTANT】10 MIPs are obtained from one 3D scan per view, each time randomly choosing half the slices of interest</li><li>对于每个视角，每次随机抽取一半数目的slice用于计算MIP</li></ul></li></ul></li></ol></li></ol><ul><li><p>similar local appearance：</p></li><li><p>strong spatial configuration：凡是涉及到椎块-wise的信息，从全局信息入手</p></li></ul>]]></content>
      
      
        <tags>
            
            <tag> challenge </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>GoogLeNet系列</title>
      <link href="/2020/04/13/GoogLeNet%E7%B3%BB%E5%88%97/"/>
      <url>/2020/04/13/GoogLeNet%E7%B3%BB%E5%88%97/</url>
      <content type="html"><![CDATA[<h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><ol><li><p>papers</p><ul><li><p>[V1] Going Deeper with Convolutions, 6.67% test error</p></li><li><p>[V2] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, 4.8% test error</p></li><li><p>[V3] Rethinking the Inception Architecture for Computer Vision, 3.5% test error</p></li><li><p>[V4] Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, 3.08% test error</p></li><li><p>[Xception] Xception: Deep Learning with Depthwise Separable Convolutions</p></li><li><p>[EfficientNet] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks </p></li><li><p>[EfficientDet] EfficientDet: Scalable and Efficient Object Detection </p></li><li><p>[EfficientNetV2] EfficientNetV2: Smaller Models and Faster Training </p></li></ul></li></ol><ol><li><p>大体思路</p><ul><li>inception V1：打破传统的conv block，设计了<strong>Inception block</strong>，将1*1、3*3、5*5的卷积结果concat，增加网络宽度</li><li>inception V2：加入了<strong>BN层</strong>，减少Internal Covariate Shift，用两个3*3替代5*5，降低参数量</li><li>inception V3：提出<strong>分解Factorization</strong>，7*7改成7*1和1*7，参数减少加速计算，增加网络深度和非线性</li><li>inception V4：结合<strong>Residual</strong> Connection</li><li>Xception：针对inception V3的分解结构的改进，使用<strong>可分离卷积</strong></li><li>EfficientNet：主要研究model scaling，针对网络深度、宽度、图像分辨率，有效地扩展CNN</li><li>EfficientDet：将EfficientNet从分类任务扩展到目标检测任务</li></ul></li></ol><h2 id="review"><a href="#review" class="headerlink" title="review"></a>review</h2><ol><li><p>review0122：conv-BN层合并运算</p><ul><li><p>reference：<a href="https://nenadmarkus.com/p/fusing-batchnorm-and-conv/" target="_blank" rel="noopener">https://nenadmarkus.com/p/fusing-batchnorm-and-conv/</a></p></li><li><p>freezed BN可以看成1x1的卷积运算</p></li><li><p>两个线性运算是可以合并的</p></li><li><p>given $W_{conv} \in R^{C<em>C_{prev}</em>k<em>k}$，$b_{conv} \in R^C $，$W_{bn}\in R^{C</em>C}$，$b_{bn}\in R^C$</p><script type="math/tex; mode=display">  F = W_{bn} * (W_{conv} * F_{prev} + b_{conv}) + b_{bn}</script></li></ul></li></ol><h2 id="V1-Going-deeper-with-convolutions"><a href="#V1-Going-deeper-with-convolutions" class="headerlink" title="V1: Going deeper with convolutions"></a>V1: Going deeper with convolutions</h2><ol><li><p>动机</p><ul><li>improved utilization of the computing resources</li><li>increasing the depth and width of the network while keeping the computational budget</li></ul></li><li><p>论点</p><ul><li>the recent trend has been to increase the number of layers and layer size, while using dropout to address the problem of overfitting </li><li>major bottleneck：large network，large number of params，limited dataset，overfitting</li><li>methods use filters of different sizes in order to handle multiple scales </li><li>NiN use  1x1 convolutional layers to easily integrate in the current CNN pipelines</li><li>we use 1x1 convs with a dual purpose of dimension reduction  </li></ul></li><li><p>方法</p><ul><li><p>Architectural</p><ul><li>1x1 conv+ReLU for compute reductions  </li><li><p>an alternative parallel pooling path since pooling operations have been essential for the success </p><p><img src="/2020/04/13/GoogLeNet系列/inceptionv1.png" width="80%"></p></li><li><p>overall architecture :</p><p><img src="/2020/04/13/GoogLeNet系列/googlenet.png" width="80%"></p></li><li><p>细节：</p><ul><li><p>rectified linear activation</p></li><li><p>mean subtraction</p></li><li><p>a move from fully connected layers to average pooling improves acc</p></li><li><p>the use of dropout remained essential  </p></li><li><p>adding auxiliary classifiers(on 4c&amp;4d) with a discount weight  </p><ul><li>5x5 avg pool, stride 3</li><li>1x1 conv+relu, 128 filters</li><li>1024 fc+relu</li><li>70% dropout</li><li><p>1000 fc+softmax</p><p><img src="/2020/04/13/GoogLeNet系列/auxilary.png" width="60%"></p></li></ul></li><li><p>asynchronous stochastic gradient descent with 0.9 momentum </p></li><li><p>fixed learning rate schedule (de- creasing the learning rate by 4% every 8 epochs </p></li><li><p>photometric distortions useful to combat overfitting </p></li><li><p>random interpolation methods for resizing </p></li></ul></li></ul></li></ul></li></ol><h2 id="V2-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift"><a href="#V2-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift" class="headerlink" title="V2: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"></a>V2: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</h2><ol><li><p>动机</p><ul><li>use much higher learning rates </li><li>be less careful about initialization </li><li>also acts as a regularizer,  eliminating the need for Dropout</li></ul></li><li><p>论点</p><ul><li><p>SGD：optimizes the parameters $\theta$ of the network, so as to minimize the loss </p><script type="math/tex; mode=display">  \theta = argmin_{\theta} \frac{1}{N}\sum_{i=1}^N loss(x_i, \theta)</script><p>  梯度更新：<script type="math/tex">\theta_{next-timestep} = \theta - \alpha \frac{\partial[\frac{1}{N}\sum_Nloss(\theta)]}{\partial \theta}</script>，$x_i$ is the full set</p><p>  batch approximation：use $\frac{1}{m} \sum_M \frac{\partial loss(\theta)}{\partial \theta}$，$x_i$ is the mini-batch set</p><ul><li>quality improves as the batch size increases </li><li>computation over a batch is much more efficient than m computations for individual examples</li><li>the learning rate and the initial values require careful tuning </li></ul></li><li><p>Internal covariate shift</p><ul><li>the input distribution of the layers changes</li><li>consider a gradient descent step above，$x$的数据分布改变了，$\theta$就要相应地 readjust to compensate for the change in the distribution of x</li></ul></li><li><p>activation</p><ul><li>对于神经元$z = sigmoid(Wx+b)$，前面层的参数变化，很容易导致当前神经元的响应值不在有效活动区间，从而导致过了当前激活函数以后梯度消失，slow down the convergence </li><li>In practice, using ReLU &amp; careful initialization  &amp; small learning rates</li><li>如果我们能使得the distribution of nonlinearity inputs remains more stable as the network trains，就不会出现神经元饱和的问题</li></ul></li><li><p>whitening</p><ul><li>对training set的预处理：linearly transformed to have zero means and unit variances, and decorrelated</li><li>使得输入数据的分布保持稳定，normal distribution</li><li>同时去除了数据间的相关性</li></ul></li><li><p>batch normalization</p><ul><li>fixes the means and variances of layer inputs </li><li>reducing the dependence of gradients on the scale of the parameters or of their initial values</li><li>makes it possible to use saturating nonlinearities </li></ul></li></ul></li></ol><pre><code>    * full whitening of each layer is costly    * so we normalize each layer independently, full set--&gt; mini-batch    * standard normal distribution并不是每个神经元所需的（如identity transform）：introduce, for each activation $x(k)$ , a pair of parameters $\gamma(k)$, $\beta(k)$, which scale and shift the normalized value to maintain the representation ability of the neuron    ​        * for convolutional networks        * we add the BN transform immediately before the nonlinearity, $z = g(Wx+b)$ to $z = g(BN(Wx))$        * since we normalize $Wx+b$, the bias b can be ignored          * obey the convolutional property——different elements of the same feature map, at different locations, are normalized in the same way        * We learn a pair of parameters $\gamma(k)$ and $\beta(k)$ **per feature map**, rather than per activation    * properties        * back-propagation through a layer is unaffected by the scale of its parameters        * Moreover, larger weights lead to smaller gradients, thus stabilizing the parameter growth        * regularizes the model：因为网络中mini-batch的数据之间是有互相影响的而非independent的</code></pre><ol><li><p>方法</p><ul><li><p>batch normalization</p><ul><li>full whitening of each layer is costly</li><li>so we normalize each layer independently, full set—&gt; mini-batch</li><li><p>standard normal distribution并不是每个神经元所需的（如identity transform）：introduce, for each activation $x(k)$ , a pair of parameters $\gamma(k)$, $\beta(k)$, which scale and shift the normalized value to maintain the representation ability of the neuron</p><p><img src="/2020/04/13/GoogLeNet系列/BN.png" width="40%"></p></li></ul></li><li><p>bp：</p><p>  <img src="/2020/04/13/GoogLeNet系列/BNbp.png" width="40%"></p></li><li><p>inference阶段：</p><ul><li>首先两个可学习参数$\gamma$和$\beta$是定下来的</li><li><p>而均值和方差不再通过输入数据来计算，而是载入训练过程中维护的参数（moving averages）</p><p><img src="/2020/04/13/GoogLeNet系列/BNinfer.png" width="40%"></p><p>​    </p></li></ul></li><li><p>for convolutional networks</p><ul><li><p>we add the BN transform immediately before the nonlinearity, $z = g(Wx+b)$ to $z = g(BN(Wx))$</p></li><li><p>since we normalize $Wx+b$, the bias b can be ignored  </p></li><li>obey the convolutional property——different elements of the same feature map, at different locations, are normalized in the same way</li><li>We learn a pair of parameters $\gamma(k)$ and $\beta(k)$ <strong>per feature map</strong>, rather than per activation</li></ul></li><li><p>properties</p><ul><li>back-propagation through a layer is unaffected by the scale of its parameters</li><li>Moreover, larger weights lead to smaller gradients, thus stabilizing the parameter growth</li><li>regularizes the model：因为网络中mini-batch的数据之间是有互相影响的而非independent的</li></ul></li></ul></li></ol><h2 id="V3-Rethinking-the-Inception-Architecture-for-Computer-Vision"><a href="#V3-Rethinking-the-Inception-Architecture-for-Computer-Vision" class="headerlink" title="V3: Rethinking the Inception Architecture for Computer Vision"></a>V3: Rethinking the Inception Architecture for Computer Vision</h2><ol><li><p>动机</p><ul><li>go deeper and wider：<ul><li>enough labeled data</li><li>computational efficiency</li><li>parameter count</li></ul></li><li>to scale up networks<ul><li>utilizing the added computation as efficiently</li><li>give general design principles and optimization ideas  <ul><li>factorized convolutions</li><li>aggressive regularization</li></ul></li></ul></li></ul></li><li><p>论点</p><ul><li>GoogleNet does not provide a clear description about the contributing factors that lead to the various design </li></ul></li><li><p>方法</p><ul><li><p>General Design Principles </p><ul><li>Avoid representational bottlenecks：特征图尺寸应该gently decrease，resolution的下降必须伴随着channel数的上升，<a href="https://www.zybuluo.com/Team/note/1332528" target="_blank" rel="noopener">避免使用max pooling层进行下采样，因为这样导致信息损失较大</a></li><li>Higher dimensional representations are easier to process locally within a network. Increasing the activa- tions per tile in a convolutional network allows for more disentangled features. The resulting networks will train faster：前半句懂了，high-reso的特征图focus在局部信息，后半句不懂，根据上一篇paper，用了batch norm以后，scale up神经元不影响bp，同时会lead to smaller gradients，为啥能加速？</li><li>Spatial aggregation can be done over lower dimensional embeddings：adjacent unit之间有strong correlation，所以可以reduce the dimension of the input representation before the spatial aggregation，不会有太大的信息损失，并且promotes faster learning</li><li>The computational budget should therefore be distributed in a balanced way between the depth and width of the network.</li></ul></li><li><p>Factorizing Convolutions Filter Size </p><ul><li>into smaller convolutions <ul><li>大filter都可以拆解成多个3x3</li><li>单纯去等价线性分解可以不使用非线性activation，但是我们使用了batch norm（increase variaty），所以观察到使用ReLU以后拟合效果更好</li></ul></li><li>into Asymmetric Convolutions <ul><li>n*n的filter拆解成1*n和n*1</li><li>this factorization does not work well on early layers, but gives very good results on medium grid-sizes (ranges between 12 and 20, using 1x7 and 7x1</li></ul></li></ul></li><li><p>Utility of Auxiliary Classifiers </p><ul><li>did not result in improved convergence early in the training：训练开始阶段没啥用，快收敛时候有点点acc提升</li><li>removal of the lower auxiliary branch did not have any adverse effect on the final quality：拿掉对最终结果没影响</li><li>所以最初的设想（help evolving the low-level features） 是错的，仅仅act as regularizer，auxiliary head里面加上batch norm会使得最终结果better</li></ul></li><li><p>Efficient Grid Size Reduction下采样模块不再使用maxpooling</p><ul><li><p>dxdxk feature map expand to (d/2)x(d/2)x2k：</p><ul><li>1x1x2k conv，stride2 pool：kxdxdx2k computation</li><li>1x1x2k stride2 conv：kx(d/2)x(d/2)x2k computation，计算量下降，但是违反principle1</li><li>parallel stride P and C blocks：kx(d/2)x(d/2)xk computation，符合principle1:reduces the grid-size while expands the filter banks</li></ul><p><img src="/2020/04/13/GoogLeNet系列/bottleneck1.png" width="40%">  <img src="/2020/04/13/GoogLeNet系列/bottleneck2.png" width="40%"></p></li></ul></li><li><p>Inception-v3</p><ul><li>开头的7x7conv已经换成了多个3x3</li><li>中间层featuremap降维到17x17的时候，开始用Asymmetric Factorization block</li><li>到8x8的时候，做了expanding the filter bank outputs</li></ul><p><img src="/2020/04/13/GoogLeNet系列/v3.png" width="70%"></p><p><img src="/2020/04/13/GoogLeNet系列/inceptionv3.png" width="40%"></p></li><li><p>Label Smoothing （<a href="https://zhuanlan.zhihu.com/p/116466239）" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/116466239）</a></p><ul><li><p>used the uniform distribution  $u(k)=1/K$</p><script type="math/tex; mode=display">  q(k) = (1-\epsilon)\delta(k) + \frac{\epsilon}{K}</script></li><li><p>对于softmax公式：$p(k)=\frac{exp(y_k)}{\sum exp(y_i)}$，这个loss训练的结果就是$y_k$无限趋近于1，其他$y_i$无限趋近于0，</p></li><li><p>交叉熵loss：$ce=\sum -y_{gt}log(y_k)$，加了label smoothing以后，loss上增加了阴性样本的regularization，正负样本的最优解被限定在有限值，通过抑制正负样本输出差值，使得网络有更强的泛化能力。</p></li></ul></li></ul></li></ol><h2 id="V4-Inception-v4-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning"><a href="#V4-Inception-v4-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning" class="headerlink" title="V4: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning"></a>V4: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</h2><ol><li><p>动机</p><ul><li>residual：whether there are any benefit in combining the Inception architecture with residual connections</li><li>inceptionV4：simplify the inception blocks</li></ul></li><li><p>论点</p><ul><li>residual connections seems to improve the training speed greatly：但是没有也能训练深层网络</li><li>made uniform choices for the Inception blocks for each grid size <ul><li>Inception-A for 35x35</li><li>Inception-B for 17x17</li><li>Inception-C for 8x8</li></ul></li><li>for residual versions  <ul><li>use cheaper Inception blocks for residual versions：简化module，因为identity部分（直接相连的线）本身包含丰富的特征信息</li><li>没有使用pooling</li><li>replace the filter concatenation stage of the Inception architecture with residual connections：原来block里面的concatenation主体放在残差path中</li><li>Each Inception block is followed by filter-expansion layer (1 × 1 convolution without activation) to match the depth of the input for addition：相加之前保证channel数一致</li><li>used batch-normalization only on top of the traditional layers, but not on top of the summations：浪费内存</li><li>number of filters exceeded 1000 causes instabilities </li><li>scaling down the residuals before adding by factors between 0.1 and 0.3：残差通道响应值不要太大</li></ul></li></ul></li><li><p>blocks</p><ul><li><p>V4 ABC：</p><p>  <img src="/2020/04/13/GoogLeNet系列/V4A.png" width="30%">  <img src="/2020/04/13/GoogLeNet系列/V4B.png" width="30%">    <img src="/2020/04/13/GoogLeNet系列/V4C.png" width="30%"></p></li><li><p>Res ABC：</p><p>  <img src="/2020/04/13/GoogLeNet系列/ResA.png" width="25%">     <img src="/2020/04/13/GoogLeNet系列/ResB.png" width="25%">        <img src="/2020/04/13/GoogLeNet系列/ResC.png" width="25%"></p></li></ul></li></ol><h2 id="Xception-Deep-Learning-with-Depthwise-Separable-Convolutions"><a href="#Xception-Deep-Learning-with-Depthwise-Separable-Convolutions" class="headerlink" title="Xception: Deep Learning with Depthwise Separable Convolutions"></a>Xception: Deep Learning with Depthwise Separable Convolutions</h2><ol><li><p>动机</p><ul><li>Inception modules have been replaced with depthwise separable convolutions</li><li>significantly outperforms Inception V3 on a larger dataset</li><li>due to more efficient use of model parameters</li></ul></li><li><p>论点</p><ul><li><p>early LeNet-style models </p><ul><li>simple stacks of convolutions for feature extraction and max-pooling operations for spatial sub-sampling </li><li>increasingly deeper</li></ul></li><li><p>complex blocks</p><ul><li>Inception modules inspired by NiN</li><li>be capable of learning richer repre- sentations with less parameters </li></ul></li><li><p>The Inception hypothesis</p><ul><li>a single convolution kernel is tasked with simultaneously mapping cross-channel correlations and spatial correlations</li><li>while Inception factors it into a series of operations that independently look at cross-channel correlations(1x1 convs) and at spatial correlations(3x3/5x5 convs)</li><li><p>suggesting that cross-channel correlations and spatial correlations are sufficiently decoupled that it is preferable not to map them jointly</p><p><img src="/2020/04/13/GoogLeNet系列/inception.png" width="45%"></p></li></ul></li><li><p>inception block先用1x1的conv将原输出映射到3-4个lower space（cross-channel correlations），然后在这些小的3d spaces上做regular conv（maps all correlations ）——进一步假设，彻底解耦，第二步只做spatial correlations</p><p><img src="/2020/04/13/GoogLeNet系列/extreme.png" width="45%"></p></li><li><p>main differences between <strong>“extreme ” Inception</strong> and <strong>depthwise separable convolution</strong></p><ul><li>order of the operations：1x1 first or latter</li><li>non-linearity：depthwise separable convolutions are usually implemented without non-linearities【QUESTION：这和MobileNet里面说的不一样啊，M里面的depthwise也是每层都带了BN和ReLU的】</li></ul></li></ul></li><li><p>要素</p><ul><li>Convolutional neural networks </li><li>The Inception family  </li><li>Depthwise separable convolutions</li><li>Residual connections</li></ul></li><li><p>方法</p><ul><li><p>architecture</p><ul><li>a linear stack of depthwise separable convolution layers with residual connections</li><li>all conv are followed by BN</li><li><p>keras的separableConv和depthwiseConv：前者由后者加上一个pointwiseConv组成，<strong>最后有activation，中间没有</strong></p><p><img src="/2020/04/13/GoogLeNet系列/xception.png" width="80%;"></p></li><li><p>cmp</p><ul><li>Xception and Inception V3 have nearly the same number of parameters </li><li>marginally better  on ImageNet</li><li>much larger performance increasement on JFT </li><li>Residual connections are clearly essential in helping with convergence, both in terms of speed and final classification performance. </li><li><p><strong>Effect of intermediate activation：</strong>the absence of any non-linearity leads to both faster convergence and better final performance </p><p><img src="/2020/04/13/GoogLeNet系列/intermediate.png" width="40%;"></p></li></ul></li></ul></li></ul></li></ol><h2 id="EfficientNet-Rethinking-Model-Scaling-for-Convolutional-Neural-Networks"><a href="#EfficientNet-Rethinking-Model-Scaling-for-Convolutional-Neural-Networks" class="headerlink" title="EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"></a>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</h2><ol><li><p>动机</p><ul><li>common sense：scaled up the network for better accuracy </li><li>we systematically study model scaling</li><li>and identify that carefully <strong>balancing network depth, width, and resolution</strong> can lead to better performance</li><li>propose a new scaling method：using <strong>compound coefficient</strong> to uniformly scale all dimensions of depth/width/resolution <ul><li>on MobileNets and ResNet</li><li>a new baseline network family EfficientNets</li></ul></li><li>much better accuracy and efficiency </li></ul></li><li><p>论点</p><ul><li>previous work scale up one of the three dimensions  <ul><li>depth：more layers</li><li>width：more channels</li><li>image resolution：higher resolution</li></ul></li><li><strong>arbitrary scaling</strong> requires tedious manual tuning and often yields sub-optimal accuracy and efficiency </li><li><p><strong>uniformly scaling</strong>：Our empirical study shows that it is critical to balance all dimensions of network width/depth/resolution, and surprisingly such balance can be achieved by simply scaling each of them with constant ratio. </p><p><img src="/2020/04/13/GoogLeNet系列/modelscaling.png" width="70%;"></p></li><li><p><strong>neural architecture search：</strong>becomes increasingly popular in designing efficient mobile-size ConvNets </p></li></ul></li><li><p>方法</p><ul><li><p>problem formulation</p><ul><li><p>ConvNets：$N = \bigodot_{i=1…s} F_i^{L_i}(X_{<h_i,w_i,c_i>})$, s for stage, L for repeat times, F for function</h_i,w_i,c_i></p></li><li><p>simplify the design problem </p><ul><li>fixing $F_i$</li><li>all layers must be scaled uniformly with constant ratio </li></ul></li><li><p>an optimization problem：d for depth coefficients, w for width coefficients, r for resolution coefficients </p><script type="math/tex; mode=display">  max_{d,w,r} \ \ \ Accuracy(N(d,w,r))\\  s.t. \ \ \ N(d,w,r) = \bigodot_{i=1...s} F_i^{d*L_i}(X_{<r*H_i,r*W_i,w*C_i>})</script></li></ul></li><li><p>observation 1</p><ul><li>Scaling up any dimension of network (width, depth, or resolution) improves accuracy, but the accuracy gain diminishes for bigger models. 准确率都会提升，最终都会饱和</li><li>depth：deeper ConvNet can capture richer and more complex features </li><li>width：wider networks tend to be able to capture more fine-grained features and are easier to train （commonly used for small size models）但是深度和宽度最好匹配，一味加宽shallow network会较难提取高级特征</li><li>resolution：higher resolution input can potentially capture more fine-grained patterns   </li></ul><p><img src="/2020/04/13/GoogLeNet系列/dimensions.png" width="60%;"></p></li><li><p>observation 2</p><ul><li><p>compound scaling：it is critical to balance all dimensions of network width, depth, and resolution</p></li><li><p>different scaling dimensions are not independent 输入更高的resolution，就需要更深的网络，以获取更大的感受野，同时还需要更宽的网络，以捕获更多的细粒度特征</p></li><li><p>compound coefficient $\phi$：</p><script type="math/tex; mode=display">  d = \alpha ^ \phi\\  w = \beta ^ \phi\\  r = \gamma ^ \phi\\  s.t. \alpha * \beta^2 * \gamma^2 \approx 2, \ \alpha\geq1,\ \beta\geq1,\ \gamma\geq1</script><ul><li>$\alpha, \beta, \gamma$ are constants determined by a small grid search, controling the assign among the 3 dimensions [d,w,r]</li><li>$\phi$ controls how many more resources are available for model scaling</li><li>the total FLOPS will approximately increase by $2^\phi$</li></ul></li></ul></li><li><p>efficientNet architecture </p><ul><li><p>having a good baseline network is also critical</p></li><li><p>thus we developed a new <strong>mobile-size baseline</strong> called EfficientNet by leveraging a <strong>multi-objective neural architecture search</strong> that <strong>optimizes both accuracy and FLOPS</strong> </p><p>  <img src="/2020/04/13/GoogLeNet系列/efficientB0.png" width="40%;"></p></li><li><p>compound scaling：fix $\phi=1$ and <strong>grid search</strong> $\alpha, \beta, \gamma$, fix $\alpha, \beta, \gamma$ and use different $\phi$</p></li></ul></li></ul></li><li><p>实验</p><ul><li><p>on MobileNets and ResNets </p><ul><li>compared to other single-dimension scaling methods</li><li>compound scaling method improves the accuracy on all </li></ul></li><li><p>on EfficientNet </p><ul><li>model with compound scaling tends to focus on more relevant regions with more object details</li><li><p>while other models are either lack of object details or unable to capture all objects in the images</p><p><img src="/2020/04/13/GoogLeNet系列/activationMap.png" width="60%;"></p></li></ul></li></ul></li><li><p>implementing details</p><ul><li>RMSProp: decay=0.9, momentum(rho)=0.9，tpu上使用lars</li><li>BN: momentum=0.99</li><li>weight decay = 1e-5</li><li>lr: initial=0.256, decays by 0.97 every 2.4 epochs</li><li>SiLU activation</li><li>AutoAugment</li><li>Stochastic depth: survive_prob = 0.8</li><li>dropout rate: 0.2 to 0.5 for B0 to B7</li></ul></li></ol><h2 id="EfficientDet-Scalable-and-Efficient-Object-Detection"><a href="#EfficientDet-Scalable-and-Efficient-Object-Detection" class="headerlink" title="EfficientDet: Scalable and Efficient Object Detection"></a>EfficientDet: Scalable and Efficient Object Detection</h2><ol><li><p>动机</p><ul><li>model efficiency</li><li>for object detection：based on <strong>one-stage</strong> detector  </li><li>特征融合：propose a weighted bi-directional feature pyramid network (BiFPN)</li><li>网络rescale：uniformly scales the resolution, depth, and width for all backbone</li><li>achieve better accuracy with much fewer parameters and FLOPs </li><li>also test on Pascal VOC 2012 semantic segmentation</li></ul></li><li><p>论点</p><ul><li>previous work tends to achieve better efficiency by sacrificing accuracy</li><li>previous work fuse feature at different resolutions by simply summing up without distinction </li><li>EfficientNet <ul><li>backbone：combine EfficientNet backbones with our propose BiFPN  </li><li>scale up：jointly scales up the resolution/depth/width for all <strong>backbone, feature network, box/class prediction network</strong></li></ul></li><li>Existing object detectors <ul><li>two-stage：have a region-of-interest proposal step  </li><li>one-stage：have not, use predefined anchors </li></ul></li></ul></li><li><p>方法</p><ul><li><p>BiFPN：efficient bidirectional cross-scale connec- tions and weighted feature fusion </p><ul><li><p>FPN：limit是只有top-bottom一条information flow</p></li><li><p>PANet：加上了一条bottom-up path，better accuracy但是more parameters and computations </p></li><li><p>NAS-FPN：基于网络搜索出的结构，irregular and difficult to interpret or modify </p></li><li><p>BiFPN</p><ul><li>remove those nodes that only have one input edge：只有一条输入的节点，没做到信息融合</li><li>add an extra edge from the original input to output node if they are at the same level：fuse more features without adding much cost </li><li><p>repeat blocks</p><p><img src="/2020/04/13/GoogLeNet系列/fpn.png" width="80%;"></p></li></ul></li><li><p>Weighted Feature Fusion </p><ul><li>since different input features are at different resolutions, they usually contribute to the output feature unequally</li><li>learnable weight that can be a scalar (per-feature), a vector (per-channel), or a multi-dimensional tensor (per-pixel)</li><li><p>weight normalization  </p><ul><li>Softmax-based：$O=\sum_i \frac{e^{w_i}}{\sum_j e^{w_j}}*I_i$</li><li>Fast normalized：$O=\sum_i \frac{w_i}{\epsilon + \sum_j w_j}*I_i$，Relu is applied after each $w_i$ to keep non-negative</li></ul><script type="math/tex; mode=display">P_6^{td} = Conv(\frac{w_1P_6^{in}+w_2Resize(P_7^{in})}{\epsilon+w_1+w_2})\\P_6^{out} = Conv(\frac{w_1P_6^{in}+w_2P_6^{td}+w_3Resize(P_5^{out})}{\epsilon+w_1+w_2+w_3})</script></li></ul></li><li><p>EfficientDet </p><ul><li>ImageNet-pretrained Effi- cientNets as the backbone </li><li>BiFPN serves as the feature network</li><li><p>the fused features(level 3-7) are fed to a class and box network respectively </p><p><img src="/2020/04/13/GoogLeNet系列/efficientDet.png" width="70%;"></p></li><li><p>compound scaling </p><ul><li><p>backbone：reuse the same width/depth scaling coefficients of EfficientNet-B0 to B6 </p></li><li><p>feature network：</p><ul><li>depth(layers)：$D=3+\phi$</li><li>width(channes)：$W=64 \cdot (1.35^{\phi}) $</li></ul></li><li><p>box/class prediction network：</p><ul><li>depth：$D=3+[\phi/3]$</li><li>width：same as FPN</li></ul></li><li><p>resolution</p><ul><li>use feature 3-7：must be dividable by $2^7$</li><li>$R=512+128*\phi$</li></ul></li><li><p>EfficientDet-D0 ($\phi=0$) to D7 ($\phi=7$) </p><p>  <img src="/2020/04/13/GoogLeNet系列/scaling.png" width="40%;"></p></li></ul></li></ul></li></ul></li></ul></li><li><p>实验</p><ul><li>for object detection <ul><li>train<ul><li>Learning rate is linearly increased from 0 to 0.16 in the first training epoch and then annealed down </li><li>employ commonly-used focal loss </li><li>3x3 anchors</li></ul></li><li>compare<ul><li>low-accuracy regime：低精度下，EfficientDet-D0和yoloV3差不多</li><li>中等精度，EfficientDet-D1和Mask-RCNN差不多</li><li>EfficientDet-D7 achieves a new state-of-the-art  </li></ul></li></ul></li><li>for semantic segmentation <ul><li>modify<ul><li>keep feature level {P2,P3,…,P7} in BiFPN</li><li>but only use P2 for the final per-pixel classification </li><li>set the channel size to 128 for BiFPN and 256 for classification head </li><li>Both BiFPN and classification head are repeated by 3 times</li></ul></li><li>compare<ul><li>和deeplabv3比的，COCO数据集</li><li>better accuracy and fewer FLOPs </li></ul></li></ul></li><li>ablation study<ul><li>backbone improves accuracy v.s. resnet50</li><li>BiFPN improves accuracy v.s. FPN</li><li>BiFPN achieves similar accuracy as repeated FPN+PANet</li><li>BiFPN + weghting achieves the best accuracy</li><li>Normalized：softmax和fast版本效果差不多，每个节点的weight在训练开始迅速变化（suggesting different features contribute to the feature fusion unequally）</li><li>Compound Scaling：这个比其他只提高一个指标的效果好就不用说了</li></ul></li></ul></li><li><p>超参：</p><p> efficientNet和efficientDet的resolution是不一样的，因为检测还有neck和head，层数更深，所以resolution更大</p><p> <img src="/2020/04/13/GoogLeNet系列/hyper.png" width="60%;"></p></li></ol><h2 id="EfficientNetV2-Smaller-Models-and-Faster-Training"><a href="#EfficientNetV2-Smaller-Models-and-Faster-Training" class="headerlink" title="EfficientNetV2: Smaller Models and Faster Training"></a>EfficientNetV2: Smaller Models and Faster Training</h2><ol><li><p>动机</p><ul><li>faster training speed and better parameter efficiency </li><li>use a new op: Fused-MBConv</li><li>propose progressive learning：adaptively adjuts regularization &amp; image size</li></ul></li><li><p>方法</p><ul><li><p>review of EfficientNet</p><ul><li><p>large image size </p><ul><li>large memory usage，small batch size，long training time</li><li>thus propose increasing image size gradually in V2</li></ul></li><li><p>extensive depthwise conv</p><ul><li>often cannot fully utilize modern accelerators</li><li><p>thus introduce Fused-MBConv in V2：When applied in early stage 1-3, Fused-MBConv can improve training speed with a small overhead on parameters and FLOPs </p><p><img src="/2020/04/13/GoogLeNet系列/Fused-MB.png" width="40%;"></p></li></ul></li><li><p>equally scaling up</p><ul><li>proved sub-optimal in nfnets</li><li>since the stages are not equally contributed to the efficiency &amp; accuracy </li><li>thus in V2<ul><li>use a non-uniform scaling strategy：gradually add more layers to later stages(s5 &amp; s6)</li><li>restrict the max image size</li></ul></li></ul></li></ul></li><li><p>EfficientNet V2 Architecture </p><ul><li><p>basic ConvBlock</p><ul><li>use fused-MBConv in the early layers</li><li>use MBConv in the latter layers</li></ul></li><li><p>expansion ratios</p><ul><li>use smaller expansion ratios</li><li>因为同样的通道数，fused-MB比MB的参数量大</li></ul></li><li><p>kernel size</p><ul><li>全图3x3，没有5x5了</li><li>add more layers to compensate the reduced receptive field </li></ul></li><li><p>last stride 1 stage</p><ul><li>effv1是7个stage</li><li><p>effv2有6个stage</p><p><img src="/2020/04/13/GoogLeNet系列/efficientB0.png" width="40%;">   <img src="/2020/04/13/GoogLeNet系列/effv2.png" width="40%;"></p></li></ul></li><li><p>scaling policy</p><ul><li>compound scaling：R、W、D一起scale</li><li>但是限制了最大inference image size=480（train=384）</li><li>gradually add more layers to later stages (s5 &amp; s6)</li></ul></li></ul></li><li><p>progressive learning</p><ul><li><p>large models require stronger regularization </p></li><li><p>larger image size leads to more computations with larger capacity，thus also needs stronger regularization </p></li><li><p>training process</p><ul><li>in the early training epochs, we train the network with smaller images and weak regularization</li><li><p>gradually increase image size but also making learning more difficult by adding stronger regularization </p><p><img src="/2020/04/13/GoogLeNet系列/progressive.png" width="50%;"></p></li></ul></li><li><p>adaptive params</p><ul><li>image size</li><li>dropout rate</li><li>randAug magnitude</li><li>mixup alpha</li><li><p>给定最大最小值，stage N，使用linear interpolation</p><p><img src="/2020/04/13/GoogLeNet系列/v2S.png" width="40%;"></p></li></ul></li></ul></li><li><p>train&amp;test details</p><ul><li>RMSProp optimizer with decay 0.9 and momentum 0.9 </li><li>batch norm momentum 0.99 </li><li>weight decay 1e-5 </li><li>trained for 350 epochs with total batch size 4096 </li><li>Learning rate is first warmed up from 0 to 0.256, and then decayed by 0.97 every 2.4 epochs </li><li>exponential moving average with 0.9999 decay rate </li><li><p>stochastic depth with 0.8 survival probability</p></li><li><p>4 stages (87 epochs per stage)：early stage with weak regularization &amp; later stronger</p></li><li>maximum image size for training is about 20% smaller than inference &amp; no further finetuning</li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> papers </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>FCN</title>
      <link href="/2020/03/28/FCN/"/>
      <url>/2020/03/28/FCN/</url>
      <content type="html"><![CDATA[<h2 id="FCN-Fully-Convolutional-Networks-for-Semantic-Segmentation"><a href="#FCN-Fully-Convolutional-Networks-for-Semantic-Segmentation" class="headerlink" title="FCN: Fully Convolutional Networks for Semantic Segmentation"></a>FCN: Fully Convolutional Networks for Semantic Segmentation</h2><ol><li><p>动机</p><ul><li>take input of arbitrary size</li><li>pixelwise prediction (semantic segmentation) </li><li>efficient inference and learning</li><li>end-to-end</li><li>with superwised-pretraining</li></ul></li><li><p>论点</p><ul><li>fully connected layers brings heavy computation</li><li>patchwise/proposals training with less efficiency (为了对一个像素分类，要扣它周围的patch，一张图的存储容量上升到k*k倍，而且相邻patch重叠的部分引入大量重复计算，同时<strong>感受野太小</strong>，没法有效利用全局信息)</li><li>fully convolutional structure are used to get a feature extractor which yield a localized, fixed-length feature </li><li>Semantic segmentation faces an inherent tension between<strong> semantics</strong> and <strong>location</strong>: global information resolves what while local information resolves where. Deep feature hierarchies jointly encode location and semantics in a local-to-global pyramid. </li><li>other semantic works (RCNN) are not end-to-end</li></ul></li><li><p>要素</p><ul><li>把全连接层换成<strong>1*1卷积</strong>，用于提取特征，形成热点图</li><li><strong>反卷积</strong>将小尺寸的热点图上采样到原尺寸的语义分割图像</li></ul><p><img src="/2020/03/28/FCN/fcn1.png" width="40%"></p><ul><li>a novel “skip” architecture to combine deep, coarse, semantic information and shallow, fine, appearance information</li></ul></li><li><p>方法</p><ul><li><p>fully convolutional network</p><ul><li><strong>receptive fields</strong>:  Locations in higher layers correspond to the locations in the image they are path-connected to</li><li>typical recognition nets: <ul><li>fixed-input</li><li>patches</li><li>the fully connected layers can be viewed as convolutions with kernels that cover their entire input regions</li></ul></li><li>our structure:<ul><li>arbitrary-input</li><li>the computation is saved by computing the overlapping regions of those patches only once</li><li>output size corresponds to the input(H/16, W/16)<ul><li>heatmap: the (H/16 * W/16) high-dims feature-map corresponds to the 1000 classes</li></ul></li></ul></li></ul></li></ul><p><img src="/2020/03/28/FCN/fcn.png" width="50%"></p><ul><li><p><a href="https://blog.csdn.net/HMH2_YY/article/details/80935394" target="_blank" rel="noopener">coarse predictions to dense</a></p><ul><li>OverFeat introduced</li><li>对于高维特征图上一个元素，对应了原图感受野一片区域，将reception field中c位填上这个元素的值</li><li>移动原图，相应的感受野对应的图片也发生了移动，高维特征图的输出变了，c位变了</li><li>移动范围stride*stride，就会得到原图尺寸的输出了</li></ul></li><li><p>upsampling</p><ul><li>simplest: bilinear interpolation</li><li><strong>in-network upsampling</strong>: backwards convolution (deconvolution) with an output stride of f</li><li>A stack of deconvolution layers and activation functions can even learn a <strong>nonlinear</strong> upsampling </li><li>factor: FCN里面inputsize和outputsize之间存在线性关系，就是所有卷积pooling层的累积采样步长乘积</li><li>kernelsize：$2 * factor - factor \% 2$</li><li>stride：$factor$</li><li>padding：$ceil((factor - 1) / 2.)$</li></ul><p><img src="/2020/03/28/FCN/deconv2.gif" width="30%"></p><p>这块的计算有点绕，$stride=factor$比较好确定，这是将特征图恢复的原图尺寸要rescale的尺度。然后在输入的相邻元素之间插入s-1个0元素，原图尺寸变为$(s-1)<em>(input_size-1)+input_size = s</em>input_size + (s-1)$，为了得到$output_size=s*input_size$输出，再至少$padding=[(s-1)/2]_{ceil}$，然后根据：</p><script type="math/tex; mode=display">(s-1) * (in-1) + in + 2p -k + 1 = out</script><p>有：</p><script type="math/tex; mode=display">2p-k+2 = s</script><p>在keras里面可以调用库函数Conv2DTranspose来实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">x = Input(shape=(<span class="number">64</span>,<span class="number">64</span>,<span class="number">16</span>))</span><br><span class="line">y = Conv2DTranspose(filters=<span class="number">16</span>, kernel_size=<span class="number">20</span>, strides=<span class="number">8</span>, padding=<span class="string">'same'</span>)(x)</span><br><span class="line">model = Model(x, y)</span><br><span class="line">model.summary()</span><br><span class="line"><span class="comment"># input: (None, 64, 64, 16)   output: (None, 512, 512, 16)   params: 102,416</span></span><br><span class="line"></span><br><span class="line">x = Input(shape=(<span class="number">32</span>,<span class="number">32</span>,<span class="number">16</span>))</span><br><span class="line">y = Conv2DTranspose(filters=<span class="number">16</span>, kernel_size=<span class="number">48</span>, strides=<span class="number">16</span>, padding=<span class="string">'same'</span>)(x)</span><br><span class="line"><span class="comment"># input: (None, 32, 32, 16)   output: (None, 512, 512, 16)   params: 589,840</span></span><br><span class="line"></span><br><span class="line">x = Input(shape=(<span class="number">16</span>,<span class="number">16</span>,<span class="number">16</span>))</span><br><span class="line">y = Conv2DTranspose(filters=<span class="number">16</span>, kernel_size=<span class="number">80</span>, strides=<span class="number">32</span>, padding=<span class="string">'same'</span>)(x)</span><br><span class="line"><span class="comment"># input: (None, 16, 16, 16)   output: (None, 512, 512, 16)   params: 1,638,416</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数参考：orig unet的total参数量为36,605,042</span></span><br><span class="line"><span class="comment"># 各级transpose的参数量为：</span></span><br><span class="line"><span class="comment"># (None, 16, 16, 512)     4,194,816</span></span><br><span class="line"><span class="comment"># (None, 32, 32, 512)     4,194,816</span></span><br><span class="line"><span class="comment"># (None, 64, 64, 256)     1,048,832</span></span><br><span class="line"><span class="comment"># (None, 128, 128, 128)   262,272</span></span><br><span class="line"><span class="comment"># (None, 256, 256, 32)    16,416</span></span><br></pre></td></tr></table></figure><p>可以看到kernel_size变大，对参数量的影响极大。（kernel_size设置的小了，只能提取到单个元素，我觉得kernel_size至少要大于stride）</p></li><li><p>Segmentation Architecture </p><ul><li>use pre-trained model</li><li>convert all fully connected layers to convolutions </li><li>append a 1*1 conv with channel dimension(including background)  to predict scores </li><li>followed by a deconvolution layer to upsample the coarse outputs to dense outputs</li></ul></li><li><p>skips</p><ul><li>the 32 pixel stride at the final prediction layer limits the scale of detail in the upsampled output</li><li>逐层upsampling，融合前几层的feature map，element-wise add</li></ul></li><li><p>finer layers: “As they see fewer pixels, the finer scale predictions should need fewer layers.” 这是针对前面的卷积网络来说，随着网络加深，特征图上的感受野变大，就需要更多的channel来记录更多的低级特征组合</p><ul><li>add a 1*1 conv on top of pool4 (zero-initialized)</li><li>adding a 2x upsampling layer on top of conv7 (We <strong>initialize this 2xupsampling to bilinear interpolation</strong>, but allow the parameters to be learned)</li><li><strong>sum</strong> the above two stride16 predictions (“Max fusion made learning difficult due to gradient switching”)</li><li>16x upsampled back to the image </li><li><p>做到第三行再往下，结果又会变差，所以做到这里就停下</p><p><img src="/2020/03/28/FCN/skip.png" width="90%"></p></li></ul></li></ul></li><li><p>总结</p><ul><li><p>在升采样过程中，分阶段增大比一步到位效果更好</p></li><li><p>在升采样的每个阶段，使用降采样对应层的特征进行辅助</p></li><li><p>8倍上采样虽然比32倍的效果好了很多，但是<strong>结果还是比较模糊</strong>和平滑，对图像中的细节不敏感，许多研究者采用MRF算法或CRF算法对FCN的输出结果做进一步优化</p></li><li><p>x8为啥好于x32：1. x32的特征图<strong>感受野过大</strong>，对小物体不敏感   2. x32的放大比例造成的失真更大</p></li><li><p><strong>unet的区别</strong>：</p><ul><li><p>unet没用imagenet的预训练模型，因为是医学图像</p></li><li><p>unet在进行浅层特征融合的时候用了concat而非element-wise add</p></li><li><p>逐层上采样，x2 vs. x8/x32</p></li><li><p>orig unet没用pad，输出小于输入，FCN则pad+crop</p></li><li><p>数据增强，FCN没用这些‘machinery’，医学图像需要强augmentation</p></li><li><p>加权loss</p></li></ul></li></ul></li></ol><p>​    </p>]]></content>
      
      
        <tags>
            
            <tag> papers </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>cs231n-RNN-review</title>
      <link href="/2020/03/15/cs231n-RNN-review/"/>
      <url>/2020/03/15/cs231n-RNN-review/</url>
      <content type="html"><![CDATA[]]></content>
      
      
    </entry>
    
    <entry>
      <title>attention系列</title>
      <link href="/2020/03/13/attention%E7%B3%BB%E5%88%97/"/>
      <url>/2020/03/13/attention%E7%B3%BB%E5%88%97/</url>
      <content type="html"><![CDATA[<h2 id="0-综述"><a href="#0-综述" class="headerlink" title="0. 综述"></a>0. 综述</h2><ol><li><p>attention的方式分为两种（<a href="https://blog.csdn.net/yideqianfenzhiyi/article/details/79422857?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">Reference</a>）</p><ul><li>学习权重分布<ul><li>部分加权（hard attention）／全部加权（soft attention）</li><li>原图上加权／特征图上加权</li><li>空间尺度加权／channel尺度加权／时间域加权／混合域加权</li><li>CAM系列、SE-block系列：花式加权，学习权重，non-local的模块，作用于某个维度</li></ul></li><li>任务分解<ul><li>设计不同的网络结构（或分支）专注于不同的子任务，</li><li>重新分配网络的学习能力，从而降低原始任务的难度，使网络更加容易训练</li><li>STN、deformable conv：添加显式的模块负责学习形变/receptive field的变化，local模块，apply by pixel</li></ul></li><li>local / non-local<ul><li>local模块的结果是pixel-specific的</li><li>non-local模块的结果是全局共同计算的的</li></ul></li></ul></li><li><p>基于权重的attention（<a href="https://blog.csdn.net/bigbug_sec/article/details/89025318" target="_blank" rel="noopener">Reference</a>）</p><ul><li>注意力机制通常由一个连接在原神经网络之后的额外的神经网络实现</li><li>整个模型仍然是端对端的，因此注意力模块能够和原模型一起同步训练</li><li>对于soft attention，注意力模块对其输入是可微的，所以整个模型仍可用梯度方法来优化</li><li>而hard attention要离散地选择其输入的一部分，这样整个系统对于输入不再是可微的</li></ul></li><li><p>papers</p><ul><li><p>[STN] <a href="https://amberzzzz.github.io/2021/02/03/transform-in-CNN/">Spatial Transformer Networks</a></p></li><li><p>[deformable conv] Deformable Convolutional Networks </p></li><li><p>[CBAM] CBAM: Convolutional Block Attention Module </p></li><li><p>[SE-Net] <a href="https://amberzzzz.github.io/2020/04/30/SE-block/">Squeeze-and-Excitation Networks</a></p></li><li><p>[SE-block的一系列变体] SC-SE（for segmentation）、CMPE-SE（复杂又没用）</p></li><li><p>[SK-Net] Selective Kernel Networks：是attension module，但是主要改进点在receptive field，trick大杂烩</p></li><li><p>[GC-Net] GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond </p></li></ul></li></ol><h2 id="CBAM-Convolutional-Block-Attention-Module"><a href="#CBAM-Convolutional-Block-Attention-Module" class="headerlink" title="CBAM: Convolutional Block Attention Module"></a>CBAM: Convolutional Block Attention Module</h2><ol><li>动机<ul><li>attention module </li><li>lightweight and general  </li><li>improvements in classification and detection </li></ul></li><li><p>论点</p><ul><li>deeper： can obtain richer representation </li><li>increased width：can outperform an extremely deep network</li><li>cardinality：results in stronger representation power than depth and width </li><li>attention：improves the representation of interests <ul><li>humans exploit a sequence of partial glimpses and selectively focus on salient parts </li><li>Residual Attention Network：computes 3d attention map</li><li>we decompose the process that learns channel attention and spatial attention separately </li><li>SE-block：use global average-pooled features</li><li>we suggest to use max-pooled features as well  </li></ul></li></ul></li><li><p>方法</p><ul><li>sequentially infers a <strong>1D channel attention map</strong> and a <strong>2D spatial attention map</strong>  </li><li><p>broadcast and element-wise multiplication </p><p><img src="/2020/03/13/attention系列/CBAM.png" width="45%;"></p></li><li><p>Channel attention module</p><ul><li>focuses on ‘what’ is meaningful  </li><li>squeeze the spatial dimension </li><li>use both average-pooled and max-pooled features simultaneously </li><li>both descriptors are then forwarded to a shared MLP to reduce dimension</li><li>【QUESTION】看论文MLP是线性的吗，没写激活函数</li><li>then use element-wise summation  </li><li>sigmoid function </li></ul></li><li>Spatial attention module <ul><li>focuses on ‘where’ </li><li>apply average-pooling and max-pooling  along the channel axis and concatenate </li><li>7x7 conv</li><li>sigmoid function </li></ul></li><li><p>Arrangement of attention modules</p><ul><li>in a parallel or sequential manner </li><li>we found sequential better than parallel </li><li>we found channel-first order slightly better than the spatial-first</li></ul><p><img src="/2020/03/13/attention系列/module.png" width="45%;"></p></li><li><p>integration</p><ul><li>apply CBAM on the convolution outputs in each block </li><li>in residual path</li><li><p>before the add operation</p><p><img src="/2020/03/13/attention系列/integration.png" width="55%;"></p></li></ul></li></ul></li><li><p>实验</p><ul><li><p>Ablation studies </p><ul><li>Channel attention：两个pooling path都有效，一起用最好</li><li>Spatial attention：1x1conv直接squeeze也行，avg+max更好，7x7conv略好于3x3conv</li><li>arrangement：前面说了，比SE的单spacial squeeze好，channel在前好于在后，串行好于并行</li></ul></li><li><p>Classification results：outperform baselines and SE</p></li><li>Network Visualization  <ul><li>cover the target object regions better </li><li>the target class scores also increase accordingly </li></ul></li><li>Object Detection results<ul><li>apply to detectors：right before every classifier </li><li>apply to backbone</li></ul></li></ul></li></ol><h2 id="SK-Net-Selective-Kernel-Networks"><a href="#SK-Net-Selective-Kernel-Networks" class="headerlink" title="SK-Net: Selective Kernel Networks"></a>SK-Net: Selective Kernel Networks</h2><ol><li><p>动机</p><ul><li>生物的神经元的感受野是随着刺激变化而变化的</li><li>propose a selective kernel unit<ul><li>adaptively adjust the RF</li><li>multiple branches with different kernel sizes</li><li>guided fusion</li><li>大杂烩：multi-branch&amp;kernel，group conv，dilated conv，attention mechanism</li></ul></li><li>SKNet<ul><li>by stacking multiple SK units</li><li>在分类任务上验证</li></ul></li></ul></li><li><p>论点</p><ul><li>multi-scale aggregation<ul><li>inception block就有了</li><li>but linear aggregation approach may be insufficient</li></ul></li><li>multi-branch network<ul><li>two-branch：以resnet为代表，主要是为了easier to train</li><li>multi-branch：以inception为代表，主要为了得到multifarious features </li></ul></li><li>grouped/depthwise/dilated conv<ul><li>grouped conv：reduce computation，提升精度</li><li>depthwise conv：reduce computation，牺牲精度</li><li>dilated conv：enlarge RF，比dense large kernel节省参数量</li></ul></li><li>attention mechanism<ul><li>加权系列：<ul><li>SENet&amp;CBAM：</li><li>相比之下SKNet多了adaptive RF</li></ul></li><li>动态卷积系列：<ul><li>STN不好训练，训好以后变换就定死了</li><li>deformable conv能够在inference的时候也动态的变化变换，但是没有multi-scale和nonlinear aggregation</li></ul></li></ul></li><li>thus we propose SK convolution <ul><li>multi-kernels：大size的conv kernel是用了dilated conv</li><li>nonlinear aggregation</li><li>computationally lightweight </li><li>could successfully embedded into small models</li><li>workflow<ul><li>split</li><li>fuse</li><li>select</li></ul></li><li>main difference from inception<ul><li>less customized</li><li>adaptive selection instead of equally addition</li></ul></li></ul></li></ul></li><li><p>方法</p><ul><li><p>selective kernel convolution</p><ul><li><p>split</p><ul><li>multi-branch with different kernel size</li><li>grouped/depthwise conv + BN + ReLU</li><li>5x5 kernel can be further replaced with dilated conv</li></ul></li><li><p>fuse</p><ul><li>to learn the control of information flow from different branches</li><li>element-wise summation</li><li>global average pooling</li><li>fc-BN-ReLU：reduce dimension，at least 32</li></ul></li><li><p>select</p><ul><li><p>channel-wise weighting factor A &amp; B &amp; more：A+B + more = 1</p></li><li><p>fc-softmax</p><p>  <img src="/2020/03/13/attention系列/softmax.png" width="50%;"></p></li><li><p>在2分支的情况下，一个权重矩阵A就够了，B是冗余的，因为可以间接算出来</p></li><li><p>reweighting</p></li></ul><p><img src="/2020/03/13/attention系列/sk.png" width="80%;"></p></li></ul></li><li><p>network</p><ul><li>start from resnext</li><li>repeated SK units：类似bottleneck<ul><li>1x1 conv</li><li>SK conv</li><li>1x1 conv</li><li>hyperparams<ul><li>number of branches M=2</li><li>group number G=32：cardinality of each path</li><li>reduction ratio r=16：fuse operator中dim-reduction的参数</li></ul></li></ul></li><li><p>嵌入到轻量的网络结构</p><ul><li>MobileNet/shuffleNet</li><li>把其中的3x3 depthwise卷积替换成SK conv</li></ul><p><img src="/2020/03/13/attention系列/skNet.png" width="90%;"></p></li></ul></li></ul></li><li><p>实验</p><ul><li><p>比sort的resnet、densenet、resnext精度都要好</p><p>  <img src="/2020/03/13/attention系列/error.png" width="40%;"></p></li></ul></li></ol><h2 id="GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond"><a href="#GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond" class="headerlink" title="GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond"></a>GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</h2><ol><li><p>动机</p><ul><li>Non-Local Network (NLNet)<ul><li>capture long-range dependencies</li><li>obtain query-specific global context</li><li>but we found global contexts are almost the same for different query positions</li></ul></li><li>we produce<ul><li>query-independent formulation</li><li>smiliar structure as SE-Net</li><li>aims at global context modeling</li></ul></li></ul></li><li><p>论点</p><ul><li><p>Capturing long-range dependency </p><ul><li><p>mainly by sdeeply stacking conv layers：inefficient </p></li><li><p>non-local network</p><ul><li>via self-attention mechanism </li><li>computes the pairwise relations between the query position then aggregate</li><li><p>但是不同位置query得到的attention map基本一致</p><p><img src="/2020/03/13/attention系列/NLNet.png" width="40%;"></p></li></ul></li><li><p>we simply the non-local block</p><ul><li>query-independent </li><li>maintain acc &amp; save computation</li></ul></li></ul></li><li><p>our proposed GC-block</p><ul><li>unifies both the NL block and the SE block</li><li>three steps<ul><li>global context modeling：</li><li>feature transform module：capture channel-wise interdependency</li><li>fusion module：merge into the original features</li></ul></li></ul></li><li><p>多种任务上均有涨点</p><ul><li>但都是在跟resnet50对比</li></ul></li></ul></li><li><p>revisit NLNet</p><ul><li><p>non-local block</p><ul><li><p><img src="/2020/03/13/attention系列/NL formulation.png" width="45%;"></p></li><li><p>$f(x_i, x_j)$：</p><ul><li>encodes the relationship between position i &amp; j</li><li>计算方式有Gaussian、Embedded Gaussian、Dot product、Concat</li><li>different instantiations achieve comparable performance</li></ul></li><li><p>$C(x)$：norm factor</p></li><li><p>$x_i + \sum^{N_p} F(x_j)$：aggregates a specific global feature on $x_i$</p></li><li><p>widely-used Embedded Gaussian：</p><p>  <img src="/2020/03/13/attention系列/NL block.png" width="50%;"></p></li><li><p>嵌入方式：</p><ul><li>Mask R-CNN with FPN and Res50 </li><li>only add one non-local block right before the last residual block of res4</li></ul></li><li><p>observations &amp; inspirations</p><ul><li>distances among inputs show that input features are discriminated</li><li>outputs &amp; attention maps are almost the same：global context after training is actually independent of query position</li><li>inspirations<ul><li>simplify the Non-local block</li><li>no need of query-specific</li></ul></li></ul></li></ul></li></ul></li><li><p>方法</p><ul><li><p>simplifying form of NL block：SNL</p><ul><li><p>求一个common的global feature，share给全图每个position</p><p><img src="/2020/03/13/attention系列/simplify1.png" width="45%;"></p></li><li><p>进一步简化：把$x_j$的1x1 conv提到前面，FLOPs大大减少，因为feature scale从HW变成了1x1</p><p>  <img src="/2020/03/13/attention系列/simplify2.png" width="45%;"></p></li><li><p>the SNL block achieves comparable performance to the NL block with significantly lower FLOPs</p></li></ul></li><li><p>global context modeling</p><ul><li>SNL可以抽象成三部分：<ul><li>global attention pooling：通过$W_k$ &amp; softmax获取attention weights，然后进行global pooling</li><li>feature transform：1x1 conv</li><li>feature aggregation：broadcast element-wise add</li></ul></li><li><p>SE-block也可以分解成类似的抽象</p><ul><li>global attention pooling：用了简单的global average pooling</li><li>feature transform：用了squeeze &amp; excite的fc-relu-fc-sigmoid</li><li>feature aggregation：broadcast element-wise multiplication </li></ul><p><img src="/2020/03/13/attention系列/gc-block.png" width="80%;"></p></li></ul></li><li><p>Global Context Block</p><ul><li>integrate the benefits of both<ul><li>SNL global attention pooling：effective modeling on long-range dependency</li><li>SE bottleneck transform：light computation（只要ratio大于2就会节省参数量和计算量）</li></ul></li><li>特别地，在SE transform的squeeze layer上，又加了BN<ul><li>ease optimization </li><li>benefit generalization </li></ul></li><li>fusion：add</li><li>嵌入方式：<ul><li>GC-ResNet50</li><li>add GC-block to all layers (c3+c4+c5) in resnet50 with se ratio of 16</li></ul></li></ul></li><li><p>relationship to SE-block</p><ul><li>首先是fusion method reflects different goals<ul><li>SE基于全局信息rescales the channels，间接使用</li><li>GC直接使用，将long-range dependency加在每个position上</li></ul></li><li>其次是norm layer<ul><li>ease optimization</li></ul></li><li>最后是global attention pooling<ul><li>SE的GAP是a special case</li><li>weighting factors shows superior</li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 注意力机制 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Deeplab系列</title>
      <link href="/2020/02/24/Deeplab%E7%B3%BB%E5%88%97/"/>
      <url>/2020/02/24/Deeplab%E7%B3%BB%E5%88%97/</url>
      <content type="html"><![CDATA[<h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><ol><li><p>papers</p><ul><li>deeplabV1: SEMANTIC IMAGE SEGMENTATION WITH DEEP CONVOLUTIONAL NETS AND FULLY CONNECTED CRFS，主要贡献提出了空洞卷积，使得feature extraction阶段输出的特征图维持较高的resolution</li><li>deeplabV2: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs，主要贡献是多尺度ASPP结构</li></ul></li></ol><ul><li>deeplabV3: Rethinking Atrous Convolution for Semantic Image Segmentation，提出了基于ResNet的串行&amp;并行两种结构，细节上提到了multi-grid，改进了ASPP模块<ul><li>deeplabV3+: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation </li></ul></li><li>Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation </li></ul><ol><li><p>分割结果比较粗糙的原因</p><ul><li>池化：将全图抽象化，降低分辨率，会丢失细节信息，平移不变性，使得边界信息不清晰</li><li>没有利用标签之间的概率关系：CNN缺少对空间、边缘信息等约束</li></ul><p>对此，deeplabV1引入了</p><ul><li><a href="https://www.jianshu.com/p/f743bd9041b3" target="_blank" rel="noopener">空洞卷积</a>：VGG中提出的多个小卷积核代替大卷积核的方法，只能使感受野线性增长，而多个空洞卷积串联，可以实现指数增长。</li><li>全连接条件随机场CRF：作为stage2，提高模型捕获细节的能力，提升边界分割精度</li></ul></li><li><p>大小物体同时分割</p><p>deeplabV2引入</p><ul><li>多尺度ASPP(Atrous Spatial Pyramid Pooling)：并行的采用多个采样率的空洞卷积提取特征，再进行特征融合</li><li>backbone model change：VGG16改为ResNet</li><li>使用不同的学习率</li></ul></li><li><p>进一步改进模型架构</p><p>deeplabV3引入</p><ul><li>ASPP嵌入ResNet后几个block</li><li>去掉了CRF</li></ul></li><li><p>使用原始的Conv/pool操作，得到的low resolution score map，pool stride会使得过程中丢弃一部分信息，上采样会得到较大的失真图像，使用空洞卷积，保留特征图上的全部信息，同时keep resolution，减少了信息损失</p></li><li><p>DeeplabV3的ASPP相比较于V2，增加了一条1x1 conv path和一条image pooling path，加GAP这条path是因为，实验中发现，随着rate的增大，有效的weight数目开始减少（<strong>部分超出边界无法有效捕捉远距离信息</strong>），因此利用global average pooling提取了image-level的特征并与ASPP的特征并在一起，来补充因为dilation丢失的信息</p><p>空洞卷积的path，V2是每条path分别空洞卷积然后接两个1x1conv（没有BN），V3是空洞卷积和BatchNormalization组合</p><p>fusion方式，V2是sum fusion，V3是所有path concat然后1x1 conv，得到最终score map</p></li><li><p>DeeplabV3的串行版本，“In order to maintain original image size, convolutions are replaced with strous convolutions with rates that differ from each other with factor 2”，ppt上说后面几个block复制了block4，每个block里面三层conv，其中最后一层conv stride2，然后为了maintain output size，空洞rate*2，这个不太理解。</p><p>multi-grid method：对每个block里面的三层卷积采用不同空洞率，unit rate（e.g.(1,2,4)） * rate （e.g. 2）</p></li></ol><h2 id="deeplabV1-SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CONVOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS"><a href="#deeplabV1-SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CONVOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS" class="headerlink" title="deeplabV1: SEMANTIC IMAGE SEGMENTATION WITH DEEP CONVOLUTIONAL NETS AND FULLY CONNECTED CRFS"></a>deeplabV1: SEMANTIC IMAGE SEGMENTATION WITH DEEP CONVOLUTIONAL NETS AND FULLY CONNECTED CRFS</h2><ol><li><p>动机</p><ul><li>brings together methods from Deep Convolutional Neural Networks and probabilistic graphical models </li><li>poor localization property of deep networks</li><li>combine a fully connected Conditional Random Field (CRF) </li><li>be able to localize segment boundaries beyond previous accuracies</li><li>speed: atrous</li><li>accuracy: </li><li>simplicity: cascade modules</li></ul></li><li><p>论点</p><ul><li>DCNN learns hierarchical abstractions of data, which is desirable for high-level vision tasks (classification)</li><li>but it hampers low-level tasks,  such as pose estimation and semantic segmentation, where we want precise localization, rather than <strong>abstraction of spatial details</strong></li><li>two technical hurdles in DCNNs when applying to image labeling tasks<ul><li>pooling, loss of resolution: we employ the ‘atrous’ (with holes) for efficient dense computation </li><li>spacial invariance: we use the fully connected pairwise CRF to capture fine edge details </li></ul></li><li><p>Our approach </p><ul><li>treats every pixel as a CRF node</li><li>exploits long-range dependencies</li><li>and uses CRF inference to directly optimize a DCNN-driven cost function </li></ul><p><img src="/2020/02/24/Deeplab系列/deeplabV1.png" width="65%"></p></li></ul></li><li><p>方法</p><ul><li><p>structure</p><ul><li>fully convolutional VGG-16</li><li>keep the first 3 subsampling blocks for a target stride of 8 </li><li>use hole algorithm conv filters for the last two blocks</li><li>keep the pooling layers for the purpose of fine-tuing，change strides from 2 to 1</li><li>for dense map(h/8), the first fully convolutional 7*7*4096 is computational, thus change to 4*4 / 3*3 convs</li><li>further computation decreasement: reduce the fc channels from 4096 to 1024</li></ul></li><li><p>train</p><ul><li>label：ground truth subsampled by 8 </li><li>loss function：cross-entropy </li></ul></li><li><p>test</p><ul><li>x8：simply bilinear interpolation </li><li>fcn：stride32 forces them to use learned upsampling layers, significantly increasing the complexity and training time</li></ul></li><li><p>CRF</p><ul><li><p>short-range：used to smooth noisy </p></li><li><p>fully connected model：to recover detailed local structure rather than further smooth it   </p></li><li><p>energy function:</p><script type="math/tex; mode=display">  E(x) = \sum_{i}\theta_i(x_i) + \sum_{ij}\theta_{ij}(x_i, x_j)\\  \theta_i(x_i) = -logP(x_i)\\</script><p>  $P(x_i)$ is the bi-linear interpolated probability output of DCNN.</p><script type="math/tex; mode=display">  \theta_{ij}(x_i, x_j) = \mu(x_i, x_j)\sum_{m=1}^K \omega_m k^m (f_i,f_j)\\  \mu(x_i, x_j) = \begin{cases}  1& \text{if }x_i \neq x_j\\  0& \text{otherwise}  \end{cases}</script><p>  $k^m(f_i, f_j)$ is the Gaussian kernel depends on features (involving pixel positions &amp; pixel color intensities)</p></li></ul></li><li><p>multi-scale prediction</p><ul><li>to increase the boundary localization accuracy </li><li>we attach to the input image and the output of each of the first four max pooling layers a two-layer MLP (first layer: 128 3x3 convolutional filters, second layer: 128 1x1 convolutional filters)  </li><li>the feature maps above is <strong>concatenated</strong> to the main network’s last layer feature map </li><li>the new outputs is enhanced by 128*5=640 channels</li><li>we only adjust the newly added weights</li><li>introducing these extra direct connections from fine-resolution layers improves localization performance, <strong>yet the effect is not as dramatic as the one obtained with the fully-connected CRF</strong></li></ul></li></ul></li><li><p>空洞卷积dilated convolution</p><p> <img src="/2020/02/24/Deeplab系列/holes.png" width="65%"></p><ul><li><p>空洞卷积相比较于正常卷积，多了一个 hyper-parameter——dilation rate，指的是kernel的间隔数量(正常的convolution dilatation rate是1)</p></li><li><p>fcn：先pooling再upsampling，过程中有信息损失，能不能设计一种新的操作，不通过pooling也能<strong>有较大的感受野</strong>看到更多的信息呢？</p></li><li><p>如图(b)的2-dilated conv，kernel size只有3x3，但是这个卷积的感受野已经增大到了7x7（假设前一层是3x3的1-dilated conv）</p></li><li><p>如图(c)的4-dilated conv，kernel size只有3x3，但是这个卷积的感受野已经增大到了15x15（假设前两层是3x3的1-dilated conv和3x3的2-dilated conv）</p></li><li><p>而传统的三个3x3的1-dilated conv堆叠，只能达到7x7的感受野</p></li><li><p>dilated使得在不做pooling损失信息的情况下，加大了感受野，让每个卷积输出都包含较大范围的信息</p></li><li><p><strong>The Gridding Effect</strong>：如下图，多次叠加3x3的2-dilated conv，会发现我们将愿输入离散化了。因此叠加卷积的 dilation rate 不能有大于1的公约数。</p><p>  <img src="/2020/02/24/Deeplab系列/grid.png" width="60%"></p></li><li><p><strong>Long-ranged information</strong>：增大dilation rate对大物体有效果，对小物体可能有弊无利</p></li><li><p>HDC(Hybrid Dilated Convolution)设计结构</p><ul><li><p>叠加卷积的 dilation rate 不能有大于1的公约数，如[2,4,6]</p></li><li><p>将 dilation rate 设计成锯齿状结构，例如 [1, 2, 5, 1, 2, 5] 循环结构，锯齿状能够同时满足小物体大物体的分割要求(小 dilation rate 来关心近距离信息，大 dilation rate 来关心远距离信息)</p></li><li><p>满足$M_i = max [M_{i+1}-2r_i, M_{i+1}-2(M_{i+1}-r_i), r_i]$，$M_i$是第i层最大dilation rate</p></li><li><p>一个可行方案[1,2,5]：</p><p>  <img src="/2020/02/24/Deeplab系列/hdc.png" width="65%"></p></li></ul></li></ul></li></ol><h2 id="deeplabV2-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs"><a href="#deeplabV2-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs" class="headerlink" title="deeplabV2: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs"></a>deeplabV2: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</h2><ol><li><p>动机</p><ul><li>atrous convolution：control the resolution </li><li>atrous spatial pyramid pooling (ASPP) ：multiple sampling rates </li><li>fully connected Conditional Random Field (CRF) </li></ul></li><li><p>论点</p><ul><li><p>three challenges in the application of DCNNs to semantic image segmentation </p><ul><li>reduced feature resolution：max-pooling and downsampling (‘striding’)  —&gt; atrous convolution</li><li>existence of objects at multiple scales：multi input scale —&gt; ASPP</li><li>reduced localization accuracy due to DCNN invariance：skip-layers —&gt; CRF</li></ul><p><img src="/2020/02/24/Deeplab系列/deeplabV2.png" width="70%"></p></li><li><p>improvements compared to its first version </p><ul><li>better segment objects at multiple scales</li><li>ResNet replaces VGG16</li><li>a more comprehensive experimental evaluation on models &amp; dataset</li></ul></li><li>related works<ul><li>jointly learning of the DCNN and CRF to form an end-to-end trainable feed-forward network </li><li>while in our work still a 2 stage process</li><li>use a series of atrous convolutional layers with increasing rates to aggregate multiscale context </li><li>while in our structure using parallel instead of serial </li></ul></li></ul></li><li><p>方法</p><ul><li><p>atrous convolution</p><ul><li>在下采样以后的特征图上，运行普通卷积，相当于在原图上运行上采样的filter<ul><li>1-D示意图上可以看出，两者感受野相同</li><li>同时能保持high resolution</li></ul></li><li><p>while both the number of filter parameters and the number of operations per position stay constant</p><p><img src="/2020/02/24/Deeplab系列/1d.png" width="40%">   <img src="/2020/02/24/Deeplab系列/2d.png" width="40%">  </p></li><li><p>把backbone中下采样的层(pooling/conv)中的stride改成1，然后将接下来的conv层都改成2-dilated conv：could allow us to compute feature responses at the original image resolution</p></li><li>efficiency/accuracy trade-off：using atrous convolution to increase the resolution by a factor of 4</li><li>followed by fast bilinear interpolation by a factor of 8 to the original image resolution </li><li><p>Bilinear interpolation is sufficient in this setting because the class score maps are quite smooth <strong>unlike FCN</strong></p><p><img src="/2020/02/24/Deeplab系列/scoremap.png" width="40%"></p></li><li><p>Atrous convolution offers easily control of the field-of-view and finds the best trade-off between accurate localization (small field-of-view) and context assimilation (large field-of-view)：大感受野，抽象融合上下文，大感受野，low-level局部信息准确</p></li><li>实现：（1）根据定义，给filter上采样，插0；（2）给feature map下采样得到k*k个reduced resolution maps，然后run orgin conv，组合位移结果</li></ul></li><li><p>ASPP</p><ul><li><p>multi input scale：</p><ul><li>run parallel DCNN branches that share the same parameters</li><li>fuse by taking at each position the maximum response across scales</li><li>computing</li></ul></li><li><p>spatial pyramid pooling</p><ul><li>run multiple parallel filters with different rates</li><li>multi-scale features are further processed in separate branches：fc7&amp;fc8</li><li><p>fuse：sum fusion</p><p><img src="/2020/02/24/Deeplab系列/aspp.png" width="55%"></p></li></ul></li></ul></li><li><p>CRF：keep the same as V1</p></li></ul></li></ol><h2 id="deeplabV3-Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation"><a href="#deeplabV3-Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation" class="headerlink" title="deeplabV3: Rethinking Atrous Convolution for Semantic Image Segmentation"></a>deeplabV3: Rethinking Atrous Convolution for Semantic Image Segmentation</h2><ol><li><p>动机</p><ul><li>for segmenting objects at multiple scales<ul><li>employ atrous convolution in cascade or in parallel with multiple atrous rates</li><li>augment ASPP with image-level features encoding global context and further boost performance</li></ul></li><li>without DenseCRF</li></ul></li><li><p>论点</p><ul><li>our proposed module consists of atrous convolution with various rates and <strong>batch normalization layers</strong></li><li>modules in cascade or in parallel：when applying a 3*3  atrous convolution with an extremely large rate, it fails to capture long range information due to image boundary effects</li></ul></li><li><p>方法</p><ul><li><p>Atrous Convolution</p><p>  for each location $i$ on the output $y$ and a filter $w$,  an $r$-rate atrous convolution is applied over the input feature map $x$：</p><script type="math/tex; mode=display">  y[i] = \sum_k x[i+rk]w[k]</script></li><li><p>in cascade</p><ul><li>duplicate several copies of the last ResNet block (block4)</li><li><p>extra block5, block6, block7 as replicas of block4 </p></li><li><p>multi-rates</p><p><img src="/2020/02/24/Deeplab系列/cascade.png" width="70%"></p></li></ul></li><li><p>ASPP</p><ul><li><p>we include batch normalization within ASPP</p></li><li><p>as the sampling rate becomes larger, the number of valid filter weights becomes smaller (beyond boundary)</p></li><li><p>to incorporate global context information：we adopt image-level features by GAP on the last feature map of the model </p><p>  GAP —&gt; 1*1*256 conv —&gt; BN —&gt; bilinearly upsample </p></li><li><p>fusion: concatenated + 1*1 conv</p><p><img src="/2020/02/24/Deeplab系列/aspp+gap.png" width="70%"></p></li><li><p>seg：final 1*1*n_classes conv</p></li></ul></li><li><p>training details</p><ul><li>large crop size required to make sure the large atrous rates effective </li><li>upsample the output: it is important to keep the groundtruths intact and instead upsample the final logits</li></ul></li></ul></li><li><p>结论</p><ul><li><p>output stride=8 好过16，但是运算速度慢了几倍</p><p><img src="/2020/02/24/Deeplab系列/result.png" width="70%"></p></li></ul></li></ol><h2 id="deeplabV3-Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation"><a href="#deeplabV3-Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation" class="headerlink" title="deeplabV3+: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation"></a>deeplabV3+: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</h2><ol><li><p>动机</p><ul><li>spatial pyramid pooling module captures rich contextual information  </li><li>encode-decoder structure captures sharp object boundaries </li><li>combine the above two methods</li><li>propose a simple yet effective decoder module </li><li>explore Xception backbone</li></ul></li><li><p>论点</p><ul><li>even though rich semantic information is encoded through ASPP, detailed information related to object boundaries is missing due to striding operations </li><li>atrous convolution could alleviate but suffer the computational balance</li><li>while encoder-decoder models lend themselves to faster computation (since no features are dilated) in the encoder path and gradually recover sharp object boundaries in the decoder path </li><li><p>所谓encoder-decoder structure，就是通过encoder和decoder之间的短连接来将不同尺度的特征集成起来，增加这样的shortcut，同时增大网络的下采样率（encoder path上不使用空洞卷积，因此为了达到同样的感受野，得增加pooling，然后保留最底端的ASPP block），既减少了计算，又enrich了local border这种细节特征</p><p><img src="/2020/02/24/Deeplab系列/deeplabV3+.png" width="50%"></p></li><li><p>applying the atrous separable convolution to both the ASPP and decoder modules：最后又引入可分离卷积，进一步提升计算效率</p></li></ul></li><li><p>方法</p><ul><li><p>atrous separable convolution </p><ul><li><p>significantly reduces the computation complexity while maintaining similar (or better) performance</p><p><img src="/2020/02/24/Deeplab系列/atrousDW.png" width="50%"></p></li></ul></li><li><p>DeepLabv3 as encoder</p><ul><li>output_stride=16/8：remove the striding of the last 1/2 blocks</li><li>atrous convolution：apply atrous convolution to the blocks without striding</li><li>ASPP：run 1x1 conv in the end to set the output channel to 256</li></ul></li><li><p>proposed decoder</p><ul><li>naive decoder：bilinearly upsampled by 16 </li><li>proposed：first bilinearly upsampled by 4, then concatenated with the corresponding low-level features</li><li>low-level features：<ul><li>apply 1x1 conv on the low-level features to reduce the number of channels to avoid <strong>outweigh the importance</strong></li><li>the last feature map in res2x residual block before striding </li></ul></li><li>combined features：apply 3x3 conv(2 layers, 256 channels) to obtain sharper segmentation results </li><li>more shortcut：observed no significant improvement</li></ul><p><img src="/2020/02/24/Deeplab系列/en-de.png" width="45%"></p></li><li><p>modified Xception backbone</p><ul><li>deeper</li><li>all the max pooling operations are replaced with depthwise separable convolutions with striding </li><li><p>DWconv-BN-ReLU-PWconv-BN-ReLU</p><p><img src="/2020/02/24/Deeplab系列/xception.png" width="50%"></p></li></ul></li></ul></li><li><p>实验</p><ol><li><p>decoder effect on border</p><p> <img src="/2020/02/24/Deeplab系列/border.png" width="50%"></p></li></ol></li><li><p>f</p></li></ol><h2 id="Auto-DeepLab-Hierarchical-Neural-Architecture-Search-for-Semantic-Image-Segmentation"><a href="#Auto-DeepLab-Hierarchical-Neural-Architecture-Search-for-Semantic-Image-Segmentation" class="headerlink" title="Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation"></a>Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation</h2>]]></content>
      
      
        <tags>
            
            <tag> papers </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>RCNN系列</title>
      <link href="/2020/01/08/RCNN%E7%B3%BB%E5%88%97/"/>
      <url>/2020/01/08/RCNN%E7%B3%BB%E5%88%97/</url>
      <content type="html"><![CDATA[<h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><ol><li><p>papers</p><p> [R-CNN] R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation </p><p> [] SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</p><p> [] Fast R-CNN: Fast Region-based Convolutional Network</p><p> [] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</p><p> [] Mask R-CNN  </p><p> [] FPN: Feature Pyramid Networks for Object Detection</p><p> [] RFCN: </p></li></ol><h2 id="R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation"><a href="#R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation" class="headerlink" title="R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation"></a>R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation</h2><ol><li><p>动机</p><ul><li>localizing objects with a deep network and training a high-capacity model with only a small quantity of annotated detection data<ul><li>apply CNN to region proposals: R-CNN represents ‘Regions with CNN features’</li><li>supervised pre-training </li></ul></li></ul></li><li><p>论点</p><ul><li>model as a regression problem: not fare well in practice  </li><li>build a sliding-window detector: have to maintain high spatial resolution</li><li><strong>what we do: </strong>our method gener- ates around 2000 category-independent region proposals for the input image, extracts a fixed-length feature vector from each proposal using a CNN, and then classifies each region with category-specific linear SVMs </li><li>conventional solution to training a large CNN is ‘using unsupervised pre-training, followed by supervised fine-tuning’</li><li><strong>what we do: </strong>‘supervised pre-training on a large auxiliary dataset (ILSVRC), followed by domain specific fine-tuning on a small dataset (PASCAL)’</li><li><strong>we also demonstrate: </strong>a simple bounding box regression method significantly reduces mislocalizations</li><li><strong>R-CNN operates on regions:</strong> it is natural to extend it to the task of semantic segmentation </li></ul></li><li><p>要素</p><ul><li>category-independent region proposals </li><li>a large convolutional neural network that extracts a fixed-length feature vector from each region </li><li><p>a set of class-specific linear SVMs</p><p><img src="/2020/01/08/RCNN系列/R-CNN.png" width="40%;"></p></li></ul></li><li><p>方法</p><ul><li><p>Region proposals: we use selective search</p></li><li><p>Feature extraction: we use Krizhevsky CNN, 227*227 RGB input, 5 convs, 2 fcs, 4096 output</p><ul><li>we first dilate the tight bounding box (padding=16)</li><li><p>then warp the bounding box to the required size (各向异性缩放)</p><p><img src="/2020/01/08/RCNN系列/warp.png" width="40%;"></p></li></ul></li><li><p>Test-time detection:</p><ul><li>we score each extracted feature vector using the SVM trained for each class</li><li>we apply a greedy non-maximum suppression (for each class independently)  </li><li>对留下的这些框进行canny边缘检测，就可以得到bounding-box</li><li>(then B-BoxRegression)</li></ul></li><li><p>Supervised pre-training: pre-trained the CNN on a large auxiliary dataset (ILSVRC 2012) with <strong>image-level annotations</strong></p></li><li><p>Domain-specific fine-tuning: </p><ul><li>continue SGD training of the CNN using only warped region proposals from VOC </li><li>replace the 1000-way classification layer with a randomly initialized 21-way layer (20 VOC classes plus background)</li><li><strong>class label: all region proposals with ≥ 0.5 IoU overlap with a ground-truth box as positives, else negatives </strong></li><li>1/10th of the initial pre-training rate</li><li>uniformly sample 32 positive windows (over all classes) and 96 background windows to construct a mini-batch of size 128 </li></ul></li><li><p>Object category classifiers:</p><ul><li>considering a binary classifier for a specific class</li><li><strong>class label: take IoU overlap threshold &lt;0.3 as negatives, take only regions tightly enclosing the object as positives </strong> </li><li>take the ground-truth bounding boxes for each class as positives</li></ul></li><li><p><strong>unexplained:</strong></p><ul><li><p>the positive and negative examples are defined differently in CNN fine-tuning versus SVM training</p><p>  CNN容易过拟合，需要大量的训练数据，所以在CNN训练阶段我们对Bounding box的位置限制条件限制的比较松(IOU只要大于0.5都被标注为正样本)，svm适用于少样本训练，所以对于训练样本数据的IOU要求比较严格，我们只有当bounding box把整个物体都包含进去了，我们才把它标注为物体类别。</p></li><li><p>it’s necessary to train detection classifiers rather than simply use outputs of the fine-tuned CNN</p><p>  上一个回答其实同时也解释了CNN的head已经是一个分类器了，还要用SVM分类：按照上述正负样本定义，CNN softmax的输出比采用svm精度低。</p></li></ul></li></ul></li><li><p>分析</p><ul><li><p>learned features:</p><ul><li>compute the units’ activations on a large set of held-out region proposals  </li><li>sort from the highest to low</li><li>perform non-maximum suppression</li><li><p>display the top-scoring regions</p><p><img src="/2020/01/08/RCNN系列/activations.png" width="70%;"></p></li></ul></li><li><p>Ablation studies:</p><ul><li><strong>without fine-tuning:</strong> features from fc7 generalize worse than features from fc6, indicating that most of the CNN’s representational power comes from its convolutional layers</li><li><strong>with fine-tuning: </strong>The boost from fine-tuning is much larger for fc6 and fc7 than for pool5, suggests that pool features learned from ImageNet are <strong>general</strong> and that most of the improvement is gained from learning <strong>domain-specific</strong> non-linear classifiers on top of them</li></ul></li><li><p>Detection error analysis:</p><ul><li>more of our errors result from poor localization rather than confusion </li><li>CNN features are much more discriminative than HOG </li><li>Loose localization likely results from our use of bottom-up region proposals and the positional invariance learned from pre-training the CNN for whole-image classification(粗暴的IOU判定前背景，二值化label，无法体现定位好坏差异)</li></ul></li><li><p>Bounding box regression：</p><ul><li>a linear regression model use the pool5 features for a selective search region proposal as input</li><li>输出为xy方向的缩放和平移</li><li>训练样本：判定为本类的候选框中和真值重叠面积大于0.6的候选框</li></ul></li><li><p>Semantic segmentation：</p><ul><li>three strategies for computing features:<ul><li>‘<em>full</em> ‘ ignores the region’s shape, two regions with different shape might have very similar bounding boxes(信息不充分)</li><li>‘<em>fg</em> ‘ slightly outperforms <em>full</em>, indicating that the masked region shape provides a stronger signal</li><li>‘<em>full+fg</em> ‘ achieves the best, indicating that the context provided by the <em>full</em> features is highly informative even given the <em>fg</em> features(形状和context信息都重要)</li></ul></li></ul></li></ul></li></ol><h2 id="SPP-net-Spatial-Pyramid-Pooling-in-Deep-Convolutional-Networks-for-Visual-Recognition"><a href="#SPP-net-Spatial-Pyramid-Pooling-in-Deep-Convolutional-Networks-for-Visual-Recognition" class="headerlink" title="SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition"></a>SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</h2><ol><li><p>动机：</p><ul><li>propose a new pooling strategy, “spatial pyramid pooling”</li><li>can generate a fixed-length representation regardless of image size/scale</li><li>also robust to object deformations </li></ul></li><li><p>论点：</p><ul><li>existing CNNs require a fixed-size input<ul><li>reduce accuracy for sub-images of an arbitrary size/scale (need cropping/warping)</li><li>cropped region lost content, while warped content generates unwanted distortion</li><li>overlooks the issues involving scales </li></ul></li><li>convolutional layers do not require a fixed image size, whle the fully-connected layers need to have fixed- size/length input by their definition</li><li>by introducing the SPP layer<ul><li>between the last convolutional layer and the first fully-connected layer</li><li>pools the features and generates fixed- length outputs</li></ul></li><li>Spatial pyramid pooling <ul><li>partitions the image into divisions from finer to coarser levels, and aggregates local features in them</li><li>generates fixed- length output </li><li>uses multi-level spatial bins(robust to object deformations )</li><li>can run at variable scales </li><li>also allows varying sizes or scales <strong>during training</strong>: <ul><li>train the network with different input size at different epoch</li><li>increases scale-invariance </li><li>reduces over-fitting </li></ul></li><li>in object detection <ul><li>run the convolutional layers only <em>once</em> on the entire image </li><li>then extract features by SPP-net on the feature maps </li><li>speedup </li><li>accuracy </li></ul></li></ul></li></ul><ol><li><p>方法：</p><ul><li><p>Convolutional Layers and Feature Maps</p><ul><li>the outputs of the convolutional layers are known as feature maps</li><li><p>feature maps involve not only the strength of the responses(the strength of activation), but also their spatial positions(the reception field)</p><p><img src="/2020/01/08/RCNN系列/feature maps.png" width="80%;"></p></li></ul></li><li><p>The Spatial Pyramid Pooling Layer</p><ul><li>it can maintain spatial information by pooling in local spatial bins</li><li>the spatial bins have sizes proportional to the image size(k-level: 1*1, 2*2, …, k*k)</li><li>we can resize the input image to any scale, which is important for the accuracy  </li><li><p>the coarsest pyramid level has a single bin that covers the entire image, which is in fact a “global pooling” operation </p><p><img src="/2020/01/08/RCNN系列/spp.png" width="40%;"></p></li><li><p>for a feature map of $a×a$, with a pyramid level of $n×n$ bins:</p><script type="math/tex; mode=display">  the\ window\ size:\ win = ceiling(a/n)\\  the\ stride:\ str = floor(a/n)</script></li></ul></li></ul></li></ol></li></ol><pre><code>    * Training the Network        * Single-size training: fixed-size input (224×224) cropped from images, cropping for data augmentation        * Multi-size training: rather than cropping, we resize the aforementioned 224×224 region to 180×180, then we train two fixed-size networks that share parameters by altenate epoch4. 分析    * **50 bins vs. 30 bins: **the gain of multi-level pooling is not simply due to more parameters, it is because the multi-level pooling is robust to the variance in object deformations and spatial layout    * **multi-size vs. single-size:  **multi results are more or less better than the single-size version    * **full vs. crop: **shows the importance of maintaining the complete content5. **SPP-NET FOR OBJECT DETECTION**    * We extract the feature maps from the entire image only once     * we apply the spatial pyramid pooling on each candidate window of the feature maps      * These representations are provided to the fully-connected layers of the network     * SVM samples: We use the ground-truth windows to generate the positive samples, use the samples with IOU&lt;30% as the negative samples     * multi-scale feature extraction:         * We resize the image at {480, 576, 688, 864, 1200}, and compute the feature maps of conv5 for each scale.         * we choose a single scale s ∈ S such that the scaled candidate window has a number of pixels closest to 224×224.         * And we use the corresponding feature map to compute the feature for this window        * this is roughly equivalent to resizing the window to 224×224     * fine-tuning:        * Since our features are pooled from the conv5 feature maps from windows of any sizes        * for simplicity we only fine-tune the fully-connected layers     * Mapping a Window to Feature Maps**        * we project the corner point of a window onto a pixel in the feature maps, such that this corner point in the image domain is closest to the center of the receptive field of that feature map pixel.          &lt;img src=&quot;RCNN系列/mapping.png&quot; width=&quot;50%;&quot; /&gt;        ​    确定原图上的两个角点（左上角和右下角），映射到 feature map上的两个对应点，使得映射点$(x^{&#39;}, y^{&#39;})$在原始图上**感受野（上图绿色框）的中心点**与$(x,y)$尽可能接近。</code></pre><h2 id="Fast-R-CNN-Fast-Region-based-Convolutional-Network"><a href="#Fast-R-CNN-Fast-Region-based-Convolutional-Network" class="headerlink" title="Fast R-CNN: Fast Region-based Convolutional Network"></a>Fast R-CNN: Fast Region-based Convolutional Network</h2><ol><li><p>动机</p><ul><li>improve training and testing speed</li><li>increase detection accuracy</li></ul></li><li><p>论点</p><ul><li>current approaches train models in multi-stage pipelines that are slow and inelegant<ul><li>R-CNN &amp; SPPnet: CNN+SVM+bounding-box regression</li><li>disk storage: features are written to disk </li><li>SPPnet: can only fine-tuning the fc layers, limits the accuracy of very deep networks</li></ul></li><li>task complexity:<ul><li>numerous candidate proposals</li><li>rough localization proposals must be refined </li></ul></li><li>We propose:<ul><li>a single-stage training algorithm </li><li>multi-task: jointly learns to classify object proposals and refine their spatial locations </li></ul></li></ul></li><li><p>要素</p><ul><li>input: an entire image and a set of object proposals </li><li>convs</li><li>a region of interest (RoI) pooling layer: extracts a fixed-length feature vector from the feature map</li><li>fcs that finally branch into two sibling output layers</li><li><p>multi-outputs:</p><ul><li>one produces softmax probability over K+1 classes</li><li>one outputs four bounding-box regression offsets per class</li></ul><p><img src="/2020/01/08/RCNN系列/fastRCNN.png" width="40%;"></p></li></ul></li><li><p>方法</p><ul><li><p>RoI pooling</p><ul><li>an RoI is a rectangular window inside a conv feature map, which can be defined by (r, c, h, w)  </li><li>the RoI pooling layer converts the features inside any valid RoI into a small feature map with a fixed size H × W </li><li>it is a special case of SPPnet when there is only one pyramid level (pooling window size = h/H * w/W)</li></ul></li><li><p>Initializing from pre-trained networks</p><ul><li>the last max pooling layer is replaced by a RoI pooling layer </li><li>the last fully connected layer and softmax is replaced by the wo sibling layers + respective head (softmax &amp; regressor)</li><li>modified to take two inputs</li></ul></li><li><p>Fine-tuning for detection</p><ul><li><p>why SPPnet is unable to update weights below the spatial pyramid pooling layer: </p><ul><li><p>原文提到feature vector来源于不同尺寸的图像——不是主要原因</p></li><li><p>feature vector在原图上的感受野通常很大（接近全图）——forward pass的计算量就很大</p></li><li><p>不同的图片forward pass的计算结果不能复用（when each training sample (<em>i.e</em>. RoI) comes from a different image, which is exactly how R-CNN and SPPnet networks are trained）</p></li></ul></li><li><p>We propose:</p><ul><li><p>takes advantage of feature sharing </p></li><li><p>mini-batches are sampled hierarchically: N images and R/N RoIs from each image</p></li><li><p>RoIs from the same image share computation and memory in the forward and backward passes </p></li><li><p>jointly optimize the two tasks</p><p>  each RoI is labeled with a ground-truth class $u$ and a ground-truth bounding-box regression target $v$ </p><p>  the network outputs are K+1 probability $p=(p_0,…p_k)$ and K b-box regression offsets $t^k=(t_x^k, t_y^k, t_w^k,t_h^k)$</p><script type="math/tex; mode=display">  L(p, u, t^u, v) = L_{cls}(p,u) + \lambda[u>0]L_{loc}(t^u,v)\\</script><p>  $L_{cls}$:</p><script type="math/tex; mode=display">  L_{cls}(p,u) = -log p_u\\</script><p>  $L_{loc}$:</p><script type="math/tex; mode=display">  L_{loc}(t^u, v) = \sum_{i \in \{x,y,w,h\}}smooth_{L_1}(t^u_i - v_i)\\  smooth_{L_1}(x) =   \begin{cases}  0.5x^2\ \ \ \ \ \ \ \ \ \ \ if |x|<1\\  |x| - 0.5\ \ \ \ \ \ otherwise  \end{cases}</script><p>  作者表示这种形式可以增强模型对异常数据的鲁棒性</p><p>  <img src="/2020/01/08/RCNN系列/smoothL1.png" width="30%;"></p></li><li><p>class label: take $IoU\geq0.5$ as a foreground object, take negatives with $IoU \in [0.1,0.5)$</p><p>  The lower threshold of 0.1 appears to act as a heuristic for hard example mining </p></li><li></li></ul></li></ul></li><li><p>Truncated SVD for faster detection</p><ul><li>Large fully connected layers are easily accelerated by compressing them with truncated SVD <script type="math/tex; mode=display">  W \approx U \Sigma_t V^T</script></li></ul></li></ul></li></ol><ul><li><p>the single fully connected layer corresponding to W is replaced by two fully connected layers, <strong>without non-linearity</strong></p></li><li><p>The first layers uses the weight matrix $\Sigma_t V^T$(and no biases)</p></li><li><p>the second uses U (with the original biases)</p></li></ul><ol><li><p>分析</p><ul><li>Fast R-CNN vs. SPPnet: even though Fast R-CNN uses single-scale training and testing, <strong>fine-tuning the conv layers</strong> provides a large improvement in mAP </li><li>Truncated SVD can reduce detection time by more than 30% with only a small (0.3 percent- age point) drop in mAP </li><li>deep vs. small networks: <ul><li>for very deep networks fine-tuning the conv layers is important </li><li>in the smaller networks (S and M) we find that conv1 is generic and task independent </li><li>all Fast R-CNN results in this paper using models L fine-tune layers conv3_1 and up</li><li>all experiments with models S and M fine-tune layers conv2 and up </li></ul></li><li>multi-task training vs. stage-wise: it has the potential to improve results because the tasks influence each other through a shared representation (the ConvNet) </li><li>single-scale vs. multi-scale: <ul><li>single-scale detection performs almost as well as multi-scale detection </li><li>deep ConvNets are adept at directly learning scale invariance </li><li>single-scale processing offers the best tradeoff be- tween speed and accuracy thus we choose single-scale</li></ul></li><li>softmax vs. SVM:<ul><li>“one-shot” fine-tuning is sufficient compared to previous multi-stage training approaches</li><li>softmax introduces competition, while SVMs are one-vs-rest </li></ul></li></ul></li></ol><h2 id="Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks"><a href="#Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks" class="headerlink" title="Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"></a>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</h2><ol><li><p>动机</p><ul><li>shares the convolutional features</li><li>merge the system using the concept of “attention” mechanisms </li><li>sharing convolutions across proposals —-&gt; across tasks</li><li>translation-Invariant &amp; scale/ratio-Invariant</li></ul></li><li><p>论点</p><ul><li>proposals are now the test-time computational bottleneck in state-of-the-art detection systems</li><li>the region proposal methods are generally implemented on the CPU</li><li>we observe that the convolutional feature maps used by region-based detectors, like Fast R- CNN, can also be used for generating region proposals</li></ul></li><li><p>要素</p><ul><li>RPN: On top of the convolutional features, we construct an RPN by adding a few additional convolutional layers that simultaneously regress region bounds and objectness scores at each location <strong>on a regular grid</strong></li><li>anchor: serves as references at multiple scales and aspect ratios </li><li><p>unify RPN and Fast R-CNN detector: we propose a training scheme that alternately fine-tuning the region proposal task and the object detection task</p><p><img src="/2020/01/08/RCNN系列/fasterRCNN.png" width="35%;"></p></li></ul></li><li><p>方法</p><p> 4.1 Region Proposal Networks</p><p> <img src="/2020/01/08/RCNN系列/RPN.png" width="65%;"></p><ul><li><p>This architecture is naturally implemented with an n×n convolutional layer followed by two sibling 1 × 1 convolutional layers (for <em>reg</em> and <em>cls</em>, respectively)</p></li><li><p>conv: an n × n sliding window </p></li><li><p>feature: 256-d for ZF(5 convs backbone) and 512-d for VGG(13 convs backbone)</p></li><li><p>two sibling fully-connected layers + respective output layer</p></li><li><p>anchors</p><ul><li>predict multiple region proposals: denoted as k</li><li>the reg head has 4k outputs, the cls head has 2k outputs</li><li>the k proposals are parameterized relative to <strong>k reference boxes</strong>————the anchors</li><li>an anchor box is centered at the sliding window in question, and is associated with a scale and aspect ratio </li><li>for a convolutional feature map of a size W × H , that is WHk anchors in total</li></ul></li><li><p>class label</p><ul><li>positives1: the anchors with the highest IoU with a ground-truth box </li><li>positives2: the anchors that has an IoU higher than 0.7 with any ground-truth box </li><li>negatives: non-positive anchors if their IoU is lower than 0.3 <strong>for all</strong> ground-truth boxes</li><li>the left: do not contribute </li><li>ignored: all cross-boundary anchors </li></ul></li><li><p>Loss function</p><ul><li><p>similar multi-task loss as fast-RCNN, with a normalization term</p></li><li><p>with $x,y,w,h$ denoting the box’s <strong>center coordinates</strong> and its width and height, the regression branch outputs $t_i$:</p><script type="math/tex; mode=display">  t_x = (x - x_a) / w_a\\  t_y = (y - y_a) / h_a\\  t_w = log(w/ w_a)\\  t_h = log(h/ h_a)</script></li></ul></li><li><p>mini-batch: sampled the positive and negative anchors from a single image with the ratio of 1:1 </p><p>4.2 the unified network </p></li><li><p>Alternating training</p><ul><li>ImageNet-pre-trained model, fine-tuning end-to-end for the region proposal task </li><li>ImageNet-pre-trained model, using the RPN proposals, fine-tuning end-to-end for the detection task </li><li>fixed detection network convs, fine-tuning the unique layers for region proposal </li><li>fixed detection network convs, fine-tuning the unique layers for detection</li></ul></li><li><p>Approximate joint training</p><ul><li>multi-task loss</li><li>approximate </li></ul><p>4.3 at training time</p></li><li><p>the total stride is 16 (input size / feature map size)</p></li><li>for a typical 1000 × 600 image, there will be roughly 20000 (60*40*9) anchors in total </li><li><p>we ignore all cross-boundary anchors, there will be about 6000 anchors per image left for training</p><p>4.4 at testing time</p></li><li><p>we use NMS(iou_thresh=0.7), that leaves 2000 proposals per image</p></li><li>then we use the top-N ranked proposal regions for detection</li></ul></li><li><p>分析</p></li></ol><h2 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h2><ol><li><p>动机</p><ul><li><strong>instance segmentation: </strong><ul><li>detects objects while simultaneously generating instance mask</li><li>注意不仅仅是目标检测了</li></ul></li><li>easy to generalize to other tasks: <ul><li>instance segmentation</li><li>bounding-box object detection</li><li>person keypoint detection</li></ul></li></ul></li><li><p>论点</p><ul><li>challenging:<ul><li>requires the correct detection of objects</li><li>requires precisely segmentation of instances</li></ul></li><li><strong>a simple, flexible, and fast system can surpass all</strong><ul><li>adding a branch for predicting segmentation on Faster-RCNN</li><li>in parallel with the existing branch for classification and regression </li><li>the mask branch is a small FCN applied to each RoI</li></ul></li><li>Faster R- CNN was not designed for pixel-to-pixel alignment  </li><li><strong>we propose RoIAlign to preserve exact spatial locations</strong></li><li>FCNs usually perform per-pixel multi-class categorization, which couples segmentation and classification</li><li><p><strong>we predict a binary mask for each class independently, decouple mask(mask branch) and class(cls branch)</strong></p></li><li><p>other combining methods are multi-stage</p></li><li><strong>our method is based on parallel prediction</strong>  </li><li>FCIS also run the system in parallel but exhibits systematic errors on overlapping instances and creates spurious edges </li><li>segmentation-first strategies attempt to cut the pixels of the same category into different instances </li><li><strong>Mask R-CNN is based on an instance-first strategy</strong></li></ul></li><li><p>要素</p><ul><li>a mask branch with $Km^2$-dims outputs for each RoI, m denotes the resolution, K denotes the number of classes</li><li>bce is key for good instance segmentation results:  $L_{mask} = [y&gt;0]\frac{1}{m^2}\sum bce_loss$</li><li>RoI features that are well aligned to the per-pixel input </li></ul></li><li><p>方法</p><ul><li><p>RoIAlign</p><ul><li>Quantizations in RoIPool: (1) RoI to feature map $[x/16]$; (2) feature map to spatial bins $[a/b]$; $[]$ denotes roundings</li><li>These quantizations introduce misalignments </li><li><p>We use bilinear interpolation to avoid quantization</p><ul><li>sample several points in the spatial bins</li><li>computes the value of each sampling point by bilinear interpolation from the nearby grid points on the feature map</li><li>aggregate the results of sampling points (using max or average) </li></ul><p><img src="/2020/01/08/RCNN系列/RoIAlign.png" width="50%;"></p></li></ul></li><li><p>Architecture </p><ul><li>backbone: using a ResNet-FPN backbone for feature extraction gives excellent gains in both accuracy and speed </li><li><p>head: use previous heads in ResNet/FPN(res5 contained in head/backbone)</p><p><img src="/2020/01/08/RCNN系列/mrcnnheads.png" width="45%;"></p></li></ul></li><li><p>Implementation Details </p><ul><li>positives: RoIs with IoU at least 0.5, otherwise negative</li><li>loss: dice loss defined only on positive RoIs</li><li>mini-batch: 2 images, N RoIs</li><li>at training time: parallel computation  for 3 branches</li><li>at test time: <ul><li>serial computation</li><li>proposals -&gt; box prediction -&gt; NMS -&gt; run mask branch on the highest scoring 100 detection boxes  </li><li>it speeds up inference and improves accuracy </li><li>the $28*28$ floating-number mask output is resized to the RoI size, and binarized at a threshold of 0.5</li></ul></li></ul></li></ul></li><li><p>分析</p><ul><li>on overlapping instances: FCIS+++ exhibits systematic artifacts </li><li>architecture: it benefits from deeper networks (50 vs. 101) and advanced designs including FPN and ResNeXt</li><li>FCN vs. MLP for mask branch</li><li><strong>Human Pose Estimation</strong><ul><li>We model a keypoint’s location as a <strong>one-hot mask</strong>, and adopt Mask R-CNN to predict K masks, one for each of K keypoint types </li><li>the training target is a one-hot $m<em>m$ binary mask where only a </em>single* pixel is labeled as foreground </li><li>use the cross-entropy loss </li><li>We found that a relatively high resolution output ($56*56$ compared to masks) is required for keypoint-level localization accuracy</li></ul></li></ul></li></ol><h2 id="FPN-Feature-Pyramid-Networks-for-Object-Detection"><a href="#FPN-Feature-Pyramid-Networks-for-Object-Detection" class="headerlink" title="FPN: Feature Pyramid Networks for Object Detection"></a>FPN: Feature Pyramid Networks for Object Detection</h2><ol><li><p>动机</p><ul><li>for object detection in multi-scale</li><li>struct feature pyramids with marginal extra cost</li><li>practical and accurate</li><li>leverage the pyramidal shape of a ConvNet’s feature hierarchy while creating a feature pyramid that has strong semantics at all scales</li></ul></li><li><p>论点</p><ul><li>single scale offers a good trade-off between accuracy and speed while multi-scale still performs better, especially for small objects</li><li>featurized image pyramids form the basis solution for multi-scale</li><li>ConvNets are proved robust to variance in scale and thus facilitate recognition from features computed on a single input scale </li><li>SSD uses the naturely feature hierarchy generated by ConvNet which introduces large semantic gaps caused by different depths<ul><li>high-level features are low-resolution but <strong>semantically strong</strong></li><li>low-level features are of lower-level semantics, but their activations are more <strong>accurately localized</strong> as subsampled fewer times</li></ul></li><li><p>thus we propose FPN:</p><ul><li>combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections </li><li>has rich semantics at all levels </li><li>built from a single scale</li><li>can be easily extended to mask proposals</li><li>can be trained end-to- end with all scales </li></ul><p><img src="/2020/01/08/RCNN系列/pyramids.png" width="40%;"></p></li><li><p>similar architectures make predictions only on a fine resolution  </p><p><img src="/2020/01/08/RCNN系列/similar structs.png" width="40%;"></p></li></ul></li><li><p>要素</p><ul><li>takes a single-scale image of an arbitrary size as input </li><li>outputs proportionally sized feature maps at multiple levels</li><li>structure <ul><li>a bottom-up pathway:  the feed-forward computation of the backbone ConvNet</li><li>a top-down pathway and lateral connection: <ul><li>upsampling the spatially coarser, but semantically stronger, feature maps from higher pyramid levels</li><li>then enhance with features from the bottom-up pathway via lateral connections</li><li>a $3<em>3$ conv is appended on each merged map <em>*to reduce the aliasing effect of upsampling</em></em></li><li>shared classifiers/regressors among all levels, thus using fixed 256 channels convs</li><li>upsamling uses nearest neighbor interpolation</li><li>low-level features undergoes a $1*1$ conv to reduce channel dimensions </li><li>merge operation is a <strong>by element-wise addition</strong></li></ul></li></ul></li><li>adopt the method in RPN &amp; Fast-RCNN for demonstration</li></ul></li><li><p>方法</p><ul><li><p>RPN</p><ul><li>original design: <ul><li>backbone Convs -&gt; single-scale feature map -&gt; dense 3×3 sliding windows -&gt; head($3<em>3$ convs + 2 sibling $1</em>1$ conv branches)</li><li>for regressor: multi-scale anchors(e.g. 3 scales 3 ratios -&gt; 9 anchors)</li></ul></li><li>new design: <ul><li>adapt FPN -&gt; multi-scale feature map -&gt; sharing heads</li><li>for regressor: set single-scale anchor for each level respectively (e.g. 5 level 3 ratios -&gt; 15 anchors)</li></ul></li><li>sharing heads:<ul><li>vs. not sharing: similar accuracy </li><li>indicates all levels of FPN share similar semantic levels (contrasted with naturally feature hierarchy of CNNs)</li></ul></li></ul></li><li><p>Fast  R-CNN</p><ul><li><p>original design: take the ROI feature map from the output of last conv layer</p></li><li><p>new design: take the specific level of ROI feature map based on ROI area</p><ul><li><p>with a $w*h$ ROI on the input image, $k_0$ refers to the target level on which an RoI with $w×h=224^2$ should be mapped into </p><script type="math/tex; mode=display">k = [k_0 + log_2 (\sqrt{wh}/224)]</script></li><li><p>the smaller the ROI area, the lower the level k, the finer the resolution of the feature map</p></li></ul></li></ul></li></ul></li><li><p>分析</p><ul><li>RPN<ul><li>use or not FPN: boost on small objects</li><li>use or not top-down pathway: semantic gaps</li><li>use or not lateral connection: locations </li><li>use or not multi-levels feature maps: <ul><li>using P2 alone leads to more anchors </li><li>more anchors are not sufficient to improve accuracy</li></ul></li></ul></li><li>Fast  R-CNN<ul><li>using P2 alone is <strong>marginally worse</strong> than that of using all pyramid levels </li><li>we argue that this is because <strong>RoI pooling is a warping-like operation</strong>, which is less sensitive to the region’s scales</li></ul></li><li><p>Faster R-CNN</p><ul><li>sharing features improves accuracy by a small margin</li><li>but reduces the testing time</li></ul></li><li><p><strong>Segmentation Proposals</strong></p><ul><li>use a fully convolutional setup for both training and inference </li><li><p>apply a small 5×5 MLP to predict 14×14 masks </p><p><img src="/2020/01/08/RCNN系列/FPNseg.png" width="40%;"></p></li></ul></li></ul></li></ol><p><strong>衍生应用：<lung nodules="" detection="" and="" segmentation="" using="" 3d="" mask-rcnn=""></lung></strong></p><ol><li>动机<ul><li>3D volume detection and segmentation </li><li>ROI ／ full scan</li><li>LUNA16：lung nodules size evaluation </li></ul></li><li>论点<ul><li>variety among nodules &amp; similarity among non-nodules</li></ul></li><li>方法<ul><li>use overlapping sliding windows </li><li>use focal loss improve class result</li><li>use IOU loss improve mask result</li><li>use heavy augmentation</li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 目标检测，two-stage </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CNN Visualization系列</title>
      <link href="/2020/01/03/CNN-Visualization%E7%B3%BB%E5%88%97/"/>
      <url>/2020/01/03/CNN-Visualization%E7%B3%BB%E5%88%97/</url>
      <content type="html"><![CDATA[<h3 id="1-Visualizing-and-Understanding-Convolutional-Networks"><a href="#1-Visualizing-and-Understanding-Convolutional-Networks" class="headerlink" title="1. Visualizing and Understanding Convolutional Networks"></a>1. Visualizing and Understanding Convolutional Networks</h3><ol><li><p>动机</p><ul><li>give insight into the internal operation and behavior of the complex models</li><li>then one can design better models</li><li>reveal which parts of the scene in image are important for classification</li><li>explore the generalization ability of the model to other datasets </li></ul></li><li><p>论点</p><ul><li><p>most visualizing methods limited to the 1st layer where projections to pixel space are possible </p></li><li><p>Our approach propose a method that could projects high level feature maps to the pixel space</p></li></ul></li></ol><pre><code>* some methods give some insight into invariances basing on a simple quadratic approximation * Our approach, by contrast, provides a non-parametric view of invariance * some methods associate patches that responsible for strong activations at higher layers* In our approach they are not just crops of input images, but rather top-down projections that reveal structures  </code></pre><ol><li><p>方法</p><p> 3.1 Deconvnet: use deconvnet to project the feature activations back to the input pixel space </p><ul><li>To examine a given convnet activation, we <strong>set all other activations in the layer to zero</strong> and pass the feature maps as input to the attached deconvnet layer</li><li>Then successively (i) unpool, (ii) rectify and (iii) filter to reconstruct the activity of the layer beneath until the input pixel space is reached</li><li>【Unpooling】using switches</li><li>【Rectification】the convnet uses relu to ensure always positive, same for back projection</li><li>【Filtering】transposed conv</li><li><p>Due to unpooling, the reconstruction obtained from a single activation resembles a small piece of the original input image</p><p><img src="/2020/01/03/CNN-Visualization系列/deconv.png" width="55%;"></p><p>3.2 CNN model</p><p><img src="/2020/01/03/CNN-Visualization系列/CNN.png" width="70%;"></p><p>3.3 visualization among layers</p></li><li><p>for each layer, we take the top9 strongest activation across the validation data</p></li><li>calculate the back projection separately</li><li><p>alongside we provide the corresponding image patches</p><p>3.4 visualization during training</p></li><li><p>randomly choose several strongest activation of a given feature map</p></li><li><p>lower layers converge fast, higher layers conversely</p><p>3.5 visualizing the Feature Invariance</p></li><li><p>5 sample images being translated, rotated and scaled by varying degrees</p></li><li>Small transformations have a dramatic effect in the first layer of the model(c2 &amp; c3对比)</li><li><p>the network is stable to translations and scalings, but not invariant to rotation </p><p>3.6 architecture selection</p></li><li><p>old architecture(stride4, filterSize11)：The first layer filters are a mix of extremely high and low frequency information, with little coverage of the mid frequencies.  The 2nd layer visualization shows aliasing artifacts caused by the large stride 4 used in the 1st layer convolutions. (这点可以参考之前vnet中提到的，deconv导致的棋盘格伪影，大stride会更明显)</p></li><li><p>smaller stride &amp; smaller filter(stride2, filterSize7)：more coverage of mid frequencies, no aliasing, no dead feature</p><p>3.7</p></li><li><p>对于物体的关键部分遮挡之后会极大的影响分类结果</p></li><li>第二个和第三个例子中分别是文字和人脸的响应更高，但是却不是关键部分。</li></ul></li><li><p>理解</p><p> 4.1 总的来说，网络学习到的特征，是具有辨别性的特征，通过可视化就可以看到我们提取到的特征忽视了背景，而是把关键的信息给提取出来了。从layer 1、layer 2学习到的特征基本上是颜色、边缘等低层特征；layer 3则开始稍微变得复杂，学习到的是纹理特征，比如上面的一些网格纹理；layer 4学习到的则是较多的类别信息，比如狗头；layer 5对应着更强的不变性，可以包含物体的整体信息。。</p><p> 4.2 在网络迭代的过程中，特征图出现了sudden jumps。低层在训练的过程中基本没啥变化，比较容易收敛，高层的特征学习则变化很大。这解释了低层网络的从训练开始，基本上没有太大的变化，因为梯度弥散。高层网络刚开始几次的迭代，变化不是很大，但是到了40~50的迭代的时候，变化很大，因此我们以后在训练网络的时候，不要着急看结果，看结果需要保证网络收敛。</p><p> 4.3 图像的平移、缩放、旋转，可以看出第一层中对于图像变化非常敏感，第7层就接近于线性变化。</p></li></ol><h3 id="2-Striving-for-Simplicity-The-All-Convolutional-Net"><a href="#2-Striving-for-Simplicity-The-All-Convolutional-Net" class="headerlink" title="2. Striving for Simplicity: The All Convolutional Net"></a>2. Striving for Simplicity: The All Convolutional Net</h3><ol><li><p>动机</p><ul><li>traditional pipeline: alternating <strong>convolution</strong> and <strong>max-pooling</strong> layers followed by a small number of <strong>fully connected layers</strong></li><li>questioning the necessity of different components in the pipeline, <strong>max-pooling</strong> layer to be specified</li><li>to analyze the network we introduce a new variant of the “deconvolution approach” for visualizing features</li></ul></li><li><p>论点</p><ul><li>two major improving directions based on traditional pipeline<ul><li>using more complex activation functions</li><li>building multiple conv modules</li></ul></li><li>we study the most simple architecture we could conceive<ul><li>a homogeneous network solely consisting of convolutional layers </li><li>without the need for complicated activation functions, any response normalization or max-pooling</li><li>reaches state of the art performance </li></ul></li></ul></li><li><p>方法</p><ul><li><p>replace the pooling layers with standard convolutional layers with stride two</p><ul><li>the spatial dimensionality reduction performed by pooling makes covering larger parts of the input in higher layers possible </li><li>which is crucial for achieving good performance with CNNs </li></ul></li><li><p>make use of small convolutional layers </p><ul><li>greatly reduce the number of parameters in a network and thus serve as a form of regularization</li><li>if the topmost convolutional layer covers a portion of the image large enough to recognize its content then fully connected layers can also be replaced by simple 1-by-1 convolutions</li></ul></li><li><p>the overall architecture consists only of convolutional layers with rectified linear non-linearities and an averaging + softmax layer to produce predictions  </p><p>  <img src="/2020/01/03/CNN-Visualization系列/basemodel.png" width="60%;"></p><p>  <img src="/2020/01/03/CNN-Visualization系列/modifiedmodel.png" width="60%;"></p><ul><li>Strided-CNN-C: pooling is removed and the preceded conv stride is increase</li><li>ConvPool-CNN-C: a dense conv is placed, to show the effect of increasing parameters</li><li>All-CNN-C: max-pooling is replaced by conv</li><li>when pooling is replaced by an additional convolution layer with stride 2, performance stabilizes and even improves  </li><li>small 3 × 3 convolutions stacked after each other seem to be enough to achieve the best performance </li></ul></li><li><p>guided backpropagation</p><ul><li>the paper above proposed ‘deconvnet’, which we observe that it does not always work well without max-pooling layers </li><li><strong>For higher layers</strong> of our network the method of Zeiler and Fergus fails to produce <strong>sharp, recognizable image structure</strong></li><li>Our architecture does not include max-pooling, thus we can ’deconvolve’ <strong>without switches</strong>, i.e. not conditioning on an input image</li><li><p>In order to obtain a reconstruction conditioned on an input image from our network without pooling layers we to combine the simple backward pass and the deconvnet</p><p><img src="/2020/01/03/CNN-Visualization系列/backwardpass.png" width="70%;"></p></li><li><p>Interestingly, the very first layer of the network does not learn the usual Gabor filters, but higher layers do</p><p><img src="/2020/01/03/CNN-Visualization系列/gabor.png" width="60%;"></p></li></ul></li></ul></li></ol><h3 id="3-Cam-Learning-Deep-Features-for-Discriminative-Localization"><a href="#3-Cam-Learning-Deep-Features-for-Discriminative-Localization" class="headerlink" title="3. Cam: Learning Deep Features for Discriminative Localization"></a>3. Cam: Learning Deep Features for Discriminative Localization</h3><ol><li><p>动机</p><ul><li>we found that CNNs actually behave as object detectors despite no supervision on the location </li><li>this ability is lost when fully-connected layers are used for classification </li><li>we found that the advantages of global average pooling layers are beyond simply acting as a regularizer</li><li>it makes it easily to localize the discriminative image regions despite not being trained for them</li></ul></li><li><p>论点</p><p> 2.1 Weakly-supervised object localization</p><ul><li>previous methods are not trained end-to-end and require multiple forward passes</li><li><p>Our approach is trained end-to-end and can localize objects in a single forward pass</p><p>2.2 Visualizing CNNs </p></li><li><p>previous methods only analyze the convolutional layers, ignoring the fully connected thereby painting an incomplete picture of the full story</p></li><li>we are able to understand our network from the beginning to the end</li></ul></li><li><p>方法</p><p> 3.1 Class Activation Mapping</p><ul><li>A class activation map for a particular category indicates the discriminative image regions used by the network to identify that category  </li><li>the network architecture: convs—-gap—-fc+softmax</li><li>we can identify the importance of the image regions by projecting back the weights of the output layer on to the convolutional feature maps </li><li><p>by simply upsampling the class activation map to the size of the input image we can identify the image regions most relevant to the particular category</p><p><img src="/2020/01/03/CNN-Visualization系列/cam.png" width="55%;"></p><p>3.2 Weakly-supervised Object Localization </p></li><li><p>our technique does not adversely impact the classification performance when learning to localize  </p></li><li>we found that the localization ability of the networks improved when the last convolutional layer before GAP had a higher spatial resolution, thus we removed several convolutional layers from the origin networks</li><li>overall we find that the classification performance is largely preserved for our GAP networks compared with the origin fc structure</li><li>our CAM approach significantly outperforms the backpropagation approach on generating bounding box </li><li><p>low mapping resolution prevents the network from obtaining accurate localizations</p><p>3.3 Visualizing Class-Specific Units </p></li><li><p>the convolutional units of various layers of CNNs act as visual concept detec- tors, identifying low-level concepts like textures or mate- rials, to high-level concepts like objects or scenes </p></li><li>Deeper into the network, the units become increasingly discriminative</li><li>given the fully-connected layers in many networks, it can be difficult to identify the importance of different units for identifying different categories </li></ul></li></ol><h3 id="4-Grad-CAM-Visual-Explanations-from-Deep-Networks-via-Gradient-based-Localization"><a href="#4-Grad-CAM-Visual-Explanations-from-Deep-Networks-via-Gradient-based-Localization" class="headerlink" title="4. Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization"></a>4. Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization</h3><h3 id="5-Grad-CAM-Improved-Visual-Explanations-for-Deep-Convolutional-Networks"><a href="#5-Grad-CAM-Improved-Visual-Explanations-for-Deep-Convolutional-Networks" class="headerlink" title="5. Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks"></a>5. Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks</h3><h2 id="6-综述"><a href="#6-综述" class="headerlink" title="6. 综述"></a>6. 综述</h2><ol><li><p>GAP</p><p> 首先回顾一下GAP，NiN中提出了GAP，主要为了解决全连接层参数过多，不易训练且容易过拟合等问题。</p><p> 对大多数分类任务来说不会因为做了gap让特征变少而让模型性能下降。因为GAP层是一个非线性操作层，这C个特征相当于是从kxkxC经过非线性变化选择出来的强特征。</p></li><li><p>heatmap</p><p> step1. 图像经过卷积网络后最后得到的特征图，在全连接层分类的权重（$w_{k,n}$）肯定不同，</p><p> step2. 利用反向传播求出每张特征图的权重，</p><p> step3. 用每张特征图乘以权重得到带权重的特征图，在第三维求均值，<strong>relu激活</strong>，归一化处理</p><ul><li>relu只保留wx大于0的值——我们正响应是对当前类别有用的特征，负响应会拉低$\sum wx$，即会降低当前类别的置信度</li><li><p>如果没有relu，定位图谱显示的不仅仅是某一类的特征。而是所有类别的特征。</p><p>step4. 将特征图resize到原图尺寸，便于叠加显示</p></li></ul></li><li><p>CAM</p><p> CAM要求必须使用GAP层，</p><p> CAM选择softmax层值最大的节点反向传播，<strong>求GAP层的梯度</strong>作为特征图的权重，每个GAP的节点对应一张特征图。</p></li><li><p>Grad-CAM</p><p> Grad-CAM不需要限制模型结构，</p><p> Grad-CAM选择softmax层值最大的节点反向传播，对<strong>最后一层卷积层</strong>求梯度，用每张特征图的梯度的均值作为该特征图的权重。</p></li></ol>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>NiN: network in network</title>
      <link href="/2019/12/25/NiN-network-in-network/"/>
      <url>/2019/12/25/NiN-network-in-network/</url>
      <content type="html"><![CDATA[<h3 id="Network-In-Network"><a href="#Network-In-Network" class="headerlink" title="Network In Network"></a>Network In Network</h3><ol><li><p>动机</p><ul><li>enhance model discriminability(获得更好的特征描述)：propose mlpconv</li><li>less prone to overfitting：propose global average pooling</li></ul></li><li><p>论点</p><p> <strong>comparison 1:</strong></p><ul><li>conventional CNN uses linear filter, which implicitly makes the assumption that the latent concepts are linearly separable. </li><li>traditional CNN is stacking [linear filters+nonlinear activation/linear+maxpooling+nonlinear]：这里引出了一个激活函数和池化层先后顺序的问题，对于avg_poolling，两种操作得到的结果是不一样的，先接激活函数会丢失部分信息，所以应该先池化再激活，对于MAX_pooling，两种操作结果一样，但是先池化下采样，可以减少激活函数的计算量，<strong>总结就是先池化再激活</strong>。但是好多网络实际实现上都是relu紧跟着conv，后面接pooling，这样比较interpretable——cross feature map pooling </li><li><p>mlpconv layer can be regarded as a highly nonlinear function(filter-fc-activation-fc-activation-fc-activation…)</p><p><img src="/2019/12/25/NiN-network-in-network/mlpconv.png" width="50%;"></p><p><strong>comparison 2:</strong></p></li><li><p>maxout network imposes the prior that instances of a latent concept lie within a convex set in the input space【QUESTION HERE】</p></li><li><p>mlpconv layer is a universal function approximator instead of a convex function approximator  </p><p><strong>comparison 3:</strong></p></li><li><p>fully connected layers are prone to overfitting and heavily depend on dropout regularization </p></li><li>global average pooling is more meaningful and interpretable, moreover it itself is a structural regularizer【QUESTION HERE】</li></ul></li><li><p>方法</p><ul><li>use <strong>mlpconv layer</strong> to replace conventional GLM(linear filters)</li><li>use <strong>global average pooling</strong> to replace traditional fully connected layers</li><li>the overall structure is a stack of mlpconv layers, on top of which lie the global average pooling and the objective cost layer</li><li>Sub-sampling layers can be added in between the mlpconv as in CNN</li><li>dropout is applied on the outputs of all but the last mlpconv layers for regularization</li><li>another regularizer applied is weight decay </li></ul><p><img src="/2019/12/25/NiN-network-in-network/NIN.png" width="70%;"></p></li><li><p>细节</p><ul><li><p>preprocessing：global contrast normalization and ZCA whitening </p></li><li><p>augmentation：translation and horizontal flipping</p></li><li><p>GAP for conventional CNN：CNN+FC+DROPOUT &lt; CNN+GAP &lt; CNN+FC</p><ul><li>gap is effective as a regularizer</li><li>slightly worse than the dropout regularizer result for some reason</li></ul></li><li><p>confidence maps </p><ul><li>explicitly enforce feature maps in the last mlpconv layer of NIN to be confidence maps of the categories by means of global average pooling：NiN将GAP的输出直接作为output layer，因此每一个类别对应的feature map可以近似认为是 confidence map。</li><li>the strongest activations appear roughly at the same region of the object in the original image：特征图上高响应区域基本与原图上目标区域对应。</li><li>this motivates the possibility of performing object detection via NIN</li></ul></li><li><p>architecture：实际中多层感知器使用1x1conv来实现，增加的多层感知器相当于是一个含参的池化层，通过对多个特征图进行含参池化，再传递到下一层继续含参池化，这种级联的<strong>跨通道的含参池化</strong>让网络有了更复杂的表征能力。</p><p>  <img src="/2019/12/25/NiN-network-in-network/architecture.png" width="55%;"></p></li></ul></li><li><p>总结</p><ol><li>mlpconv：stronger local reception unit</li><li>gap：regularizer &amp; bring confidence maps</li></ol></li></ol>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>unet &amp; vnet</title>
      <link href="/2019/12/05/unet-vnet/"/>
      <url>/2019/12/05/unet-vnet/</url>
      <content type="html"><![CDATA[<h2 id="U-NET-Convolutional-Networks-for-Biomedical-Image-Segmentation"><a href="#U-NET-Convolutional-Networks-for-Biomedical-Image-Segmentation" class="headerlink" title="U-NET: Convolutional Networks for Biomedical Image Segmentation"></a>U-NET: Convolutional Networks for Biomedical Image Segmentation</h2><ol><li><p>动机：</p><ul><li>train from very few images </li><li>outperforms more precisely on segmentation tasks</li><li>fast</li></ul></li><li><p>要素：</p><ul><li>编码：a contracting path to capture context</li><li>解码：a symmetric expanding path that enables precise localization</li><li>实现：pooling operators &amp; upsampling operators</li></ul></li><li><p>论点：</p><ul><li><p>when we talk about deep convolutional networks：</p><ul><li>larger and deeper</li><li>millions of parameters </li><li>millions of training samples </li></ul></li><li><p>representative method：run a sliding-window and predict a pixel label based on its‘ patch</p></li><li>drawbacks：<ul><li>calculating redundancy of overlapping patches</li><li>big patch：more max-pooling layers that reduce the localization accuracy</li><li>small patch：less involvement of context</li></ul></li><li>metioned but not further explained：cascade structure</li></ul></li><li><p>方法：</p><ol><li><p>In order to localize, high resolution features from the contracting path are combined with the upsampled output. A successive convolution layer can then learn to assemble a more precise output based on this information. </p><p> 理解：深层特征层感受野较大，带有全局信息，将其上采样用于提供localization information，而横向add过来特征层带有局部特征信息。两个3*3的conv block用于将两类信息整合，输出更精确的表达。</p></li><li><p>In the upsampling part we have also a large number of feature channels, which allow the network to propagate context information to higher resolution layers.  </p><p> 理解：应该是字面意思吧，为上采样的卷积层保留更多的特征通道，就相当于保留了更多的上下文信息。</p></li><li><p>we use excessive data augmentation.</p></li></ol></li><li><p>细节：</p><ol><li><p>contracting path：</p><ul><li>typical CNN：blocks of [2 3*3 unpadded convs+ReLU+2*2 stride2 maxpooling]</li><li>At each downsampling step we double the number of feature channels</li></ul></li><li><p>expansive path：</p><ul><li>upsampling：<ul><li>2*2 up-conv that half the channels</li><li>concatenation the corresponding cropped feature map from the contracting path</li><li>2 [3x3 conv+ReLU]</li></ul></li><li>final layer：use a 1*1 conv to map the feature vectors to class vectors</li></ul><p><img src="/2019/12/05/unet-vnet/unet.png" width="60%"></p></li><li><p>train：</p><ul><li>prefer larger input size to larger batch size</li><li>sgd with 0.99 momentum so that the previously seen samples dominate the optimization</li></ul></li><li><p>loss：softmax &amp; cross entropy </p></li><li><p>unbalanced weight：</p><ul><li>pre-compute the weight map base on the frequency of pixels for a certain class </li><li>add the weight for a certain element to force the learning emphasis：e.g. the small separation borders </li><li>initialization：Gaussian distribution </li></ul></li><li><p>data augmentation：</p><ul><li>deformations </li><li>“Drop-out layers at the end of the contracting path perform further implicit data augmentation”</li></ul></li><li><p>metrics：“warping error”, the “Rand error” and the “pixel error”  for EM segmentation challenge  and average IOU for ISBI cell tracking challenge </p></li><li><p>prediction：</p><p>按照论文的模型结构，<strong>输入和输出的维度是不一样的</strong>——在valid padding的过程中有边缘信息损失。</p><p>那么如果我们想要预测黄框内的分割结果，需要输入一张更大的图（蓝框）作为输入，在图片边缘的时候，我们通过镜像的方式补全。</p><p><img src="/2019/12/05/unet-vnet/predict.png" width="60%;"></p><p><strong>因果关系：</strong></p><ul><li>首先因为内存限制，输入的不是整张图，是图片patch，</li><li>为了保留上下文信息，使得预测更准确，我们给图片patch添加一圈border的上下文信息（实际感兴趣的是黄框区域）</li><li>在训练时，为了避免重叠引入的计算，卷积层使用了valid padding</li><li>因此在网络的输出层，输出尺寸才是我们真正关注的部分</li><li>如果训练样本尺寸不那么huge，完全可以全图输入，然后使用same padding，直接预测全图mask</li></ul></li></ol></li><li><p>总结：</p><ul><li>train from very few images —-&gt; data augmentation</li><li>fast —-&gt; full convolution layers</li><li>precise —-&gt; global?</li></ul></li></ol><h2 id="V-Net-Fully-Convolutional-Neural-Networks-for-Volumetric-Medical-Image-Segmentation"><a href="#V-Net-Fully-Convolutional-Neural-Networks-for-Volumetric-Medical-Image-Segmentation" class="headerlink" title="V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation"></a>V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation</h2><ol><li><p>动机</p><ul><li>entire 3D volume</li><li>imbalance between the number of foreground and background voxels：dice coefficient</li><li>limited data：apply random non-linear transformations and histogram matching</li><li>fast and accurate</li></ul></li><li><p>论点：</p><ul><li>early approaches based on patches<ul><li>local context</li><li>challenging modailities</li><li>efficiency issues</li></ul></li><li>fully convolutional networks<ul><li>2D so far</li></ul></li><li>imbalance issue：the anatomy of interest occupies only a very small region of the scan thus predictions are strongly biased towards the background.<ul><li>re-weighting</li><li>dice coefficient claims to be better that above</li></ul></li></ul></li><li><p>要素：</p><ul><li>a compression path</li><li><p>a decompression path</p><p><img src="/2019/12/05/unet-vnet/vnet.png" width="55%"></p></li></ul></li><li><p>方法：</p><ul><li><p>compression：</p><ul><li>add residual能够加速收敛</li><li>resolution is reduced by [2*2*2 conv with stride 2]相比于maxpooling节省了bp所需switch map的memory消耗</li><li>double the number of feature maps as we reduce their resolution</li><li>PReLU</li></ul></li><li><p>decompression：</p><ul><li>horizontal connections：1) gather fine grained detail that would be otherwise lost in the compression path  2) improve the convergence time </li><li><p>residual conv：blocks of [5*5*5 conv with stride 1] 提取特征继续增大感受野</p><p><img src="/2019/12/05/unet-vnet/receptive.png" width="50%"></p></li><li><p>up-conv：expands the spatial support of the lower resolution feature maps </p><p><img src="/2019/12/05/unet-vnet/deconv.png" width="60%"></p></li><li><p>last layer：run [1*1*1conv with 2 channel+softmax] to obtain the voxelwise probabilistic segmentations of the foreground and background </p></li></ul></li><li><p>dice coefficient： [0,1] which we aim to maximise，assume $p_i$、$g_i$ belong to two <strong>binary volumes</strong></p><script type="math/tex; mode=display">  D = \frac{2\sum_i^N p_i g_i}{\sum_i^N p_i^2 + \sum_i^N g_i^2}</script></li><li><p>train：</p><ul><li>input fix size 128 × 128 × 64 voxels and a spatial resolution of 1 × 1 × 1.5 millimeters</li><li>each mini-batch contains 2 volumes</li><li>online augmentation：<ul><li>randomly deformation</li><li>vary the intensity distribution：随机选取样本的灰度分布作为当前训练样本的灰度分布</li></ul></li><li>used a momentum of 0.99 and a initial learning rate of 0.0001 which decreases by one order of magnitude every 25K iterations</li></ul></li><li><p>metrics：</p><ul><li>Dice coefficient</li><li>Hausdorff distance of the predicted delineation to the ground truth annotation</li><li>the score obtained on the challenge </li></ul></li></ul></li></ol><h2 id="dice-loss-amp-focal-loss"><a href="#dice-loss-amp-focal-loss" class="headerlink" title="dice loss &amp; focal loss"></a>dice loss &amp; focal loss</h2><ol><li><p>CE &amp; BCE</p><ul><li><p>CE：categorical_crossentropy，针对所有类别计算，类别间互斥</p><script type="math/tex; mode=display">CE(x) = -\sum_{i=1}^{n\_class}y_i log f_i(x)</script><blockquote><p>$x$是输入样本，$y_i$是第$i$个类别对应的真实标签，$f_i(x)$是对应的模型输出值。</p><p>对分类问题，$y_i$是one-hot，$f_i(x)$是个一维向量。最终得到一个数值。</p></blockquote></li><li><p>BCE：binary_crossentropy，针对每个类别计算</p><script type="math/tex; mode=display">BCE(x)_i = - [y_i log f_i(x) + (1-y_i)log(1-f_i(x))]</script><blockquote><p>$i$是类别编号，最终得到一个维度为$n_class$的向量。</p><p>再求类均值得到一个数值作为单个样本的loss。</p></blockquote><script type="math/tex; mode=display">BCE(x) = \frac{\sum_{i=1}^{n\_class}BCE_i(x)}{n\_class}</script></li><li><p>batch loss：对batch中所有样本的loss求均值。</p></li><li><p>从公式上看，CE的输出通常是经过了softmax，softmax的某一个输出增大，必然导致其它类别的输出减小，因此在计算loss的时候关注正确类别的预测值是否被拉高即可。使用BCE的场景通常是使用sigmoid，类别间不会互相压制，因此既要考虑所属类别的预测概率够高，也要考虑不所属类别的预测概率足够低（这一项在softmax中被实现了故CE不需要这一项）。</p></li><li>场景：<ul><li>二分类：只有一个输出节点，$f(x) \in (0,1)$，应该使用sigmoid+BCE作为最后的输出层配置。</li><li>单标签多分类：应该使用softmax+CE的方案，BCE也同样适用。</li><li>多标签多分类：multi-label每个标签的输出是相互独立的，因此常用配置是sigmoid+BCE。</li></ul></li><li>对分割场景来说，输出的每一个channel对应一个类别的预测map，可以看成是多个channel间的单标签多分类（softmax+CE），也可以看成是每个独立通道类别map的二分类（sigmoid+BCE）。unet论文用了weighted的softmax+CE。vnet论文用了dice_loss。</li></ul></li><li><p>re-weighting(WCE)</p><p> 基于CE&amp;BCE，给了样本不同的权重。</p><p> unet论文中提到了基于pixel frequency为不同的类别创建了weight map。</p><p> 一种实现：基于每个类别的weight map，在实现CE的时候改成加权平均即可。</p><p> 另一种实现：基于每个样本的weight map，作为网络的附加输入，在实现CE的时候乘在loss map上。</p></li><li><p>focal loss</p><p> 提出是在目标检测领域，用于解决正负样本比例严重失调的问题。</p><p> 也是一种加权，但是相比较于re-weighting，<strong>困难样本的权重由网络自行推断出</strong>，通过添加$(\alpha)$和$(-)^\lambda$这一加权项：</p><script type="math/tex; mode=display"> focal\_loss(x)_i = -[\alpha y_i (1-p_i)^\lambda log (p_i)+(1-\alpha)(1-y_i)p_i^\lambda log(1-p_i)]</script><ul><li><p>对于类别间不均衡的情况（通常负样本远远多于正样本），$(\alpha)$项用于平衡正负样本权重。</p></li><li><p>对于类内困难样本的挖掘，$(-)^\lambda$项用于调整简单样本和困难样本的权重，预测概率更接近真实label的样本（简单样本）的权重会衰减更快，预测概率比较不准确的样本（苦难样本）的权重则更高些。</p><p>由于分割网络的输出的单通道／多通道的图片，直接使用focal loss会导致loss值很大。</p><p>​    1. 通常与其他loss加权组合使用</p><p>​    2. sum可以改成mean</p><p>​    3.不建议在训练初期就加入，可在训练后期用于优化模型</p><p>​    4. 公式中含log计算，可能导致nan，要对log中的元素clip</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">focal_loss</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    gamma = <span class="number">2.</span></span><br><span class="line">    alpha = <span class="number">0.25</span></span><br><span class="line">    <span class="comment"># score = alpha * y_true * K.pow(1 - y_pred, gamma) * K.log(y_pred) +            # this works when y_true==1</span></span><br><span class="line">    <span class="comment">#         (1 - alpha) * (1 - y_true) * K.pow(y_pred, gamma) * K.log(1 - y_pred)  # this works when y_true==0</span></span><br><span class="line">    pt_1 = tf.where(tf.equal(y_true, <span class="number">1</span>), y_pred, tf.ones_like(y_pred))</span><br><span class="line">    pt_0 = tf.where(tf.equal(y_true, <span class="number">0</span>), y_pred, tf.zeros_like(y_pred))</span><br><span class="line">    <span class="comment"># avoid nan</span></span><br><span class="line">    pt_1 = K.clip(pt_1, <span class="number">1e-3</span>, <span class="number">.999</span>)</span><br><span class="line">    pt_0 = K.clip(pt_0, <span class="number">1e-3</span>, <span class="number">.999</span>)</span><br><span class="line">    score = -K.sum(alpha * K.pow(<span class="number">1.</span> - pt_1, gamma) * K.log(pt_1)) -  \</span><br><span class="line">            K.sum((<span class="number">1</span> - alpha) * K.pow(pt_0, gamma) * K.log(<span class="number">1.</span> - pt_0))</span><br><span class="line">    <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure></li></ul></li><li><p>dice loss</p><p>dice定义两个mask的相似程度：</p><script type="math/tex; mode=display">dice = \frac{2 * A \bigcap B}{|A|+|B|} = \frac{2 * TP}{2*TP + FN + FP}</script><ul><li>分子是TP——只关注前景</li><li><p>分母可以是$|A|$（逐个元素相加），也可以是平方形式$|A|^2$</p></li><li><p><strong>梯度：</strong>“使用dice loss有时会不可信，原因是对于softmax或log loss其梯度简言之是p-t ，t为目标值，p为预测值。而dice loss 为 2t2 / (p+t)2</p><p> 如果p，t过小会导致梯度变化剧烈，导致训练困难。”</p><p> 【详细解释下】交叉熵loss：$L=-(1-|t-p|)log(1-|t-p|)$，求导得到$\frac{\partial L}{\partial p}=-log(1-|t-p|)$，其实就可以简化看作$t-p$，很显然这个梯度是有界的，因此使用交叉熵loss的优化过程比较稳定。而dice loss的两种形式（不平方&amp;平方）：$L=\frac{2pt}{p+t}\ or\  L=\frac{2pt}{p^2+t^2}$，求导以后分别是$\frac{\partial L}{\partial p} = \frac{t^2+2pt}{(p+t)^2} \ or\ \frac{3tp^2+t^3}{(p^2+t^2)^2}$计算结果比较复杂，pt都很小的情况下，梯度值可能很大，可能导致训练不稳定，loss曲线混乱。</p></li></ul></li></ol><p>  vnet论文中的定义在分母上稍有不同（see below）。smoothing的好处：</p><ul><li>避免分子除0</li><li><p>减少过拟合</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dice_coef</span><span class="params">(y_true, y_pred)</span>:</span> </span><br><span class="line">  smooth = <span class="number">1.</span></span><br><span class="line">    intersection = K.sum(y_true * y_pred, axis=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) </span><br><span class="line">    union = K.sum(y_true, axis=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) + K.sum(y_pred, axis=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) </span><br><span class="line">    <span class="keyword">return</span> K.mean( (<span class="number">2.</span> * intersection + smooth) / (union + smooth), axis=<span class="number">0</span>) </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dice_coef_loss</span><span class="params">(y_true, y_pred)</span>:</span> </span><br><span class="line">    <span class="number">1</span> - dice_coef(y_true, y_pred, smooth=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li></ul><ol><li><p>iou loss</p><p> dice loss衍生，intersection over union：</p><script type="math/tex; mode=display"> iou = \frac{A \bigcap B}{A \bigcup B}</script><p> 分母上比dice少了一个intersection。</p><ul><li>“IOU loss的缺点同DICE loss，训练曲线可能并不可信，训练的过程也可能并不稳定，有时不如使用softmax loss等的曲线有直观性，通常而言softmax loss得到的loss下降曲线较为平滑。”</li></ul></li><li><p>boundary loss</p><p> dice loss和iou loss是基于<strong>区域面积匹配度</strong>去学习，我们也可以使用<strong>边界匹配度</strong>去监督网络的学习。</p><p> 只对边界上的像素进行评估，和GT的边界吻合则为0，不吻合的点，根据其距离边界的距离评估它的Loss。</p></li><li><p>Hausdorff distance</p><p> 用于度量两个点集之间的相似程度，denote 点集$A\{a_1, a_2, …, a_p\}$，点集$B\{b_1, b_2, …, b_p\}$：</p><script type="math/tex; mode=display"> HD(A, B) = max\{hd(A,B), hd(B,A)\}\\ hd(A,B) = max_{a \in A} min_{b in B} ||a-b||\\ hd(B,A) = max_{b \in B} min_{a in A} ||b-a||</script><p> 其中HD(A,B)是Hausdorff distance的基本形式，称为双向距离</p><p> hd(A,B)描述的是单向距离，首先找到点集A中每个点在点集B中距离最近的点作为匹配点，然后计算这些a-b-pair的距离的最大值。</p><p> HD(A,B)取单向距离中的最大值，描述了两个点集合的最大不匹配程度。</p></li><li><p>mix loss</p><ul><li>BCE + dice loss：在数据较为平衡的情况下有改善作用，但是在数据极度不均衡的情况下，交叉熵损失会在几个训练之后远小于Dice 损失，效果会损失。</li><li>focal loss + dice loss：数量级问题</li></ul></li><li><p>MSE</p><p> 关键点检测有时候也会采用分割框架，这时候ground truth是高斯map，dice是针对二值化mask的，这时候还可以用MSE。</p></li><li><p>ohnm</p><p>online hard negative mining 困难样本挖掘</p></li><li><p>Tversky loss</p><p>一种加权的dice loss，dice loss会平等的权衡FP（精度，假阳）和FN（召回，假阴），但是医学图像中病灶数目远少于背景数量，很可能导致训练结果偏向高精度但是低召回率，Tversky loss控制loss更偏向FN：</p><script type="math/tex; mode=display">loss = 1-\frac{|PG|}{|PG|+\alpha|P\backslash G|+\beta|G\backslash P|}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tversky_loss</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    y_true_pos = K.flatten(y_true)</span><br><span class="line">    y_pred_pos = K.flatten(y_pred)</span><br><span class="line">    <span class="comment"># TP</span></span><br><span class="line">    true_pos = K.sum(y_true_pos * y_pred_pos)</span><br><span class="line">    <span class="comment"># FN</span></span><br><span class="line">    false_neg = K.sum(y_true_pos * (<span class="number">1</span>-y_pred_pos))</span><br><span class="line">    <span class="comment"># FP</span></span><br><span class="line">    false_pos = K.sum((<span class="number">1</span>-y_true_pos) * y_pred_pos)</span><br><span class="line">    alpha = <span class="number">0.7</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - (true_pos + K.epsilon())/(true_pos + alpha * false_neg + (<span class="number">1</span>-alpha) * false_pos + K.epsilon())</span><br></pre></td></tr></table></figure></li><li><p>Lovasz hinge &amp; Lovasz-Softmax loss</p><p>IOU loss衍生，jaccard loss只适用于离散情况，而网络预测是连续值，如果不使用某个超参将神经元输出二值化，就不可导。blabla</p><p>不是很懂直接用吧：<a href="https://github.com/bermanmaxim/LovaszSoftmax" target="_blank" rel="noopener">https://github.com/bermanmaxim/LovaszSoftmax</a></p></li></ol><h2 id="一些补充"><a href="#一些补充" class="headerlink" title="一些补充"></a>一些补充</h2><ol><li><p>改进：</p><ol><li>dropout、batch normalization：从论文上看，unet只在最深层卷积层后面添加了dropout layer，BN未表，而common sense用每一个conv层后面接BN层能够替换掉dropout并能获得性能提升的。</li><li>UpSampling2D、Conv2DTranspose：unet使用了上采样，vnet使用了deconv，但是“DeConv will produce image with checkerboard effect, which can be revised by upsample and conv”(<a href="https://distill.pub/2016/deconv-checkerboard/" target="_blank" rel="noopener">Reference</a>)。</li><li>valid padding、same padding：unet论文使用图像patch作为输入，特征提取时使用valid padding，损失边缘信息。</li><li>network blocks：unet用的conv block是两个一组的3*3conv，vnet稍微不同一点，可以尝试的block有ResNet／ResNext、DenseNet、DeepLab等。</li><li>pretrained encoder：feature extraction path使用一些现有的backbone，可以加载预训练权重(<a href="https://arxiv.org/abs/1801.05746" target="_blank" rel="noopener">Reference</a>)，加速训练，防止过拟合。</li><li>加入SE模块(<a href="https://zhuanlan.zhihu.com/p/36890585" target="_blank" rel="noopener">Reference</a>)：对每个通道的特征加权</li><li>attention mechanisms：</li><li>引用nn-Unet主要<strong>结构改进</strong>合集：“Just to provide some prominent examples: variations of encoder-decoder style architectures with skip connections, first introduced by the U-Net [12], include the introduction of residual connections [9], dense connections [6], at- tention mechanisms [10], additional loss layers [5], feature recalibration [13], and others [11]. </li></ol></li><li><p>衍生：</p><ol><li><p>TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation </p></li><li><p>nnU-Net: Breaking the Spell on Successful Medical Image Segmentation</p></li></ol></li></ol><h2 id="TernausNet-U-Net-with-VGG11-Encoder-Pre-Trained-on-ImageNet-for-Image-Segmentation"><a href="#TernausNet-U-Net-with-VGG11-Encoder-Pre-Trained-on-ImageNet-for-Image-Segmentation" class="headerlink" title="TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation"></a>TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation</h2><ol><li><p>动机：</p><ul><li>neural network initialized with pre-trained weights usually shows better performance than those trained from scratch on a small dataset. </li><li>保留encoder-decoder的结构，同时充分利用迁移学习的优势</li></ul></li><li><p>论点：</p><ul><li>load pretrained weights</li><li>用huge dataset做预训练</li></ul></li><li><p>方法：</p><ul><li>用vgg11替换原始的encoder，并load pre-trained weights on ImageNet：</li><li>最深层输入(maxpooling5)：use a single conv of 512 channels that <strong>serves as a bottleneck central part</strong> of the network</li></ul><p><img src="/2019/12/05/unet-vnet/TernausNet.png" width="50%;">                      <img src="/2019/12/05/unet-vnet/vgg11.png" width="30%;"></p><ul><li><p>upsampling换成了convTranspose</p></li><li><p>loss function：IOU + BCE：</p><script type="math/tex; mode=display"> L = BCE - log(IOU)</script></li><li><p>inference：choose a <strong>threshold 0.3</strong>, all pixel values below which are set to be zero</p></li></ul></li><li><p>结论：</p><ol><li>converge faster</li><li>better IOU</li></ol></li></ol><h2 id="nnU-Net-Breaking-the-Spell-on-Successful-Medical-Image-Segmentation"><a href="#nnU-Net-Breaking-the-Spell-on-Successful-Medical-Image-Segmentation" class="headerlink" title="nnU-Net: Breaking the Spell on Successful Medical Image Segmentation"></a>nnU-Net: Breaking the Spell on Successful Medical Image Segmentation</h2><ol><li><p>动机</p><ul><li>many proposed methods fail to generalize: 对于分割任务，从unet出来之后的几年里，在网络结构上已经没有多少的突破了，结构修改越多，反而越容易过拟合</li><li>relies on just a simple U-Net architecture embedded in a robust training scheme</li><li>automate necessary adaptations such as preprocessing, the exact patch size, batch size, and inference settings based on the properties of a given dataset: 更多的提升其实在于理解数据，针对数据采用适当的预处理和训练方法和技巧</li></ul></li><li><p>论点</p><ul><li>the diversity and individual peculiarities of imaging datasets make it difficult to generalize </li><li>prominent modifications focus on architectural modifications, merely brushing over all the other hyperparameters</li><li>we propose: 使用基础版unet：nnUNet（no-new-Net）<ul><li>a formalism for automatic adaptation to new datasets</li><li>automatically designs and executes a network training pipeline </li><li>without any manual fine-tuning</li></ul></li></ul></li><li><p>要素</p><p>a segmentation task: $f_{\theta}(X) = \hat Y$,  in this paper we seek for a $g(X,Y)=\theta$.</p><p>First we distinguish two type of hyperparameters:</p><ul><li>static params：in this case the network architecture and a robust training scheme </li><li>dynamic params：those that need to be changed in dependence of $X$ and $Y$</li></ul><p>Second we define g——a set of heuristics rules covering the entire process of the task:</p><ul><li>预处理：resampling和normalization</li><li>训练：loss，optimizer设置、数据增广</li><li>推理：patch-based策略、test-time-augmentations集成和模型集成等</li><li>后处理：增强单连通域等</li></ul></li><li><p>方法</p><ol><li><p>Preprocessing</p><ul><li>Image Normalization：<ul><li>CT：$normed_intensity = (intensity  - fg_mean) / fg_standard_deviation$,   $fg$ for $[0.05,0.95]$ foreground intensity</li><li>not CT：$normed_intensity = (intensity  - mean) / standard_deviation $</li></ul></li><li>Voxel Spacing：<ul><li>for each axis chooses the median as the target spacing</li><li>image resampled with third order spline interpolation</li><li>z-axis using nearest neighbor interpolation if ‘anisotropic spacing’ occurs</li><li>mask resampled with third order spline interpolation</li></ul></li></ul></li><li><p>Training Procedure </p><ul><li><p>Network Architecture：</p><ul><li><p>3 <strong>independent</strong> model：a 2D U-Net, a 3D U-Net and a cascade of two 3D U-Net  </p><p>  <img src="/2019/12/05/unet-vnet/nnUnet.jpg" width="80%;"></p></li><li><p>padded convolutions：to achieve identical output and input shapes </p></li><li><p><strong>instance normalization</strong>：“BN适用于判别模型，比如图片分类模型。因为BN注重对每个batch进行归一化，从而保证数据分布的一致性，而判别模型的结果正是取决于数据整体分布。但是BN对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布；IN适用于生成模型，比如图片风格迁移。因为图片生成的结果主要依赖于某个图像实例，所以对整个batch归一化不适合图像风格化，在风格迁移中使用Instance Normalization不仅可以加速模型收敛，并且可以保持每个图像实例之间的独立。”</p></li><li><p>Leaky ReLUs</p></li></ul></li><li><p>Network Hyperparameters：</p><ul><li>sets the batch size, patch size and number of pooling operations for each axis based on the memory consumption </li><li>large patch sizes are favored over large batch sizes </li><li>pooling along each axis is done until the voxel size=4</li><li>start num of filters=30, double after each pooling</li><li>If the selected patch size covers less than 25% of the voxels, train the 3D U-Net cascade on a downsampled version of the training data  to keep sufficient context </li></ul></li><li><p>Network Training:</p><ul><li>five-fold cross-validation </li><li>One epoch is defined as processing 250 batches </li><li>loss = dice loss + cross-entropy loss </li><li>Adam(lr=3e-4, decay=3e-5)</li><li>lrReduce: EMA(train_loss), 30 epoch, factor=0.2</li><li>earlyStop: earning rate drops below 10 6 or 1000 epochs are exceeded</li><li>data augmentation: elastic deformations, random scaling and random rotations as well as <strong>gamma augmentation</strong>($g(x,y)=f(x,y)^{gamma}$)</li><li>keep transformations in 2D-plane if ‘anisotropic spacing’ occurs</li></ul></li><li><p>Inference </p><ul><li>sliding window with half the patch size: this increases the weight of the predictions close to the center relative to the borders</li><li>ensemble:<ul><li>U-Net configurations (2D, 3D and cascade)</li><li>furthermore uses the five models (five-fold cross-validation)</li></ul></li><li></li></ul></li></ul></li></ol></li><li><p>Ablation studies</p><p><img src="/2019/12/05/unet-vnet/nn-Unet Ablation studies.png" width="70%;"></p></li></ol><h2 id="3D-U-Net-Learning-Dense-Volumetric-Segmentation-from-Sparse-Annotation"><a href="#3D-U-Net-Learning-Dense-Volumetric-Segmentation-from-Sparse-Annotation" class="headerlink" title="3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation"></a>3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation</h2><ol><li><p>动机</p><ul><li>learns from sparsely/full annotated volumetric images (user annotates some slices)</li><li>provides a dense 3D segmentation </li></ul></li></ol><ol><li><p>要素</p><ul><li>3D operations </li><li>avoid bottlenecks and use batch normalization for faster convergence</li><li>on-the-fly elastic deformation</li><li>train from scratch</li></ul></li><li><p>论点</p><ul><li>neighboring slices show almost the same information </li><li>many biomedical applications generalizes reasonably well because medical images comprises repetitive structures  </li><li>thus we suggest dense-volume-segmentation-network that only requires some annotated 2D slices for training</li><li><p>scenarios</p><ul><li>manual annotated 一部分slice，然后训练网络实现dense seg</li><li>用一部分 sparsely annotated的dataset作为training set，然后训练的网络实现在新的数据集上dense seg</li></ul><p><img src="/2019/12/05/unet-vnet/scenarios.png" width="50%;"></p></li></ul></li><li><p>方法</p><ul><li><p>Network Architecture </p><ul><li><p>compression：2*3x3x3 convs(+BN)+relu+2x2x2 maxpooling</p></li><li><p>decompression：2x2x2 upconv+2*3x3x3 convs+relu</p></li><li><p>head：1x1x1 conv</p></li><li><p>concat shortcut connections</p></li><li><p>【QUESTION】avoid bottlenecks by doubling the number of channels already before max pooling</p><p>  个人理解这个double channel是在跟原始的unet结构对比，原始unet每个stage的两个conv的filter num是一样的，然后进行max pooling会损失部分信息，但是分割任务本身是个dense prediction，所以增大channel来减少信息损失</p><p>  但是不理解什么叫“avoid bottlenecks”</p><p>  原文说是参考了《Rethinking the inception architecture for computer vision》大名鼎鼎的inception V3</p><p>  可能对应的是“1. Avoid representational bottlenecks, especially early in the network.”，从输入到输出，要逐渐减少feature map的尺寸，同时要逐渐增加feature map的数量。</p></li></ul><p><img src="/2019/12/05/unet-vnet/3Dunet.png" width="50%;"></p><ul><li>input：132x132x116 voxel tile </li><li>output：44x44x28</li><li>BN：before each ReLU</li></ul></li></ul></li></ol><ul><li><p><strong>weighted softmax loss function</strong>：setting the weights of unlabeled pixels to zero makes it possible to learn from only the labelled ones and, hence, to generalize to the whole volume（是不是random set the loss zeros of some samples总能让网络更好的generalize？）</p><ul><li><p>Data</p><ul><li>manually annotated some orthogonal xy, xz, and yz slices </li><li>annotation slices were sampled uniformly</li></ul></li><li><p>ran on down-sampled versions of the original resolution by factor of two </p></li><li><p>labels：0: “inside the tubule”; 1: “tubule”; 2: “background”, and 3: “unlabeled”.  </p></li><li><p>Training</p><ul><li>rotation, scaling and gray value augmentation</li></ul></li><li>a smooth dense deformation：random vector, normal distribution, B-spline interpolation <ul><li>weighted cross-entropy loss：increase weights  “inside the tubule”, reduce weights “background”, set zero “unlabeled”</li></ul></li></ul></li></ul><h2 id="2-5D-UNet-Automatic-Segmentation-of-Vestibular-Schwannoma-from-T2-Weighted-MRI-by-Deep-Spatial-Attention-with-Hardness-Weighted-Loss"><a href="#2-5D-UNet-Automatic-Segmentation-of-Vestibular-Schwannoma-from-T2-Weighted-MRI-by-Deep-Spatial-Attention-with-Hardness-Weighted-Loss" class="headerlink" title="2.5D-UNet: Automatic Segmentation of Vestibular Schwannoma from T2-Weighted MRI by Deep Spatial Attention with Hardness-Weighted Loss"></a>2.5D-UNet: Automatic Segmentation of Vestibular Schwannoma from T2-Weighted MRI by Deep Spatial Attention with Hardness-Weighted Loss</h2><ol><li><p>专业术语</p><ul><li>Vestibular Schwannoma(VS) tumors：前庭神经鞘瘤</li><li>through-plane resolution：层厚</li><li>isotropic resolution：各向同性</li><li>anisotropic resolutions：各向异性</li></ul></li><li><p>动机</p><ul><li><p>tumor的精确自动分割</p></li><li><p>challenge</p><ul><li>low contrast：hardness-weighted Dice loss functio </li><li>small target region：attention module </li><li>low through-plane resolution：2.5D</li></ul></li></ul></li><li><p>论点</p><ul><li>segment small structures from large image contexts<ul><li>coarse-to-fine </li><li>attention map </li><li>Dice loss </li><li>our method<ul><li>end-to-end supervision on the learning of attention map </li><li>voxel-level hardness- weighted Dice loss function </li></ul></li></ul></li><li>CNN<ul><li>2D CNNs ignore inter-slice correlation </li><li>3D CNNs most applied to images with isotropic resolution requiring upsampling</li><li>to balance the physical receptive field (in terms of mm rather than voxels)：memory rise</li><li>our method<ul><li>high in-plane resolution &amp; low through-plane resolution </li><li>2.5D CNN combining 2D and 3D convolutions </li><li>use inter-slice features </li><li>more efficient than 3D CNNs</li></ul></li></ul></li><li>数据<ul><li>T2-weighted MR images of 245 patients with VS tumor</li><li>high in-plane resolution around 0.4 mm×0.4 mm，512x512</li><li>slice thickness and inter-slice spacing 1.5 mm，slice number 19 to 118</li><li>cropped cube size：100 mm×50 mm×50 mm </li></ul></li></ul></li><li><p>方法</p><ul><li><p>architecture</p><ul><li>five levels：L1、L2 use 2D，L3、L4、L5 use 3D</li><li>After the first two max-pooling layers that downsample the feature maps only in 2D, the feature maps in L3 and the followings have a near- isotropic 3D resolution.  </li><li>start channels：16</li><li>conv block：conv-BN-pReLU</li><li><p>add a spatial attention module to each level of the decoder </p><p><img src="/2019/12/05/unet-vnet/2.5D UNet.png" width="70%;"></p></li></ul></li><li><p>spatial attention module </p><ul><li>A spatial attention map can be seen as a single-channel image of attention coefficient </li><li>input：feature map with channel $N_l$</li><li>conv1+ReLU： channel $N_l/2$</li><li>conv2+Sigmoid：channel 1，outputs the attention map</li><li>multiplied the feature map with the attention map</li><li>a residual connection</li><li>explicit supervision <ul><li>multi-scale attention loss </li><li>$L_{attention} = \frac{1}{L} \sum_{L} l(A_l, G_l^f)$</li><li>$A_l$是每一层的attention map，$G_l^f$是每一层是前景ground truth average-pool到当前resolution的mask</li></ul></li></ul></li><li><p>Voxel-Level Hardness-Weighted Dice Loss</p><ul><li><p>automatic hard voxel weighting：$w_i = \lambda * abs(p_i - g_i) + (1-\lambda)$</p></li><li><p>$\lambda \in [0,1]$，controls the degree of hard voxel weighting</p></li><li><p>hardness-weighted Dice loss (HDL) ：</p><script type="math/tex; mode=display">  l(P,G) = 1.0 - \frac{1}{C}\sum_{C} \frac{2\sum_i w_i p_i g_i + \epsilon}{\sum_i w_i (p_i + g_i) + \epsilon}</script></li><li><p>total loss：</p><script type="math/tex; mode=display">  L = \frac{1}{L} \sum_{L} l(A_l, G_l^f) + l(P,G)</script></li></ul></li></ul></li></ol><h2 id="Combining-analysis-of-multi-parametric-MR-images-into-a-convolutional-neural-network-Precise-target-delineation-for-vestibular-schwannoma-treatment-planning"><a href="#Combining-analysis-of-multi-parametric-MR-images-into-a-convolutional-neural-network-Precise-target-delineation-for-vestibular-schwannoma-treatment-planning" class="headerlink" title="Combining analysis of multi-parametric MR images into a convolutional neural network: Precise target delineation for vestibular schwannoma treatment planning"></a>Combining analysis of multi-parametric MR images into a convolutional neural network: Precise target delineation for vestibular schwannoma treatment planning</h2><p>只有摘要和一幅图</p><p><img src="/2019/12/05/unet-vnet/two-pathway-unet.jpg" width="80%;"></p><ul><li>multi-parametric MR images：T1W、T2W、T1C</li><li>two-pathway U-Net model<ul><li>kernel 3 × 3 × 1 and 1 × 1 × 3 respectively</li><li>to extract the in-plane and through-plane features of the anisotropic MR images</li></ul></li><li>结论<ul><li>The proposed two-pathway U-Net model outperformed the single-pathway U-Net model when segmenting VS using anisotropic MR images.</li><li>multi-inputs（T1、T2）outperforms single-inputs</li></ul></li></ul>]]></content>
      
      
        <tags>
            
            <tag> paper, 语义分割 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>yolo系列</title>
      <link href="/2019/11/28/yolo%E7%B3%BB%E5%88%97/"/>
      <url>/2019/11/28/yolo%E7%B3%BB%E5%88%97/</url>
      <content type="html"><![CDATA[<h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><ol><li>[yolov1] Yolov1: You Only Look Once: Unified, Real-Time Object Detection</li><li>[yolov2] Yolov2: YOLO9000: Better, Faster, Stronger</li><li>[yolov3] Yolov3: An Incremental Improvement </li><li>[yolov4] YOLOv4: Optimal Speed and Accuracy of Object Detection </li><li>[poly-yolo] POLY-YOLO: HIGHER SPEED, MORE PRECISE DETECTION AND INSTANCE SEGMENTATION FOR YOLOV3 </li><li>[scaled-yolov4] Scaled-YOLOv4: Scaling Cross Stage Partial Network </li></ol><h2 id="0-review"><a href="#0-review" class="headerlink" title="0. review"></a>0. review</h2><ol><li><p>review0121：关于yolo loss</p><p> 之前看keras版的yolo loss，包含分类的bce，回归的l2/mse，以及confidence的回归loss，其中conf loss被建模成单纯的0-1分类问题，用bce来实现。</p><p> 事实上原版的yolo loss中，objectness是iou（pred和gt的iou），从意义上，不仅指示当前格子有无目标，还对当前的box prediction做了评估</p><ul><li>回传梯度</li><li><p>不回传梯度</p><p>iou是通过xywh计算的，scaled_yolov4中把这个梯度截断，只作为一个值，对confidence进行梯度回传，</p><p>梯度不截断也没有问题，相当于对xywh再回传一个iou的loss</p></li></ul></li></ol><h2 id="1-Yolov1-You-Only-Look-Once-Unified-Real-Time-Object-Detection"><a href="#1-Yolov1-You-Only-Look-Once-Unified-Real-Time-Object-Detection" class="headerlink" title="1. Yolov1: You Only Look Once: Unified, Real-Time Object Detection"></a>1. Yolov1: You Only Look Once: Unified, Real-Time Object Detection</h2><ol><li>动机:<ul><li>end-to-end: 2 stages —-&gt; 1 stage</li><li>real-time</li></ul></li><li><p>论点：</p><ul><li>past methods:  complex pipelines, hard to optimize(trained separately) <ul><li>DPM use a sliding window and a classifier to evaluate an object at various locations </li><li>R-CNN use region proposal and run classifier on the proposed boxes,  then post-processing </li></ul></li><li>in this paper:  you only look once at an image <ul><li>rebuild the framework as a <strong>single</strong> regression problem:  single stands for <strong>you don’t have to run classifiers on each patch</strong></li><li><strong>straight</strong> from image pixels to bounding box coordinates and class probabilities:  straight stands for <strong>you obtain the bounding box and the classification results side by side, comparing to the previous serial pipeline </strong></li></ul></li></ul></li><li><p>advantages：</p><ul><li>fast &amp; twice the <strong>mean average precision</strong> of other real-time systems</li><li>CNN sees the entire image thus encodes contextual information </li><li>generalize better</li></ul></li><li><p>disadvantage:</p><ul><li>accuracy: “ it struggles to precisely localize some objects, especially small ones”</li></ul></li><li><p>细节：</p><ul><li><p>grid：</p><p>Our system divides the input image into an S × S grid. </p><p>If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object. </p><p><img src="/2019/11/28/yolo系列/grid.png" width="60%;"></p></li><li><p>prediction：</p><p> Each grid cell predicts B bounding boxes,  confidence scores <strong>for these boxes</strong> , and C conditional class probabilities <strong>for each grid</strong></p><p> that is an <script type="math/tex">S*S*(B*5+C)</script> tensor</p><ul><li>We normalize the bounding box width and height by the image width and height so that they fall between 0 and 1.</li><li>We parametrize the bounding box x and y coordinates to be offsets of a particular grid cell so they are also bounded between 0 and 1. </li></ul></li><li><p>at test time：</p><p> We obtain the <strong>class-specific confidence</strong> for individual box by multiply the class probability and box confidence：</p><script type="math/tex; mode=display"> Pr(Class_i | Object) * Pr(Object)* IOU^{truth}_{pred} = Pr(Class_i)* IOU^{truth}_{pred}</script></li><li><p>network：</p><p>the convolutional layers extract features from the image </p><p>while the fully connected layers predict the probabilities and coordinates</p><p><img src="/2019/11/28/yolo系列/network.png" width="60%"></p></li><li><p>training：</p><p> activation：use a linear activation function for the final layer and leaky rectified linear activation all the other layers</p><p> optimization：use sum-squared error, however it does not perfectly align with the goal of maximizing average precision</p><p> ​    * weights equally the localization error and classification error：$\lambda_{coord}$</p><p> ​    * weights equally the grid cells containing and not-containing objects：$\lambda_{noobj}$</p><p> ​    * weights equally the large boxes and small boxes：square roots the h&amp;w insteand of the straight h&amp;w</p><p> loss：pick the box predictor has the highest current IOU with the ground truth per grid cell</p><p> avoid overfitting：dropout &amp; data augmentation</p><p> ​    * use dropout after the first connected layer,</p><p> ​    * introduce random scaling and translations of up to 20% of the original image size for data augmentation</p><p> ​    * randomly adjust the exposure and saturation of the image by up to a factor of 1.5 in the HSV color space for data augmentation</p></li><li><p>inference：</p><p> multiple detections：some objects locates near the border of multiple cells and <strong>can be well localized by multiple cells</strong>. <strong>Non-maximal suppression</strong> is proved critical, adding 2- 3% in mAP. </p></li></ul></li><li><p>Limitations：</p><ul><li>strong spatial constraints：decided by the settings of bounding boxes</li><li><p>softmax classification：can only have one class for each grid </p><blockquote><p>“This spatial constraint lim- its the number of nearby objects that our model can pre- dict. Our model struggles with small objects that appear in groups, such as flocks of birds. “</p><p>“ It struggles to generalize to objects in new or unusual aspect ratios or configurations. “</p></blockquote></li><li><p>coarse bounding box prediction：the architecture has multiple downsampling layers</p></li><li><p>the loss function treats errors the same in small bounding boxes versus large bounding boxes：</p><blockquote><p>The same error has much greater effect on a small box’s IOU than a big box.</p><p>“Our main source of error is incorrect localizations. “</p></blockquote></li></ul></li><li><p>Comparison：</p><ul><li>mAP among <strong>real-time detectors</strong> and <strong>Less Than Real-Time detectors</strong>：less mAP than fast-rcnn but much faster</li><li>error analysis between yolo and fast-rcnn：greater localization error and less background false-positive</li><li>combination analysis：[fast-rcnn+yolo] defeats [fast-rcnn+fast-rcnn] since YOLO makes different kinds of mistakes  with fast-rcnn</li><li>generalizability：RCNN degrades more because the Selective Search is tuned for natural images, change of dataset makes the proposals get worse. YOLO degrades less because it models the size and shape of objects, change of dataset varies less at object level but more at pixel level.</li></ul></li></ol><h2 id="2-Yolov2-YOLO9000-Better-Faster-Stronger"><a href="#2-Yolov2-YOLO9000-Better-Faster-Stronger" class="headerlink" title="2. Yolov2: YOLO9000: Better, Faster, Stronger"></a>2. Yolov2: YOLO9000: Better, Faster, Stronger</h2><ol><li>动机：<ul><li>run at varying sizes：offering an easy tradeoff between speed and accuracy</li><li>recognize a wide variety of objects ：jointly train on object detection and classification, so that the model can predict objects that aren’t labelled in detection data</li><li>better performance but still fast</li></ul></li><li><p>论点：</p><ul><li>Current object detection datasets are limited compared to classification datasets  <ul><li>leverage the classification data to expand the scope of current detection system</li><li>joint training algorithm making the object detectors working on both detection and classification data</li></ul></li><li><strong>Better performance</strong> often hinges on larger networks or ensembling multiple models. However we want a more accurate detector that is <strong>still fast</strong></li><li>YOLOv1’s shortcomings<ul><li>more localization errors</li><li>low recall</li></ul></li></ul></li><li><p>要素：</p><ol><li><p>better</p><p> <img src="/2019/11/28/yolo系列/better.png" width="70%;"></p></li><li><p>faster</p><ul><li>backbone</li></ul></li><li><p>stronger</p><ul><li><p>uses labeled detection images to learn to precisely localize objects </p></li><li><p>uses classification images to increase its vocabulary and robustness</p></li></ul></li></ol></li><li><p>方法：</p><ol><li><p>better：</p><ol><li><p>batch normalization：convergence &amp; regularization</p><blockquote><p>add batch normalization on all of the convolutional layers </p><p>remove dropout from the model </p></blockquote></li><li><p>high resolution classifier：pretrain a hi-res classifier </p><blockquote><p>first fine tune the classification network at the full 448 × 448 resolution for 10 epochs on ImageNet</p><p>then fine tune the resulting network on detection </p></blockquote></li><li><p>convolutional with anchor boxes：</p><p>YOLOv1通过网络最后的<strong>全连接层</strong>，直接预测每个grid上bounding box的坐标</p><p>而RPN基于先验框，使用最后一层<strong>卷积层</strong>，在特征图的各位置预测bounding box的offset和confidence</p><blockquote><p> “Predicting offsets instead of coordinates simplifies the problem and makes it easier for the network to learn”</p></blockquote><p>YOLOv2去掉了全连接层，也使用anchor box来回归bounding box</p><blockquote><p>eliminate one pooling layer to make the network output have higher resolution</p><p>shrink the network input to 416<em>416 to obtain an odd number so that there is a <em>*single center cell</em></em> in the feature map</p><p>predict class and objectness for every anchor box(offset prediction) instead of nothing(direct location&amp;scale prediction)</p></blockquote></li><li><p>dimension clustering：</p><p>what we want are priors that lead to good IOU scores, thus comes the distance metric：</p><script type="math/tex; mode=display">d(box, centroid) = 1 - IOU(box, centroid)</script></li><li><p>direct location prediction：</p><p>YOLOv1 encounter model instability issue for predicting the (x, y) locations for the box</p><p>RPN also takes a long time to stabilize  by predicting a (tx, ty) and obtain the (x, y) center coordinates indirectly because this formulation is unconstrained so any anchor box can end up at any point in the image：</p><script type="math/tex; mode=display">x = t_x * w_a - x_a\\y = t_y * h_a - y_a</script><blockquote><p>学习RPN：回归一个相对量，比盲猜回归一个绝对location（YOLOv1）更好学习</p><p>学习YOLOv1：基于cell的预测，将bounding box限定在有限区域，不是全图飞（RPN）</p></blockquote><p>YOLOv2对每个cell，基于5个prior anchor size，预测5个bounding box，每个bounding box具有5维：</p><script type="math/tex; mode=display">b_x = \sigma(t_x) + c_x\\b_y = \sigma(t_y) + c_y\\b_w = p_w e^{t_w}\\b_h = p_h e^{t_h}\\Pr(object)*IOU(b,object) = \sigma(t_o)</script><ul><li>$t_x\ \&amp;\ t_y$用于回归bounding box的位置，通过sigmoid激活函数被限定在0-1，通过上式能够间接得到bounding box的归一化位置（相对原图）</li><li>$t_w\ \&amp;\ t_h$用于回归bounding box的尺度，输出应该不是0-1限定，$p_w\ \&amp;\ p_h$是先验框的归一化尺度，通过上式能够间接得到bounding box的归一化尺度（相对原图）</li><li>$t_o$用于回归objectness，通过sigmoid限定在0-1之间，因为$Pr(object)\ \&amp;\ IOU(b,object)$都是0-1之间的值，IOU通过前面四个值能够求解，进而可以解耦objectness</li></ul><p><img src="/2019/11/28/yolo系列/regression.png" width="40%;"></p></li><li><p>fine-grained features：</p><p>motive：小物体的检测依赖更加细粒度的特征</p><p>cascade：Faster R-CNN and SSD both run their proposal networks at various feature maps in the network to get a range of resolutions</p><p>【QUESTION】YOLOv2 simply adds a passthrough layer from an earlier layer at 26 × 26 resolution：</p><blockquote><p>latter featuremap —-&gt; upsampling</p><p>concatenate with early featuremap</p><p>the detector runs on top of this expanded feature map </p></blockquote><p>predicts a $N<em>N</em>(3*(4+1+80))$ tensor for each scale</p></li><li><p>multi-scale training：</p><p>模型本身不限定输入尺寸：model only uses convolutional and pooling layers thus it can be resized on the fly </p></li></ol><ul><li>forces the network to learn to predict well across a variety of input dimensions <ul><li>the same network can predict detections at different resolutions</li></ul></li></ul><ol><li><p>loss：<strong>cited from the latter yolov3 paper</strong></p><ul><li>use sum of squared error loss for box coordinate(x,y,w,h)：then the gradient is $y_{true} - y_{pred}$</li></ul></li></ol><ul><li>use logistic regression for objectness score：which should be 1 if the bounding box prior overlaps a ground truth object by more than any other bounding box prior<ul><li>if a bounding box prior is not assigned to a ground truth object it incurs no loss(coordinate&amp;objectness)</li></ul></li><li>use binary cross-entropy loss for multilabel classification</li></ul></li></ol></li><li><p>faster：</p><ol><li><p>darknet-19：</p><p> YOLOv1中讨论过换VGG-16和YOLOv1使用的backbone对比，前者有map提升，但是耗时。</p><p> YOLOv2的新backbone，参数更少，而且相对于VGG16在ImageNet上精度更高。</p><p> <img src="/2019/11/28/yolo系列/darknet19.png" width="40%;"></p></li><li><p>training for classification：</p><pre><code> * first train on ImageNet using 224*224 * then fine-tuning on 448*448</code></pre><ol><li><p>training for detection：</p><ul><li><p>remove the last convolutional layer</p></li><li><p>add on three 3 × 3 convolutional layers with 1024 filters each followed by a final 1×1 convolutional layer with the number of outputs we need for detection </p></li><li>add a passthrough from the final 3 × 3 × 512 layer to the second to last convolutional layer so that our model can use fine grain features. </li></ul></li></ol></li><li><p>stronger：</p><p> jointly training：以后再填坑</p><ul><li>构造标签树</li><li>classification sample用cls loss，detection sample用detect loss</li><li>预测正确的classification sample给一个.3 IOU的假设值用于计算objectness loss</li></ul></li></ol></li></ol><h2 id="3-Yolov3-An-Incremental-Improvement"><a href="#3-Yolov3-An-Incremental-Improvement" class="headerlink" title="3. Yolov3: An Incremental Improvement"></a>3. Yolov3: An Incremental Improvement</h2><ol><li><p>动机：</p><p> nothing like super interesting, just a bunch of small changes that make it better</p></li><li><p>方法：</p><ol><li><p>bounding box prediction：</p><p> use anchor boxes and predicts offsets for each bounding box</p><p> use sum of squared error loss for training</p><p> predicts the objectness score for each bounding box using logistic regression</p><p> one ground truth coresponds to one best box and one loss</p></li><li><p>class prediction：</p><p> use binary cross-entropy loss for multilabel classification</p></li><li><p><strong>【NEW】prediction across scales：</strong></p><p> the detector：a few more convolutional layers following the feature map, the last of which predicts a 3-d(for 3 priors) tensor encoding bounding box, objectness, and class predictions</p><p> expanded feature map：upsampling the deeper feature map by 2X and concatenating with the former features</p><blockquote><p>“With the new multi-scale predictions, YOLOv3 has better perfomance on small objects and comparatively worse performance on medium and larger size objects “</p></blockquote></li><li><p><strong>【NEW】feature extractor：</strong></p><p> darknet-53 !</p><p> <img src="/2019/11/28/yolo系列/darknet53.png" width="40%;"></p></li><li><p>training：common skills</p></li></ol></li></ol><h2 id="4-一些补充"><a href="#4-一些补充" class="headerlink" title="4. 一些补充"></a>4. 一些补充</h2><ol><li><p>metrics：mAP</p><p> 最早由PASCAL VOC提出，输出结果是一个ranked list，每一项包含框、confidence、class，</p><p> yolov3提到了一个“COCOs weird average mean AP metric ”</p><ul><li><p>IoU：预测框与ground truth的交并比，也被称为Jaccard指数，我们通常用其来判断每个检测的正确性。PASCAL VOC数据集用0.5为阈值来判定预测框是True Positive还是False Positive，COCO数据集则建议对不同的IoU阈值进行计算。</p><p><img src="/2019/11/28/yolo系列/iou.png" width="20%;"></p></li><li><p>置信度：通过改变置信度阈值，我们可以改变一个预测框是Positive还是 Negative。</p></li><li><p>precision &amp; recall：precision = TP ／(TP + FP)，recall = TP／(TP + FN)。图片中我们没有预测到的每个部分都是Negative，因此计算True Negatives比较难办。但是我们只需要计算False Negatives，即我们模型所漏检的物体。</p><p>  <img src="/2019/11/28/yolo系列/pr.png" width="30%;"></p></li><li><p>AP：不同的置信度下会得到不同的precision-recall。为了得到precision-recall曲线，首先对模型预测结果进行排序，按照各个预测值置信度降序排列。给定不同的置信度阈值，就有不同的ranked output，Recall和Precision仅在高于该rank值的预测结果中计算。这里共选择11个不同的recall（[0, 0.1, …, 0.9, 1.0]），那么AP就定义为在这11个recall下precision的平均值，其可以表征整个precision-recall曲线（曲线下面积）。给定recall下的precision计算，是通过一种插值的方式：</p><script type="math/tex; mode=display">  AP = \frac{1}{11}\sum_{r\in\{0,0.1,...,1.0\}}p_{interp}(r) \\  p_{interp}(r) = max_{\tilde r: \tilde r > r} p(\tilde r)</script></li><li><p>mAP：此度量指标在<strong>信息检索</strong>和<strong>目标检测</strong>领域有不同的计算方式。对于目标检测，对于各个类别，分别按照上述方式计算AP，取所有类别的AP平均值就是mAP。</p><p>  <img src="/2019/11/28/yolo系列/map.png" width="40%;"></p></li></ul></li><li><p>eval：</p><ol><li>yolo_head输出：box_xy是box的中心坐标，(0~1)相对值；box_wh是box的宽高，(0~1)相对值；box_confidence是框中物体置信度；box_class_probs是类别置信度；</li><li>yolo_correct_boxes函数：能够将box中心的相对信息转换成[y_min,x_min,y_max,x_max]的绝对值</li><li>yolo_boxes_and_scores函数：输出网络预测的所有box</li><li>yolo_eval函数：基于score_threshold、max_boxes两项过滤，类内NMS，得到最终输出</li></ol></li></ol><h2 id="4-YOLOv4-Optimal-Speed-and-Accuracy-of-Object-Detection"><a href="#4-YOLOv4-Optimal-Speed-and-Accuracy-of-Object-Detection" class="headerlink" title="4. YOLOv4: Optimal Speed and Accuracy of Object Detection"></a>4. YOLOv4: Optimal Speed and Accuracy of Object Detection</h2><ol><li><p>动机</p><ul><li>Practical testing the tricks of improving CNN</li><li>some features<ul><li>work for certain problems/dataset exclusively</li><li>applicable to the majority of models, tasks, and datasets</li><li>only increase the training cost  [bag-of-freebies]</li><li>only increase the inference cost by a small amount but can significantly improve the accuracy  [bag-of-specials]</li></ul></li><li>Optimal Speed and Accuracy </li></ul></li><li><p>论点</p><ul><li>head：<ul><li>predict classes and bounding boxes </li><li>one-stage head<ul><li>YOLO, SSD, RetinaNet </li><li>anchor-free：CenterNet, CornerNet, FCOS  </li></ul></li><li>two-stage head<ul><li>R-CNN series </li><li>anchor-free：RepPoints </li></ul></li></ul></li><li>neck：<ul><li>collect feature maps from different stages </li><li>FPN, PAN, BiFPN, NAS-FPN </li></ul></li><li><p>backbone：</p><ul><li>pre-trained on ImageNet</li><li>VGG, ResNet, ResNeXt, DenseNet</li></ul><p><img src="/2019/11/28/yolo系列/detector.png" width="70%;"></p></li><li><p>Bag of freebies</p><ul><li>data augmentation <ul><li>pixel-wise adjustments<ul><li>photometric distortions：brightness, contrast, hue, saturation, and noise </li><li>geometric distortions：random scaling, cropping, flipping, and rotating </li></ul></li><li>object-wise<ul><li>cut：<ul><li>to image：CutOut </li><li>to featuremaps：DropOut, DropConnect, DropBlock </li></ul></li><li>add：MixUp, CutMix, GAN </li></ul></li></ul></li><li>data imbalance for classification<ul><li>two-stage：hard example mining </li><li>one-stage：focal loss, soft label  </li></ul></li><li>bounding box regression <ul><li>MSE-regression：treat [x,y,w,h] as independent variables</li><li>IoU loss：consider the integrity &amp; scale invariant </li></ul></li></ul></li><li>Bag of specials <ul><li>enlarging receptive field：improved SPP, ASPP, RFB</li><li>introducing attention mechanism <ul><li>channel-wise attention：SE, increase the inference time by about 10%</li><li>point-wise attention：Spatial Attention Module (SAM), does not affect the speed of inference</li></ul></li><li>strengthening feature integration<ul><li>channel-wise level：SFAM</li><li>point-wise level：ASFF </li><li>scale-wise level：BiFPN </li></ul></li><li>activation function：A good activation function can make the gradient more efficiently propagated</li><li>post-processing：各种NMS</li></ul></li></ul></li><li><p>方法</p><ul><li><p>choose a backbone —- CSPDarknet53 </p><ul><li>Higher input network size (resolution) – for detecting multiple small-sized objects </li><li>More conv layers – for a higher receptive field to cover the increased size of input network </li><li>More parameters – for greater capacity of a model to detect multiple objects of different sizes in a single image </li></ul></li><li><p>add the SPP block over the CSPDarknet53 </p><ul><li>significantly increases the receptive field </li><li>separates out the most significant context features  </li><li>causes almost no re- duction of the network operation speed</li></ul></li><li><p>use PANet as the method of parameter aggregation </p><ul><li>Modified PAN</li><li><p>replace shortcut connection of PAN to concatenation </p><p><img src="/2019/11/28/yolo系列/pan.png" width="30%;"></p></li></ul></li><li><p>use YOLOv3 (anchor based) head </p></li><li><p>Mosaic data augmentation</p><ul><li>mixes 4 training images </li><li>allows detection of objects outside their normal context</li><li><p>reduces the need for a large mini-batch size </p><p><img src="/2019/11/28/yolo系列/mosaic.png" width="40%;"></p></li></ul></li><li><p>Self-Adversarial Training (SAT) data augmentation</p><ul><li>1st stage alters images</li><li>2nd stage train on the modified images</li></ul></li><li><p>CmBN：a CBN modified version </p><p>  <img src="/2019/11/28/yolo系列/cmbn.png" width="40%;"></p></li><li><p>modified SAM：from spatial-wise attention to point- wise attention </p><p>  <img src="/2019/11/28/yolo系列/sam.png" width="35%;"></p></li></ul></li><li><p>实验</p><ul><li><p>Influence of different features on Classifier training </p><ul><li><p>Bluring和Swish没有提升</p><p><img src="/2019/11/28/yolo系列/cls.png" width="35%;"></p></li></ul></li><li><p>Influence of different features on Detector training </p><ul><li>IoU threshold, CmBN, Cosine annealing sheduler, CIOU有提升</li></ul></li></ul></li></ol><h2 id="POLY-YOLO-HIGHER-SPEED-MORE-PRECISE-DETECTION-AND-INSTANCE-SEGMENTATION-FOR-YOLOV3"><a href="#POLY-YOLO-HIGHER-SPEED-MORE-PRECISE-DETECTION-AND-INSTANCE-SEGMENTATION-FOR-YOLOV3" class="headerlink" title="POLY-YOLO: HIGHER SPEED, MORE PRECISE DETECTION AND INSTANCE SEGMENTATION FOR YOLOV3"></a>POLY-YOLO: HIGHER SPEED, MORE PRECISE DETECTION AND INSTANCE SEGMENTATION FOR YOLOV3</h2><ol><li><p>动机</p><ul><li><p>yoloV3’s weakness</p><ul><li>rewritten labels </li><li>inefficient distribution of anchors </li></ul></li><li><p>light backbone：</p><ul><li>stairstep upsampling </li></ul></li><li>single scale output </li><li>to extend instance segmentation <ul><li>detect size-independent polygons defined on a polar grid </li><li>real-time processing </li></ul></li></ul></li><li><p>论点</p><ul><li><p>yolov3</p><ul><li>real-time</li><li>low precision cmp with RetinaNet, EfficientDet <ul><li>low precision of the detection of big boxes </li><li>rewriting of labels by each-other due to the coarse resolution </li></ul></li></ul></li><li><p>this paper solution：</p><ul><li>解决yolo精度问题：propose a brand-new feature decoder with a single ouput tensor that goes to head with higher resolution</li><li>多尺度特征融合：utilize stairstep upscaling</li><li>实例分割：bounding polygon within a poly grid</li></ul></li><li><p>instance segmentation</p><ul><li>two-stage：mask-rcnn</li><li>one-stage：<ul><li>top-down：segmenting this object within a bounding box</li><li>bottom-up：start with clustering pixels</li><li>direct methods：既不需要bounding box也不需要clustered pixels，PolarMask </li></ul></li></ul></li><li><p>cmp with PolarMask </p><ul><li>size-independent：尺度，大小目标都能检测</li><li>dynamic number of vertices：多边形定点可变</li></ul></li><li><p>yolov3 issues</p><ul><li><p>rewriting of labels：</p><ul><li>两个目标如果落在同一个格子里，在一个尺度上ground truth label只会保留一个box</li><li><p>对越小的特征图，grid越大，这个问题越严重</p><p><img src="/2019/11/28/yolo系列/rewriting.png" width="90%;"></p></li></ul></li><li><p>imbalanced distribution of anchors across output scales </p><ul><li>anchor如果选的不合理，会导致特征图尺度和anchor尺度不匹配</li><li>most of the boxes will be captured by the middle output layer and the two other layers will be underused</li><li>如上面车的case，大多数车的框很小，聚类出的给level0和level1的anchor shape还是很小，但是level0是稀疏grid<ul><li>一方面，grid shape和anchor shape不匹配</li><li>一方面，label rewriten问题会升级</li></ul></li><li>反过来，如果dense grid上预测大目标，会受到感受野的制约</li><li>一种解决方案是基于感受野首先对gt box分成三组，然后分别聚类，然后9选1</li></ul></li><li><p>yolov3原文：YOLOv3 has relatively high $AP_{small}$ performance. However, it has comparatively worse performance on medium and larger size objects. More investigation is needed to get to the bottom of this.</p><ul><li>小目标performance更好，大目标worse，主要是就是因为coarse grid上存在label rewriten问题，存在部分gt box被抑制掉了。</li></ul></li></ul></li></ul></li><li><p>方法</p><ul><li><p>architecture </p><p>  <img src="/2019/11/28/yolo系列/poly-yolo.png" width="80%;"></p><ul><li><p>single output  </p></li><li><p>higher resolution：stride4</p></li><li><p>handle all the anchors at once </p></li><li><p>cross-scale fusion</p><ul><li>hypercolumn technique：add operation</li><li><p>stairstep interpolation：x2 x2 …</p><p><img src="/2019/11/28/yolo系列/hypercolumn.png" width="80%;"></p></li></ul></li><li><p>SE-blocks</p></li><li>reduced the number of convolutional filters to 75%  in the feature extraction phase </li></ul></li><li><p>bounding polygons </p><ul><li>extend the box tuple：$b_i=\{b_i^{x^1},b_i^{y^1},b_i^{x^2},b_i^{y^2},V_i\}$</li><li>The center of a bounding box is used as the origin </li><li>polygon tuple：$v_{i,j}=\{\alpha_{i,j},\beta_{i,j},\gamma_{i,j}\}$</li><li>polar coordinate：distance &amp; oriented angle，相对距离（相对anchor box的对角线），相对角度（norm到[0,1]）</li><li><p>polar cell：一定角度的扇形区域 内，如果sector内没有定点，conf=0</p><p><img src="/2019/11/28/yolo系列/polar.png" width="80%;"></p></li><li><p>general shape：</p><ul><li>不同尺度，形状相同的object，在polar coord下表示是一样的</li><li>distance*anchor box的对角线，转换成绝对尺度</li><li>bounding box的两个对角预测，负责尺度估计，polygon只负责预测形状</li><li>sharing values should make the learning easier</li></ul></li></ul></li><li><p>mix loss</p><ul><li>output：a*(4+1+3*n_vmax)</li><li>box center loss：bce</li><li>box wh loss：l2 loss</li><li>conf loss：bce with ignore mask</li><li>cls loss：bce</li><li>polygon loss：$\gamma<em>(log(\frac{\alpha}{anchor^d})-\hat a)^2 + \gamma</em>bce(\beta,\hat{beta})+bce(\gamma, \hat \gamma)$</li><li>auxiliary task learning：<ul><li>任务间相互boost</li><li>converge faster </li></ul></li></ul></li></ul></li></ol><h2 id="Scaled-YOLOv4-Scaling-Cross-Stage-Partial-Network"><a href="#Scaled-YOLOv4-Scaling-Cross-Stage-Partial-Network" class="headerlink" title="Scaled-YOLOv4: Scaling Cross Stage Partial Network"></a>Scaled-YOLOv4: Scaling Cross Stage Partial Network</h2><ol><li><p>动机</p><ul><li>model scaling method</li><li>redesign yolov4 and propose yolov4-CSP</li><li>develop scaled yolov4<ul><li>yolov4-tiny</li><li>yolov4-large</li></ul></li><li>没什么技术细节，就是网络结构大更新</li></ul></li><li><p>论点</p><ul><li><p>common technique changes depth &amp; width of the backbone</p></li><li><p>recently there are NAS</p></li><li><p>model scaling</p><ul><li><p>input size、width、depth对网络计算量呈现square, linear, and square increase</p><p><img src="/2019/11/28/yolo系列/flop.png" width="50%;"></p></li><li><p>改成CSP版本以后，能够减少参数量、计算量，提高acc，缩短inference time</p><p><img src="/2019/11/28/yolo系列/CSPflop.png" width="50%;"></p></li><li><p>检测的准确性高度依赖reception field，RF随着depth线性增长，随着stride倍数增长，所以一般先组合调节input size和stage，然后再根据算力调整depth和width</p><p>  <img src="/2019/11/28/yolo系列/reception.png" width="50%;"></p></li></ul></li></ul></li><li><p>方法</p><ul><li><p>backbone：CSPDarknet53 </p></li><li><p>neck：CSP-PAN，减少40%计算量，SPP</p><p>  <img src="/2019/11/28/yolo系列/CSPpan.png" width="50%;"></p></li><li><p>yoloV4-tiny</p><p>  <img src="/2019/11/28/yolo系列/tiny.png" width="40%;"></p></li><li><p>yoloV4-large：P456</p><p>  <img src="/2019/11/28/yolo系列/large.png" width="80%;"></p></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 目标检测，one-stage </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>triplet-center-loss论文</title>
      <link href="/2019/11/13/triplet-center-loss%E8%AE%BA%E6%96%87/"/>
      <url>/2019/11/13/triplet-center-loss%E8%AE%BA%E6%96%87/</url>
      <content type="html"><![CDATA[<h2 id="0-before-reading"><a href="#0-before-reading" class="headerlink" title="0. before reading"></a>0. before reading</h2><p>结合：</p><ul><li><p>triplet loss：考虑类间关系，但计算复杂度高，困难样本难挖掘</p></li><li><p>center loss：考虑类内关系</p></li><li><p>TCL：同时增加类内数据的紧实度（compactness）和类间的分离度（separability）</p><p>  三元组只考虑样本、所属类中心、最近邻类的中心。避免了建立triplets的复杂度和mining hard samples的难度。</p></li></ul><h2 id="title：Triplet-Center-Loss-for-Multi-View-3D-Object-Retrieval"><a href="#title：Triplet-Center-Loss-for-Multi-View-3D-Object-Retrieval" class="headerlink" title="title：Triplet-Center Loss for Multi-View 3D Object Retrieval"></a>title：Triplet-Center Loss for Multi-View 3D Object Retrieval</h2><ol><li><p>动机：<em>deep metric learning</em> </p><ul><li><p>the learned features using softmax loss are not discriminative enough in nature </p></li><li><p>although samples of the two classes are separated by the decision boundary elaborately, there exists significant intra-class variations</p></li><li><p><strong>QUESTION1</strong>：so what? how does this affect the current task? 动机描述不充分。</p></li><li><p><strong>QUESTION2</strong>：在二维平面上overlap不代表在高维空间中overlap，这种illustration究竟是否有意义。</p></li><li><p><strong>ANSWER for above</strong>：高维空间可分，投影到二维平面不一定可分，但是反过来，二维平面上高度可分，映射会高维空间数据仍旧是高度可分的。只能说，后者能够确保不同类别数据离散性更好，不能说明前者数据离散性不好（如果定义了高维距离，也可以说明）。</p><p> <img src="/2019/11/13/triplet-center-loss论文/distance.png" width="80%"></p></li></ul></li><li><p>应用场景：<em>3D object retrieval</em> </p></li><li><p>要素：</p><ul><li>learns a center for each class </li><li>requires that the distances between samples and centers from the same class are smaller than those from different classes, in this way the samples are pulled closer to the corresponding center and meanwhile pushed away from the  different centers</li><li>both the inter-class separability  and the intra-class variations are considered</li></ul></li><li><p>论点：</p><ul><li><p>Compared with triplet loss, TCL avoids the complex construction of triplets and hard sample mining mechanism. </p></li><li><p>Compared with center loss, TCL not only considers to reduce the intra-class variations.</p></li><li><p><strong>QUESTION</strong>：what about the comparison with [softmax loss + center loss]?</p></li><li><p><strong>ANSWER for above</strong>：center-loss is actually representing for the joint loss [softmax loss + center loss]. </p><p>  ‘’Since the class centers are updated at each iteration based on a mini-batch instead of the whole dataset, which can be very unstable, it has to be under the joint supervision of softmax loss during training. ‘’</p></li></ul></li><li><p>本文做法：</p><ul><li>the proposed TCL is used as the supervision loss</li><li>the softmax loss could be also combined in as an addition</li></ul></li><li><p>细节：</p><ul><li><p>TCL：</p><script type="math/tex; mode=display">  L_{tc} = \sum_{i=1}^Mmax(D(f_i, c_{y^i}) + m - min_{j\neq y^i}D(f_i, c_j), 0)</script><p>  前半部分是center-loss，类内欧几里得距离，后半部分是每个样本和与其最近的negative center之间的距离。</p></li><li><p>‘Unlike center loss, TCL can be used independently from softmax loss. <strong>However</strong>… ‘</p><p>  作者解释说，因为center layer是随机初始化出来的，而且是batch updating，因此开始阶段会比较tricky，’while softmax loss could serve as a good guider for seeking better class centers ‘</p></li><li><p>调参中提到’m is fixed to 5’，说明本文对feature vector没有做normalization（相比之下facenet做了归一化，限定所有embedding分布在高维球面上）。</p></li><li><p>衡量指标：AUC和MAP，这是一个retrieval任务，最终需要的是embedding，给定Query，召回top matches。</p></li></ul></li><li><p>reviews：</p><ul><li>个人理解：<ol><li>softmax分类器旨在数据可分，对于分类边界、feature vector的空间意义不存在一个具象的描述。deep metric learning能够引入这种具象的、图像学的意义，在此基础上，探讨distance、center才有意义。</li><li>就封闭类数据（类别有限且已知）分类来讲，分类边界有无图像学描述其实意义不大。已知的数据分布尽可能discriminative的主要意义是针对未知类别，我们希望给到模型一个<strong>未知数据</strong>时，它能够检测出来，而不是划入某个已知类（softmax）。</li><li>TCL的最大贡献应该是想到用center替代样本来进行metric judgement，改善triplet-loss复杂计算量这一问题，后者实际训起来太难了，没有感情的GPU吞噬机器。</li></ol></li><li>XXX：</li><li></li></ul></li></ol><p>能够引入这种具象的、图像学的意义，在此基础上，我们探讨distance、center才有意义。</p><p>​        </p>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>dicomReader</title>
      <link href="/2019/11/11/dicomReader/"/>
      <url>/2019/11/11/dicomReader/</url>
      <content type="html"><![CDATA[<ol><li><p>read a dcm file</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> SimpleITK <span class="keyword">as</span> sitk</span><br><span class="line"></span><br><span class="line">image = sitk.ReadImage(dcm_file)</span><br><span class="line">image_arr = sitk.GetArrayFromImage(image)</span><br></pre></td></tr></table></figure></li><li><p>read a dcm series</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">series_IDs = sitk.ImageSeriesReader.GetGDCMSeriesIDs(series_path)</span><br><span class="line"></span><br><span class="line">nb_series = len(series_IDs)</span><br><span class="line">print(nb_series)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认获取第一个序列的所有切片路径</span></span><br><span class="line">dicom_names = sitk.ImageSeriesReader.GetGDCMSeriesFileNames(file_path)</span><br><span class="line">series_reader = sitk.ImageSeriesReader()</span><br><span class="line">series_reader.SetFileNames(dicom_names)</span><br><span class="line">image3D = series_reader.Execute()</span><br></pre></td></tr></table></figure></li><li><p>read a dcm case</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">series_IDs = sitk.ImageSeriesReader.GetGDCMSeriesIDs(case_path)</span><br><span class="line"><span class="keyword">for</span> series_id <span class="keyword">in</span> series_IDs:</span><br><span class="line">    dicom_names = sitk.ImageSeriesReader.GetGDCMSeriesFileNames(case_path, series_id)</span><br><span class="line">    series_reader = sitk.ImageSeriesReader()</span><br><span class="line">    series_reader.SetFileNames(dicom_names)</span><br><span class="line">    image3D = series_reader.Execute()</span><br></pre></td></tr></table></figure></li><li><p>read tag</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先得到image对象</span></span><br><span class="line">Image_type = image.GetMetaData(<span class="string">"0008|0008"</span>) <span class="keyword">if</span> image.HasMetaData(<span class="string">"0008|0008"</span>) <span class="keyword">else</span> <span class="string">'Nan'</span></span><br></pre></td></tr></table></figure></li><li><p>发现一种序列，每张图的尺寸不同，这样执行series_reader的时候会报错，因为series_reader会依照第一层的图像尺寸申请空间，所以要么异常要么逐张读。</p><p> reference: <a href="http://itk-users.7.n7.nabble.com/ITK-users-Reader-InvalidRequestedRegionError-td38608.html" target="_blank" rel="noopener">http://itk-users.7.n7.nabble.com/ITK-users-Reader-InvalidRequestedRegionError-td38608.html</a></p></li></ol>]]></content>
      
      
        <tags>
            
            <tag> SimpleITK </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>c++ tricks in engineering</title>
      <link href="/2019/11/06/c-tricks-in-engineering/"/>
      <url>/2019/11/06/c-tricks-in-engineering/</url>
      <content type="html"><![CDATA[<ol><li><p>数组传参</p><p> 工程化被坑了好多回！</p><p> C/C++ 传递数组，虽然传递的是首地址地址，但是<strong>参数到了函数内就成了普通指针</strong>。</p><p> 所以试图在调用函数中求取所传递数组的长度是行不通的。</p></li><li><p>vector传参</p><p> 传值—&gt;拷贝构造，传引用／指针—&gt;不发生拷贝构造。</p><p> 实际工程化中遇到的问题是，构建了一个vector\<cv::mat\> imgs对象，传入函数以后，在函数内部创建空间cv::Mat img，然后将img push进vector。在函数外读取该vector的时候发现其内部没值。</cv::mat\></p><p> <strong>要点：1. 要传引用，2. push clone：imgs.push_back(img)</strong></p><p> 另外，vector可以作为函数返回值。</p></li><li></li></ol>]]></content>
      
      
        <tags>
            
            <tag> c++ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>图像算法综述</title>
      <link href="/2019/10/31/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0/"/>
      <url>/2019/10/31/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0/</url>
      <content type="html"><![CDATA[<ol><li><p>类别</p><ul><li>按照任务类型：度量学习（metric learning）和描述子学习（image descriptor learning）</li><li>按照网络结构：pairwise的siamese结构、triplet的three branch结构、以及引入尺度信息的central-surround结构</li><li>按照网络输出：特征向量（feature embedding）和单个概率值（pairwise similarity）</li><li>按照损失函数：对比损失函数、交叉熵损失函数、triplet loss、hinge loss等，此外损失函数可以带有隐式的<strong>困难样本挖掘</strong>，例如pn-net中的softpn等，也可以是显示的困难挖掘。</li></ul></li><li><p>Plain网络</p><p> 主要是说AlexNet／VGG-Net，后者更常用一些。</p><p> Plain网络的设计主要遵循以下几个准则：</p><p> （1）输出特征图尺寸相同的层使用相同数量的滤波器。</p><p> （2）如果特征图尺寸减半，那么滤波器数量就加倍，从而保证每层的时间复杂度相同（这是为啥？？）。</p></li><li><p>名词</p><ul><li><p>感受野：卷积神经网络每一层输出的特征图上的像素点在<strong>原始图像</strong>上映射区域的大小。通俗的说，就是输入图像对这一层输出的神经元的影响有多大。</p><p>  感受野计算：由当前层向前推，需要的参数是kernel size和stride。</p><script type="math/tex; mode=display">  N\_RF = kernel\_size + (cur\_RF-1)*stride</script><p>  其中$cur_RF$是当前层（start from 1），$N_RF$、$kernel_size$、$stride$是上一层参数。</p></li><li><p>有效感受野：并不是感受野内所有像素对输出向量的贡献相同，在很多情况下感受野区域内像素的影响分布是高斯，有效感受野仅占理论感受野的一部分，且高斯分布从中心到边缘快速衰减。</p></li><li><p>感受野大小：</p><ul><li>小感受野：local，位置信息更准确</li></ul></li><li>大感受野：global，语义信息更丰富</li></ul></li></ol><ul><li><p>inception module：下图为其中一种。</p><p>意义：增加网络深度和宽度的同时，减少参数。结构中嵌入了多尺度信息，集成了多种不同感受野上的特征。</p><p><img src="/2019/10/31/图像算法综述/inception.png" width="50%;"></p></li><li><p>building block：左边这种，红色框框里面是一个block。</p><p>几个相同的building block堆叠为一层conv。在第一个building Block块中，输出特征图的尺寸下降一半（第一个卷积stride=2），剩余的building Block块输入输出尺寸是一样的。</p></li><li><p>bottleneck：右边这种，蓝色框框block。字面意思，瓶颈，形容输入输出维度差距较大。</p><p>第一个1*1负责降低维度，第二个1*1负责恢复维度，3*3层就处在一个输入／输出维度较小的瓶颈。</p><pre><code>左右两种结构时间复杂度相似。&lt;img src=&quot;图像算法综述/block.png&quot; width=&quot;30%;&quot; /&gt;&lt;img src=&quot;图像算法综述/ImageNet.png&quot; width=&quot;110%;&quot; /&gt;</code></pre><ul><li><p>top-1和top-5：top-1就是预测概率最大的类别，top-5则取最后预测概率的前五个，只要其中包含正确类别则认为预测正确。</p><p>使用top-5主要是因为ImageNet中很多图片中其实是包含多个物体的。    </p></li><li><p>accuracy、error rate、F1-score、sensitivity、specificity、precision、recall</p><ul><li>accuracy：总体准确率</li><li>precision：从结果角度，单一类别准确率</li><li>recall：从输入角度，预测类别真实为1的准确率</li><li>P-R曲线：选用不同阈值，precision-recall围成的曲线</li><li>AP：平均精度，P-R曲线围住的面积</li><li>F1-score：对于某个分类，综合了Precision和Recall的一个判断指标，因为选用不同阈值，precision-recall会随之变化，F1-score用于选出最佳阈值。</li><li>sensitivity：=recall</li><li>specificity：预测类别真实为0的准确率</li></ul><p>reference：<a href="https://zhuanlan.zhihu.com/p/33273532" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/33273532</a></p></li><li><p>trade-off：</p></li><li><p>FLOPS：每秒浮点运算次数是每秒所执行的浮点运算次数的简称，被用来估算电脑效能。</p></li><li><p>ROC、AUC、MAP：</p><ul><li>ROC：TPR和FPR围成的曲线</li><li>AUC：ROC围住的面积</li><li>mAP：所有类别AP的平均值</li></ul></li><li><p>梯度弥散：</p></li><li><p>“底层先收敛、高层再收敛”：</p></li><li><p>特征图：卷积层通过线性滤波器进行线性卷积运算，然后再接个非线性激活函数，最终生成特征图。</p></li><li><p>TTA test time augmentation：测试时增强，为原始图像造出多个不同版本，包括不同区域裁剪和更改缩放程度等，并将它们输入到模型中；然后对多个版本进行计算得到平均输出，作为图像的最终输出分数。</p></li><li><p>pooling mode: </p><ul><li>full mode：从filter和image刚开始相交开始卷积</li><li>same mode：当filter的中心和image的角重合时开始卷积，如果stride=1，那么输入输出尺寸相同</li><li>valid mode：当filter完全在image里面时开始卷积</li></ul><p><img src="/2019/10/31/图像算法综述/full.png" width="20%;"> <img src="/2019/10/31/图像算法综述/same.png" width="20%;"> <img src="/2019/10/31/图像算法综述/valid.png" width="20%;"></p></li><li><p><a href="https://zhangting2020.github.io/2018/05/30/Transform-Invariance/" target="_blank" rel="noopener">空间不变性</a>：</p><ul><li>平移不变性：不管输入如何平移，系统产生完全相同的响应，<strong>比如图像分类任务</strong>，图像中的目标不管被移动到图片的哪个位置，得到的结果（标签）应该是相同的</li><li><strong>平移同变性（translation equivariance）：</strong>系统在不同位置的工作原理相同，但它的响应随着目标位置的变化而变化，<strong>比如实例分割任务</strong>，目标如果被平移了，那么输出的实例掩码也相应变化</li><li>局部连接：每个神经元没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息</li><li>权值共享：对于这个图像上的所有位置，我们都能使用同样的学习特征</li><li>池化：<a href="https://www.zhihu.com/question/36980971" target="_blank" rel="noopener">通过消除非极大值，降低了上层的计算复杂度</a>。最大池化返回感受野中的最大值，如果最大值被移动了，但是仍然在这个感受野中，那么池化层也仍然会输出相同的最大值。</li><li>卷积和池化这两种操作<strong>共同</strong>提供了<strong>一些</strong>平移不变性，即使图像被平移，卷积保证仍然能检测到它的特征，池化则尽可能地保持一致的表达。</li><li>同理，所谓的CNN的尺度、旋转不变性，也是由于pooling操作，引入的微小形变的鲁棒性。</li></ul></li><li><p>模型大小与参数量：float32是4个字节，因此模型大小字节数=参数量×4</p></li></ul></li></ul><ol><li><p>训练技巧</p><ul><li><p>迁移学习：当数据集太小，无法用来训练一个足够好的神经网络，可以选择fine-tune一些预训练网络。使用时修改最后几层，降低学习率。</p><p>  keras中一些预训练权重下载地址：<a href="https://github.com/fchollet/deep-learning-models/releases/" target="_blank" rel="noopener">https://github.com/fchollet/deep-learning-models/releases/</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/24825503" target="_blank" rel="noopener">K-fold交叉验证</a>：</p><ol><li><p>我们不能将全部数据集用于训练——这样就没有数据来测试模型性能了</p></li><li><p>将数据集分割为training set 和 test set，衡量结果取决于数据集划分，training set和全集之间存在bias，不同test下结果variety很大</p></li><li><p>交叉验证Cross-Validation：</p><ul><li>极端情况LOOCV：全集N，每次取一个做test，其他做train，重复N次，得到N个模型，并计算N个test做平均</li><li>K-fold：全集切分成k份，每次取一个做test，其他做train，重复k次～</li><li>实验显示LOOCV和10-foldCV的结果很相近，后者计算成本明显减小</li><li>Bias-Variance Trade-Off：K越大，train set越接近全集，bias越小，但是每个train set之间相关性越大，<strong>而这种大相关性会导致最终的test error具有更大的Variance</strong></li></ul></li></ol></li></ul></li></ol><ol><li><p>分割</p><ul><li><p>实例分割&amp;语义分割</p><ul><li>instance segmentation：标记实例和语义, 不仅要分割出<code>人</code>这个类, 而且要分割出<code>这个人是谁</code>, 也就是具体的实例</li><li><p>semantic segmentation：只标记语义, 也就是说只分割出<code>人</code>这个类来</p><p><img src="/2019/10/31/图像算法综述/segmentation.png" width="60%;"></p></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Segmentation</title>
      <link href="/2019/08/22/Segmentation/"/>
      <url>/2019/08/22/Segmentation/</url>
      <content type="html"><![CDATA[<p>idea: </p><ol><li>CT图一般是单通道灰度图像，假如我将128张CT图堆叠在一起（即128通道的图像），然后用2D卷积（会考虑通道数128），这样和直接用3D卷积会有结果上的差别吗？</li><li>3d网络可以结合图像层间信息，能够保证隔层图像Mask之间的一个变化连续性，效果会比2d好。层间距大的图像，在预处理中会有插值。</li><li>3d网络因为显存的限制，一种处理方式是裁成3d patch作为输入，导致其感受野有限，通常只能专注于细节和局部特征，适合作为第二级网络用于对细节做精优化。一种处理方式是降采样，分割精度下降。</li><li>2.5D网络。</li></ol>]]></content>
      
      
    </entry>
    
    <entry>
      <title>keras note</title>
      <link href="/2019/08/14/keras-note/"/>
      <url>/2019/08/14/keras-note/</url>
      <content type="html"><![CDATA[<h3 id="1-keras-Lambda自定义层"><a href="#1-keras-Lambda自定义层" class="headerlink" title="1. keras Lambda自定义层"></a>1. keras Lambda自定义层</h3><p>官方文档：将任意表达式(function)封装为 Layer 对象。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Lambda(function, output_shape=<span class="keyword">None</span>, mask=<span class="keyword">None</span>, arguments=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure></p><ul><li>function: 需要封装的函数。 将输入张量作为第一个参数。</li><li>output_shape: 预期的函数输出尺寸。(使用 TensorFlow 时，可自动推理得到)</li><li>arguments: 可选的需要传递给函数的关键字参数。<strong>以字典形式传入。</strong></li></ul><p>几个栗子：</p><p>1.1 最简：使用匿名函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.add(Lambda(<span class="keyword">lambda</span> x: x ** <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">x = Lambda(<span class="keyword">lambda</span> image: K.image.resize_images(image, (target_size, target_size)))(inpt)</span><br></pre></td></tr></table></figure></p><p>其中，<strong>lambda</strong>是python的匿名函数，后面的[xx: xxxx]用来描述函数的表达形式，<br><strong>lambda xx: xxxx</strong>整体作为<strong>Lambda</strong>函数的function参数。</p><p>1.2 中级：通过字典传参，封装自定义函数，实现数据切片<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Lambda, Dense, Activation, Reshape, concatenate</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> plot_model</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">slice</span><span class="params">(x, index)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x[:, :, index]</span><br><span class="line"></span><br><span class="line">a = Input(shape=(<span class="number">4</span>,<span class="number">2</span>))</span><br><span class="line">x1 = Lambda(slice,output_shape=(<span class="number">4</span>,<span class="number">1</span>),arguments=&#123;<span class="string">'index'</span>:<span class="number">0</span>&#125;)(a)</span><br><span class="line">x2 = Lambda(slice,output_shape=(<span class="number">4</span>,<span class="number">1</span>),arguments=&#123;<span class="string">'index'</span>:<span class="number">1</span>&#125;)(a)</span><br><span class="line">x1 = Reshape((<span class="number">4</span>,<span class="number">1</span>,<span class="number">1</span>))(x1)</span><br><span class="line">x2 = Reshape((<span class="number">4</span>,<span class="number">1</span>,<span class="number">1</span>))(x2)</span><br><span class="line">output = concatenate([x1,x2])</span><br><span class="line">model = Model(a, output)</span><br><span class="line">plot_model(model, to_file=<span class="string">'model.png'</span>, show_shapes=<span class="keyword">True</span>, show_layer_names=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure></p><p>模型结构图如下：</p><p><img src="/2019/08/14/keras-note/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/keras-note/slice.png" alt="slice img" style="zoom:67%;"></p><p>1.3 高级：自定义损失函数<br>    step 1. 把y_true定义为一个输入<br>    step 2. 把loss写成一个层，作为网络的最终输出<br>    step 3. 在compile的时候，将loss设置为y_pred</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yolov3 train.py create_model:</span></span><br><span class="line">model_loss = Lambda(yolo_loss, output_shape=(<span class="number">1</span>,), name=<span class="string">'yolo_loss'</span>, </span><br><span class="line">                    arguments=&#123;<span class="string">'anchors'</span>: anchors, <span class="string">'num_classes'</span>: num_classes, <span class="string">'ignore_thresh'</span>: <span class="number">0.5</span>&#125;)(</span><br><span class="line">                    [*model_body.output, *y_true])</span><br><span class="line">model = Model([model_body.input, *y_true], model_loss)</span><br><span class="line">model.compile(optimizer=Adam(lr=<span class="number">1e-3</span>), loss=&#123;<span class="string">'yolo_loss'</span>: <span class="keyword">lambda</span> y_true, y_pred: y_pred&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># yolov3 model.py yolo_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">yolo_loss</span><span class="params">(args, anchors, num_classes, ignore_thresh=<span class="number">.5</span>, print_loss=False)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="2-keras-自定义loss"><a href="#2-keras-自定义loss" class="headerlink" title="2. keras 自定义loss"></a>2. keras 自定义loss</h3><p>补充1.3: 也可以不定义为网络层，直接调用自定义loss函数<br>参数：</p><ul><li>y_true: 真实标签，Theano/Tensorflow 张量。</li><li>y_pred: 预测值。和 y_true 相同尺寸的 Theano/TensorFlow 张量。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mycrossentropy</span><span class="params">(y_true, y_pred, e=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span>-e)*K.categorical_crossentropy(y_pred,y_true) + e*K.categorical_crossentropy(y_pred, K.ones_like(y_pred)/num_classes)</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,loss=mycrossentropy, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure></li></ul><p>带参数的自定义loss：</p><p>有时候我们计算loss的时候不只要用到y_true和y_pred，还想引入一些参数，但是keras限定构造loss函数时只能接收(y_true, y_pred)两个参数，如何优雅的传入参数？</p><p>优雅的解决方案如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># build model</span></span><br><span class="line">model = my_model()</span><br><span class="line"><span class="comment"># define loss func</span></span><br><span class="line">model_loss = dice_loss(smooth=<span class="number">1e-5</span>, thresh=<span class="number">0.5</span>)</span><br><span class="line">model.compile(loss=model_loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dice_loss</span><span class="params">(smooth, thresh)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">dice</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>-dice_coef(y_true, y_pred, smooth, thresh)</span><br><span class="line">  <span class="keyword">return</span> dice</span><br></pre></td></tr></table></figure><h3 id="3-keras-自定义metrics"><a href="#3-keras-自定义metrics" class="headerlink" title="3. keras 自定义metrics"></a>3. keras 自定义metrics</h3><p>model.compile里面除了loss还有一个metrics，用于模型性能评估<br>参数：</p><ul><li>y_true: 真实标签，Theano/Tensorflow 张量。</li><li>y_pred: 预测值。和 y_true 相同尺寸的 Theano/TensorFlow 张量。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">precision</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    <span class="comment"># Calculates the precision</span></span><br><span class="line">    true_positives = K.sum(K.round(K.clip(y_true * y_pred, <span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">    predicted_positives = K.sum(K.round(K.clip(y_pred, <span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">    precision = true_positives / (predicted_positives + K.epsilon())</span><br><span class="line">    <span class="keyword">return</span> precision</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recall</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    <span class="comment"># Calculates the recall</span></span><br><span class="line">    true_positives = K.sum(K.round(K.clip(y_true * y_pred, <span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">    possible_positives = K.sum(K.round(K.clip(y_true, <span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">    recall = true_positives / (possible_positives + K.epsilon())</span><br><span class="line">    <span class="keyword">return</span> recall</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,loss=mycrossentropy, metrics=[<span class="string">'accuracy'</span>, recall, precision])</span><br></pre></td></tr></table></figure></li></ul><h3 id="4-keras-自定义Layer"><a href="#4-keras-自定义Layer" class="headerlink" title="4. keras 自定义Layer"></a>4. keras 自定义Layer</h3><p>源代码：<a href="https://github.com/keras-team/keras/blob/master/keras/engine/base_layer.py" target="_blank" rel="noopener">https://github.com/keras-team/keras/blob/master/keras/engine/base_layer.py</a></p><p>自定义layer继承keras的Layer类，需要实现三个方法：</p><ul><li><code>build(input_shape)</code>：定义权重，调用add_weight来创建层的权重矩阵，其中有参数trainable声明该参数的权重是否为可训练权重，若trainable==True，会执行self._trainable_weights.append(weight)将该权重加入到可训练权重的lst中。</li><li><code>call(x)</code>：编写层逻辑</li><li><code>compute_output_shape(input_shape)</code>：定义张量形状的变化逻辑</li><li>get_config：返回一个字典，获取当前层的参数信息</li></ul><p>看了keras一些层的实现，keras中层（如conv、depthwise conv）的call函数基本都是通过调用tf.backend中的方法来实现</p><p>4.1 栗子：CenterLossLayer</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CenterLossLayer</span><span class="params">(Layer)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, alpha=<span class="number">0.5</span>, **kwargs)</span>:</span>     <span class="comment"># alpha：center update的学习率</span></span><br><span class="line">        super().__init__(**kwargs)</span><br><span class="line">        self.alpha = alpha</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">     <span class="comment"># add_weight：为该层创建一个可训练／不可训练的权重</span></span><br><span class="line">        self.centers = self.add_weight(name=<span class="string">'centers'</span>,</span><br><span class="line">                                       shape=(<span class="number">10</span>, <span class="number">2</span>),</span><br><span class="line">                                       initializer=<span class="string">'uniform'</span>,</span><br><span class="line">                                       trainable=<span class="keyword">False</span>)</span><br><span class="line">        <span class="comment"># 一定要在最后调用它</span></span><br><span class="line">        super().build(input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x, mask=None)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># x[0] is Nx2, x[1] is Nx10 onehot, self.centers is 10x2</span></span><br><span class="line">        delta_centers = K.dot(K.transpose(x[<span class="number">1</span>]), (K.dot(x[<span class="number">1</span>], self.centers) - x[<span class="number">0</span>]))  <span class="comment"># 10x2</span></span><br><span class="line">        center_counts = K.sum(K.transpose(x[<span class="number">1</span>]), axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>) + <span class="number">1</span>  <span class="comment"># 10x1</span></span><br><span class="line">        delta_centers /= center_counts</span><br><span class="line">        new_centers = self.centers - self.alpha * delta_centers</span><br><span class="line">        <span class="comment"># add_update：更新层内参数(build中定义的参数)的方法</span></span><br><span class="line">        self.add_update((self.centers, new_centers), x)</span><br><span class="line">        self.result = x[<span class="number">0</span>] - K.dot(x[<span class="number">1</span>], self.centers)</span><br><span class="line">        self.result = K.sum(self.result ** <span class="number">2</span>, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>) <span class="comment">#/ K.dot(x[1], center_counts)</span></span><br><span class="line">        <span class="keyword">return</span> self.result <span class="comment"># Nx1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_output_shape</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> K.int_shape(self.result)</span><br></pre></td></tr></table></figure><p>有一些自定义层，有时候会不实现compute_output_shape和get_config</p><ul><li>在call方法中，输出tensor如果发生了shape的变化，keras layer是不会自动推导出输出shape的，所以要显示的自定义compute_output_shape</li><li>不管定不定义get_config方法，都可以使用load_weights方法加载保存的权重</li><li>但是如果要使用load_model方法载入包含自定义层的model，必须要显示自定义get_config方法，否则keras 无法获知 Linear 的配置参数！<ul><li>在 <code>__init__</code> 的最后加上 <code>**kwargs</code> 参数，并用 <code>**kwargs</code> 参数初始化父类。</li><li>实现上述的 <code>get_config</code> 方法，返回自定义的参数配置和默认的参数配置</li></ul></li></ul><p>4.2 补充1.3 &amp; 2: 自定义损失函数除了可以用Lambda层，也可以定义Layer层，这是个没有权重的自定义Layer。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 官方示例：Custom loss layer</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomVariationalLayer</span><span class="params">(Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        self.is_placeholder = <span class="keyword">True</span></span><br><span class="line">        super(CustomVariationalLayer, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">vae_loss</span><span class="params">(self, x, x_decoded_mean)</span>:</span></span><br><span class="line">        xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)<span class="comment">#Square Loss</span></span><br><span class="line">        kl_loss = - <span class="number">0.5</span> * K.sum(<span class="number">1</span> + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=<span class="number">-1</span>)<span class="comment"># KL-Divergence Loss</span></span><br><span class="line">        <span class="keyword">return</span> K.mean(xent_loss + kl_loss)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        x = inputs[<span class="number">0</span>]</span><br><span class="line">        x_decoded_mean = inputs[<span class="number">1</span>]</span><br><span class="line">        loss = self.vae_loss(x, x_decoded_mean)</span><br><span class="line">        self.add_loss(loss, inputs=inputs)</span><br><span class="line">        <span class="comment"># We won't actually use the output.</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>4.3 补充</p><p>call方法的完整参数：call(self, inputs, <em>args, *</em>kwargs)</p><ul><li>其中inputs就是层输入，tensor/tensors</li><li>除此之外还有两个reserved keyword arguments：training&amp;mask，一个用于bn/dropout这种train/test计算有区别的flag，一个用于RNNlayers约束时序相关关系</li><li><em>args和*</em>kwargs是预留为了以后扩展更多输入参数的</li></ul><h3 id="5-keras-Generator"><a href="#5-keras-Generator" class="headerlink" title="5. keras Generator"></a>5. keras Generator</h3><p>本质上就是python的生成器，每次返回<strong>一个batch</strong>的样本及标签<br>自定义generator的时候要写成死循环（while true），因为model.fit_generator()在使用在个函数的时候，并不会在每一个epoch之后重新调用，那么如果这时候generator自己结束了就会有问题。<br>栗子是我为mixup写的generator：<br>没有显示的while True是因为创建keras自带的generator的时候已经是死循环了（for永不跳出）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Datagen_mixup</span><span class="params">(data_path, img_size, batch_size, is_train=True, mix_prop=<span class="number">0.8</span>, alpha=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> is_train:</span><br><span class="line">        datagen = ImageDataGenerator()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        datagen = ImageDataGenerator()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># using keras库函数</span></span><br><span class="line">    generator = datagen.flow_from_directory(data_path, target_size=(img_size, img_size),</span><br><span class="line">                                            batch_size=batch_size,</span><br><span class="line">                                            color_mode=<span class="string">"grayscale"</span>,</span><br><span class="line">                                            shuffle=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> generator:     <span class="comment"># a batch of &lt;img, label&gt;</span></span><br><span class="line">        <span class="keyword">if</span> alpha &gt; <span class="number">0</span>:</span><br><span class="line">            lam = np.random.beta(alpha, alpha)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            lam = <span class="number">1</span></span><br><span class="line">        idx = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">0</span>])]</span><br><span class="line">        random.shuffle(idx)</span><br><span class="line">        mixed_x = lam*x + (<span class="number">1</span>-lam)*x[idx]</span><br><span class="line">        mixed_y = lam*y + (<span class="number">1</span>-lam)*y[idx]</span><br><span class="line"></span><br><span class="line">        n_origin = int(batch_size * mix_prop)</span><br><span class="line">        gen_x = np.vstack(x[:n_origin], mixed_x[:(batch_size-n_origin)])</span><br><span class="line">        gen_y = np.vstack(y[:n_origin], mixed_y[:(batch_size-n_origin)])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> gen_x, gen_y</span><br></pre></td></tr></table></figure></p><p>【多进程】fit_generator中有一个参数use_multiprocessing，默认设置为false，因为‘using a generator with use_multiprocessing=True and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence’ class’</p><p>如果设置多进程use_multiprocessing，代码会把你的数据复制几份，分给不同的workers进行输入，这显然不是我们希望的，我们希望一份数据直接平均分给多个workers帮忙输入，这样才是最快的。而Sequence数据类能完美解决这个问题。</p><p><strong>keras.utils.Sequence()</strong>：</p><ul><li>每一个 <code>Sequence</code> 必须实现 <code>__getitem__</code> 和 <code>__len__</code> 方法</li><li><code>__getitem__</code> 方法应该范围一个完整的批次</li><li><strong>如果你想在迭代之间修改你的数据集，你可以实现 <code>on_epoch_end</code></strong>（会在每个迭代之间被隐式调用）\</li><li>github上有issue反映on_epoch_end不会没调用，解决方案：在__len__方法中显示自行调用</li></ul><p>直接看栗子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.applications <span class="keyword">import</span> ResNet50</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataGenerator</span><span class="params">(keras.utils.Sequence)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data, batch_size=<span class="number">1</span>, shuffle=True)</span>:</span></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.data = data</span><br><span class="line">        self.indexes = np.arange(len(self.data))</span><br><span class="line">        self.shuffle = shuffle</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 计算每一个epoch的迭代次数</span></span><br><span class="line">        <span class="keyword">return</span> math.ceil(len(self.data) / float(self.batch_size))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="comment"># 生成每个batch数据</span></span><br><span class="line">        batch_indices = self.indexes[index*self.batch_size:(index+<span class="number">1</span>)*self.batch_size]</span><br><span class="line">        batch_data = [self.data[k] <span class="keyword">for</span> k <span class="keyword">in</span> batch_indices]</span><br><span class="line"></span><br><span class="line">        x_batch, y_batch = self.data_generation(batch_data)</span><br><span class="line">        <span class="keyword">return</span> x_batch, y_batch</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.shuffle == <span class="keyword">True</span>:</span><br><span class="line">            np.random.shuffle(self.indexes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">data_generation</span><span class="params">(self, batch_data)</span>:</span></span><br><span class="line">        images = []</span><br><span class="line">        labels = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成数据</span></span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(batch_data):</span><br><span class="line">            image = cv2.imread(data, <span class="number">0</span>)</span><br><span class="line">            image = cv2.resize(image, dsize=(<span class="number">64</span>,<span class="number">64</span>), interpolation=cv2.INTER_LINEAR)</span><br><span class="line">            <span class="keyword">if</span> np.max(image)&gt;<span class="number">1</span>:</span><br><span class="line">                image = image / <span class="number">255.</span></span><br><span class="line">            image = np.expand_dims(image, axis=<span class="number">-1</span>)</span><br><span class="line">            images.append(image)</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'d0'</span> <span class="keyword">in</span> data:</span><br><span class="line">                labels.append([<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                labels.append([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> np.array(images), np.array(labels)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">  <span class="comment"># data</span></span><br><span class="line">    data_dir = <span class="string">"/Users/amber/dataset/mnist"</span></span><br><span class="line">    data_lst = []</span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(data_dir+<span class="string">"/d0"</span>)[:]:</span><br><span class="line">        data_lst.append(os.path.join(data_dir, <span class="string">"d0"</span>, file))</span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(data_dir+<span class="string">"/d1"</span>)[:]:</span><br><span class="line">        data_lst.append(os.path.join(data_dir, <span class="string">"d1"</span>, file))</span><br><span class="line">    training_generator = DataGenerator(data_lst, batch_size=<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># model</span></span><br><span class="line">    model = ResNet50(input_shape=(<span class="number">64</span>,<span class="number">64</span>,<span class="number">1</span>),weights=<span class="keyword">None</span>, classes=<span class="number">2</span>)</span><br><span class="line">    model.compile(optimizer=SGD(<span class="number">1e-3</span>), loss=<span class="string">'categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">    model.fit_generator(training_generator, epochs=<span class="number">50</span>,max_queue_size=<span class="number">200</span>,workers=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>经验值：</p><ul><li>workers：2/3</li><li>max_queue_size：默认10，具体基于GPU处于空闲状态适量调节</li></ul><p>【附加】实验中还发现一个问题，最开始定义了一个sequential model，然后在调用fit_generator一直报错：model not compile，但是显然model是compile过了的，网上查到的解释：‘Sequential model works with model.fit but not with model.fit_generator’</p><h3 id="6-多GPU"><a href="#6-多GPU" class="headerlink" title="6. 多GPU"></a>6. 多GPU</h3><p>多GPU运行分为两种情况：</p><pre><code>* 数据并行* 设备并行</code></pre><p>6.1 数据并行</p><p>数据并行将目标模型在多个GPU上各复制一份，使用每个复制品处理数据集的不同部分。</p><p>一个栗子：写tripleNet模型时，取了batch=4，总共15类，那么三元组总共有$(4/2)^2*15=60$个，训练用了224的图像，单张GPU内存会溢出，因此需要单机多卡数据并行。</p><p>​    step1. 在模型定义中，用multi_gpu_model封一层，<strong>需要在model.compile之前</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.util <span class="keyword">import</span> multi_gpu_model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triple_model</span><span class="params">(input_shape=<span class="params">(<span class="number">512</span>,<span class="number">512</span>,<span class="number">1</span>)</span>, n_classes=<span class="number">10</span>, multi_gpu=False)</span>:</span></span><br><span class="line">  anchor_input = Input(shape=input_shape)</span><br><span class="line">  positive_input = Input(shape=input_shape)</span><br><span class="line">  negative_input = Input(shape=input_shape)</span><br><span class="line">  </span><br><span class="line">  sharedCNN = base_model(input_shape)</span><br><span class="line">  encoded_anchor = sharedCNN(anchor_input)</span><br><span class="line">  encoded_positive = sharedCNN(positive_input)</span><br><span class="line">  encoded_negative = sharedCNN(negative_input)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># class branch</span></span><br><span class="line">  x = Dense(n_classses, activation=<span class="string">'softmax'</span>)(encoded_anchor)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># distance branch</span></span><br><span class="line">  encoded_anchor = Activation(<span class="string">'sigmoid'</span>)(encoded_anchor)</span><br><span class="line">  encoded_positive = Activation(<span class="string">'sigmoid'</span>)(encoded_positive)</span><br><span class="line">  encoded_negative = Activation(<span class="string">'sigmoid'</span>)(encoded_negative)</span><br><span class="line">  merged = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=<span class="number">-1</span>, name=<span class="string">'tripleLossLayer'</span>)</span><br><span class="line">  </span><br><span class="line">  model = Model(inputs=[anchor_input,positive_input,negative_input], outputs=[x, merged])</span><br><span class="line">  <span class="keyword">if</span> multi_gpu:</span><br><span class="line">    model = multi_gpu_model(model, GPU_COUNT)</span><br><span class="line">  </span><br><span class="line">  model.compile(optimizer=SGD, loss=[cls_loss, triplet_loss], metrics=[cls_acc])</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>​    step2. 在定义checkpoint时，要用ParallelModelCheckpoint封一层，初始化参数的model要传原始模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> ModelCheckpoint</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ParallelModelCheckpoint</span><span class="params">(ModelCheckpoint)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,model,filepath, monitor=<span class="string">'val_loss'</span>, verbose=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 save_best_only=False, save_weights_only=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 mode=<span class="string">'auto'</span>, period=<span class="number">1</span>)</span>:</span></span><br><span class="line">        self.single_model = model</span><br><span class="line">        super(ParallelModelCheckpoint,self).__init__(filepath, monitor, verbose,save_best_only, save_weights_only,mode, period)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_model</span><span class="params">(self, model)</span>:</span></span><br><span class="line">        super(ParallelModelCheckpoint,self).set_model(self.single_model)</span><br><span class="line"></span><br><span class="line">model = triple_model(multi_gpu=<span class="keyword">True</span>)</span><br><span class="line">single_model = triple_model(multi_gpu=<span class="keyword">False</span>)</span><br><span class="line">filepath = <span class="string">"./tripleNet_&#123;epoch:02d&#125;_val_loss_&#123;val_loss:.3f&#125;.h5"</span></span><br><span class="line">check_point = ParallelModelCheckpoint(single_model, filepath)</span><br></pre></td></tr></table></figure><p>​    step3. 在保存权重时，通过cpu模型来保存。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例化基础模型，这样定义模型权重会存储在CPU内存中</span></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">model = Resnet50(input_shape=(<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), classes=<span class="number">4</span>, weights=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">parallel_model = multi_gpu_model(model, GPU_COUNT)</span><br><span class="line">parallel_model.fit(x,y, epochs=<span class="number">20</span>, batch_size=<span class="number">32</span>)</span><br><span class="line">model.save(<span class="string">'model.h5'</span>)</span><br></pre></td></tr></table></figure><p>​    【attention】同理，在load权重时，也是load单模型的权重，再调用multi_gpu_model将模型复制到多个GPU上：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = Model(inputs=[anchor_input,positive_input,negative_input], outputs=[x, merged])</span><br><span class="line"><span class="keyword">if</span> multi_gpu:</span><br><span class="line"><span class="keyword">if</span> os.path.exists(weight_pt):</span><br><span class="line">model.load_weights(weight_pt)</span><br><span class="line">  model = multi_gpu_model(model, GPU_COUNT)</span><br></pre></td></tr></table></figure><p>【ATTENTION】实验中发现一个问题：在有些case中，我们使用了自定义loss作为网络的输出，<strong>此时网络的输出是个标量</strong>，但是在调用multi_gpu_model这个方法时，具体实现在multi_gpu_utils.py中，最后一个步骤要merge几个device的输出，通过axis=0的concat实现，网络输出是标量的话就会报错——list assignment index out of range。</p><p>尝试的解决方案是改成相加：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Merge outputs under expected scope.</span></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span> <span class="keyword">if</span> cpu_merge <span class="keyword">else</span> <span class="string">'/gpu:%d'</span> % target_gpu_ids[<span class="number">0</span>]):</span><br><span class="line">    merged = []</span><br><span class="line">    <span class="keyword">for</span> name, outputs <span class="keyword">in</span> zip(output_names, all_outputs):</span><br><span class="line">        merged.append(Lambda(<span class="keyword">lambda</span> x: K.sum(x))(outputs)) </span><br><span class="line">        <span class="comment"># merged.append(concatenate(outputs, axis=0, name=name))</span></span><br><span class="line">    <span class="keyword">return</span> Model(model.inputs, merged)</span><br></pre></td></tr></table></figure><p>【ATTENTION++】网络的输出不能是标量！！永远会隐藏保留一个batch dim，之前是写错了！！</p><ul><li>model loss是一个标量</li><li>作为输出层的loss是保留batch dim的！！</li></ul><p>6.2 设备并行</p><p>设备并行适用于<strong>多分支结构</strong>，一个分支用一个GPU。通过使用TensorFlow device scopes实现。</p><p>栗子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Model where a shared LSTM is used to encode two different sequences in parallel</span></span><br><span class="line">input_a = keras.Input(shape=(<span class="number">140</span>, <span class="number">256</span>))</span><br><span class="line">input_b = keras.Input(shape=(<span class="number">140</span>, <span class="number">256</span>))</span><br><span class="line"></span><br><span class="line">shared_lstm = keras.layers.LSTM(<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Process the first sequence on one GPU</span></span><br><span class="line"><span class="keyword">with</span> tf.device_scope(<span class="string">'/gpu:0'</span>):</span><br><span class="line">    encoded_a = shared_lstm(tweet_a)</span><br><span class="line"><span class="comment"># Process the next sequence on another GPU</span></span><br><span class="line"><span class="keyword">with</span> tf.device_scope(<span class="string">'/gpu:1'</span>):</span><br><span class="line">    encoded_b = shared_lstm(tweet_b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Concatenate results on CPU</span></span><br><span class="line"><span class="keyword">with</span> tf.device_scope(<span class="string">'/cpu:0'</span>):</span><br><span class="line">    merged_vector = keras.layers.concatenate([encoded_a, encoded_b],axis=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h3 id="7-库函数讲解"><a href="#7-库函数讲解" class="headerlink" title="7. 库函数讲解"></a>7. 库函数讲解</h3><p>7.1 BatchNormalization(axis=-1)</p><p>用于在每个batch上将前一层的激活值重新规范化，即使得其输出数据的均值接近0，其标准差接近1。</p><p>常用参数axis：指定要规范化的轴，通常为特征轴，如在“channels_first”的data format下，axis=1，反之axis=-1。</p><p>7.2 LSTM</p><p>参数：</p><ul><li>units：输出维度（最后一维），标准输入NxTxD，N for batch，T for time-step，D for vector-dimension。</li><li>activation：激活函数</li><li>recurrent_activation：用于循环时间步的激活函数</li><li>dropout：在 0 和 1 之间的浮点数。 单元的丢弃比例，用于输入的线性转换</li><li>recurrent_dropout：在 0 和 1 之间的浮点数。 单元的丢弃比例，用于循环层状态的线性转换</li><li><strong>return_sequences: </strong>布尔值，默认False。是返回输出序列中的最后一个输出，还是全部序列的输出。即many-to-one还是many-to-many，简单来讲，当我们需要时序输出（many-to-many）的时候，就set True。</li><li><strong>return_state</strong>: 布尔值，默认False。除了输出之外是否返回<strong>最后一个</strong>状态（cell值）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># return_sequences</span></span><br><span class="line">inputs1 = Input(tensor=(<span class="number">1</span>，<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">lstm1 = LSTM(<span class="number">1</span>, return_sequences=<span class="keyword">True</span>)(inputs1)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">输出结果为</span></span><br><span class="line"><span class="string">[[[-0.02243521]</span></span><br><span class="line"><span class="string">[-0.06210149]</span></span><br><span class="line"><span class="string">[-0.11457888]]]</span></span><br><span class="line"><span class="string">表示每个time-step，LSTM cell的输出</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># return_state</span></span><br><span class="line">lstm1, state_h, state_c = LSTM(<span class="number">1</span>, return_state=<span class="keyword">True</span>)(inputs1)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">输出结果为</span></span><br><span class="line"><span class="string">[array([[ 0.10951342]], dtype=float32),</span></span><br><span class="line"><span class="string"> array([[ 0.10951342]], dtype=float32),</span></span><br><span class="line"><span class="string"> array([[ 0.24143776]], dtype=float32)]</span></span><br><span class="line"><span class="string"> list中依次为网络输出，最后一个time-step的LSTM cell的输出值和cell值</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><p>7.2.5 TimeDistributed</p><p>顺便再说下TimeDistributed，当我们使用many-to-many模型，最后一层LSTM的输出维度为k，而我们想要的最终输出维度为n，那么就需要引入Dense层，对于时序模型，我们要对每一个time-step引入dense层，这实质上是多个Dense操作，那么我们就可以用TimeDistributed来包裹Dense层来实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(LSTM(<span class="number">3</span>, input_shape=(length, <span class="number">1</span>), return_sequences=<span class="keyword">True</span>))</span><br><span class="line">model.add(TimeDistributed(Dense(<span class="number">1</span>)))</span><br></pre></td></tr></table></figure><p>官方文档：这个封装器将一个层应用于输入的每个时间片。</p><ul><li>当该层作为第一层时，应显式说明input_shape</li><li>TimeDistributed可以应用于任意层，如Conv3D：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 例如我的crnn model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crnn</span><span class="params">(input_shape, cnn, n_classes=<span class="number">24</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">    inpt = Input(input_shape)</span><br><span class="line"></span><br><span class="line">    x = TimeDistributed(cnn, input_shape=input_shape)(inpt)</span><br><span class="line">    x = LSTM(<span class="number">128</span>, return_sequences=<span class="keyword">True</span>)(x)</span><br><span class="line">    x = LSTM(<span class="number">256</span>, return_sequences=<span class="keyword">True</span>)(x)</span><br><span class="line"></span><br><span class="line">    x = TimeDistributed(Dense(n_classes))(x)</span><br><span class="line"></span><br><span class="line">    model = Model(inpt, x)</span><br><span class="line">    model.summary()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"> </span><br><span class="line">crnn_model = crnn((<span class="number">24</span>,<span class="number">128</span>,<span class="number">128</span>,<span class="number">128</span>,<span class="number">2</span>), cnn_model)</span><br></pre></td></tr></table></figure><p>7.3 Embedding</p><p>用于将稀疏编码映射为固定尺寸的密集表示。</p><p>输入形如（samples，sequence_length）的2D张量，输出形如(samples, sequence_length, output_dim)的3D张量。</p><p>参数：</p><ul><li>input_dim：字典长度，即输入数据最大下标+1</li><li>output_dim：</li><li>input_length：</li></ul><p>栗子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># centerloss branch</span></span><br><span class="line">lambda_c = <span class="number">1</span></span><br><span class="line">input_ = Input(shape=(<span class="number">1</span>,))</span><br><span class="line">centers = Embedding(<span class="number">10</span>,<span class="number">2</span>)(input_)    <span class="comment"># (None, 1, 2)</span></span><br><span class="line"><span class="comment"># 这里的输入是0-9的枚举（dim=10），然后映射成一个簇心</span></span><br><span class="line">intra_loss = Lambda(<span class="keyword">lambda</span> x:K.sum(K.square(x[<span class="number">0</span>]-x[<span class="number">1</span>][:,<span class="number">0</span>]),<span class="number">1</span>,keepdims=<span class="keyword">True</span>))([out1,centers])</span><br><span class="line">model_center_loss = Model([inputs,input_],[out2,intra_loss])</span><br><span class="line">model_center_loss.compile(optimizer=<span class="string">"sgd"</span>,</span><br><span class="line">                          loss=[<span class="string">"categorical_crossentropy"</span>,<span class="keyword">lambda</span> y_true,y_pred:y_pred],</span><br><span class="line">                          loss_weights=[<span class="number">1</span>,lambda_c/<span class="number">2.</span>],</span><br><span class="line">                          metrics=[<span class="string">"acc"</span>])</span><br><span class="line">model_center_loss.summary()</span><br></pre></td></tr></table></figure><p>7.4 plot_model</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from keras.utils import plot_model</span><br><span class="line">plot_model(model, to_file=&apos;model.png&apos;, show_shapes=False, show_layer_names=True)</span><br></pre></td></tr></table></figure><p>7.5 K.function</p><p>获取模型某层的输出，一种方法是创建一个新的模型，使它的输出是目标层，然后调用predict。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = ...    <span class="comment"># the original model</span></span><br><span class="line"></span><br><span class="line">new_model = Model(input=model.input,</span><br><span class="line">                  output=model.get_layer(<span class="string">'my_layer'</span>).output)</span><br><span class="line"></span><br><span class="line">intermediate_output = new_model.predict(input_data）</span><br></pre></td></tr></table></figure><p>也可以创建一个函数来实现：keras.backend.function(inputs, outputs, updates=<strong>None</strong>)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这是写center-loss时写的栗子：</span></span><br><span class="line">func = K.function(inputs=[model.input[<span class="number">0</span>]],          </span><br><span class="line">                  outputs=[model.get_layer(<span class="string">'out1'</span>).output]) </span><br><span class="line"><span class="comment"># model.input[0]: one input of the multi-input model</span></span><br><span class="line"></span><br><span class="line">test_features = func([x_test])[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>7.6 K.gradients(y,x)</p><p>求y关于x的导数，y和x可以是张量／张量列表。返回张量列表，列表长度同x列表，列表中元素shape同x列表中元素。</p><p>对于$y=[y_1, y_2], x=[x_1, x_2, x_3]$，有返回值$[grad_1, grad_2, grad_3]$，真实的计算过程为：</p><script type="math/tex; mode=display">grad_1 = \frac{\partial y_1}{\partial x_1} + \frac{\partial y_2}{\partial x_1} \\grad_2 = \frac{\partial y_1}{\partial x_2} + \frac{\partial y_2}{\partial x_2} \\grad_3 = \frac{\partial y_1}{\partial x_3} + \frac{\partial y_2}{\partial x_3}</script><p>7.7  ModelCheckpoint、ReduceLROnPlateau、EarlyStopping、LearningRateScheduler、Tensorboard</p><ul><li><p>模型检查点ModelCheckpoint</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">ModelCheckpoint(filepath, monitor=<span class="string">'val_loss'</span>, verbose=<span class="number">0</span>, save_best_only=<span class="keyword">False</span>, save_weights_only=<span class="keyword">False</span>, mode=<span class="string">'auto'</span>, period=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><ul><li>filepath可以由 <code>epoch</code> 的值和 <code>logs</code> 的键来填充，如weights.{epoch:02d}-{val_loss:.2f}.hdf5。</li><li>moniter：被监测的数据</li><li>mode：在 <code>auto</code> 模式中，方向会自动从被监测的数据的<strong>名字</strong>(不靠谱🤷‍♀️)中判断出来。</li></ul></li><li><p>学习率衰减ReduceLROnPlateau</p><p>  学习率的方案相对简单，要么在验证集的损失或准确率开始稳定时调低学习率，要么在固定间隔上调低学习率。</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ReduceLROnPlateau(monitor=<span class="string">'val_loss'</span>, factor=<span class="number">0.1</span>, patience=<span class="number">10</span>, verbose=<span class="number">0</span>, mode=<span class="string">'auto'</span>, min_delta=<span class="number">0.0001</span>, cooldown=<span class="number">0</span>, min_lr=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>  当学习停止时，模型总是会受益于降低 2-10 倍的学习速率。</p><ul><li>moniter：被监测的数据</li><li>factor：新的学习速率 = 学习速率 * factor</li><li>patience：被监测数据没有进步的训练轮数，在这之后训练速率会被降低。</li></ul></li><li><p>更复杂的学习率变化模式定义LearningRateScheduler</p><p>  前提是只需要用到默认参数是epoch</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先定义一个变化模式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">warmup_scheduler</span><span class="params">(epoch, mode=<span class="string">'power_decay'</span>)</span>:</span></span><br><span class="line">lr_base = <span class="number">1e-5</span></span><br><span class="line">lr_stable = <span class="number">1e-4</span></span><br><span class="line"></span><br><span class="line">lr = lr_base * math.pow(<span class="number">10</span>, epoch)</span><br><span class="line"><span class="keyword">if</span> lr&gt;lr_stable:</span><br><span class="line"><span class="keyword">return</span> lr_stable</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> lr</span><br><span class="line">  </span><br><span class="line"><span class="comment"># 然后调用LearningRateScheduler方法wrapper这个scheduler</span></span><br><span class="line">scheduler = LearningRateScheduler(warmup_scheduler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在使用的时候放在callbacks的list里面，在每个epoch结束触发</span></span><br><span class="line">callbacks = [checkpoint, reduce_lr, scheduler, early_stopping]</span><br></pre></td></tr></table></figure></li><li><p>更更复杂的学习率变化模式定义可以直接继承Callback</p><ul><li>当我们需要传入更丰富的自定义参数/需要进行by step的参数更新等，可以直接继承Callback，进行更自由的自定义</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以余弦退火算法为例：</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CosineAnnealingScheduler</span><span class="params">(Callback)</span>:</span></span><br><span class="line">    <span class="string">"""Cosine annealing scheduler.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, epochs, scale=<span class="number">1.6</span>, shift=<span class="number">0</span>, verbose=<span class="number">0</span>)</span>:</span></span><br><span class="line">        super(CosineAnnealingScheduler, self).__init__()</span><br><span class="line">        self.epochs = epochs</span><br><span class="line">        self.scale = scale</span><br><span class="line">        self.shift = shift</span><br><span class="line">        self.verbose = verbose</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_begin</span><span class="params">(self, epoch, logs=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> epoch&lt;=<span class="number">6</span>:</span><br><span class="line">            <span class="comment"># linearly increase from 0 to 1.6 in first 5 epochs</span></span><br><span class="line">            lr = <span class="number">1.6</span> / <span class="number">5</span> * (epoch+<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># cosine annealing</span></span><br><span class="line">lr = self.shift + self.scale * (<span class="number">1</span> + math.cos(math.pi * (epoch+<span class="number">1</span><span class="number">-5</span>) / self.epochs)) / <span class="number">2</span></span><br><span class="line">        K.set_value(self.model.optimizer.lr, lr)</span><br><span class="line">        <span class="keyword">if</span> self.verbose &gt; <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'\nEpoch %05d: CosineAnnealingScheduler setting learning rate to %s.'</span> % (epoch+<span class="number">1</span>, lr))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span><span class="params">(self, epoch, logs=None)</span>:</span></span><br><span class="line">        logs = logs <span class="keyword">or</span> &#123;&#125;</span><br><span class="line">        logs[<span class="string">'lr'</span>] = K.get_value(self.model.optimizer.lr)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="comment"># 调用</span></span><br><span class="line">lrscheduler = CosineAnnealingScheduler(epochs=<span class="number">2</span>, verbose=<span class="number">1</span>)</span><br><span class="line">callbacks = [checkpoint, lrscheduler]</span><br><span class="line">model.fit(...,</span><br><span class="line">         callbacks=callbacks)</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>提前停止训练EarlyStopping</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EarlyStopping(monitor=<span class="string">'val_loss'</span>, min_delta=<span class="number">0</span>, patience=<span class="number">0</span>, verbose=<span class="number">0</span>, mode=<span class="string">'auto'</span>, baseline=<span class="keyword">None</span>, restore_best_weights=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><ul><li>moniter：被监测的数据</li><li>patience：被监测数据没有进步的训练轮数，在这之后训练速率会被降低。</li><li>min_delta：在被监测的数据中被认为是提升的最小变化，小于 min_delta 的绝对变化会被认为没有提升。</li><li>baseline: 要监控的数量的基准值。</li></ul></li><li><p>以上这四个都是继承自keras.callbacks()</p></li><li><p>可视化工具TensorBoard</p><p>  这个回调函数为 Tensorboard 编写一个日志， 这样你可以可视化测试和训练的标准评估的动态图像， 也可以可视化模型中不同层的激活值直方图。</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TensorBoard(log_dir=<span class="string">'./logs'</span>, histogram_freq=<span class="number">0</span>, batch_size=<span class="number">32</span>, write_graph=<span class="keyword">True</span>, write_grads=<span class="keyword">False</span>, write_images=<span class="keyword">False</span>, embeddings_freq=<span class="number">0</span>, embeddings_layer_names=<span class="keyword">None</span>, embeddings_metadata=<span class="keyword">None</span>, embeddings_data=<span class="keyword">None</span>, update_freq=<span class="string">'epoch'</span>)</span><br></pre></td></tr></table></figure><p>  实际使用时关注第一个参数log_dir就好，查看时通过命令行启动：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=/full_path_to_your_logs</span><br></pre></td></tr></table></figure></li></ul><p><strong>这几个回调函数，通通在训练时（model.fit / fit_generator）放在callbacks关键字里面。</strong></p><p>7.8 反卷积 Conv2DTranspose</p><p>三个核心的参数filtes、kernel_size、strides、padding=’valid’</p><ul><li>filtes：输出通道数</li><li>strides：步长</li><li>kernel_size：一般需要通过上面两项计算得到</li></ul><p>反卷积运算和正向卷积运算保持一致，即：</p><script type="math/tex; mode=display">(output\_shape - kernel\_size) / stride + 1 = input\_shape</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fcn example: current feature map x (,32,32,32), input_shape (512,512,2), output_shape (,512,512,1)</span></span><br><span class="line">strides = <span class="number">2</span></span><br><span class="line">kernel_size = input_shape[<span class="number">0</span>] - (x.get_shape().as_list()[<span class="number">1</span>] - <span class="number">1</span>)*strides</span><br><span class="line">y = Conv2DTranspose(<span class="number">1</span>, kernel_size, padding=<span class="string">'valid'</span>, strides=strides)</span><br></pre></td></tr></table></figure><p>7.9 K.shape &amp; K.int_shape &amp; tensor._keras_shape</p><ul><li>tensor._keras_shape等价于K.int_shape：张量的shape，返回值是个tuple</li><li>K.shape：返回值是个tensor，tensor是个一维向量，其中每一个元素可以用[i]来访问，是个标量tensor</li></ul><p>两个方法的主要区别是：前者返回值是个常量，只能表征语句执行时刻（如构建图）tensor的状态，后者返回值是个变量，wrapper的方法可以看成一个节点，在graph的作用域内始终有效，在构建图的时候可以是None，在实际流入数据流的时候传值就行，如batch_size！！！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input</span><br><span class="line"></span><br><span class="line">x = Input((<span class="number">22</span>,<span class="number">22</span>,<span class="number">1</span>))</span><br><span class="line">print(K.shape(x))</span><br><span class="line"><span class="comment"># Tensor("Shape:0", shape=(4,), dtype=int32)</span></span><br><span class="line">print(K.shape(x)[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># Tensor("strided_slice:0", shape=(), dtype=int32)</span></span><br><span class="line"></span><br><span class="line">print(K.int_shape(x))</span><br><span class="line"><span class="comment"># (None, 22, 22, 1)</span></span><br></pre></td></tr></table></figure><p>7.10 binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0)</p><ul><li>from_logits：logits表示网络的直接输出——没经过sigmoid或者softmax的概率化，默认情况下，我们认为y_pred是已经处理过的概率分布</li><li></li></ul><h3 id="8-衍生：一些tf函数"><a href="#8-衍生：一些tf函数" class="headerlink" title="8. 衍生：一些tf函数"></a>8. 衍生：一些tf函数</h3><p>8.1 tf.where(condition, x=None, y=None,name=None)</p><p>两种用法：</p><ul><li>如果x，y为空，返回值是满足condition元素的<strong>索引</strong>，每个索引占一行。</li><li>如果x，y不为空，那么condition、x、y 和返回值相同维度，condition为True的位置替换x中对应元素，condition为False的位置替换y中对应元素。</li></ul><p>关于索引indices：</p><ul><li><p>condition的shape的dim，就是每一行索引vector的shape，例：</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">condition1 = np.array([[<span class="keyword">True</span>,<span class="keyword">False</span>,<span class="keyword">False</span>],[<span class="keyword">False</span>,<span class="keyword">True</span>,<span class="keyword">True</span>]])</span><br><span class="line">print(condition1.shape)    <span class="comment"># (2,3)</span></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(tf.where(condition1)))</span><br><span class="line"><span class="comment"># [[0 0]</span></span><br><span class="line"><span class="comment"># [1 1]</span></span><br><span class="line"><span class="comment"># [1 2]]</span></span><br><span class="line"><span class="comment"># condition是2x3的arr，也就是dim=2，那么索引vector的shape就是2，纵轴的shape是满足cond的数量</span></span><br></pre></td></tr></table></figure></li><li><p>索引通常与tf.gather和tf.gather_nd搭配使用：</p><ul><li>tf.gather(params,indices,axis=0.name=None)：tf.gather只能接受1-D的索引，axis用来指定轴，一个索引取回对应维度的一个向量</li><li>tf.gather_nd(params,indices)：tf.gather_nd可以接受多维的索引，如果索引的dim小于params的dim，则从axis=0开始索引，后面的取全部。</li></ul></li></ul><p>8.2 tf.Print()</p><p>相当于一个节点，定义了数据的流入和流出。</p><p>一个error：在模型定义中，直接调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">intra_distance = tf.Print(intra_distance,</span><br><span class="line">                          [intra_distance],</span><br><span class="line">                          message=<span class="string">'Debug info: '</span>,</span><br><span class="line">                          summarize=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>会报错：AttributeError: ‘Tensor’ object has no attribute ‘_keras_history’</p><p>参考：<a href="https://stackoverflow.com/questions/56096399/creating-model-throws-attributeerror-tensor-object-has-no-attribute-keras" target="_blank" rel="noopener">https://stackoverflow.com/questions/56096399/creating-model-throws-attributeerror-tensor-object-has-no-attribute-keras</a></p><blockquote><p>You cannot use backend functions directly in Keras tensors, every operation in these tensors must be a layer. You need to wrap each custom operation in a Lambda layer and provide the appropriate inputs to the layer.</p></blockquote><p>之前一直没注意到这个问题，凡是调用了tf.XXX的operation，都要wrapper在Lambda层里。</p><p>改写：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># wrapper function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">debug</span><span class="params">(args)</span>:</span></span><br><span class="line">    intra_distance, min_inter_distance = args</span><br><span class="line">    intra_distance = tf.Print(intra_distance,</span><br><span class="line">                              [intra_distance],</span><br><span class="line">                              message=<span class="string">'Debug info: '</span>,</span><br><span class="line">                              summarize=<span class="number">10</span>)</span><br><span class="line">    min_inter_distance = tf.Print(min_inter_distance,</span><br><span class="line">                                  [min_inter_distance],</span><br><span class="line">                                  message=<span class="string">'Debug info: '</span>,</span><br><span class="line">                                  summarize=<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> [intra_distance, min_inter_distance]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型内</span></span><br><span class="line">intra_distance, min_inter_distance = Lambda(debug)([intra_distance, min_inter_distance])</span><br></pre></td></tr></table></figure><p>【夹带私货】tf.Print同时也可以打印wrapper function内的中间变量，都放在列表里面就可以了。</p><p>8.3 tf.while_loop(cond, body, init_value)</p><p>tensorflow中实现循环的语句</p><ul><li>终止条件cond：是一个函数</li><li>循环体body：是一个函数</li><li>init_value：是一个list，保存循环相关参数</li></ul><ol><li>cond、body的参数是要与init_value列表中变量一一对应的</li><li>body返回值的格式要与init_value变量一致（tensor形状保持不变）</li><li>若非要变怎么办（有时候我们希望在while_loop的过程中，维护一个list）？动态数组TensorArray／高级参数shape_invariants</li></ol><p>8.3.1 动态数组</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义</span></span><br><span class="line">b_boxes = tf.TensorArray(K.dtype(boxes), size=<span class="number">1</span>, dynamic_size=<span class="keyword">True</span>, clear_after_read=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写入指定位置</span></span><br><span class="line">b_boxes = b_boxes.write(b, boxes_)</span><br></pre></td></tr></table></figure><p>​    tensor array变量中一个位置只能写入一次</p><p>8.3.2 shape_invariants</p><p>​    <a href="https://stackoverflow.com/questions/41233462/tensorflow-while-loop-dealing-with-lists" target="_blank" rel="noopener">reference stackoverflow</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">i = tf.constant(<span class="number">0</span>)</span><br><span class="line">l = tf.Variable([])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">body</span><span class="params">(i, l)</span>:</span>                                               </span><br><span class="line">    temp = tf.gather(array,i)</span><br><span class="line">    l = tf.concat([l, [temp]], <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> i+<span class="number">1</span>, l</span><br><span class="line"></span><br><span class="line">index, list_vals = tf.while_loop(cond, body, [i, l],</span><br><span class="line">                                 shape_invariants=[i.get_shape(), tf.TensorShape([<span class="keyword">None</span>])])</span><br></pre></td></tr></table></figure><p>​    在while_loop中显示地指定参数的shape，上面的例子用了tf.TensorShape([None])令其自动推断，而不是固定检查，因此可以解决变化长度列表。</p><p>一个完整的栗子：第一次见while_loop，在yolo_loss里面</p><ol><li>基于batch维度做遍历</li><li>loop结束后将动态数据stack起来，重获batch dim</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Find ignore mask, iterate over each of batch.</span></span><br><span class="line"><span class="comment"># extract the elements on the mask which has iou &lt; ignore_thresh</span></span><br><span class="line">ignore_mask = tf.TensorArray(K.dtype(y_true[<span class="number">0</span>]), size=<span class="number">1</span>, dynamic_size=<span class="keyword">True</span>)  <span class="comment"># 动态size数组</span></span><br><span class="line">object_mask_bool = K.cast(object_mask, <span class="string">'bool'</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loop_body</span><span class="params">(b, ignore_mask)</span>:</span></span><br><span class="line">    true_box = tf.boolean_mask(y_true[l][b,...,<span class="number">0</span>:<span class="number">4</span>], object_mask_bool[b,...,<span class="number">0</span>])   <span class="comment"># (H,W,3,5)</span></span><br><span class="line">    iou = box_iou(pred_box[b], true_box)     <span class="comment"># (H,W,3,1)</span></span><br><span class="line">    best_iou = K.max(iou, axis=<span class="number">-1</span>)</span><br><span class="line">    ignore_mask = ignore_mask.write(b, K.cast(best_iou&lt;ignore_thresh, K.dtype(true_box)))</span><br><span class="line">    <span class="keyword">return</span> b+<span class="number">1</span>, ignore_mask</span><br><span class="line">_, ignore_mask = K.control_flow_ops.while_loop(<span class="keyword">lambda</span> b,*args: b&lt;m, loop_body, [<span class="number">0</span>, ignore_mask])</span><br><span class="line">ignore_mask = ignore_mask.stack()</span><br><span class="line">ignore_mask = K.expand_dims(ignore_mask, <span class="number">-1</span>)     <span class="comment"># （N,H,W,3,1）</span></span><br></pre></td></tr></table></figure><p>8.4 tf.image.non_max_suppression()</p><p>非最大值抑制：贪婪算法，按scores由大到小排序，选定第一个，依次对之后的框求iou，删除那些和选定框iou大于阈值的box。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回是被选中边框在参数boxes中的下标位置</span></span><br><span class="line">selected_indices=tf.image.non_max_suppression(boxes, scores, max_output_size, iou_threshold=<span class="number">0.5</span>, name=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据indices获取边框</span></span><br><span class="line">selected_boxes=tf.gather(boxes,selected_indices)</span><br></pre></td></tr></table></figure><ul><li>boxes：2-D的float类型的，大小为[num_boxes,4]的张量</li><li>scores：1-D的float类型的，大小为[num_boxes]，对应的每一个box的一个score</li><li>max_output_size：标量整数Tensor，输出框的最大数量</li><li>iou_threshold：浮点数，IOU阈值</li><li>selected_indices：1-D的整数张量，大小为[M]，留下来的边框下标，M小于等于max_output_size</li></ul><p>【拓展】还有Multi-class version of NMS——tf.multiclass_non_max_suppression()</p><p>8.5 限制GPU用量</p><ul><li><p>linux下查看GPU使用情况，1秒刷新一次：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">watch -n 1 nvidia-smi</span><br></pre></td></tr></table></figure></li><li><p>指定显卡号</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"2"</span></span><br></pre></td></tr></table></figure></li><li><p>限制GPU用量</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置百分比</span></span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.per_process_gpu_memory_fraction = <span class="number">0.3</span></span><br><span class="line">session = tf.Session(config=config)</span><br><span class="line">K.set_session(session)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置动态申请</span></span><br><span class="line">config = tf.ConfigProto()  </span><br><span class="line">config.gpu_options.allow_growth=<span class="keyword">True</span>   <span class="comment">#不全部占满显存, 按需分配</span></span><br><span class="line">session = tf.Session(config=config)</span><br><span class="line">K.set_session(session)</span><br></pre></td></tr></table></figure></li></ul><p>8.6 tf.boolean_mask()</p><p>tf.boolean_mask(tensor,mask,name=’boolean_mask’,axis=None)</p><p>其中，tensor是N维度的，mask是K维度的，$K \leq N$</p><p>axis表示mask的起始维度，被mask的维度只保留mask为True的数据，同时这部分数据flatten成一维，最终tensor的维度是N-K+1</p><p>栗子：yolov3里面，把特征图上有object的grid提取出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># y_trues: [b,h,w,a,4]</span></span><br><span class="line"><span class="comment"># conf_gt: [b,h,w,a,1]</span></span><br><span class="line">true_box = tf.boolean_mask(y_trues[i][b,...,<span class="number">0</span>:<span class="number">4</span>], conf_gt[b,...,<span class="number">0</span>])</span><br></pre></td></tr></table></figure><h2 id="9-keras自定义优化器optimizer"><a href="#9-keras自定义优化器optimizer" class="headerlink" title="9. keras自定义优化器optimizer"></a>9. keras自定义优化器optimizer</h2><p>9.1 关于梯度的优化器公共参数，用于梯度裁剪</p><ul><li>clipnorm：对所有梯度进行downscale，使得梯度vector中l2范数最大为1（g * 1 / max(1, l2_norm)）</li><li>clipvalue：对绝对值进行上下限截断</li></ul><p>9.2 keras的Optimizier对象</p><ul><li>keras的官方代码有optimizier_v1和optimizier_v2两版，分别面向tf1和tf2，v1的看起来简洁一些</li><li><a href="https://stackoverflow.com/questions/56806419/keras-how-to-reset-optimizer-state/56807007#56807007" target="_blank" rel="noopener">self.updates &amp; self.weights</a><ul><li>self.updates：stores the variables that will be updated with every batch that is processed by the model in training<ul><li>用来保存与模型训练相关的参数（iterations、params、moments、accumulators，etc）</li><li>symbolic graph variable，通过K.update_add方法说明图的operation</li></ul></li><li>self.weights：the functions that save and load optimizers will save and load this property<ul><li>用来保存与优化器相关的参数</li><li>model.save()方法中涉及include_optimizer=False，决定优化器的保存和重载</li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Optimizer</span><span class="params">(object)</span>:</span></span><br><span class="line"><span class="comment"># - 抽象类，所有真实的优化器继承自Optimizer对象</span></span><br><span class="line"><span class="comment"># - 提供两个用于梯度截断的公共参数</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">    allowed_kwargs = &#123;<span class="string">'clipnorm'</span>, <span class="string">'clipvalue'</span>&#125;</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> kwargs:</span><br><span class="line">      <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> allowed_kwargs:</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">'Unexpected keyword argument passed to optimizer: '</span> + str(k))</span><br><span class="line">      <span class="comment"># checks that clipnorm &gt;= 0 and clipvalue &gt;= 0</span></span><br><span class="line">      <span class="keyword">if</span> kwargs[k] &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Expected &#123;&#125; &gt;= 0, received: &#123;&#125;'</span>.format(k, kwargs[k]))</span><br><span class="line">    self.__dict__.update(kwargs)</span><br><span class="line">    self.updates = []   <span class="comment"># 计算更新的参数</span></span><br><span class="line">    self.weights = []   <span class="comment"># 优化器带来的权重，在get_updates以后才有元素，在保存模型时会被保存</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Set this to False, indicating `apply_gradients` does not take the</span></span><br><span class="line">  <span class="comment"># `experimental_aggregate_gradients` argument.</span></span><br><span class="line">  _HAS_AGGREGATE_GRAD = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_create_all_weights</span><span class="params">(self, params)</span>:</span></span><br><span class="line">    <span class="comment"># 声明除了grads以外用于梯度更新的参数，创建内存空间，在get_updates方法中使用</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_updates</span><span class="params">(self, loss, params)</span>:</span></span><br><span class="line">    <span class="comment"># 定义梯度更新的计算方法, 更新self.updates</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">   </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># config里面是优化器相关的参数，默认只有两个梯度截断的参数，需要根据实际优化器添加（lr、decay ...）</span></span><br><span class="line">    config = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> hasattr(self, <span class="string">'clipnorm'</span>):</span><br><span class="line">      config[<span class="string">'clipnorm'</span>] = self.clipnorm</span><br><span class="line">    <span class="keyword">if</span> hasattr(self, <span class="string">'clipvalue'</span>):</span><br><span class="line">      config[<span class="string">'clipvalue'</span>] = self.clipvalue</span><br><span class="line">    <span class="keyword">return</span> config</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_gradients</span><span class="params">(self, loss, params)</span>:</span></span><br><span class="line">    <span class="comment"># 计算梯度值，并在有必要时进行梯度截断</span></span><br><span class="line">    grads = K.gradients(loss, params)</span><br><span class="line">    <span class="keyword">if</span> any(g <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">for</span> g <span class="keyword">in</span> grads):</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">'An operation has `None` for gradient. '</span></span><br><span class="line">                       <span class="string">'Please make sure that all of your ops have a '</span></span><br><span class="line">                       <span class="string">'gradient defined (i.e. are differentiable). '</span></span><br><span class="line">                       <span class="string">'Common ops without gradient: '</span></span><br><span class="line">                       <span class="string">'K.argmax, K.round, K.eval.'</span>)</span><br><span class="line">    <span class="keyword">if</span> hasattr(self, <span class="string">'clipnorm'</span>):</span><br><span class="line">      grads = [tf.clip_by_norm(g, self.clipnorm) <span class="keyword">for</span> g <span class="keyword">in</span> grads]</span><br><span class="line">    <span class="keyword">if</span> hasattr(self, <span class="string">'clipvalue'</span>):</span><br><span class="line">      grads = [</span><br><span class="line">          tf.clip_by_value(g, -self.clipvalue, self.clipvalue)</span><br><span class="line">          <span class="keyword">for</span> g <span class="keyword">in</span> grads</span><br><span class="line">      ]</span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">set_weights</span><span class="params">(self, weights)</span>:</span></span><br><span class="line"><span class="comment"># 给optimizer的weights用一系列np array赋值</span></span><br><span class="line">    <span class="comment"># 没看到有调用，省略code： K.batch_set_value()</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># 获取weights的np array值</span></span><br><span class="line">    <span class="comment"># 没看到有调用，省略code： K.batch_get_value()</span></span><br><span class="line"></span><br><span class="line"><span class="meta">  @classmethod</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">from_config</span><span class="params">(cls, config)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> cls(**config)</span><br></pre></td></tr></table></figure><p>9.3 实例化一个优化器</p><ul><li>based on keras.Optimizer对象</li><li>主要需要重写get_updates和get_config方法<ul><li>get_updates用来定义梯度更新的计算方法</li><li>get_config用来定义实例用到的参数</li></ul></li><li>以SoftSGD为例：<ul><li>每隔一定的batch才更新一次参数，不更新梯度的step梯度不清空，执行累加，从而实现batchsize的变相扩大</li><li>建议搭配间隔更新参数的BN层来使用，否则BN还是基于小batchsize来更新均值和方差</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SoftSGD</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line">    <span class="comment"># [new arg] steps_per_update: how many batch to update gradient</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, lr=<span class="number">0.01</span>, momentum=<span class="number">0.</span>, decay=<span class="number">0.</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 nesterov=False, steps_per_update=<span class="number">2</span>, **kwargs)</span>:</span></span><br><span class="line">        super(SoftSGD, self).__init__(**kwargs)</span><br><span class="line">        <span class="keyword">with</span> K.name_scope(self.__class__.__name__):</span><br><span class="line">            self.iterations = K.variable(<span class="number">0</span>, dtype=<span class="string">'int64'</span>, name=<span class="string">'iterations'</span>)</span><br><span class="line">            self.lr = K.variable(lr, name=<span class="string">'lr'</span>)</span><br><span class="line">            self.steps_per_update = steps_per_update  <span class="comment"># 多少batch才更新一次</span></span><br><span class="line">            self.momentum = K.variable(momentum, name=<span class="string">'momentum'</span>)</span><br><span class="line">            self.decay = K.variable(decay, name=<span class="string">'decay'</span>)</span><br><span class="line">        self.initial_decay = decay</span><br><span class="line">        self.nesterov = nesterov</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_updates</span><span class="params">(self, loss, params)</span>:</span></span><br><span class="line">        <span class="comment"># learning rate decay</span></span><br><span class="line">        lr = self.lr</span><br><span class="line">        <span class="keyword">if</span> self.initial_decay &gt; <span class="number">0</span>:</span><br><span class="line">            lr = lr * (<span class="number">1.</span> / (<span class="number">1.</span> + self.decay * K.cast(self.iterations, K.dtype(self.decay))))</span><br><span class="line"> </span><br><span class="line">        shapes = [K.int_shape(p) <span class="keyword">for</span> p <span class="keyword">in</span> params]</span><br><span class="line">        sum_grads = [K.zeros(shape) <span class="keyword">for</span> shape <span class="keyword">in</span> shapes]  <span class="comment"># 平均梯度，用来梯度下降</span></span><br><span class="line">        grads = self.get_gradients(loss, params)  <span class="comment"># 当前batch梯度</span></span><br><span class="line">        self.updates = [K.update_add(self.iterations, <span class="number">1</span>)]</span><br><span class="line">        self.weights = [self.iterations] + sum_grads</span><br><span class="line">        <span class="keyword">for</span> p, g, sg <span class="keyword">in</span> zip(params, grads, sum_grads):</span><br><span class="line">            <span class="comment"># momentum 梯度下降</span></span><br><span class="line">            v = self.momentum * sg / float(self.steps_per_update) - lr * g  <span class="comment"># velocity</span></span><br><span class="line">            <span class="keyword">if</span> self.nesterov:</span><br><span class="line">                new_p = p + self.momentum * v - lr * sg / float(self.steps_per_update)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                new_p = p + v</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># 如果有约束，对参数加上约束</span></span><br><span class="line">            <span class="keyword">if</span> getattr(p, <span class="string">'constraint'</span>, <span class="keyword">None</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                new_p = p.constraint(new_p)</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># 满足条件才更新参数</span></span><br><span class="line">            cond = K.equal(self.iterations % self.steps_per_update, <span class="number">0</span>)</span><br><span class="line">            self.updates.append(K.switch(cond, K.update(p, new_p), p))</span><br><span class="line">            self.updates.append(K.switch(cond, K.update(sg, g), K.update(sg, sg + g)))</span><br><span class="line">        <span class="keyword">return</span> self.updates</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self)</span>:</span></span><br><span class="line">        config = &#123;<span class="string">'lr'</span>: float(K.get_value(self.lr)),</span><br><span class="line">                  <span class="string">'steps_per_update'</span>: self.steps_per_update,</span><br><span class="line">                  <span class="string">'momentum'</span>: float(K.get_value(self.momentum)),</span><br><span class="line">                  <span class="string">'decay'</span>: float(K.get_value(self.decay)),</span><br><span class="line">                  <span class="string">'nesterov'</span>: self.nesterov</span><br><span class="line">                  &#125;</span><br><span class="line">        base_config = super(SoftSGD, self).get_config()</span><br><span class="line">        <span class="keyword">return</span> dict(list(base_config.items()) + list(config.items()))</span><br></pre></td></tr></table></figure><h2 id="10-keras自定义激活函数activation"><a href="#10-keras自定义激活函数activation" class="headerlink" title="10. keras自定义激活函数activation"></a>10. keras自定义激活函数activation</h2><p>10.1 定义激活函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gelu</span><span class="params">(x)</span>:</span></span><br><span class="line">    cdf = <span class="number">0.5</span> * (<span class="number">1.0</span> + tf.erf(x / tf.sqrt(<span class="number">2.0</span>)))</span><br><span class="line">    <span class="keyword">return</span> x*cdf</span><br></pre></td></tr></table></figure><p>10.2 使用自定义激活函数</p><ul><li><p>使用Activation方法</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = Activation(gelu)(x)</span><br></pre></td></tr></table></figure></li><li><p>不能整合进带有activation参数的层（如Conv2D），因为Conv基类的get_config()方法从keras.activations里面读取相应的激活函数，其中带参数的激活函数如PReLU（Advanced activations）、以及自定义的激活函数都不在这个字典中，否则会报错：</p><p>  AttributeError: ‘Activation’ object has no attribute ‘<strong>name</strong>‘</p></li></ul><p>10.3 checkpoint issue</p><p>网上还有另一种写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Activation</span><br><span class="line"><span class="keyword">from</span> keras.utils.generic_utils <span class="keyword">import</span> get_custom_objects</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gelu</span><span class="params">(x)</span>:</span></span><br><span class="line">    cdf = <span class="number">0.5</span> * (<span class="number">1.0</span> + tf.erf(x / tf.sqrt(<span class="number">2.0</span>)))</span><br><span class="line">    <span class="keyword">return</span> x*cdf</span><br><span class="line">  </span><br><span class="line">get_custom_objects().update(&#123;<span class="string">'gelu'</span>: Activation(gelu)&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 后面可以通过名字调用激活函数</span></span><br><span class="line">x = Activation(<span class="string">'gelu'</span>)(x)</span><br></pre></td></tr></table></figure><p>这种写法在使用ModelCheckpoints方法保存权重时会报错：</p><p>AttributeError: ‘Activation’ object has no attribute ‘<strong>name</strong>‘</p><p>看log发现当使用名字代表激活层的时候，在保存模型的时候，又会有一个get_config()函数从keras.activations中查表</p><h2 id="11-keras自定义正则化器regularizers"><a href="#11-keras自定义正则化器regularizers" class="headerlink" title="11. keras自定义正则化器regularizers"></a>11. keras自定义正则化器regularizers</h2><p>11.1 使用封装好的regularizers</p><ul><li><p>keras的正则化器没有global的一键添加方法，要layer-wise为每一层添加</p></li><li><p>keras的层share 3 common参数接口：</p><ul><li>kernel_regularizer</li><li>bias_regularizer</li><li>activity_regularizer</li></ul></li><li><p>可选用的正则化器</p><ul><li>keras.regularizers.l1(0.01) </li><li>keras.regularizers.l2(0.01) </li><li>keras.regularizers.l1_l2(l1=0.01, l2=0.01)</li></ul></li><li><p>使用</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">layer = tf.keras.layers.Dense(<span class="number">5</span>, kernel_initializer=<span class="string">'ones'</span>,</span><br><span class="line">                              kernel_regularizer=tf.keras.regularizers.l1(<span class="number">0.01</span>),</span><br><span class="line">                              activity_regularizer=tf.keras.regularizers.l2(<span class="number">0.01</span>))</span><br><span class="line">tensor = tf.ones(shape=(<span class="number">5</span>, <span class="number">5</span>)) * <span class="number">2.0</span></span><br><span class="line">out = layer(tensor)</span><br><span class="line"><span class="comment"># The kernel regularization term is 0.25</span></span><br><span class="line"><span class="comment"># The activity regularization term (after dividing by the batch size) is 5</span></span><br><span class="line">print(tf.math.reduce_sum(layer.losses))  <span class="comment"># 5.25 (= 5 + 0.25)</span></span><br></pre></td></tr></table></figure></li></ul><p>11.2 custom regularizer</p><p>一般不会自定义这个东西，硬要custom的话，两种方式</p><ul><li>简单版，接口参数是weight_matrix，无额外参数，层直接调用</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_regularizer</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1e-3</span> * tf.reduce_sum(tf.square(x))</span><br><span class="line"></span><br><span class="line">layer = tf.keras.layers.Dense(<span class="number">5</span>, kernel_initializer=<span class="string">'ones'</span>, kernel_regularizer=my_regularizer)</span><br></pre></td></tr></table></figure><ul><li>子类继承版，可以加额外参数，需要补充get_config方法，支持读写权重时的串行化</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyRegularizer</span><span class="params">(regularizers.Regularizer)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, strength)</span>:</span></span><br><span class="line">        self.strength = strength</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.strength * tf.reduce_sum(tf.square(x))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'strength'</span>: self.strength&#125;</span><br><span class="line"></span><br><span class="line">layer = tf.keras.layers.Dense(<span class="number">5</span>, kernel_initializer=<span class="string">'ones'</span>, kernel_regularizer=MyRegularizer(<span class="number">0.01</span>))</span><br></pre></td></tr></table></figure><p>11.3 强行global</p><ul><li>每层加起来太烦了，批量加的实质也是逐层加，只不过写成循环</li><li>核心是layer的add_loss方法</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model = keras.applications.ResNet50(include_top=<span class="keyword">True</span>, weights=<span class="string">'imagenet'</span>)</span><br><span class="line">alpha = <span class="number">0.00002</span>  <span class="comment"># weight decay coefficient</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> model.layers:</span><br><span class="line">    <span class="keyword">if</span> isinstance(layer, keras.layers.Conv2D) <span class="keyword">or</span> isinstance(layer, keras.layers.Dense):</span><br><span class="line">        layer.add_loss(<span class="keyword">lambda</span>: keras.regularizers.l2(alpha)(layer.kernel))</span><br><span class="line">    <span class="keyword">if</span> hasattr(layer, <span class="string">'bias_regularizer'</span>) <span class="keyword">and</span> layer.use_bias:</span><br><span class="line">        layer.add_loss(<span class="keyword">lambda</span>: keras.regularizers.l2(alpha)(layer.bias))</span><br></pre></td></tr></table></figure><h2 id="12-keras查看梯度-amp-权重"><a href="#12-keras查看梯度-amp-权重" class="headerlink" title="12. keras查看梯度&amp;权重"></a>12. keras查看梯度&amp;权重</h2><p>12.1 easiest way</p><ul><li>查看梯度最简单的方法：通过K.gradients方法定义一个求梯度的func，然后给定输入，得到梯度（CAM就是这么干的）</li><li>查看权重最简单的方法：存在h5文件，然后花式h5py解析</li></ul><p>12.2 dig deeper</p><ul><li>一个思路：将梯度保存在optimizer的self.weights中，并在model.save得到的模型中解析</li></ul><h2 id="13-keras实现权重滑动平均"><a href="#13-keras实现权重滑动平均" class="headerlink" title="13. keras实现权重滑动平均"></a>13. keras实现权重滑动平均</h2><p>13.1 why EMA on weights</p><ul><li><p>[reference1][<a href="https://www.jiqizhixin.com/articles/2019-05-07-18]：权重滑动平均是提供训练稳定性的有效方法，要么在优化器里面实现，要么外嵌在训练代码里" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2019-05-07-18]：权重滑动平均是提供训练稳定性的有效方法，要么在优化器里面实现，要么外嵌在训练代码里</a></p></li><li><p>[reference2][<a href="https://cloud.tencent.com/developer/article/1636781]：这里面举的例子很清晰了，就是为了权重每个step前后变化不大" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1636781]：这里面举的例子很清晰了，就是为了权重每个step前后变化不大</a></p></li></ul><p>权重EMA的计算方式有点类似于BN的running mean&amp;var：</p><ul><li>在训练阶段：它不改变每个training step的优化方向，而是从initial weights开始，另外维护一组shadow weights，用每次的updating weights来进行滑动更新</li><li>在inference阶段，我们要用shadow weights来替换当前权重文件保存的weights（current step下计算的新权重）</li><li>如果要继续训练，要将替换的权重在换回来，因为【EMA不影响模型的优化轨迹】</li></ul><p>13.2 who uses EMA</p><ul><li>很多GAN的论文都用了EMA，</li><li>还有NLP阅读理解模型QANet，</li><li>还有Google的efficientNet、resnet_rs</li></ul><p>13.3 how to implement outside</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExponentialMovingAverage</span>:</span></span><br><span class="line">    <span class="string">"""对模型权重进行指数滑动平均。</span></span><br><span class="line"><span class="string">    用法：在model.compile之后、第一次训练之前使用；</span></span><br><span class="line"><span class="string">    先初始化对象，然后执行inject方法。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model, momentum=<span class="number">0.9999</span>)</span>:</span></span><br><span class="line">        self.momentum = momentum</span><br><span class="line">        self.model = model</span><br><span class="line">        self.ema_weights = [K.zeros(K.shape(w)) <span class="keyword">for</span> w <span class="keyword">in</span> model.weights]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inject</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""添加更新算子到model.metrics_updates。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.initialize()</span><br><span class="line">        <span class="keyword">for</span> w1, w2 <span class="keyword">in</span> zip(self.ema_weights, self.model.weights):</span><br><span class="line">            op = K.moving_average_update(w1, w2, self.momentum)</span><br><span class="line">            self.model.metrics_updates.append(op)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""ema_weights初始化跟原模型初始化一致。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.old_weights = K.batch_get_value(self.model.weights)</span><br><span class="line">        K.batch_set_value(zip(self.ema_weights, self.old_weights))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply_ema_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""备份原模型权重，然后将平均权重应用到模型上去。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.old_weights = K.batch_get_value(self.model.weights)</span><br><span class="line">        ema_weights = K.batch_get_value(self.ema_weights)</span><br><span class="line">        K.batch_set_value(zip(self.model.weights, ema_weights))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_old_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""恢复模型到旧权重。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        K.batch_set_value(zip(self.model.weights, self.old_weights))</span><br></pre></td></tr></table></figure><ul><li>then train</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">EMAer = ExponentialMovingAverage(model) <span class="comment"># 在模型compile之后执行</span></span><br><span class="line">EMAer.inject() <span class="comment"># 在模型compile之后执行</span></span><br><span class="line"></span><br><span class="line">model.fit(x_train, y_train) <span class="comment"># 训练模型</span></span><br></pre></td></tr></table></figure><ul><li>then inference</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MAer.apply_ema_weights() <span class="comment"># 将EMA的权重应用到模型中</span></span><br><span class="line">model.predict(x_test) <span class="comment"># 进行预测、验证、保存等操作</span></span><br><span class="line"></span><br><span class="line">EMAer.reset_old_weights() <span class="comment"># 继续训练之前，要恢复模型旧权重。还是那句话，EMA不影响模型的优化轨迹。</span></span><br><span class="line">model.fit(x_train, y_train) <span class="comment"># 继续训练</span></span><br></pre></td></tr></table></figure><h2 id="14-keras的Model类继承"><a href="#14-keras的Model类继承" class="headerlink" title="14. keras的Model类继承"></a>14. keras的Model类继承</h2><p>14.1 定义模型的方式</p><ul><li>Sequential：最简单，但是不能表示复杂拓扑结构</li><li>函数式 API：和Sequential用法基本一致，输入张量和输出张量用于定义 tf.keras.Model实例</li><li>模型子类化：引入于 Keras 2.2.0</li><li>keras源代码定义在：<a href="https://github.com/keras-team/keras/blob/master/keras/engine/training.py" target="_blank" rel="noopener">https://github.com/keras-team/keras/blob/master/keras/engine/training.py</a></li></ul><p>14.2 模型子类化overview</p><ul><li>既可以用来定义一个model，也可以用来定义一个复杂的网络层，为实现复杂模型提供更大的灵活性</li><li>有点类似于torch的语法<ul><li>网络层定义在 <code>__init__(self, ...)</code> 中：跟torch语法的主要区别在于层不能复用，torch同一个层在forward中每调用一次能够创建一个实例，keras每个层应该是在init中声明并创建，所以不能复用</li><li>前向传播在 <code>call(self, inputs)</code> 中，这里面也可以添加loss</li><li>compute_output_shape计算模型输出的形状</li></ul></li><li>和keras自定义层的语法也很相似<ul><li>build(input_shape)：主要区别就在于build，因为自定义层有build，显式声明了数据流的shape，能够构造出静态图</li><li>call(x)：</li><li>compute_output_shape(input_shape)：</li></ul></li><li>【以下方法和属性不适用于类继承模型】，所以还是推荐优先使用函数式 API<ul><li>model.inputs &amp; model.outputs</li><li>model.to_yaml() &amp; model.to_json()</li><li>model.get_config() &amp; model.save()：！！！只能save_weights！！！</li></ul></li></ul><p>14.3 栗子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleMLP</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes=<span class="number">10</span>)</span>:</span></span><br><span class="line">        super(SimpleMLP, self).__init__(name=<span class="string">'mlp'</span>)</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.dense1 = keras.layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.dense2 = keras.layers.Dense(num_classes, activation=<span class="string">'softmax'</span>)</span><br><span class="line">        self.dp = keras.layers.Dropout(<span class="number">0.5</span>)</span><br><span class="line">        self.bn = keras.layers.BatchNormalization(axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None, mask=None)</span>:</span></span><br><span class="line">        x = self.dense1(inputs)</span><br><span class="line">        x = self.dp(x)</span><br><span class="line">        x = self.bn(x, training=training)</span><br><span class="line">        <span class="keyword">return</span> self.dense2(x)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_output_shape</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        batch, dim = input_shape</span><br><span class="line">        <span class="keyword">return</span> (batch, self.num_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = SimpleMLP()</span><br><span class="line">model.compile(<span class="string">'adam'</span>, loss=<span class="string">'categorical_crossentropy'</span>)</span><br><span class="line">x = np.random.uniform(<span class="number">0</span>,<span class="number">1</span>,(<span class="number">32</span>,<span class="number">100</span>))</span><br><span class="line">y = np.random.randint(<span class="number">0</span>, <span class="number">2</span>, (<span class="number">32</span>,<span class="number">10</span>))</span><br><span class="line">model.fit(x, y)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><ul><li>可以看到，类继承模型是没有指明input_shape的，所以也就不存在静态图，要在有真正数据流以后，model才被build，才能够调用summay方法，查看图结构</li><li>第二个是，call方法的默认参数：def call(self, inputs, training=None, mask=None)，<ul><li>子类继承模型不支持显式的多输入定义，所有的输入构成inputs</li><li>需要手工管理training参数，bn/dropout等在train/inference mode下计算不一样的情况，要显式传入training参数</li><li>mask在构建Attention机制或者序列模型时会使用到，如果previous layer生成了掩码（embedding的mask_zero参数为True），前两种构建模型的方法中，mask会自动传入当前层的call方法中</li></ul></li></ul>]]></content>
      
      
        <tags>
            
            <tag> keras </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>opencv库函数</title>
      <link href="/2019/08/11/opencv%E5%BA%93%E5%87%BD%E6%95%B0/"/>
      <url>/2019/08/11/opencv%E5%BA%93%E5%87%BD%E6%95%B0/</url>
      <content type="html"><![CDATA[<h3 id="1-imshow"><a href="#1-imshow" class="headerlink" title="1. imshow"></a>1. imshow</h3><p>有时候imshow的图片会显示的和原图不一样，要查看read进来的数据格式，imshow会根据读入的数据格式自动进行归一化，映射到0-255。</p><ul><li>如果image是默认的8-bit unsigned（0-255），不做处理。</li><li>如果image是16-bit unsigned（0-65535）或者32-bit integer（？？贼大），像素值除以256，[0,255*256]归一化到[0，255]。</li><li>如果image是32-bit float，像素值乘以255，[0,1]归一化到[0，255]。</li></ul><h3 id="2-imwrite"><a href="#2-imwrite" class="headerlink" title="2. imwrite"></a>2. imwrite</h3><p>通常imwrite把所有数据都强制转换成uchar（0-255）。</p>]]></content>
      
      
    </entry>
    
    <entry>
      <title>beautifulsoup saving file </title>
      <link href="/2019/06/11/beautifulsoup-saving-file/"/>
      <url>/2019/06/11/beautifulsoup-saving-file/</url>
      <content type="html"><![CDATA[<p>最近用bs4处理xml文件，遇到了一个在爬虫时候从未思考过的问题——</p><p><strong>修正从xml文件中解析出的文件树，并将changes保存到原来的xml文件中。</strong></p><p>我一直在beautifulsoup的手册中去寻找库函数，实际只需要简单的文件读写操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(open(<span class="string">'test.xml'</span>), <span class="string">'xml'</span>)</span><br><span class="line">add = BeautifulSoup(<span class="string">"&lt;a&gt;Foo&lt;/a&gt;"</span>, <span class="string">'xml'</span>)</span><br><span class="line">soup.orderlist.append(add)</span><br><span class="line">print(soup.prettify())</span><br><span class="line">f = open(<span class="string">'test.xml'</span>, <span class="string">'w'</span>)</span><br><span class="line">f.write(str(soup))</span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure><p>附一个简单xml文件用来实验：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="utf-8"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">orderlist</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">order</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">customer</span>&gt;</span>姓名1<span class="tag">&lt;/<span class="name">customer</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">phone</span>&gt;</span>电话1<span class="tag">&lt;/<span class="name">phone</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">address</span>&gt;</span>地址1<span class="tag">&lt;/<span class="name">address</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">count</span>&gt;</span>点餐次数1<span class="tag">&lt;/<span class="name">count</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">order</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">order</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">customer</span>&gt;</span>姓名2<span class="tag">&lt;/<span class="name">customer</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">phone</span>&gt;</span>电话2<span class="tag">&lt;/<span class="name">phone</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">address</span>&gt;</span>地址2<span class="tag">&lt;/<span class="name">address</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">count</span>&gt;</span>点餐次数2<span class="tag">&lt;/<span class="name">count</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">order</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ssh visualization</title>
      <link href="/2018/12/29/ssh-visualization/"/>
      <url>/2018/12/29/ssh-visualization/</url>
      <content type="html"><![CDATA[<ol><li><p>ssh boocax@192.168.1.100</p><p>密码：robot123</p></li><li><p>echo $ROS_MASTER_URI</p><p>查看端口号11311</p></li><li><p>小车端：</p><p>export ROS_MASTER_URI=<a href="http://192.168.1.100:11311" target="_blank" rel="noopener">http://192.168.1.100:11311</a></p><p>export ROS_IP=192.168.1.100</p></li><li><p>虚拟机端：</p><p>export ROS_MASTER_URI=<a href="http://192.168.1.100:11311" target="_blank" rel="noopener">http://192.168.1.100:11311</a></p><p>export ROS_IP=172.16.128.142</p></li></ol><h4 id=""><a href="#" class="headerlink" title=" "></a> </h4><h6 id="-1"><a href="#-1" class="headerlink" title="#"></a>#</h6><h4 id="-2"><a href="#-2" class="headerlink" title=" "></a> </h4><ol><li><p>nav远程开启三个终端（代码重构以前）：</p><p>move_base: roslaunch teleop_twist_joy real_nav.launch </p><p>mapserver: rosrun map_server map_server catkin_ws2/src/patrol/map/p1.yaml </p><p>amcl: roslaunch patrol real_loc.launch </p></li><li><p>本地可视化：rviz／rqt_graph / rosservice call /rostopic pub</p><ul><li><p>全局定位：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rosservice call /global_localization "&#123;&#125;"</span><br></pre></td></tr></table></figure></li><li><p>设置导航目标点：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 相对base_link坐标系</span><br><span class="line">rostopic pub -1 /navigation_simple/goal geometry_msgs/PoseStamped '&#123; header: &#123; frame_id: "base_link" &#125;, pose: &#123; position: &#123; x: 0.5, y: 0.0, z: 0 &#125;, orientation: &#123; x: 0, y: 0, z: 0, w: 1 &#125; &#125; &#125;'</span><br><span class="line"></span><br><span class="line">// 相对map坐标系</span><br><span class="line">rostopic pub -1 /navigation_simple/goal geometry_msgs/PoseStamped '&#123; header: &#123; frame_id: "map" &#125;, pose: &#123; position: &#123; x: 5, y: 0.0, z: 0 &#125;, orientation: &#123; x: 0, y: 0, z: 0, w: 1 &#125; &#125; &#125;'</span><br></pre></td></tr></table></figure><p>注意<strong>-1</strong>，否则循环发布。</p></li></ul></li></ol><h4 id="-3"><a href="#-3" class="headerlink" title=" "></a> </h4><h6 id="-4"><a href="#-4" class="headerlink" title="#"></a>#</h6><h4 id="-5"><a href="#-5" class="headerlink" title=" "></a> </h4><p>往回备份：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r boocax@192.168.1.100:/home/boocax/catkin_ws2019 bkp/</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> ROS </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Occupancy Grid Map</title>
      <link href="/2018/12/04/Occupancy-Grid-Map/"/>
      <url>/2018/12/04/Occupancy-Grid-Map/</url>
      <content type="html"><![CDATA[<p>to be completed…</p><ul><li>Inverse Sensor Model</li><li>Incremental Updating</li></ul>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>object tracking</title>
      <link href="/2018/12/03/object-tracking/"/>
      <url>/2018/12/03/object-tracking/</url>
      <content type="html"><![CDATA[<ol><li><p>范围限定：过滤掉较远范围的点云数据</p></li><li><p>聚类：K-Means／欧式聚类，因为前者需要设定K，故使用后者。</p><script type="math/tex; mode=display">D(p_i, p_{i+1}) = \sqrt{r_i^2 + r_{i+1}^2 - 2r_ir_{i+1}cos(\varphi_{i+1} - \varphi_i)}</script><p>如果连续扫描点之间的距离小于一个阈值$D_t$，那么这两个点被认为属于同一个对象。这个阈值是根据当前参考点的距离动态调整的。</p><script type="math/tex; mode=display">D_t = D_0 + a*r_i*sin(\Delta \varphi)</script></li><li><p>运动目标特征提取：（中心坐标，长／宽／半径，反射强度）</p></li><li><p>由上一时刻的位置速度设置ROI：</p><ul><li>基于局部匹配：通过相似度计算选取响应值最高的目标</li><li>基于分类器：动态目标已知（人腿），采集正负样本，构造分类器，</li></ul></li><li><p>卡尔曼滤波：</p></li></ol>]]></content>
      
      
        <tags>
            
            <tag> extensions for slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>matplotlib的colormap</title>
      <link href="/2018/11/29/matplotlib%E7%9A%84colormap/"/>
      <url>/2018/11/29/matplotlib%E7%9A%84colormap/</url>
      <content type="html"><![CDATA[<p>用plt的imshow画图，总是找不到心仪的colorbar，可以自定义：</p><ol><li><p>在原有cmap基础上自定义：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">colorbar = plt.get_cmap(<span class="string">'Greys'</span>)(range(<span class="number">180</span>))</span><br><span class="line">cm = LinearSegmentedColormap.from_list(name=<span class="string">"grey_cm"</span>, colors=colorbar)</span><br><span class="line">plt.register_cmap(cmap=cm)</span><br><span class="line"></span><br><span class="line">plt.imshow(map2d.data, cmap=<span class="string">'grey_cm'</span>)</span><br></pre></td></tr></table></figure></li><li><p>define一个新的cmap：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">colormap</span><span class="params">()</span>:</span></span><br><span class="line">    colors = [<span class="string">'#FFFFFF'</span>, <span class="string">'#9ff113'</span>, <span class="string">'#5fbb44'</span>, <span class="string">'#f5f329'</span>, <span class="string">'#e50b32'</span>]</span><br><span class="line">    <span class="keyword">return</span> colors.ListedColormap(colors, <span class="string">'my_cmap'</span>)</span><br><span class="line"></span><br><span class="line">my_cmap = colormap()</span><br><span class="line">plt.imshow(map2d.data, cmap=my_cmap)</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>mpv-video-cutter</title>
      <link href="/2018/11/26/mpv-video-cutter/"/>
      <url>/2018/11/26/mpv-video-cutter/</url>
      <content type="html"><![CDATA[<p>mpv的小插件，能够一键（三键）剪辑。</p><p>工程地址：<a href="https://github.com/rushmj/mpv-video-cutter" target="_blank" rel="noopener">https://github.com/rushmj/mpv-video-cutter</a></p><p>step1：把c_concat.sh和cutter.lua两个文件复制到~/.config/mpv/scripts/目录下。</p><p>step2：<strong>给c_concat.sh脚本添加执行权限</strong>。</p><p>step3：用命令行打开文件，c-c-o在原目录下生成剪辑文件。</p>]]></content>
      
      
        <tags>
            
            <tag> tool </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>problems with ROS</title>
      <link href="/2018/11/22/problems-with-ROS/"/>
      <url>/2018/11/22/problems-with-ROS/</url>
      <content type="html"><![CDATA[<ol><li><p><code>[WARN] Detected jump back in time of 5.51266s. Clearing TF buffer.</code></p><p>手动建图的时候，时不时的就跳出来这个，然后小车跳变到初始位置，而且还是根据TF buffer回溯回去的，真高级。。。</p><p>排查原因发现竟然是忘记运行roscore了，mmp。</p></li><li><p><code>[rosrun] Couldn&#39;t find executable named patrol.py below /home/carrol/catkin_ws/src/patrol</code></p><p>原因如提示，python是脚本执行，要添加可执行权限。</p></li><li><p><code>error: ‘array’ is not a member of ‘std’</code></p><p>编译导航包时反复出现这个错误，因为cmake版本比较低（2.8），不会自动找c++11，解决办法在对应package的cmake文件中添加c++声明：<code>add_definitions(-std=c++11)</code></p></li><li><p>同样的错误catkin_make时重复出现，我还以为问题没解决：</p><p>删除build文件夹中对应包，再进行catkin_make。如果删除了某个包，还要删除devel文件夹再编译。</p></li><li><p>cmake warning conflicts with Anaconda：</p><p>编译到最后会卡死，错误具体啥意思我也没弄明白，粗暴解决了，将系统环境变量里面的anaconda path暂时屏蔽，首先查看环境变量：<code>echo $PATH</code>，然后返回结果：</p><p>/home/[username]/anaconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games</p><p>然后在当前命令行执行：<code>export PATH=&quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games&quot;</code></p></li><li><p><code>c++: internal compiler error: Killed (program cc1plus)</code></p><p>虚拟机内存不足。</p></li><li><p><code>undefined error with CONSOLE_BRIDGE_logError/CONSOLE_BRIDGE_logWarn</code></p><p>安装并编译<code>console_bridge</code>包，<strong>注意build instructions：</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git clone git://github.com/ros/console_bridge.git</span><br><span class="line">cd console_bridge</span><br><span class="line">cmake .</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure></li><li><p><code>there are no arguments to ‘logDebug’ that depend on a template parameter, so a declaration of ‘logDebug’ must be available [-fpermissive]</code></p><p>参考（<a href="https://talk.apolloauto.io/t/topic/77" target="_blank" rel="noopener">Reference</a>），还是上面的问题，    <code>console_bridge</code>的API变了，将<code>logDebug</code>改成<code>CONSOLE_BRIDGE_logDebug</code>就行了。</p></li><li><p>running environment相关包的缺失和安装：</p><p>在<a href="https://packages.ubuntu.com/xenial/libconsole-bridge-dev" target="_blank" rel="noopener">官网</a>查找相关包和依赖，然后执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># install</span><br><span class="line">sudo dpkg -i 软件包名.deb</span><br><span class="line"></span><br><span class="line"># uninstall</span><br><span class="line">sudo apt-get remove 软件包名称</span><br></pre></td></tr></table></figure></li><li></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ROS </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>amcl</title>
      <link href="/2018/11/16/amcl/"/>
      <url>/2018/11/16/amcl/</url>
      <content type="html"><![CDATA[<h4 id="滤波："><a href="#滤波：" class="headerlink" title="滤波："></a>滤波：</h4><p>机器人从已知点$x_0$开始运动，里程计误差逐渐累积，位置不确定性将越来越大（$x_1, x_2$）。因此需要借助外部环境信息对自己进行定位，于是引入测量值$d$，计算出当前位置$x_2^{‘}$，再结合预测值$x_2$，得到一个矫正位置$x_2^{‘’}$，使其<strong>不确定性降到最小</strong>。</p><p><img src="/2018/11/16/amcl/filter.png" alt=""></p><h4 id="贝叶斯滤波：-p-x-z-frac-p-z-x-p-x-p-z"><a href="#贝叶斯滤波：-p-x-z-frac-p-z-x-p-x-p-z" class="headerlink" title="贝叶斯滤波：$p(x|z) = \frac{p(z|x)p(x)}{p(z)}$"></a>贝叶斯滤波：$p(x|z) = \frac{p(z|x)p(x)}{p(z)}$</h4><p>先验：$p(x_t|u_t, x_{t-1})$，通过预测方程得到</p><p>似然：$p(z_t| x_t)$，通过测量方程得到</p><p>后验：$p(x_t|z_t)$，通过贝叶斯方程得到</p><p>对于一般的非线性、非高斯系统，很难通过上述方法得到后验概率的解析解。</p><h4 id="蒙特卡洛采样："><a href="#蒙特卡洛采样：" class="headerlink" title="蒙特卡洛采样："></a>蒙特卡洛采样：</h4><p>假设能从一个目标分布$p(x)$获得一系列样本$x_1, x2, …, x_N$，那么就能利用这些样本去估计这个分布的某些函数的期望值。</p><script type="math/tex; mode=display">E(f(x)) = \int_a^{b}f(x)p(x)dx \approx\frac{f(x_1) + f(x_2) + ... + f(x_N)}{N}</script><p>蒙特卡洛采样的核心思想就是用均值来代替积分。</p><p>假设可以从后验概率中采样到N个样本，那么后验概率可以表示为：</p><script type="math/tex; mode=display">\hat p(x_t|z_{1:t}) = \frac{1}{N} \sum_{i=1}^{N}\delta(x_n - x_n^{i}) \approx p(x_t|z_{1:t})</script><h4 id="粒子滤波："><a href="#粒子滤波：" class="headerlink" title="粒子滤波："></a>粒子滤波：</h4><script type="math/tex; mode=display">\begin{align}E(f(x)) & \approx \int f(x_n) \hat p(x_t|z_{1:t}) dx \nonumber\\& = \frac{1}{N}\sum_N f(x_n) \delta(x_n - x_n^{i})\nonumber\\& = \frac{1}{N}\sum_N f(x_n^{i}) \nonumber\end{align}</script><p>用采样粒子（服从后验概率）的状态值直接平均作为期望值，这就是粒子滤波。</p><h4 id="MCL：蒙特卡洛定位／粒子滤波定位"><a href="#MCL：蒙特卡洛定位／粒子滤波定位" class="headerlink" title="MCL：蒙特卡洛定位／粒子滤波定位"></a><a href="https://en.wikipedia.org/wiki/Monte_Carlo_localization" target="_blank" rel="noopener">MCL</a>：蒙特卡洛定位／粒子滤波定位</h4><ol><li><p>Randomly generate a bunch of particles</p></li><li><p>Predict next state of the particles</p></li><li><p>Update the weighting of the particles based on the measurement. </p></li><li><p>Resample：Discard highly improbable particle and replace them with copies of the more probable particles. </p><blockquote><p>This leads to a new particle set with uniform importance weights, but with an increased number of particles near the three likely places. </p></blockquote></li><li><p>Compute the weighted mean and covariance of the set of particles to get a state estimate.</p></li></ol><p><strong>权值退化：</strong>如果任由粒子权值增长，只有少数粒子的权值较大，其余粒子的权值可以忽略不计，变成无效粒子，因此需要引入<strong>重采样</strong>。采用$N_{eff}$衡量粒子权值的退化程度。</p><script type="math/tex; mode=display">N_{eff} \approx \hat{N_{eff}} = \frac{1}{\sum_N (w_k^{i})^2}</script><p><strong>粒子多样性：</strong>通常我们会舍弃权值较小的粒子，代之以权值较大的粒子。这样会导致权值小的粒子逐渐绝种，粒子群多样性减弱，从而不足以近似表征后验密度。</p><h4 id="重要性采样"><a href="#重要性采样" class="headerlink" title="重要性采样"></a>重要性采样</h4><p>实际上后验概率并不知道，谈何采样（$x_n^i$）。我们可以从一个已知的分布$q(x|z)$里来采样，间接得到滤波值。</p><script type="math/tex; mode=display">\begin{align}E(f(x_k))&  = \int f(x) \frac{p(x|z)}{q(x|z)} q(x|z) dx \nonumber\\& = \frac{E_{q(x|z)}W_k(x_k)f(x_k)}{E_{q(x|z)}W_k(x_k)}\nonumber\\& \approx \frac{\frac{1}{N} \sum_{i=1}^N W_k({x_k^{i})f(x_k^{i})}}{\frac{1}{N} \sum_{i=1}^N W_k({x_k^{i}})}\nonumber\\& = \sum_N \hat W_k(x_k^i)f(x_k^i) \nonumber\end{align}</script><p>相比较于原始的均值表示，变成了加权平均值。不同粒子拥有了不同的权重。</p><script type="math/tex; mode=display">\hat W_k(x_k^i) = \frac{W_k(x_k^i)}{\sum_N W_k(x_k^i)}\\W_k (x_k) \propto \frac{p(x_k|z_{1:k})}{q(x_k|z_{1:k})}</script><p>已知的$q$分布叫做<strong>重要性概率密度函数</strong>。</p><h4 id="递推算法：序贯重要性采样"><a href="#递推算法：序贯重要性采样" class="headerlink" title="递推算法：序贯重要性采样"></a>递推算法：序贯重要性采样</h4><script type="math/tex; mode=display">\{x_k^i, w_k^i\} = SIS(\{x_{k-1}, w_{k-1}\})_{i=1}^N, y_k)</script><p>首先假设重要性分布$q(x|z)$满足：</p><script type="math/tex; mode=display">q(x_k | x_{0:k-1}, y_{1:k}) = q(x_k|x_{k-1}, y_k)</script><p>即只和前一时刻的状态$x_{k-1}$和测量$y_k$有关。于是有：</p><script type="math/tex; mode=display">w_k^i \approx w_{k-1}^i \frac{p(y_k|x_k^i)p(x_k^i|x_{k-1}^i)}{q(x_k^i|x_{k-1}^i, y_k)}</script><p>伪代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">For i=<span class="number">1</span>:N</span><br><span class="line">(<span class="number">1</span>)采样：式<span class="number">1</span></span><br><span class="line">(<span class="number">2</span>)权值更新：式<span class="number">2</span></span><br><span class="line">End For</span><br><span class="line">权值归一化</span><br><span class="line">加权平均得到粒子滤波值，也就是当前状态的估计值</span><br><span class="line">重采样</span><br></pre></td></tr></table></figure><h4 id="重采样"><a href="#重采样" class="headerlink" title="重采样"></a>重采样</h4><p>既然权重小的那些粒子不起作用了，那就不要了。为了保持粒子数目不变，就要补充新粒子，最简单的办法就是复制权重大的粒子。用$x_k^i$表示k时刻的粒子，$x_k^j$表示重采样以后的粒子，那么：</p><script type="math/tex; mode=display">\tilde p (x_k|y_{1:k}) = \sum_N \frac{1}{N}\delta(x_k - x_k^j) = \sum_N \frac{n_i}{N}\delta(x_k - x_i^j)</script><p>总的来说，新粒子按照权重比例来补充，算法流程为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">计算概率累积和wcum(N)</span><br><span class="line">用[<span class="number">0</span>,<span class="number">1</span>]之间的均匀分布随机采样N个值u(N)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="number">1</span>:N:</span><br><span class="line">k = <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> u(i)&lt;wcum(k):</span><br><span class="line">k += <span class="number">1</span></span><br><span class="line">end <span class="keyword">while</span></span><br><span class="line">resample(i) = k</span><br></pre></td></tr></table></figure><p><img src="/2018/11/16/amcl/resample.png" alt=""></p><h4 id="SIR滤波器（Sampling-Importance-Resampling-Filter-）"><a href="#SIR滤波器（Sampling-Importance-Resampling-Filter-）" class="headerlink" title="SIR滤波器（Sampling Importance Resampling Filter ）"></a>SIR滤波器（Sampling Importance Resampling Filter ）</h4><p>选取特定的重要性概率密度函数：</p><script type="math/tex; mode=display">q(x_k^i|x_{k-1}^i, y_k) = p(x_k^i|x_{k-1}^i)</script><p>于是权重更新公式可以简化：</p><script type="math/tex; mode=display">w_k^i \propto w_{k-1}^i \frac{p(z|x)p(x|x_{k-1})}{q(x|x_{k-1})}</script><p>由于重采样以后，粒子分布更新，权值统一为$\frac{1}{N}$，于是权重更新公式进一步简化：</p><script type="math/tex; mode=display">w_k^i \propto p(z_k|x_k^i)</script><p>根据测量方程可知，上面这个概率就是以真实测量值为均值，以噪声方差为方差的高斯分布。</p><p>此算法中的采样，并没有加入测量$z_k$，只凭先验知识$p(x_k|x_{k-1})$，虽然简单易用，但是存在效率不高和对奇异点(outliers)敏感的问题。 </p><h4 id="AMCL"><a href="#AMCL" class="headerlink" title="AMCL"></a>AMCL</h4><p>MCL算法能够用于全局定位，但是无法从<strong>机器人绑架</strong>或<strong>全局定位失败</strong>中恢复过来，因为随着位置被获取，其他地方的不正确粒子会逐渐消失。稳定状态下，粒子只“生存”在一个单一的姿态附近，如果这个姿态恰好不正确（在重采样步骤中可能意外的丢弃所有正确位姿附近的粒子），算法就无法恢复。</p><p>AMCL就是为了解决上述问题：结合了自适应（Augmented_MCL）和库尔贝克-莱不勒散度采样（KLD_Sampling_MCL）</p><ul><li>Augmented_MCL：在机器人遭到绑架的时候，它会在发现粒子们的平均分数突然降低了，这意味着正确的粒子在某次迭代中被抛弃了，此时会随机的全局注入粒子（injection of random particles）。</li><li>KLD_Sampling_MCL：动态调整粒子数，当机器人定位差不多得到了的时候，粒子都集中在一块了，就没必要维持这么多的粒子了——在栅格地图中，看粒子占了多少栅格。占得多，说明粒子很分散，在每次迭代重采样的时候，允许粒子数量的上限高一些。占得少，说明粒子都已经集中了，那就将上限设低。</li></ul><p><img src="/2018/11/16/amcl/mcl&amp;amcl.png" alt="mcl&amp;amcl"></p><p>算法流程上看，augmented_MCL算法最显著的区别就是引入了四个参数用于失效恢复：</p><ul><li>$w_{slow}$：长期似然平均估计</li><li>$w_{fast}$：短期似然平均估计</li><li>$\alpha_{slow}$：长期指数滤波器衰减率</li><li>$\alpha_{fast}$：短期指数滤波器衰减率</li></ul><p>失效恢复的核心思想是：测量似然的一个突然衰减（短期似然劣于长期似然）象征着粒子质量的下降，这将引起随机采样数目的增加。</p><p>$w_{avg}$计算了粒子的平均权重，当粒子质量下降时，平均权重随之下降，$w_{slow}、w_{fast}$也会随之下降，但是显然$w_{fast}$下降的速度要快于$w_{slow}$——这由衰减率决定，因此随机概率$p = 1 - \frac{w_{fast}}{w_{slow}}$会增大，随机粒子数目增加。而当粒子质量提高时，粒子短期权重要好于长期，随机概率小于0，不生成随机粒子。</p><p><img src="/2018/11/16/amcl/重定位.png" alt="重定位"></p><h4 id="ROS-amcl参数解析"><a href="#ROS-amcl参数解析" class="headerlink" title="ROS amcl参数解析"></a>ROS amcl<a href="https://www.cnblogs.com/dyan1024/p/7825988.html" target="_blank" rel="noopener">参数解析</a></h4><p>Augmented_MCL：</p><ul><li><code>&lt;param name=&quot;recovery_alpha_slow&quot; value=&quot;0.0&quot;/&gt;</code>：默认0（MCL），我的0.001。</li><li><code>&lt;param name=&quot;recovery_alpha_fast&quot; value=&quot;0.0&quot;/&gt;</code>：默认0（MCL），我的0.8。</li><li>在rviz里通过2D Pose Estimate按钮移动机器人来触发，机器人位置突变后要过一会儿才注入随机粒子，因为概率是渐变的。</li></ul><p>KLD：</p><ul><li><code>&lt;param name=&quot;kld_z&quot; value=&quot;0.99&quot;/&gt;</code>： KLD采样以概率$1-\delta（kld_z）$确定样本数。</li><li><code>&lt;param name=&quot;kld_err&quot; value=&quot;0.05&quot;/&gt;</code>： 真实的后验与基于采样的近似之间的误差。</li></ul><p>动态障碍物：环境中的动态物体总是会获得比静态障碍物更短的读数，因此可以根据这样的不对称性去除异常值。</p><ul><li>静态障碍物应该服从稳定的高斯分布，以距离传感器的真实距离为均值。</li><li>扫描到动态目标的beam则服从衰减分布，$-\eta e ^{-\lambda z}$。</li><li><code>laser_model_type</code>：使用beam model时会用到四个<strong>混合权重</strong>参数z_hit，z_short，z_max和z_rand，使用likelihood_field model时使用两个z_hit和z_rand。<ul><li><code>laser_z_hit</code>：default=0.95，以真实值为均值的噪声高斯分布</li><li><code>laser_z_rand</code>：defualt=0.05，随机测量权重，均匀分布</li><li><code>laser_z_short</code>：default=0.1，意外对象权重，衰减分布</li><li><code>laser_z_max</code>：default=0.05，测量失败权重，0/1分布</li></ul></li></ul><p>初始位姿：</p><ul><li><p>可以在rviz里通过2D Pose Estimate按钮设定（rviz会发布initialPose话题）。</p></li><li><p>或者写在launch文件中：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">"initial_pose_x"</span>            <span class="attr">value</span>=<span class="string">"0.0"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">"initial_pose_y"</span>            <span class="attr">value</span>=<span class="string">"0.0"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">"initial_pose_a"</span>            <span class="attr">value</span>=<span class="string">"0.0"</span>/&gt;</span>      </span><br><span class="line"><span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">"initial_cov_xx"</span>            <span class="attr">value</span>=<span class="string">"0.25"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">"initial_cov_yy"</span>            <span class="attr">value</span>=<span class="string">"0.25"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">"initial_cov_aa"</span>            <span class="attr">value</span>=<span class="string">"(pi/12)*(pi/12)"</span>/&gt;</span></span><br></pre></td></tr></table></figure></li><li><p><a href="http://wiki.ros.org/Robots/TIAGo/Tutorials/Navigation/Localization" target="_blank" rel="noopener">调用全局定位服务</a>：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rosservice call /global_localization <span class="string">"&#123;&#125;"</span></span><br></pre></td></tr></table></figure><p>位姿随机初始化，粒子洒满地图：</p><p><img src="/2018/11/16/amcl/global.png" alt=""></p></li></ul><p>transform_tolerance：</p><ul><li>默认是0.1seconds，官方定义是Time with which to post-date the transform that is published, to indicate that this transform is valid into the future. tf变换发布推迟的时间，意思是tf变换在未来这段时间内是可用的。</li><li>【存疑】我个人理解tf的更新频率应该越快越准确，launch文件中最开始设定为0.5，但是实际上机器人移动速度调快时，会报错<code>Costmap2DROS transform timeout...Could not get robot pose, cancelling reconfiguration</code>，然后我调整为1.5就不报错了。</li><li>目前设定为1.0，仿真里观测不出差异。</li></ul>]]></content>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>recovery behavior</title>
      <link href="/2018/11/16/recovery-behavior/"/>
      <url>/2018/11/16/recovery-behavior/</url>
      <content type="html"><![CDATA[<p>navigation stack的move_base包中一个插件。<a href="https://amberzzzz.github.io/2018/11/15/dynamic-window-approach/">DWA</a>的速度空间中如果没有可行的采样点，那么机器人get stuck，触发recovery行为。</p><p>recovery行为的实质是clear out space——试图搞清楚自己的处境：</p><ol><li>首先机器人会清扫地图——conservative reset</li><li>然后原地旋转360度，刷新处境——clearing rotation</li><li>如果还是导航失败，机器人会更加激进的清扫地图——aggressive reset</li><li>然后原地旋转360度，刷新处境——clearing rotation</li><li>如果仍旧失败——mission impossible</li></ol><p><img src="/2018/11/16/recovery-behavior/recovery_behaviors.png" alt=""></p><p>源代码在move_base.cpp里面，继承了nav_core的接口，设置在move_base_params.yaml配置文件中。</p><ul><li>nav_core的recovery_behavior.h封装了RecoveryBehavior类。</li><li>move_base中创建了名为”clear_costmap_recovery/ClearCostmapRecovery”、”rotate_recovery/RotateRecovery”、”clear_costmap_recovery/ClearCostmapRecovery”的默认对象。</li><li>move_base的主程序是一个状态机，case CLEARING就调用RecoveryBehavior的runBehavior()。</li></ul>]]></content>
      
      
        <tags>
            
            <tag> ROS </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>dynamic window approach</title>
      <link href="/2018/11/15/dynamic-window-approach/"/>
      <url>/2018/11/15/dynamic-window-approach/</url>
      <content type="html"><![CDATA[<p><strong>动态窗口：</strong></p><p>窗口框的是速度空间的采样点$(v_t, w_t)$，一对$(v_t, w_t)$就代表一段轨迹，轨迹通过机器人底盘的运动学建模得到。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// base_local_planner的simple_trajectory_generator.cpp</span></span><br><span class="line"><span class="comment">// 直线模型</span></span><br><span class="line"><span class="keyword">double</span> dt = (current_time - last_time).toSec();</span><br><span class="line"><span class="keyword">double</span> delta_x = (vx * <span class="built_in">cos</span>(theta) - vy * <span class="built_in">sin</span>(theta)) * dt;</span><br><span class="line"><span class="keyword">double</span> delta_y = (vx * <span class="built_in">sin</span>(theta) + vy * <span class="built_in">cos</span>(theta)) * dt;</span><br><span class="line"><span class="keyword">double</span> delta_th = vth * dt;</span><br><span class="line"></span><br><span class="line">x += delta_x;</span><br><span class="line">y += delta_y;</span><br><span class="line">theta += delta_th;</span><br></pre></td></tr></table></figure><p><strong>窗口的选择：</strong></p><ol><li><p>速度限制</p><script type="math/tex; mode=display">(V, W) = \{v \in[v_{min}, v_{max}], w \in [w_{min}, w_{max}] \}</script></li><li><p>加速度限制</p><script type="math/tex; mode=display">(V, W) = \left\{\begin{array}& v \in[v_c - \dot{v}*\Delta t, v_c + \dot{v}*\Delta t], \\w \in [w_c - \dot{w}*\Delta t, w_c + \dot{w}*\Delta t] \end{array}\right\}</script></li><li><p>障碍物制动限制</p><script type="math/tex; mode=display">(V, W) = \left\{v \leq \sqrt{2*dist(v,w)*\dot{v}}, w \leq \sqrt{2*dist(v,w)*\dot{w}}\right\}</script><p>$dist(v,w)$表示采样点$(v, w)$对应轨迹上离障碍物最近的距离。</p></li></ol><p>确定窗口后进行采样，可以得到一系列轨迹：</p><p><img src="/2018/11/15/dynamic-window-approach/轨迹.png" alt=""></p><p><strong>轨迹的选择：</strong></p><p>原始论文中采用评价函数：</p><script type="math/tex; mode=display">G(v,w) = \sigma [\alpha * heading(v, w) + \beta * dist(v,w) + \gamma * velocity(v,w)]</script><ol><li>方位角评价函数：采用当前采样点设定下，达到模拟轨迹末端时机器人的朝向角与目标点朝向角的差距。</li><li>空隙评价：当前模拟轨迹上与最近障碍物之间的距离。</li><li>速度评价：采样点速度与最大速度的差距。</li></ol><p>上述评价函数要进行<strong>归一化</strong>。</p><p>算法流程：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">BEGIN <span class="title">DWA</span><span class="params">(robotPose,robotGoal,robotModel)</span></span></span><br><span class="line"><span class="function">    laserscan </span>= readScanner()</span><br><span class="line">    allowable_v = generateWindow(robotV, robotModel)</span><br><span class="line">    allowable_w  = generateWindow(robotW, robotModel)</span><br><span class="line"><span class="keyword">for</span> each v in allowable_v</span><br><span class="line">        <span class="keyword">for</span> each w in allowable_w</span><br><span class="line">            dist = find_dist(v,w,laserscan,robotModel)</span><br><span class="line">            breakDist = calculateBreakingDistance(v)</span><br><span class="line">            <span class="keyword">if</span> (dist &gt; breakDist)  <span class="comment">//can stop in time</span></span><br><span class="line">                heading = hDiff(robotPose,goalPose, v,w) </span><br><span class="line">                clearance = (dist-breakDist)/(dmax - breakDist) </span><br><span class="line">                cost = costFunction(heading,clearance, <span class="built_in">abs</span>(desired_v - v))</span><br><span class="line">                <span class="keyword">if</span> (cost &gt; optimal)</span><br><span class="line">                    best_v = v</span><br><span class="line">                    best_w = w</span><br><span class="line">                    optimal = cost</span><br><span class="line"><span class="built_in">set</span> robot trajectory to best_v, best_w</span><br><span class="line">END</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>A-star algorithm</title>
      <link href="/2018/11/14/A-star-algorithm/"/>
      <url>/2018/11/14/A-star-algorithm/</url>
      <content type="html"><![CDATA[<p>有目的性地寻找最佳路径，首先定义一个损失函数，表示节点消耗：</p><script type="math/tex; mode=display">f = g+h</script><p>$g$表示起点到当前节点的已知消耗</p><p>$h$表示对当前节点到终点消耗的猜测，<strong>估价函数</strong>有多种形式——启发式探索的核心</p><p>算法流程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">初始化openList</span><br><span class="line">初始化closeList</span><br><span class="line">将起点放入openList</span><br><span class="line"><span class="keyword">while</span> openList非空：</span><br><span class="line">找到开启列表上f最小的节点，记为q</span><br><span class="line">    找到q周围的子节点p，记其父节点为q</span><br><span class="line">    <span class="keyword">for</span> 每一个子节点p：</span><br><span class="line">    <span class="keyword">if</span> p是终点：</span><br><span class="line">        算法终止</span><br><span class="line">            </span><br><span class="line">        p.g = q.g + qp</span><br><span class="line">        p.h = h(p, terminate)</span><br><span class="line">        p.f = p.g + p.h</span><br><span class="line">        <span class="keyword">if</span> p已经在开启列表中且保存的f值小于当前计算值||p在关闭列表中：</span><br><span class="line">        跳过该节点</span><br><span class="line"><span class="keyword">else</span>：</span><br><span class="line">        <span class="keyword">if</span> p已经在开启列表中：</span><br><span class="line">        修改该节点的信息（父节点、fgh）</span><br><span class="line"><span class="keyword">else</span>：</span><br><span class="line">            将该节点加入openList</span><br><span class="line">将q放入closeList</span><br></pre></td></tr></table></figure><p>算法性能在细节上的优化：<a href="http://theory.stanford.edu/~amitp/GameProgramming/" target="_blank" rel="noopener">http://theory.stanford.edu/~amitp/GameProgramming/</a></p><p><strong>序言：路径搜索算法的前世今生</strong></p><p><strong>Dijkstra算法：</strong>从初始节点开始向外扩展，直到到达目标节点。算法保证能找到从初始点到目标点的最短路径。</p><p><img src="/2018/11/14/A-star-algorithm/D算法.png" alt=""></p><p><strong>最佳优先搜索BFS算法：</strong>算法能够评估任意节点到目标点的代价，并优先选择离目标最近的结点。启发式算法，比Dijkstra算法运行快得多，但是不能保证路径最短。</p><p><img src="/2018/11/14/A-star-algorithm/BFS算法.png" alt=""></p><p>如下面这种情况：</p><p><img src="/2018/11/14/A-star-algorithm/D1.png" alt="D1">     <img src="/2018/11/14/A-star-algorithm/B1.png" alt="B1"></p><p>因为BFS是基于贪心策略的，它只关注到尽可能向着目标点移动，而不考虑已花费的代价。Dijkstra算法则正相反，它会确保每一步都是最优的，但是为此要遍历周围全部的节点。</p><p><strong>A*算法：</strong>将两种路径搜索算法的思想结合起来，考虑两个极端及其中间的情况：</p><ul><li><p>如果$h(n)$是0，只有$g(n)$起作用，那么算法演变为Dijkstra算法。</p></li><li><p>如果$h(n)$能够始终满足“比当前节点移动到目标节点的实际代价小”，那么算法保证能够找到最短路径。（$h(n)$越小，算法扩展的节点数就越多）</p></li><li>如果$h(n)$能够精确等于“当前节点移动到目标节点的实际代价”，那么算法将会仅仅扩展最优路径。而不扩展其他节点，算法运行非常快。</li><li>如果$h(n)$有时会“比当前节点移动到目标节点的实际代价大”，那么此时算法不能保证最短路径了。</li><li>如果$g(n)$比$h(n)$小的多，只有$h(n)$起作用，那么算法演变为BFS算法。</li></ul><p><strong>估价函数Heuristic function $h(a, b)$</strong> </p><p>估价函数的选择可以follow以下的instructions：</p><ol><li><p>square grid that allows 4 directions：use Manhattan distance (L1)</p><script type="math/tex; mode=display">dx = abs(a.x - b.x)\\dy = abs(a.y - b.y)\\dis = D*(dx+dy)</script></li><li><p>square grid that allows 8 directions：use Diagonal distance (L∞)</p><script type="math/tex; mode=display">dx = abs(a.x - b.x)\\dy = abs(a.y - b.y)\\dis = D*(dx+dy) + (D2 - 2*D)*min(dx, dy)</script><p>当$D = D2 =1$时，$dis = dx + dy -min(dx, dy) = max(dx, dy)$，这个距离称为切比雪夫距离。</p><p>当$D = 1, D2 =\sqrt 2$时，这个距离称为<em>octile distance</em>。</p></li><li><p>square grid that allows any direcitons：use Euclidean distance (L2)</p><script type="math/tex; mode=display">dx = abs(a.x - b.x)\\dy = abs(a.y - b.y)\\dis = D*\sqrt{dx*dx + dy*dy}</script><blockquote><p>If A* is finding paths on the grid but you are allowing movement not on the grid, you may want to consider other representations of the map</p></blockquote><p>​    欧几里得距离并不适用于栅格地图，因为这会导致代价函数g和估价函数的不匹配（代价函数并不是连续的）。</p><p>由于欧几里得距离引入了开根号计算，一些算法会直接用$dis = D<em>(dx</em>dx + dy<em>dy)$来代替，<em>*不建议！</em></em>，会引入尺度问题，$f = g + h$，其中代价函数会逐渐增长，估价函数则逐渐减小，平方会导致两个函数的变化速率不match，使得估价函数的权重过大，导致BFS。</p></li></ol><p>code：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">public <span class="function"><span class="keyword">function</span> <span class="title">manhattanHeuristic</span>(<span class="params">a:Object, b:Object</span>):<span class="title">Number</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> graph.distance(a, b) + simpleCost(a, b) - <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public <span class="function"><span class="keyword">function</span> <span class="title">simpleCost</span>(<span class="params">a:Object, b:Object</span>):<span class="title">Number</span> </span>&#123;</span><br><span class="line"><span class="keyword">var</span> c:<span class="built_in">Number</span> = costs[graph.nodeToString(b)];</span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">isNaN</span>(c)) &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> c;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// simpleCost限定为小于等于1的数</span></span><br></pre></td></tr></table></figure><p>此时$h(a,b) = dis(a,b)+c-1 \leq h^{*}(a,b)$，此时能够找到最优解。</p>]]></content>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>branch and bound</title>
      <link href="/2018/11/13/branch-and-bound/"/>
      <url>/2018/11/13/branch-and-bound/</url>
      <content type="html"><![CDATA[<p>一个栗子：整数规划问题欲求$max\ z = 5x_1 + 8x_2$</p><script type="math/tex; mode=display">\left\{\begin{align} &   x_1 + x_2 \leq 6 \nonumber\\  &   5x_1 + 9x_2 \leq 45 \nonumber\\  & x_1\geq0, x_2\geq0 \nonumber\\  & x_1,x_2为整数 \nonumber\end{align}   \right.</script><p>根据方程组可以绘制下图：</p><p><img src="/2018/11/13/branch-and-bound/规划.png" alt=""></p><p>于是可以得到实数空间上的最优解：$x_1 = 2.25, x_2 = 3.75, z_0 = 41.25$。——<strong>松弛问题</strong></p><p>由于存在整数限定条件：</p><ol><li>最优解$0 \leq z^{*} \leq 41$，且必为整数</li><li>x_2的最优解不在3和4之间，因为限定为整数</li></ol><p>一、分支</p><p>于是问题可以拆分为：$max\ z = 5x_1 + 8x_2$</p><script type="math/tex; mode=display">p1\left\{\begin{align} &   x_1 + x_2 \leq 6 \nonumber\\ &   5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\& x_2\leq 3 \nonumber\end{align}   \right.\\</script><script type="math/tex; mode=display">p2\left\{\begin{align} &   x_1 + x_2 \leq 6 \nonumber\\ &   5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\& x_2\geq 4 \nonumber\end{align}   \right.\\</script><p>问题拆分的实质是将$x_2$在3和4之间的小数部分划掉了，将可行域拆分成$x_2 \leq 3$ 和$x_3 \geq 4$，但是没有排除任何整数可行解。——<strong>分支</strong></p><p>二、定界</p><p>子问题$p1$的最优解为：$x_1 = 3, x_2=3, z^{*}=39$</p><p>子问题$p2$的最优解为：$x_1 = 1.8, x_2=4, z^{*}=41$</p><p>也就是说，子问题$p1$的整个参数空间上，能够取得的最优解为39，子问题$p2$上则为41，显然最优解应该位于子问题$p2$所在的参数空间中，且$39\leq z^{<em>} \leq41$。——<em>*定界</em></em></p><p>三、迭代</p><p>对$p2$参数空间再分支，参数$x_1$可以拆分为$x_1 \leq 1$和$x_1 \geq 2$：</p><script type="math/tex; mode=display">p3\left\{\begin{align} &   x_1 + x_2 \leq 6 \nonumber\\ &   5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\ & x_1\leq 1 \nonumber\\& x_2\geq 4 \nonumber\end{align}   \right.\\</script><script type="math/tex; mode=display">p4\left\{\begin{align} &   x_1 + x_2 \leq 6 \nonumber\\ &   5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\ & x_1\geq 2 \nonumber \\& x_2\geq 4 \nonumber\end{align}   \right.\\</script><p>四、总结</p><p>分支定界算法的总体流程如下：</p><ol><li>先求解相应的松弛问题，得到最优解，检查其是否符合原问题约束，若符合则为最优解，否则进行下一步。</li><li>定界，取各分支中目标函数最大的作为上界$U_z$，取其余分支中目标函数中最大的作为下界$L_z$。$L_z \leq z^{*} \leq U_z$。</li><li>分支，否则选择一个不符合原问题条件的变量，构建子问题。</li><li>对各分支，有序地，进行步骤1。</li></ol><p>在求解对应的松弛问题时，通常会有以下几种情况：</p><ol><li>松弛问题没有可行解，那么原问题也没有可行解。</li><li>松弛问题的最优解也满足原问题约束，那么该解也是原问题的最优解，算法终止。</li><li>松弛问题的最优解小于现有下界，那么应该对该子问题进行剪枝。</li></ol><p>五、DFS</p><p>对一颗搜索树，不用计算每一层全部节点的score（BFS），我们会维护一个优先队列，其中按照score的大小存放节点，然后选择score最大的节点（the most promising child）进行分支。</p>]]></content>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>trifles with arduino</title>
      <link href="/2018/10/30/trifles-with-arduino/"/>
      <url>/2018/10/30/trifles-with-arduino/</url>
      <content type="html"><![CDATA[<p>系统总体的通信架构如下：</p><p><img src="/2018/10/30/trifles-with-arduino/通信.png" alt=""></p><p>底盘驱动板Arduino负责接收上层的运动控制指令，并驱动电机，两块板子通过串口进行通信。</p><p>ROS提供了一个<a href="https://github.com/hbrobotics/ros_arduino_bridge" target="_blank" rel="noopener">ros_arduino_bridge</a>功能包集，它包括了Arduino库（ROSArduinoBridge）以及一系列用来控制Arduino-based robot的ROS功能包，<strong>这个功能包可以实现读取Twist控制信息，以及发布里程计信息等任务</strong>，封装了Raspberry Pi和Arduino之间的底层通信。</p><p>Arduino库（ROSArduinoBridge）位于ros_arduino_firmware/src/libraries/路径下，里面是一些arduino脚本和头文件，将这个文件夹复制到我们Arduino IDE的SKETCHBOOK_PATH下，然后在Arduino IDE中就可以直接打开这个sketch项目。</p><p>ROSArduinoBridge文件下是一些配置选项，另外commands.h文件中给出了一些可用的串口控制指令，如电机控制指令：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">m <span class="number">20</span> <span class="number">20</span>   <span class="comment">// move the robot forward at 20 encoder ticks per second</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>skid-steer drive</title>
      <link href="/2018/10/29/skid-steer-drive/"/>
      <url>/2018/10/29/skid-steer-drive/</url>
      <content type="html"><![CDATA[<p>实验中使用了两种类型的底盘，基于差速驱动的2WD底盘和基于滑动转向的4WD底盘。两种驱动方式原理相似，也有其显著的区别。</p><p><strong>相同点：</strong></p><p>两种底盘都没有显示的转动机制，采用差速驱动的方式通过以不同的方向或速度驱动两边轮子来实现方向控制。</p><p>差速驱动的运动形式通常有一下几种类型：</p><ul><li>第一种是原地旋转，左右轮的速度大小相等，方向相反，这样相当于绕着底盘的形心原地打转。</li><li>第二种是沿着某个方向直线行走，此时左右轮速度相同。</li><li>第三种是沿着某条曲线前行或后退，此时左右轮速度方向相同，大小不同。</li><li>第四种是旋转转弯，此时左右轮速度方向相反。</li></ul><p>两种机构共同的优势是：没有显示的转向机构，极大地简化了运动学模型。</p><p>而两种机构共同的缺点是：由于两侧的轮子是由独立电机分别驱动的，直线运动要求两侧的轮子以相同速度转动，这将很难完成。</p><p><strong>不同点：</strong></p><p>差速驱动底盘通常是由一个两轮系统，每个轮子都带有独立的执行机构（直流电机），以及一个无驱动轮（可以是脚轮或者万向滚珠）组成，<strong>机器人的运动矢量是每个独立车轮运动的总和</strong>。</p><p><img src="/2018/10/29/skid-steer-drive/Diffdrv.jpg" alt="Diffdrv"></p><p>滑动转向底盘通常被用在履带车上，比如坦克和推土机，也被用于某些四轮六轮机构上，相比较于两轮差速底盘，滑动转向的主要区别在于：</p><ul><li>优势：滑动转向使用了两个额外的驱动轮代替了差速驱动的脚轮，增大了牵引力。</li><li>劣势：引入了滑动，在对里程计要求高的场景中，滑动是一个致命的缺陷，因为这会对编码器造成负面影响，滑动的轮子不会跟踪机器人的确切运动。</li></ul><p><img src="/2018/10/29/skid-steer-drive/4WD.png" alt="4WD"></p><p><strong>运动学分析：</strong></p><p>对于差速驱动机构，移动机器人航向角变化了多少角度，它就绕其运动轨迹的圆心旋转了多少角度。这句话很好验证，我们让机器人做圆周运动，从起点出发绕圆心一圈回到起点处，在这过程中机器人累计的航向角为360度，同时它也确实绕轨迹圆心运动了360度。</p><p>机器人的速度是指两个相邻的控制时刻之间的速度，因此小车的行驶轨迹可以分解为连续的圆弧片段，对于每一段圆弧，根据<strong>阿克曼转向几何原理</strong>，在小车转向时，为保证行驶稳定性，两侧轮胎都近似围绕一个中心点旋转。即整个小车底盘都围绕一个中心点旋转，已知小车中心的线速度（上层算法给定），此时小车底盘的运动学模型如下图：</p><p><img src="/2018/10/29/skid-steer-drive/底盘.png" alt=""></p><p>参数说明：</p><blockquote><p>$\alpha_1$是小车前左轮和后左轮的转角。</p><p>$\alpha_2$是小车前右轮和后右轮的转角。</p><p>$2L$是左右轮距离。</p><p>$2K$是前后轮距离。</p><p>$w$是小车转轴的角速度。</p><p>$v$是小车几何中心的线速度。</p><p>$v1, v2, v3, v4$是四个车轮的速度。</p><p>$i$是电机的减速比。</p><p>$r$是车轮半径。</p></blockquote><p>首先可以得到各车轮速度和角速度的关系：</p><script type="math/tex; mode=display">V_1 = w * R_1 = w * \frac{K}{sin\alpha_1}\\V_2 = w * R_2 = w * \frac{K}{sin\alpha_2}\\V_3 = V_1 = w * \frac{K}{sin\alpha_1}\\V_4 = V_2 = w * \frac{K}{sin\alpha_2}\\</script><p>其中车轮沿着转动方向（$y$方向）的速度由电机提供，切向速度由地面摩擦提供，车轮沿着$y$方向的速度为：</p><script type="math/tex; mode=display">R =\frac{v}{w}\\V_{1y} = V_1 * cos\alpha_1 = w * \frac{K}{tan \alpha_1} = w(R-L)\\V_{2y} = V_2 * cos\alpha_2 = w * \frac{K}{tan \alpha_2} = w(R+L)\\V_{3y} = V_{1y} =  w(R-L)\\V_{4y} = V_{2y} = w(R+L)\\</script><p>那么电机的角速度为：</p><script type="math/tex; mode=display">w_n= \frac{V_{ny}*i}{r}, n = 1,2,3,4\\</script><p>相应电机的转速（by rpm）为：</p><script type="math/tex; mode=display">n  = \frac{w_n*60}{2\pi}</script><p>整理成矩阵表达式为：</p><script type="math/tex; mode=display">\begin{bmatrix}w_1\\w_2\\w_3\\w_4\end{bmatrix}=\begin{bmatrix}1 & - L\\1 & L\\1 & - L\\1 & L\end{bmatrix}\begin{bmatrix}v\\w\end{bmatrix}</script><p>该表达式反映了机器人关键点速度与主动轮转速之间的关系。给定小车底盘电机转速就可以求出机器人关键点的速度，并由此得到机器人上任意一点的速度（如激光雷达的安装位置的速度），上层算法给出的关键点速度控制信号也可以由此转化成电机的控制量。</p><p><strong>里程计模型 ／ 机器人定位方法</strong></p><p><strong>坐标变换模型：</strong></p><p><img src="/2018/10/29/skid-steer-drive/编码器.png" alt="编码器"></p><p>在一个较短的时间间隔$\Delta t$内，假定机器人左右轮的移动距离分别是$\Delta l$和$\Delta r$，那么在<strong>机器人坐标系下</strong>：机器人中心沿着机器人坐标系的$x$轴方向前进的距离为$\Delta u = (\Delta l + \Delta r)/2$，$y$轴方向前进的距离为$\Delta v = 0$，转过的角度为$\Delta \varphi = (\Delta l - \Delta r)/b$。机器人坐标系到世界坐标系的旋转变换矩阵为$R(\phi)$。</p><p>那么转换到<strong>世界坐标系下</strong>机器人的运动增量为：</p><script type="math/tex; mode=display">\begin{bmatrix}\Delta x\\\Delta y\\\Delta \phi\end{bmatrix} = \begin{bmatrix}R(\phi) & 0\\0 & 1\end{bmatrix}\begin{bmatrix}\Delta u\\\Delta v\\\Delta \varphi\end{bmatrix} = \begin{bmatrix}cos\phi & sin\phi & 0\\-sin\phi & cos\phi & 0\\0 & 0 & 1\end{bmatrix}\begin{bmatrix}(\Delta l + \Delta r)/2\\0\\(\Delta l - \Delta r)/b\end{bmatrix}</script><p>世界坐标系下机器人位姿更新为：</p><script type="math/tex; mode=display">\begin{bmatrix}x_t\\y_t\\\phi_t\end{bmatrix} =\begin{bmatrix}cos\phi_{t-1}(\Delta l + \Delta r)/2\\sin\phi_{t-1}(\Delta l + \Delta r)/2\\(\Delta l - \Delta r)/b\end{bmatrix} +\begin{bmatrix}x_{t-1}\\y_{t-1}\\\phi_{t-1}\end{bmatrix}</script><p>beside from测量误差，利用坐标变换模型去推算里程计信息是引入了<strong>模型误差</strong>的——在时间间隔$\Delta t$内，为了简化计算，机器人坐标系相对世界坐标系的旋转变换矩阵被假定为起始值$R(\phi)$。在转向运动比较多的情况下，里程计信息会迅速恶化。</p><p><strong>圆弧模型：</strong></p><p>将极小时间间隔内小车运动的轨迹看作是一段圆弧，那么就可以确定该时刻的转动中心$C_{t-1}$，及内侧轮的转动半径为$R_{t-1}$，根据几何关系：</p><p><img src="/2018/10/29/skid-steer-drive/圆弧模型.png" alt="圆弧模型"></p><script type="math/tex; mode=display">\left\{ \begin{align}&\Delta l = (b + R)\Delta \varphi\\&\Delta r = R  \Delta \varphi\end{align}\right.</script><p>解得：</p><script type="math/tex; mode=display">\left\{ \begin{align}&R = \frac{b\Delta r}{\Delta l - \Delta r}\\&\Delta \varphi = \frac{\Delta l - \Delta r }{b}\end{align}\right.</script><p>由三角相似得：</p><script type="math/tex; mode=display">\frac{Rsin(\Delta \varphi/2)}{D/2} = \frac{R}{R + b/2}</script><p>解得弦$D$的长度为：</p><script type="math/tex; mode=display">D = [b(\Delta l + \Delta r)/(\Delta l-\Delta r)]sin[(\Delta l - \Delta r)/2b]</script><p>弦$D$与世界坐标系$x$轴正向的夹角为$\theta = \phi - \Delta \varphi/2$，那么机器人在世界坐标系下的位姿增量为：</p><script type="math/tex; mode=display">\left\{\begin{align}&\Delta x_{t-1} = D_{t-1} cos\theta_{t-1}\\&\Delta y_{t-1} = D_{t-1} sin\theta_{t-1}\\&\Delta \varphi = (\Delta l - \Delta r )/b\end{align}\right.</script><p>使用圆弧模型对里程计增量进行推算，完全依照几何关系来计算，计算过程中没有近似，能够有效控制误差累积。</p><p><strong>概率模型：</strong></p><p>不是单纯的基于里程计的估计，而是结合其他传感器的测量值对里程计进行矫正，详见<a href="https://amberzzzz.github.io/2018/11/16/amcl/">滤波算法</a>。</p><p><strong>scan match模型：</strong></p><p>同样也不是单纯的基于里程计的估计，详见<a href="https://amberzzzz.github.io/2018/06/05/karto-key-concepts/">karto scanMatch</a>。与滤波的区别在于，返回的不是概率分布，而是一个代表最佳估计的值。</p>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>scan matcher</title>
      <link href="/2018/10/26/scan-matcher/"/>
      <url>/2018/10/26/scan-matcher/</url>
      <content type="html"><![CDATA[<p>目前开源算法中采取的scanMatching方法主要是以下四种：</p><ol><li>Gmapping：ICP（simple Gradient Descent）</li><li>Hector：Guass-Newton（multi-resolution map）</li><li>karto：Real-time CSM（multi-resolution + 三维窗口遍历寻优）</li><li>cartographer：Fast CSM（multi-resolution + branch and bound）</li></ol><p>scanMatcher主要涉及两个评价函数，一个score用于优化调整粒子pose作为参考，一个likelihoodAndScore用于更新粒子权重：</p><script type="math/tex; mode=display">s(x, z, m) = \sum_i s(x, z^i, m)\\s(x, z^i, m) = e^{d^2/ \sigma}</script><p>粒子权重根据地图的匹配度更新：</p><script type="math/tex; mode=display">laser\ frame \to map\ frame: \hat{z}^i = x \oplus z^i\\map\ cell: (x,\ y)^T\\d^2 = (\hat{z}^i - (x,y)^T)^T (\hat{z}^i - (x,y)^T)</script>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>navigation stack</title>
      <link href="/2018/10/23/navigation-stack/"/>
      <url>/2018/10/23/navigation-stack/</url>
      <content type="html"><![CDATA[<h4 id="part0"><a href="#part0" class="headerlink" title="part0"></a>part0</h4><p>现实中想要移动平台移动到指定地点，机器人运动控制系统架构包括了如下几个层次：</p><p><img src="/2018/10/23/navigation-stack/navi.png" alt=""></p><p>最底层是机器人的底盘控制部分，驱动器接收的是机器人的期望速度（Twist），将速度解算为左右轮的期望速度，并根据期望速度对左右轮分别进行PID驱控速，输出电机的转速。</p><blockquote><p>这部分ROS社区已经有针对Arduino封装好的Package——rosserial_arduino。</p></blockquote><p>中间层是通信层，电脑端发布速度指令给平台，同时接收平台发布的当前速度，然后发布/odom topic，让其他节点订阅。</p><p>最上层是决策层，也就是导航规划层，goal、localization（matching&amp;里程计）、path planner以及最终输出速度指令，这一部分都在navigation stack里面。</p><h4 id="part1-packages"><a href="#part1-packages" class="headerlink" title="part1 packages"></a>part1 packages</h4><p>navigation stack是ROS提供的导航方案，内部集成了很多个package，模块之间完全解耦，可以个性化选择提供的方法。</p><p><img src="/2018/10/23/navigation-stack/navigation%20stack.png" alt=""></p><p>官网上给出了导航栈宏观的结构描述：</p><p><img src="/2018/10/23/navigation-stack/module.png" alt=""></p><p>move_base中主要包含三个部分，global_plan、local_plan以及recovery behavior。对应的插件有：</p><ul><li>global_plan：global_planner（实现了dijkstra和A*算法），carrot_planner，navfn</li><li>local_plan：base_local_planner（实现了Trajectory Rollout和DWA算法），dwa_local_planner</li><li>recovery：clear_costmap_recovery，move_slow_and_clear，rotate_recovery</li></ul><p>nav_core是一个接口插件，包含了以上插件基类的头文件，move_base中的方法都是在其规则上扩展的。</p><p>两个灰色的插件map_server和amcl表示可选可不选：</p><ul><li>可以使用meta package提供的map_server节点来进行<strong>代价地图</strong>管理，也可以使用其他节点（例如直接使用gmapping的输出）。</li><li>可以使用meta package提供的amcl节点来进行自定位，也可以使用其他算法包（例如ROS里面还有一个robot_pose_ekf节点）。</li></ul><p>costmap_2d将不同传感器的输入处理成统一的栅格地图格式。以层的概念来组织图层，用户可以根据需要自己配置（通过Social Costmap Layer、Range Sensor Layer等开源插件），默认的层有：</p><ul><li>static_layer：静态地图层，（通过订阅map_server的/map主题）来生成。</li><li>obstacle_layer：障碍地图层，根据动态的传感器信息来生成。</li><li>inflation_layer：膨胀层，将前两个图层的信息综合进行缓冲区扩展。</li></ul><p>voxel_grid是三维代价地图。</p><p>fake_localization用来做定位仿真，内含/base_pose_ground_truth话题。</p><h4 id="part2-params"><a href="#part2-params" class="headerlink" title="part2 params"></a>part2 params</h4><ol><li>move_base_params.yaml：<ul><li>planner_frequency：<strong>全局规划</strong>的执行频率，如果设置为0.0则全局规划器仅在接受到新目标点或者局部规划器报告路径堵塞时才会重新执行。</li></ul></li><li>global_planner_params.yaml：<ul><li>default_tolerance：当设置的目的地被占据时，以该参数为半径的范围内选取最近的点作为新目标点。</li></ul></li><li>dwa_local_planner_params.yaml：<ul><li>latch_xy_goal_tolerance：如果设置为true，达到xy_goal_tolerance以内机器人就会原地旋转，即使会转出容错圈外。</li><li>sim_granularity：间隔尺寸，轨迹上采样点步长。</li><li>scaling_speed：启动机器人底盘的速度。</li></ul></li><li><p>global_costmap_params.yaml：</p><ul><li>raytrace_range：实时清除代价地图上障碍物的最大范围，清除的是obstacle_layer的数据。</li></ul></li></ol><h4 id="part3-topics"><a href="#part3-topics" class="headerlink" title="part3 topics"></a>part3 topics</h4><ol><li><p>move_base &amp; move_base_simple：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ros::<span class="function">NodeHandle <span class="title">action_nh</span><span class="params">(<span class="string">"move_base"</span>)</span></span>;</span><br><span class="line">action_goal_pub_ = action_nh.advertise&lt;move_base_msgs::MoveBaseActionGoal&gt;(<span class="string">"goal"</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//we'll provide a mechanism for some people to send goals as PoseStamped messages over a topic</span></span><br><span class="line"><span class="comment">//they won't get any useful information back about its status, but this is useful for tools</span></span><br><span class="line"><span class="comment">//like nav_view and rviz</span></span><br><span class="line">ros::<span class="function">NodeHandle <span class="title">simple_nh</span><span class="params">(<span class="string">"move_base_simple"</span>)</span></span>;</span><br><span class="line">goal_sub_ = simple_nh.subscribe&lt;geometry_msgs::PoseStamped&gt;(<span class="string">"goal"</span>, <span class="number">1</span>, boost::bind(&amp;MoveBase::goalCB, <span class="keyword">this</span>, _1));</span><br></pre></td></tr></table></figure><p>之前查看节点图的时候发现这两个节点都提供goal，一直没想通两者的关系，发现代码注释里面有，move_base继承了actionlib，有状态反馈（<a href="http://wiki.ros.org/move_base" target="_blank" rel="noopener">详见wiki 1.1.2 ActionAPI</a>），move_base_simple就是一个publisher（topic可以来自rviz／cmd line）。</p><p><img src="/2018/10/23/navigation-stack/rostopic.png" alt=""></p><p>/result 记录了Goal reached</p><p>/feedback 记录了每个时刻机器人的位姿</p><p>/status 记录了任务进程（goal accepted、failed、aborting）</p><p>/cancel 没echo出信息，应该与上层对接</p><p>【定点巡航】另外，定点巡航的时候将global_path的buffer设置为n就可以显示多条路径了。</p></li><li><p>DWAPlanner的global_plan &amp; local_plan：</p><p>local_plan就是DWA算法每个时刻计算的最优预期路径。global_plan是整个局部代价地图上的路径，它是全局路径的crop，因为局部动态环境不会影响全局路径，我们只研究落在localmap以内这一段路径是否需要矫正。</p><p><img src="/2018/10/23/navigation-stack/path.png" alt=""></p></li></ol>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Autolabor</title>
      <link href="/2018/10/15/autolabor/"/>
      <url>/2018/10/15/autolabor/</url>
      <content type="html"><![CDATA[<p>今天发现了一个支持二次开发的开源方案，说白了就是把karto、acml、navigation stack等几个ROS开源包整合的比较漂亮，代码结构值得借鉴。</p><ol><li><p>执行keyboard_control之前要先执行脚本，添加键盘。</p><blockquote><p>老AS包里面没有keyboard control，可以执行teleop_twist_keyboard包。autolabor_fake是虚拟小车的driver，建模了电机、odom相关信息，订阅cmd_vel信息。控制真实小车时这个节点要替换。</p></blockquote></li><li><p>base是对机器人底盘的仿真，launch文件的默认Fixed Frame是base_link，想要控制小车运动可以将Frame切成real_map或odom。</p><blockquote><p>todolist:</p><p>命令行里面控制量的显示不太好看，可以尝试在源文件里面优化，添加交互提示。</p></blockquote><p><img src="/2018/10/15/autolabor/keyboard.png" alt=""></p></li><li><p>stage是对场景的仿真，场景由map_server读取，launch以后就能查看当前场景地图。</p><blockquote><p>rostopic里面有一个initialpose信息，由rviz发布。rostopic list里面好多topic都是rviz发布的，在displays栏目里面取消勾选就不会发布了。</p></blockquote><p><img src="/2018/10/15/autolabor/initial.png" alt=""></p></li><li><p>object是对障碍物的仿真，调用stage，添加interactivemarker，<strong>然后选择Interact工具</strong>，理论上地图上应该出现障碍物，但是我没找到。。。状态显示waiting for tf info。</p><p><img src="/2018/10/15/autolabor/marker.png" alt=""></p><p><img src="/2018/10/15/autolabor/param.png" alt=""></p><p><strong>修正</strong>：添加的marker要在topic里面选择，不要在type栏下。然后地图上放好障碍物以后<strong>要右键apply</strong>。</p><p><img src="/2018/10/15/autolabor/obstacle.png" alt=""></p></li><li><p>lidar是对雷达点云的仿真，launch中给了一个lidar和map的静态tf，实际使用中应该给lidar和base_link的。</p><blockquote><p>todolist:</p><p>map和real_map给的有点混乱，明天会统一一下。</p></blockquote></li><li><p>老AS包create_map仿真过程中，由于场景提供mapserver和slam算法同时publish了/map这个topic，要进行区分，在launch文件里面对其中一个进行remap：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">node</span> <span class="attr">pkg</span>=<span class="string">"map_server"</span> <span class="attr">type</span>=<span class="string">"map_server"</span> <span class="attr">name</span>=<span class="string">"map_server"</span> <span class="attr">args</span>=<span class="string">"$(find simulation_launch)/map/MG_map.yaml"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">remap</span> <span class="attr">from</span>=<span class="string">"/map"</span> <span class="attr">to</span>=<span class="string">"real_map"</span>  /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="/2018/10/15/autolabor/remap.png" alt=""></p><p>这时rostopic里面就会出现real_map这个话题，两个地图能够同时显示。</p><p><img src="/2018/10/15/autolabor/real_map.png" alt="real_map"></p></li></ol><h4 id="代码解析"><a href="#代码解析" class="headerlink" title="代码解析"></a>代码解析</h4><p><strong>首先是simulation包：</strong></p><ol><li><p>autolabor_description没啥好说的，urdf文件里面定义了一个robot，整个机器人被渲染成了一个base_link，没有子节点，懒。</p></li><li><p>autolabor_fake包是底盘驱动，提供了一个<code>autolabor_fake_node</code>节点，其订阅类型为<code>geometry_msgs/Twist</code>的话题<code>cmd_vel</code>，信息来源可以是joystick／keyboard（tele_op_xxx）／cmd line（rostopic pub xxx）。其发布类型为<code>nav_msgs/Odometry</code>的话题<code>odom</code>。同时该节点还会将odom frame到base_link frame的transform信息提供给tf node，用来tf_broadcast。</p><p><img src="/2018/10/15/autolabor/rosgraph.png" alt="rosgraph"></p></li><li><p>lidar_simulation包自身提供了两个节点<code>lidar_simulation</code>和<code>obstacle_simulation</code>。</p><p>3.1 <code>LidarSimulation::getPose</code>这个函数中有一段代码开始比较困惑：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ROS_INFO("roll and pitch and yaw and esp :%lf %lf %lf %lf", roll, pitch, yaw, esp);  //esp=0.000001,r&amp;p=0.000000</span></span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">pow</span>(roll,<span class="number">2</span>) + <span class="built_in">pow</span>(pitch,<span class="number">2</span>) &gt; esp)&#123;</span><br><span class="line">start_angle = yaw + max_angle_;</span><br><span class="line">reverse = <span class="number">-1.0</span>;</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;  <span class="comment">// default situation: </span></span><br><span class="line">start_angle = yaw + min_angle_;</span><br><span class="line">reverse = <span class="number">1.0</span>;</span><br></pre></td></tr></table></figure><p>通常情况下，我们都使用右手坐标系，二维平面下，global_frame_到lidar_frame_的坐标变换transform欧拉角形式下的r和p角应该始终是$0.0$，yaw代表了激光雷达$x$轴的变换，加上<code>min_angle_</code>就切换成了激光光束的初始发射角度<code>start_angle</code>。如果坐标系定义反了，r&amp;p就应该有值，这时因为坐标轴定义反过来了，激光光束的初始发射角度就变成了从正方向上的<code>max_angle_</code>开始的。</p><p>3.2 <code>LidarSimulation::updateMap</code>这个函数值得注意，这是一个service client，用来调用地图更新，在当前功能包的默认launch文件中，只加载了一次地图，没有体现出它的作用。当执行建图任务时，因为map frame和odom frame会不断进行矫正，建图包就会call这个request来实时更新地图。</p><p><img src="/2018/10/15/autolabor/mapupdate.png" alt="mapupdate"></p><p>3.3 该功能包下还自定义了一个obstacle service，提供<code>obstacle_simulation</code>节点来更新障碍物信息。这里的障碍物是指手动添加的障碍物（interactiveMarker），launch文件中可以定义其形状顶点。</p><p>​    其中的<code>ObstacleSimulation::pnpoly</code>函数用来判断某点是否落在多边形内，之前<strong>刷算法</strong>时有考虑过这个问题，这里给出的解法不知道是不是最优的，just for record：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bool</span> ObstacleSimulation::pnpoly(geometry_msgs::Polygon&amp; footprint, <span class="keyword">float</span>&amp; x, <span class="keyword">float</span>&amp; y)&#123;</span><br><span class="line">  <span class="keyword">int</span> i,j;</span><br><span class="line">  <span class="keyword">bool</span> c = <span class="literal">false</span>;</span><br><span class="line">  <span class="keyword">for</span> (i=<span class="number">0</span>, j=footprint.points.size()<span class="number">-1</span>; i&lt;footprint.points.size(); j = i++)&#123;</span><br><span class="line">    <span class="keyword">if</span> ( ( (footprint.points.at(i).y &gt; y) != (footprint.points.at(j).y &gt; y) ) &amp;&amp;</span><br><span class="line">         (x &lt; (footprint.points.at(j).x-footprint.points.at(i).x) * (y-footprint.points.at(i).y) / (footprint.points.at(j).y - footprint.points.at(i).y) + footprint.points.at(i).x) )&#123;</span><br><span class="line">      c = !c;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> c;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里面的for，循环条件遍历的是多边形的每一条边，如$(3,0), (0,1), (1,2), (2, 3)$这样。判定的是给定点是否落在给定边的<strong>内侧</strong>，这里所谓的内侧是以给定边的起始节点为原点，给定线段的顺时针方向。</p><p>3.4 obstacle的具体操作定义在static_map中，这里面出现了世界坐标系World，值得注意的是，栅格地图的原点在地图的一角，栅格的位置用整型来表示，而世界坐标系中栅格的位置由其中心来表示，两者相差$0.5$个resolution。lidar_simulation里面创建了一个static_map对象<code>map_</code>，以及回调函数<code>LidarSimulation::obstacleHandleServer</code>。</p><p>3.5 lidar_simulation功能包中的这两个节点：<code>lidar_simulation</code>是map级的，<code>obstacle_simulation</code>是obstacle级的。</p></li></ol><p><strong>接下来看simulation_launch包：</strong></p><p>这个包里面没有源代码，只提供了几个launch文件，用来仿真几种不同的情况：</p><ul><li><code>sim_move_simulation.launch</code>就是简单的底盘控制，控制小车在给定地图上运动，同时可视化雷达点云信息。</li><li><code>create_map_simulation.launch</code>用来建图，在底盘控制的基础上，启动了<strong>建图功能包</strong>。发布默认名字为/map的topic，命令行执行map_saver保存。</li><li><code>move_base_simulation.launch</code>用来导航，在底盘控制的基础上，启动了<strong>导航套件acml&amp;move_base</strong>，这时小车的底盘控制节点<code>autolabor_fake_node</code>订阅的<code>cmd_vel</code>信息不再来自teleop_keyboard，而是来自move_base的规划结果。</li></ul><p><strong>最后是move_base_sim这个功能包：</strong></p><p>是用作<strong>真实底盘</strong>控制的（目测就是对ROS开源的move_base包的二次封装，貌似删了一些不用的插件），先skip，接下来我会直接解析ROS导航套件。</p>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>sublime无法安装插件</title>
      <link href="/2018/10/12/sublime%E6%97%A0%E6%B3%95%E5%AE%89%E8%A3%85%E6%8F%92%E4%BB%B6/"/>
      <url>/2018/10/12/sublime%E6%97%A0%E6%B3%95%E5%AE%89%E8%A3%85%E6%8F%92%E4%BB%B6/</url>
      <content type="html"><![CDATA[<p>Package Control的配置文件中添加：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">"downloader_precedence":</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"linux"</span>:</span><br><span class="line">[</span><br><span class="line"><span class="string">"curl"</span>,</span><br><span class="line"><span class="string">"urllib"</span>,</span><br><span class="line"><span class="string">"wget"</span></span><br><span class="line">],</span><br><span class="line"><span class="attr">"osx"</span>:</span><br><span class="line">[</span><br><span class="line"><span class="string">"curl"</span>,</span><br><span class="line"><span class="string">"urllib"</span></span><br><span class="line">],</span><br><span class="line"><span class="attr">"windows"</span>:</span><br><span class="line">[</span><br><span class="line"><span class="string">"wininet"</span></span><br><span class="line">]</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><p>OSX和ubuntu下亲测均有效。</p>]]></content>
      
      
        <tags>
            
            <tag> tool </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>VMware扩容</title>
      <link href="/2018/10/10/VMware%E6%89%A9%E5%AE%B9/"/>
      <url>/2018/10/10/VMware%E6%89%A9%E5%AE%B9/</url>
      <content type="html"><![CDATA[<p>之前创建虚拟机时不知道需要多大空间，给了20G，装了ROS相关套件之后磁盘使用率到了90%。</p><p>然后在硬盘设置里面调整到了60G，我以为这样就可以了。。。。</p><p><img src="/2018/10/10/VMware扩容/硬盘.png" alt=""></p><p>too naive。。。还是要手动配置一下，需要安装工具Gparted：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install gparted</span><br></pre></td></tr></table></figure><p>然后先抹掉extended和swap两个分区，然后就可以resize主分区了，然后在重新创建那两个分区就好了。</p><p><img src="/2018/10/10/VMware扩容/gparted.png" alt=""></p><p><strong>Attention：</strong>磁盘只能扩展，不能变小，因此建议逐渐扩展。</p>]]></content>
      
      
        <tags>
            
            <tag> tool </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CSM, Correlative Scan Matching</title>
      <link href="/2018/10/08/CSM/"/>
      <url>/2018/10/08/CSM/</url>
      <content type="html"><![CDATA[<p>SLAM前端主要解决两个问题，一是从不同时刻的传感器输入中识别出同一个地图特征，二是计算每个当前时刻机器人相对该特征的位姿。</p><p>vSLAM能够轻松解决前者，LidarSLAM解决后者无压力。本文讨论激光SLAM前端问题1——特征点匹配。目前两大常用思路：scan2scan——ICP，scan2map——CSM。</p><p>basic的CSM算法思路如下：从前一帧机器人位姿开始，寻找最优刚性变换，使雷达点在栅格地图中的位置尽量对应于占据度为1的栅格。</p><p>为了保持定位信息原有的数位精度，使用双线性插值方法来获取雷达点的地图值(占据度)，而不是使用其所在栅格的地图值来直接对应：</p><p><img src="/2018/10/08/CSM/插值.png" alt="插值"></p><p>$Pm$是雷达点，$P_{00}, P_{01}, P_{10}, P_{11}$是雷达点邻近的四个<strong>栅格中心</strong>点。于是得到地图值：</p><script type="math/tex; mode=display">\begin{align}M(P_m)&  \approx \frac{y-y_0}{y_1-y_0}M(I_0) +\frac{y_1-y}{y_1-y_0}M(I_1)\\& =\frac{y-y_0}{y_1-y_0} \left ( \frac{x-x_0}{x_1-x_0}M(P_{11}) + \frac{x_1-x}{x_1-x_0}M(P_{01})\right)\\& +\frac{y_1-y}{y_1-y_0}\left ( \frac{x-x_0}{x_1-x_0}M(P_{10}) + \frac{x_1-x}{x_1-x_0}M(P_{00})\right)\end{align}</script><p>雷达点所在位置的地图值变化梯度：</p><script type="math/tex; mode=display">\triangledown M(P_m) = \left [\frac{\delta M(P_m)}{\delta x}, \frac{\delta M(P_m)}{\delta y} \right]\\\frac{\delta M(P_m)}{\delta x} =\frac{y-y_0}{(y_1-y_0)(x_1-x_0)}(M(P_{11})- M(P_{01}))  +\frac{y_1-y}{(y_1-y_0)(x_1-x_0)}(M(P_{10})- M(P_{00}))\\\frac{\delta M(P_m)}{\delta y} =\frac{x-x_0}{(y_1-y_0)(x_1-x_0)}(M(P_{11})- M(P_{10}))  +\frac{x_1-x}{(y_1-y_0)(x_1-x_0)}(M(P_{01})- M(P_{00}))\\</script><p>记当前时刻的已有地图$M$，当前帧共输入$n$个雷达点$S_1, …, S_n$，其对应位置的占据度为$M(S_k)$，最优变换定义为$\xi = (\Delta x, \Delta y, \psi)^T$，则最优问题的最小二乘描述为：</p><script type="math/tex; mode=display">\xi = argmin_{\xi} \sum_{k=1}^n[1-M(S_k(\xi))]^2</script><p>scan2map的鲁棒性更强，但是实时性上打了折扣。对此主要有两个改进措施：一是局部搜索，二是分辨率金字塔。</p><p>一、局部搜索</p><p>实际计算中会<strong>选定一个搜索区间</strong>，通常只在上一时刻地图位置的附近，对其中包含的全部可能位姿进行评分。</p><p>上式（最小二乘表达式）只包含了雷达端点，考虑到传感器的噪点影响，<strong>局部极值</strong>影响大。</p><ul><li>基于模版的匹配：雷达端点及射线所在栅格构成的多边形区域，以此作为局部地图，进行map2map匹配。减少局部极值的影响，提高计算代价，同时考虑到动态目标，会引入新的局部极值。</li></ul><p><img src="/2018/10/08/CSM/模版.png" alt="模版"></p><ul><li><p>LM算法：迭代的方式求解最小二乘的最优解。</p></li><li><p>分支界定算法：基于广度优先搜索的算法，通过对解空间搜索树的分支进行扩展和剪枝，不断调整搜索方向，加快找到全局最优解的速度。界定核心：若当前分支的下界$C_{branch}$小于解空间上界$C_{HB}$，则进行拓展，否则进行裁剪，直至到达叶子结点，即找到最小代价解。</p></li></ul><p>二、多分辨率金字塔</p><p>两帧雷达点云的相似区域并不会影响匹配的最终结果，但会参与计算，导致搜索效率降低，需要更多的迭代次数达到收敛。</p><p>当地图分辨率较低时，部分地图信息会被忽略，这种高、低分辨率下的差异，有助于对地图中的相似场景进行区分。实际使用中，首先将初始位姿对应的雷达点云与最上层（粗分辨率）的地图进行匹配，计算出当前分辨率下的位姿，并作为初始值进入次一级地图进行匹配，以此类推。</p>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>leetcode95-二叉搜索树</title>
      <link href="/2018/08/16/leetcode95-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/"/>
      <url>/2018/08/16/leetcode95-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/</url>
      <content type="html"><![CDATA[<p>96和95都是二叉搜索树，先做的96，求树的结构有几种，没注意结点大小关系，用<strong>动态规划</strong>来做，$dp[i] = dp[0]*dp[i-1] + … + dp[i-1]*dp[0]$。注意递归调用的时间复杂度，<strong>自底向上</strong>来算：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">numTrees</span><span class="params">(self, n)</span>:</span></span><br><span class="line">res = [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">if</span> n &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> res[n]</span><br><span class="line">    </span><br><span class="line">    res += [<span class="number">0</span>]*(n<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, n+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</span><br><span class="line">            res[i] += res[j]*res[i<span class="number">-1</span>-j]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> res[n]</span><br></pre></td></tr></table></figure><p>95要列出结构了，才发现<strong>什么是二叉搜索树来着</strong>——左子树的结点值均小于根节点，右子树的结点值均大于根节点。按照惯例，求数量用DP，求枚举则用DFS：</p><p>遍历每一个数字$i$作为根节点，那么$[1, 2, …, i-1]$构成其左子树，$[i+1, i+2, …, n]$构成其右子树。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateTrees</span><span class="params">(self, n)</span>:</span></span><br><span class="line"><span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line">    <span class="keyword">return</span> self.dfs(<span class="number">1</span>,n)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(self, b, e)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> b &gt; e:</span><br><span class="line">        <span class="keyword">return</span> [<span class="keyword">None</span>]</span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(b, e+<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># set as the root node</span></span><br><span class="line">        leftTrees = self.dfs(b, i<span class="number">-1</span>)</span><br><span class="line">        rightTrees = self.dfs(i+<span class="number">1</span>, e)</span><br><span class="line">        <span class="keyword">for</span> left <span class="keyword">in</span> leftTrees:</span><br><span class="line">            <span class="keyword">for</span> right <span class="keyword">in</span> rightTrees:</span><br><span class="line">                root = TreeNode(i)</span><br><span class="line">                root.left = left</span><br><span class="line">                root.right = right</span><br><span class="line">                res.append(root)</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>状态估计</title>
      <link href="/2018/06/30/%E7%8A%B6%E6%80%81%E4%BC%B0%E8%AE%A1/"/>
      <url>/2018/06/30/%E7%8A%B6%E6%80%81%E4%BC%B0%E8%AE%A1/</url>
      <content type="html"><![CDATA[<h4 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h4><ul><li>经典的SLAM模型由一个运动方程和一个观测方程构成：<script type="math/tex; mode=display">\left\{\begin{split}& x_k = f(x_{k-1}, u_k) + w_k\\& z_{k,j} = h(y_i, x_k) + v_{kj}\end{split}\right.</script>其中，$x_k$表示相机的位姿，可以用变换矩阵或李代数来表示，$y_i$表示路标，也就是图像中的特征点，$w_k$和$v_{kj}$表示噪声项，假设满足零均值的高斯分布。</li></ul><ul><li>我们希望通过带噪声的观测数据$z$和输入数据$u$推断位姿$x$和地图$y$的概率分布。主要采用两大类方法，一类是滤波方法：基于当前状态来估计下一状态，忽略历史；一类是非线性优化方法，使用所有时刻的数据估计新状态的最优分布。 <ul><li>滤波方法主要分为扩展卡尔曼滤波和粒子滤波两大类。</li><li>非线性优化根据实现细节的不同主要分为滑动窗口法和Pose Graph法。</li></ul></li></ul><h4 id="2-非线性优化"><a href="#2-非线性优化" class="headerlink" title="2. 非线性优化"></a>2. 非线性优化</h4><ul><li><p>非线性优化基于历史，同时也作用于历史，因此把所有待估计的变量放在一个状态变量中：</p><script type="math/tex; mode=display">x = \{x_1, ..., x_N, y_1, ..., y_M\}</script><p>在已知观测数据$z$和输入数据$u$的条件下，对机器人的状态估计：</p><script type="math/tex; mode=display">P(x | z,u)</script><p>先忽略测量运动的传感器，仅考虑测量方程，根据<strong>贝叶斯法则</strong>：</p><script type="math/tex; mode=display">P(x|z) = \frac{P(z|x)P(x)}{P(z)} \varpropto P(z|x)P(x)</script><ul><li>先验概率$P(x)$：先验的概念最好理解，就是一个事件的概率分布。</li><li>似然概率$P(z|x)$：已知事件的概率分布，事件中某状态的概率。</li><li>后验概率$P(x|z)$：在给定数据条件下，不确定性的条件分布。</li><li>$Posterior  \varpropto Likelihood * Prior$</li></ul><p>求解后验分布比较困难，但是求一个状态最优估计（使得后验概率最大化）是可行的：</p><script type="math/tex; mode=display">x^*_{MAP} = argmaxP(x|z) = argmaxP(z|x)P(x) = argmaxP(z|x)</script><p>因为先验概率不知道，所以问题直接转成为求解极大似然估计，问题中的未知数是$x$，直观意义就是：寻找一个最优的状态分布，使其最可能产生当前观测到的数据。</p></li><li><p>假设了噪声项$v_{kj} \thicksim N(0, Q_{k,j})$，所以极大似然概率也服从一个高斯分布：</p><script type="math/tex; mode=display">P(z_{kj}|x_k, y_j) = N(h (y_j, x_i), Q)</script><p>求高斯分布的最值通常取负对数处理，最大化变成求最小化：</p><script type="math/tex; mode=display">P(x) = \frac{1}{\sqrt{(2\pi)^Ndet(\Sigma)}} exp\bigg(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\bigg)\\-ln(P(x) ) = \frac{1}{2}ln\big((2\pi)^Ndet(\Sigma)\big) + \frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)</script><p>对数第一项与$x$无关，第二项等价于噪声的平方项。因此可以得到优化问题的目标函数：</p><script type="math/tex; mode=display">e_{k} = x_k - f(x_{k-1}, u_k)\\e_{kj} = z_{kj} - h(x_k, y_j)\\J(x) = \Sigma e_k^TR^{-1}_ke_{k} + \Sigma e_{kj}^TQ^{-1}_ke_{kj}\\x* = argmin_x J(x)</script><p>以上的最小二乘问题可以采用各种各样的梯度下降法求解最优解（参考<a href="https://amberzzzz.github.io/2018/05/02/graph-based-optimization/">图优化</a>）。</p></li><li><p>Bundle Ajustment</p><p>优化问题最终可以表示成$H\Delta x = g$的形式，其对角线上的两个矩阵为稀疏矩阵，且右下角的矩阵维度往往远大于左上角（因为特征点的数目远大于位姿节点）：</p><script type="math/tex; mode=display">H = \begin{bmatrix}B &E\\E^T & C\end{bmatrix}</script><p>一个有效的求解方式称为<strong>Schur消元</strong>，也叫<strong>边缘化Marginalization</strong>，主要思路如下：首先求解$C$矩阵的逆矩阵，然后对$H$矩阵进行消元，目标是消去其右上角的$E$矩阵，这样就能够先独立求解相机参数$\Delta x_c$，再利用求得的解来求landmarks参数$\Delta x_p$：</p><script type="math/tex; mode=display">\begin{bmatrix}I &-EC^{-1}\\0 & I\end{bmatrix}\begin{bmatrix}B &E\\E^T & C\end{bmatrix}\begin{bmatrix}\Delta x_c \\\Delta x_p\end{bmatrix} = \begin{bmatrix}I &-EC^{-1}\\0 & I\end{bmatrix}\begin{bmatrix}v \\w\end{bmatrix} \\\begin{bmatrix}B - EC^{-1}E^T & 0\\E^T & C\end{bmatrix}\begin{bmatrix}\Delta x_c \\\Delta x_p\end{bmatrix} = \begin{bmatrix}v  - EC^{-1}w\\w\end{bmatrix}</script><p>因此可以解得$\Delta x_c$：</p><script type="math/tex; mode=display">\begin{bmatrix}B - EC^{-1}E^T\end{bmatrix} \Delta x_c = v - EC^{-1}w</script><ul><li>这个矩阵称为消元之后的$S$矩阵，它的维度与相机参数的维度一致</li><li>$S$矩阵的意义是两个相机变量之间是否存在着共同观测点</li><li>$S$矩阵的稀疏性由实际数据情况决定，因此只能通过普通的矩阵分解的方式来求解</li></ul></li><li><p>核函数</p><p>当误差很大时，二范数增长的很快，为了防止其过大掩盖掉其他的边，可以将其替换成增长没那么快的函数，使得整个优化结果更为稳健，因此又叫鲁棒核函数，常用的核有Huber核、Cauchy核、Tukey核等，Huber核的定义如下：</p><script type="math/tex; mode=display">H(e) = \left\{\begin{split}   & \frac{1}{2} e ^2, \ \ \ |e| \leq \delta\\  &  \delta(|e| -  \frac{1}{2}\delta), \ \ else\end{split}   \right.</script></li></ul><h4 id="3-卡尔曼滤波"><a href="#3-卡尔曼滤波" class="headerlink" title="3. 卡尔曼滤波"></a>3. 卡尔曼滤波</h4><p>滤波思路基于一个重要的<strong>假设：一阶马尔可夫性——k时刻状态只与k-1时刻状态有关</strong>，整理成两个要素如下：</p><ul><li>$x_{k-1}$ contains the whole history</li><li>$x_k = f(x_{k-1}, u_k, z_k)$</li></ul><p>在这里我们只需要维护一个状态量$x_k$，并对它不断进行迭代更新，<strong>moreover，如果状态量服从高斯分布，我们只需要维护状态量的均值和方差即可（进一步简化）</strong>。</p><p>首先考虑一个<strong>线性系统</strong>：</p><script type="math/tex; mode=display">\left\{\begin{split}& x_k = A_k x_{k-1}+u_k + w_k\\& z_k = C_k x_k + v_k\end{split}\right.\\w_k \thicksim N(0, R), v_k \thicksim N(0, Q)</script><p>卡尔曼滤波器的第一步<strong>预测</strong>，通过运动方程确定$x_k$的先验分布，注意用不同的上标区分不同的概率分布：尖帽子$\hat x_k$表示后验，平帽子$\bar x_k$表示先验：</p><script type="math/tex; mode=display">P(\bar x_k) =  N(\bar x_k, \bar P_k)\\\bar x_k = A_k \hat x_{k-1} + u_k\\\bar P_k = A_k\hat P_{k-1}A_k^T+R</script><p>第二步为<strong>观测</strong>，通过分析实际观测值，计算<strong>在某状态下应该产生怎样的分布</strong>：</p><script type="math/tex; mode=display">P(z_k|x_k) = N(C_kx_k, Q)</script><p>第三步为<strong>更新</strong>，根据第一节中的<strong>贝叶斯法则</strong>，得到$x_k$的后验分布：</p><script type="math/tex; mode=display">(\hat x_k, \hat P_k) = N(C_kx_k, Q)N(\bar x_k, \bar P_k)\\K = \bar P_k C_k^T(C_k\bar P_k C_k^T + Q_k)^{-1}\\\hat x_k = \bar x_k + K(z_k-C_k\bar x_k)\\\hat P_k = (I - KC_k)\bar P_k</script><p>整体的流程图如下：</p><p><img src="/2018/06/30/状态估计/KF.jpeg" alt=""></p><p>具体过程本节中不做展开，详情可以参考<a href="">卡尔曼滤波</a>。高斯分布经过线性变换仍然服从高斯分布，因此整个过程没有发生任何的近似，因此可以说卡尔曼滤波器构成了线性系统的最优无偏估计。</p><p>下面考虑<strong>非线性系统</strong>：</p><p>SLAM中不管是三维还是平面刚体运动，因为都引入了旋转，因此其运动方程和观测方程都是非线性函数。一个高斯分布，经过非线性变换，通常就不再服从高斯分布，因此<strong>对于非线性系统，必须采取一定的近似，将一个非高斯分布近似成高斯分布</strong>。</p><p>通常的做法是，将k时刻的运动方程和观测方程在$\hat x_{k-1}$，$\hat P_{k-1}$处做一阶泰勒展开，得到两个雅可比矩阵：</p><script type="math/tex; mode=display">F = \frac{\partial f}{\partial x_{k-1}}\bigg|_{\hat x_{k-1}}\\H = \frac{\partial h}{\partial x_k}\bigg|_{\hat x_k}</script><p>中间量卡尔曼增益$K_k$：</p><script type="math/tex; mode=display">\bar P_k = F\hat P_{k-1}F^T + R_k\\K_k = \bar P_k H^T(H \bar P_k H^T + Q_k)^{-1}</script><p>后验概率：</p><script type="math/tex; mode=display">\hat x_k = \bar x_k + K_k(z_k - h(\bar x_k))\\\hat P_k = (I - K_k H)\bar P_k</script><p> 对于SLAM这种非线性的情况，EKF给出的是单次线性近似下的最大后验估计（MAP）。</p><h4 id="4-EKF-VS-Graph-Optimization"><a href="#4-EKF-VS-Graph-Optimization" class="headerlink" title="4. EKF VS Graph-Optimization"></a>4. EKF VS Graph-Optimization</h4><ol><li>马尔可夫性抛弃了更久之前的状态，优化方法则运用了更多的信息。</li><li>非线性误差：两种方法都使用了线性化近似，EKF只在$x_{k-1}$处做了一次线性化，图优化法则在每一次迭代更新时都对新的状态点做泰勒展开，其线性化的模型更接近原始非线性模型。</li><li>存储：EKF维护的是状态的均值和方差，存储量与状态维度成平方增长，图优化存储的是每个状态点的位姿，存储线性增长。</li></ol>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>优化库：Ceres &amp; g2o</title>
      <link href="/2018/06/28/%E4%BC%98%E5%8C%96%E5%BA%93%EF%BC%9Ag2o-Ceres/"/>
      <url>/2018/06/28/%E4%BC%98%E5%8C%96%E5%BA%93%EF%BC%9Ag2o-Ceres/</url>
      <content type="html"><![CDATA[<h3 id="Ceres"><a href="#Ceres" class="headerlink" title="Ceres"></a>Ceres</h3><p>使用Ceres求解非线性优化问题，主要分为三个部分：</p><ul><li><p>第一部分：构建代价函数<code>Cost_Functor</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义一个实例化时才知道的类型T</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 运算符()的重载，用来得到残差fi</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> T* <span class="keyword">const</span> x, T* residual)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">     residual[<span class="number">0</span>] = T(<span class="number">10.0</span>) - x[<span class="number">0</span>];</span><br><span class="line">     <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></li><li><p>第二部分：构建最小二乘问题<code>problem</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Problem problem;</span><br><span class="line"><span class="comment">// 使用自动求导，第一个1是输出维度（残差项），第二个1是输入维度（优化项）</span></span><br><span class="line">CostFunction* cost_function = <span class="keyword">new</span> AutoDiffCostFunction&lt;CostFunctor, <span class="number">1</span>, <span class="number">1</span>&gt;(<span class="keyword">new</span> CostFunctor);</span><br><span class="line"><span class="comment">// 添加误差项，NULL表示不使用核函数，x是优化项</span></span><br><span class="line">problem.AddResidualBlock(cost_function, <span class="literal">NULL</span>, &amp;x);</span><br></pre></td></tr></table></figure></li><li><p>第三部分：求解器参数配置<code>Solver</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Solver::Options options;</span><br><span class="line">options.linear_solver_type = ceres::DENSE_QR; <span class="comment">//配置增量方程的解法，稠密的QR分解</span></span><br><span class="line">options.minimizer_progress_to_stdout = <span class="literal">true</span>;<span class="comment">//输出到cout</span></span><br><span class="line">Solver::Summary summary;<span class="comment">//优化信息</span></span><br><span class="line">Solve(options, &amp;problem, &amp;summary);<span class="comment">//求解</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; summary.BriefReport() &lt;&lt; <span class="string">"\n"</span>;<span class="comment">//输出优化的简要信息</span></span><br></pre></td></tr></table></figure></li></ul><p><strong>使用核函数</strong>：数据中往往存在<strong>离群点</strong>，离群点会对寻优结果造成影响，这时可以使用一些损失核函数来对离群点的影响加以消除，Ceres库中提供的核函数主要有：TrivialLoss 、HuberLoss、 SoftLOneLoss 、 CauchyLoss。 </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用核函数</span></span><br><span class="line">problem.AddResidualBlock(cost_function, <span class="keyword">new</span> CauchyLoss(<span class="number">0.5</span>, &amp;x);</span><br></pre></td></tr></table></figure><p><img src="/2018/06/28/优化库：g2o-Ceres/without kernel.png" alt="without kernel"></p><p><img src="/2018/06/28/优化库：g2o-Ceres/with kernel.png" alt="with kernel"></p><h3 id="g2o"><a href="#g2o" class="headerlink" title="g2o"></a>g2o</h3><p>用g2o优化库来进行优化的步骤如下：</p><ol><li><p>定义节点和边的类型，通常在默认的基础类型上做修改</p><p>定义顶点，顶点的基类为<code>g2o::BaseVertex&lt;优化变量维度，数据类型&gt;</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CurveFittingVertex</span>:</span> <span class="keyword">public</span> g2o::BaseVertex&lt;<span class="number">3</span>, Eigen::Vector3d&gt;</span><br></pre></td></tr></table></figure><p>顶点的更新函数<code>oplusImpl</code>：定义增量加法，因为优化变量和增量之间并不一定是线性叠加的关系，如位姿变换。</p><p>定义边， 本例中的边为一元边，基类为<code>g2o::BaseUnaryEdge&lt;观测值维度，数据类型，连接顶点类型&gt;</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CurveFittingEdge</span>:</span> <span class="keyword">public</span>  g2o::BaseUnaryEdge&lt;<span class="number">1</span>, <span class="keyword">double</span> , CurveFittingVertex&gt;</span><br></pre></td></tr></table></figure><p>误差项计算函数<code>computeError</code>：计算<strong>预测值</strong>和<strong>观测值</strong>的误差。估计值是基于当前对优化变量的estimate计算出的，观测值是直接获取的，如本例中的y值。</p></li></ol><ol><li><p>构建图模型</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// vertex</span></span><br><span class="line">g2o::VertexSE3Expmap* pose = <span class="keyword">new</span> g2o::VertexSE3Expmap(); <span class="comment">// camera pose</span></span><br><span class="line">pose-&gt;setId( index );</span><br><span class="line">pose-&gt;setEstimate( expression );</span><br><span class="line">optimizer.addVertex ( pose );</span><br><span class="line"></span><br><span class="line"><span class="comment">// edge</span></span><br><span class="line">g2o::EdgeProjectXYZ2UV* edge = <span class="keyword">new</span> g2o::EdgeProjectXYZ2UV();</span><br><span class="line">edge-&gt;setId ( index );</span><br><span class="line">edge-&gt;setVertex ( <span class="number">0</span>, point );</span><br><span class="line">edge-&gt;setVertex ( <span class="number">1</span>, pose );</span><br><span class="line">edge-&gt;setMeasurement ( Eigen::Vector2d ( p.x, p.y ) );     <span class="comment">// 导入观测值</span></span><br><span class="line">edge-&gt;setParameterId ( <span class="number">0</span>,<span class="number">0</span> );</span><br><span class="line">edge-&gt;setInformation ( Eigen::Matrix2d::Identity() );      <span class="comment">// 设置信息矩阵</span></span><br><span class="line">optimizer.addEdge ( edge );</span><br></pre></td></tr></table></figure><p>信息矩阵<code>edge-&gt;setInformation(信息矩阵)</code>：因为最终的优化函数是$\sum e_i^T \Sigma^{-1}e_i$，是误差项和信息矩阵乘积的形式。</p></li><li><p>优化器配置</p><ul><li>矩阵块Block</li><li>优化算法solver</li><li>图模型optimizer</li></ul></li><li><p>执行优化</p></li></ol>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>直接法</title>
      <link href="/2018/06/16/%E7%9B%B4%E6%8E%A5%E6%B3%95/"/>
      <url>/2018/06/16/%E7%9B%B4%E6%8E%A5%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>前面说完了 <a href="https://amberzzzz.github.io/2018/06/07/PnP-Perspective-n-Point/">PnP</a>，趁热打铁更新直接法，因为两者的思路基本一致，主要的差别在于PnP中利用的是<strong>特征点的重投影误差</strong>——匹配点在query帧像素平面上的实际位置和估计位置的误差，直接法不提取特征点，而是采用<strong>像素亮度误差</strong>。</p><h3 id="1-直接法的推导"><a href="#1-直接法的推导" class="headerlink" title="1. 直接法的推导"></a>1. 直接法的推导</h3><p>以第一个相机为参考系，第二个相机的运动参数为$R, t, \xi$，对某个空间点$P$：</p><script type="math/tex; mode=display">p_1 = \begin{bmatrix}u_1\\v_1\\1\end{bmatrix} = \frac{1}{Z_1}KP\\p_2 = \begin{bmatrix}u_2\\v_2\\1\end{bmatrix} = \frac{1}{Z_2}K(RP+t) = \frac{1}{Z_2}Kexp(\xi^{\wedge})P</script><p>两个像素点的亮度误差：</p><script type="math/tex; mode=display">e = I_1(p_1) - I_2(p_2)</script><p>目标函数：</p><script type="math/tex; mode=display">min_{\xi} J(\xi) = \sum_{i=1}^N||e_i^Te_i||^2_2</script><p>误差函数关于优化变量的导数：</p><script type="math/tex; mode=display">\begin{split}e(\xi + \delta \xi)&  = I_1\big(\frac{1}{Z_1}KP\big) - I_2\big(\frac{1}{Z_2}K(exp(\delta \xi^{\wedge})exp(\xi^{\wedge})P\big)\\& \approx I_1\big(\frac{1}{Z_1}KP\big) - I_2\big(\frac{1}{Z_2}K(1+\delta \xi^{\wedge})exp(\xi^{\wedge})P\big)\\& = I_1\big(\frac{1}{Z_1}KP\big) -  I_2\big(\frac{1}{Z_2}Kexp(\xi^{\wedge})P\big) - I_2\big(\frac{1}{Z_2}K\delta \xi^{\wedge} exp(\xi^{\wedge})P \big)\\& = e(\xi) -  I_2\big(\frac{1}{Z_2}K\delta \xi^{\wedge} exp(\xi^{\wedge})P \big)\end{split}</script><p>上面的扰动相关项中，记：</p><script type="math/tex; mode=display">q =  \delta \xi^{\wedge} exp(\xi^{\wedge})P\\u =  \frac{1}{Z_2}Kq\\</script><p>误差函数线性化：</p><script type="math/tex; mode=display">e(\xi + \delta \xi) = e(\xi) - I_2(u)\\\therefore \frac{e(\xi + \delta \xi)}{\partial \delta \xi} = \frac{-I_2(u)}{\partial \delta \xi}\\e(\xi + \delta \xi) =e(\xi) - (\frac{\partial I_2}{\partial u}\frac{\partial u}{\partial q}\frac{\partial q}{\partial \delta \xi})\delta \xi</script><p>$q$表示扰动分量在第二相机坐标系下的坐标（回顾关于$R$的微分方程：$\dot R(t) = \phi_0^{\wedge}R(t)$），因此$u$的意义为像素坐标，$\frac{\partial I_2}{\partial u}$的物理意义为像素梯度，$\frac{\partial u}{\partial q}$的物理意义为像素坐标关于三维点的导数（参考针孔相机模型），$\frac{\partial q}{\partial \delta \xi}$的物理意义为三维点关于扰动的导数（参考李代数）。</p><h3 id="2-直接法分类"><a href="#2-直接法分类" class="headerlink" title="2. 直接法分类"></a>2. 直接法分类</h3><p>根据P的来源，直接法分为三类：</p><ol><li>P来自于稀疏关键点——稀疏直接法</li><li>P来自部分像素，只使用带有梯度的像素点——半稠密直接法</li><li>P为所有像素——稠密直接法</li></ol><h3 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3. 代码实现"></a>3. 代码实现</h3><p>主要关注Edge类里面重定义的增量更新函数linearizeOplus()里面Jacobian矩阵的写法。</p><script type="math/tex; mode=display">\begin{split}J & = -\frac{\partial I_2}{\partial u}\frac{\partial u}{\partial q}\frac{\partial q}{\partial \delta \xi}\\& = -\frac{\partial I_2}{\partial u}\frac{\partial u}{\partial \delta \xi}\\\end{split}</script><ul><li>前一项是$u$处的像素梯度，使用数值导数：</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// jacobian from I to u (1*2)</span></span><br><span class="line">Eigen::Matrix&lt;<span class="keyword">double</span>, <span class="number">1</span>, <span class="number">2</span>&gt; jacobian_pixel_uv;</span><br><span class="line">jacobian_pixel_uv ( <span class="number">0</span>,<span class="number">0</span> ) = ( getPixelValue ( u+<span class="number">1</span>,v )-getPixelValue ( u<span class="number">-1</span>,v ) ) /<span class="number">2</span>;</span><br><span class="line">jacobian_pixel_uv ( <span class="number">0</span>,<span class="number">1</span> ) = ( getPixelValue ( u,v+<span class="number">1</span> )-getPixelValue ( u,v<span class="number">-1</span> ) ) /<span class="number">2</span>;</span><br></pre></td></tr></table></figure><ul><li><p>getPixelValue这个函数涉及到一个<strong>双线性插值</strong>，因为上面的二维坐标uv是通过相机投影变换得到的，是浮点形式，而像素值是离散的整数值，为了更精细地表示像素亮度，要对图像进行进行插值。</p><ul><li>线性插值：已知数据$(x_0, y_0)$和$(x_1, y_1)$，要计算$[x_0, x_1]$区间内任一$x$对应的$y$值：</li></ul><script type="math/tex; mode=display">\frac{y - y_0}{x-x_0} = \frac{y_1-y_0}{x_1-x_0}\\\therefore y = \frac{x_1 -x}{x_1 - x_0}y_0 + \frac{x-x_0}{x_1-x_0}y_1</script><ul><li><p>双线性插值：本质上就是在两个方向上做线性插值：</p><p><img src="/2018/06/16/直接法/interpolation.png" alt="interpolate"></p><p>首先是x方向：</p><script type="math/tex; mode=display">f(R_1) = \frac{x_2 - x}{x_2-x_1}f(Q_{11}) + \frac{x - x_1}{x_2-x_1}f(Q_{21}), \ where\ R_1 = (x, y_1)\\f(R_2) = \frac{x_2 - x}{x_2-x_1}f(Q_{12}) + \frac{x - x_1}{x_2-x_1}f(Q_{22}),  \ where\ R_2 = (x, y_2)</script><p>然后y方向：</p><script type="math/tex; mode=display">f(P) = \frac{y_2 - y}{y_2 - y_1}f(R_1) + \frac{y-y_1}{y_2-y_1}f(R_2)</script><p>综合起来就是： </p><script type="math/tex; mode=display">f(x,y) = \frac{f(Q_{11})}{(x_2-x_1)(y_2-y_1)}(x_2-x)(y_2-y) +  \frac{f(Q_{21})}{(x_2-x_1)(y_2-y_1)}(x-x_1)(y_2-y) \\+ \frac{f(Q_{12})}{(x_2-x_1)(y_2-y_1)}(x_2-x)(y-y_1) +  \frac{f(Q_{22})}{(x_2-x_1)(y_2-y_1)}(x-x_1)(y-y_1)</script></li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">float</span> <span class="title">getPixelValue</span> <span class="params">( <span class="keyword">float</span> x, <span class="keyword">float</span> y )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    uchar* data = &amp; image_-&gt;data[ <span class="keyword">int</span> ( y ) * image_-&gt;step + <span class="keyword">int</span> ( x ) ];</span><br><span class="line">    <span class="keyword">float</span> xx = x - <span class="built_in">floor</span> ( x );</span><br><span class="line">    <span class="keyword">float</span> yy = y - <span class="built_in">floor</span> ( y );</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">float</span> (</span><br><span class="line">            ( <span class="number">1</span>-xx ) * ( <span class="number">1</span>-yy ) * data[<span class="number">0</span>] +</span><br><span class="line">            xx* ( <span class="number">1</span>-yy ) * data[<span class="number">1</span>] +</span><br><span class="line">            ( <span class="number">1</span>-xx ) *yy*data[ image_-&gt;step ] +</span><br><span class="line">            xx*yy*data[image_-&gt;step+<span class="number">1</span>]</span><br><span class="line">    );</span><br></pre></td></tr></table></figure><ul><li>后两项都是与相机参数和三维点坐标有关，可以合并，同时注意g2o中对SE3的定义平移和旋转和本文设定是反过来的。</li></ul><script type="math/tex; mode=display">\xi = \begin{bmatrix}\rho\\\phi\end{bmatrix}\\\frac{\partial u}{\partial \delta \xi}=\begin{bmatrix}\frac{f_x}{Z} & 0 & -\frac{f_xX}{Z^2} & |& -\frac{f_xXY}{Z^2} & f_x + \frac{f_xX^2}{Z^2} & -\frac{f_xY}{Z}\\0 & \frac{f_y}{Z}  & -\frac{f_yY}{Z^2}& | & - f_y - \frac{f_xY^2}{Z^2}  & \frac{f_yXY}{Z^2} &  \frac{f_yX}{Z}\\\end{bmatrix}</script><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// jacobian from u to xi (2*6)</span></span><br><span class="line">Eigen::Matrix&lt;<span class="keyword">double</span>, <span class="number">2</span>, <span class="number">6</span>&gt; jacobian_uv_ksai;</span><br><span class="line"><span class="comment">// xi = [\phi, \pho]</span></span><br><span class="line">jacobian_uv_ksai ( <span class="number">0</span>,<span class="number">0</span> ) = - x*y*invz_2 *fx_;</span><br><span class="line">jacobian_uv_ksai ( <span class="number">0</span>,<span class="number">1</span> ) = ( <span class="number">1</span>+ ( x*x*invz_2 ) ) *fx_;</span><br><span class="line">jacobian_uv_ksai ( <span class="number">0</span>,<span class="number">2</span> ) = - y*invz *fx_;</span><br><span class="line">jacobian_uv_ksai ( <span class="number">0</span>,<span class="number">3</span> ) = invz *fx_;</span><br><span class="line">jacobian_uv_ksai ( <span class="number">0</span>,<span class="number">4</span> ) = <span class="number">0</span>;</span><br><span class="line">jacobian_uv_ksai ( <span class="number">0</span>,<span class="number">5</span> ) = -x*invz_2 *fx_;</span><br><span class="line"></span><br><span class="line">jacobian_uv_ksai ( <span class="number">1</span>,<span class="number">0</span> ) = - ( <span class="number">1</span>+y*y*invz_2 ) *fy_;</span><br><span class="line">jacobian_uv_ksai ( <span class="number">1</span>,<span class="number">1</span> ) = x*y*invz_2 *fy_;</span><br><span class="line">jacobian_uv_ksai ( <span class="number">1</span>,<span class="number">2</span> ) = x*invz *fy_;</span><br><span class="line">jacobian_uv_ksai ( <span class="number">1</span>,<span class="number">3</span> ) = <span class="number">0</span>;</span><br><span class="line">jacobian_uv_ksai ( <span class="number">1</span>,<span class="number">4</span> ) = invz *fy_;</span><br><span class="line">jacobian_uv_ksai ( <span class="number">1</span>,<span class="number">5</span> ) = -y*invz_2 *fy_;</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>OpenCV定制源码编译</title>
      <link href="/2018/06/12/OpenCV%E5%AE%9A%E5%88%B6%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"/>
      <url>/2018/06/12/OpenCV%E5%AE%9A%E5%88%B6%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/</url>
      <content type="html"><![CDATA[<h3 id="1-cmake选项"><a href="#1-cmake选项" class="headerlink" title="1. cmake选项"></a>1. cmake选项</h3><ul><li>测试单元可以关掉：BUILD_DOCS，BUILD_EXAMPLES，BUILD_XXX_TESTS，BUILD_opencv_ts(一些单元测试代码)，BUILD_PACKAGE (CPACK_BINARY_XXX，CPACK_SOURCE_XXX)，INSTALL_XXX</li><li>减少引入体积：打开世界模块开关BUILD_opencv_world(暂时没开，因为编译之后发现找不到要引用的头文件了)，打开BUILD_SHARED_LIBS</li><li>关掉音视频处理相关模块：BUILD_opencv_video，BUILD_opencv_videoio，BUILD_opencv_videostab，WITH_1394，WITH_GSTREAMER_XXX</li><li>关闭GPU相关模块：WITH_OPENCL，WITH_CUDA</li><li>打开TBB模块：隐式的并行计算程序，底层依赖于操作系统的多线程库，BUILD_TBB</li><li>打开viz模块：WITH_VTK，BUILD_opencv_viz</li><li>暂时没开启Java相关模块：ant，就没brew过这个包</li></ul><p>以上reference from <a href="https://blog.csdn.net/rrrfff/article/details/76796261" target="_blank" rel="noopener">博客1</a>，<a href="http://tianchunbinghe.blog.163.com/blog/static/7001201151592834161/" target="_blank" rel="noopener">博客2</a></p><h3 id="2-extra-modules"><a href="#2-extra-modules" class="headerlink" title="2. extra modules"></a>2. extra modules</h3><blockquote><p>with opencv3.0, SURF/SIFT and some other things have been moved to a seperate <a href="https://github.com/itseez/opencv_contrib/" target="_blank" rel="noopener">opencv_contrib repo</a>.</p></blockquote><p>一部分模块被独立到了opencv_contrib这个包，首先clone到本地，然后在cmake选项里面找到OPENCV_EXTRA_MODULES_PATH，填好。</p><h3 id="2-其他说明"><a href="#2-其他说明" class="headerlink" title="2. 其他说明"></a>2. 其他说明</h3><p>另外之前brew install的opencv包一定要卸载掉，不要乱link，否则INCLUDE和LIBS的路径都会出问题，手动修改cmake文件不要太酸爽。</p>]]></content>
      
      
        <tags>
            
            <tag> opencv </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>leetcode 81 搜索旋转数组2</title>
      <link href="/2018/06/10/leetcode-81-%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%842/"/>
      <url>/2018/06/10/leetcode-81-%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%842/</url>
      <content type="html"><![CDATA[<p>实名diss这道题，一看是旋转排序数组直接做了，然后才发现测试样例里面出现了左右边界重合的情况，然后仔细再审题才发现这行小字：</p><p><img src="/2018/06/10/leetcode-81-搜索旋转数组2/屏幕快照 2018-06-10 14.59.19.png" alt=""></p><p>不包含重复元素的情况下代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">search</span><span class="params">(self, nums, target)</span>:</span></span><br><span class="line"></span><br><span class="line">        size = len(nums)</span><br><span class="line">        start = <span class="number">0</span></span><br><span class="line">        end = size - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> start &lt;= end:</span><br><span class="line"></span><br><span class="line">            mid = (start + end) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> nums[mid] == target:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> nums[mid] &lt;= nums[end]:</span><br><span class="line">                <span class="keyword">if</span> target &lt; nums[mid] <span class="keyword">or</span> target &gt; nums[end]:</span><br><span class="line">                    end = mid - <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    start = mid + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> target &gt; nums[mid] <span class="keyword">or</span> target &lt; nums[start]:</span><br><span class="line">                    start = mid + <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    end = mid - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br></pre></td></tr></table></figure><p>因为数组中现在存在重复的元素，因此有一个特殊的情况：左右边界值相同，并且nums[mid]值与边界值也相同，这时nums[mid]可能位于两段数组的任意一边。因此要独立讨论一下这个情况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">search</span><span class="params">(self, nums, target)</span>:</span></span><br><span class="line"></span><br><span class="line">        size = len(nums)</span><br><span class="line">        start = <span class="number">0</span></span><br><span class="line">        end = size - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> start &lt;= end:</span><br><span class="line"></span><br><span class="line">            mid = (start + end) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> nums[mid] == target:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> nums[mid] &lt; nums[end]:</span><br><span class="line">            <span class="keyword">if</span> target &lt; nums[mid] <span class="keyword">or</span> target &gt; nums[end]:</span><br><span class="line">                end = mid - <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                start = mid + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">elif</span> nums[mid] &gt; nums[end]:</span><br><span class="line">                <span class="keyword">if</span> target &gt; nums[mid] <span class="keyword">or</span> target &lt; nums[start]:</span><br><span class="line">                    start = mid + <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    end = mid - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># nums[mid] = nums[end]的情况</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">            end -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br></pre></td></tr></table></figure><p>测试用时50ms，因为边界重复的循环没有有效地二分数组，但是思路贼简单啊。</p>]]></content>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PnP, Perspective-n-Point</title>
      <link href="/2018/06/07/PnP-Perspective-n-Point/"/>
      <url>/2018/06/07/PnP-Perspective-n-Point/</url>
      <content type="html"><![CDATA[<h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><blockquote><p>PnP是求解3D到2D点对运动的方法，它描述了当知道n个3D空间点及其投影位置时，如何估计相机的位姿变换。最少只需要3个点对就可以估计相机的运动。</p></blockquote><p>该方法使用的条件是，参考点为世界坐标系下的特征点，其空间位置已知，并且知道query帧中对应参考点的像素坐标。</p><p>PnP问题的求解方法有很多种：</p><ul><li>直接线性变换</li><li>P3P</li><li>非线性优化方法——Bundle Ajustment</li></ul><p>本节着眼于BA求解方法，其他方法暂时不做展开。</p><h3 id="2-Bundle-Ajustment"><a href="#2-Bundle-Ajustment" class="headerlink" title="2. Bundle Ajustment"></a>2. Bundle Ajustment</h3><p>利用优化求解的思路是：<strong>最小化重投影误差</strong>——期望计算query帧的相机位姿$R, t$，它的李代数为$\xi$，空间特征点的坐标为$P_i = [X_i, Y_i, Z_i]^T$，其在query帧上的像素坐标为$u_i = [u_i, v_i]^T$，那么理论上：</p><script type="math/tex; mode=display">s_i u_i = K exp(\xi^{\wedge})P_i</script><p>构建成最小二乘问题就是：寻找最优的相机位姿$\xi$，使得误差最小化：</p><script type="math/tex; mode=display">\xi^* = argmin_{\xi} \frac{1}{2}\sum_{i=1}^{n}||u_i -\frac{1}{s_i}Kexp(\xi^{\wedge})P_i||^2_2</script><p>在使用优化库来求解之前，还有一个问题——每个误差项$e_i = u_i -\frac{1}{s_i}Kexp(\xi^{\wedge})P_i$的导数$J_i$。</p><blockquote><p>回忆<a href="https://amberzzzz.github.io/2018/05/02/graph-based-optimization/">图优化</a>中讲过的，优化问题最终转化成为矩阵的线性求解$H\Delta x = g$，其中矩阵$H$是由单个误差项一阶展开$e(x+\Delta x) = e(x) + J\Delta x$中的雅可比矩阵$J_i$ 构成的稀疏对称阵。</p></blockquote><p>误差项是一个二维向量（像素坐标差），优化变量是一个六维向量（空间位姿李代数），因此$J$是一个2*6的矩阵。</p><script type="math/tex; mode=display">\frac{\partial e}{\partial \delta \xi} = \lim_{\delta \xi \rightarrow0} \frac{\partial e}{\partial P^{'}}\frac{\partial P^{'}}{\partial \delta \xi}</script><p>其中$P^{‘}$是特征点转换到相机坐标系下的空间坐标：</p><script type="math/tex; mode=display">su = KP^{'}\\u = f_x \frac{X^{'}}{Z^{'}} + c_x\\v = f_y \frac{X^{'}}{Z^{'}} + c_y</script><p>因此误差项导数的第一项为：</p><script type="math/tex; mode=display">\frac{\partial e}{\partial P^{'}} = -\begin{bmatrix}\frac{\partial u}{\partial X^{'}} & \frac{\partial u}{\partial Y^{'}} & \frac{\partial u}{\partial Z^{'}}\\\frac{\partial v}{\partial X^{'}} & \frac{\partial v}{\partial Y^{'}} & \frac{\partial v}{\partial Z^{'}}\end{bmatrix}=-\begin{bmatrix}\frac{f_x}{Z^{'}} & 0 & -\frac{f_xX^{'}}{Z^{'2}}\\0 & \frac{f_y}{Z^{'}}  & -\frac{f_yY^{'}}{Z^{'2}}\\\end{bmatrix}</script><p>误差项的第二项为变换后的点关于李代数的导数，参考<a href="https://amberzzzz.github.io/2018/05/12/%E4%B8%89%E7%BB%B4%E5%88%9A%E4%BD%93%E8%BF%90%E5%8A%A8-%E6%9D%8E%E4%BB%A3%E6%95%B0/">李代数</a>节：</p><script type="math/tex; mode=display">\frac{\partial TP}{\partial \delta \xi} = \begin{bmatrix}I & -P^{'\wedge}\\0^T & 0^T\end{bmatrix}</script><p>其中$P^{‘\wedge}$是$P^{‘}$的反对称阵：</p><script type="math/tex; mode=display">P^{'\wedge} = \begin{bmatrix}0 & -Z^{'} & Y^{'}\\Z^{'} & 0 & -X^{'}\\-Y^{'} & X^{'} & 0\end{bmatrix}</script><p>因此得到完整的雅可比矩阵：</p><script type="math/tex; mode=display">\frac{\partial e}{\partial \delta \xi} =\frac{\partial e}{\partial P^{'}}\begin{bmatrix}I & P^{'\wedge}\end{bmatrix}=-\begin{bmatrix}\frac{f_x}{Z^{'}} & 0 & -\frac{f_xX^{'}}{Z^{'2}} & |& -\frac{f_xX^{'}Y^{'}}{Z^{'2}} & f_x + \frac{f_xX^{'2}}{Z^{'2}} & -\frac{f_xY^{'}}{Z^{'}}\\0 & \frac{f_y}{Z^{'}}  & -\frac{f_yY^{'}}{Z^{'2}}& | & - f_y - \frac{f_xY^{'2}}{Z^{'2}}  & \frac{f_yX^{'}Y^{'}}{Z^{'2}} &  \frac{f_yX^{'}}{Z^{'}}\\\end{bmatrix}</script><p>除了优化相机位姿以外，还可以同时优化特征点的空间位置$P$：</p><script type="math/tex; mode=display">\frac{\partial e}{\partial P} = \frac{\partial e}{\partial P^{'}} \frac{\partial P^{'}}{\partial P}</script><p>其中的第二项为：</p><script type="math/tex; mode=display">P^{'} = exp(\xi^{\wedge})P = RP +t\\\therefore \frac{\partial P^{'}}{\partial P} = R^T</script><h3 id="3-优化库使用"><a href="#3-优化库使用" class="headerlink" title="3. 优化库使用"></a>3. 优化库使用</h3><p>构建图模型：</p><ul><li>优化变量1：节点1：query相机位姿（六维向量李代数）</li><li>优化变量2：节点2：特征点空间位置（三维向量坐标描述）</li><li>预测值：边n：根据当前estimate的优化量，投影到投影平面的像素坐标$z_i = h(\xi, P_i)$</li><li>观测值：能够直接读出的，query帧上对应特征点的实际投影坐标$u_i$</li></ul><p>g2o中已经提供了相近的基类（在g2o/types/sba/types_six_dof_expmap.h中）：李代数位姿节点VertexSE3Expmap、空间点位置节点VertexSBAPointXYZ、投影方程边EdgeProjectXYZ2UV。</p><p>边类里面要关注linearizeOplus函数，这个函数描述的是非线性函数进行线性化的过程中，求导的解析解（当然也可以使用数值解），最后函数给出的是每个节点的导数矩阵（$\frac{\partial e}{\partial \delta \xi} $和$\frac{\partial e}{\partial P_i}$） 。<strong>这点是Ceres库和g2o库的一点主要差别：Ceres都是使用自动的数值导数，g2o要自己求导</strong>。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> EdgeProjectXYZ2UV::linearizeOplus()</span><br><span class="line">&#123;</span><br><span class="line">    VertexSE3Expmap * vj = <span class="keyword">static_cast</span>&lt;VertexSE3Expmap*&gt;(_vertices[<span class="number">1</span>]);</span><br><span class="line">    VertexSBAPointXYZ* vi = <span class="keyword">static_cast</span>&lt;VertexSBAPointXYZ*&gt;(_vertices[<span class="number">0</span>]);</span><br><span class="line">    ...</span><br><span class="line">    ...</span><br><span class="line">    _jacobianOplusXi = <span class="number">-1.</span>/z * tmp * T.rotation().toRotationMatrix();</span><br><span class="line"> </span><br><span class="line">    _jacobianOplusXj(<span class="number">0</span>,<span class="number">0</span>) = x*y/z^<span class="number">2</span> * cam-&gt;focal_length;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-OpenCV内置函数"><a href="#4-OpenCV内置函数" class="headerlink" title="4. OpenCV内置函数"></a>4. OpenCV内置函数</h3><p><strong>basic：</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> cv::solvePnP ( pts3d, pts2d, K, Mat(), r, t, <span class="literal">false</span> );</span><br><span class="line"></span><br><span class="line">Mat r, t, R;</span><br><span class="line">cv::solvePnP ( pts3d, pts2d, K, Mat(), r, t, <span class="literal">false</span> );</span><br><span class="line">cv::Rodrigues ( r, R );     <span class="comment">// 旋转向量和旋转矩阵的转换</span></span><br></pre></td></tr></table></figure><p><strong>advanced：</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bool</span> cv::solvePnPRansac( </span><br><span class="line">    InputArray objectPoints,      <span class="comment">// 3D空间坐标 vector&lt;cv::Point3f&gt; pts3d</span></span><br><span class="line">    InputArray imagePoints,       <span class="comment">// 2D像素坐标 vector&lt;cv::Point2f&gt; pts2d</span></span><br><span class="line">    InputArray cameraMatrix,      <span class="comment">// 相机内部参数矩阵 K</span></span><br><span class="line">    InputArray distCoeffs,        <span class="comment">// 畸变系数向量 cv::Mat()</span></span><br><span class="line">    OutputArray rvec,             <span class="comment">// 旋转向量</span></span><br><span class="line">    OutputArray tvec,             <span class="comment">// 平移向量</span></span><br><span class="line">    <span class="keyword">bool</span> useExtrinsicGuess = <span class="literal">false</span>, <span class="comment">// If true, the function uses the provided rvec and tvec values as initial</span></span><br><span class="line">    <span class="keyword">int</span> iterationsCount = <span class="number">100</span>,    <span class="comment">// Number of iterations</span></span><br><span class="line">    <span class="keyword">float</span> reprojectionError = <span class="number">8.0</span>,<span class="comment">// 重投影误差最大值</span></span><br><span class="line">    <span class="keyword">double</span> confidence = <span class="number">0.99</span>,</span><br><span class="line">    OutputArray inliers = noArray(), <span class="comment">// Output vector that contains indices of inliers in objectPoints and imagePoints .</span></span><br><span class="line">    <span class="keyword">int</span> flags = SOLVEPNP_ITERATIVE   <span class="comment">// method for solving PnP</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>Ransac：考虑到我们提供的匹配里面存在误匹配的情况，OpenCV采用“随机采样一致性算法”（Random Sample Consensus），从现有匹配中随机取一部分用来估计运动（PnP的解析解法最少只需要三个点就能计算相对位姿），正确的匹配结果都是近似的，从而剔除误匹配。</li><li>inlier：内点，函数最终给出的匹配可信的点。</li><li>RANSAC只采用少数几个随机点来计算PnP，容易受到噪声影响。工程上通常使用RANSAC的解作为初值，再使用非线性优化方法求解最优值。</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Ransac粗匹配</span></span><br><span class="line">cv::solvePnPRansac( pts3d, pts2d, K, Mat(), rvec, tvec, <span class="literal">false</span>, <span class="number">100</span>, <span class="number">4.0</span>, <span class="number">0.99</span>, inliers );</span><br><span class="line">cv::Rodrigues ( rvec, R ); </span><br><span class="line">Eigen::Matrix3d rotation_matrix = R.at&lt;<span class="keyword">double</span>&gt;;</span><br><span class="line">T_c_r_estimated_ = SE3d(</span><br><span class="line">        SO3d(rotation_matrix),</span><br><span class="line">        Vector3d( tvec.at&lt;<span class="keyword">double</span>&gt;(<span class="number">0</span>,<span class="number">0</span>), tvec.at&lt;<span class="keyword">double</span>&gt;(<span class="number">1</span>,<span class="number">0</span>), tvec.at&lt;<span class="keyword">double</span>&gt;(<span class="number">2</span>,<span class="number">0</span>))</span><br><span class="line">    );</span><br><span class="line">    </span><br><span class="line"><span class="comment">// BA局部优化</span></span><br><span class="line">g2o::VertexSE3Expmap* pose <span class="keyword">new</span> g2o::VertexSE3Expmap();</span><br><span class="line">...</span><br><span class="line">pose-&gt;setEstimate(g2o::SE3Quat( T_c_r_estimated_.rotationMatrix(), T_c_r_estimated_.translation()));</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>karto key concepts</title>
      <link href="/2018/06/05/karto-key-concepts/"/>
      <url>/2018/06/05/karto-key-concepts/</url>
      <content type="html"><![CDATA[<p>基于极其标准的图优化SLAM框架来实现：</p><p><img src="/2018/06/05/karto-key-concepts/1.png" alt="framework"></p><p>提出了采用稀疏点调整（SPA）的方法来高效求解位姿图优化问题，针对本算法的文献为《Efficient Sparse Pose Adjustment for 2D Mapping》。</p><p>scan matching部分的参考文献为《Real-time correlative scan matching》，M3RSM的前身！</p><h4 id="key-concepts："><a href="#key-concepts：" class="headerlink" title="key concepts："></a>key concepts：</h4><ul><li><p>keyScan：机器人运动一定的距离或角度（关键帧），储存在sensorManager，无地图缓存。</p></li><li><p>look-up table查找表：查找表的意义就是相比于暴力匹配，不需要每次都重新计算每个激光数据信息，<strong>相同角度不同位置</strong>的激光数据信息只需要被索引一次。</p></li><li><p>response响应值：将查找表以一定的位移投到子图上，总共有n个点被查找表击中（hit），击中的每个点得分不同（score），累加得分并除以可以达到的最高分。</p><script type="math/tex; mode=display">response = \frac{\sum_{i=0}^n goal_i}{goalmax}</script></li><li><p>协方差：文献专门用了一节计算协方差，但是没看到用在哪，是为了后面求误差做准备吗？？？</p></li><li><p>addScans添加顶点和边：边是误差值，添加的边约束来自两部分，</p><p>（1）link to running scans，距当前帧一定范围内的激光数据链（RunningScan chain）。</p><p>（2）link to other near chains，从当前节点开始广度优先遍历一定距离范围内所有节点，依据当前id从sensorManager中分别递增和递减寻找一定范围内的chain（不一定直接相连）。</p></li><li><p>回环检测：操作与添加边约束类似，位姿图上要<strong>去除那些和当前节点的时间相邻的节点</strong>。</p><p>（1）找到一定距离范围内（near）和相连（adjacent）的节点添加进nearLinkedScans。</p><p>（2）<code>MapperGraph::FindPossibleLoopClosure</code>：从sensorManager中从前到后，依据序号挑选与当前节点在一定距离范围内，且不在nearLinkedScans中的candidate。返回潜在chain。其中涉及两个参数：</p><ul><li>LoopSearchMaximumDistance：candidateScan与当前scan的距离必须在可容许的距离内。</li><li>LoopMatchMinimumChainSize：chain中的节点数必须不小于限定值。</li></ul><p>（3）<code>MapperGraph::TryCloseLoop</code>：scan2map匹配，当response和covariance达到一定要求认为闭环检测到，得到correct pose（也就是认为candidateScan的pose才是当前帧的实际pose）。</p><p>（4）add link to loop，构成全局闭环。</p><p>（5）触发correctPose，进行spa优化。</p></li></ul><p><img src="/2018/06/05/karto-key-concepts/graph opti.jpeg" alt=""></p><h4 id="代码随手记："><a href="#代码随手记：" class="headerlink" title="代码随手记："></a>代码随手记：</h4><ol><li><p>ROS上面提供三个开源包：nav2d_karto, open_karto, slam_karto。</p><p>ROS Wiki上这么描述nav2d_karto这个package：Graph-based Simultaneous Localization and Mapping module. Includes OpenKarto GraphSLAM library by “SRI International”.</p><p>open_karto：开源的karto包，实现底层的kartoslam</p><p>slam_karto：ros层，应用层的kartoslam接口</p></li><li><p>The LaserRangeFinder contains parameters for physical laser sensor used by the mapper for scan matching Also contains information about the maximum range of the sensor and provides a threshold for limiting the range of readings.</p><p>The optimal value for the range threshold depends on the angular resolution of the scan and the desired map resolution.</p></li><li><p>resolution：0.25 &amp; 0.5 &amp; 1 degree</p><p>number of range readings (beams)：（maximumAngle - minimumAngle）／angularResolution + 1</p></li><li><p>GridStates：0 for Unknown，100 for Occupied， 255 for Free。</p></li><li><p>flipY：最开始机器人应该处在世界坐标系的原点，传感器坐标系与机器人baselink存在一个坐标变换，原始的传感器坐标系位置应该与地图坐标系重合，这就是world和grid之间的offset。flip是啥呢？？</p></li><li><p>LookupArray[index]：Create lookup tables for point readings <strong>at varying angles</strong> in grid. This is to speed up finding best angle/position for a localized range scan</p></li><li><p>MapperGraph：花式构造位姿图</p></li><li><p>CorrelationGrid：Implementation of a <strong>correlation</strong> grid used for scan matching</p></li><li><p>Region of Interest ROI：</p></li><li><p>smear：The point readings are smeared by this value in X and Y to create a smoother response. 个人理解这句话是说点容易生成突变，用以点为中心的一小片区域平滑一点。</p></li><li><p>ScanMatch：返回响应值response</p><p>   前端匹配调用<code>m_pSequentialScanMatcher-&gt;MatchScan</code></p><p>   闭环检测调用<code>m_pLoopScanMatcher-&gt;MatchScan</code></p><p>   两个函数继承于<code>ScanMatcher::MatchScan</code>：</p>   <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">kt_double ScanMatcher::MatchScan(</span><br><span class="line">LocalizedRangeScan* pScan, </span><br><span class="line">    <span class="keyword">const</span> LocalizedRangeScanVector&amp; rBaseScans, </span><br><span class="line">    Pose2&amp; rMean, </span><br><span class="line">    Matrix3&amp; rCovariance, </span><br><span class="line">    kt_bool doPenalize, </span><br><span class="line">    kt_bool doRefineMatch)    <span class="comment">// default is True, 决定是否做精匹配</span></span><br><span class="line"><span class="comment">// @return: strength of response (best response)</span></span><br></pre></td></tr></table></figure><p>   其中会调用<code>ScanMatcher::CorrelateScan</code>方法。<code>ScanMatcher::CorrelateScan</code>方法中调用<code>ScanMatcher::GetResponse</code>方法计算响应值。</p>   <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kt_double ScanMatcher::GetResponse(</span><br><span class="line">kt_int32u angleIndex, </span><br><span class="line">kt_int32s gridPositionIndex) <span class="keyword">const</span></span><br></pre></td></tr></table></figure><p>   <code>GetResponse</code>的核心在<code>kt_int8u* pByte</code>和<code>const LookupArray* pOffsets</code>两个数据结构：</p><ul><li>前者是在correlationGrid范围内的real sensed占据情况。</li><li><p>后者是lookup-table中（已知地图）读取的栅格占据情况，只包含占据的栅格，key是angular。</p><p>计算response只要看地图上的占据点是否在观测中是否也是占据的：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (kt_int32u i = <span class="number">0</span>; i &lt; pOffsets-&gt;GetSize(); i++)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">// ignore points that fall off the grid</span></span><br><span class="line">  kt_int32s pointGridIndex = gridPositionIndex + pAngleIndexPointer[i];</span><br><span class="line">  <span class="keyword">if</span> (!math::IsUpTo(pointGridIndex, m_pCorrelationGrid-&gt;GetDataSize()) || pAngleIndexPointer[i] == INVALID_SCAN)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">continue</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// uses index offsets to efficiently find location of point in the grid</span></span><br><span class="line">  response += pByte[pAngleIndexPointer[i]];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最终的response要normalize：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// normalize response</span></span><br><span class="line">response /= (nPoints * GridStates_Occupied);   <span class="comment">// GridStates_Occupied = 100,</span></span><br><span class="line">assert(<span class="built_in">fabs</span>(response) &lt;= <span class="number">1.0</span>);</span><br></pre></td></tr></table></figure></li></ul></li><li><p>karto只在闭环的时候触发后端优化<code>CorrectPoses()</code>，ScanSolver的实现在Samples路径下的SpaSolver，调用了现有的BA求解器<a href="http://users.ics.forth.gr/~lourakis/sba/" target="_blank" rel="noopener">sba(A Generic Sparse Bundle Adjustment C/C++ Package Based on the Levenberg-Marquardt Algorithm)</a>。</p></li></ol><h4 id="参数-amp-优化方向"><a href="#参数-amp-优化方向" class="headerlink" title="参数&amp;优化方向"></a><a href="http://docs.ros.org/jade/api/nav2d_karto/html/classkarto_1_1OpenMapper.html" target="_blank" rel="noopener">参数</a>&amp;优化方向</h4><ol><li>闭环中candidate数量的调整：<ul><li>减小LoopSearchMaximumDistance，进入candidate范围的节点数据减少</li><li>减小LoopMatchMinimumChainSize，用来计算优化的candidate数量减少</li><li>增大minimum_travel_distance和minimum_travel_heading，这样总体的节点数减少</li></ul></li><li>Map_update_interval：发布地图的间隔，其主要过程是遍历当前所有节点数据，对每个栅格的占有状态进行判定，生成栅格地图。</li><li>ScanBufferSize和ScanBufferMaximumScanDistance：控制buffer也就是chain的大小。chain不能太大也不能太小，太小会造成前端误差累积，太大会导致构建闭环的节点数不足。推荐值是ScanBufferMaximumScanDistance／minimum_travel_distance。</li><li>位姿纠正中：<ul><li>CorrelationSearchSpaceDimension：The size of the <strong>search grid</strong></li><li>CorrelationSearchSpaceResolution：The size of the <strong>correlation grid</strong></li></ul></li><li>回环检测中：<ul><li>LoopSearchMaximumDistance：闭环检测的搜索距离，数值越大能越早发现闭环，也能容忍更大的偏离误差。</li><li>LoopMatchMinimumResponseCoarse和LoopMatchMinimumResponseFine：粗匹配和精匹配的响应阈值，与闭环中candidate数量相关。阈值过低会导致candidate迅速被填满，真正好的点还没找到。阈值过高会导致回环失败（一直找不到回环点），地图上出现重影。</li></ul></li></ol><h4 id="CPU-Usage"><a href="#CPU-Usage" class="headerlink" title="CPU Usage"></a>CPU Usage</h4><p>算法资源占用的主要压力来源：</p><ol><li>地图更新</li><li>回环检测</li><li>SPA优化</li></ol>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>leetcode79 单词搜索</title>
      <link href="/2018/06/04/leetcode79-%E5%8D%95%E8%AF%8D%E6%90%9C%E7%B4%A2/"/>
      <url>/2018/06/04/leetcode79-%E5%8D%95%E8%AF%8D%E6%90%9C%E7%B4%A2/</url>
      <content type="html"><![CDATA[<p>挂一道很猥琐的题，二维网格中搜索单词，同一单元格不能重复使用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">board =</span><br><span class="line">[</span><br><span class="line">  [&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;E&apos;],</span><br><span class="line">  [&apos;S&apos;,&apos;F&apos;,&apos;C&apos;,&apos;S&apos;],</span><br><span class="line">  [&apos;A&apos;,&apos;D&apos;,&apos;E&apos;,&apos;E&apos;]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">给定 word = &quot;ABCCED&quot;, 返回 true.</span><br><span class="line">给定 word = &quot;SEE&quot;, 返回 true.</span><br><span class="line">给定 word = &quot;ABCB&quot;, 返回 false.</span><br></pre></td></tr></table></figure><p>没啥好算法，就是DFS，但是坑在于visited的存储，python数组默认浅拷贝，递归传进去再回到上一层网格状态就变了，之前一贯的做法就是新开一块内存空间，传新的数组进去，然而这次超时了，因为测试用例的二维数组尺寸贼大，终于有机会正视这个问题，并获取正确的打开方式：</p><ul><li><p>尺寸贼大的二维数组，每次只需要修改一个值，重新划空间拷贝再修改时间复杂度瞬间增大O(m*n)倍，很明显传原来的数组进去比较合适。</p></li><li><p>但是深层递归会修改传进去的参数，因此<strong>在每次递归之前先创建一个tmp，记录修改行为</strong>，递归函数进行完以后，再根据记录恢复原来的参数，保证本层参数不变。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search_tail</span><span class="params">(board, word, h, w)</span>:</span></span><br><span class="line">    size = len(word)</span><br><span class="line">    char = word[<span class="number">0</span>]</span><br><span class="line">    height, width = len(board), len(board[<span class="number">0</span>])</span><br><span class="line">    exist = <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">if</span> h - <span class="number">1</span> &gt;= <span class="number">0</span> <span class="keyword">and</span> board[h<span class="number">-1</span>][w] == char:</span><br><span class="line">        <span class="keyword">if</span> size == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tmp = board[h<span class="number">-1</span>][w]</span><br><span class="line">            board[h<span class="number">-1</span>][w] = <span class="string">'INF'</span></span><br><span class="line">            exist = search_tail(board, word[<span class="number">1</span>:], h<span class="number">-1</span>, w)</span><br><span class="line">            <span class="keyword">if</span> exist:</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">            board[h<span class="number">-1</span>][w] = tmp</span><br><span class="line">    <span class="keyword">if</span> h + <span class="number">1</span> &lt; height <span class="keyword">and</span> board[h+<span class="number">1</span>][w] == char:</span><br><span class="line">        ...</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure></li><li><p>然后针对本题还有一个骚操作，很多人专门创建一个visited表来记录搜索路径，但是因为本题的二维数组限定存储字母，所以任意一个非字母都可以作为标志位，美滋滋又省下一个O(m*n)。</p><p><img src="/2018/06/04/leetcode79-单词搜索/188ms.png" alt="188ms"></p></li></ul>]]></content>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>kd-tree</title>
      <link href="/2018/06/01/kd-tree/"/>
      <url>/2018/06/01/kd-tree/</url>
      <content type="html"><![CDATA[<p><em>Reference：Bentley J L. Multidimensional Binary Search Trees Used for Associative Searching[J]. Communications of the Acm, 1975, 18(9):509-517.</em></p><p>前面更新<a href="https://amberzzzz.github.io/2018/05/05/ICP-Iterative-Closest-Points/">basic ICP</a>的时候留了一个坑——最近邻的求法。线性扫描？手动挥手。</p><h4 id="1-先说说距离吧"><a href="#1-先说说距离吧" class="headerlink" title="1. 先说说距离吧"></a>1. 先说说距离吧</h4><p><strong>1.1 欧式距离</strong></p><script type="math/tex; mode=display">d(x,y) = \sqrt{(x_1 - y_1)^2 + (x_2-y_2)^2 + ... + (x_n-y_n)^2} = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}</script><p><strong>1.2 标准化欧式距离</strong></p><p>首先将数据各维度分量都标准化到均值为0，方差为1，再求欧式距离：</p><script type="math/tex; mode=display">X_{stand} = \frac{X - \mu}{\sigma}</script><p>简单推导后可以发现，标准化欧式距离实际上就是一种加权欧式距离：</p><script type="math/tex; mode=display">d(x,y) = \sqrt{\sum_{i=1}^n (\frac{x_i-y_i}{s_i})^2}</script><p><strong>1.3 马氏距离（Mahalanobis Distance）</strong></p><script type="math/tex; mode=display">D(X_i,X_j) = \sqrt{(X_i - X_j)S^{-1}(X_i-X_j)}</script><p>其中$S$为协方差矩阵$Cov$：</p><script type="math/tex; mode=display">Cov(X,Y) = E\{[X-E(X)][Y-E(Y)]\}</script><p><strong>若协方差矩阵是单位阵（样本各维度之间独立同分布）</strong>，那公式就变成欧式距离了，若协方差矩阵是对角阵，那公式就变成标准化欧式距离了。</p><p><strong>1.4 相似度</strong></p><p>相似度也是距离的一种表征方式，距离越相近，相似度越高。</p><ul><li>欧式距离相似度：将欧式距离限定在0 1之间变化</li></ul><script type="math/tex; mode=display">相似度 = \frac{1}{1+欧式距离}</script><ul><li>余弦相似度：-1到1之间变化</li></ul><script type="math/tex; mode=display">cos\theta = \frac{A\cdot B}{||A||\ ||B||}</script><ul><li>皮尔逊相关系数：-1到1之间变化</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> numpy.corrcoef(A, B)</span><br></pre></td></tr></table></figure><h4 id="2-KD树"><a href="#2-KD树" class="headerlink" title="2. KD树"></a>2. KD树</h4><ul><li>k-NN算法</li><li>推荐系统</li><li>SIFT特征匹配</li><li>ICP迭代最近点</li><li>最近在做的M3RCM中的堆结构（这个有点牵强，因为不是二叉树）</li></ul><p>总之以上这些基于匹配／比较的目的而进行的数据库查找／图像检索，本质上都可以归结为通过距离函数在高维矢量之间的相似性检索问题。</p><p>一维向量有二分法查找，对应地高维空间有树形结构便于快速检索。<strong>利用树形结构可以省去对大部分数据点的搜索，从而减少检索的计算量</strong>。 </p><p>KD树是一种<strong>二叉树</strong>，通过不断地用垂直于某坐标轴的超平面将k维空间切分构造而成。 </p><p><img src="/2018/06/01/kd-tree/2d.png" alt="2d"></p><p><img src="/2018/06/01/kd-tree/3d.png" alt="3d"></p><p><strong>2.1 构造树</strong></p><p>递归创建节点：节点信息包含切分坐标轴和切分点，从而确定超平面，将当前空间切分为左右两个子空间，递归直到当前子空间内没有实例为止。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, point, axis)</span>:</span></span><br><span class="line">self.value = point</span><br><span class="line">self.axis = axis</span><br><span class="line">self.left = <span class="keyword">None</span></span><br><span class="line">self.right = <span class="keyword">None</span></span><br></pre></td></tr></table></figure><p>为了使得构造出的KD树尽可能平衡（高效分割空间）：</p><ul><li>选择坐标轴：简单点的方式是循环交替选择坐标轴，复杂点的做法是选择当前方差最大的轴作为切分轴。</li><li>选择切分点：取选定坐标轴上数据的中值作为切分点。</li></ul><p>注意：KD树的构造旨在高效分割空间，<strong>其叶子节点并非是最近邻搜索等应用场景的最优解</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kdTree</span><span class="params">(points, depth)</span>:</span></span><br><span class="line"><span class="keyword">if</span> len(points) == <span class="number">0</span>:</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">axis = depth % len(points[<span class="number">0</span>])</span><br><span class="line">points.sort(key=<span class="keyword">lambda</span> x: x[axis])</span><br><span class="line">cut_idx = centreValue(points)</span><br><span class="line">node = Node(points[cut_idx], axis)</span><br><span class="line">node.left = kdTree(points[:cut_idx], depth+<span class="number">1</span>)</span><br><span class="line">node.right = kdTree(points[cut_idx+<span class="number">1</span>:], depth+<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> node</span><br></pre></td></tr></table></figure><p>对于包含n个实例的k维数据来说，构造KD树的时间复杂度为O(k*n*log n)。</p><p><strong>2.2 新增节点</strong></p><p>递归实现：从根节点开始做比较，大于则插入左子树，小于则插入右子树。直到达到叶子节点，并创建新的叶子节点。</p><p><strong>2.3 删除节点</strong></p><p>将待删除的节点的所有子节点组成一个集合，重新构建KD子树，替换待删除节点。</p><p><strong>2.4 最近邻搜索</strong></p><p>搜索最近邻算法主要分为两部分：首先是深度优先遍历，直到遇到叶子节点，生成搜索路径。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">searchNearest</span><span class="params">(node, target)</span>:</span></span><br><span class="line">    <span class="comment"># input: node: root node of the tree</span></span><br><span class="line">    <span class="comment">#        target: list</span></span><br><span class="line">    <span class="comment"># output: nearest: list</span></span><br><span class="line">    <span class="comment">#       dist: distance between target and nearest</span></span><br><span class="line">    <span class="keyword">if</span> node == <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="comment"># 生成搜索路径</span></span><br><span class="line">    search_path = deque()</span><br><span class="line">    nearest = node</span><br><span class="line">    print(<span class="string">"search path: "</span>)</span><br><span class="line">    <span class="keyword">while</span> node:</span><br><span class="line">        print(node.value)</span><br><span class="line">        search_path.append(node)</span><br><span class="line">        <span class="comment"># if Dist(nearest.value, target) &gt; Dist(node.value, target):</span></span><br><span class="line">        <span class="comment"># nearest.value = node.value</span></span><br><span class="line">        <span class="comment"># minDist = Dist(node.value, target)</span></span><br><span class="line"></span><br><span class="line">        axis = node.axis</span><br><span class="line">        <span class="keyword">if</span> target[axis] &gt; node.value[axis]:</span><br><span class="line">            node = node.right</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            node = node.left</span><br><span class="line">    ...</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>然后是<strong>回溯查找</strong>，如果目标点和当前最近点构成的球形区域与其上溯节点相交，那么就有一种潜在的可能——上溯节点的另一个子空间的实例可能位于当前这个球形区域内，因此要进行一次判断。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">searchNearest</span><span class="params">(node, target)</span>:</span></span><br><span class="line">    <span class="comment"># input: node: root node of the tree</span></span><br><span class="line">    <span class="comment">#        target: list</span></span><br><span class="line">    <span class="comment"># output: nearest: list</span></span><br><span class="line">    <span class="comment">#       dist: distance between target and nearest</span></span><br><span class="line">    ...</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># 回溯</span></span><br><span class="line">    print(<span class="string">"\nsearch backwards: "</span>)</span><br><span class="line">    nearest = search_path.pop()</span><br><span class="line">    minDist = Dist(nearest.value, target)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> search_path:</span><br><span class="line">        node = search_path.pop()</span><br><span class="line">        print(node.value)</span><br><span class="line">        <span class="keyword">if</span> node.axis:</span><br><span class="line">            axis = node.axis</span><br><span class="line">            <span class="keyword">if</span> minDist &gt; Dist1(node.value[axis], target[axis]):</span><br><span class="line">                <span class="keyword">if</span> target[axis] &gt; node.value[axis]:</span><br><span class="line">                    search_path.append(node.left)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    search_path.append(node.right)</span><br><span class="line">        <span class="keyword">if</span> Dist(target, nearest.value) &gt; Dist(node.value, target):</span><br><span class="line">            nearest = node</span><br><span class="line">            minDist = Dist(node.value, target)</span><br><span class="line">    <span class="keyword">return</span> nearest.value, minDist</span><br></pre></td></tr></table></figure><p>两个参考点：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">samples = [(<span class="number">2</span>,<span class="number">3</span>), (<span class="number">5</span>,<span class="number">4</span>), (<span class="number">9</span>,<span class="number">6</span>), (<span class="number">4</span>,<span class="number">7</span>), (<span class="number">8</span>,<span class="number">1</span>), (<span class="number">7</span>,<span class="number">2</span>)]</span><br><span class="line">target = (<span class="number">2.1</span>, <span class="number">3.1</span>)</span><br><span class="line">target = (<span class="number">2</span>, <span class="number">4.5</span>)</span><br></pre></td></tr></table></figure><p>KD树搜索的核心就是：当查询点的邻域与分割超平面两侧空间交割时，需要查找另一侧子空间！！！算法平均复杂度O(N logN)。实际时间复杂度与实例分布情况有关，$t_{worst} = O(kN^{1-\frac{1}{k}})$，通常要求数据规模达到$N \geq 2^D$才能达到高效的搜索。</p><p><img src="/2018/06/01/kd-tree/search1.jpg" alt="s1"></p><p><img src="/2018/06/01/kd-tree/search2.jpg" alt="s2"></p><h4 id="3-改进算法：BBF算法"><a href="#3-改进算法：BBF算法" class="headerlink" title="3. 改进算法：BBF算法"></a>3. 改进算法：BBF算法</h4><p>回溯是由查询路径决定的，因此一种算法改进思路就是<strong>将查询路径上的结点排序</strong>，回溯检查总是从优先级最高的树节点开始——Best-Bin-First BBF算法。该算法能确保优先检索包含最邻近点可能性较高的空间。</p><ul><li>优先队列：优先级取决于它们离查询点的距离，距离越近，优先级越高，回溯的时候优先遍历。</li><li>对回溯可能需要路过的结点加入队列：切分的时候，把未选中的那个兄弟结点加入到队列中。</li></ul>]]></content>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Real-Time loop Loop Closure in 2D Lidar SLAM 论文笔记</title>
      <link href="/2018/05/27/Real-Time-loop-Loop-Closure-in-2D-Lidar-SLAM/"/>
      <url>/2018/05/27/Real-Time-loop-Loop-Closure-in-2D-Lidar-SLAM/</url>
      <content type="html"><![CDATA[<p>文章的核心思想在于解决loop closure问题。</p><p>全局地图由一系列的submap构成，每个submap则由一系列的位姿节点及对应的scan数据构成。</p><p>文章的重点在第四部分和第五部分：</p><ul><li>第四部分：local 2d slam，将scan与<strong>当前submap</strong>的匹配问题转化成一个最小二乘优化问题，由ceres来求解。参考文献《Many-to-Many Multi-Resolution Scan Matching 》</li><li>第五部分：closing loop，采用SPA进行后端loop closure，提出一种并行的scan与<strong>finished submaps</strong>匹配的方法BBS，大幅提高精度和速度。参考文献《Efficient Sparse Pose Adjustment for 2D Mapping(SPA)》、《Real-Time Correlative Scan Matching(BBS)》</li></ul>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>三维刚体运动 &amp; 李代数</title>
      <link href="/2018/05/12/%E4%B8%89%E7%BB%B4%E5%88%9A%E4%BD%93%E8%BF%90%E5%8A%A8-%E6%9D%8E%E4%BB%A3%E6%95%B0/"/>
      <url>/2018/05/12/%E4%B8%89%E7%BB%B4%E5%88%9A%E4%BD%93%E8%BF%90%E5%8A%A8-%E6%9D%8E%E4%BB%A3%E6%95%B0/</url>
      <content type="html"><![CDATA[<h4 id="0-向量"><a href="#0-向量" class="headerlink" title="0. 向量"></a>0. 向量</h4><ul><li><p>坐标：首先确定一个坐标系，也就确定了一组基$(e_1, e_2, e_3)$，那么向量$a$的坐标为：</p><script type="math/tex; mode=display">a = [e_1, e_2, e_3]\begin{bmatrix}a_1\\a_2\\a_3\end{bmatrix} = a_1e_1 + a_2e_2 + a_3e_3</script></li><li><p>内积：对向量$a, b \in R^3$，其内积为：</p><script type="math/tex; mode=display">a \cdot b = a^Tb = \Sigma a_ib_i = |a||b|cos<a,b></script><p>内积可以描述向量间的投影关系。</p></li><li><p>外积：</p><script type="math/tex; mode=display">a \times b = \begin{vmatrix}i & j & k\\a_1 & a_2 & a_3\\b_1 & b_2 & b_3\end{vmatrix}=\begin{bmatrix}0 & -a_3 & a_2\\a_3 & 0 & -a_1\\-a_2 & a_1 & 0\end{bmatrix}b=a^{\wedge}b</script><p>外积的方向垂直与这两个向量，大小为$|a||b|sin<a,b>$。</a,b></p><p>外积可以表示向量的旋转，向量$a$到$b$的<strong>旋转向量</strong>，外积的方向是旋转向量的方向，大小由夹角决定。</p></li></ul><h4 id="1-旋转矩阵R与变换矩阵T"><a href="#1-旋转矩阵R与变换矩阵T" class="headerlink" title="1. 旋转矩阵R与变换矩阵T"></a>1. 旋转矩阵R与变换矩阵T</h4><ul><li><p>通常设置固定的世界坐标系$O_w$和运动的相机坐标系$O_c$，相机运动是<strong>刚体运动</strong>，两个坐标系之间的变换称为<strong>欧式变换</strong>。</p></li><li><p>旋转矩阵$R$：可以描述相机的旋转</p><p><strong>坐标系旋转</strong>前后同一个向量的坐标变换关系：</p><script type="math/tex; mode=display">a =\begin{bmatrix}a_1\\a_2\\a_3\end{bmatrix}=\begin{bmatrix}e_1^T\\e_2^T\\e_3^T\end{bmatrix}\begin{bmatrix}e_1^{'} & e_2^{'}& e_3^{'}\end{bmatrix} \begin{bmatrix}a_1^{'}\\a_2^{'}\\a_3^{'}\end{bmatrix}=Ra^{'}</script><p>不难验证<strong>旋转矩阵是行列式为1的正交矩阵</strong>，因此可以把旋转矩阵的集合<strong>特殊正交群</strong>定义如下：</p><script type="math/tex; mode=display">SO(n) = \{R \in R^{n*n} | RR^T=I, det(R)=1\}</script><p>相反的旋转：</p><script type="math/tex; mode=display">a = Ra^{'}\\a^{'} = R^{-1}a = R^Ta</script></li><li><p>欧式变换：包括旋转和平移</p><script type="math/tex; mode=display">a^{'} = Ra + t</script></li><li><p>齐次坐标：射影几何的概念，每个分量同乘一个非零常数仍然表示同一个点：</p><script type="math/tex; mode=display">\tilde{x} = [x,y,z,w]^T=[x/w, y/w, z/w, 1]^T</script></li><li><p>齐次变换矩阵$T$：使得欧式变换仍旧保持线性关系：</p><script type="math/tex; mode=display">\begin{bmatrix}a^{'}\\1\end{bmatrix}=\begin{bmatrix}R & t\\0 &1\end{bmatrix}\begin{bmatrix}a\\1\end{bmatrix} =T\begin{bmatrix}a\\1\end{bmatrix}</script><p>变换矩阵的集合<strong>特殊欧式群</strong>：</p><script type="math/tex; mode=display">SE(3) = \left\{T=\begin{bmatrix}R & t\\0 & 1\end{bmatrix} \in R^{4*4} | R\in SO(3), t \in R^3\right\}</script></li></ul><h4 id="2-旋转向量-Axis-Angle"><a href="#2-旋转向量-Axis-Angle" class="headerlink" title="2. 旋转向量 Axis-Angle"></a>2. 旋转向量 Axis-Angle</h4><p>一个旋转只有3个自由度，旋转矩阵R要用9的参数来描述，显然是冗余的。一种紧凑的方式——任何旋转都可以用一个旋转轴$n$和一个旋转角$\theta$来刻画：</p><script type="math/tex; mode=display">R = cos\theta I + (1-cos\theta)nn^T + sin\theta n^{\wedge}\\\theta = arccos(\frac{tr(R)-1}{2})\\</script><p>旋转轴上的向量在旋转后不发生改变，因此有：</p><script type="math/tex; mode=display">Rn = n</script><p>转轴$n$是旋转矩阵$R$的特征值1对应的特征向量，可以由此来计算转轴$n$。</p><h4 id="3-欧拉角-rpy"><a href="#3-欧拉角-rpy" class="headerlink" title="3. 欧拉角 rpy"></a>3. 欧拉角 rpy</h4><p>把旋转分解到3个轴上，rpy角的旋转顺序是ZYX：</p><ul><li>首先绕物体的Z轴旋转，得到偏航角yaw</li><li>然后绕旋转之后的Y轴旋转，得到俯仰角pitch</li><li>绕旋转之后的X轴旋转，得到滚转角roll</li></ul><p>万向锁问题：在俯仰角为$\pm 90^{\circ}$时，第一次和第三次旋转使用同一根轴，丢了自由度——<strong>奇异性问题</strong>。</p><h4 id="4-四元数-q"><a href="#4-四元数-q" class="headerlink" title="4. 四元数 q"></a>4. 四元数 q</h4><p>四元数是一种扩展的负数，由一个实部和三个虚部组成，可以把三个虚部脑补成空间中的三根轴：</p><script type="math/tex; mode=display">q = q_0 + q_1i + q_2j + q_3k\\\left\{\begin{split}& i^2 = j^2=k^2=-1\\& ij = k, ji = -k\\& jk = i, kj = -i\\& ki=j, ik=-j\end{split}\right.</script><ul><li><p>乘以$i$对应着绕$i$轴旋转$180^{\circ}$</p></li><li><p>任意的旋转可以由两个互为相反数的四元数表示</p></li><li><p>与旋转向量$n = [n_x, n_y, n_z]^T, \theta$转换关系：</p><script type="math/tex; mode=display">q = [cos\frac{\theta}{2}, n_xsin\frac{\theta}{2}, n_ysin\frac{\theta}{2}, n_zsin\frac{\theta}{2}]^T\\\left\{\begin{split}& \theta = 2arccos q_0\\& [n_x, n_y, n_z]^T = [q_1, q_2, q_3]^T/sin\frac{\theta}{2}\end{split}\right.</script></li><li><p>与旋转矩阵$R$的关系：</p><script type="math/tex; mode=display">R = \begin{bmatrix}1-2q_2^2 - 2q_3^2 & 2q_1q_2-2q_0q_3 & 2q_1q_3+2q_0q_2\\ 2q_1q_2+2q_0q_3  & 1-2q_1^2 - 2q_3^2 &  2q_2q_3-2q_0q_1\\ 2q_1q_3-2q_0q_2 &  2q_2q_3+2q_0q_1 & 1-2q_1^2 - 2q_2^2 \end{bmatrix}\\q_0 = \frac{\sqrt{tr(R)+1}}{2}, q_1 = \frac{R_{23}-R_{32}}{4q_0}, q_2 = \frac{R_{31}-R_{13}}{4q_0}, q_3 = \frac{R_{12}-R_{21}}{4q_0}</script></li><li><p>表示旋转：</p><p>空间中点$p = [x, y,z]^T\in R^3$，已知旋转轴角$n,\theta$，旋转之后点坐标变为$p^{‘}$，如果用旋转矩阵描述：</p><script type="math/tex; mode=display">p^{'} = Rp</script><p>四元数$q = [cos\frac{\theta}{2}, nsin\frac{\theta}{2}]$，那么旋转后的点$p^{‘}$可以表示为：</p><script type="math/tex; mode=display">p^{'} = qpq^{-1}</script></li></ul><h4 id="5-李群"><a href="#5-李群" class="headerlink" title="5. 李群"></a>5. 李群</h4><p>上面提到了旋转矩阵构成的特殊正交群$SO(3)$和由变换矩阵构成的特殊欧式群$SE(3)$：</p><script type="math/tex; mode=display">SO(n) = \left\{R \in R^{n*n} | RR^T=I, det(R)=1\right\}\\SE(3) = \left\{T=\begin{bmatrix}R & t\\0 & 1\end{bmatrix} \in R^{4*4} | R\in SO(3), t \in R^3\right\}</script><ul><li>$SO(n)$和$SE(n)$对<strong>加法不封闭，对乘法是封闭</strong>的。</li><li>群是一种集合$A$加上一种运算$\ \cdot \ $的代数结构，记作$G = (A, \ \cdot \ )$，群内元素满足封闭性、结合律、幺元、可逆四个条件。</li><li>李群是指具有连续性质的群。刚体在空间中能够连续地运动，因此$SO(n)$和$SE(n)$是李群。</li></ul><h4 id="6-李代数"><a href="#6-李代数" class="headerlink" title="6. 李代数"></a>6. 李代数</h4><p><strong>6.1 引入</strong></p><p>对任意旋转矩阵$R$，都满足$RR^T=I$。把它写成关于时间的函数$R(t)$有：</p><script type="math/tex; mode=display">R(t)R(t)^T = I</script><p>对等式两边求导：</p><script type="math/tex; mode=display">\dot R(t)R(t)^T + R(t)\dot R(t)^T=0\\\dot R(t)R(t)^T =-\big(\dot R(t)R(t)^T \big)^T</script><p>可以看出$\dot R(t)R(t)^T $是一个<strong>反对称阵</strong>，对任意一个反对称阵，都可以找到一个与之对应的<strong>向量</strong>：</p><script type="math/tex; mode=display">a^{\wedge} = A, A^{\vee}=a</script><p>于是可以找到一个三维向量$\phi(t) \in R^3$与之对应：</p><script type="math/tex; mode=display">\dot R(t)R(t)^T  = \phi(t)^{\wedge}\\\dot R(t) = \phi(t)^{\wedge}R(t)</script><p>可以看到，<strong>每对旋转矩阵求一次导数，只需左乘一个反对称阵$\phi(t)^{\wedge}$即可</strong>。</p><p>求解上面的微分方程，可以得到$R(t) = exp(\phi^{\wedge}t)$。也就是说$\phi$描述了$R$在局部的导数关系。</p><p><strong>6.2 李代数</strong></p><ul><li><p>每个李群都有与之对应的李代数。李代数描述了李群的局部性质。</p></li><li><p>李代数由一个集合$V$，一个数域$F$，和一个二元运算<strong>李括号</strong>$[,]$组成，记作$( V, F, [,])$。李代数的元素满足封闭性、双线性、自反性、雅可比等价四条性质。</p></li><li><p>上一节的$\phi$就是$SO(3)$对应的李代数$so(3)$，两者的关系由<strong>指数映射</strong>给定：</p><script type="math/tex; mode=display">R = exp(\phi^{\wedge})\\so(3) = \left\{ \phi \in R^3, \Phi = \phi^{\wedge} \in R^{3*3}\right\}</script></li><li><p>$SE(3)$对应的李代数$se(3)$位于$R^6$空间中：</p><script type="math/tex; mode=display">se(3) = \left\{ \xi = \begin{bmatrix}\rho\\\phi\end{bmatrix}\in R^6, \rho \in so(3), \xi^{\wedge} =\begin{bmatrix}\phi^{\wedge} & \rho\\0^T & 0\end{bmatrix} \in R^{4*4}\right\}</script></li><li><p>指数映射</p><p>由于$\phi$是一个三维向量，因此可以写作$\theta a$的形式，$a$是一个单位向量，因此具有以下性质：</p><script type="math/tex; mode=display">a^{\wedge}a^{\wedge} = aa^T-I\\a^{\wedge}a^{\wedge}a^{\wedge} = -a^{\wedge}</script><p>对$so(3)$李代数的指数映射做泰勒展开，可以得到：</p><script type="math/tex; mode=display">\begin{split}&R= exp(\phi^{\wedge}) = exp(\theta a^{\wedge})=\Sigma_{n=0}^{\infty}\frac{1}{n!} (\theta a^{\wedge})^n\\& =cos\theta I + (1-cos\theta)aa^T+sin\theta a^{\wedge}\end{split}</script><p>可以看到$so(3)$实际上就是旋转向量组成的空间，指数映射即是罗德里格斯公式。</p><p>指数映射是一个<strong>满射</strong>，每个$SO(3)$中的元素，都可以找到至少一个$so(3)$元素与之对应（$\theta + 2k\pi$）。</p><p>$se(3)$上的指数映射为：</p><script type="math/tex; mode=display">T = exp(\xi^{\wedge}) = \begin{bmatrix}R & J\rho\\0 &1\end{bmatrix}\\J = \frac{sin\theta}{\theta}I + (1-\frac{sin\theta}{\theta})aa^T + \frac{1-cos\theta}{\theta}a^{\wedge}</script></li></ul><p><strong>6.3 李代数求导 </strong></p><ul><li><p>两个李代数指数映射乘积的完整形式由BCH公式给出：</p><script type="math/tex; mode=display">ln(exp(A)exp(B)) = A+B + \frac{1}{2}[A, B] + \frac{1}{12}[A,[A,B]] + ...</script></li><li><p>对$ln(exp(\phi_1^{\wedge})exp(\phi_2^{\wedge}))^{\vee}$，当$\phi_1$或$\phi_2$为小量时，BCH公式给出了线性近似表达：</p><script type="math/tex; mode=display">ln(exp(\phi_1^{\wedge})exp(\phi_2^{\wedge}))^{\vee} = \left\{\begin{split}J_l(\phi_2)^{-1}\phi_1 + \phi_2\ \ \ \ \ \ \phi_1为小量\\J_r(\phi_1)^{-1}\phi_2 + \phi_1\ \ \ \ \ \ \phi_2为小量\\\end{split}\right.</script><p>BCH近似雅可比$J_l$就是上一节的$J$：</p><script type="math/tex; mode=display">J_l = J = \frac{sin\theta}{\theta}I + (1-\frac{sin\theta}{\theta})aa^T + \frac{1-cos\theta}{\theta}a^{\wedge}\\J_l^{-1} = \frac{\theta}{2}cot\frac{\theta}{2}I + (1-\frac{\theta}{2}cot\frac{\theta}{2})aa^T - \frac{\theta}{2}a^{\wedge}\\J_r(\phi) = J_l(-\phi)</script><p>由以上公式说明了<strong>李群乘法</strong>和<strong>李代数加法</strong>的近似转换关系。</p></li><li><p>在$SO(3)、SE(3)$上没有良好定义的加法，而李代数由向量组成，有良好的加法运算。因此在计算位姿的导数时，通常使用李代数解决，李代数求导的两种思路：</p><ul><li>李代数求导$\delta \phi$：用李代数表示姿态，然后转化成对李代数求导$\phi + \delta \phi$</li></ul><script type="math/tex; mode=display">\begin{split}&\frac{\partial (Rp)}{\partial R} = \frac{\partial(exp(\phi^{\wedge})p)}{\partial \phi}\\&= lim \frac{exp((\phi+\delta\phi)^{\wedge})p-exp(\phi^{\wedge})p}{\partial \phi}\\&=-(Rp)^{\wedge}J_l\end{split}</script><ul><li>扰动模型$\Delta R$：对$R$进行扰动，然后对扰动求导$\Delta R R$</li></ul><script type="math/tex; mode=display">\begin{split}&\frac{\partial (Rp)}{\partial R} = lim \frac{exp(\varphi^{\wedge})exp(\phi^{\wedge})p-exp(\phi^{\wedge})p}{\partial \varphi}= -(Rp)^{\wedge}\\& \frac{\partial Tp}{\partial \delta \xi} = \begin{bmatrix}I & -(Rp+t)^{\wedge}\\0 & 0\end{bmatrix} = (Tp)^{\odot}\end{split}</script></li></ul>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>c++ for record</title>
      <link href="/2018/05/10/c-for-record/"/>
      <url>/2018/05/10/c-for-record/</url>
      <content type="html"><![CDATA[<p>最近开始着手写slam代码，看一些常用库源码的时候发现各种力不从心，一些c++11的骚操作竟然没见过，是时候完整撸一发c++ primer祭天了。</p><ol><li><p>iostream</p><ul><li>标准输入：cin</li><li>标准输出：cout、cerr、clog</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> v1=<span class="number">0</span>, v2=<span class="number">0</span>;</span><br><span class="line"><span class="built_in">cin</span> &gt;&gt; v1 &gt;&gt; v2;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; v1+v2 &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cerr</span> &lt;&lt; <span class="string">"This is nonsense."</span> &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure><ul><li>&lt;&lt; 和 &gt;&gt; 的方向表示了数据流的走向，也就是赋值的方向。cerr用来输出错误信息。</li></ul></li><li><p>控制流</p><ul><li><p>while：每次执行循环之前先检查<strong>循环条件</strong></p></li><li><p>do while：先执行循环体后检查条件</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (condition)</span><br><span class="line">    statement</span><br><span class="line">   </span><br><span class="line"><span class="keyword">do</span> </span><br><span class="line">    statement</span><br><span class="line"><span class="keyword">while</span> (condition);</span><br></pre></td></tr></table></figure></li><li><p>for：每次执行循环之前先检查<strong>循环条件</strong>，执行循环之后执行<strong>表达式</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (init-statement; condition; expression)</span><br><span class="line">&#123;</span><br><span class="line">    statemnt</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 范围for语句</span></span><br><span class="line"><span class="keyword">for</span> (declaration : expression)</span><br><span class="line">statement</span><br></pre></td></tr></table></figure></li><li><p>switch：</p><ul><li>case label：case标签必须是<strong>整形常量</strong>表达式</li><li>如果某个case标签匹配成功，会<strong>往后顺序执行所有case分支</strong>，直到结尾或者遇到break</li><li>default标签</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">switch</span>(ch)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">'a'</span>: <span class="keyword">case</span> <span class="string">'b'</span>: <span class="keyword">case</span> <span class="string">'c'</span>:</span><br><span class="line">    ++cnt;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>break：负责终止离他最近的while、do while、for或switch语句。</p></li><li><p>continue：负责终止离他最近的while、do while、for循环的当前迭代。</p></li><li><p>goto：无条件跳转到同一函数内的某个<strong>带标签语句</strong>。</p><blockquote><p>labeled statement: <code>label: statement</code></p></blockquote></li><li><p>异常</p><ul><li><p>throw：<strong>引发异常</strong>，后面紧随一个异常类型，终止当前函数，将控制权转移给能够处理该异常的代码。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdexcept&gt;</span></span></span><br><span class="line"><span class="comment">// runtime_error 标准库异常类型</span></span><br><span class="line"><span class="keyword">throw</span> runtime_error(<span class="string">"Data must refer to same name"</span>);</span><br></pre></td></tr></table></figure></li><li><p>try：<strong>处理异常</strong>，后面紧随一套catch子句用来处理异常。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>&#123;</span><br><span class="line">    program statements</span><br><span class="line">&#125; <span class="keyword">catch</span> (exception-declaration) &#123;</span><br><span class="line">    handler-statements</span><br><span class="line">&#125; <span class="keyword">catch</span> (exception-declaration) &#123;</span><br><span class="line">    handler-statements</span><br><span class="line">&#125; ...</span><br></pre></td></tr></table></figure><p>try语句块内声明的变量在块外无法访问，即使是catch语句。</p><p>catch一旦完成，程序跳转到最后一个catch子句之后的语句。</p></li></ul></li></ul></li><li><p>类</p><p>类型 &amp; 对象（实例），变量 &amp; 行为（方法）。</p><ul><li><p>存在类内默认初始化</p></li><li><p>类通常被定义在头文件中，头文件名字应与类的名字保持一致</p><blockquote><p>头文件通常包含只能被定义一次的实体，如类、const等。</p><p>头文件保护符#ifndef系列，创建<strong>预处理变量</strong>，防止多次包含。</p></blockquote></li><li><p>构造函数初始值列表：冒号以及冒号和花括号之间的代码</p><blockquote><p>列表只说明用于初始化成员的值，而不限定初始化的具体顺序。</p><p>成员的初始化顺序与它们在类定义中的出现顺序一致。</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 为类成员初始化</span></span><br><span class="line">Sales_data(<span class="keyword">const</span> <span class="built_in">string</span> &amp;s, <span class="keyword">unsigned</span> n, <span class="keyword">double</span> p)</span><br><span class="line"> : bookNo(s), units_sold(n), revenue(p*n) &#123;&#125; </span><br><span class="line"></span><br><span class="line"><span class="comment">// 区别于赋值</span></span><br><span class="line">Sales_data(<span class="keyword">const</span> <span class="built_in">string</span> &amp;s, <span class="keyword">unsigned</span> n, <span class="keyword">double</span> p)</span><br><span class="line">&#123;</span><br><span class="line">    bookNo = s;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>接口与封装：</p><ul><li>定义在private说明符之后的成员只能被类内成员函数访问，封装了类的实现细节。</li><li>定义在public说明符之后的成员可以在整个程序内被访问，定义类的接口。</li></ul></li><li><p>class和struct的区别：成员访问权限</p><ul><li>struct：定义在第一个说明符之前的成员是public</li><li>class：定义在第一个说明符之前的成员是private</li></ul></li><li><p>友元：允许其他类或函数访问它的非公有成员，在类内添加以friend关键字开始的友元声明。</p><blockquote><p>友元的声明仅仅指定了访问权限，而非一个通常意义上的函数声明。</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sales_data</span> &#123;</span></span><br><span class="line"><span class="comment">// 友元声明</span></span><br><span class="line"><span class="function"><span class="keyword">friend</span> Sales_data <span class="title">add</span><span class="params">(<span class="keyword">const</span> Sales_data&amp;, <span class="keyword">const</span> Sales_data&amp;)</span></span>;</span><br><span class="line"><span class="comment">// 非公有成员</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"><span class="built_in">string</span> bookNo;</span><br><span class="line"><span class="keyword">double</span> revenue = <span class="number">0.0</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></li><li><p>静态成员static：与类本身相关联，不属于任何一个对象，因此不是在创建类对象的时候被定义的，因此通常在类的外部定义和初始化，在类内部添加以static关键字开始的静态成员声明。</p></li></ul></li><li><p>内置类型</p><ul><li>内存中的一个地址对应一个字节</li><li>unsigned类型表示<strong>大于等于0</strong>的数（$[0, 2^{n}-1]$），被赋给一个超出表示范围的数时，<strong>自动取余</strong>，作为循环条件时当心进入<strong>无限循环</strong></li><li>signed类型正负值范围平衡（$[-2^{n-1}, 2^{n-1}-1]$），被赋给一个超出表示范围的数时，<strong>结果未定义</strong></li><li>字符型char，单引号，一个字节</li><li>字符串型，双引号，常量字符数组，结尾隐含空字符 ‘\0’</li><li>nullptr = 0（传统NULL包含在cstdlib头文件内）</li></ul></li><li><p>变量</p><ul><li><p><strong>列表初始化</strong>，花括号</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 拷贝初始化</span></span><br><span class="line"><span class="keyword">int</span> x=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> x=&#123;<span class="number">0</span>&#125;;</span><br><span class="line"><span class="comment">// 直接初始化</span></span><br><span class="line"><span class="keyword">int</span> x&#123;<span class="number">0</span>&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">x</span><span class="params">(<span class="number">0</span>)</span></span>;</span><br></pre></td></tr></table></figure></li><li><p>变量声明extern，源于<strong>分离式编译</strong>机制，一个变量只能被定义一次，可以声明多次</p></li><li><p>作用域，嵌套作用域 &amp; 内部重定义</p></li></ul></li><li><p>复合类型</p><ul><li><p>引用，<code>typename &amp;declaration</code>，浅拷贝，绑定一个对象，<strong>引用不是对象</strong></p></li><li><p>指针，<code>typename *declaration</code>，存放对象地址</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a;</span><br><span class="line"><span class="keyword">int</span> *p, *q=a;</span><br><span class="line">p = &amp;a;</span><br><span class="line">p = q;</span><br></pre></td></tr></table></figure></li><li><p>取地址符&amp;</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> *p = a;</span><br><span class="line"><span class="keyword">int</span> *p = &amp;a;</span><br><span class="line"><span class="comment">// a---&gt;对象 &amp;a---&gt;地址</span></span><br></pre></td></tr></table></figure></li><li><p>解引用符*</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a;</span><br><span class="line"><span class="keyword">int</span> *p;</span><br><span class="line">*p ---&gt; undefined</span><br><span class="line">p = &amp;a;</span><br><span class="line">*p = <span class="number">10</span>;</span><br><span class="line"><span class="comment">// p---&gt;指针 *p---&gt;对象</span></span><br></pre></td></tr></table></figure></li><li><p>void* 指针，可以指向任意类型的对象，但是不能进行对象操作</p></li></ul></li><li><p>const限定符</p><ul><li><p>参与编译预处理</p></li><li><p>要实现多个文件共享，必须在const变量<strong>定义</strong>之前加上extern关键字</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// define</span></span><br><span class="line"><span class="keyword">extern</span> <span class="keyword">const</span> <span class="keyword">int</span> bufferSize = fcn();</span><br><span class="line"><span class="comment">// declare</span></span><br><span class="line"><span class="keyword">extern</span> <span class="keyword">const</span> <span class="keyword">int</span> bufferSize;</span><br></pre></td></tr></table></figure></li><li><p>允许任意表达式作为初始值（允许隐式类型转换）</p></li><li><p>常量引用，允许非常量赋值，实际引用一个内存中的“临时值”</p></li><li><p>指向常量的指针，允许非常量赋值，但是不能通过该指针修改对象</p></li><li><p>常量指针，指针始终指向同一个对象</p></li><li><p>常量表达式constexpr，表达式在编译过程中就能得到计算结果</p></li></ul></li><li><p>处理类型</p><ul><li><p>类型别名typedef &amp; using</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 传统</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">double</span> base;</span><br><span class="line"><span class="keyword">typedef</span> base *p;    <span class="comment">// p是double指针</span></span><br><span class="line">base a;</span><br><span class="line">p p1=&amp;a;</span><br><span class="line"><span class="comment">// c++11</span></span><br><span class="line"><span class="keyword">using</span> base = <span class="keyword">double</span>;</span><br></pre></td></tr></table></figure></li><li><p>auto类型说明符，让编译器分析表达式所属类型并为变量赋值</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 一条类型声明语句中所有变量的类型必须保持一致</span></span><br><span class="line"><span class="keyword">auto</span> i=<span class="number">0</span>, *p=&amp;i;</span><br></pre></td></tr></table></figure></li><li><p>decltype类型指示符，仅分析表达式返回类型，不做赋值（因此不做实际计算）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">decltype</span>(f()) a=x;</span><br></pre></td></tr></table></figure></li></ul></li></ol><ol><li><p>string</p><ul><li><p>读取，<code>&gt;&gt;</code>不读取空白，遇到空白符停止，<code>getline</code>保留空白符，遇到换行符停止。</p></li><li><p>字符串字面值不是string对象，而是<strong>C风格字符串</strong>，<code>c_str()</code>成员函数能够将string对象转化成C风格字符串</p></li><li><p>遍历，<strong>范围for语句</strong>，每次迭代declare的变量会被初始化为expression的下一个元素</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (declaration : expression)</span><br><span class="line">statement</span><br><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">str</span><span class="params">(<span class="string">"some string"</span>)</span></span>;</span><br><span class="line"><span class="comment">// 赋值</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> c: str)</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; c &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="comment">// 引用</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> &amp;c: str)</span><br><span class="line">    c = <span class="built_in">toupper</span>(c);</span><br></pre></td></tr></table></figure></li><li><p><code>size()</code>返回的类型是<code>string::size_type</code>，通常用auto</p></li></ul></li><li><p>vector</p><ul><li>类模版，<strong>相同类型</strong>对象的集合，声明时必须提供元素类型<code>vector&lt;int&gt;</code></li><li>添加元素<code>push_back()</code></li></ul></li><li><p>迭代器</p><ul><li>所有标准库容器都支持迭代器，只有少数支持下标访问</li><li><code>begin()</code>返回指向第一个元素的迭代器，<code>end()</code>返回<strong>尾后元素</strong>的迭代器</li><li><code>cbegin()</code>和<code>cend()</code>操作类似，返回值是const_iterator，不能修改对象</li><li>迭代器的类型是<code>container::iterator</code>和<code>container::const_iterator</code>，通常用auto</li><li>解引用迭代器得到对象</li><li>箭头运算符<code>-&gt;</code>，结合解引用+成员访问两个操作</li><li>迭代器失效：容器改变容量</li></ul></li><li><p>数组</p><ul><li>大小固定，编译的时候维度应该已知，因此必须是常量表达式</li><li>不能用做拷贝和赋值</li></ul></li><li><p>表达式</p><ul><li><p>左值和右值</p><p>​    C语言中，<strong>左值</strong>指的是既能出现在等号左边也能出现在等号右边的变量或表达式，通常来说就是有名字的变量，而<strong>右值</strong>只能出现在等号右侧，通常就是一些没有名字也取不到地址的中间结果。</p><p>继承到C++中归纳来讲就是：当一个对象被用作右值的时候，用的是对象的值（内容），当被用作左值的时候，用的是对象的身份（在内存中的位置）。</p></li><li><p>求值顺序</p><blockquote><p>有四种运算符明确规定了求值顺序，逻辑与（&amp;&amp;）、逻辑或（||）、条件（?:）、逗号（,）运算符。</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; i &lt;&lt; ++i &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure></li><li><p>前置版本和后置版本的递增递减</p><p>用于复合运算中时，</p><p>前置版本首先修改对象，然后将对象本身作为左值返回。</p><p>后置版本将对象<strong>原始值的副本</strong>作为右值返回。</p></li><li><p>位运算</p><ul><li>整形提升，char8-&gt;int32</li><li>添0，越界丢弃</li></ul></li><li><p>逗号运算符：含有两个运算对象，首先对左表达式求值，然后将求值结果丢弃掉，最右边的表达式的值将作为整个逗号表达式的值。本质上，逗号的作用是导致一些列运算被顺序执行。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 分别对逗号表达式内对象赋值，然后返回最右cnt的值</span></span><br><span class="line">var = (count=<span class="number">19</span>, incr=<span class="number">10</span>, cnt++)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>函数</p></li></ol><ul><li><p>局部静态对象static：首次调用时被初始化，直到程序终止才被销毁。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 只初始化一次，函数调用结束以后这个值仍有效</span></span><br><span class="line">    <span class="keyword">static</span> cnt = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">return</span> ++cnt;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>参数传递：如果形参被声明为引用类型，它将<strong>绑定</strong>到对应的实参上（传引用调用），否则将实参的值<strong>拷贝</strong>后赋给形参（传值调用）。</p></li><li><p>含有可变形参的函数</p><ul><li><p>所有实参类型相同，可以使用initializer_list模版类型的形参，列表中元素是const。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">initializer_list</span>&lt;T&gt; lst;</span><br><span class="line"><span class="built_in">initializer_list</span>&lt;T&gt; lst&#123;a, b, c, ...&#125;;</span><br></pre></td></tr></table></figure></li><li><p>编写可变参数<strong>模版</strong></p></li><li><p>省略符形参：对应的实参无需类型检查</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 带部分形参类型</span><br><span class="line">void foo(parm_list, ...);</span><br><span class="line"></span><br><span class="line">void foo(...);</span><br></pre></td></tr></table></figure></li></ul></li><li><p>内联函数incline：避免函数调用开销</p></li><li><p>调试帮助</p><ul><li>NDEBUG预处理变量：用于关闭调试状态，assert将跳过不执行。</li><li>assert (expr) 预处理宏：如果表达式为假，assert输出信息并终止程序。</li></ul><blockquote><p>预处理名字由预处理而非编译器管理，因此可以直接使用名字而无须提供using声明。</p></blockquote></li></ul><ol><li>static_cast和dynamic_cast强制类型转换<ul><li>static_cast \<type-id> (expression)：暴力类型转换，不运行类型检查。</type-id></li><li>dynamic_cast\<type-id> (expression)：运行类型检查，下行转换安全。</type-id></li></ul></li><li>new &amp; delete：new [] 要和 delete []对应上。</li><li>c++的oop特性（private public…）只在编译时刻有意义。同一类的对象可以互相访问私有成员。</li><li>firend：注意方向是give acess to，授权friend访问自己的private。编译时刻检查。</li><li>composition：组合，用一系列对象构造对象。</li><li>inheritance：继承，用一些类来构造新的类。</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span> :</span> <span class="keyword">public</span> A&#123;</span><br><span class="line">    ....</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>构造：子类构造的时候要先构造父类，析构的时候反过来，先析构子类。</p><p>重名：name hiding，special for c++。</p><ol><li>​    protected：designed for sub class。子类可以直接访问。其他类看不到。</li><li>overload：参数表必须不同，否则编译器无法识别。</li><li>default argument：defaults must be added from right to left。must be declared in .h files。发生在编译时刻。</li><li>inline：不用真正调用函数，而是直接插入汇编代码段。tradeoff between space and time consuming。区别于宏，宏是没有类型检查的。</li><li><p>const</p><ul><li>declare a variable：是变量，而不是常数</li><li></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> basic </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>cmake for record</title>
      <link href="/2018/05/08/cmake/"/>
      <url>/2018/05/08/cmake/</url>
      <content type="html"><![CDATA[<h4 id="0-变量"><a href="#0-变量" class="headerlink" title="0. 变量"></a>0. 变量</h4><p>变量使用 ${ } 的方式取值，但是在if控制语句中直接使用变量名。</p><h4 id="1-project"><a href="#1-project" class="headerlink" title="1. project"></a>1. project</h4><p>project ( project_name [CXX] [C] [Java] )</p><p>用来指定工程名称和工程语言（可省略），指令隐式定义了projectname_BINARY_DIR和projectname_SOURCE_DIR两个变量（写在cmake_cache里面），指的是<strong>编译发生</strong>的当前目录。</p><h4 id="2-set"><a href="#2-set" class="headerlink" title="2. set"></a>2. set</h4><p>set ( VAR  [VALUE] )</p><p>用来显式定义变量，如set (SRC_LIST main.c t1.c t2.c) 。（竟然不用套括号？）</p><h4 id="3-message"><a href="#3-message" class="headerlink" title="3. message"></a>3. message</h4><p>message ( [SEND_ERROR | STATUS | FATAL_ERROR]  “message to display”  VAR ) </p><p>用来向终端输出用户定义的信息。</p><h4 id="4-add-executable"><a href="#4-add-executable" class="headerlink" title="4. add_executable"></a>4. add_executable</h4><p>add_executable ( executable_filename  [source_filename] )</p><p>生成名字为executable_filename的可执行文件，相关的源文件 [source_filename] 可以是一个源文件列表。</p><h4 id="5-清理构建结果"><a href="#5-清理构建结果" class="headerlink" title="5. 清理构建结果"></a>5. 清理构建结果</h4><p>make clean</p><p>对构建出的可执行文件进行清理。</p><h4 id="6-外部构建"><a href="#6-外部构建" class="headerlink" title="6. 外部构建"></a>6. 外部构建</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br></pre></td></tr></table></figure><p>所有编译动作发生在编译目录，对原有工程没有任何影响。</p><h4 id="7-add-subdirectory"><a href="#7-add-subdirectory" class="headerlink" title="7. add_subdirectory"></a>7. add_subdirectory</h4><p>add_subdirectory ( source_dir  [binary_dir]   [EXCLUDE_FROM_ALL] )</p><p>向当前工程目录添加存放源文件的子目录source_dir，并指定存放中间二进制文件和目标二进制文件的位置binary_dir。指令隐式修改 EXECUTABLE_OUTPUT_PATH 和 LIBRARY_OUTPUT_PATH 两个变量。</p><h4 id="8-更加像一个工程"><a href="#8-更加像一个工程" class="headerlink" title="8. 更加像一个工程"></a>8. 更加像一个工程</h4><ul><li>创建工程根目录，创建CMakeLists.txt。</li></ul><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定最低编译版本</span></span><br><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.7</span>)</span><br><span class="line"><span class="comment"># 指定工程名字</span></span><br><span class="line"><span class="keyword">PROJECT</span>(HELLO)</span><br><span class="line"><span class="comment"># 测试类打印信息</span></span><br><span class="line"><span class="keyword">MESSAGE</span>(STATUS <span class="string">"This is BINARY dir "</span> <span class="variable">$&#123;HELLO_BINARY_DIR&#125;</span>)</span><br><span class="line"><span class="keyword">MESSAGE</span>(STATUS <span class="string">"This is SOURCE dir "</span> <span class="variable">$&#123;HELLO_SOURCE_DIR&#125;</span>)</span><br></pre></td></tr></table></figure><ul><li>添加子目录src，用来存放源文件，为子目录创建CMakeLists.txt。</li></ul><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在根目录CMakeLists.txt中添加子目录声明</span></span><br><span class="line"><span class="keyword">add_subdirectory</span>(src bin)</span><br><span class="line"><span class="comment"># 编译产生的中间文件以及目标文件将保存在编译文件夹的bin子目录下</span></span><br></pre></td></tr></table></figure><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编写当前子目录的CMakeLists.txt</span></span><br><span class="line"><span class="keyword">add_executable</span>(hello main.c)</span><br><span class="line"><span class="comment"># 修改最终生成的可执行文件以及库的路径，这两个指令要追随对应的add_executable()和add_library()指令</span></span><br><span class="line"><span class="keyword">set</span>(EXECUTABLE_OUTPUT_PATH <span class="variable">$&#123;PROJECT_BINARY_PATH&#125;</span>/bin)</span><br><span class="line"><span class="keyword">set</span>(LIBRARY_OUTPUT_PATH <span class="variable">$&#123;PROJECT_BINARY_DIR&#125;</span>/lib)</span><br></pre></td></tr></table></figure><ul><li>添加子目录build，作为外部编译文件夹（ ${PROJECT_BINARY_DIR} ），存放编译的过程和目标文件。    </li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br></pre></td></tr></table></figure><ul><li>添加子目录doc，用来存放工程文档hello.txt。</li><li>添加文本文件README，COPYRIGHT。</li><li>添加runhello.sh脚本，用来调用可执行文件hello。</li></ul><h4 id="9-打包安装"><a href="#9-打包安装" class="headerlink" title="9. 打包安装"></a>9. 打包安装</h4><ul><li>在根目录的CMakeList.txt中添加安装信息</li></ul><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装COPYRIGHT/README到&lt;prefix&gt;/share/doc/cmake/t2</span></span><br><span class="line"><span class="keyword">INSTALL</span>(FILES COPYRIGHT README DESTINATION share/doc/cmake/t2)</span><br><span class="line"><span class="comment"># 安装runhello.sh到&lt;prefix&gt;/bin</span></span><br><span class="line"><span class="keyword">INSTALL</span>(PROGRAMS runhello.sh DESTINATION bin)</span><br><span class="line"><span class="comment"># 安装工程文档到&lt;prefix&gt;/share/doc/cmake/t2</span></span><br><span class="line"><span class="keyword">INSTALL</span>(DIRECTORY doc/ DESTINATION share/doc/cmake/t2)</span><br></pre></td></tr></table></figure><ul><li>在子目录的CMakeList.txt中添加安装信息</li></ul><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装脚本要调用的可执行文件hello到&lt;prefix&gt;/bin，</span></span><br><span class="line"><span class="comment"># 注意install(targets)指令也要追随对应add_executable()和add_library()指令的路径</span></span><br><span class="line"><span class="keyword">INSTALL</span>(TARGETS hello</span><br><span class="line">RUNTIME DESTINATION bin)</span><br></pre></td></tr></table></figure><ul><li>安装程序包</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cd build</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在cmake命令中指明安装目录的前缀&lt;prefix&gt;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> CMAKE_INSTALL_PREFIX 默认是/usr/<span class="built_in">local</span></span></span><br><span class="line">cmake -DCMAKE_INSTALL_PREFIX=/Users/carrol/tmp ..</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看目标文件夹</span></span><br><span class="line">j tmp</span><br><span class="line">tree -a</span><br></pre></td></tr></table></figure><h4 id="10-add-library"><a href="#10-add-library" class="headerlink" title="10. add_library"></a>10. add_library</h4><p>add_library ( name  [SHARED | STATIC | MODULE]  [source_filename] )</p><p>生成名字为<strong>libname.X</strong>的库文件。</p><ul><li>SHARED，动态库，libname.dylib</li><li>STATIC，静态库，libname.a</li></ul><p><strong>设置目标动态库和静态库同名 set_target_properties</strong></p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置目标动静态库同名</span></span><br><span class="line"><span class="keyword">add_library</span>(hello SHARED hello.c)</span><br><span class="line"><span class="keyword">add_library</span>(hello_static hello.c)</span><br><span class="line"><span class="keyword">set_target_properties</span>(hello_static</span><br><span class="line">                      PROPERTIES OUTPUT_NAME hello)</span><br></pre></td></tr></table></figure><p><strong>防止构建中清理同名文件 set_target_properties</strong></p><p>cmake在构建一个target时，会尝试清理掉其他使用这个名字的库——在构建libhello.a时会清理掉libhello.dylib。</p><blockquote><p>我实际操作时候会保留两个库文件，但是在作为第三方被引用的时候会报错：</p><p>dyld: Library not loaded: libhello.dylib</p><p>Reason: image not found</p></blockquote><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SET_TARGET_PROPERTIES</span>(hello </span><br><span class="line">                      PROPERTIES CLEAN_DIRECT_OUTPUT <span class="number">1</span>)</span><br><span class="line"><span class="keyword">SET_TARGET_PROPERTIES</span>(hello_static </span><br><span class="line">                      PROPERTIES CLEAN_DIRECT_OUTPUT <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><strong>设置动态版本号 set_target_properties</strong></p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置动态库版本号</span></span><br><span class="line"><span class="keyword">set_target_properties</span>(hello</span><br><span class="line">                      PROPERTIES VERSION <span class="number">1.2</span></span><br><span class="line">                      SOVERSION <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>编译文件夹下生成了libhello.1.2.dylib、libhello.1.dylib、libhello.dylib三个动态库文件，只有一个是真的，另外两个是替身。</p><p><strong>安装共享库和头文件</strong></p><p>修改库的源文件夹下的CMakeLIsts.txt</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 库文件</span></span><br><span class="line"><span class="keyword">install</span>(TARGETS hello hello_static</span><br><span class="line">        ARCHIVE DESTINATION lib   //静态库</span><br><span class="line">        LIBRARY DESTINATION lib)  //动态库</span><br><span class="line"><span class="comment"># 头文件</span></span><br><span class="line"><span class="keyword">install</span>(FILES hello.h DESTINATION <span class="keyword">include</span>/hello)</span><br></pre></td></tr></table></figure><h4 id="11-include-directories"><a href="#11-include-directories" class="headerlink" title="11. include_directories"></a>11. include_directories</h4><p>include_directories( dir1 dir2 … )</p><p>用来向工程添加多个特定的<strong>头文件搜索路径</strong></p><h4 id="12-link-directories-amp-target-link-libraries"><a href="#12-link-directories-amp-target-link-libraries" class="headerlink" title="12. link_directories &amp; target_link_libraries"></a>12. link_directories &amp; target_link_libraries</h4><p>link_directories( dir1 dir2 … )</p><p>添加非标准的<strong>共享库搜索路径</strong></p><p>target_link_libraries( target lib1 lib2 … )</p><p>用来为目标target添加需要链接的共享库，target可以是一个可执行文件，也可以是一个库文件。</p><p><strong>查看生成目标的库依赖情况</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 生成的目标可执行文件为main</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">for</span> OSX</span></span><br><span class="line">otool -L main</span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">for</span> linux</span></span><br><span class="line">ldd main</span><br></pre></td></tr></table></figure><blockquote><p>只能列出动态库。</p></blockquote><h4 id="13-常用变量"><a href="#13-常用变量" class="headerlink" title="13. 常用变量"></a>13. 常用变量</h4><p><strong>PROJECT_BINARY_DIR</strong>：编译发生的目录</p><p><strong>PROJECT_SOURCE_DIR</strong>：工程顶层目录</p><p><strong>CMAKE_CURRENT_SOURCE_DIR</strong>：当前CMakeLists.txt所在目录</p><p><strong>CMAKE_MODULE_PATH</strong>：自定义的<strong>cmake模块</strong>所在路径</p><p><strong>LIBRARY_OUTPUT_PATH</strong>：重定义目标库文件存放目录</p><p><strong>EXECUTABLE_OUTPUT_PATH</strong>：重定义目标可执行文件存放目录</p><h4 id="14-findNAME-cmake模块"><a href="#14-findNAME-cmake模块" class="headerlink" title="14. findNAME.cmake模块"></a>14. findNAME.cmake模块</h4><ol><li>在工程目录中创建cmake文件夹，并创建FindHELLO.cmake模块：</li></ol><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例</span></span><br><span class="line"><span class="keyword">FIND_PATH</span>(HELLO_INCLUDE_DIR hello.h /usr/local/<span class="keyword">include</span>/hello)</span><br><span class="line"><span class="keyword">FIND_LIBRARY</span>(HELLO_LIBRARY hello /usr/local/lib)</span><br><span class="line"><span class="keyword">IF</span> (HELLO_INCLUDE_DIR <span class="keyword">AND</span> HELLO_LIBRARY)</span><br><span class="line"><span class="keyword">SET</span>(HELLO_FOUND <span class="keyword">TRUE</span>)</span><br><span class="line"><span class="keyword">ENDIF</span> (HELLO_INCLUDE_DIR <span class="keyword">AND</span> HELLO_LIBRARY)</span><br><span class="line"><span class="keyword">IF</span> (HELLO_FOUND)</span><br><span class="line"><span class="keyword">IF</span> (NOT HELLO_FIND_QUIETLY)</span><br><span class="line"><span class="keyword">MESSAGE</span>(STATUS <span class="string">"Found Hello: $&#123;HELLO_LIBRARY&#125;"</span>)</span><br><span class="line"><span class="keyword">ENDIF</span> (NOT HELLO_FIND_QUIETLY)</span><br><span class="line"><span class="keyword">ELSE</span> (HELLO_FOUND)</span><br><span class="line"><span class="keyword">IF</span> (HELLO_FIND_REQUIRED)</span><br><span class="line"><span class="keyword">MESSAGE</span>(FATAL_ERROR <span class="string">"Could not find hello library"</span>)</span><br><span class="line"><span class="keyword">ENDIF</span> (HELLO_FIND_REQUIRED)</span><br><span class="line"><span class="keyword">ENDIF</span> (HELLO_FOUND)</span><br></pre></td></tr></table></figure><ol><li>在主目录CMakeLists.txt中添加cmake模块所在路径：</li></ol><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为find_package()指令成功执行</span></span><br><span class="line"><span class="keyword">set</span>(CMAKE_MODULE_PATH <span class="variable">$&#123;PROJECT_SOURCE_DIR&#125;</span>/cmake)</span><br></pre></td></tr></table></figure><ol><li>然后就可以在源文件CMakeLists.txt中调用 find_package：</li></ol><p>find_package ( name  [QUIET]  [REQUIRED] )</p><p>用来调用预定义在<strong>CMAKE_MODULE_PATH</strong>下的Find\<name>.cmake模块。</name></p><p>每一个模块都会定义以下几个变量：</p><ul><li>NAME_FOUND</li><li>NAME_INCLUDE_DIR or NAME_INCLUDES </li><li>NAME_LIBRARY or NAME_LIBRARIES </li></ul><p>根据指令后面的参数还会有以下变量：</p><ul><li><p>NAME_FIND_QUIETLY，如果指定了QUIET参数，就不会执行如下语句：</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">MESSAGE</span>(STATUS <span class="string">"Found Hello: $&#123;NAME_LIBRARY&#125;"</span>)</span><br></pre></td></tr></table></figure></li><li><p>NAME_FIND_REQUIRED，如果指定了REQUIRED参数，就是指这个共享库是工程必须的，如果找不到，工程就不能编译，对应地会执行如下语句：</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">MESSAGE</span>(FATAL_ERROR <span class="string">"Could not find NAME library"</span>)</span><br></pre></td></tr></table></figure></li></ul><p>可以通过\<name>_FOUND判断模块是否被找到，并执行不同的操作（如添加非标准路径、输出错误信息等）。</name></p><h4 id="15-find-指令"><a href="#15-find-指令" class="headerlink" title="15. find_指令"></a>15. find_指令</h4><ul><li><p><strong>find_path</strong></p><p>find_path ( VAR name1 path1 path2 … )</p><p>VAR变量代表<strong>包含</strong>name1文件的<strong>路径</strong>——路径。</p></li><li><p><strong>find_library</strong></p><p>find_library ( VAR name1 path1 path2 …)    </p><p>VAR变量包含找到的库的<strong>全路径</strong>，<strong>包括库文件名</strong>——路径下的所有文件。</p></li></ul>]]></content>
      
      
        <tags>
            
            <tag> basic </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ICP, Iterative Closest Points</title>
      <link href="/2018/05/05/ICP-Iterative-Closest-Points/"/>
      <url>/2018/05/05/ICP-Iterative-Closest-Points/</url>
      <content type="html"><![CDATA[<h3 id="1-基本实现"><a href="#1-基本实现" class="headerlink" title="1 基本实现"></a>1 基本实现</h3><p>数据点云配准，最经典的方法就是ICP迭代最近点法。</p><ul><li><strong>最近点</strong>：欧几里得意义上距离最近的点。</li><li><strong>迭代</strong>：迭代目标是通过不断更新运动参数，使得两个点云的重叠部分充分吻合。</li></ul><p>ICP的求解分为两种方式：</p><ul><li>利用线性代数求解（SVD），<strong>在给定了匹配的情况下</strong>，最小二乘问题实际上具有解析解。</li><li>利用非线性优化方式求解，类似于BA方法，<strong>适用于匹配未知的情况</strong>。</li></ul><h3 id="2-SVD方法求解"><a href="#2-SVD方法求解" class="headerlink" title="2 SVD方法求解"></a>2 SVD方法求解</h3><p>算法推导如下：</p><ol><li><p>首先将点云文件进行<strong>粗匹配</strong>，如ORB特征点匹配。</p></li><li><p>从点集$P={\overrightarrow{p_1}, \overrightarrow{p_2}, …, \overrightarrow{p_n}}$中随机选取指定数量的点$\{\overrightarrow{p_t}\}$作为<strong>参考点</strong>，参考点的数量决定了ICP算法的计算效率和配准精度。</p></li><li><p>在另一个点集$Q={\overrightarrow{q_1}, \overrightarrow{q_2}, …, \overrightarrow{q_m}}$是待匹配的点<code>query points</code>，那么想要找到一个欧式变换$R, t$，使得$\forall i, p_i = Rq_i + t$。</p></li><li><p>求解欧式变换$T^k$，使得$E^k=\Sigma| \overrightarrow{p_t} - T^k \overrightarrow{q_t}|^2$最小化。 将空间变换分解为旋转和平移两部分，首先定义两个点云的质心：</p><script type="math/tex; mode=display">\overrightarrow{p} = \frac{1}{n} \Sigma \overrightarrow{p_t}, \ \  \overrightarrow{q} = \frac{1}{n} \Sigma \overrightarrow{q_t}，质心,\ 用于描述平移\\\overrightarrow p_i =  \overrightarrow{p_t} - \overrightarrow{p}, \ \ \overrightarrow q_i = \overrightarrow{q_t} - \overrightarrow p，中心化点云,\ 用于描述旋转\\</script><p>于是有目标函数：</p><script type="math/tex; mode=display">\begin{split} E^k &  = \Sigma|\overrightarrow{p_t} - T^k \overrightarrow{q_t}|^2 = \Sigma|(p+p_i) -T (q+q_i)|^2\\& = \Sigma|(p+p_i) -R (q+q_i) -t|^2\\& = \Sigma |(p_i - Rq_i) + (p - Rq -t)|^2\\& = \Sigma( |p_i - Rq_i|^2 + |p - Rq -t|^2)\\& = \Sigma( |p_i - Rq_i|^2\\ J &= \frac{1}{2} \sum e_i = \frac{1}{2} E^k\end{split}</script><p>对目标函数展开，而且已知旋转矩阵是正交阵，$R^TR=I​$，所以目标函数的前两项都与$R​$无关：</p><script type="math/tex; mode=display">R^* = argmin_R J = \frac{1}{2}\sum p_i^Tp_i + q_i^TR^TRq_i - 2p_i^TRq_i</script><p>只有最后一项与$R$有关，于是得到关于$R$的目标函数：</p><script type="math/tex; mode=display">J(R) = \sum_{unrelated} -\ p_i^TRq_i = \sum - \ tr(Rq_ip_i^T) = -tr(R\sum_{i=1}^nq_ip_i^T)</script><p>然后通过<strong>SVD奇异值分解</strong>求解上述问题的最优$R$，首先定义$W = \sum_1^n pq^T$，当$W$满秩时：</p><script type="math/tex; mode=display">W = \sum_{i=1}^{n} \overrightarrow{p_i}*\overrightarrow{q_i^T} = U\begin{bmatrix}\sigma1 & 0 & 0 \\0 & \sigma2  & 0 \\0 & 0 & \sigma3\end{bmatrix}V^T\\R = UV^T</script><p>然后间接得到平移$t$：</p><script type="math/tex; mode=display">t = {p} - R{q}</script></li></ol><p>代码实现如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">pose_estimation_3d3d</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Point3f&gt;&amp; pts1,         <span class="comment">// point cloud 1</span></span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Point3f&gt;&amp; pts2,         <span class="comment">// point cloud 2</span></span></span></span><br><span class="line"><span class="function"><span class="params">        Mat&amp; R, Mat&amp; t,</span></span></span><br><span class="line"><span class="function"><span class="params">        Eigen::Matrix3d&amp; R_, Eigen::Vector3d&amp; t_</span></span></span><br><span class="line"><span class="function"><span class="params">        )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Point3f p1, p2;         <span class="comment">// center of Mass</span></span><br><span class="line">    <span class="keyword">int</span> N = pts1.size();</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        p1 += pts1[i];</span><br><span class="line">        p2 += pts2[i];</span><br><span class="line">    &#125;</span><br><span class="line">    p1 /= N;</span><br><span class="line">    p2 /= N;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span>&lt;Point3f&gt; q1(N), q2(N);      <span class="comment">// remove the COM</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        q1[i] = pts1[i] - p1;</span><br><span class="line">        q2[i] = pts2[i] - p2;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Eigen::Matrix3d W = Eigen::Matrix3d::Zero();       <span class="comment">// calculate W matrix</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        W += Eigen::Vector3d(q1[i].x, q1[i].y, q1[i].z) * Eigen::Vector3d(q2[i].x, q2[i].y, q2[i].z).transpose();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// SVD decomposition</span></span><br><span class="line">    Eigen::JacobiSVD&lt;Eigen::Matrix3d&gt; svd(W, Eigen::ComputeFullU|Eigen::ComputeFullV);         <span class="comment">// SVD</span></span><br><span class="line">    Eigen::Matrix3d U = svd.matrixU();</span><br><span class="line">    Eigen::Matrix3d V = svd.matrixV();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// calculate R,t</span></span><br><span class="line">    R_ = U * V.transpose();</span><br><span class="line">    t_ = Eigen::Vector3d(p1.x, p1.y, p1.z) - R_ * Eigen::Vector3d(p2.x, p2.y, p2.z);</span><br></pre></td></tr></table></figure><h3 id="3-非线性优化方法"><a href="#3-非线性优化方法" class="headerlink" title="3 非线性优化方法"></a>3 非线性优化方法</h3><p>另一种方式是通过迭代的方式来寻找最优值，误差项的表示与上一节相同，用李代数来表达位姿，旋转和平移不用再解耦表示，目标函数为：</p><script type="math/tex; mode=display">\xi^* = argmin \frac{1}{2}\sum_{i=1}^n ||p_i - exp(\xi ^{\wedge})q_i||_2^2</script><p><strong>单个误差项关于位姿的导数</strong>可以使用<a href="https://amberzzzz.github.io/2018/05/12/%E4%B8%89%E7%BB%B4%E5%88%9A%E4%BD%93%E8%BF%90%E5%8A%A8-%E6%9D%8E%E4%BB%A3%E6%95%B0/">李代数扰动模型</a>来描述：</p><script type="math/tex; mode=display">\frac{\partial e}{\partial \delta \xi} = (e)^{\odot} = (p_i - exp(\xi^{\wedge})q_i)^{\odot}</script><p>其中$p_i$作为参考点，对扰动的导数为0，因此：</p><script type="math/tex; mode=display">\frac{\partial e}{\partial \delta \xi} =   - (exp(\xi^{\wedge})q_i)^{\odot}</script><p>将最小二乘问题进行图描述：优化变量为李代数表达的位姿$\xi$，因此图中只有一个节点，误差项为一元边（从当前节点指向当前节点），对误差项做线性展开：</p><script type="math/tex; mode=display">e_i(\xi + \delta \xi) = e(\xi) + J(\xi)\delta \xi</script><p>其中的<strong>雅可比矩阵</strong>也就是上面说的，单个误差项关于位姿的一阶导数。</p><h3 id="4算法优化"><a href="#4算法优化" class="headerlink" title="4算法优化"></a>4算法优化</h3><ul><li>删除点云数据采集中产生的噪声及异常值。</li><li>查找最近点的过程采用KD-Tree数据结构，减少时间复杂度。</li></ul>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CLion for record</title>
      <link href="/2018/05/03/clion%E4%BD%BF%E7%94%A8/"/>
      <url>/2018/05/03/clion%E4%BD%BF%E7%94%A8/</url>
      <content type="html"><![CDATA[<h3 id="1-cmake"><a href="#1-cmake" class="headerlink" title="1 cmake"></a>1 cmake</h3><p>详见cmake for record。</p><h3 id="2-简单配置"><a href="#2-简单配置" class="headerlink" title="2 简单配置"></a>2 简单配置</h3><p>主要就是keymap很不适应，基本上删除了大部分editing的配置，因为可以用vim。剩下的修改主要延续sublime和OSX的习惯。</p><h4 id="2-1-搜索"><a href="#2-1-搜索" class="headerlink" title="2.1 搜索"></a>2.1 搜索</h4><p>全局搜索：cmd + F</p><p>剩下的交给vim。</p><h4 id="2-2-导航"><a href="#2-2-导航" class="headerlink" title="2.2 导航"></a>2.2 导航</h4><p>search for file：cmd + O</p><p>search for class：opt + cmd + O</p><p>search for symbol：shift + cmd + O</p><p>go to line：cmd + G</p><p>back：ctrl + cmd + left</p><p>forward：ctrl + cmd + right</p><p>剩下的交给vim。</p><h4 id="2-3-注释"><a href="#2-3-注释" class="headerlink" title="2.3 注释"></a>2.3 注释</h4><p>代码块注释：shift + cmd + ／</p><h4 id="2-4-智能提示"><a href="#2-4-智能提示" class="headerlink" title="2.4 智能提示"></a>2.4 智能提示</h4><p>看见小灯泡就：opt + enter</p><h4 id="2-5-run-amp-build"><a href="#2-5-run-amp-build" class="headerlink" title="2.5 run &amp; build"></a>2.5 run &amp; build</h4><p>run：cmd + R</p><p>build：cmd + B</p><h4 id="2-6-代码生成"><a href="#2-6-代码生成" class="headerlink" title="2.6 代码生成"></a>2.6 代码生成</h4><p>insert：cmd + J</p><blockquote><p>最近在熟悉Eigen库，经常要打印东西，加了一个split模版快速分割代码片段。</p></blockquote><p>generate：cmd + N</p><p>还有一些vim与ide冲突的键，可以手动选择是服从ide还是vim。 </p>]]></content>
      
      
        <tags>
            
            <tag> ide </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Graph-based Optimization</title>
      <link href="/2018/05/02/graph-based-optimization/"/>
      <url>/2018/05/02/graph-based-optimization/</url>
      <content type="html"><![CDATA[<h3 id="1-综述"><a href="#1-综述" class="headerlink" title="1 综述"></a>1 综述</h3><p>基于图优化的slam主要分为以下三个部分：</p><ul><li>前端：基于传感器数据建图，<strong>匹配相邻帧</strong>，添加节点和边（raw graph），节点表示机器人的位姿，边表示节点之间的位姿联系。<strong>位姿信息可以来自里程计计算，可以来自ICP激光点云匹配，也可以来自闭环检测反馈</strong>。</li><li>后端：<strong>优化</strong>图，<strong>基于历史信息的约束</strong>，调整新加入的机器人位姿顶点使其尽量满足边的约束（optimized graph）。</li><li>宏观的闭环检测：根据闭环信息<strong>优化</strong>矫正整个拓扑图。</li></ul><p>这里面涉及到了<strong>两个优化</strong>，一个是后端局部优化，一个是全局闭环优化，两者计算的思路是一样的。</p><h3 id="2-优化"><a href="#2-优化" class="headerlink" title="2 优化"></a>2 优化</h3><h4 id="2-1-全局闭环优化，用于矫正整个拓扑图"><a href="#2-1-全局闭环优化，用于矫正整个拓扑图" class="headerlink" title="2.1 全局闭环优化，用于矫正整个拓扑图"></a>2.1 全局闭环优化，用于矫正整个拓扑图</h4><p>前端后端完成的事情是<strong>探索并创建新的节点</strong>，获得新的测量值，添加新的位姿关系方程：</p><script type="math/tex; mode=display">\begin{eqnarray}\begin{split}& x_0 + z_{01} = x_1\\& x_1 + z_{12} = x_2\\& ...\\& x_{k-1} + z_{k-1, k} = x_{k}\\\end{split}\end{eqnarray}</script><p>而全局闭环检测<strong>添加已知节点之间</strong>的位姿约束关系：</p><script type="math/tex; mode=display">\begin{equation}x_i + z_{i j} = x_j, \ \ \ \ i,j\in [0, k]\end{equation}</script><p>再添加一个<strong>初始条件</strong>（不是必须的，但是后面实验表明固定一个顶点比不固定效果要好——相当于有一个明确可信的基准）：</p><script type="math/tex; mode=display">x_0 = 0</script><ul><li>以上线性方程组中，<strong>闭环检测部分的方程中的两个结点都在前面出现过，因此不增加矩阵的秩</strong>，因此最终要求解包含$k$个方程$k+1$个未知数的线性方程组。</li><li>闭环的关键性：如果没有闭环条件，方程组$Ax=b$左右两边秩是相等的——有唯一解，添加了闭环条件以后，相当于方程组左侧$A$的秩不变，但是右侧$b$的秩则增加了，$rank(A) &lt; rank(A, b)$——没有解析解，只有最优。</li><li>实际上状态$\textbf x$是一个包含夹角$\theta$的向量$[x, y, \theta]$，实际相对位姿的计算<strong>并非简单的线性叠加</strong>：</li></ul><script type="math/tex; mode=display">\textbf x \oplus \Delta \textbf x = \begin{pmatrix}x + \Delta x cos\theta - \Delta y sin \theta  \\y + \Delta x sin\theta + \Delta y cos \theta  \\normAngle(\theta + \Delta \theta)\end{pmatrix}</script><p><strong>举个栗子</strong>：</p><blockquote><p>机器人从起始位置$x_0=0$处出发，里程计测得它向前移动了1m，到达$x_1$，接着测得它向后移动了0.8m，到达$x_2$，这时通过闭环检测，发现他回到了起始位置。</p></blockquote><p>首先根据给出信息构建图：</p><script type="math/tex; mode=display">x_0 + 1 = x_1\\x_1 - 0.8 = x_2</script><p>然后根据闭环条件添加约束：</p><script type="math/tex; mode=display">x_2 = x_0</script><p>补充初始条件：</p><script type="math/tex; mode=display">x_0 = 0</script><p>使用<strong>最小二乘法</strong>求上述方程组的最优解，首先构建<strong>残差平方和函数</strong>：</p><script type="math/tex; mode=display">\begin{eqnarray}\begin{split}& f_1 = x_0 = 0\\& f_2 = x_1 - x_0 - 1 = 0\\& f_3 = x_2 - x_1 + 0.8 = 0\\& f_4 = x_2 - x_0 = 0\end{split}\end{eqnarray}</script><script type="math/tex; mode=display">c = \Sigma_{i=1}^4 f_i^2 = x_0^2 + (x_1-x_0-1)^2 + (x_2-x_1+0.8)^2 + (x_2-x_0)^2</script><p>然后对每个参数<strong>求偏导</strong>：</p><script type="math/tex; mode=display">\frac{\partial c}{\partial x_1} = -x_0 + 2x_1-x_2 -1.8=0\\\frac{\partial c}{\partial x_2} = -x_0 - x_1 +2x_2 + 0.8 = 0</script><p>解得$x_1 = 0.93, x_2 = 0.07$，可以看到闭环矫正了所有节点的位姿，优化了整个拓扑图。</p><h4 id="2-2-后端局部优化，用于矫正局部地图"><a href="#2-2-后端局部优化，用于矫正局部地图" class="headerlink" title="2.2 后端局部优化，用于矫正局部地图"></a>2.2 后端局部优化，用于矫正局部地图</h4><p>再举个栗子：</p><blockquote><p>机器人从起始位置$x_0=0$处出发，并观测到其正前方2m处有一个路标$l_0$，里程计测得它向前移动了1m，到达$x_1$，这时观测到路标在其正前方0.8m处。</p></blockquote><p>首先根据前端信息建图 raw graph（这样建图明显是存在累积误差的）：</p><script type="math/tex; mode=display">x_0 + 1 = x_1</script><p>然后添加闭环约束：</p><script type="math/tex; mode=display">x_1 + 0.8 = l_0\\x_0 + 2 = l_0</script><p>初始条件：</p><script type="math/tex; mode=display">x_0 = 0</script><p>构建残差平方和：</p><script type="math/tex; mode=display">c = \Sigma_{i=1}^4 f_i^2 = x_0^2 + (x_1-x_0-1)^2 +(l_0-x_0-2)^2 + (l_0-x_1-0.8)^2</script><p>求偏导求解：$x_1 = 1.07, l_0 = 1.93$，可以看到后端是对前端新添加进来的节点位姿做了矫正，消除部分测量误差。</p><p>这里面涉及到两种传感器信息——里程计和激光雷达，两种传感器的精度是有差别的，我们对其的信任程度也应该不同，反映到公式中就是<strong>要为不同传感器信息赋予不同的权重</strong>。假设编码器信息更准确，那么：</p><script type="math/tex; mode=display">c = \Sigma_{i=1}^4 f_i^2 = x_0^2 + \textbf{10}(x_1-x_0-1)^2 +(l_0-x_0-2)^2 + (l_0-x_1-0.8)^2</script><p>调整权重之后解得：$x_1 = 1.01, l_0 = 1.9$，可以看到<strong>计算结果会向着更信任的传感器的测量结果靠近</strong>。</p><h4 id="2-3-严格推导"><a href="#2-3-严格推导" class="headerlink" title="2.3 严格推导"></a>2.3 严格推导</h4><p><strong>2.3.1 信息矩阵（误差权重矩阵)</strong></p><p>图优化问题转化为最小二乘问题，首先是带权重的残差平方和函数的一般形式：</p><script type="math/tex; mode=display">F(x) = \Sigma_{i,j} e(x_i, x_j, z_{i,j})^T\Omega_{i,j}e(x_i, x_j, z_{i,j})</script><script type="math/tex; mode=display">x^{*} = argminF(x)</script><p>其中的$\Omega_{i,j}$项就是上文提到的误差权重矩阵，它的正式名字叫<strong>信息矩阵</strong>。</p><p>传感器的测量值，可以看作是以真值为中心的多元高斯分布：</p><script type="math/tex; mode=display">f_x(x_1, x_2, ..., x_k) = \frac{1}{\sqrt{}(2\pi)^k|\Sigma|}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))</script><p>协方差矩阵$\Sigma$对角线上的值表示每一维对应的方差，该方差值越大表示该维度不确定性越大，对应的信息权重应该越小。<strong>实际上拓扑图上每条边对应的信息矩阵就是对应测量协方差矩阵的逆</strong>。</p><p><strong>2.3.2 非线性</strong></p><p>上文已经提到，位姿变化非线性——<strong>非线性最小二乘问题</strong>，要采用迭代法求解。迭代法需要有一个<strong>好的初始假设值</strong>，然后在这个值附近<strong>增量式迭代</strong>寻找最优解。</p><script type="math/tex; mode=display">f(x) = \Sigma e^T\Omega e\\最小二乘问题目标函数：min \frac{1}{2}||f(x)||_2^2</script><p>首先要将非线性函数转化成关于增量$\Delta x$的线性函数——泰勒展开，根据具体的展开形式又分为：</p><ul><li><p>一阶、二阶梯度法</p><p>直接<strong>对目标函数</strong>在$x$附近进行泰勒展开：</p><script type="math/tex; mode=display">||f(x+\Delta x)||_2^2 \approx ||f(x)||_2^2 +J(x) \Delta x = \frac{1}{2} \Delta x^T H \Delta x</script><p><strong>一阶梯度法（最速下降法）</strong>：只保留一阶梯度，并引入步长$\lambda$：</p><script type="math/tex; mode=display">\Delta x^* = -\lambda J^T(x)</script><p><strong>二阶梯度法（牛顿法）</strong>：保留一阶和二阶梯度信息</p><script type="math/tex; mode=display">J^T(x)+H\Delta x^*=0</script><p>最速下降法过于贪心，容易走出锯齿路线，增加迭代次数。牛顿法需要计算目标函数的二阶导数（Hessian矩阵），计算困难。</p></li></ul><ul><li><p>高斯牛顿法</p><p><strong>对$f(x)$而不是目标函数$f(x)^2$</strong>在$x$附近进行一阶泰勒展开：</p><script type="math/tex; mode=display">f(x+\Delta x) \approx f(x) + J(x) \Delta x</script><p>对应每一个误差函数$e_{ij}$：</p><script type="math/tex; mode=display">\begin{split}& e_{ij}(x_i+\Delta x_i, x_j+\Delta x_j) = e_{i,j}(x+\Delta x)  \\&\approx e_{ij} +\frac{\partial e_{ij}}{\partial x}\Delta x = e_{ij} +  J_{ij}\Delta x\end{split}</script><p>​            其中$J_{ij}$为初始值附近的雅可比矩阵（定义见卡尔曼滤波）。</p><p>带入目标函数得到<strong>近似二阶展开</strong>：</p><script type="math/tex; mode=display">\begin{split}& F_{ij}(x+\Delta x) = e_{ij}(x+\Delta x)^T \Omega_{ij}e_{ij}(x+\Delta x)\\& \approx (e_{ij} +  J_{ij}\Delta x)^T \Omega_{ij}(e_{ij} +  J_{ij}\Delta x)\\& = \underbrace{e_{ij}^T\Omega_{ij}e_{ij}}_{c_{ij}} + 2\underbrace{e_{ij}^T\Omega_{ij}J_{ij}}_{b_{ij}^T}\Delta x + \Delta x^T \underbrace{J_{ij}^T\Omega_{ij}J_{ij}}_{H_{ij}}\Delta x\\& = c_{ij} + 2b_{ij}^T\Delta x + \Delta x^T H_{ij}\Delta x\end{split}</script><p>求解增量$\Delta x$：</p><script type="math/tex; mode=display">2b + 2H\Delta x^* = 0\\高斯牛顿方程：H\Delta x^* = -b\\</script><p>对比牛顿法可见，<strong>高斯牛顿法用$J^TJ$作为二阶Hessian矩阵的近似，简化了计算</strong>。</p><p>上述算法要求近似$H$矩阵是正定且可逆的，实际数据很难满足，因而在使用高斯牛顿算法时可能出现<strong>$H$为奇异矩阵或病态</strong>的情况，增量稳定性较差，导致算法不收敛。</p><blockquote><p>图形上来思考，就是近似后的梯度方向不再是梯度变化最快的方向，可能引起不稳定。</p></blockquote></li></ul><ul><li><p>列文伯格—马夸尔特法</p><p>为$\Delta x$添加一个信赖区域，不让它因为过大而使得近似$f(x+\Delta x) = f(x) + J(x)\Delta x$不准确。</p><script type="math/tex; mode=display">\rho = \frac{f(x+\Delta x) - f(x)}{J(x) \Delta x}</script><p>可以看到如果$\rho$接近1，说明近似比较好。如果$\rho$比较大，说明实际减小的值远大于估计减小的值，需要放大近似范围，反之你懂的。</p><script type="math/tex; mode=display">||D\Delta x_k||_2^2 \leq \mu</script><p>将每次迭代得到的$\Delta x$限定在一个半径为信赖区域的椭球中，根据$\rho$的大小修改信赖区域。于是问题转化成为了<strong>带不等式约束的优化问题</strong>：</p><script type="math/tex; mode=display">min_\Delta x \frac{1}{2}||f(x) + J(x)\Delta x||^2_2, \ \ s.t. ||D \Delta x ||^2 = \mu</script><p>用拉格朗日乘子转化成无约束问题：</p><script type="math/tex; mode=display">min_\Delta x \frac{1}{2}||f(x) + J(x)\Delta x||^2_2 + \frac{\lambda}{2}||D \Delta x ||^2</script><p>展开后得到如下形式：</p><script type="math/tex; mode=display">(H + \lambda D^TD)\Delta x^* = -b</script><p>通常把$D$取值为单位阵$I$，得到更简化形式：</p><script type="math/tex; mode=display">(H + \lambda I)\Delta x^* = -b</script><p>当$\lambda$较小时，$H$占主要地位，说明二次近似模型较好，LM算法更接近高斯牛顿法。当$\lambda$较大时，$\lambda I$占主要地位，LM算法更接近一阶梯度法。修正了线性方程组矩阵的病态问题，比高斯牛顿法更加健壮，但是收敛速度也更慢。</p><blockquote><p>图形上思考，LM算法修正了高斯牛顿法得到的梯度，以此固定一个搜索区域，在区域内寻找最优。</p></blockquote></li></ul><p><strong>2.3.3 稀疏矩阵</strong></p><p>对于误差函数$e_{ij}$，它只和$e_i$和$e_j$有关，因此它的雅可比矩阵有如下结构（行数是$x$的维度，列数是拓扑图中节点映射关系的数目）：</p><script type="math/tex; mode=display">J_{ij} = \begin{bmatrix}0 & ... & 0 & \underbrace{\frac{\partial e_{i}}{\partial x_i}}_{A_{ij}} & 0 & ... &  \underbrace{\frac{\partial e_{j}}{\partial x_j}}_{B_{ij}} & 0 & ... & 0\end{bmatrix}</script><p>相应地$b_{ij}$是一个包含很多0的列向量：</p><script type="math/tex; mode=display">\begin{split} b_{ij}^T& = e_{ij}^T \Omega_{ij} J_{ij}\\&= e_{ij}^T\Omega_{ij} (0 ... A_{ij}...B_{ij}...0)\\&=(0...e_{ij}^T\Omega_{ij}A_{ij}...e_{ij}^T\Omega_{ij}B_{ij}...0)\end{split}</script><p>$b = \Sigma_{ij} b_{ij}$：</p><p><img src="/2018/05/02/graph-based-optimization/b.png" alt=""></p><p>$H_{ij}$是一个包含很多0的对称阵：</p><script type="math/tex; mode=display">\begin{split}H_{ij}& = J_{ij}^T \Omega_{ij}J_{ij} \\& = \begin{pmatrix}...\\A_{ij}^T\\...\\B_{ij}^T\\...\end{pmatrix}\Omega_{ij}\begin{pmatrix}... & A_{ij} & ... & B_{ij} & ...\end{pmatrix}\\& =\begin{pmatrix}    &  \\& A_{ij}^T\Omega{ij}A_{ij} & A_{ij}^T\Omega_{ij}B_{ij} & \\  & \\& B{ij}^T\Omega_{ij}A_{ij} & B_{ij}^T\Omega_{ij}B_{ij}& \\   & \end{pmatrix}\end{split}</script><p>$H=\Sigma_{ij}H_{ij}$：</p><p><img src="/2018/05/02/graph-based-optimization/H.png" alt=""></p><p>梳理一下计算流程：$e_{ij} \to  J_{ij}  \to A_{ij}, B_{ij} \to b_{ij}, H_{ij} \to b, H \to \Delta x^* \to x$</p><p><strong>2.3.4 误差函数</strong></p><p>前面定义过位姿的非线性叠加，显然位姿误差也不是简单的线性加减关系：</p><script type="math/tex; mode=display">e_{ij}(x_i, x_j) = t2v(Z_{ij}^{-1}(X_i^{-1}.X_j))</script><p>其中的$Z_{ij}$、$X_i$、$X_j$都是矩阵形式。$X_i^{-1}X_j$表示节点j到节点i之间的位姿差异$\hat Z_{ij}$，假设这个转移矩阵形式如下：</p><script type="math/tex; mode=display">\hat Z_{ij} = \begin{bmatrix}R_{2*2} & t_{2*1} \\0 & 1\end{bmatrix}</script><p>假设测量值$Z_{ij}$形式如下：</p><script type="math/tex; mode=display">Z_{ij} = \begin{bmatrix}R^{'} & t^{'}\\0 & 1\end{bmatrix}</script><p>分块矩阵的求逆过程如下：</p><script type="math/tex; mode=display">\begin{split}\begin{bmatrix}R & t\\0 & 1\end{bmatrix}^{-1}& =\begin{bmatrix}\begin{bmatrix}I & t\\0 & 1\end{bmatrix}\begin{bmatrix}R & 0\\0 & 1\end{bmatrix}\end{bmatrix}^{-1}=\begin{bmatrix}R & 0\\0 & 1\end{bmatrix}^{-1}\begin{bmatrix}I & t\\0 & 1\end{bmatrix}^{-1}\\& =\begin{bmatrix}R^T & 0\\0 & 1\end{bmatrix}\begin{bmatrix}I & -t\\0 & 1\end{bmatrix}=\begin{bmatrix}R^T & -R^Tt\\0 & 1\end{bmatrix}\end{split}</script><p>所以误差$e_{ij}$计算如下：</p><script type="math/tex; mode=display">E_{ij} = Z_{ij}^{-1}\hat Z_{ij} = \begin{bmatrix}R^{'} & t^{'}\\0 & 1\end{bmatrix}^{-1}\begin{bmatrix}R & t\\0 & 1\end{bmatrix}=\begin{bmatrix}R^{'T} & -R^{'T}t^{'}\\0 & 1\end{bmatrix}\begin{bmatrix}R & t\\0 & 1\end{bmatrix}=\begin{bmatrix}R^{'T}R & R^{'T}(t-t^{'})\\0 & 1\end{bmatrix}</script><script type="math/tex; mode=display">e_{ij} = t2v(E_{ij})=\begin{bmatrix}R_z(t_{\hat z} - t_z)_{2*1}\\\theta_\hat z - \theta_z\end{bmatrix}_{3*1}=\begin{bmatrix}R_z(x_{\hat z} - x_z)\\R_z(y_{\hat z} - y_z)\\\theta_\hat z - \theta_z\end{bmatrix}=\begin{bmatrix}R_z[R_i(x_{j} - x_i) - x_z]\\R_z[R_i(y_{j} - y_{i}) - y_z]\\\theta_j - \theta_i - \theta_z\end{bmatrix}</script><p>求解雅可比矩阵$J_{ij}$：</p><script type="math/tex; mode=display">A_{ij} = \begin{bmatrix}\frac{\partial e_1}{\partial x_i} & \frac{\partial e_1}{\partial y_i} & \frac{\partial e_1}{\partial \theta_i}\\\frac{\partial e_2}{\partial x_i} & \frac{\partial e_2}{\partial y_i} & \frac{\partial e_2}{\partial \theta_i}\\\frac{\partial e_3}{\partial x_i} & \frac{\partial e_3}{\partial y_i} & \frac{\partial e_3}{\partial \theta_i}\\\end{bmatrix}=\begin{bmatrix}-R_z^TR_i^T & 0 & R_z^T\frac{\partial R_I^T}{\partial \theta_i}(x_j-x_i)\\0 & -R_z^TR_i^T  & R_z^T\frac{\partial R_I^T}{\partial \theta_i}(y_j-y_i)\\0 & 0 & -1\end{bmatrix}</script><script type="math/tex; mode=display">B_{ij} = \begin{bmatrix}R_z^TR_i^T & 0\\0 & 1\end{bmatrix}</script><p>累加$b$和$H$矩阵： </p><script type="math/tex; mode=display">b_{[i]} += A_{ij}\Omega_{ij}e_{ij}\\b_{[j]} += B_{ij}^T\Omega_{ij}e_{ij}\\H_{[ii]} += A_{ij}^T\Omega_{ij}A_{ij}\\H_{[ij]} += A_{ij}^T\Omega_{ij}B_{ij}\\H_{[ji]} += B_{ij}^T\Omega_{ij}A_{ij}\\H_{[jj]} += B_{ij}^T\Omega_{ij}B_{ij}</script>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关联分析</title>
      <link href="/2018/04/29/%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90/"/>
      <url>/2018/04/29/%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90/</url>
      <content type="html"><![CDATA[<h4 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h4><p><strong>频繁项集</strong>：集合，${a, b, c}$</p><p><strong>关联规则</strong>：映射，$a\to b$</p><p><strong>支持度</strong>：针对某个频繁项集，$support(频繁项集a) = \frac{freq(频繁项集a)}{freq(所有项集)}$</p><p><strong>可信度</strong>：衡量某条关联规则，$confidence(a\to b) = \frac{support(a|b)}{support(a)}$</p><p>对于包含N个元素的数据集，可能的集合有$2^N - 1$种，暴力遍历显然药丸，因此引入Apriori原理。</p><p><strong>Apriori原理</strong>：减少可能的项集，避免指数增长。</p><ul><li>backwards：如果某个项集是频繁的，那么它的所有子集也是频繁的。</li><li>forwards：如果一个项集是非频繁项集，那么它的所有超集也是非频繁的。</li></ul><h4 id="2-Apriori算法"><a href="#2-Apriori算法" class="headerlink" title="2 Apriori算法"></a>2 Apriori算法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apriori</span><span class="params">(dataSet, minSupport=<span class="number">0.5</span>)</span>:</span></span><br></pre></td></tr></table></figure><p>算法思路：从单个项集开始检查，去掉那些不满足最小支持度的项集，然后对剩下的集合进行组合，得到包含两个元素的项集，重复扫描，然后将剩余项集组合成包含三个元素的集合，依次类推，直到所有项集都被去掉。</p><ul><li>为啥最后会得到空集：因为包含所有元素的项集一定不是频繁项集，否则根据Apriori原理，它的全部子集都是频繁项集。</li><li>如何从包含k个元素的项集集合生成包含k+1个元素的项集集合：从k个元素的项集到k+1个元素项集的扩充，<strong>只允许有一个元素的不同</strong>，算法中为了避免重复结果，<strong>只对前k-1个元素相同的两个项集求并集</strong>。</li></ul><p>代码实现过程中发现了几个知识记录一下：</p><ul><li>map函数的返回值：python2下直接返回列表，python3下返回的是迭代器：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">map(frozenset, C1)</span><br><span class="line"><span class="comment"># 返回 &lt;map object at 0x101e78940&gt; </span></span><br><span class="line"></span><br><span class="line">list(map(frozenset, C1))</span><br><span class="line"><span class="comment"># 返回 list[frozenset1(), frozenset2(), ...]</span></span><br></pre></td></tr></table></figure><ul><li>字典的update方法：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 将dict2的键值添加到dict1中，在涉及迭代操作时可以省略传递中间值</span><br><span class="line">dict1.update(dict2)</span><br></pre></td></tr></table></figure><ul><li>set &amp; frozenset：set<strong>无排序</strong>且不重复，并且可变，因此unhashable。frozenset不可变，可以用作字典的key。</li></ul><h4 id="3-关联规则"><a href="#3-关联规则" class="headerlink" title="3 关联规则"></a>3 关联规则</h4><p>对一个包含k个元素的频繁项集，其中可能的关联规则有：</p><script type="math/tex; mode=display">C_N^1 + C_N^2 + ... + C_N^{N-1}</script><p>暴力遍历肯定又药丸，因此延续Apriori的思路，关联规则也有一条类似的属性：</p><ul><li>如果某条规则的前件不满足最小可信度要求，那么它的所有子集也不满足最小可信度要求。</li><li>对应的，如果某条规则的后件不满足最小可信度要求，那么它的所有超集也不满足。</li></ul><p>算法思路：对每个至少包含两个元素的频繁项集，从后部只包含一个元素的规则开始，对这些规则进行测试，接下来对所有剩余规则的后件进行组合，得到包含两个元素的后件（对应的补集就是前件），依次类推，直到测试完所有可能的后件。</p><ul><li>为啥只检查<strong>前后件互补</strong>的规则：因为一个频繁项集的所有子集也都是频繁项集，所以一个频繁项集中不互补的规则将是该频繁项集的某个子集的互补规则。</li></ul><h4 id="4-FP-growth算法"><a href="#4-FP-growth算法" class="headerlink" title="4 FP-growth算法"></a>4 FP-growth算法</h4><p>Apriori算法避免了暴力遍历子项集的指数式增长，但是对每一个新生成的频繁项集，都要扫描整个数据集，当数据集很大时，这种抛物线式增长的时间复杂度也不太令人满意。</p><p>FP-growth算法借助一种称为<strong>FP树</strong>的数据结构存储数据，来抽象原始数据集：</p><ul><li><strong>项集</strong>以路径的方式存储在树中</li><li><strong>相似项</strong>之间相连接成链表</li></ul><ul><li>一个元素项可以在FP树中出现多次</li><li>FP树存储的是元素的出现频率</li><li>项集完全不同时，树才会分叉，否则会有复用路径</li></ul><p>​算法思路：首先遍历一遍原始数据集，记录元素的出现频率，去掉不满足最小支持度的元素。然后再遍历一遍剩下的集合元素，<strong>构建FP树</strong>。然后就可以通过FP树挖掘频繁项集。</p><p>构建FP树：依次遍历每一个项集，首先将其中的非频繁项移除，并按照元素出现频率对过滤后的元素进行重排序。<strong>对过滤、排序后的集合</strong>，如果树中已存在现有元素，则增加现有元素的计数值，如果不存在，则向树中添加一个分支，新增节点的同时还要更新链表元素。主要就涉及两个数据结构：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义节点数据结构</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">treeNode</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, nameValue, numOccur, parentNode)</span>:</span></span><br><span class="line">        self.name</span><br><span class="line">        self.count</span><br><span class="line">        self.nodeLink   <span class="comment"># 链表信息，指向下一个相似项</span></span><br><span class="line">        self.parent</span><br><span class="line">        self.children</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="comment"># 用于存储元素frequency以及链接相似项的字典数据结构</span></span><br><span class="line">freq = &#123;&#125;</span><br><span class="line">freq[node_name] = [frequency, node1, node2, ...]</span><br></pre></td></tr></table></figure><ul><li>因为集合中元素的出现频率可能相等，因此过滤排序的结果不唯一，生成的树结构也会有差异。</li><li>第一次遍历删除非频繁元素时，发现字典在迭代过程中不能删除item，我转化成list暴力解决了，不知道有没有什么优雅的方式。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> freq[item]</span><br><span class="line"><span class="comment"># 返回 RuntimeError: dictionary changed size during iteration</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> list(freq.keys()):</span><br><span class="line">    <span class="keyword">if</span> freq[item] &lt; minSupport:</span><br><span class="line">        <span class="keyword">del</span>(freq[item])</span><br></pre></td></tr></table></figure><p>挖掘频繁项集：首先创建条件模式基，然后利用条件模式基，构建条件FP树。</p><p><strong>1 条件模式基</strong>：以所查找元素项为结尾的<strong>前缀路径</strong>集合，并且每条前缀路径都与<strong>起始元素项</strong>的计数值相关联。（这里面用到了前面定义的parent和nodeLink属性）</p><p><strong>2 构造条件FP树</strong>：与构造树的过程相同，使用的dataSet换成了条件模式基而已，函数参数count就是预留彩蛋。这样得到的就是<strong>指定频繁项的条件FP树</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateTree</span><span class="params">(cond_set, myTree, freq_header, count)</span>:</span></span><br><span class="line"><span class="comment"># cond_set: 一条path</span></span><br><span class="line"><span class="comment"># myTree: 根节点</span></span><br><span class="line"><span class="comment"># freq_header: dict[node_name] = [frequency, head_node]</span></span><br><span class="line"><span class="comment"># count: path对应的count</span></span><br></pre></td></tr></table></figure><ul><li>构造的条件FP树过滤掉了条件模式基中的一些元素：这些元素本身是频繁项，但是与指定元素组合的集合不是频繁的。</li><li>相应地，条件树中剩余元素与指定频繁项组合的集合是频繁的。</li></ul><p><strong>3 迭代</strong>：从生成的条件FP树中，可以得到更复杂的频繁项。求解复杂频繁项的条件模式基，进而生成对应的条件FP树，就能得到更复杂的频繁项，依次类推进行迭代，直到FP树为空。</p><p>​    </p>]]></content>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>sublime注册码被无限次移除</title>
      <link href="/2018/04/25/sublime%E6%B3%A8%E5%86%8C%E7%A0%81%E8%A2%AB%E6%97%A0%E9%99%90%E6%AC%A1%E7%A7%BB%E9%99%A4/"/>
      <url>/2018/04/25/sublime%E6%B3%A8%E5%86%8C%E7%A0%81%E8%A2%AB%E6%97%A0%E9%99%90%E6%AC%A1%E7%A7%BB%E9%99%A4/</url>
      <content type="html"><![CDATA[<p>最近不知道sublime3抽什么风，突然开始验证注册码了，输入一个valid code分分钟给你移除。</p><p>收藏一个解决办法，有效性待验证：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># add the following to your host file(/private/etc/hosts)</span><br><span class="line">127.0.0.1 license.sublimehq.com</span><br><span class="line">127.0.0.1 45.55.255.55</span><br><span class="line">127.0.0.1 45.55.41.223</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> tool </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>VTK编译报错no override found for vtkpolydatamapper</title>
      <link href="/2018/04/12/VTK%E7%BC%96%E8%AF%91%E6%8A%A5%E9%94%99no-override-found-for-vtkpolydatamapper/"/>
      <url>/2018/04/12/VTK%E7%BC%96%E8%AF%91%E6%8A%A5%E9%94%99no-override-found-for-vtkpolydatamapper/</url>
      <content type="html"><![CDATA[<p>报错原因是通过IDE编译而不是直接通过cmake，因此要添加如下代码段：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"vtkAutoInit.h"</span>   </span></span><br><span class="line">VTK_MODULE_INIT(vtkRenderingOpenGL2); </span><br><span class="line">VTK_MODULE_INIT(vtkInteractionStyle);</span><br></pre></td></tr></table></figure><p>先记录解决办法，more details 留到以后。</p><p>基础测试：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"vtkAutoInit.h"</span></span></span><br><span class="line">VTK_MODULE_INIT(vtkRenderingOpenGL2); <span class="comment">// VTK was built with vtkRenderingOpenGL2</span></span><br><span class="line">VTK_MODULE_INIT(vtkInteractionStyle);</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vtkSphereSource.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vtkPolyData.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vtkSmartPointer.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vtkPolyDataMapper.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vtkActor.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vtkRenderWindow.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vtkRenderer.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vtkRenderWindowInteractor.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span>, <span class="keyword">char</span> *[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">// Create a sphere</span></span><br><span class="line">  vtkSmartPointer&lt;vtkSphereSource&gt; sphereSource =</span><br><span class="line">          vtkSmartPointer&lt;vtkSphereSource&gt;::New();</span><br><span class="line">  sphereSource-&gt;SetCenter(<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>);</span><br><span class="line">  sphereSource-&gt;SetRadius(<span class="number">5.0</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//mapper</span></span><br><span class="line">  vtkSmartPointer&lt;vtkPolyDataMapper&gt; mapper =</span><br><span class="line">          vtkSmartPointer&lt;vtkPolyDataMapper&gt;::New();</span><br><span class="line">  mapper-&gt;SetInputConnection(sphereSource-&gt;GetOutputPort());</span><br><span class="line"></span><br><span class="line">  <span class="comment">//actor</span></span><br><span class="line">  vtkSmartPointer&lt;vtkActor&gt; actor =</span><br><span class="line">          vtkSmartPointer&lt;vtkActor&gt;::New();</span><br><span class="line">  actor-&gt;SetMapper(mapper);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//renderer ,renderWindow, renderWindowInteractor.</span></span><br><span class="line">  vtkSmartPointer&lt;vtkRenderer&gt; renderer =</span><br><span class="line">          vtkSmartPointer&lt;vtkRenderer&gt;::New();</span><br><span class="line">  vtkSmartPointer&lt;vtkRenderWindow&gt; renderWindow =</span><br><span class="line">          vtkSmartPointer&lt;vtkRenderWindow&gt;::New();</span><br><span class="line">  renderWindow-&gt;AddRenderer(renderer);</span><br><span class="line">  vtkSmartPointer&lt;vtkRenderWindowInteractor&gt; renderWindowInteractor =</span><br><span class="line">          vtkSmartPointer&lt;vtkRenderWindowInteractor&gt;::New();</span><br><span class="line">  renderWindowInteractor-&gt;SetRenderWindow(renderWindow);</span><br><span class="line"></span><br><span class="line">  renderer-&gt;AddActor(actor);</span><br><span class="line">  renderer-&gt;SetBackground(<span class="number">.3</span>, <span class="number">.6</span>, <span class="number">.3</span>); <span class="comment">// Background color green</span></span><br><span class="line">  renderWindow-&gt;Render();</span><br><span class="line">  renderWindowInteractor-&gt;Start();</span><br><span class="line">  <span class="keyword">return</span> EXIT_SUCCESS;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> basic </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
