<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[seg-transformers]]></title>
    <url>%2F2021%2F11%2F18%2Fseg-transformers%2F</url>
    <content type="text"><![CDATA[‰πãÂâçÈÇ£ÁØá„Äätransformers„ÄãÂ§™Èïø‰∫ÜÔºåÊñ∞ÂºÄ‰∏Ä‰∏™ÂàÜÂâ≤ÊñπÂêëÁöÑ‰∏ìÈ¢òÔºåpapersÔºö ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîprevious‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî- [SETR] Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with TransformersÔºåÂ§çÊó¶ÔºåÊ∞¥ÔºåÊÑüËßâÂ∞±ÊòØÊääFCNÁöÑbackÊç¢Êàêtransformer [UNETR 2021] UNETR: Transformers for 3D Medical Image SegmentationÔºåËã±‰ºüËææÔºåÁõ¥Êé•‰ΩøÁî®transformer encoderÂÅöunet encoder [TransUNet 2021] TransUNet: Transformers Make Strong Encoders for Medical Image SegmentationÔºåencoder streamÈáåÈù¢Âä†transformer block [TransFuse 2021] TransFuse: Fusing Transformers and CNNs for Medical Image SegmentationÔºåÂ§ßÂ≠¶ÔºåCNN featureÂíåTransformer featureËøõË°åbifusion ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-new‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî- [Swin-Unet 2021] Swin-Unet: Unet-like Pure Transformer for Medical Image SegmentationÔºåTUMÔºå2DÁöÑUnet-like pure transformerÔºåÁî®swinÂÅöencoderÔºåÂíå‰∏éÂÖ∂ÂØπÁß∞ÁöÑdecoder [nnFormer 2021] nnFormer: Interleaved Transformer for Volumetric SegmentationÔºåÊ∏ØÂ§ßÔºåÂØπÊ†ánn-UnetÔºå3DÁâàÊú¨ÁöÑSwin-UnetÔºåÂÆåÂÖ®Â∞±ÊòØÁÖßÁùÄ‰∏ä‰∏ÄÁØáÂÜôÁöÑ nnFormer: Interleaved Transformer for Volumetric Segmentation Âä®Êú∫ Áî®transformerÁöÑability to exploit long-term dependenciesÔºåÂéªÂº•Ë°•Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÂÖàÂ§©ÁöÑspatial inductive bias recently transformer-based approaches Â∞Ütransformer‰Ωú‰∏∫‰∏Ä‰∏™ËæÖÂä©Ê®°ÂùóÔºåÁî®‰∫éÁºñÁ†Åglobal context Ê≤°ÊúâÂ∞ÜtransformerÊúÄÊ†∏ÂøÉÁöÑÔºåself-attentionÔºåÊúâÊïàÁöÑÊï¥ÂêàËøõCNN nnFormerÔºönot-another transFormer volume-based self-attentionÔºåÊûÅÂ§ßÈôç‰ΩéËÆ°ÁÆóÈáè ÊâìË¥•‰∫ÜSwin-UnetÂíånnUnet ËÆ∫ÁÇπ Transformers self-attention capture long-range dependencies give predictions more consisitent with humans previous approaches TransUNetÔºöUnetÁªìÊûÑÁ±ª‰ººÔºåCNNÊèêÂèñÁâπÂæÅÔºåÂÜçÊé•‰∏Ä‰∏™transformerËæÖÂä©ÁºñÁ†ÅÂÖ®Â±Ä‰ø°ÊÅØÔºå‰ΩÜÊòØ‰∏Ä‰∏§Â±ÇÁöÑtransformer layerÂπ∂‰∏çË∂≥‰ª•ÊèêÂèñÂà∞ËøôÁßçÈïøË∑ùÁ¶ªÁ∫¶Êùü Swin-UNetÔºöÊúâ‰∫ÜappropriateÁöÑ‰∏ãÈááÊ†∑ÊñπÊ≥ïÔºåtransformerËÉΩÂ§üÂ≠¶‰π†hierarchical object concepts at different scalesÔºå‰ΩÜÂÆÉÊòØ‰∏Ä‰∏™Á∫ØtransformerÁöÑÁªìÊûÑÔºåÁî®hierarchicalÁöÑtransformer blockÊûÑÈÄ†encoderÂíådecoderÔºåÊï¥‰Ωì‰πüÊòØUnetÁªìÊûÑÔºåÊ≤°ÊúâÊé¢Á¥¢Â¶Ç‰ΩïÂ∞ÜÂç∑ÁßØÂíåself-attentionÊúâÊú∫ÁªìÂêà nnFormer contributions hybrid stemÔºöÂç∑ÁßØÂíåself-attentionÈÉΩÁî®‰∏ä‰∫ÜÔºåÂπ∂‰∏îÈÉΩËÉΩÂÖÖÂàÜÂèëÊå•ËÉΩÂäõÔºå‰ªñÁöÑencoderÔºö È¶ñÂÖàÊòØ‰∏Ä‰∏™ËΩªÈáèÁöÑconv embedding layerÔºåÂ•ΩÂ§ÑÊòØÂç∑ÁßØËÉΩÂ§üÊèê‰æõÊõ¥preciseÁöÑspatial informationÔºå ÁÑ∂ÂêéÊòØ‰∫§ÊõøÁöÑtransformer blocksÂíåconvolutional down-sampling blocksÔºåcapture long-term dependencies at various scales V-MSAÔºövolume-based multi-head self-attention a computational-efficient way to capture inter-slice dependencies ËÆ°ÁÆóÂ§çÊùÇÂ∫¶Èôç‰Ωé90%‰ª•‰∏ä Â∫îËØ•Â∞±ÊòØÁ±ª‰ºº‰∫éswinÈÇ£Áßçinter-patch &amp; inter-patchÂêßÔºü ÊñπÊ≥ï overview U-netÁªìÊûÑÔºö embedding block + encoder + decoder + patch expanding block ‰∏âÊ¨°‰∏ãÈááÊ†∑ &amp; ‰∏âÊ¨°‰∏äÈááÊ†∑ long residual connections encoder inputÔºö3D patch $X \in R^{H \times W \times D}$ embedding block Â∞Ü3D patchËΩ¨ÂåñÊàêpatch tokensÔºå$X_e \in R^{\frac{H}{4}\frac{W}{4}\frac{D}{2}C}$Ôºå‰ª£Ë°®ÁöÑÊòØhigh-resolution spatial information $\frac{H}{4}\frac{W}{4}\frac{D}{2}$ÊòØtoken‰∏™Êï∞ CÊòØtensor channelÔºå192/96 4‰∏™ËøûÁª≠ÁöÑkernel3x3ÁöÑÂç∑ÁßØÂ±ÇÊõø‰ª£SwinÈáåÈù¢ÁöÑbig kernelÔºöÂ∞èÂç∑ÁßØÊ†∏ÁªôÂá∫ÁöÑËß£ÈáäÊòØËÆ°ÁÆóÈáè&amp;ÊÑüÂèóÈáéÔºåÊ≤°‰ªÄ‰πàÁâπÂà´ÁöÑÔºåÁî®Âç∑ÁßØembeddingÁªôÂá∫ÁöÑËß£ÈáäÊòØpixel-levelÁºñÁ†ÅÂ±ÄÈÉ®spatial‰ø°ÊÅØÔºåmore precisely Ââç‰∏âÂ±ÇÂç∑ÁßØÂêéÈù¢+GELU+LNÔºåstrideÂú®1„ÄÅ3Â±ÇÔºåÂ¶ÇÂõæ transformer block hierarchical compute self-attention within 3D local volumes (instead of 2D local windows) inputÔºötokens representation of 3D patchÔºå $X_t \in R^{L \times C}$ È¶ñÂÖàreshapeÔºöÂØπtoken sequenceÔºåÂÜçÊ¨°ÂàíÂàÜlocal volumeÔºå$\tilde X_t \in R^{N_V \times N_T \times C}$ local volumeÈáåÈù¢ÂåÖÂê´‰∏ÄÁªÑÁ©∫Èó¥Áõ∏ÈÇªÁöÑtokens $N_V$ÊòØvolumeÁöÑÊï∞ÁõÆÔºàÁ±ª‰ººSwinÈáåÈù¢windowÁöÑÊï∞ÁõÆÔºâ $N_T=S_H \times S_W \times S_D$ ÊòØÊØè‰∏™local volumesÈáåÈù¢tokenÁöÑ‰∏™Êï∞Ôºå{4,4,4}/{5,5,3} ÁÑ∂ÂêéË∑üSwin‰∏ÄÊ†∑Ôºå‰∏§‰∏™ËøûÁª≠ÁöÑtransformer blocksÔºå3D windows instead of 2D V-MSAÔºövolume-based multi-head self-attention SV-MSAÔºöshifted version ÂèçÊ≠£Â∞±ÊòØ3DÁâàÁöÑswinÔºåÂõûÂéªÁúãswinÊõ¥Ê∏ÖÊô∞ down-sampling block Â∞±ÊòØstrided convÔºåËØ¥ÊòØÁõ∏ÊØîËæÉ‰∫éneighboring concatenationÔºåËÉΩ‰∫ßÁîüÊõ¥hierarchicalÁöÑrepresentationÔºåÊúâÂä©‰∫élearn at multi scales decoder ÂíåencoderÈ´òÂ∫¶ÂØπÁß∞ down-sampÂØπÊ†áÊõøÊç¢Êàêstrided deconvolution ÁÑ∂ÂêéÂíåencoder‰πãÈó¥ËøòÊúâlong-range connectionÔºåËûçÂêàsemanticÂíåfine-grained information ÊúÄÂêéÁöÑexpanding block‰πüÊòØÁî®‰∫Üdeconv ÂÆûÈ™å Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation Âä®Êú∫ Unet-like pure Transformer Áî®Swin transformerÂÅöencoder ÂØπÁß∞ÁöÑdecoderÔºåÁî®patch expanding layerÂÅö‰∏äÈááÊ†∑ outperforms full-convolution / combined methods ËÆ∫ÁÇπ CNNÁöÑÂ±ÄÈôêÊÄß ÊèêÂèñexplicit global &amp; long-range information meanwhile SwinÂú®ÂêÑÈ°π‰ªªÂä°‰∏äSOTA‰∫Ü Swin-Unet the first pure Transformer-based Unet-shaped architecture consists of encoder, bottleneck, decoder and skip connections tokenÔºönon-overlapping patches split from the input image fed into encoderÔºöÂæóÂà∞context features fed into decoderÔºöÂ∞Üglobal featuresÂÜçupsampleÂõûinput resolution patch expanding layerÔºö‰∏çÁî®conv/interpolationÔºåÂÆûÁé∞spatialÂíåfeature-dimÁöÑincrease skip connectionÔºöÂØπTransformer-basedÁöÑUnetÁªìÊûÑ‰ªçÊóßÊúâÊïà ÊñπÊ≥ï overview patch partition Â∞ÜÂõæÂÉèÂàáÂàÜÊàê‰∏çÈáçÂè†ÁöÑpatchesÔºåpatch sizeÊòØ4x4 ÊØè‰∏™patchÁöÑfeature dimensionÂ∞±ÊòØ4x4x3=48Ôºå‰πüÂ∞±ÊòØ48-dim vec linear embedding Â∞ÜÂõ∫ÂÆöÁöÑpatch dimensionÊò†Â∞ÑÂà∞‰ªªÊÑèÁªôÂÆöÁª¥Â∫¶C ‰∫§ÊõøÁöÑSwin Transformer blocksÂíåPatch Merging generate hierarchical feature representations Swin Transformer block ÊòØË¥üË¥£Â≠¶feature representationÁöÑ Patch MergingÊòØË¥üË¥£Áª¥Â∫¶ÂèòÊç¢Ôºà‰∏ãÈááÊ†∑/‰∏äÈááÊ†∑ÔºâÁöÑ ÂØπÁß∞ÁöÑdecoderÔºö‰∫§ÊõøÁöÑSwin Transformer blocksÂíåPatch Expanding Patch ExpandingÂ∞ÜÁõ∏ÈÇªÁâπÂæÅÂõæÊãºÊé•Ëµ∑Êù•ÔºåÁªÑÊàê2xÂ§ßÁöÑÁâπÂæÅÂõæÔºåÂêåÊó∂ÂáèÂ∞ëÁâπÂæÅÁª¥Â∫¶ ÊúÄÂêé‰∏Ä‰∏™Patch Expanding layerÂàôÊâßË°å4ÂÄç‰∏äÈááÊ†∑ Swin Transformer block based on shifted windows ‰∏§‰∏™ËøûÁª≠ÁöÑTransformer block‰∏∫‰∏ÄÁªÑ ÊØè‰∏™blockÂÜÖÈÉ®ÈÉΩÊòØLN-MSA-LN-MLPÔºåresidualÔºåGELU Á¨¨‰∏Ä‰∏™blockÁöÑMSAÊòØW-MSA Á¨¨‰∫å‰∏™blockÁöÑMSAÊòØSW-MSA encoder inputÔºöC-dim tokensÔºå$\frac{H}{4} \times \frac{W}{4}$‰∏™tokens patch merging layer Â∞ÜpatchesÂàáÂàÜÊàê2x2ÁöÑ4‰∏™parts ÁÑ∂ÂêéÂ∞Ü4‰∏™partÂú®ÁâπÂæÅÁª¥Â∫¶‰∏äconcat ÁÑ∂ÂêéÊé•‰∏Ä‰∏™linear layerÔºåÂ∞ÜÁâπÂæÅÁª¥Â∫¶ÁöÑdimËΩ¨Êç¢‰∏∫2C ËøôÊ†∑spatial resolutionÂ∞±downsampled by 2x ÁâπÂæÅÁª¥Â∫¶Âä†ÂÄç‰∫Ü2x bottleneck encoderÂíådecoder‰∏≠Èó¥ÈÇ£‰∏™ÈÉ®ÂàÜ Áî®‰∫Ü‰∏§‰∏™ËøûÁª≠ÁöÑSwin transformer block „ÄêQUESTION„Äë‰πüÊòØshifited windowsÁöÑÂêóÔºü Ëøô‰∏™partÁâπÂæÅÁª¥Â∫¶‰∏çÂèò decoder patch expanding layer given input featuresÔºö$(\frac{W}{32} \times \frac{H}{32}\times 8C)$ ÂÖàÊòØ‰∏Ä‰∏™linear layerÔºåÂä†ÂÄçfeature dimÔºö$(\frac{W}{32} \times \frac{H}{32}\times 16C)$ ÁÑ∂ÂêéÂêàÂπ∂Áõ∏ÈÇª4‰∏™patch tokensÔºö$(\frac{W}{16} \times \frac{H}{16}\times 4C)$ skip connection concat‰ª•ÂêéÊé•‰∏Ä‰∏™linear layerÔºå‰øùÊåÅÁâπÂæÅÁª¥Â∫¶‰∏çÂèò ÂÆûÈ™å]]></content>
      <tags>
        <tag>transformer, segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GRAPH ATTENTION NETWORKS]]></title>
    <url>%2F2021%2F11%2F17%2FGRAPH-ATTENTION-NETWORKS%2F</url>
    <content type="text"><![CDATA[official repo: https://github.com/PetarV-/GAT reference: https://zhuanlan.zhihu.com/p/34232818 ÂΩíÁ∫≥Â≠¶‰π†ÔºàInductive LearningÔºâÔºöÂÖà‰ªéËÆ≠ÁªÉÊ†∑Êú¨‰∏≠Â≠¶‰π†Âà∞‰∏ÄÂÆöÁöÑÊ®°ÂºèÔºåÁÑ∂ÂêéÂà©Áî®ÂÖ∂ÂØπÊµãËØïÊ†∑Êú¨ËøõË°åÈ¢ÑÊµãÔºàÂç≥È¶ñÂÖà‰ªéÁâπÊÆäÂà∞‰∏ÄËà¨ÔºåÁÑ∂ÂêéÂÜç‰ªé‰∏ÄËà¨Âà∞ÁâπÊÆäÔºâÔºåËøôÁ±ªÊ®°ÂûãÂ¶ÇÂ∏∏ËßÅÁöÑË¥ùÂè∂ÊñØÊ®°Âûã„ÄÇ ËΩ¨ÂØºÂ≠¶‰π†ÔºàTransductive LearningÔºâÔºöÂÖàËßÇÂØüÁâπÂÆöÁöÑËÆ≠ÁªÉÊ†∑Êú¨ÔºåÁÑ∂ÂêéÂØπÁâπÂÆöÁöÑÊµãËØïÊ†∑Êú¨ÂÅöÂá∫È¢ÑÊµãÔºà‰ªéÁâπÊÆäÂà∞ÁâπÊÆäÔºâÔºåËøôÁ±ªÊ®°ÂûãÂ¶ÇkËøëÈÇª„ÄÅSVMÁ≠â„ÄÇ GRAPH ATTENTION NETWORKS Âä®Êú∫ taskÔºönode classification Âú®GCNÂü∫Á°Ä‰∏äÂºïÂÖ• masked self-attentional layers specify different weights to different nodes in a neighborhoodÔºåÊÑüËßâÊòØÁî®attentionÁü©ÈòµÊõøÊç¢ÈÇªÊé•Áü©ÈòµÔºü ËÆ∫ÁÇπ the attention architecture properties parallelizableÔºåËÆ°ÁÆóÈ´òÊïà can be applied to graph nodes having different degreesÔºåËøô‰∏™ÈÇªÊé•Áü©Èòµ‰πüÂèØ‰ª•Âïä directly applicable to inductive learning problemsÔºåÊòØËØ¥ÂéüÂßãGCNÈÇ£Áßçsemi-supervisedÂú∫ÊôØÂêó ÊÑüËßâÂêéÈù¢‰∏§ÁÇπÊúâÁÇπÁâµÂº∫ GCN ÂèØ‰ª•ÈÅøÂÖçÂ§çÊùÇÁöÑÁü©ÈòµËøêÁÆó ‰ΩÜÊòØ‰æùËµñÂõ∫ÂÆöÂõæÁªìÊûÑÔºå‰∏çËÉΩÁõ¥Êé•Áî®‰∫éÂÖ∂‰ªñÂõæ methods graph attentional layer inputÔºönode features N‰∏™ËäÇÁÇπÔºåF-dim representation $h=\{\overrightarrow {h_1},\overrightarrow {h_2},‚Ä¶,\overrightarrow {h_N} \}$Ôºå$\overrightarrow {h_i} \in R^F$ outputÔºöa new set of node features $h=\{\overrightarrow {h_1^{‚Äò}},\overrightarrow {h_2^{‚Äò}},‚Ä¶,\overrightarrow {h_N^{‚Äò}} \}$Ôºå$\overrightarrow {h_i} \in R^{F^{‚Äò}}$ a weight matrix $W \in R^{F \times F^{‚Äò}}$ applied to every node then self-attention compute attention coefficientsÔºö$e_{ij} = a(W\overrightarrow {h_i},W\overrightarrow {h_j})$ attention mechanism aÔºöÊòØ‰∏Ä‰∏™single-layer feedforward neural network + LeakyReLU(0.2) weight vector $\in R^{2F^{‚Äò}}$ softmax norm overall expression ‰∏§‰∏™feature vector concatÂà∞‰∏ÄËµ∑ ÁÑ∂ÂêéÂÖ®ËøûÊé•Â±Ç+LeakyReLU ÁÑ∂Âêésoftmax Ë°®ËææÁöÑÊòØËäÇÁÇπjÂØπËäÇÁÇπiÁöÑÈáçË¶ÅÊÄß masked attentionÔºöinject graph structureÔºåÂè™ËÆ°ÁÆóËäÇÁÇπiÁöÑneighborhoodÁöÑimportance neighborhoodÔºöthe first-order neighbors Âä†ÊùÉÂíå + nonlinearity multi-head attentionÔºö trainable weightsÊúâÂ§öÁªÑÔºå‰∏Ä‰∏™ËäÇÁÇπ‰∏éÂÖ∂neighborhoodÁöÑattention coefficientsÊúâÂ§öÁªÑ ÊúÄÂêéÊØèÁªÑweightsËÆ°ÁÆóÂá∫ÈÇ£‰∏™new node featureÔºàÂä†ÊùÉÂπ≥Âùá+nonlinear unitÔºâÔºåÂèØ‰ª•ÈÄâÊã©concat/avgÔºå‰Ωú‰∏∫ÊúÄÁªàËæìÂá∫ concat Â¶ÇÊûúÊòØÁΩëÁªúÊúÄÂêé‰∏ÄÂ±ÇÁöÑMHA layerÔºåÂÖàavgÔºåÂÜçÈùûÁ∫øÊÄßÊøÄÊ¥ªÂáΩÊï∞Ôºö overall comparisons to related work our proposed GAT layer directly address several issues that were present in prior approaches computationally efficientÔºåËØ¥ÊòØÊØîÁü©ÈòµÊìç‰ΩúÈ´òÊïàÔºåËøô‰∏™‰∏çÊáÇ assign different importance to nodes of a neighborhoodÔºåËøô‰∏™GCN with tranable adjacent matrix‰∏ç‰πüÊòØ‰∏ÄÊ†∑ÊÄßË¥®ÁöÑÂêóÔºå‰∏çÊáÇ enable ÊúâÂêëÂõæ enable inductive learningÔºåÂèØ‰ª•Ë¢´Áõ¥Êé•Áî®‰∫éËß£ÂÜ≥ÂΩíÁ∫≥Â≠¶‰π†ÈóÆÈ¢òÔºåÂç≥ÂèØ‰ª•ÂØπ‰ªéÊú™ËßÅËøáÁöÑÂõæÁªìÊûÑËøõË°åÂ§ÑÁêÜÔºå‰∏∫Âï•ÂèØ‰ª•‰∏çÊáÇ Êï∞Â≠¶Ë°®Ëææ‰∏äÁúãÔºåattentionÂíåadjacent matrixÊú¨Ë¥®‰∏äÈÉΩÊòØÁî®Êù•Âª∫Ê®°graph edgesÁöÑ adj-trainable GCNÔºödag paperÈáåÈù¢ÈÇ£ÁßçÔºåadjacent matrixÊú¨Ë∫´Â∞±ÊòØ‰∏Ä‰∏™ÂèØËÆ≠ÁªÉÂèòÈáè(N,N)ÔºåÈöèÁùÄËÆ≠ÁªÉÊõ¥Êñ∞ÂèÇÊï∞ GATÔºöattentionÁöÑÊõ¥Êñ∞ÂºïÂÖ•‰∫ÜÊñ∞ÁöÑÁ∫øÊÄßÂ±ÇÊùÉÈáç $ W \in R^{2F^{‚Äò}}$]]></content>
      <tags>
        <tag>GCN, attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GPT]]></title>
    <url>%2F2021%2F10%2F27%2FGPT%2F</url>
    <content type="text"><![CDATA[GPT papersÔºåopenAI‰∏âÈÉ®Êõ≤ÔºåÈÄöÁî®È¢ÑËÆ≠ÁªÉÊ®°Âûã [2018 GPT-1] Improving Language Understanding by Generative Pre-TrainingÔºötransformer-basedÔºåpre-training+task-specific finetuningÔºåÂ∞ÜÊâÄÊúâÁöÑtaskÁöÑËæìÂÖ•ÈÉΩÊï¥ÂêàÊàêsequence-to-sequence formÔºåÁªìÊûÑ‰∏ä‰∏çÈúÄË¶Åtask-specific architecture [2019 GPT-2] Language Models are Unsupervised Multitask LearnersÔºöÂØπGPT-1ÁªìÊûÑ‰∏äÂæÆË∞ÉÔºåÂºïÂÖ•huge datasetËøõË°åÊó†ÁõëÁù£ËÆ≠ÁªÉ [2020 GPT-3] Language models are few-shot learnersÔºöscaling up LMsÔºåzero-shot BERTÊúâ3‰∫øÂèÇÊï∞ GPT-1: Improving Language Understanding by Generative Pre-Training Âä®Êú∫ NLP tasks textual entailmentÔºöÊñáÊú¨Ëï¥Âê´ question answering semantic similarity assessment document classification labeled dataÂ∞ëÔºåunlabeled corporaÂÖÖË∂≥ large gains can be realized by generative pre-training of a language model on diverse unlabeled corpusÔºåÊó†ÁõëÁù£general modelÔºålearn universal representations discriminative fine-tuning on specific taskÔºåÊúâÁõëÁù£task-specific modelÔºåadapt to wide range of tasks general task-agnostic modelËÉΩÂ§üÊâìË¥•discriminatively trained models use task-aware input transformations ËÆ∫ÁÇπ learn from raw text &amp;alleviate the dependence on supervised learning still challengingÔºö ‰∏çÊ∏ÖÊ•öÈÄâ‰ªÄ‰πàoptmization objectivesÔºölanguage modeling/machine translation/discourse coherence effective way to transferÔºöÂä†task-specificÊ®°ÂûãÁªìÊûÑÊîπÂä®/auxiliary learning objectives/learning schemes two-stage training procedure pretrain + fine-tuning use TransformerÔºöbetter handling long-term dependencies task-specific input adaptionsÂ∞ÜËæìÂÖ•Â§ÑÁêÜÊàêstructuredËØçÂêëÈáèÂ∫èÂàó evaluate on natural language inference question answering semantic similarity text classification ÊñπÊ≥ï overview architectureÔºötransformer decoder training objectives unsupervisedÔºötext predictionÔºåÂâçÊñáÈ¢ÑÊµãÂêéÊñá supervisedÔºötask classifierÔºåÂØπÊï¥‰∏™Â∫èÂàóÂàÜÁ±ª Unsupervised pre-training given unsupervised corpus of tokens $U={u_1, ‚Ä¶, u_n}$ context window size $k$ use standard language modeling objectiveÔºö$L_1(U)=\sum log P(u_i|u_{i-k},‚Ä¶,u_{i-1};\Theta)$ use multi-layer Transformer decoder inputÔºö$h_0 = UW_e + W_p$ attention blocksÔºö$h_l = tranformer_block(h_{l-1}), \forall l\in[1,n]$ outputÔºö$P(u)=softmax(h_l W_e^T)$ use SGD Supervised fine-tuning given labeled dataset $C$ consists of $[x^1,‚Ä¶,x^m;y]$ instances use the final transformer block‚Äôs activation $h_l^m$ fed into an linear+softmax output layerÔºö$P(y|x^1,‚Ä¶,x^m)=softmax(h_l^mW_y)$ ‰ºòÂåñÁõÆÊ†áÊòØyÔºö$L_2(C) = \sum log(P(y|x^1,‚Ä¶,x^m))$ ÂÆûÈ™åÂèëÁé∞Âä†‰∏äUnsupervised loss helps learningÔºöÊèêÂçáÊ≥õÂåñÊÄßÔºåÂä†ÈÄüÊî∂Êïõ L_3(C) = L_2(C) + \lambda * L_1(C) Task-specific input transformations certain tasks has structured inputsÂ¶ÇÈóÆÁ≠îpairs/triplets we convert them into ordered sequences Textual entailmentÔºöÂ∞ÜÂâçÊèêpremiseÂíåÊé®ÁêÜhypothesis concatÂú®‰∏ÄËµ∑ Similarity tasksÔºö‰∏§‰∏™ÊñáÊú¨Ê≤°ÊúâÂÖàÂêéÈ°∫Â∫èÂÖ≥Á≥ªÔºåÊâÄ‰ª•‰∏ÄÂØπÊñáÊú¨ÂèòÊàêÈ°∫Â∫è‰∫§Êç¢ÁöÑ‰∏§‰∏™sequenceÔºåÊúÄÂêéÁöÑhidden units $h^m_l$Áõ∏Âä†ÔºåÁÑ∂ÂêéÊé•ËæìÂá∫Â±Ç Question Answering and Commonsense ReasoningÔºögiven document $z$, question $q$, and possible answers $\{a_k\}$Ôºåcontext $zq$ÂíåÊØè‰∏™Á≠îÊ°à$a_i$ÈÉΩÊûÑÈÄ†‰∏ÄÁªÑËøûÊé•ÔºåÁÑ∂ÂêéÂàÜÂà´independently processed with our modelÔºåÊúÄÂêéÂÖ±ÂêåÊé•ÂÖ•‰∏Ä‰∏™softmaxÔºåÁîüÊàêÂØπÊâÄÊúâpossible answersÁöÑÊ¶ÇÁéáÂàÜÂ∏É ÊâÄÊúâÁöÑËøûÊé•ÈÉΩ‰ΩøÁî®ÂàÜÈöîÁ¨¶$ ÊâÄÊúâÁöÑsequenceÁöÑÈ¶ñÂ∞æÈÉΩÂä†‰∏ä‰∏Ä‰∏™randomly initialized start&amp;end tokens ÂÆûÈ™å GPT-2: Language Models are Unsupervised Multitask Learners Âä®Êú∫ more general models which can perform many tasks train language model without explicit supervision trained on a new dataset of millions of webpages called WebText outperforms several baselines GPT-2Ôºöa 1.5B parameter Transformer ËÆ∫ÁÇπ Machine learning systems are sensitive to slight changes of data distribution task specification ‚Äònarrow experts‚Äô lack of generalization since ingle task training on single domain datasets methods multitask trainingÔºöËøò‰∏çÊàêÁÜü pretraining + finetuningÔºöstill require supervised training this paper connect the two lines above perform donw-stream tasks in a zero-shot setting ÊñπÊ≥ï natural sequential characteristic makes the general formulation $p(output|input)$ p(x) = \Pi_{i=1}^n p(s_{n-k}, ..., s_n|s_1, ..., s_{n-k-1}) task specific system requires the probabilistic framework also condition on the task to be performed $p(output|input, task)$ architectural levelÔºötask-specific encoders/decoders algorithmic levelÔºölike MAML or in a more flexible way to specify tasksÔºöwrite all as sequences translationÔºö(translate to french, english text, french text) comprehensionÔºö(answer the question, document, question, answer) training dataset Êµ∑ÈáèdocumentÂèØ‰ª•ÈÄöËøáÁà¨Ëô´Ëé∑Âæóbut significant data quality issues ‰∏étarget dataset similarÁöÑÂ§ñÈÉ®docÁöÑÂ≠êÈõÜËÉΩÂ§üÁªôÂà∞ÊèêÂçá Âõ†Ê≠§Êú¨ÊñáËÆæÂÆö‰∫Ü‰∏Ä‰∏™ÊêúÈõÜÊñáÊú¨ÁöÑÊú∫Âà∂ÔºöRedditÁöÑÂ§ñÈìæÔºåÂéªÊéâWikipedia input representation word-level language model VS byte-level language model word-level performs better ‰ΩÜÊòØÂèóÂà∞vocabularyÈôêÂà∂ Byte Pair Encoding (BPE) combine the empirical benefits of word-level LMs with the generality of byte-level approaches ÂÖ∑‰ΩìÊîπËøõËøòÊ≤°ÁêÜËß£ model Transformer-basedÔºåfew modifications on GPT-1 model layer normalization was moved to the input of each sub-block additional layer normalization was added after the final self-attention block initialization on residual pathÔºöN‰∏™residual layersÔºåÂ∞±Â∞Üresidual weights rescale $\frac{1}{\sqrt{N}}$ context sizeÔºö1024 batch sizeÔºö512 residual block ÂÆûÈ™å GPT-3: Language Models are Few-Shot Learners Âä®Êú∫ zero-shotÔºöpretraining+finetuning schemeËøòÊòØÈúÄË¶Åtask-specific finetuning datset scale-upÔºöscaling up language models greatly improves general few-shot performance]]></content>
      <tags>
        <tag>GPT, transformer, semi-supervised</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dag]]></title>
    <url>%2F2021%2F10%2F11%2Fdag%2F</url>
    <content type="text"><![CDATA[ÁæéÁ†îÈô¢ÁöÑËÆ∫ÊñáÔºåÊ£ÄÊµãÔºåÁî®‰∫éËÖ∞Ê§é/È´ãÂÖ≥ËäÇÂÖ≥ÈîÆÁÇπÊèêÂèñ preparations hrnet pspModule Structured Landmark Detection via Topology-Adapting Deep Graph Learning Âä®Êú∫ landmark detection ÁâπÂæÅÁÇπÊ£ÄÊµã identify the locations of predefined fiducial points capture relationships among Ëß£ÂâñÂ≠¶ÁâπÂæÅÁÇπ ‰∏Ä‰∏™ÈöæÁÇπÔºöÈÅÆÊå°/Â§çÊùÇÂâçÊôØÁä∂ÊÄÅ‰∏ãÔºålandmarkÁöÑÂáÜÁ°ÆÊ£ÄÊµãÂíåÂÆö‰Ωç‚Äî‚Äîstructual information the proposed method Áî®‰∫éfacial and medical landmark detection topology-adaptingÔºölearnable connectivity learn end-to-end with two GCNs ËÆ∫ÁÇπ heatmap regression based methods Â∞ÜlandmarksÂª∫Ê®°ÊàêheatmapsÔºåÁÑ∂ÂêéÂõûÂΩí lacking a global representation Ê†∏ÂøÉË¶ÅÁ¥†Êúâbottom-up/top-down paths &amp; multi-scale fusions &amp; high resolution heatmap outputs coordinate regression based methods potentially incorporate structural knowledge but a lot yet to be explored falls behind heatmap-based ones Ê†∏ÂøÉË¶ÅÁ¥†ÊòØcascaded &amp; global &amp; local Â•ΩÂ§ÑÊòØÁªìÊûÑÂåñÔºå‰∏ç‰∏¢ÁÇπÔºå‰∏çÂ§öÁÇπÔºå‰ΩÜÊòØ‰∏ç‰∏ÄÂÆöÂáÜ graph methods Âü∫‰∫élandmark locationsÂíålandmark-to-landmark-relationshipsÊûÑÂª∫ÂõæÁªìÊûÑ most methods relies on heatmap detection results we would directly regress landmark locations from raw input image we propose DAGÔºödeep adaptive graph Â∞ÜlandmarksÂª∫Ê®°ÊàêgraphÂõæ employ global-to-local cascaded Graph Convolution NetworksÈÄêÊ∏êÂ∞ÜlandmarkËÅöÁÑ¶Âú®ÁõÆÊ†á‰ΩçÁΩÆ graph signals combines local image features graph shape features cascade two GCNs Á¨¨‰∏Ä‰∏™È¢ÑÊµã‰∏Ä‰∏™global transform Á¨¨‰∫å‰∏™È¢ÑÊµãlocal offsets to further adjust contributions effectively exploit the structural knowledge allow rich exchange among landmarks narrow the gap between coordinate &amp; heatmap based methods ÊñπÊ≥ï the cascaded-regression framework input image initial landmarks from the mean shape outputs predicted landmark coordinates in multiple steps feature use graph representation G = (V,E,F) VÊòØËäÇÁÇπÔºå‰ª£Ë°®landmarksÔºå‰πüÂ∞±ÊòØÁâπÂæÅÁÇπÔºåË°®Á§∫‰∏∫(x,y)ÁöÑÂùêÊ†á EÊòØËæπÔºå‰ª£Ë°®connectivity between landmarksÔºåË°®Á§∫‰∏∫(id_i, id_k)ÁöÑÊó†Âêë/ÊúâÂêëÊò†Â∞ÑÔºåÊï¥‰ΩìÁöÑE matrixÊòØ‰∏™Á®ÄÁñèÁü©Èòµ FÊòØgraph signalsÔºåcapturing appearance and shape informationÔºåË°®Á§∫‰∏∫È´òÁª¥ÂêëÈáèÔºåÂ¶Ç256-dim vecÔºå‰∏éËäÇÁÇπV‰∏Ä‰∏ÄÂØπÂ∫îÔºåÁî®‰∫éÂÇ®Â≠òËäÇÁÇπ‰ø°ÊÅØÔºåÂú®GCN‰∏≠ÂÆûÈôÖËøõË°åËÆ°ÁÆó‰∫§‰∫í overview summary cascadeÔºö‰∏Ä‰∏™GCN-globalÂÅöÁ≤óÂÆö‰ΩçÔºåËø≠‰ª£Â§ö‰∏™GCN-localÂÅöpreciseÂÆö‰Ωç interpolationÔºöfeature mapÂà∞feature nodesÁöÑËΩ¨Êç¢ÔºåÈÄöËøáinterpolationÔºå„ÄêÊòØglobal interpÂêóÔºåÊòØÂü∫‰∫éinitial mean coordsÂêó„Äë regressionÔºö„ÄêtargetsÁöÑÂÖ∑‰ΩìÂùêÊ†áË°®Á§∫ÔºüÔºüÔºü„Äë inital graphÔºöËÆ≠ÁªÉÈõÜÁöÑÂπ≥ÂùáÂÄº graph signalÔºövisual featureÂíåshape feature Cascaded GCNs GCN-globalÔºöglobal transformation GCN-localÔºöcoordinate offsets share the same GCN architecture graph convolution Ê†∏ÂøÉÊÄùÊÉ≥Â∞±ÊòØÔºöÁªôÂÆö‰∏Ä‰∏™ÂõæÁªìÊûÑÔºàwith connectivity EÔºâÔºåÊØè‰∏ÄÊ¨°Â†ÜÂè†graph convolutionÔºåÂ∞±ÊòØÂú®ÂØπÊØè‰∏™ÂõæËäÇÁÇπÔºåÂü∫‰∫éÂÖ∂Ëá™Ë∫´$f_k^i$ÂíåÈÇªÂ±ÖËäÇÁÇπ$f_k^j$ÁöÑÂΩìÂâçgraph featureÔºåweighted aggregatingÔºåÁªìÊûú‰Ωú‰∏∫Ëøô‰∏™ËäÇÁÇπËøôÊ¨°ÂõæÂç∑ÁßØÁöÑËæìÂá∫$f_{k+1}^i$ f_{k+1}^i = W_1 f_k^i + \sum_j e_{ij}W_2 f_k^j learnable weight matrices $W_1$ Âíå $W_2$ ÂèØ‰ª•Áúã‰ΩúÊòØÈÇªÂ±ÖËäÇÁÇπÈó¥‰ø°ÊÅØ‰∫§‰∫íÁöÑ‰∏ÄÁßçÊñπÂºè Global Transformation GCN Ëøô‰∏™modelÁöÑ‰ΩúÁî®ÊòØÂ∞Üinitial landmarksÂèòÊç¢Âà∞coarse targets ÂèÇÁÖßSTNÔºå recall STN ‰ΩøÁî®perspective transformationÈÄèËßÜÂèòÊç¢ÔºåÂºïÂÖ•9‰∏™scalarsÔºåËøõË°åÂõæÂΩ¢Âèò workflow given a target image initialize landmark locations $V^0$ using trainingset mean GCN-global + GIN È¢ÑÊµãperspective transformation ËøõËÄåÂæóÂà∞ÂèòÊç¢ÂêéÁöÑËäÇÁÇπ‰ΩçÁΩÆ graph isomorphism network (GIN) ÂõæÁöÑÁ∫øÊÄßÂ±Ç ËæìÂÖ•ÊòØGCN-globalÁöÑgraph features $\{f_k^i\}$ ËæìÂá∫ÊòØ9-dim vector ËÆ°ÁÆóÊñπÂºè READOUTÔºösum the features from all nodes CONCATÔºöÂæóÂà∞‰∏Ä‰∏™È´òÁª¥ÂêëÈáè MLPÔºö9-dim fc ÊúÄÂêéÂæóÂà∞9-dimÁöÑperspective transformation scalar coordinate update Â∞Ü9-dim $f^G$ reshapeÊàê3x3 transformation matrix M ÁÑ∂ÂêéÂú®ÂΩìÂâçÁöÑlandmark locations $V^0$‰∏äÊñΩÂä†ÂèòÊç¢‚Äî‚ÄîÁü©ÈòµÂ∑¶‰πò Local Refinement GCN GCNÁªìÊûÑ‰∏églobalÁöÑ‰∏ÄËá¥Ôºå‰ΩÜÊòØ‰∏çshareÊùÉÈáç ÊúÄÂêéÁöÑGINÂ§¥Âèò‰∫Ü ËæìÂá∫ÊîπÊàê2-dim vector represents coordinate offsets coordinate update Âä†Ê≥ïÔºåÂàÜÂà´Âú®x/yËΩ¥ÂùêÊ†á‰∏ä we perform T=3 iterations Graph signal with appearance and shape information Visual Feature denote CNNËæìÂá∫ÁöÑfeature map H with D channels encodingÊï¥‰∏™feature mapÔºöbi-linear interpolation at the landmark location $v_i$ÔºåËÆ∞‰Ωú$p_i$ÔºåÊòØ‰∏™D-dim vector Shape Feature visual featureÂØπËäÇÁÇπÈó¥ÂÖ≥Á≥ªÁöÑÂª∫Ê®°ÔºåÂü∫‰∫églobal mapÂÖ®Â±Ä‰ø°ÊÅØÊèêÂèñÔºåÊØîËæÉÈöêÂºè„ÄÅÈó¥Êé• ‰∫ãÂÆû‰∏äÂõæÁªìÊûÑËÉΩÂ§üÁõ¥Êé•ÂØπglobal landmarks shapeËøõË°åencoding Êú¨ÊñáÁî®displacement vectorsÔºåÂ∞±ÊòØË∑ùÁ¶ªÔºåÊØè‰∏™ËäÇÁÇπÁöÑdisplacement vectorËÆ∞‰Ωú$q_i=\{v_j-v_i\}_{j!=i}$ÔºåflattenÊàê‰∏ÄÁª¥ÔºåÂØπÊúâN‰∏™ËäÇÁÇπÁöÑÂõæÔºåÊØè‰∏™ËäÇÁÇπÁöÑq-vecÁª¥Â∫¶‰∏∫2*(N-1) shape feature‰øùÂ≠ò‰∫Üstructural informationÔºåÂΩì‰∫∫ËÑ∏ÁöÑÂò¥Ë¢´ÈÅÆ‰ΩèÁöÑÊÉÖÂÜµ‰∏ãÔºåÂü∫‰∫éÁúºÁùõÂíåÈºªÂ≠ê‰ª•ÂèäÁªìÊûÑÊÄß‰ø°ÊÅØÔºåÂ∞±ËÉΩÂ§üÊé®Êñ≠Âò¥ÁöÑ‰ΩçÁΩÆÔºåËøôÊòØVisual Feature‰∏çËÉΩÁõ¥Êé•Ë°®ËææÁöÑ graph signal concat result in a feature vector $f_i \in R^{D+2(N-1)}$ Landmark graph with learnable connectivity Â§ßÂ§öÊï∞ÊñπÊ≥ïÁöÑÂõæÂü∫‰∫éÂÖàÈ™åÁü•ËØÜÊûÑÂª∫ we learn task-specific graph connectivity during training phase ÂõæÁöÑconnectivity serves as a gateÔºåÁî®ÈÇªÊé•Áü©ÈòµË°®Á§∫ÔºåÂπ∂Â∞ÜÂÖ∂‰Ωú‰∏∫learnable weights training GCN-global margin loss $v_i^1$ÊòØGCN-globalÁöÑÈ¢ÑÊµãËäÇÁÇπÂùêÊ†á mÊòØmargin $[u]_+$ÊòØ$max(0,u)$ pushËäÇÁÇπÂùêÊ†áÂà∞ÊØîËæÉÊé•Ëøëground truthÂ∞±ÂÅúÊ≠¢‰∫ÜÔºåÈò≤Ê≠¢‰∏çÁ®≥ÂÆö GCN-local L1 loss $v_i^T$ÊòØÁ¨¨T‰∏™iteration GCN-localÁöÑÈ¢ÑÊµãËäÇÁÇπÂùêÊ†á overall loss Âä†ÊùÉÂíå ÁΩëÁªúÁªìÊûÑ GCN-global ‰∏âÂ±Çbasic graph convolution layer with residualÔºàid pathÔºâ concat distance vector ‰∏ÄÂ±Çbasic graph convolution mean axis1Ôºànode axisÔºâ fcÔºåËæìÂá∫9-dim scalarÔºå(b,9) GCN-local ‰∏âÂ±Çbasic graph convolution layer with residualÔºàid pathÔºâ relu concat distance vector ‰∏ÄÂ±Çbasic graph convolution fcÔºåËæìÂá∫2-dim coords for each nodeÔºå(b,24,2)]]></content>
      <tags>
        <tag>GCN, landmark detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KL Divergence]]></title>
    <url>%2F2021%2F09%2F27%2FKL-Divergence%2F</url>
    <content type="text"><![CDATA[KL divergenceÁî®‰∫éÂ∫¶Èáè‰∏§‰∏™ÂàÜÂ∏ÉPÂíåQÁöÑÂ∑ÆÂºÇÔºåËøôÁßçÂ∫¶Èáè„Äê‰∏çÂÖ∑Êúâ„ÄëÂØπÁß∞ÊÄß PÊòØÂÆûÈôÖÂàÜÂ∏ÉÔºàpred probsÔºâ QÊòØÂª∫Ê®°ÂàÜÂ∏ÉÔºàgtÔºâ $D_{KL}(P||Q)=\sum_i P(i)ln\frac{P(i)}{Q(i)}$ Êï£Â∫¶ÂÆö‰πâ‰∏∫ÂàÜÂ∏ÉPÂíåÂàÜÂ∏ÉQ‰πãÈó¥ÁöÑÂØπÊï∞Â∑ÆÂºÇÁöÑÂä†ÊùÉÂíåÔºåÁî®PÁöÑÊ¶ÇÁéáÂéªÂä†ÊùÉ ÂΩìQÊòØone-hot labelÁöÑÊó∂ÂÄôÔºåË¶ÅÂÖàclipÂÜçlog ÊñπÊ≥ï torch.nn.functional.kl_div(input, target, size_average=None, reduce=None, reduction=‚Äômean‚Äô) inputÔºöÂØπÊï∞Ê¶ÇÁéá targetÔºöÊ¶ÇÁéá tf.distributions.kl_divergence(distribution_a, distribution_b, allow_nan_stats=True, name=None) distribution_a&amp;b Êù•Ëá™tf.distributions.Categorical(logits=None, prob=None, ‚Ä¶) ‰º†ÂÖ•logits/probsÔºåÂÖàËΩ¨Êç¢ÊàêdistributionÔºåÂÜçËÆ°ÁÆókl divergence torch.nn.KLDivLoss tf.keras.losses.KLDivergence tf.keras.losses.kullback_leibler_divergence code 12345678910111213141516171819202122232425# torch versionimport torch.nn as nnimport torch.nn.functional as Fclass KL(nn.Module): def __init__(self, args): super(KL, self).__init__() self.T = args.temperature def forward(self, logits_p, logits_q): log_p = F.log_softmax(logits_p/self.T, dim=1) q = F.softmax(logits_q/self.T, dim=1) loss = F.kl_div(log_p, p_t) # keras versionimport tensorflow as tfimport keras.backend as Kdef kl_div(logits_p, logits_q): T = 4. log_p = tf.nn.log_softmax(logits_p/T) # (b,cls) log_q = tf.nn.log_softmax(logits_q/T) p = K.exp(log_p) return K.sum(p*(log_p-log_q), axis=-1) # (b,)]]></content>
      <tags>
        <tag>Êï∞Â≠¶</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Self-Knowledge Distillation]]></title>
    <url>%2F2021%2F09%2F17%2FSelf-Knowledge-Distillation%2F</url>
    <content type="text"><![CDATA[Refine Myself by Teaching Myself : Feature Refinement via Self-Knowledge Distillation Âä®Êú∫ ‰º†ÁªüÁöÑÁü•ËØÜËí∏È¶è by stageÔºöÂÖàËÆ≠ÁªÉÂ∫ûÂ§ßÁöÑteacher self knowledge distillation without the pretrained network ÂàÜ‰∏∫data augmentation based approach Âíå auxiliary network based approach data augmentation approachÂ¶ÇUDAÔºåÈÄöËøáÁõëÁù£ÂéüÂßãÂõæÂÉèÂíåÂ¢ûÂº∫ÂõæÂÉèÁöÑ‰∏ÄËá¥ÊÄßÔºå‰ΩÜÊòØ‰ºöloose local informationÔºåÂØπpixel-level tasks‰∏çÂèãÂ•ΩÔºåËÄå‰∏îÁõëÁù£‰ø°ÊÅØÊòØ‰ªélogitsÂ±ÇÔºåÊ≤°ÊúâÁõ¥Êé•Âéªrefine feature maps our approach FRSKD auxiliary network based approach utilize both soft label and featuremap distillation ËÆ∫ÁÇπ various distillation methods aÊòØ‰º†ÁªüÁü•ËØÜËí∏È¶èÔºåÊ∑±ÁªøËâ≤ÊòØpretrained teacherÔºåÊµÖÁªøËâ≤ÊòØstudentÔºåÊ©ôËâ≤ÁÆ≠Â§¥ÊòØfeatureËí∏È¶èÔºåÁªøËâ≤ÁÆ≠Â§¥ÊòØsoft labelËí∏È¶è bÊòØdata augmentation based Ëá™Ëí∏È¶èÔºåshared ÁΩëÁªúÔºåÂéüÂõæÂíåÂ¢ûÂº∫ÂêéÁöÑÂõæÔºåÁî®soft logitsÊù•Ëí∏È¶è cÊòØauxiliary classifier based Ëá™Ëí∏È¶èÔºåcascadedÂàÜÁ±ªÂ§¥ÔºåÊØè‰∏™ÂàÜÁ±ªÂô®ÈÉΩÊé•Ââç‰∏Ä‰∏™ÁöÑ dÊòØÊú¨ÊñáËá™Ëí∏È¶èÔºåÂíåcÊúÄÂ§ßÁöÑ‰∏çÂêåÊòØbifpnÁªìÊûÑ‰ΩøÂæó‰∏§‰∏™ÂàÜÁ±ªÂô®ÊØè‰∏™levelÁöÑÁâπÂæÅÂõæ‰πãÈó¥ÈÉΩÊúâËøûÁªìÔºåÁõëÁù£ÊñπÂºè‰∏ÄÊ†∑ÁöÑ FPN PANetÔºö‰∏äË°å+‰∏ãË°å biFPNÔºö‰∏äË°å+‰∏ãË°å+ÂêåÂ±ÇÁ∫ßËÅî ÊñπÊ≥ï overview notations dataset $D=\{(x_1,y_1), (x_2,y_2),‚Ä¶, (x_N,y_N)\}$ feature map $F_{i,j}$Ôºåi-th sampleÔºåj-th block channel dimension $c_j$Ôºåj-th block self-teacher network self-teacher networkÁöÑÁõÆÁöÑÊòØÊèê‰æõrefined feature mapÂíåsoft labels‰Ωú‰∏∫ÁõëÁù£‰ø°ÊÅØ inputsÔºöfeature maps $F_1, F_2, ‚Ä¶, F_n$Ôºå‰πüÂ∞±ÊòØËØ¥teacherÂú®ËøõË°åÊ¢ØÂ∫¶Âõû‰º†ÁöÑÊó∂ÂÄôÂà∞FÂ∞±ÂÅúÊ≠¢‰∫ÜÔºå‰∏ç‰ºöÊõ¥Êñ∞student modelÁöÑÂèÇÊï∞ modified biFPN Á¨¨‰∏Ä‰∏™‰∏çÂêåÔºöÂà´ÁöÑFPNÈÉΩÊòØÂú®fuse‰πãÂâçÂÖàÁî®‰∏Ä‰∏™fixed-dim 1x1 convÂ∞ÜÊâÄÊúâlevelÁöÑfeature mapËΩ¨Êç¢ÊàêÁõ∏ÂêåÈÄöÈÅìÊï∞ÔºàÂ¶Ç256ÔºâÔºåwe design $d_i$ according to $c_i$ÔºåÂºïÂÖ•‰∏Ä‰∏™ÂÆΩÂ∫¶Á≥ªÊï∞widthÔºå$d_i=width*c_i$Ôºå Á¨¨‰∫å‰∏™‰∏çÂêåÔºö‰ΩøÁî®depth-wise convolution notations BiFPNÔºöÊØèÂ±ÇdimÂõ∫ÂÆöÁöÑÁâàÊú¨ BiFPNcÔºöÊØèÂ±ÇdimÈöèËæìÂÖ•ÂèòÂåñÁöÑÁâàÊú¨ self-feature distillation feature distillation adapt attention transfer ÂØπfeature mapÂÖàËøõË°åchannel-wiseÁöÑpoolingÔºåÁÑ∂ÂêéL2 normÔºåÊèêÂèñspatial information soft label distillation ‰∏§‰∏™ÂàÜÁ±ªÂ§¥ÁöÑKL divergence CE with gt ‰∏§‰∏™ÂàÜÁ±ªÂ§¥ÂàÜÂà´ËøòÊúâÊ≠£Â∏∏ÁöÑCE loss overall ÊÄªÁöÑlossÊòØ4‰∏™lossÁõ∏Âä†Ôºö$L_{FRSKD}(x,y,\theta_c, \theta_t, K)=L_{CE}(x,y,\theta_c)+L_{CE}(x,y,\theta_t)+\alpha L_{KD}(x,\theta_c,\theta_t, K) + \beta L_{F}(T,F,\theta_c,\theta_T)$ $\alpha \in [1,2,3]$ $\beta \in [100,200]$ „ÄêQUESTION„ÄëFRSKD updates the parameters by the distillation lossÔºå$L_{KD}$ and $L_F$Ôºåwhich is only applied to the student networkÔºåËøô‰∏™Âï•ÊÑèÊÄùÊöÇÊó∂Ê≤°ÁêÜËß£ ÂÆûÈ™å experiment settings FRSKD\FÔºöÂè™ÂÅösoft labelÁöÑÁõëÁù£Ôºå‰∏çÂÅöfeature mapÁöÑÁõëÁù£ FRSKDÔºöÊ†áÂáÜÁöÑÊú¨ÊñáÊñπÊ≥ï FRSKD+SLAÔºöÊú¨ÊñáÊñπÊ≥ïÁöÑÂü∫Á°Ä‰∏äattach data augmentation based distillation]]></content>
  </entry>
  <entry>
    <title><![CDATA[L2 Regularization and Batch Norm]]></title>
    <url>%2F2021%2F09%2F16%2FL2-Regularization-and-Batch-Norm%2F</url>
    <content type="text"><![CDATA[referenceÔºö https://blog.janestreet.com/l2-regularization-and-batch-norm/ https://zhuanlan.zhihu.com/p/56142484 https://vitalab.github.io/article/2020/01/24/L2-reg-vs-BN.html Ëß£Èáä‰∫Ü‰πãÂâçÁöÑ‰∏Ä‰∏™ÁñëÁÇπÔºö Âú®kerasËá™ÂÆö‰πâÁöÑBNÂ±Ç‰∏≠ÔºåÊ≤°ÊúâÁ±ª‰ººkernel_regularizerËøôÊ†∑ÁöÑÂèÇÊï∞ Âú®Êàë‰ª¨ÂÜôËá™ÂÆö‰πâoptmizerÁöÑÊó∂ÂÄôÔºåBNÂ±Ç‰πü‰∏çËøõË°åweight decayÁöÑ L2 Regularization versus Batch and Weight Normalization Âä®Êú∫ ‰∏§‰∏™common tricksÔºöNormalizationÔºàBN„ÄÅWN„ÄÅLNÁ≠âÔºâÂíåL2 Regularization ÂèëÁé∞‰∏§ËÄÖÁªìÂêàÊó∂L2 regularizationÂØπnormalizationÂ±ÇÊ≤°ÊúâÊ≠£ÂàôÊïàÊûú L2 regularizationÂèçËÄåÂØπnorm layerÁöÑscaleÊúâÂΩ±ÂìçÔºåÈó¥Êé•ÂΩ±Âìç‰∫Ülearning rate Áé∞‰ª£‰ºòÂåñÂô®Â¶ÇAdamÂè™ËÉΩÈó¥Êé•Ê∂àÈô§ËøôÁßçÂΩ±Âìç ËÆ∫ÁÇπ BN popular in training deep networks solve the problem of covariate shift ‰ΩøÂæóÊØè‰∏™Á•ûÁªèÂÖÉÁöÑËæìÂÖ•‰øùÊåÅnormalÂàÜÂ∏ÉÔºåÂä†ÈÄüËÆ≠ÁªÉ mean &amp; varianceÔºötraining timeÂü∫‰∫éÊØè‰∏™mini-batchËÆ°ÁÆóÔºåtest time‰ΩøÁî®ÊâÄÊúâiterationÁöÑmean &amp; varianceÁöÑEMA usually trained with SGD with L2 regularization result in weight decayÔºö‰ªéÊï∞Â≠¶Ë°®Á§∫‰∏äÁ≠â‰ª∑‰∫éÂØπÊùÉÈáçÂÅöË°∞Âáè ÊØè‰∏ÄÊ≠•ÊùÉÈáçscaled by a Â∞è‰∫é1ÁöÑÊï∞ ‰ΩÜÊòØnormalization strategiesÊòØÂØπscale of the weights invariantÁöÑÔºåÂõ†‰∏∫Âú®ËæìÂÖ•Á•ûÁªèÂÖÉ‰πãÂâçÈÉΩ‰ºöËøõË°ånorm therefore there is no regularizing effect rather strongly influence the learning rate??üëÇ L2 Regularization formulationÔºö Âú®lossÁöÑÂü∫Á°Ä‰∏äÂä†‰∏Ä‰∏™regularization termÔºå$L_{\lambda}(w)=L(w)+\lambda ||w||^2_2$ lossÊòØÊØè‰∏™Ê†∑Êú¨ÁªèËøá‰∏ÄÁ≥ªÂàóÊùÉÈáçËøêÁÆóÔºå$L(w)=\sum_N l_i (y(X_i;w,\gamma,\beta))$ ÂΩì‰ΩøÁî®normalization layerÁöÑÊó∂ÂÄôÔºö$y(X_i;w,\alpha,\beta)=y(X_i;\alpha w,\gamma,\beta)$ÔºåÂç≥loss term‰∏ç‰ºöÂèò $L_{\lambda}(\alpha w)=L(w)+\lambda||w||^2_2$ Âú®Êúânormalization layerÁöÑÊó∂ÂÄôÔºåL2 penaltyËøòÊòØËÉΩÂ§üÈÄöËøáreg term forceÊùÉÈáçÁöÑscaleË∂äÊù•Ë∂äÂ∞èÔºå‰ΩÜÊòØ‰∏ç‰ºöÂΩ±Âìç‰ºòÂåñËøõÁ®ãÔºà‰∏çÂΩ±Âìçmain objective valueÔºâÔºåÂõ†‰∏∫loss term‰∏çÂèò Effect of the Scale of Weights on Learning Rate BNÂ±ÇÁöÑËæìÂá∫ÊòØscale invariantÁöÑÔºå‰ΩÜÊòØÊ¢ØÂ∫¶‰∏çÊòØÔºåÊ¢ØÂ∫¶ÊòØÊàêÂèçÊØîË¢´ÊäëÂà∂ÁöÑÔºÅ ÊâÄ‰ª•weightsÂú®ÂèòÂ∞èÔºåÂêåÊó∂Ê¢ØÂ∫¶Âú®ÂèòÂ§ßÔºÅ Âú®ÂáèÂ∞èweight scaleÁöÑÊó∂ÂÄôÔºåÁΩëÁªúÁöÑÊ¢ØÂ∫¶‰ºöÂèòÂ§ßÔºåÁ≠â‰ª∑‰∫éÂ≠¶‰π†ÁéáÂú®ÂèòÂ§ßÔºå‰ºöÂºïËµ∑ÈúáËç°‰∏çÁ®≥ÂÆö ÊâÄ‰ª•Âú®ËÆæÂÆöhyperÁöÑÊó∂ÂÄôÔºåÂ¶ÇÊûúÊàë‰ª¨Ë¶ÅÈÄÇÂΩìÂä†Â§ßweight decay $\lambda$ÔºåÂ∞±Ë¶ÅÂèçÊØîscaleÂ≠¶‰π†Áéá Effect of Regularization on the Scale of Weights during training the scale of weights will change the gradients of the loss function will cause the norm of the weights to grow the regularization term causes the weights to shrink]]></content>
      <tags>
        <tag>Ê≠£ÂàôÂåñ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SAM]]></title>
    <url>%2F2021%2F09%2F10%2FSAM%2F</url>
    <content type="text"><![CDATA[google brainÔºåÂºïÁî®Èáè51Ôºå‰ΩÜÊòØImageNetÊ¶úÂçï/SOTAÊ®°ÂûãÁöÑÂØπÊØîÂÆûÈ™åÈáåÈù¢ÁªèÂ∏∏ËÉΩÂ§üÁúãÂà∞Ëøô‰∏™SAMÔºåÂá∫ÂúàÂΩ¢Âºè‰∏∫ÂàÜÁ±ªÊ®°Âûã+SAM SAMÔºöSharpness-Aware MinimizationÔºåÈîêÂ∫¶ÊÑüÁü•ÊúÄÂ∞èÂåñ official repoÔºöhttps://github.com/google-research/sam Sharpness-Aware Minimization for Efficiently Improving Generalization Âä®Êú∫ heavily overparametered modelsÔºötraining lossËÉΩËÆ≠Âà∞ÊûÅÂ∞èÔºå‰ΩÜÊòØgeneralization issue we propose Sharpness-Aware Minimization (SAM) ÂêåÊó∂ÊúÄÂ∞èÂåñlossÂíåloss sharpness improve model generalization robustness to label noise verified on CIFAR 10&amp;100 ImageNet finetuning tasks ËÆ∫ÁÇπ typical loss &amp; optimizer population lossÔºöÊàë‰ª¨ÂÆûÈôÖÊÉ≥ÂæóÂà∞ÁöÑÊòØÂú®ÂΩìÂâçËÆ≠ÁªÉÈõÜÊâÄ‰ª£Ë°®ÁöÑÂàÜÂ∏É‰∏ãÁöÑÊúÄ‰ºòËß£ training set lossÔºö‰ΩÜ‰∫ãÂÆû‰∏äÊàë‰ª¨Âè™ËÉΩÁî®ÊâÄÊúâÁöÑËÆ≠ÁªÉÊ†∑Êú¨Êù•‰ª£Ë°®Ëøô‰∏™ÂàÜÂ∏É Âõ†‰∏∫lossÂáΩÊï∞ÊòØnon-convexÁöÑÔºåÊâÄ‰ª•ÂèØËÉΩÂ≠òÂú®Â§ö‰∏™local even global minimaÂØπÂ∫îÁöÑloss valueÊòØ‰∏ÄÊ†∑ÁöÑÔºå‰ΩÜÊòØgeneralization performanceÁ°ÆÊòØ‰∏çÂêåÁöÑ ÊàêÁÜüÁöÑÂÖ®Â•óÈò≤Ê≠¢ËøáÊãüÂêàÊâãÊÆµ loss optimizer dropout batch normalization mixed sample augmentations our approach directly leverage the geometry of the loss landscape and its connection to generalization (generalization bound) proved additive to existing techniques ÊñπÊ≥ï motivation rather than ÂØªÊâæ‰∏Ä‰∏™weight value that have low lossÔºåÊàë‰ª¨ÂØªÊâæÁöÑÊòØÈÇ£ÁßçËøûÂ∏¶‰ªñ‰∏¥ËøëÁöÑvalueÈÉΩËÉΩÊúâlow lossÁöÑvalue ‰πüÂ∞±ÊòØÊó¢Êúâlow lossÂèàÊúâlowÊõ≤Â∫¶ sharpness term $\max \limits_{||\epsilon||_p &lt; \rho} L_s(w+\epsilon) - L_s(w)$ Ë°°ÈáèÊ®°ÂûãÂú®wÂ§ÑÁöÑsharpness Sharpness-Aware Minimization (SAM) formulation sharpness termÂÜçÂä†‰∏ätrain lossÂÜçÂä†‰∏äregularization term $L_S^{SAM}(w)=\max\limits_{a} L_s(w+\epsilon)$ $\min \limits_{w} L_S^{SAM}(w) + \lambda ||w||^2_2$ prevent the model from converting to a sharp minimum effective approximation bound with $\frac{1}{p} + \frac{1}{q} = 1$ approximation pseudo code given a min-batch È¶ñÂÖàËÆ°ÁÆóÂΩìÂâçbatchÁöÑtraining lossÔºåÂíåÂΩìÂâçÊ¢ØÂ∫¶Ôºå$w_t$ to $w_{t+1}$ ÁÑ∂ÂêéËÆ°ÁÆóËøë‰ºº‰∏∫Ê¢ØÂ∫¶normÁöÑÊ≠•Èïø$\hat\epsilon(w)$Ôºåequation2Ôºå$w_t$ to $w_{adv}$ÔºåËøôÈáåÈù¢ÁöÑadvËÅîÂä®‰∫ÜÂè¶‰∏ÄÁØáËÆ∫Êñá„ÄäAdvProp: Adversarial Examples Improve Image Recognition„Äã ÁÑ∂ÂêéËÆ°ÁÆóËøë‰ººÁöÑsharpness termÔºåÂèØ‰ª•ÁêÜËß£‰∏∫training lossÂú®wÈÇªÂ±ÖÂ§ÑÁöÑÊ¢ØÂ∫¶Ôºåequation3ÔºåÂ∫îËØ•ÊòØËìùËâ≤ÁÆ≠Â§¥ÁöÑÂèçÊñπÂêëÔºåÂõæ‰∏äÊ≤°Ê†áËÆ∞Âá∫Êù• Áî®wÈÇªÂ±ÖÁöÑÊ¢ØÂ∫¶Êù•Êõ¥Êñ∞wÁöÑÊùÉÈáçÔºåÁî®Ë¥üÊ¢ØÂ∫¶ÔºàËìùËâ≤ÁÆ≠Â§¥Ôºâ overllÂ∞±ÊòØÔºöË¶ÅÂêëÂâçËµ∞‰πãÂâçÔºåÂÖàÂõûÈÄÄÔºåÁº∫ÁÇπÊòØ‰∏§Ê¨°Ê¢ØÂ∫¶ËÆ°ÁÆóÔºåÊó∂Èó¥double ÂÆûÈ™åÁªìËÆ∫ ËÉΩ‰ºòÂåñÂà∞ÊçüÂ§±ÁöÑÊúÄÂπ≥Âù¶ÁöÑÊúÄÂ∞èÂÄºÁöÑÂú∞ÊñπÔºåÂ¢ûÂº∫Ê≥õÂåñËÉΩÂäõ]]></content>
  </entry>
  <entry>
    <title><![CDATA[MuSTË∞∑Ê≠åÂ§ö‰ªªÂä°Ëá™ËÆ≠ÁªÉ]]></title>
    <url>%2F2021%2F09%2F01%2FMuST%2F</url>
    <content type="text"><![CDATA[recollect [SimCLR] [MoCo] Multi-Task Self-Training for Learning General Representations Âä®Êú∫ learning general feature representations expect a single general model Áõ∏ÊØîËæÉ‰∫étraining specialized models for various tasks harness from independent specialized teacher models with a multi-task pseudo dataset trained with multi-task learning evalutate on 6 vision tasks image recognition (classification, detection, segmentation) 3D geometry estimation ËÆ∫ÁÇπ pretraining &amp; transfer learning transformer‰∏ÄËà¨ÈÉΩÊòØËøô‰∏™Â•óË∑ØÔºåBiT&amp;ViT pretraining supervised / unsupervised learn feature representations transfer learning on downstream tasks the features may not necessarily be useful ÊúÄÂÖ∏ÂûãÁöÑÂ∞±ÊòØImageNet pre-trainingÂπ∂‰∏çËÉΩimprove COCO segmentationÔºå‰ΩÜÊòØObjects365ËÉΩÂ§üÂ§ßÂπÖÊèêÂçá pretraining tasksÂøÖÈ°ªË¶ÅÂíådownstream task alignÔºålearn specialized featuresÔºå‰∏çÁÑ∂ÁôΩË¥π learning general features a model simultaneously do well on multiple tasks NLPÁöÑbertÊòØ‰∏Ä‰∏™ÂÖ∏ÂûãÁî®Â§ö‰ªªÂä°ÊèêÂçágeneral abilityÁöÑ CVÊØîËæÉÈöæËøôÊ†∑ÂÅöÊòØÂõ†‰∏∫Ê†áÁ≠ævarietyÔºåÊ≤°ÊúâËøôÊ†∑ÁöÑÂ§ßÂûãmulti-task dataset multi-task learning shared backbone (Â¶ÇResNet-FPN) small task-specific heads self-training use a supervised model to generate pseudo labels on unlabeled data then a student model is trained on the pseudo labeled data Âú®ÂêÑÁ±ª‰ªªÂä°‰∏äÈÉΩprovedÊ∂®ÁÇπ ‰ΩÜÊòØËøÑ‰ªä‰∏∫Ê≠¢ÈÉΩÊòØfocused on a single task in this work lack of large scale multi-task datasetÁöÑissueÔºåÈÄöËøáself-training to fixÔºåÁî®pseudo label specialized/general issueÔºåÈÄöËøáÂ§ö‰ªªÂä°ÔºåËÆ≠ÁªÉÁõÆÊ†áÂ∞±ÊòØÂÖ≠ËæπÂΩ¢ÊàòÂ£´Ôºåabsorb the knowledge of different tasks in the shared backbone three steps trains specialized teachers independently on labeled datasets ÔºàÂàÜÁ±ª„ÄÅÂàÜÂâ≤„ÄÅÊ£ÄÊµã„ÄÅÊ∑±Â∫¶‰º∞ËÆ°Ôºâ the specialized teachers are then used to label a larger unlabeled datasetÔºàImageNetÔºâ to create a multi- task pseudo labeled dataset train a student model with multi-task learning MuSTÁöÑÁâπË¥® improve with more unlabeled dataÔºåÊï∞ÊçÆË∂äÂ§ögeneral featureË∂äÂ•Ω can improve upon already strong checkpointsÔºåÂú®Êµ∑ÈáèÁõëÁù£È´òÁ≤æÂ∫¶Ê®°ÂûãÂü∫Á°Ä‰∏äfine-tuneÔºå‰ªçÊóßËÉΩÂú®downstream tasksÊ∂®ÁÇπ ÊñπÊ≥ï Specialized Teacher Models 4 teacher models classificationÔºötrain from scratchÔºåImageNet detectionÔºötrain from scratchÔºåObject365 segmentationÔºötrain from scratchÔºåCOCO depth estimationÔºöfine-tuning from pre-trained checkpoint pseudo labeling unlabeled / partially labeled datasets for detectionÔºöhard score threshold of 0.5 for segmentationÔºöhard score threshold of 0.5 for classificationÔºösoft labels‚Äî‚Äîprobs distribution for depthÔºöÁõ¥Êé•Áî® Multi-Task Student Model Ê®°ÂûãÁªìÊûÑ shared back C5Ôºöfor classification feature pyramids {P3,P4,P5,P6,P7}Ôºöfor detection fused P2Ôºöfor pixel-wise predictionÔºåÊääfeature pyramids rescaleÂà∞level2ÁÑ∂Âêésum heads classification headÔºöResNet designÔºåGAP C5 + Á∫øÊÄßÂ±Ç object detection taskÔºöMask R-CNN designÔºåRPNÊòØ2 hidden convsÔºåFast R-CNNÊòØ4 hidden convs + 1 fc pixel-wise prediction headsÔºö3 hiddent convs + 1 linear conv headÔºåÂàÜÂâ≤ÂíåÊ∑±Â∫¶‰º∞ËÆ°‰ªªÂä°independentÔºå‰∏çshare heads Teacher-student training using the same architecture same data augmentation teacherÂíåstudentÁöÑmain differenceÂ∞±ÊòØdatasetÂíålabels Learning From Multiple Teachers every image has supervision for all tasks labels may come from supervised or pseudo labels Â¶ÇÊûú‰ΩøÁî®ImageNetÊï∞ÊçÆÈõÜÔºåclassificationÂ∞±ÊòØÁúüÊ†áÁ≠æÔºådet/seg/depth supervisionÂàôÊòØ‰º™Ê†áÁ≠æ balance the loss contribution Âä†ÊùÉÂíåÔºåtask-specific weights for ImageNetÔºåuse $w_i = \frac{b_slr_{it}}{b_{it}lr_{s}}$ follow the scaling ruleÔºölrÂíåbatch sizeÊàêÊ≠£ÊØî except for depth loss Cross Dataset Training training across ImageNet, object365 and COCO ÊúâÊ†áÁ≠æÁöÑÂ∞±Áî®ÂéüÊ†áÁ≠æÔºåÊ≤°ÊúâÁöÑÁî®‰º™Ê†áÁ≠æÔºåsupervised labels and pseudo labels are treated equallyÔºåËÄå‰∏çÊòØÂàÜÂà´ÈááÊ†∑ÂíåËÆ≠ÁªÉ balance the datasetsÔºöÂêàÂú®‰∏ÄËµ∑ÁÑ∂ÂêéÂùáÂåÄÈááÊ†∑ Transfer Learning ÂæóÂà∞general student model‰ª•ÂêéÔºåfine-tune on ‰∏ÄÁ≥ªÂàódownstream tasks Ëøô‰∫õdownstream datasets‰∏éMuST modelÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈÉΩÊòØnot alignÁöÑ Ëøô‰∏™ÂÆûÈ™åË¶ÅËØÅÊòéÁöÑÊòØsupervised modelÔºàÂ¶Çteacher modelÔºâÂíåself-supervised modelÔºàÂ¶ÇÁî®pseudo labelËÆ≠ÁªÉÂá∫Êù•ÁöÑstudent modelÔºâÔºåÂú®downstream tasks‰∏äËøÅÁßªÂ≠¶‰π†ËÉΩperformanceÊòØÂ∑Æ‰∏çÂ§öÁöÑÔºå„ÄêÊ≥®ÊÑè‚ö†Ô∏èÔºöÂ¶ÇÊûúËøÅÁßªdatasetsÂâçÂêéalignÂ∞±‰∏çÊòØËøôÊ†∑‰∫ÜÔºåpretrainÊòæÁÑ∂‰ºöÊõ¥Â•ΩÔºÅÔºÅÔºÅ„Äë]]></content>
      <tags>
        <tag>multi-taskÔºåself-trainingÔºå</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GHM]]></title>
    <url>%2F2021%2F08%2F31%2FGHM%2F</url>
    <content type="text"><![CDATA[families: [class-imbalanced CE] [focal loss] [generalized focal loss] focal loss(CE)ÁöÑËøûÁª≠ÁâàÊú¨ [ohem] keras implementation: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def weightedCE_loss(y_true, y_pred): alpha = .8 pt = K.abs(y_true-y_pred) # clip pt = K.clip(pt, K.epsilon(), 1-K.epsilon()) # ce ce = -K.log(1.-pt) # pos/neg reweight wce = tf.where(y_true&gt;0.5, alpha* , (1-alpha)* ) return wcedef focal_loss(y_true, y_pred): alpha = .25 gamma = 2 pt = K.abs(y_true-y_pred) # clip pt = K.clip(pt, K.epsilon(), 1-K.epsilon()) # easy/hard reweight fl = -K.pow(pt, gamma) * K.log(1.-pt) # pos/neg reweight fl = tf.where(y_true&gt;0.5, alpha*fl, (1-alpha)*fl) return fl def generalized_focal_loss(y_true, y_pred): # CE = -ytlog(yp)-(1-yt)log(1-yp) # GFL = |yt-yp|^beta * CE beta = 2 # clip y_pred y_pred = K.clip(y_pred, K.epsilon(), 1-K.epsilon()) # ce ce = -y_true*K.log(y_pred) - (1-y_true)*K.log(1-y_pred) # [N,C] # easy/hard reweight gfl = K.pow(K.abs(y_true-y_pred), beta) * ce return gfldef ce_ohem(y_true, y_pred): pt = K.abs(y_true-y_pred) # clip pt = K.clip(pt, K.epsilon(), 1-K.epsilon()) # ce ce = -K.log(1.-pt) # sort loss k = 50 ohem_loss, indices = tf.nn.top_k(ce, k=k) # topk loss: [k,], topk indices: [k,], idx among 0-b mask = tf.where(ce&gt;=ohem_loss[k-1], tf.ones_like(ce), tf.zeros_like(ce)) return mask*ce Gradient Harmonized Single-stage Detector Âä®Êú∫ one-stage detector Ê†∏ÂøÉchallengeÂ∞±ÊòØimbalance issue imbalance between positives and negatives imbalance between easy and hard examples Ëøô‰∏§È°πÈÉΩËÉΩÂΩíÁªì‰∏∫ÂØπÊ¢ØÂ∫¶ÁöÑ‰ΩúÁî®Ôºöa term of the gradient we propose a novel gradient harmonizing mechanism (GHM) balance the gradient flow easy to embed in cls/reg losses like CE/smoothL1 GHM-C for anchor classification GHM-R for bounding box refinement proved substantial improvement on COCO 41.6 mAP surpass FL by 0.8 ËÆ∫ÁÇπ imbalance issue easy and hardÔºö OHEM directly abandon examples ÂØºËá¥ËÆ≠ÁªÉ‰∏çÂÖÖÂàÜ positive and negative focal loss Êúâ‰∏§‰∏™Ë∂ÖÂèÇÔºåË∑üdata distributionÁªëÂÆö not adaptive ÈÄöÂ∏∏Ê≠£Ê†∑Êú¨Êó¢ÊòØÂ∞ëÈáèÊ†∑Êú¨ÂèàÊòØÂõ∞ÈöæÊ†∑Êú¨ÔºåËÄå‰∏îÂèØ‰ª•ÈÄöÈÄöÂΩíÁªì‰∏∫Ê¢ØÂ∫¶ÂàÜÂ∏É‰∏çÂùáÂåÄÁöÑÈóÆÈ¢ò Â§ßÈáèÊ†∑Êú¨Âè™Ë¥°ÁåÆÂæàÂ∞èÁöÑÊ¢ØÂ∫¶ÔºåÈÄöÂ∏∏ÂØπÂ∫îÁùÄÂ§ßÈáèË¥üÊ†∑Êú¨ÔºåÊÄªÈáèÂ§ö‰∫Ü‰πüÂèØËÉΩ‰ºöÂºïÂØºÊ¢ØÂ∫¶ÔºàÂ∑¶ÂõæÔºâ hardÊ†∑Êú¨Ë¶ÅÊØîmediumÊ†∑Êú¨Êï∞ÈáèÂ§ßÔºåÊàë‰ª¨ÈÄöÂ∏∏Â∞ÜÂÖ∂Áúã‰ΩúÁ¶ªÁæ§ÁÇπÔºåÂõ†‰∏∫Ê®°ÂûãÁ®≥ÂÆö‰ª•ÂêéËøô‰∫õhard examples‰ªçÊóßÂ≠òÂú®Ôºå‰ªñ‰ª¨‰ºöÂΩ±ÂìçÊ®°ÂûãÁ®≥ÂÆöÊÄßÔºàÂ∑¶ÂõæÔºâ GHMÁöÑÁõÆÊ†áÂ∞±ÊòØÂ∏åÊúõ‰∏çÂêåÊ†∑Êú¨ÁöÑgradient contribution‰øùÊåÅharmonyÔºåÁõ∏ÊØîËæÉ‰∫éCEÂíåFLÔºåÁÆÄÂçïÊ†∑Êú¨ÂíåoutlierÁöÑtotal contributionÈÉΩË¢´downweightÔºåÊØîËæÉharmonyÔºàÂè≥ÂõæÔºâ we propose gradient harmonizing mechanism (GHM) Â∏åÊúõ‰∏çÂêåÊ†∑Êú¨ÁöÑgradient contribution‰øùÊåÅharmony È¶ñÂÖàÁ†îÁ©∂gradient densityÔºåÊåâÁÖßÊ¢ØÂ∫¶ËÅöÁ±ªÊ†∑Êú¨ÔºåÂπ∂Áõ∏Â∫îreweight ÈíàÂØπÂàÜÁ±ªÂíåÂõûÂΩíËÆæËÆ°GHM-C lossÂíåGHM-R loss verified on COCO GHM-CÊØîCEÂ•ΩÂæóÂ§öÔºåsligtly better than FL GHM-R‰πüÊØîsmoothL1Â•Ω attains SOTA dynamic lossÔºöadapt to each batch ÊñπÊ≥ï Problem Description define gradient norm $g = |p - p^*|$ the distribution g from a converged model easyÊ†∑Êú¨ÈùûÂ∏∏Â§öÔºå‰∏çÂú®‰∏Ä‰∏™Êï∞ÈáèÁ∫ßÔºå‰ºö‰∏ªÂØºglobal gradient Âç≥‰ΩøÊî∂ÊïõÊ®°Âûã‰πüÊó†Ê≥ïhandle‰∏Ä‰∫õÊûÅÈöæÊ†∑Êú¨ÔºåËøô‰∫õÊ†∑Êú¨Ê¢ØÂ∫¶‰∏éÂÖ∂‰ªñÊ†∑Êú¨Â∑ÆÂºÇËæÉÂ§ßÔºåÊï∞ÈáèËøò‰∏çÂ∞ëÔºå‰πü‰ºöËØØÂØºÊ®°Âûã Gradient Density define gradient density $GD(g) = \frac{1}{l_{\epsilon}(g)} \sum_{k=1} \delta_{\epsilon}(g_k,g)$ given a gradient value g ÁªüËÆ°ËêΩÂú®‰∏≠ÂøÉvalue‰∏∫$g$ÔºåÂ∏¶ÂÆΩ‰∏∫$\epsilon$ÁöÑËåÉÂõ¥ÂÜÖÁöÑÊ¢ØÂ∫¶ÁöÑÊ†∑Êú¨Èáè ÂÜçÁî®Â∏¶ÂÆΩÂéªnorm define the gradient density harmony parameter $\beta_i = \frac{N}{GD(g_i)}$ NÊòØÊÄªÊ†∑Êú¨Èáè ÂÖ∂ÂÆûÂ∞±ÊòØ‰∏édensityÊàêÂèçÊØî large densityÂØπÂ∫îÊ†∑Êú¨‰ºöË¢´downweight GHM-C Loss Â∞Üharmony param‰Ωú‰∏∫loss weightÔºåÂä†ÂÖ•Áé∞Êúâloss ÂèØ‰ª•ÁúãÂà∞FL‰∏ªË¶ÅÂéãÁÆÄÂçïÊ†∑Êú¨ÔºàÂü∫‰∫ésample lossÔºâÔºåGHM‰∏§Â§¥ÂéãÔºàÂü∫‰∫ésample densityÔºâ ÊúÄÁªàharmonize the total gradient contribution of different density group dynamic wrt mini-batchÔºö‰ΩøÂæóËÆ≠ÁªÉÊõ¥Âä†efficientÂíårobust Unit Region Approximation Â∞Ügradient norm [0,1]ÂàÜËß£ÊàêM‰∏™unit region ÊØè‰∏™regionÁöÑÂÆΩÂ∫¶$\epsilon = \frac{1}{M}$ ËêΩÂú®ÊØè‰∏™regionÂÜÖÁöÑÊ†∑Êú¨Êï∞ËÆ°‰Ωú$R_{ind(g)}$Ôºå$ind(g)$ÊòØgÊâÄÂú®regionÁöÑstart idx the approximate gradient densityÔºö$\hat {GD}(g) = \frac{R_{ind(g)}}{\epsilon} =R_{ind(g)}M $ approximate harmony parameter &amp; lossÔºö we can attain good performance with quite small M ‰∏Ä‰∏™ÂØÜÂ∫¶Âå∫Èó¥ÂÜÖÁöÑÊ†∑Êú¨ÂèØ‰ª•Âπ∂Ë°åËÆ°ÁÆóÔºåËÆ°ÁÆóÂ§çÊùÇÂ∫¶O(MN) EMA ‰∏Ä‰∏™mini-batchÂèØËÉΩÊòØ‰∏çÁ®≥ÂÆöÁöÑ ÊâÄ‰ª•ÈÄöËøáÂéÜÂè≤Á¥ØÁßØÊù•Êõ¥Êñ∞Áª¥Á®≥ÔºöSGDMÂíåBNÈÉΩÁî®‰∫ÜEMA Áé∞Âú®ÊØè‰∏™regionÈáåÈù¢ÁöÑÊ†∑Êú¨‰ΩøÁî®Âêå‰∏ÄÁªÑÊ¢ØÂ∫¶ÔºåÊàë‰ª¨ÂØπÊØè‰∏™regionÁöÑÊ†∑Êú¨ÈáèÂ∫îÁî®‰∫ÜEMA t-th iteraion j-th region we have $R_j^t$ apply EMAÔºö$S_j^t = \alpha S_j^(t-1) + (1-\alpha )R_j^t$ $\hat GD(g) = S_{ind(g)} M$ ËøôÊ†∑gradient density‰ºöÊõ¥smooth and insensitive to extreme data GHM-R loss smooth L1Ôºö ÈÄöÂ∏∏ÂàÜÁïåÁÇπËÆæÁΩÆÊàê$\frac{1}{9}$ SL1Âú®Á∫øÊÄßÈÉ®ÂàÜÁöÑÂØºÊï∞Ê∞∏ËøúÊòØÂ∏∏Êï∞ÔºåÊ≤°Ê≥ïÂéªdistinguishing of examples Áî®$|d|$‰Ωú‰∏∫gradient normÂàôÂ≠òÂú®inf ÊâÄ‰ª•ÂÖàÊîπÈÄ†smooth L1ÔºöAuthentic Smooth L1 $\mu=0.02$ Ê¢ØÂ∫¶ËåÉÂõ¥Ê≠£Â•ΩÂú®[0,1) define gradient norm as $gr = |\frac{d}{\sqrt{d^2+\mu^2}}|$ ËßÇÂØüconverged model‚Äòs gradient norm for ASL1ÔºåÂèëÁé∞Â§ßÈáèÊòØoutliers ÂêåÊ†∑Áî®gradient densityËøõË°åreweighting Êî∂ÊïõÁä∂ÊÄÅ‰∏ãÔºå‰∏çÂêåÁ±ªÂûãÁöÑÊ†∑Êú¨ÂØπÊ®°ÂûãÁöÑgradient contribution regressionÊòØÂØπÊâÄÊúâÊ≠£Ê†∑Êú¨ËøõË°åËÆ°ÁÆóÔºå‰∏ªË¶ÅÊòØÈíàÂØπÁ¶ªÁæ§ÁÇπËøõË°ådownweighting ËøôÈáåÈù¢ÁöÑ‰∏Ä‰∏™ËßÇÁÇπÊòØÔºöÂú®regression taskÈáåÈù¢ÔºåÂπ∂ÈùûÊâÄÊúâeasyÊ†∑Êú¨ÈÉΩÊòØ‰∏çÈáçË¶ÅÁöÑÔºåÂú®ÂàÜÁ±ªtaskÈáåÈù¢ÔºåeasyÊ†∑Êú¨Â§ßÈÉ®ÂàÜÈÉΩÊòØÁÆÄÂçïÁöÑËÉåÊôØÁ±ªÔºå‰ΩÜÊòØregressionÂàÜÊîØÈáåÈù¢ÁöÑeasy sampleÊòØÂâçÊôØboxÔºåËÄå‰∏îstill deviated from ground truthÔºå‰ªçÊóßÂÖ∑ÊúâÂÖÖÂàÜÁöÑ‰ºòÂåñ‰ª∑ÂÄº ÊâÄ‰ª•GHM-R‰∏ªË¶ÅÊòØupweight the important part of easy samples and downweight the outliers ÂÆûÈ™å]]></content>
      <tags>
        <tag>single-stage detector, data imbalance, loss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R-FCN]]></title>
    <url>%2F2021%2F08%2F31%2FR-FCN%2F</url>
    <content type="text"><![CDATA[referenceÔºöhttps://zhuanlan.zhihu.com/p/32903856 ÂºïÁî®ÈáèÔºö4193 R-FCN: Object Detection via Region-based Fully Convolutional Networks Âä®Êú∫ region-basedÔºö ÂÖàÊ°ÜÂÆöregion of interestÁöÑÊ£ÄÊµãÁÆóÊ≥ï previous methodsÔºöFast/Faster R-CNNÔºåapply costly per-region subnetwork hundreds of times fully convolutional Êó®Âú®Ëß£ÂÜ≥Faster R-CNNÁ¨¨‰∫åÈò∂ÊÆµËÆ°ÁÆó‰∏çÂÖ±‰∫´ÔºåÊïàÁéá‰ΩéÁöÑÈóÆÈ¢ò we propose position-sensitive score maps translation-invariance in image classification translation-variance in object detection verified on PASCAL VOC ËÆ∫ÁÇπ ‰∏ªÊµÅÁöÑ‰∏§Èò∂ÊÆµÊ£ÄÊµãÊû∂ÊûÑ two subnetworks a shared fully convolutionalÔºöËøô‰∏ÄÈÉ®ÂàÜÊèêÂèñÈÄöÁî®ÁâπÂæÅÔºå‰ΩúÁî®‰∫éÂÖ®Âõæ an RoI-wise subnetworkÔºöËøô‰∏ÄÈÉ®ÂàÜ‰∏çËÉΩÂÖ±‰∫´ËÆ°ÁÆóÔºå‰ΩúÁî®‰∫éproposalsÔºåÂõ†‰∏∫ÊòØË¶ÅÈíàÂØπÊØè‰∏™‰ΩçÁΩÆÁöÑROIËøõË°åÂàÜÁ±ªÂíåÂõûÂΩí ‰πüÂ∞±ÊòØËØ¥ÔºåÁ¨¨‰∏ÄÈÉ®ÂàÜÊòØ‰ΩçÁΩÆ‰∏çÊïèÊÑüÁöÑÔºåÁ¨¨‰∫åÈÉ®ÂàÜÊòØ‰ΩçÁΩÆÊïèÊÑüÁöÑ ÁΩëÁªúË∂äÊ∑±Ë∂ätranslation invariantÔºåÁõÆÊ†áÊÄé‰πàÊâ≠Êõ≤„ÄÅÂπ≥ÁßªÊúÄÁªàÁöÑÂàÜÁ±ªÁªìÊûúÈÉΩ‰∏çÂèòÔºåÂ§öÂ±ÇpoolingÂêéÁöÑÂ∞èfeature map‰∏ä‰πüÊÑüÁü•‰∏çÂà∞Â∞è‰ΩçÁßªÔºåÂπ≥ÁßªÂèØÂèòÊÄßÔºàtranslation varianceÔºâÔºåÂØπÂÆö‰Ωç‰ªªÂä°‰∏çÂèãÂ•Ω ÊâÄ‰ª•resnet-back-detectorÊàë‰ª¨ÊòØÊääROI PoolingÊîæÂú®stage4ÂêéÈù¢ÔºåË∑ü‰∏Ä‰∏™RoI-wiseÁöÑstage5 improves accuracy lower speed due to RoI-wise R-FCN Ë¶ÅËß£ÂÜ≥ÁöÑÊ†πÊú¨ÈóÆÈ¢òÊòØRoI-wiseÈÉ®ÂàÜ‰∏çÂÖ±‰∫´ÔºåÈÄüÂ∫¶ÊÖ¢Ôºö300‰∏™proposalË¶ÅËÆ°ÁÆó300Ê¨° ÂçïÁ∫ØÂú∞Â∞ÜÁΩëÁªúÊèêÂâçÊîæÂà∞shared backÈáåÈù¢‰∏çË°åÔºå‰ºöÈÄ†Êàêtranslation invariantÔºå‰ΩçÁΩÆÁ≤æÂ∫¶‰ºö‰∏ãÈôç ÂøÖÈ°ªÈÄöËøáÂÖ∂‰ªñÊñπÊ≥ïÂä†Âº∫ÁΩëÁªúÁöÑÂπ≥ÁßªÂèØÂèòÊÄßÔºåÊâÄ‰ª•ÊèêÂá∫‰∫Üposition-sensitive score map Â∞ÜÂÖ®ÂõæÂàíÂàÜ‰∏∫kxk‰∏™Âå∫Âüü position-sensitive score mapÔºöÁîüÊàêkxkx(C+1)‰∏™ÁâπÂæÅÂõæ ÊØè‰∏™‰ΩçÁΩÆÂØπÂ∫îC+1‰∏™ÁâπÂæÅÂõæ ÂÅöRoIPoolingÁöÑÊó∂ÂÄôÔºåÊØè‰∏™binÊù•Ëá™ÊØè‰∏™positionÂØπÂ∫îÁöÑC+1‰∏™mapÔºàËøôÂíãÊÉ≥ÁöÑÔºåspace dimÂà∞channel dimÂÜçÂà∞space dimÔºüÔºâ ÊñπÊ≥ï overview two-stage region proposalÔºöRPN region classificationÔºöthe R-FCN R-FCN ÂÖ®Âç∑ÁßØ ËæìÂá∫convÂ±ÇÊúâkxkx(C+1)‰∏™channel kxkÂØπÂ∫îgrid positions C+1ÂØπÂ∫îC‰∏™ÂâçÊôØ+background ÊúÄÂêéÊòØposition-sensitive RoI pooling layer aggregates from last conv and RPNÔºü generate scores for each RoI each bin aggregates responses fromÂØπÂ∫îÁöÑpositionÁöÑchannel score mapsÔºåËÄå‰∏çÊòØÂÖ®ÈÉ®ÈÄöÈÅì forceÊ®°ÂûãÂú®ÈÄöÈÅì‰∏äÂΩ¢ÊàêÂØπ‰∏çÂêå‰ΩçÁΩÆÁöÑÊïèÊÑüËÉΩÂäõ R-FCN architecture backÔºöResNet-101Ôºåpre-trained on ImageNetÔºåblock5 ËæìÂá∫ÊòØ2048-d ÁÑ∂ÂêéÊé•‰∫Ürandom initialized 1x1 convÔºåÈôçÁª¥ cls brach Êé•$k^2(C+1)$ÁöÑconvÁîüÊàêscore maps ÁÑ∂ÂêéÊòØPosition-sensitive RoI pooling Â∞ÜÊØè‰∏™ROIÂùáÂåÄÂàáÂàÜÊàêkxk‰∏™bins ÊØè‰∏™binÂú®ÂØπÂ∫îÁöÑPosition-sensitive score maps‰∏≠ÊâæÂà∞ÂîØ‰∏ÄÁöÑÈÄöÈÅìÔºåËøõË°åaverage pooling ÊúÄÁªàÂæóÂà∞kxkÁöÑpooling mapÔºåC+1‰∏™ÈÄöÈÅì Â∞Üpooling map performs average poolingÔºåÂæóÂà∞C+1ÁöÑvectorÔºåÁÑ∂Âêésoftmax box branch Êé•$4k^2$ÁöÑconvÁîüÊàêscore maps Position-sensitive RoI pooling ÂæóÂà∞kxkÁöÑpooling mapÔºå4‰∏™ÈÄöÈÅì average poolingÔºåÂæóÂà∞4d vectorÔºå‰Ωú‰∏∫ÂõûÂΩíÂÄº$(t_x,t_y,t_w,t_h)$ there is no learnable layer after the ROI layerÔºåenable nearly cost-free region-wise computation Training R-FCN positives / negativesÔºöÂíågt boxÁöÑIoU&gt;0.5ÁöÑproposasl adopt OHEM sort all ROI loss and select the highest 128 ÂÖ∂‰ªñsettingsÂü∫Êú¨ÂíåFaster-RCNN‰∏ÄËá¥ Atrous and stride ÁâπÂà´Âú∞ÔºåÂØπresnetÁöÑblock5ËøõË°å‰∫ÜÊîπÂèò stride2ÊîπÊàêstride1 ÊâÄÊúâÁöÑconvÊîπÊàêÁ©∫Ê¥ûÂç∑ÁßØ RPNÊòØÊé•Âú®block4ÁöÑËæìÂá∫‰∏äÔºåÊâÄ‰ª•‰∏çÂèóÁ©∫Ê¥ûÂç∑ÁßØÁöÑÂΩ±ÂìçÔºåÂè™ÂΩ±ÂìçR-FCN head]]></content>
      <tags>
        <tag>ÁõÆÊ†áÊ£ÄÊµãÔºåÂÖ®Âç∑ÁßØÔºåregion-basedÔºåtwo-stage detector</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Meta Pseudo Labels]]></title>
    <url>%2F2021%2F08%2F23%2FMeta-Pseudo-Labels%2F</url>
    <content type="text"><![CDATA[papers [MPL 2021] Meta Pseudo Labels [UDA 2009] Unsupervised Data Augmentation for Consistency Training [Entropy Minimization 2004] Semi-supervised Learning by Entropy Minimization Meta Pseudo Labels Âä®Êú∫ semi-supervised learning Pseudo LabelsÔºöfixed teacher Meta Pseudo LabelsÔºöconstantly adapted teacher by the feedback of the student SOTA on ImageNetÔºötop-1 acc 90.2% ËÆ∫ÁÇπ Pseudo Labels methods teacher generates pseudo labels on unlabeled images pseudo labeled images are then combined with labeled images to train the student confirmation bias problemÔºöstudentÁöÑÁ≤æÂ∫¶ÂèñÂÜ≥‰∫é‰º™Ê†áÁ≠æÁöÑË¥®Èáè we propose Meta Pseudo Labels teacher observes how its pseudo labels affect the student then correct the bias the feedback signal is the performance of the student on the labeled dataset ÊÄªÁöÑÊù•ËØ¥ÔºåteacherÂíåstudentÊòØtrain in parallelÁöÑ student learns from pseudo labels from the teacher teacher learns from reward signal from how well student perform on labeled set dataset ImageNet as labeled set JFT-300M as unlabeled set model teacherÔºöEfficientNet-L2 studentÔºöEfficientNet-L2 main difference Pseudo LabelsÊñπÊ≥ï‰∏≠ÔºåteacherÂú®ÂçïÂêëÁöÑÂΩ±Âìçstudent Meta Pseudo LabelsÊñπÊ≥ï‰∏≠ÔºåteacherÂíåstudentÊòØ‰∫§‰∫í‰ΩúÁî®ÁöÑ ÊñπÊ≥ï notations models teacher model T &amp; $\theta_T$ student model S &amp; $\theta_S$ data labeled set $(x_l, y_l)$ unlabeled set $(x_u)$ predictions soft predictions by teacher $T(x_u, \theta_T)$ student $S(x_u, \theta_S)$ &amp; $S(x_l, \theta_S)$ loss $CE(q,p)$ÔºåÂÖ∂‰∏≠$q$ÊòØone-hot labelÔºåe.g. $CE(y_l, S(x_l, \theta_S))$ Pseudo Labels given a fixed teacher $\theta_T$ train the student model to minimize the cross-entropy loss on unlabeled data \theta_S^{PL} = argmin_{\theta_S}CE(T(x_u,\theta_T), S(x_u, \theta_S)) $\theta_S^{PL}$ also achieve a low loss on labeled data $\theta_S^{PL}$ explicitly depends on $\theta_T$Ôºö$\theta_S^{PL}(\theta_T)$ student loss on labeled data is also a function of $\theta_T$Ôºö$L_l(\theta_S^{PL}(\theta_T))$ Meta Pseudo Labels intuitionÔºöminimize $L_l$ with respect to $\theta_T$ ‰ΩÜÊòØÂÆûÈôÖ‰∏ädependency of $\theta_S^{PL}(\theta_T)$ on $\theta_T$ ÈùûÂ∏∏Â§çÊùÇ Âõ†‰∏∫Êàë‰ª¨Áî®‰∫Üteacher predictionÁöÑhard labelsÂéªËÆ≠ÁªÉstudent an alternating optimization procedure teacher‚Äôs auxiliary losses augment the teacher‚Äôs training with a supervised learning objective and a semi-supervise learning objective supervised objective train on labeled data CE semi-supervised objective train on unlabeled data UDA(Unsupervised Data Augmentation)ÔºöÂ∞ÜÊ†∑Êú¨ËøõË°åÁÆÄÂçïÂ¢ûÂº∫ÔºåÈÄöËøáË°°Èáè‰∏ÄËá¥ÊÄßÊçüÂ§±ÔºåÊ®°ÂûãÁöÑÊ≥õÂåñÊïàÊûúÂæóÂà∞ÊèêÂçá consistency training lossÔºöKLÊï£Â∫¶ finetuning student Âú®meta pseudo labelsËÆ≠ÁªÉËøáÁ®ã‰∏≠Ôºåstudent only learns from the unlabeled data ÊâÄ‰ª•Âú®ËÆ≠ÁªÉËøáÁ®ãÁªìÊùüÂêéÔºåÂèØ‰ª•finetune it on labeled data to improve accuracy overall algorithm * ËøôÈáåÈù¢Êúâ‰∏ÄÂ§Ñ‰∏ãÊ†áÂÜôÈîô‰∫ÜÔºåÂ∞±ÊòØteacherÁöÑUDA gradientÔºåÊòØÂú®unlabeled data‰∏äÈù¢ÁÆóÁöÑÔºåÈÇ£‰∏§‰∏™$x_l$ÂæóÊîπÊàê$x_u$ * UDA lossËÆ∫ÊñáÈáå‰ΩøÁî®‰∏§‰∏™predicted logitsÁöÑÊï£Â∫¶ÔºåËøôÈáåÊòØCE Unsupervised Data Augmentation for Consistency Training Âä®Êú∫ data augmentation in previous works ËÉΩÂú®‰∏ÄÂÆöÁ®ãÂ∫¶‰∏äÁºìËß£ÈúÄË¶ÅÂ§ßÈáèÊ†áÊ≥®Êï∞ÊçÆÁöÑÈóÆÈ¢ò Â§öÁî®Âú®supervised model‰∏ä achieved limited gains we propose UDA apply data augmentation in semi-supervised learning setting use harder and more realistic noise to generate the augmented samples encourage the prediction to be consistent between unlabeled &amp; augmented unlabeled sample Âú®Ë∂äÂ∞èÁöÑÊï∞ÊçÆÈõÜ‰∏äÊèêÂçáË∂äÂ§ß verified on six language tasks three vision tasks ImageNet-10%ÔºöÔºötop1/top5 68.7/88.5% ImageNet-extra unlabeledÔºötop1/top5 79.0/94.5% ËÆ∫ÁÇπ semi-supervised learning three categories graph-based label propagation via graph convolution and graph embeddings modeling prediction target as latent variables consistency / smoothness enforcing ÊúÄÂêéËøô‰∏ÄÁ±ªÊñπÊ≥ïshown to work wellÔºå enforce the model predictions on the two examples to be similar ‰∏ªË¶ÅÂå∫Âà´Âú®‰∫éperturbation functionÁöÑËÆæËÆ° we propose UDA use state-of-the-art data augmentation methods we show that better augmentation methods(AutoAugment) lead to greater improvements minimizes the KL divergence can be applied even the class distributions of labeled and unlabeled data mismatch we propose TSA a training technique prevent overfitting when much more unlabeled data is avaiable than labeled data ÊñπÊ≥ï formulation given an input $x\in U$ and a small noise $\epsilon$ compute the output distribution $p_{\theta}(y|x)$ and $p_{\theta}(y|x,\epsilon)$ minimize the divergence between two predicted distributions $D(p_{\theta}(y|x)||p_{\theta}(y|x,\epsilon))$ add a CE loss on labeled data UDAÁöÑ‰ºòÂåñÁõÆÊ†á enforce the model to be insensitive to perturbation thus smoother to the changes in the input space $\lambda=1$ for most experiments use different batchsize for labeled &amp; unlabeled Augmentation Strategies for Different Tasks AutoAugment for Image Classification ÈÄöËøáRLÊêúÂá∫Êù•ÁöÑ‰∏ÄÁªÑoptimal combination of aug operations Back translation for Text Classification TF-IDF based word replacing for Text Classification Trade-off Between Diversity and Validity for Data Augmentation ÂØπÂéüÂßãsampleÂÅöÂèòÊç¢ÁöÑÊó∂ÂÄôÔºåÊúâ‰∏ÄÂÆöÊ¶ÇÁéáÂØºËá¥gt labelÂèòÂåñ AutoAugmentÂ∑≤ÁªèÊòØoptmial trade-off‰∫ÜÔºåÊâÄ‰ª•‰∏çÁî®ÁÆ° text tasksÈúÄË¶ÅË∞ÉËäÇtemperature Additional Training Techniques TSA(Training Signal Annealing) situationÔºöunlabeled dataËøúÊØîlabeled dataÂ§öÁöÑÊÉÖÂÜµÔºåÊàë‰ª¨ÈúÄË¶Ålarge enough modelÂéªÂÖÖÂàÜÂà©Áî®Â§ßÊï∞ÊçÆÔºå‰ΩÜÂèàÂÆπÊòìÂØπÂ∞ètrainsetËøáÊãüÂêà for each training step set a threshold $\frac{1}{K}\leq \eta_t\leq 1$ÔºåK is the number of categories Â¶ÇÊûúÊ†∑Êú¨Âú®gt cls‰∏äÁöÑÈ¢ÑÊµãÊ¶ÇÁéáÂ§ß‰∫éËøô‰∏™thresholdÔºåÂ∞±ÊääËøô‰∏™Ê†∑Êú¨ÁöÑlossÂéªÊéâ $\eta_t$ serves as a ceiling to prevent the model from over-training on examples that the model is already confident about gradually release the training signals of the labeled examplesÔºåÁºìËß£overfitting schedules of $\eta_t$ log-scheduleÔºö$\lambda_t = 1-exp(-\frac{t}{T}*5)$ linear-scheduleÔºö$\lambda_t = \frac{t}{T}$ exp-scheduleÔºö$\lambda_t = exp((\frac{t}{T}-1)*5)$ Â¶ÇÊûúÊ®°ÂûãÈùûÂ∏∏ÂÆπÊòìËøáÊãüÂêàÔºåÁî®exp-scheduleÔºåÂèçËøáÊù•Ôºàabundant labeled data/effective regularizationsÔºâÔºåÁî®log-schedule Sharpening Predictions situationÔºöthe predicted distributions on unlabeled examples tend to be over-flat across categoriesÔºåtaskÊØîËæÉÂõ∞ÈöæÔºåËÆ≠ÁªÉÊï∞ÊçÆÊØîËæÉÂ∞ëÊó∂ÔºåÂú®unlabeled data‰∏äÊØèÁ±ªÁöÑÈ¢ÑÊµãÊ¶ÇÁéáÈÉΩÂ∑Æ‰∏çÂ§ö‰ΩéÔºåÊ≤°ÊúâÂÄæÂêëÊÄß ËøôÊó∂ÂÄôKL divergenceÁöÑÁõëÁù£‰ø°ÊÅØÂ∞±ÂæàÂº± thus we need to sharpen the predicted distribution on unlabeled examples Confidence-based maskingÔºöÂ∞Ücurrent model not confident enough to predictÁöÑÊ†∑Êú¨ËøáÊª§ÊéâÔºåÂè™‰øùÁïôÊúÄÂ§ßÈ¢ÑÊµãÊ¶ÇÁéáÂ§ß‰∫é0.6ÁöÑÊ†∑Êú¨ËÆ°ÁÆóconsistency loss Entropy minimizationÔºöadd an entropy term to the overall objective softmax temperatureÔºöÂú®ËÆ°ÁÆósoftmaxÊó∂ÂÖàÂØπlogitsËøõË°årescaleÔºå$Softmax(logits/\tau)$Ôºåa lower temperature corresponds to a sharper distribution in practiceÂèëÁé∞Confidence-based maskingÂíåsoftmax temperatureÊõ¥ÈÄÇÁî®‰∫éÂ∞èlabeled setÔºåEntropy minimizationÈÄÇÁî®‰∫éÁõ∏ÂØπÂ§ß‰∏ÄÁÇπÁöÑlabeled set Domain-relevance Data Filtering ÂÖ∂ÂÆû‰πüÊòØConfidence-based maskingÔºåÂÖàÁî®labeled dataËÆ≠ÁªÉ‰∏Ä‰∏™base modelÔºåÁÑ∂Âêéinference the out-of-domain datasetÔºåÊåëÂá∫È¢ÑÊµãÊ¶ÇÁéáËæÉÂ§ßÁöÑÊ†∑Êú¨ Semi-supervised Learning by Entropy Minimization]]></content>
      <tags>
        <tag>semi-supervised learning, teacher-student, classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Generalized Focal Loss]]></title>
    <url>%2F2021%2F08%2F20%2FGeneralized-Focal-Loss%2F</url>
    <content type="text"><![CDATA[Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection Âä®Êú∫ one-stage detectors dense prediction three fundamental elements class branch box localization branch an individual quality branch to estimate the quality of localization current problems the inconsistent usage of the quality estimation in train &amp; test the inflexible Dirac delta distributionÔºöÂ∞Übox regressionÁöÑvalueÂª∫Ê®°ÊàêÁúüÂÄºÈôÑËøëÁöÑËÑâÂÜ≤ÂàÜÂ∏ÉÔºåÁî®Êù•ÊèèËø∞ËæπÁïå‰∏çÊ∏ÖÊô∞/ÈÅÆÊå°ÁöÑcaseÂèØËÉΩ‰∏çÂáÜÁ°Æ we design new representations for these three elements merge quality estimation into class predictionÔºöÂ∞Üobjectness/centernessÊï¥ÂêàËøõcls predictionÔºåÁõ¥Êé•Áî®‰ΩúNMS score continout labels propose GFL(Generalized Focal Loss) that generalizes Focal Loss from discrete form into continous version test on COCO ResNet-101-?-GFL: 45.0% AP defeat ATSS ËÆ∫ÁÇπ inconsistent usage of localization quality estimation and classification score ËÆ≠ÁªÉÁöÑÊó∂ÂÄôqualityÂíåcls branchÊòØindependent branch box branchÁöÑsupervisionÂè™‰ΩúÁî®Âú®positiveÊ†∑Êú¨‰∏äÔºöwhich is unreliable on predicting negatives ÊµãËØïÈò∂ÊÆµÂ∞ÜqualityÂíåcls score‰πòËµ∑Êù•ÊúâÂèØËÉΩÊãâÈ´òË¥üÊ†∑Êú¨ÁöÑÂàÜÊï∞Ôºå‰ª•Ëá≥‰∫éÂú®NMSÈò∂ÊÆµÊää‰ΩéÂàÜÊ≠£Ê†∑Êú¨Êå§Êéâ inflexible representation of bounding boxes most methodÂª∫Ê®°ÊàêËÑâÂÜ≤ÂàÜÂ∏ÉÔºöÂè™Âú®IoUÂ§ß‰∫é‰∏ÄÂÆöÈòàÂÄºÁöÑÊ†ºÂ≠ê‰∏äÊúâÂìçÂ∫îÔºåÂà´ÁöÑÊ†ºÂ≠êÈÉΩÊòØ0 some recent workÂª∫Ê®°ÊàêÈ´òÊñØÂàÜÂ∏É in fact the real distribution can be more arbitrary and flexibleÔºåËøûÁª≠‰∏î‰∏ç‰∏•Ê†ºÈïúÂÉè thus we propose merge the quality representation into the class branchÔºö class vectorÁöÑÊØè‰∏™ÂÖÉÁ¥†‰ª£Ë°®‰∫ÜÊ†ºÂ≠êÁöÑlocalization quality(Â¶ÇIoU score) Âú®inferenceÈò∂ÊÆµ‰πüÊòØÁõ¥Êé•Áî®‰Ωúcls score propose arbitrary/general distribution ÊúâÊòéÁ°ÆËæπÁïåÁöÑÁõÆÊ†áÁöÑËæπÁöÑÂàÜÂ∏ÉÊòØÊØîËæÉsharpÁöÑ Ê≤°ÊúâÊòéÊòæËæπÁïåÁöÑËæπÂàÜÂ∏ÉÂ∞±ÊòØflatten‰∏ÄÁÇπ Generalized Focal Loss (GFL) joint class representationÊòØcontinuous IoU label (0‚àº1) imbalanceÈóÆÈ¢ò‰ªçÁÑ∂Â≠òÂú®Ôºå‰ΩÜÊòØstandart Focal Loss‰ªÖÊîØÊåÅ[0,1] sample ‰øÆÊîπÊàêcontinuousÂΩ¢ÂºèÔºåÂêåÊó∂specialized into Quality Focal Loss (QFL) and Distribution Focal Loss (DFL) QFL for cls branchÔºöfocuses on a sparse set of hard examples DFL for box branchÔºö focus on learning the probabilities of values around the continuous target locations ÊñπÊ≥ï Focal Loss (FL) standard CE partÔºö$-log(p_t)$ scaling factorÔºödown-weights the easy examplesÔºåfocus on hard examples Quality Focal Loss (QFL) soft one-hot labelÔºöÊ≠£Ê†∑Êú¨Âú®ÂØπÂ∫îÁ±ªÂà´‰∏äÊúâ‰∏™(0,1]ÁöÑfloat scoreÔºåË¥üÊ†∑Êú¨ÂÖ®0 float scoreÂÆö‰πâ‰∏∫È¢ÑÊµãÊ°ÜÂíågt boxÁöÑIoU score we adopt multiple binary classification with sigmoid modify FL CE part ÊîπÊàêcomplete formÔºö$-ylog(\hat y)-(1-y)log(1-\hat y)$ scaling partÁî®vector distanceÊõøÊç¢ÂáèÊ≥ïÔºö$|y-\hat y |^{\beta}$ $\beta$ controls the down-weighting rate smoothly &amp; $\beta=2$ works best Distribution Focal Loss (DFL) use relative offsets from the location to the four sides of a bounding box as the regression targets ÂõûÂΩíÈóÆÈ¢òformulation ËøûÁª≠Ôºö$\hat y = \int_{y_0}^{y_n}P(x)xdx$ Á¶ªÊï£ÂåñÔºö$\hat y = \sum_{i=0}^n P(y_i)y_i$ P(x) can be easily implemented through a softmax layer containing n+1 unitsÔºö DFL force predictions to focus values near label $y$Ôºöexplicitly enlarge the probabilities of $y_i$Âíå$y_{i+1}$Ôºågiven $y_i \leq y \leq y_{i+1}$ $log(S_i)$ force the probabilities gap balance the ‰∏ä‰∏ãÈôêÔºå‰ΩøÂæó$\hat y$ÁöÑglobal mininum solutionÊó†ÈôêÈÄºËøëÁúüÂÄº$y$ÔºåÂ¶ÇÊûúÁúüÂÄºÊé•ËøëÁöÑÊòØ$\hat y_{i+1}$ÔºåÂèØ‰ª•ÁúãÂà∞$log(S_i)$ÈÇ£È°πË¢´downscale‰∫Ü Generalized Focal Loss (GFL) ‰ª•ÂâçÁöÑcls preditionsÂú®ÊµãËØïÈò∂ÊÆµË¶ÅÁªìÂêàquality predictions‰Ωú‰∏∫NMS scoreÔºåÁé∞Âú®Áõ¥Êé•Â∞±ÊòØ ‰ª•Ââçregression targetsÊØè‰∏™ÂõûÂΩí‰∏Ä‰∏™ÂÄºÔºåÁé∞Âú®ÊòØn+1‰∏™ÂÄº overall Á¨¨‰∏ÄÈ°πcls lossÔºåÂ∞±ÊòØQFLÔºådense on ÊâÄÊúâÊ†ºÂ≠êÔºåÁî®Ê≠£Ê†∑Êú¨Êï∞Âéªnorm Á¨¨‰∫åÈ°πbox lossÔºåGIoU loss + DFLÔºå$\lambda_0$ÈªòËÆ§2Ôºå$\lambda_1$ÈªòËÆ§1/4ÔºåÂè™ËÆ°ÁÆóÊúâIoUÁöÑÊ†ºÂ≠ê we also utilize the quality scores to weight $L_B$ and $L_D$ during training ÂΩ©Ëõã IoU branch always superior than centerness-branch centernessÂ§©ÁîüÂÄºËæÉÂ∞èÔºåÂΩ±ÂìçÂè¨ÂõûÔºåIoUÁöÑÂÄºËæÉÂ§ß]]></content>
      <tags>
        <tag>one-stage detector, object-detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[soft teacher]]></title>
    <url>%2F2021%2F08%2F12%2Fsoft-teacher%2F</url>
    <content type="text"><![CDATA[keywordsÔºösemi-supervised, curriculum, pseudo labels, End-to-End Semi-Supervised Object Detection with Soft Teacher Âä®Êú∫ end-to-end trainingÔºöÁõ∏ÊØîËæÉ‰∫éÂÖ∂‰ªñÊñπÊ≥ïÁöÑmulti-stage semi-supervisedÔºöÁî®Â§ñÈÉ®unlabeledÊï∞ÊçÆÔºå‰ª•Âèäpseudo-label based approach propose two techniques soft teacher mechanismÔºöpseudoÊ†∑Êú¨ÁöÑclassification lossÁî®teacher modelÁöÑprediction scoreÊù•Âä†ÊùÉ box jittering mechanismÔºöÊåëÈÄâreliable pseudo boxes verified use SWIN-L as baseline metric on COCOÔºö60.4 mAP if pretrained with Object365Ôºö61.3 mAP ËÆ∫ÁÇπ we present this end-to-end pseudo-label based semi-supervised object detection framework simultaneously performs pseudo-labelingÔºöteacher training detector use the current pseudo-labels &amp; a few training sampleÔºöstudent teacher is an exponential moving average (EMA) of the student model mutually enforce each other soft teacher approach teacher modelÁöÑ‰ΩúÁî®ÊòØÁªôstudent modelÁîüÊàêÁöÑbox candidatesÊâìÂàÜÔºå È´ò‰∫é‰∏ÄÂÆöÈòàÂÄºÁöÑ‰∏∫ÂâçÊôØÔºå‰ΩÜÊòØÂèØËÉΩÊúâÈÉ®ÂàÜÂâçÊôØË¢´ÂΩíÁ±ª‰∏∫ËÉåÊôØÔºåÊâÄ‰ª•Áî®Ëøô‰∏™score‰Ωú‰∏∫reliability measureÔºåÁªôÊ†áËÆ∞‰∏∫ËÉåÊôØÊ°ÜÁöÑcls lossËøõË°åÂä†ÊùÉ reliability measure ÊñπÊ≥ï overview ‰∏§‰∏™modelÔºöstudentÂíåteacher teacher modelÁî®Êù•ÁîüÊàêpseudo labelsÔºötwo set of pseudo boxesÔºå‰∏Ä‰∏™Áî®‰∫éclass branchÔºå‰∏Ä‰∏™Áî®‰∫éregression branch student modelÁî®supervised&amp;unsupervised sampleÁöÑlossÊù•Êõ¥Êñ∞ teacher modelÁî®student modelÁöÑEMAÊù•Êõ¥Êñ∞ two crucial designs soft teacher box jittering Êï¥‰ΩìÁöÑÂ∑•‰ΩúÊµÅÁ®ãÂ∞±ÊòØÔºåÊØè‰∏™training iterationÔºåÂÖàÊåâÁÖß‰∏ÄÂÆöÊØî‰æãÊäΩÂèñlabeled&amp;unlabeled sampleÊûÑÊàêdata batchÔºåÁÑ∂ÂêéÁî®teacher modelÁîüÊàêunlabeled dataÁöÑpseudo labelÔºàthousands of box candidates+NMS+score filterÔºâÔºåÁÑ∂ÂêéÂ∞ÜÂÖ∂‰Ωú‰∏∫unlabeled sampleÁöÑground truthÔºåËÆ≠ÁªÉstudent modelÔºåoverall lossÊòØsupervised lossÂíåunsupervised lossÁöÑÂä†ÊùÉÂíå Âú®ËÆ≠ÁªÉÂºÄÂßãÈò∂ÊÆµÔºå‰∏§‰∏™Ê®°ÂûãÈÉΩÊòØÈöèÊú∫ÂàùÂßãÂåñÁöÑÔºåteacherÊ®°ÂûãÈöèÁùÄstudentÊ®°ÂûãÁöÑÊõ¥Êñ∞ËÄåÊõ¥Êñ∞ FixMatchÔºö ËæìÂÖ•ÁªôteacherÊ®°ÂûãÁöÑÊ†∑Êú¨‰ΩøÁî®weak aug ËæìÂÖ•ÁªôstudentÊ®°ÂûãÁöÑÊ†∑Êú¨‰ΩøÁî®strong aug soft teacher detectorÁöÑpseudo-labelË¥®ÈáèÂæàÈáçË¶Å ÊâÄ‰ª•Áî®score thresh=0.9ÂéªÂÆö‰πâbox candidatesÁöÑÂâç/ËÉåÊôØ ‰ΩÜÊòØËøôÊó∂ÂÄôÂ¶ÇÊûúÁî®‰º†ÁªüÁöÑIoUÊù•ÂÆö‰πâstudent modelÁöÑbox candidatesÁöÑpos/negÔºå‰ºöÊúâ‰∏ÄÈÉ®ÂàÜÂâçÊôØÊ°ÜË¢´ÂΩì‰ΩúËÉåÊôØ to alleviate assess the reliability of each student-generated box candidate to be a real background given a student-generated box candidateÔºåÁî®teacher modelÁöÑdetection headÂéªÈ¢ÑÊµãËøô‰∏™Ê°ÜÁöÑbackground score overall unsupervised cls loss L_u^{cls} = \frac{1}{N_b^{fg}} \sum_i^{N_b^{fg}} l_{cls} (b_i^{fg}, G_{cls}) + \sum_j^{N_b^{bg}}w_j l_{cls} (b_j^{bg}, G_{cls})\\ w_j = \frac{r_j}{\sum_{k=1}^{N_b^{bg}}r_k} $G_{cls}$ÊòØthe set of boxes teacher generated for classificationÔºåÂ∞±ÊòØteacher modelÈ¢ÑÊµãÁöÑtop1000ÁªèËøánmsÂíåscore filter‰πãÂêéÁöÑboxes $b_i^{fg}$ÊòØstudent candidates‰∏≠Ë¢´assign‰∏∫ÂâçÊôØÁöÑÊ°ÜÔºå$b_i^{bg}$ÊòØstudent candidates‰∏≠Ë¢´assign‰∏∫ËÉåÊôØÁöÑÊ°ÜÔºåassignÁöÑÂéüÂàôÂ∞±ÊòØscore&gt;0.9 $w_j$ÊòØÂØπassign‰∏∫ËÉåÊôØÁöÑÊ°ÜÁöÑÂä†ÊùÉ $r_k$ÊòØreliability scoreÔºåÊòØstudent modelÈÄöËøáhard score thresh assign‰∏∫ËÉåÊôØÁöÑÊ°ÜÔºåÁî®teacher modelÁöÑdetection headÂéªÈ¢ÑÊµãÁöÑbg score box jittering fg score threshÂíåbox iouÂπ∂‰∏çÂëàÁé∞strong positive correlationÔºåËØ¥ÊòéÂü∫‰∫éËøô‰∏™ÂéüÂàô‰∫ßÁîüÁöÑÊ°Üpseudo-labelsÂπ∂‰∏ç‰∏ÄÂÆöÈÄÇÂêàbox regression localization reliabilityÔºö Ë°°Èáè‰∏Ä‰∏™pseudo boxÁöÑconsistency given a pseudo boxÔºåsample‰∏ÄÁ≥ªÂàójitter box around itÔºåÂÜçÁî®teacher modelÂéªÈ¢ÑÊµãËøô‰∫õjitter boxÂæóÂà∞refined boxes refined boxÂíåpseudo boxÁöÑvarianceË∂äÂ∞èÔºåËØ¥ÊòéËøô‰∏™Ê°ÜÁöÑlocalization reliabilityË∂äÈ´ò \hat b_i = refine(jitter(b_i))\\ \overline \sigma_i = \frac{1}{4} \sum_1^4 \hat \sigma_k\\ \hat \sigma_k = \frac{\sigma_k}{0.5 (h(b_i)+w(b_i))} $\hat b_i$ÊòØrefined boxes $\sigma_k$ÊòØrefine boxesÁöÑÂõõ‰∏™ÂùêÊ†áÂü∫‰∫éÂéüÂßãboxÁöÑÊ†áÂáÜÂ∑Æ $\hat \sigma_k$ÊòØ‰∏äÈù¢ÈÇ£‰∏™Ê†áÂáÜÂ∑ÆÂü∫‰∫éÂéüÂßãboxÁöÑÂ∞∫Â∫¶ËøõË°åÂΩí‰∏ÄÂåñ $\overline\sigma$ÊòØrefine boxesÂõõ‰∏™ÂùêÊ†áÁöÑnormed stdÁöÑÂπ≥ÂùáÂÄº Âè™ËÆ°ÁÆóteacher box candidatesÈáåÈù¢Ôºåfg score&gt;0.5ÁöÑÈÇ£ÈÉ®ÂàÜ overall unsupervised reg loss L_u^{reg} = \frac{1}{N_b^{fg}} \sum_1^{N_b^{fg}} l_{reg} (b_i^{fg}, G_{reg}) $b_i^{fg}$ÊòØstudent candidates‰∏≠Ë¢´assign‰∏∫ÂâçÊôØÁöÑÊ°ÜÔºåÂç≥cls score&gt;0.9ÈÇ£‰∫õÈ¢ÑÊµãÊ°Ü $G_{cls}$ÊòØthe set of boxes teacher generated for regressionÔºåÂ∞±ÊòØjittered reliabilityÂ§ß‰∫é‰∏ÄÂÆöÈòàÂÄºÁöÑcandidates overall unsupervised lossÔºöcls lossÂíåreg loss‰πãÂíåÔºåÁÑ∂ÂêéÁî®Ê†∑Êú¨Êï∞ËøõË°ånorm ÂÆûÈ™å]]></content>
      <tags>
        <tag>semi-supervised, object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GNN&GCN]]></title>
    <url>%2F2021%2F07%2F13%2FGNN-GCN%2F</url>
    <content type="text"><![CDATA[ÁªºËø∞ referenceÔºöhttps://www.cnblogs.com/siviltaram/p/graph_neural_network_2.html key concepts ÂõæÁ•ûÁªèÁΩëÁªúÔºàGraph Neural NetworkÔºåGNNÔºâ ÂõæÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàGraph Convolutional Neural NetworkÔºâ È¢ëÂüüÔºàSpectral-domainÔºâ Á©∫ÂüüÔºàSpatial-domainÔºâ ÂõæÁ•ûÁªèÁΩëÁªú image &amp; graph ËäÇÁÇπÔºàNodeÔºâ ÊØè‰∏™ËäÇÁÇπÊúâÂÖ∂ÁâπÂæÅÔºåÁî®$x_v$Ë°®Á§∫ ËæπÔºàEdgeÔºâ ËøûÊé•‰∏§‰∏™ËäÇÁÇπÁöÑËæπ‰πüÊúâÂÖ∂ÁâπÂæÅÔºåÁî®$x_{v,u}$Ë°®Á§∫ ÈöêËóèÁä∂ÊÄÅ ÂõæÁöÑÂ≠¶‰π†ÁõÆÊ†áÊòØËé∑ÂæóÊØè‰∏™ËäÇÁÇπÁöÑÈöêËóèÁä∂ÊÄÅ Â±ÄÈÉ®ËæìÂá∫ÂáΩÊï∞ ÈÄâÂèñ‰∏Ä‰∏™ËäÇÁÇπ ÂõæÂç∑ÁßØ ‰∏ÄÂº†ÂõæÁâáÂ∞±ÂèØ‰ª•Áúã‰Ωú‰∏Ä‰∏™ÈùûÂ∏∏Á®†ÂØÜÁöÑÂõæÔºåÈò¥ÂΩ±ÈÉ®ÂàÜ‰ª£Ë°®Âç∑ÁßØÊ†∏ÔºåÂè≥‰æßÂàôÊòØ‰∏Ä‰∏™ÊôÆÈÄöÁöÑÂõæÔºåÂíåÂõæÂç∑ÁßØÊ†∏ Âú®image‰∏∫‰ª£Ë°®ÁöÑÊ¨ßÂºèÁ©∫Èó¥‰∏≠ÔºåÁªìÁÇπÁöÑÈÇªÂ±ÖÊï∞ÈáèÈÉΩÊòØÂõ∫ÂÆöÁöÑÔºå‰ΩÜÂú®graphËøôÁßçÈùûÊ¨ßÁ©∫Èó¥‰∏≠ÔºåÁªìÁÇπÊúâÂ§öÂ∞ëÈÇªÂ±ÖÂπ∂‰∏çÂõ∫ÂÆö ‰º†ÁªüÁöÑÂç∑ÁßØÊ†∏‰∏çËÉΩÁõ¥Êé•Áî®‰∫éÊäΩÂèñÂõæ‰∏äÁªìÁÇπÁöÑÁâπÂæÅ ‰∏§‰∏™‰∏ªÊµÅÊÄùË∑Ø ÊääÈùûÊ¨ßÁ©∫Èó¥ÁöÑÂõæËΩ¨Êç¢ÊàêÊ¨ßÂºèÁ©∫Èó¥ÔºåÁÑ∂Âêé‰ΩøÁî®‰º†ÁªüÂç∑ÁßØ ÊâæÂá∫‰∏ÄÁßçÂèØÂ§ÑÁêÜÂèòÈïøÈÇªÂ±ÖÁªìÁÇπÁöÑÂç∑ÁßØÊ†∏Âú®Âõæ‰∏äÊäΩÂèñÁâπÂæÅ]]></content>
      <tags>
        <tag>ÂõæÁ•ûÁªèÁΩëÁªúÔºåÂõæÂç∑ÁßØ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[few-shot]]></title>
    <url>%2F2021%2F06%2F22%2Ffew-shot%2F</url>
    <content type="text"><![CDATA[ÁªºËø∞ few-shot few-shot learningÔºöÈÄöËøáÂ∞ëÈáèÊ†∑Êú¨Â≠¶‰π†ËØÜÂà´Ê®°Âûã ÈóÆÈ¢òÔºöËøáÊãüÂêà&amp;Ê≥õÂåñÊÄßÔºåÊï∞ÊçÆÂ¢ûÂº∫ÂíåÊ≠£ÂàôËÉΩÂú®‰∏ÄÂÆöÁ®ãÂ∫¶‰∏äÁºìËß£‰ΩÜ‰∏çËß£ÂÜ≥ÔºåËøòÊòØÊé®Ëçê‰ªéÂ§ßÊï∞ÊçÆ‰∏äËøÅÁßªÂ≠¶‰π† ÂÖ±ËØÜÔºö Ê†∑Êú¨ÈáèÊúâÈôêÁöÑÊÉÖÂÜµ‰∏ãÔºå‰∏ç‰æùÈù†Â§ñÈÉ®Êï∞ÊçÆÂæàÈöæÂæóÂà∞‰∏çÈîôÁöÑÁªìÊûúÔºåÂΩì‰∏ãÊâÄÊúâÁöÑËß£ÂÜ≥ÊñπÊ°àÈÉΩÊòØÂÄüÂä©Â§ñÈÉ®Êï∞ÊçÆ‰Ωú‰∏∫ÂÖàÈ™åÁü•ËØÜÔºåÊûÑÈÄ†Â≠¶‰π†‰ªªÂä° ËøÅÁßªÊï∞ÊçÆ‰πü‰∏çÊòØÈöè‰æøÊâæÁöÑÔºåÊï∞ÊçÆÈõÜÁöÑdomain differenceË∂äÂ§ßÔºåËøÅÁßªÊïàÊûúË∂äÂ∑ÆÔºàe.g. Áî®miniImagenetÂÅöÁ±ªÈó¥ËøÅÁßªÔºåÊïàÊûú‰∏çÈîôÔºå‰ΩÜÊòØÁî®miniImagenetÂÅöbase classÁî®CUBÂÅönovel classÔºåÂ≠¶‰π†ÊïàÊûú‰ºöÊòéÊòæ‰∏ãÈôçÔºâ Êï∞ÊçÆÈõÜÔºö miniImagenetÔºöËá™ÁÑ∂ÂõæÂÉèÔºå600Âº†Ôºå100Á±ª OmniglotÔºöÊâãÂÜôÂ≠óÁ¨¶Ôºå1623Âº†Ôºå50Á±ª CUBÔºöÈ∏üÈõÜÔºå11788Âº†Ôºå200Á±ªÔºåÂèØÁî®‰∫éÁªÜÁ≤íÂ∫¶ÔºåÂèØ‰ª•Áî®‰∫ézero-shot methods pretraining + finetuning pretrainingÈò∂ÊÆµÁî®base classËÆ≠ÁªÉ‰∏Ä‰∏™feature extractor finetuningÈò∂ÊÆµfix feature extractorÈáçÊñ∞ËÆ≠ÁªÉ‰∏Ä‰∏™classifier Âü∫‰∫éÂ∫¶ÈáèÂ≠¶‰π† ÂºïÂÖ•distance metricÂÖ∂ÂÆûÈÉΩÁÆóÂ∫¶ÈáèÂ≠¶‰π†ÔºåÊâÄ‰ª•‰∏äÈù¢Ôºàpretraining+finetuningÔºâÂíå‰∏ãÈù¢Ôºàmeta learningÔºâÁöÑÊñπÊ≥ïÈÉΩÊúâÂ±û‰∫éÂ∫¶ÈáèÂ≠¶‰π†ÁöÑÊñπÊ≥ï Âü∫‰∫éÂÖÉÂ≠¶‰π† base class&amp;novel classÔºöbase classÊòØÂ∑≤ÊúâÁöÑÂ§ßÊï∞ÊçÆÈõÜÔºåÂ§öÁ±ªÂà´ÔºåÂ§ßÊ†∑Êú¨ÈáèÔºånovel classÊòØÊàë‰ª¨Ë¶ÅËß£ÂÜ≥ÁöÑÂ∞èÊï∞ÊçÆÈõÜÔºåÁ±ªÂà´Â∞ëÔºåÊØèÁ±ªÊ†∑Êú¨‰πüÂ∞ë N-way-K-shotÔºöÂü∫‰∫énovel classÂÖàÂú®base class‰∏äÊûÑÂª∫Â§ö‰∏™Â≠ê‰ªªÂä°ÔºåN-wayÂ∞±ÊòØÊûÑÂª∫ÈöèÊú∫N‰∏™Á±ªÂà´ÁöÑÂàÜÁ±ª‰ªªÂä°ÔºåK-shotÂ∞±ÊòØÊØè‰∏™Á±ªÂà´ÂØπÂ∫îÊ†∑Êú¨Èáè‰∏∫K supportset S &amp; queryset QÔºöN-way-K-shotÁöÑËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜÔºåÊù•Ëá™base class‰∏≠Áõ∏ÂêåÁöÑÁ±ªÂà´ÔºåÂùáÁî®‰∫étraining procedure ‰∏é‰º†ÁªüÂàÜÁ±ª‰ªªÂä°ÂØπÊØîÔºö leaderboardÔºöhttps://few-shot.yyliu.net/miniimagenet.html papers [2015 siamese]ÔºöSiamese Neural Networks for One-shot Image RecognitionÔºåÊ†∏ÂøÉÊÄùÊÉ≥Â∞±ÊòØÂü∫‰∫éÂ≠™ÁîüÁΩëÁªúÊûÑÂª∫similarity‰ªªÂä°ÔºåÁî®‰∏Ä‰∏™Â§ßÊï∞ÊçÆÈõÜÊûÑÈÄ†ÁöÑsame/diff pairsÂéªËÆ≠ÁªÉÔºåÁÑ∂ÂêéÁõ¥Êé•Áî®Âú®novel set‰∏äÔºåmetricÊòØreweighted L1 [2016 MatchingNet]ÔºöMatching Networks for One Shot LearningÔºåÊú¨Ë¥®‰∏ä‰πüÊòØÂ≠™ÁîüÁΩëÁªú+metric learningÔºåÁõëÁù£ÁöÑÊòØsupport set SÂíåtest set BÁöÑÁõ∏‰ººÂ∫¶‚Äî‚ÄîÂú®S‰∏ãËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®BÁöÑÈ¢ÑÊµãÁªìÊûúËØØÂ∑ÆÊúÄÂ∞èÔºåÁΩëÁªú‰∏äÁöÑÂàõÊñ∞ÊòØÁî®‰∫Ümemory&amp;attentionÔºåtrain procedureÁöÑÂàõÊñ∞Âú®‰∫é‚Äútest and train conditions must match N-way-K-shot‚ÄùÔºå [2017 ProtoNet]ÔºöPrototypical Networks for Few-shot LearningÔºå [2019 few-shotÁªºËø∞]ÔºöA CLOSER LOOK AT FEW-SHOT CLASSIFICATION Siamese Neural Networks for One-shot Image Recognition Âä®Êú∫ learning good features is expensive when little data is availableÔºö‰∏Ä‰∏™ÂÖ∏Âûã‰ªªÂä°one-shot learning we desire generalize to the new distribution without extensive retraining we propose train a siamese network to rank similarity between inputs capitalize on powerful discriminative features generalize the network to new data/new classes experiment on character recognition ÊñπÊ≥ï general strategy learn image representationÔºösupervised metric-based approachÔºåsiamese neural network reuse the feature extractorÔºöon new dataÔºåwithout any retraining why siamese we hypothesize that networks which do well at verification tasks should generalize to one-shot classification siamese nets twin networks accept distinct inputs that are joined by an energy function at the top twin back shares the weightsÔºösymmetric ÂéüÂßãËÆ∫ÊñáÁî®‰∫Ücontrastive energy functionÔºöcontains dual terms to increase like-pairs energy &amp; decrease unlike-pairs energy in this paper we use weighted L1 + sigmoid model conv-relu-maxpoolingÔºöconv of varying sizes ÊúÄÂêé‰∏Ä‰∏™conv-reluÂÆå‰∫ÜÊé•flatten-fc-sigmoidÂæóÂà∞ÂΩí‰∏ÄÂåñÁöÑfeature vector ÁÑ∂ÂêéÊòØjoined layerÔºöËÆ°ÁÆó‰∏§‰∏™feature vectorÁöÑL1 distanceÂêélearnable reweighting ÁÑ∂ÂêéÊé•sigmoid loss binary classifier regularized CE loss functionÈáåÈù¢Âä†‰∫Ülayer-wise-L2Ê≠£Âàô bpÁöÑÊó∂ÂÄô‰∏§‰∏™Â≠™ÁîüÁΩëÁªúÁöÑbp gradientÊòØadditiveÁöÑ weight initialization conv weightsÔºömean 0 &amp; std var 0.01 conv biasÔºömean 0.5 &amp; std var 0.01 fc weightsÔºömean 0 &amp; std var 0.2 fc biasÔºömean 0.5 &amp; std var 0.01 learning schedule uniform lr decay 0.01 individual lr rate &amp; momentum annealing augmentation individual affine distortions ÊØè‰∏™affine paramÁöÑprobability 0.5 &lt;img src=&quot;few-shot/affine.png&quot; width=&quot;45%;&quot; /&gt; ÂÆûÈ™å dataset OmniglotÔºö50‰∏™Â≠óÊØçÔºàinternational/lesser known/fictitiousÔºâ ËÆ≠ÁªÉÁî®ÁöÑÂ≠êÈõÜÔºö60% of the total dataÔºå12‰∏™drawerÂàõÂª∫ÁöÑ30‰∏™Â≠óÊØçÔºåÊØèÁ±ªÊ†∑Êú¨Êï∞‰∏ÄÊ†∑Â§ö validationÔºö4‰∏™drawerÁöÑ10‰∏™Â≠óÊØç testÔºö4‰∏™drawerÁöÑ10‰∏™Â≠óÊØç 8‰∏™affine transformsÔºö9ÂÄçÊ†∑Êú¨ÈáèÔºåsame&amp;different pairs Âú®‰∏çÁªèËøáÂæÆË∞ÉËÆ≠ÁªÉÁöÑÊù°‰ª∂‰∏ãÔºåÊ®°ÂûãÁõ¥Êé•Â∫îÁî®Âú®MNISTÊï∞ÊçÆÈõÜÔºå‰ªçÊúâ70%ÁöÑÂáÜÁ°ÆÁéáÔºöÊ≥õÂåñËÉΩÂäõ ËØÑ‰ª∑ Â≠™ÁîüÁΩëÁªúÂØπ‰∫é‰∏§‰∏™ÂõæÂÉè‰πãÈó¥ÁöÑÂ∑ÆÂºÇÊòØÈùûÂ∏∏ÊïèÊÑüÁöÑ ‰∏ÄÂè™ÈªÑËâ≤ÁöÑÁå´ÂíåÈªÑËâ≤ÁöÑËÄÅËôé‰πãÈó¥ÁöÑÂ∑ÆÂà´Ë¶ÅÊØî‰∏ÄÂè™ÈªÑËâ≤ÁöÑÁå´ÂíåÈªëËâ≤ÁöÑÁå´‰πãÈó¥ÁöÑÂ∑ÆÂà´Êõ¥Â∞è ‰∏Ä‰∏™Áâ©‰ΩìÂá∫Áé∞Âú®ÂõæÂÉèÁöÑÂ∑¶‰∏äËßíÂíåÂõæÂÉèÁöÑÂè≥‰∏ãËßíÊó∂ÂÖ∂ÊèêÂèñÂà∞ÁöÑÁâπÂæÅ‰ø°ÊÅØÂèØËÉΩÊà™ÁÑ∂‰∏çÂêå Â∞§ÂÖ∂ÁªèËøáÂÖ®ËøûÊé•Â±ÇÔºåÁ©∫Èó¥‰ΩçÁΩÆ‰ø°ÊÅØË¢´Á†¥Âùè ÊâãÂÜôÂ≠óÁ¨¶Êï∞ÊçÆÈõÜÁõ∏ÊØîËæÉ‰∫éImageNetÂ§™ÁÆÄÂçï‰∫Ü ‰ºòÂåñÁΩëÁªúÁªìÊûÑÔºöMatchingNet Êõ¥Â•ΩÁöÑËÆ≠ÁªÉÁ≠ñÁï•Ôºömeta learning Áé∞Âú®ÂéªÂ§çÁé∞Â∑≤ÁªèÊ≤°Âï•ÊÑè‰πâÔºåÁÆóÊòØmetric learningÂú®Â∞èÊ†∑Êú¨Â≠¶‰π†‰∏äÁöÑ‰∏Ä‰∏™startupÂêß MatchingNet: Matching Networks for One Shot Learning Âä®Êú∫ learning new concepts rapidly from little data employ ideas metric learning memory cell define one-shot learning problems Omniglot ImageNet language tasks ËÆ∫ÁÇπ parametric models learns slow and require large datasets non-parametric models rapidly assimilate new examples we aim to incorporate both we propose Matching Nets uses recent advances in attention and memory that enable rapid learning test and train conditions must matchÔºöÂ¶ÇÊûúË¶ÅÊµãËØï‰∏Ä‰∏™nÁ±ªÁöÑÊñ∞ÂàÜÂ∏ÉÔºåÂ∞±Ë¶ÅÂú®mÁ±ªÂ§ßÊï∞ÊçÆÈõÜ‰∏äËÆ≠Á±ª‰ººÁöÑminibatch‚Äî‚ÄîÊäΩn‰∏™Á±ªÔºåÊØèÁ±ªshow a few examples ÊñπÊ≥ï build one-shot learning within the set-to-set framework ËÆ≠ÁªÉ‰ª•ÂêéÁöÑÊ®°Âûã‰∏çÈúÄË¶ÅËøõ‰∏ÄÊ≠•tuningÂ∞±ËÉΩproduce sensible test labels for unobserved classes given a small support set $S=\{(x_i,y_i)\}^k_{i=0}$ train a classifier $c_S$ given a test example $\hat x$Ôºöwe get a probability distribution $\hat y=c_S(\hat x)$ define the mappingÔºö$S \rightarrow c_S $ to be $P(\hat y| \hat x ,S)$ when given a new support set $S^{‚Äò}=\{\hat x\}$ÔºöÁõ¥Êé•Áî®Ê®°ÂûãPÂéªÈ¢ÑÊµã$\hat y$Â∞±ÂèØ‰ª•‰∫Ü simplest formÔºö \hat y = \sum_{i=1}^k a(\hat x, x_i)y_i aÊòØattention mechanismÔºöÂ¶ÇÊûúÂíåÊµãËØïÊ†∑Êú¨$\hat x$ÊúÄËøúÁöÑb‰∏™ÊîØÊåÅÊ†∑Êú¨$x_i$ÁöÑattentionÊòØ0ÔºåÂÖ∂‰Ωô‰∏∫‰∏Ä‰∏™ÂÆöÂÄºÔºåËøôÂ∞±Á≠â‰ª∑‰∫é‰∏Ä‰∏™k-b-NNÊú∫Âà∂ $y_i$ act as memoriesÔºöÂèØ‰ª•ÊääÊØè‰∏™$y_i$Áúã‰ΩúÊòØÊØè‰∏™$x_i$ÊèêÂèñÂà∞ÁöÑ‰ø°ÊÅØ‰øùÂ≠òÊàêmemory workflowÂÆö‰πâÔºögiven a inputÔºåÊàë‰ª¨Âü∫‰∫éattentionÈîÅÂÆöcorresponding samples in the support setÔºåÂπ∂retrieve the label attention kernel Áî®‰∏Ä‰∏™embedding functionÂÖàÂ∞Ü$\hat x$Âíå$x_i$ËΩ¨ÂåñÊàêembeddings ÁÑ∂ÂêéËÆ°ÁÆóÂíåÊØè‰∏™$x_i$ embeddingÁöÑcosine distance ÁÑ∂ÂêésoftmaxÔºåÂæóÂà∞ÊØè‰∏™ÁöÑattention value softmax‰πãÂêéÁöÑattention valueÔºåÂ§ßÈÉ®ÂàÜÊòØNÈÄâ1ÔºåÂ¶ÇÊûúÊØè‰∏™attention valueÈÉΩ‰∏çÈ´òÔºåËØ¥Êòéquery sampleÂíåËÆ≠ÁªÉÈõÜÊØèÁ±ªÈÉΩ‰∏çÂÉèÔºåÊòØ‰∏™novel Full Context EmbeddingsÔºàFCEÔºâ ÁÆÄÂçïÁöÑÊ®°Âºè‰∏ãfÂíågÂ∞±ÊòØ‰∏§‰∏™shared weightsÁöÑCNN feature extractorÔºåFCEÊòØÊé•Âú®Â∏∏ËßÑfeature vectorÂêéÈù¢Á≤æÂøÉËÆæËÆ°ÁöÑ‰∏Ä‰∏™ÁªìÊûÑ ËÆæËÆ°ÊÄùË∑Ø gÔºösupport set don‚Äôt get embedded individually fÔºösupport set modify how we embed the test image the first issueÔºö bidirectional Long-Short Term Memory encoder the whole support set as contextsÔºåeach time stepÁöÑËæìÂÖ•ÊòØ$g^{‚Äò}(x_i)$ skip connection g(x_i) = \overrightarrow{h_i}+\overleftarrow{h_i}+g^{'}(x_i) the second issue LSTM with read attention over the whole set S $f(\hat x, S)=attLSTM(f^{‚Äò}(\hat x), g(S), K)$ $f^{‚Äò}(\hat x)$ÊòØquery sampleÁöÑfeature vectorÔºå‰Ωú‰∏∫LSTM each time stepÁöÑËæìÂÖ• $K$ÊòØfixed number of unrolling stepsÔºåÈôêÂà∂LSTMËÆ°ÁÆóÁöÑstepÔºå‰πüÂ∞±ÊòØfeature vectorÂèÇ‰∏éLSTMÂæ™ÁéØËÆ°ÁÆóÁöÑÊ¨°Êï∞ÔºåÊúÄÁªàÁöÑËæìÂá∫ÊòØ$h_K$ skip connection as above support set SÁöÑÂºïÂÖ•Ôºö content based attention + softmax $r_{k-1}$Âíå$h_{k-1}$ÊòØconcatÂà∞‰∏ÄËµ∑Ôºå‰Ωú‰∏∫hidden statesÔºö„ÄêQUESTION„ÄëËøôÊ†∑lstm cellÁöÑhidden sizeÂ∞±Âèò‰∫ÜÂïäÔºüÔºüÔºü attention of K fixed unrolling steps encode $x_i$ in the context of the support set S training strategy the training procedure has to be chosen carefully so as to match the never seen task defineÔºö‰ªéÂÖ®ÈõÜ‰∏≠ÈÄâÂèñfew unique classes(e.g. 5)ÔºåÊØè‰∏™Á±ªÂà´ÈÄâÂèñfew examples(e.g. 1-5)ÔºåÊûÑÊàêsupport set SÔºåÂÜç‰ªéÂØπÂ∫îÁ±ªÂà´ÊäΩ‰∏Ä‰∏™batch BÔºåËÆ≠ÁªÉÁõÆÊ†áÂ∞±ÊòØminimise the error predicting the labels in the batch B conditioned on the support set S batch BÁöÑÈ¢ÑÊµãËøáÁ®ãÂ∞±ÊòØfigure1ÔºöÈúÄË¶Å$g(S(x_i,y_i))$Âíå$f(\hat x)$ËÆ°ÁÆó$P(\hat y|\hat x, S)$ÔºåÁÑ∂ÂêéÂíå$gt(\hat y)$ËÆ°ÁÆólog loss ÂÆûÈ™å Ê®°Âºè N-way-K-shot train one-shot testÔºöÁî®ÂîØ‰∏ÄÁöÑone-shot novel sampleÁîüÊàêÂØπÂ∫îÁ±ªÂà´ÁöÑfeature vectorÔºåÁÑ∂ÂêéÂØπÊØè‰∏™test sampleËÆ°ÁÆócosine distanceÔºåÈÄâÊã©ÊúÄËøëÁöÑ‰Ωú‰∏∫ÂÖ∂Á±ªÂà´ comparing methods baseline classifier + NN MANN Convolutional Siamese Net + NN further finetuningÔºöone-shot ÁªìËÆ∫ using more examples for k-shot classification helps all models 5-way is easier than 20-way siamese netÂú®5-shotÁöÑÊó∂ÂÄôË∑üour methodÂ∑Æ‰∏çÂ§öÔºå‰ΩÜÊòØone-shot degrades rapidly FCEÂú®ÁÆÄÂçïÊï∞ÊçÆÈõÜÔºàOmniglotÔºâ‰∏äÊ≤°Âï•Áî®ÔºåÂú®harder taskÔºàminiImageNetÔºâÊòæËëóÊèêÂçá A CLOSER LOOK AT FEW-SHOT CLASSIFICATION Âä®Êú∫ ‰∏∫‰∏ªÊµÅÊñπÊ≥ïÊèê‰æõ‰∏Ä‰∏™consistent comparative analysisÔºåÂπ∂‰∏îÂèëÁé∞Ôºö deeper backbones significantly reduce differences reducing intra-class variation is an important factor when shallow backbone propose a modified baseline method achieves com- petitive performance verified on miniImageNet &amp; CUB in realistic cross-domain settings generalization analysis baseline method with standard fine-tuning win ËÆ∫ÁÇπ three main categories of methods initialization based aims to learn good model initialization to achieve rapid adaption with a limited number of training samples have difficulty in handling domain shifts metric learning based ËÆ≠ÁªÉÁõÆÊ†áÊòØlearn to compare if a model can determine the similarity of two images, it can classify an unseen input image with the labeled instancesÔºöÊú¨Ë¥®ÊòØsimilarityËÆ°ÁÆóÂô®ÔºåËÑ±Á¶ªlabel level Ëä±ÂºèËÆ≠ÁªÉÁ≠ñÁï•Ôºömeta learning/graph Ëä±ÂºèË∑ùÁ¶ªmetricÔºöcosine/Euclidean turns outÂ§ßÂèØ‰∏çÂøÖÔºö a simple baseline method with a distance- based classifier is competitive to the sophisticated algorithms simply reducing intra-class variation in a baseline method leads to competitive performance hallucination based Áî®base classËÆ≠ÁªÉ‰∏Ä‰∏™ÁîüÊàêÊ®°ÂûãÔºåÁÑ∂ÂêéÁî®ÁîüÊàêÊ®°ÂûãÁªônovel classÈÄ†ÂÅáÊï∞ÊçÆ ÈÄöÂ∏∏Âíåmetric-basedÊ®°ÂûãÁªìÂêàËµ∑Êù•Áî®Ôºå‰∏çÂçïÁã¨ÂàÜÊûê two main challenges Ê≤°Ê≥ïÁªü‰∏ÄÊ®™ÂêëÊØîËæÉ implementation detailsÊúâÂ∑ÆÂºÇÔºåbaseline approachË¢´under-estimatedÔºöÊó†Ê≥ïÂáÜÁ°ÆÈáèÂåñthe relative performance gain lack of domain shift between base &amp; novel datasetsÔºömakes the evaluation scenarios unrealistic our work ÈíàÂØπ‰ª£Ë°®ÊÄßÊñπÊ≥ïconduct consistent comparative experiments on common ground discoveries on deeper backbones ËΩªÂæÆÊîπÂä®baseline methodËé∑ÂæóÊòæËëóÊèêÂçá replace the linear classifier with distance-based classifier practical sceneries with domain shift ÂèëÁé∞ËøôÁßçÁé∞ÂÆûÂú∫ÊôØ‰∏ãÔºåÈÇ£‰∫õ‰ª£Ë°®ÊÄßÁöÑfew-shot methodsÂèçËÄåÂπ≤‰∏çËøábaseline method open source codeÔºöhttps://github.com/wyharveychen/CloserLookFewShot ÊñπÊ≥ï baseline standard transfer learningÔºöpre-training + fine-tuning training stage train a feature extractor $f_{\theta}$ and a classifier $C_{W_b}$ use abundant base class labeled data standard CE loss fine-tuning stage fix feature extractor $f_{\theta}$ train a new classifier $C_{W_n}$ use the few labeled novel samples standard CE loss baseline++ variant of the baselineÔºöÂîØ‰∏ÄÁöÑ‰∏çÂêåÂ∞±Âú®‰∫éclassifier design ÊòæÂºèÂú∞reduce intra-class varation among features during trainingÔºåÂíåcenter lossÊÄùË∑ØÊúâÁÇπÂÉèÔºå‰ΩÜÊòØcenter lossÁöÑË¥®ÂøÉÊòØÊªëÂä®Âπ≥ÂùáÁöÑÔºåËøôÈáåÈù¢ÁöÑË¥®ÂøÉÊòØlearnableÁöÑ training stage write the weight matrix $W_b$ as $[w_1, w_2, ‚Ä¶, w_c]$ÔºåÁ±ª‰ººÊØèÁ±ªÁöÑÁ∞áÂøÉ for an input featureÔºåcompute cosine similarity multiply a class-wise learnable scalar to adjust origin [-1,1] value to fit softmax ÁÑ∂ÂêéÁî®softmaxÂØπsimilarity vectorËøõË°åÂΩí‰∏ÄÂåñÔºå‰Ωú‰∏∫predict label the softmax function prevents the learned weight vectors collapsing to zerosÔºöÊØèÁ±ªÁöÑÈ¢ÑÊµãdistanceÈÉΩÊòØ0ÊòØÁΩëÁªúÊØîËæÉÂÆπÊòìÈô∑ÂÖ•ÁöÑÂ±ÄÈÉ®ÊúÄ‰ºòËß£ „Äêin fine-tuning stageÔºüÔºü„Äë meta-learning algorithms three distance metric based methodsÔºöMatchingNetÔºåProtoNetÔºåRelationNet one initialization based methodÔºöMAML meta-training stage a collection of N-way-K-shot tasks ‰ΩøÂæóÊ®°Âûã$M(*|S)$Â≠¶‰ºöÁöÑÊòØ‰∏ÄÁßçÂ≠¶‰π†Ê®°Âºè‚Äî‚ÄîÂú®ÊúâÈôêÊï∞ÊçÆ‰∏ãÂÅöÈ¢ÑÊµã meta-testing stage ÊâÄÊúâÁöÑnovel dataÈÉΩ‰Ωú‰∏∫ÂØπÂ∫îÁ±ªÂà´ÁöÑsupport set (class mean) Ê®°ÂûãÂ∞±Áî®Ëøô‰∏™Êñ∞ÁöÑsupport setÊù•ËøõË°åÈ¢ÑÊµã Different meta-learning methods‰∏ªË¶ÅÂå∫Âà´Âú®‰∫éÂ¶Ç‰ΩïÂü∫‰∫ésupport setÂÅöÈ¢ÑÊµãÔºå‰πüÂ∞±ÊòØclassifierÁöÑËÆæËÆ° MatchingNetËÆ°ÁÆóÁöÑÊòØqueryÂíåsupport setÁöÑÊØè‰∏™cosine distanceÔºåÁÑ∂Âêémean per class ProtoNetÊòØÂÖàÂØπsupport featuresÊ±Çclass meanÔºåÁÑ∂ÂêéEuclidean distance RelationNetÂÖàÂØπsupport featuresÊ±Çclass meanÔºåÁÑ∂ÂêéÂ∞ÜË∑ùÁ¶ªËÆ°ÁÆóÊ®°ÂùóÊõøÊç¢Êàêlearnable relation module ÂÆûÈ™å three scenarios generic object recognitionÔºömini-ImageNetÔºå100Á±ªÔºå600Âº†per classÔºå„Äê64-baseÔºå16-valÔºå20-novel„Äë fine-grained image classificationÔºöCUB-200-2011Ôºå200Á±ªÔºåÊÄªÂÖ±11,788Âº†Ôºå„Äêrandom 100-baseÔºå50-valÔºå50-novel„Äë cross-domain adaptationÔºömini-ImageNet ‚Äî&gt; CUBÔºå„Äê100-mini-ImageNet-baseÔºå50-CUB-valÔºå50-CUB-test„Äë training details baselineÂíåbaseline++Ê®°ÂûãÔºötrain 400 epochsÔºåbatch size 16 meta learning methodsÔºö train 60000 episodes for 5-way-1-shot tasksÔºåtrain 40000 episodes for 5-way-5-shot tasks use validation set to select the training episodes with the best acc k-shot for support setÔºå16 instances for query set Adam with 1e-3 initial lr standard data augmentationÔºöcropÔºåleft-right flipÔºåcolor jitter testing stage average over 600 experiments each experiment randomly choose 5-way-k-shot support set + 16 instances query set meta learning methodsÁõ¥Êé•Âü∫‰∫ésupport setÁªôÂá∫ÂØπquery setÁöÑÈ¢ÑÊµãÁªìÊûú baseline methodsÂü∫‰∫ésupport setËÆ≠ÁªÉ‰∏Ä‰∏™Êñ∞ÁöÑÂàÜÁ±ªÂ§¥Ôºå100 iterationsÔºåbatch size 4 Ê®°Âûãdetails baseline++ÁöÑsimilarity‰πò‰∏ä‰∫Üclass-wise learnable scalar MachingNetÁî®‰∫ÜFCE classification layer without fine-tuningÁâàÊú¨Ôºå‰πü‰πò‰∫Üclass-wise learnable scalar RelationNetÂ∞ÜL2 normÊõøÊç¢ÊàêsoftmaxÂä†ÈÄüËÆ≠ÁªÉ MAML‰ΩøÁî®‰∫Ü‰∏ÄÈò∂Ê¢ØÂ∫¶Ëøë‰ººfor efficiency ÂàùÊ≠•ÁªìÊûú 4-layer conv backbone input size 84x84 originÂíåre-implementationÁöÑÁ≤æÂ∫¶ÂØπÊØî ÂéüÂßãÁöÑbaselineÊ≤°Âä†data augmentationÔºåÊâÄ‰ª•ËøáÊãüÂêà‰∫ÜÁ≤æÂ∫¶Â∑ÆÔºåË¢´underestimated‰∫Ü MatchingNetÂä†‰∫ÜÈÇ£‰∏™scalar shiftÁöÑÊîπËøõ‰ª•ÂêéÁ≤æÂ∫¶ÊúâÊòæËëóÊèêÂçá ProtoNetÂéüËÆ∫ÊñáÊòØ20-shot&amp;30-shotÔºåÊú¨Êñá‰∏ªË¶ÅÊØîËæÉ1-shotÂíå5-shotÔºåÁ≤æÂ∫¶ÈÉΩÊîæÂá∫Êù•‰∫Ü our experiment setting‰∏ãÂêÑÊ®°ÂûãÁöÑÁ≤æÂ∫¶ÂØπÊØî baseline++Â§ßÂπÖÊèêÂçáÁ≤æÂ∫¶ÔºåÂ∑≤ÁªèË∑ümeta learning methodsÂ∑Æ‰∏çÂ§ö‰∫Ü ËØ¥Êòéfew-shotÁöÑkey factorÊòØreduce intra-class variation ‰ΩÜÊòØË¶ÅÊ≥®ÊÑèÁöÑÊòØËøôÊòØÂú®4-layer-convÁöÑbackbone setting‰∏ãÔºådeeper backbone can inherently reduce intra-class variation Â¢ûÂä†ÁΩëÁªúÊ∑±Â∫¶ ‰∏äÈù¢ËØ¥‰∫ÜÔºådeeper backboneËÉΩÂ§üÈöêÂºèÂú∞Èôç‰ΩéÁ±ªÂÜÖË∑ùÁ¶ª deeper models conv4 conv6ÔºöÁõ∏ÂØπ‰∫éconv4ÈÇ£‰∏™Ê®°ÂûãÔºåÂä†‰∫Ü‰∏§Â±Çconv blocks without pooling resnet10ÔºöÁÆÄÂåñÁâàresnet18Ôºår18ÈáåÈù¢conv blockÁöÑ‰∏§Â±ÇÂç∑ÁßØÊç¢Êàê‰∏ÄÂ±Ç resnet18Ôºöorigin paper resnet34Ôºöorigin paper ÈöèÁùÄÁΩëÁªúÂä†Ê∑±ÔºåÂêÑÊñπÊ≥ïÁöÑÁ≤æÂ∫¶Â∑ÆÂºÇÁº©Â∞èÔºåbaselineÊñπÊ≥ïÁîöËá≥ÂèçË∂Ö‰∫Ü‰∏Ä‰∫õmeta learningÊñπÊ≥ï effect of domain shift ‰∏Ä‰∏™Áé∞ÂÆûÂú∫ÊôØÔºömini-ImageNet ‚Äî&gt; CUBÔºåÊî∂ÈõÜgeneral class dataÁõ∏ÂØπÂÆπÊòìÔºåÊî∂ÈõÜfine-grainedÊï∞ÊçÆÈõÜÂàôÊõ¥Âõ∞Èöæ Áî®resnet18ÂÆûÈ™å Baseline outperforms all meta-learning methods under this scenario Âõ†‰∏∫meta learning methodsÁöÑÂ≠¶‰π†ÂÆåÂÖ®‰æùËµñ‰∫ébase support classÔºånot able to adapt ÈöèÁùÄdomain difference get largerÔºåBaselineÁõ∏ÂØπ‰∫éÂÖ∂‰ªñÊñπÊ≥ïÁöÑgap‰πüÈÄêÊ∏êÊãâÂ§ß ËØ¥Êòé‰∫ÜÂú®domain shiftÂú∫ÊôØ‰∏ãÔºåadaptation based methodÁöÑÂøÖË¶ÅÊÄß further adapt meta-learning methods MatchingNet &amp; ProtoNetÔºöË∑übaselineÊñπÊ≥ï‰∏ÄÊ†∑Ôºåfix feature extractorÔºåÁÑ∂ÂêéÁî®novel set train a new classifier MAMLÔºönot feasible to fix the featureÔºåÁî®novel set finetuneÊï¥‰∏™ÁΩëÁªú RelationNetÔºöfeaturesÊòØconv mapsËÄå‰∏çÊòØvectorÔºårandomly split‰∏ÄÈÉ®ÂàÜnovel set‰Ωú‰∏∫ËÆ≠ÁªÉÈõÜ MatchingNet &amp; MAMLÈÉΩÊúâÂ§ßÂπÖÁ≤æÂ∫¶ÊèêÂçáÔºåÂ∞§ÂÖ∂Âú®domain shiftÂú∫ÊôØ‰∏ãÔºå‰ΩÜÊòØProtoNet‰ºöÊéâÁÇπÔºåËØ¥ÊòéadaptationÊòØÂΩ±ÂìçÁ≤æÂ∫¶ÁöÑkey factorÔºå‰ΩÜÊòØËøòÊ≤°ÊúâÂÆåÁæéËß£ÂÜ≥ÊñπÊ°à]]></content>
      <tags>
        <tag>Â∞èÊ†∑Êú¨</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ATSS]]></title>
    <url>%2F2021%2F06%2F17%2FATSS%2F</url>
    <content type="text"><![CDATA[ATSS: Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection Âä®Êú∫ anchor-basedÂíåanchor-freeÊñπÊ≥ïÁöÑÊú¨Ë¥®Âå∫Âà´ÊòØÂØπÊ≠£Ë¥üÊ†∑Êú¨ÁöÑÂÆö‰πâÔºåËøô‰πüÁõ¥Êé•ÂØºËá¥‰∫Üperformance gap we propose ATSS adaptive training sample selection automatically select positive and negative samples according to statistical characteristics of objects anchor-based&amp;anchor-freeÊ®°Âûã‰∏äÈÉΩÊ∂®ÁÇπ discuss tiling multiple anchors ËÆ∫ÁÇπ ‰∏ªÊµÅanchor-basedÊñπÊ≥ï one-stage/two-stage tile a large number of preset anchors on the image output these refined anchors as detection results anchor-free detectors‰∏ªË¶ÅÂàÜÊàê‰∏§Áßç key-point basedÔºöÈ¢ÑÊµãËßíÁÇπ/ËΩÆÂªìÁÇπ/heatmapÔºåÁÑ∂ÂêéboundËΩÆÂªìÂæóÂà∞Ê°Ü center-basedÔºöÈ¢ÑÊµã‰∏≠ÂøÉÁÇπÔºåÁÑ∂ÂêéÂü∫‰∫é‰∏≠ÂøÉÁÇπÂõûÂΩí4‰∏™Ë∑ùÁ¶ª Ê∂àÈô§pre-defined anchorsÁöÑhyper-paramsÔºöÂº∫Âåñgeneralization ability ‰∏æ‰æãÂØπÊØîRetinaNet&amp;FCOS RetinaNetÔºöone-stage anchor-based FCOSÔºöcenter-based anchor-free ‰∏ªË¶ÅÂå∫Âà´1ÔºöanchorÊï∞ÈáèÔºåRetinaNetÊòØhxwx9ÔºåFCOSÊòØhxwx1 ‰∏ªË¶ÅÂå∫Âà´2ÔºöÊ≠£Ê†∑Êú¨ÂÆö‰πâÔºåRetinaNetÊòØ‰∏égt boxÁöÑIOUÂ§ß‰∫é‰∏ÄÂÆöÈòàÂÄºÁöÑanchorÔºåFCOSÊòØfeaturemap‰∏äÊâÄÊúâËêΩËøõÊ°ÜÂÜÖÁöÑÊ†ºÂ≠êÁÇπ ‰∏ªË¶ÅÂå∫Âà´3ÔºöÂõûÂΩíÊñπÂºèÔºåRetinaNetÊòØÂõûÂΩígtÁõ∏ÂØπpos anchorÁöÑÁõ∏ÂØπÂÅèÁßªÈáèÔºåFCOSÊòØÈ¢ÑÊµãÂõõÊù°ËæπÁõ∏ÂØπ‰∏≠ÂøÉÁÇπÁöÑÁªùÂØπË∑ùÁ¶ª Difference Analysis of Anchor-based and Anchor-free Detection we focus on the last two differencesÔºöÊ≠£Ë¥üÊ†∑Êú¨ÂÆö‰πâ &amp; ÂõûÂΩístarting status ËÆæÂÆöRetinaNet‰πüÊòØone square anchor per locationÔºåÂíåFCOS‰øùÊåÅ‰∏ÄËá¥ experiment setting MS COCOÔºö80Á±ªÂâçÊôØÔºåcommon split ImageNet pretrained ResNet-50 resize input SGDÔºå90K iterationsÔºå0.9 momentumÔºå1e-4 weight decayÔºå16 batch sizeÔºå0.01 lr with 0.1 lr decay/60K testingÔºö 0.05 score to filter out bg boxes output top 1000 detections per feature pyramid 0.6 IoU thresh per class NMS to give final top 100 detections per image inconsistency removal ‰∫îÂ§ßimprovementsÂä†Âú®FCOS‰∏äËøõ‰∏ÄÊ≠•boost the gap Êàë‰ª¨Â∞ÜÂÖ∂ÈÄêÊ≠•Âä†Âú®RetinaNet‰∏äÔºåËÉΩÂ§üÊãâÂà∞37%ÔºåÂíåFCOSËøòÊúâ0.8‰∏™ÁÇπÁöÑÂ∑ÆË∑ù ÂàÜÊûêessential difference ËÆ≠ÁªÉ‰∏Ä‰∏™Ê£ÄÊµãÊ®°ÂûãÔºåÈ¶ñÂÖàË¶ÅÂàÜÂá∫Ê≠£Ë¥üÊ†∑Êú¨ÔºåÁÑ∂ÂêéÁî®Ê≠£Ê†∑Êú¨Êù•ÂõûÂΩí Classification RetinaNetÁî®anchor boxes‰∏égt boxÁöÑIoUÂÜ≥ÂÆöÊ≠£Ë¥üÊ†∑Êú¨Ôºöbest match anchorÂíåÂ§ß‰∫é‰∏ÄÂÆöIoU threshÁöÑanchorÊòØÊ≠£Ê†∑Êú¨ÔºåÂ∞è‰∫é‰∏ÄÂÆöIoU threshÁöÑanchorÊòØË¥üÊ†∑Êú¨ÔºåÂÖ∂‰ªñÁöÑÊòØignoreÊ†∑Êú¨ FCOSÁî®spatial and scale constraintsÈÄâÊã©Ê≠£Ë¥üÊ†∑Êú¨Ôºögt box‰ª•ÂÜÖÁöÑÊâÄÊúâÂÉèÁ¥†‰Ωú‰∏∫ÂÄôÈÄâÊ≠£Ê†∑Êú¨ÔºåÁÑ∂ÂêéÂéªÊéâÈÉ®ÂàÜÂ∞∫Â∫¶‰∏çÂåπÈÖçÁöÑÂÄôÈÄâÊ†∑Êú¨ÔºåÊ≠£Ê†∑Êú¨‰ª•Â§ñÈÉΩÊòØË¥üÊ†∑Êú¨ÔºåÊ≤°Êúâignore ‰∏§‰∏™Ê®°ÂûãÂú®‰∏§ÁßçÊ†∑Êú¨ÈÄâÊã©Á≠ñÁï•‰∏äÂÆûÈ™åÔºöSpatial and Scale ConstraintÁõ∏ÊØîËæÉ‰∫éIoUÈÉΩ‰ºöÊòæËëóÊèêÁÇπ ÂΩì‰∏§ÁßçÊñπÊ≥ïÈÉΩ‰ΩøÁî®Spatial and Scale ConstraintÁ≠ñÁï•ÈÄâÊã©Ê≠£Ë¥üÊ†∑Êú¨ÔºåÊ®°ÂûãÁ≤æÂ∫¶Â∞±Ê≤°Âï•Â∑ÆÂà´‰∫Ü Regression RetinaNet regresses from the anchor box with 4 offsetsÔºöÂõûÂΩígt boxÁõ∏ÂØπ‰∫éanchor boxÁöÑÂÅèÁßªÈáèÔºåregression starting statusÊòØ‰∏™box FCOS regresses from the anchor point with 4 distancesÔºöÂõûÂΩígt boxÂõõÊù°ËæπÁõ∏ÂØπ‰∫éanchor centerÁöÑË∑ùÁ¶ªÔºåregression starting statusÊòØ‰∏™point ‰∏äÈù¢ÈÇ£‰∏™Ë°®ËØ¥Êòé‰∫ÜÈÄâÊã©ÂêåÊ†∑ÁöÑÊ≠£Ë¥üÊ†∑Êú¨Ôºåregression starting statusÂ∞±ÊòØ‰∏™Êó†ÂÖ≥È°πÔºå‰∏çÂΩ±ÂìçÁ≤æÂ∫¶ Adaptive Training Sample Selection ÔºàATSSÔºâ ÂΩ±ÂìçÊ£ÄÊµãÊ®°ÂûãÁ≤æÂ∫¶ÁöÑessential differenceÂú®‰∫éhow to define positive and negative training samples previous strategiesÈÉΩÊúâsensitive hyperparametersÔºàanchors/scaleÔºâÔºåsome outer objects may be neglected we propose ATSS almost no hyper divides pos/neg samples according to data statistical characteristics ÂØπÊØè‰∏™gt boxÔºåÈ¶ñÂÖàÂú®ÊØè‰∏™level‰∏äÔºåÂü∫‰∫éL2 center distanceÔºåÊâæÂà∞k-closest anchor‚Äî‚Äîk*L‰∏™candidates per gt box ËÆ°ÁÆóÊØè‰∏™candidatesÁöÑmean &amp; var Âü∫‰∫émean &amp; var ËÆ°ÁÆóËøô‰∏™gt boxÁöÑIoU thresh Âú®candidatesÈáåÈù¢ÈÄâÂèñÂ§ß‰∫éÁ≠â‰∫éIoU threshÔºåÂêåÊó∂anchor centerÂú®gt boxÂÜÖÁöÑÔºåÁïô‰ΩúÊ≠£Ê†∑Êú¨ Â¶ÇÊûú‰∏Ä‰∏™acnhor boxÂåπÈÖç‰∫ÜÂ§ö‰∏™gt boxÔºåÈÄâÊã©IoUÂ§ßÁöÑÈÇ£‰∏™‰Ωú‰∏∫Ê†áÁ≠æ Âü∫‰∫écenter distanceÈÄâÊã©anchor boxÔºöÂõ†‰∏∫Ë∂äÈù†ËøëÁõÆÊ†á‰∏≠ÂøÉÔºåË∂äÂÆπÊòìproduceÈ´òÂìÅË¥®Ê°Ü Áî®mean+var‰Ωú‰∏∫IoU threshÔºö higher mean indicates high-quality candidatesÔºåÂØπÂ∫îÁöÑIoU threshÂ∫îËØ•È´ò‰∏ÄÁÇπ higher variation indicates level specificÔºåmean+var‰Ωú‰∏∫threshËÉΩÂ∞ÜcandidatesÈáåÈù¢IoUËæÉÈ´òÁöÑÁ≠õÈÄâÂá∫Êù• limit the anchor center in objectÔºöanchor‰∏≠ÂøÉ‰∏çÂú®ÁõÆÊ†áÊ°ÜÂÜÖÊòæÁÑ∂‰∏çÊòØ‰∏™Â•ΩÊ°ÜÔºåÁî®‰∫éÁ≠õÊéâÂâç‰∏§Ê≠•ÈáåÁöÑÊºèÁΩë‰πãÈ±ºÔºåÂèå‰øùÈô© fair between different objects ÁªüËÆ°‰∏ãÊù•ÊØèÁ±ªÁõÆÊ†áÈÉΩÊúâÂ∑Æ‰∏çÂ§ö0.2kL‰∏™Ê≠£Ê†∑Êú¨Ôºå‰∏éÂ∞∫Â∫¶Êó†ÂÖ≥ ‰ΩÜÊòØRetinaNetÂíåFCOSÈÉΩÊòØÂ§ßÁõÆÊ†áÊ≠£Ê†∑Êú¨Â§öÔºåÂ∞èÁõÆÊ†áÊ≠£Ê†∑Êú¨Â∞ë hyperparam-freeÔºöÂè™Êúâ‰∏Ä‰∏™kÔºå„ÄêËøòÊúâanchor-settingÂë¢ÔºüÔºüÔºü„Äë verification lite versionÔºöË¢´FCOSÂÆòÊñπÂºïÁî®Âπ∂Áß∞‰Ωúcenter samplingÔºåscale limit still exists in this version full versionÔºöÊú¨ÊñáÁâàÊú¨ ‰∏§‰∏™ÊñπÊ≥ïÈÄâÊã©candidatesÁöÑÊñπÊ≥ïÂÆåÂÖ®‰∏ÄËá¥ÔºåÂ∞±ÊòØselect final postivesÁöÑÊñπÊ≥ï‰∏çÂêå hyperparamÁöÑÈ≤ÅÊ£íÊÄß kÂú®‰∏ÄÂÆöËåÉÂõ¥ÂÜÖÔºà7-17ÔºâÁõ∏ÂØπinsensitiveÔºåÂ§™Â§ö‰∫Ü‰ΩéË¥®ÈáèÊ°ÜÂ§™Â§öÔºåÂ§™Â∞ë‰∫Üless statistical Â∞ùËØï‰∏çÂêåÁöÑfix-ratio anchor scaleÂíåfix-scale anchor ratioÔºåÂèëÁé∞Á≤æÂ∫¶Áõ∏ÂØπÁ®≥ÂÆöÔºåËØ¥Êòérobust to anchor settings multi-anchors settings RetinaNetÂú®‰∏çÂêåÁöÑanchor setting‰∏ãÔºåÁ≤æÂ∫¶Âü∫Êú¨‰∏çÂèòÔºåËØ¥Êòé‰∏ªË¶ÅÊ≠£Ê†∑Êú¨ÈÄâÁöÑÂ•ΩÔºå‰∏çÁÆ°‰∏Ä‰∏™locationÁªëÂÆöÂá†‰∏™anchorÁªìÊûúÈÉΩ‰∏ÄÊ†∑]]></content>
      <tags>
        <tag>ÁõÆÊ†áÊ£ÄÊµãÔºåanchor-free&amp;anchor-basedÔºåtricks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[re-labeling]]></title>
    <url>%2F2021%2F05%2F27%2Fre-labeling%2F</url>
    <content type="text"><![CDATA[Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels Âä®Êú∫ label noise single-label benchmark but contains multiple classes in one sample a random crop may contain an entirely different object from the gt label exhaustive multi-label annotations per image is too cost mismatch researches refine the validation set with multi-labels propose new multi-label evaluation metrics ‰ΩÜÊòØÈÄ†ÊàêÊï∞ÊçÆÈõÜÁöÑmismatch we propose re-label use a strong image classifier trained on extra source of data to generate the multi-labels use pixel-wise multi-label predictions before GAPÔºöaddtional location-specific supervision then trained on re-labeled samples further boost with CutMix from single to multi-labelsÔºöÂ§öÊ†áÁ≠æ from global to localizedÔºödense prediction map ËÆ∫ÁÇπ single-label Âíåmulti-label validation setÁöÑmismatch random crop augmentationÂä†Ââß‰∫ÜÈóÆÈ¢ò Èô§‰∫ÜÂ§öÁõÆÊ†áËøòÊúâÂâçËÉåÊôØÔºåÂè™Êúâ23%ÁöÑrandom crops IOU&gt;0.5 ideally label the full set of classes‚Äî‚Äîmulti-label where each objects‚Äî‚Äîlocalized label results in a dense pixel labeling $L\in \{0,1\}^{HWC}$ we propose a re-labeling strategy ReLabel strong classifier external training data generate feature map predictions LabelPooling with dense labels &amp; random crop pooling the label scores from crop region evaluations baseline r50Ôºö77.5% r50 + ReLabelÔºö78.9% r50 + ReLabel + CutMixÔºö80.2% „ÄêQUESTION„ÄëÂêåÊ†∑ÊòØÂºïÂÖ•Â§ñÈÉ®Êï∞ÊçÆÂÆûÁé∞Êó†ÁóõÈïøÁÇπÔºå‰∏énoisy studentÁöÑÂå∫Âà´/Â•ΩÂùèÔºüÔºüÔºü ÁõÆÂâçËÆ∫ÊñáÊèêÂà∞ÁöÑÂ∞±Âè™ÊúâefficiencyÔºåReLabelÊòØone-time costÁöÑÔºåÁü•ËØÜËí∏È¶èÊòØiterative&amp;on-the-flyÁöÑ ÊñπÊ≥ï Re-labeling super annotator state-of-the-art classifier trained on super large dataset fine-tuned on ImageNet and predict ImageNet labels we use open-source trained weights as annotators though trained with single-label supervision still tend to make multi-label predictions EfficientNet-L2 input size 475 feature map size 15x15x5504 output dense label size 15x15x1000 location-specific labels remove GAP heads add a 1x1 conv ËØ¥ÁôΩ‰∫ÜÂ∞±ÊòØ‰∏Ä‰∏™fcn original classifierÁöÑfcÂ±ÇÊùÉÈáç‰∏éÊñ∞Ê∑ªÂä†ÁöÑ1x1 convÂ±ÇÁöÑÊùÉÈáçÊòØ‰∏ÄÊ†∑ÁöÑ labelÁöÑÊØè‰∏™channelÂØπÂ∫î‰∫Ü‰∏Ä‰∏™Á±ªÂà´ÁöÑheatmapÔºåÂèØ‰ª•ÁúãÂà∞disjointly located at each object‚Äôs position LabelPooling loads the pre-computed label map region pooling (RoIAlign) on the label map GAP + softmax to get multi-label vector train a classifier with the multi-label vector uses CE choices space consumption ‰∏ªË¶ÅÊòØÂ≠òÂÇ®label mapÁöÑÁ©∫Èó¥ store only top-5 predictions per imageÔºö10GB time consumption ‰∏ªË¶ÅÊòØËØ¥ÁîüÊàêlabel mapÁöÑone-shot-inference timeÂíålabelPoolingÂºïÂÖ•ÁöÑÈ¢ùÂ§ñËÆ°ÁÆóÊó∂Èó¥ relabelingÔºö10-GPU hours labelPoolingÔºö0.5% additiona training time more efficient than KD annotators Ê†áÊ≥®Â∑•ÂÖ∑Âì™ÂÆ∂Âº∫ÔºöÁõÆÂâçÁúã‰∏ãÊù•eff-L2ÁöÑsupervisionÊïàÊûúÊúÄÂº∫ supervision confidence ÈöèÁùÄimage crop‰∏éÂâçÊôØÁâ©‰ΩìÁöÑIOUÂ¢ûÂ§ßÔºåconfidenceÈÄêÊ∏êÂ¢ûÂä† ËØ¥Êòésupervision provides some uncertainty when low IOU ÂÆûÈ™å]]></content>
      <tags>
        <tag>pretaining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mlpÁ≥ªÂàó]]></title>
    <url>%2F2021%2F05%2F27%2Fmlp%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[[papers] [MLP-Mixer] MLP-Mixer: An all-MLP Architecture for VisionÔºåGoogle [ResMLP] ResMLP: Feedforward networks for image classification with data-efficient trainingÔºåFacebook [references] https://mp.weixin.qq.com/s?__biz=MzUyMjE2MTE0Mw==&amp;mid=2247493478&amp;idx=1&amp;sn=2be608d776b2469b3357da30c42d9770&amp;chksm=f9d2b9fecea530e8cbf07847c2029a1dabb131dbc1d6bd91ed227e41a396dd333afc83b64cf8&amp;scene=21#wechat_redirect https://mp.weixin.qq.com/s/8f9yC2P3n3HYygsOo_5zww MLP-Mixer: An all-MLP Architecture for Vision Âä®Êú∫ image classification task neither of [CNN, attention] are necessary our proposed MLP-Mixer ‰ªÖÂåÖÂê´multi-layer-perceptrons independently to image patches repeated applied across either spatial locations or feature channels two types applied independently to image patches applied across patches ÊñπÊ≥ï overview ËæìÂÖ•ÊòØtoken sequences non-overlapping image patches linear projected to dimension C Mixer Layer maintain the input dimension channel-mixing MLP operate on each token independently ÂèØ‰ª•Áúã‰ΩúÊòØ1x1ÁöÑconv token-mixing MLP operate on each channel independently take each spatial vectors (hxw)x1 as inputs ÂèØ‰ª•Áúã‰ΩúÊòØ‰∏Ä‰∏™global depth-wise convÔºås1Ôºåsame padÔºåkernel sizeÊòØ(h,w) ÊúÄÂêéÂØπtoken embeddingÂÅöGAPÔºåÊèêÂèñsequence vecÔºåÁÑ∂ÂêéËøõË°åÁ±ªÂà´È¢ÑÊµã idea behind Mixer clearly separate the per-location operations &amp; cross-location operations CNNÊòØÂêåÊó∂ËøõË°åËøô‰ø©ÁöÑ transformerÁöÑMSAÂêåÊó∂ËøõË°åËøô‰ø©ÔºåMLPÂè™ËøõË°åper-location operations Mixer Layer two MLP blocks given input $X\in R^{SC}$ÔºåS for spatial dimÔºåC for channel dim ÂÖàÊòØtoken-mixing MLP acts on S dim maps $R^S$ to $R^S$ share across C-axis LN-FC-GELU-FC-residual ÁÑ∂ÂêéÊòØchannel-mixing MLP acts on C dim maps $R^C$ to $R^C$ share across S-axis LN-FC-GELU-FC-residual fixed widthÔºåÊõ¥Êé•Ëøëtransformer/RNNÔºåËÄå‰∏çÊòØCNNÈÇ£ÁßçÈáëÂ≠óÂ°îÁªìÊûÑ ‰∏ç‰ΩøÁî®positional embeddings the token-mixing MLPs are sensitive to the order of the input tokens may learn to represent locations ÂÆûÈ™å ResMLP: Feedforward networks for image classification with data-efficient training Âä®Êú∫ entirely build upon MLP alternates from a simple residual network a linear layer to interact with image patches a two-layer FFN to interact independently with each patch affine transformÊõø‰ª£LNÊòØ‰∏Ä‰∏™ÁâπÂà´‰πãÂ§Ñ trained with modern strategy heavy data-augmentation optionally distillation show good performace on ImageNet classification ËÆ∫ÁÇπ strongly inspired by ViT but simpler Ê≤°ÊúâattentionÂ±ÇÔºåÂè™ÊúâfcÂ±Ç+gelu Ê≤°ÊúânormÂ±ÇÔºåÂõ†‰∏∫much more stable to trainÔºå‰ΩÜÊòØÁî®‰∫Üaffine transformation ÊñπÊ≥ï overview takes flattened patches as inputs typically N=16Ôºö16x16 linear project the patches into embeddings form $N^2$ d-dim embeddings ResMLP Layer main the dim throughout $[N^2,d]$ a simple linear layer interaction between the patches applied to all channels independently Á±ª‰ººdepth-wise conv with global kernelÁöÑ‰∏úË•øÔºåÁ∫øÊÄßÔºÅÔºÅ a two-layer-mlp fc-GELU-fc independently applied to all patches ÈùûÁ∫øÊÄßÔºÅÔºÅ average pooled $[d-dim]$ + linear classifier $cls-dim$ Residual Multi-Layer Perceptron Layer a linear layer + a FFN layer each layer is paralleled with a skip-connection Ê≤°Áî®LNÔºå‰ΩÜÊòØÁî®‰∫Ülearnable affine transformation $Aff_{\alpha, \beta} (x) = Diag(\alpha) x + \beta$ rescale and shifts the input component-wiseÔºöÂØπÊØè‰∏™patchÔºåÂàÜÂà´ÂÅöaffineÂèòÊç¢ Âú®Êé®ÁêÜÈò∂ÊÆµÂèØ‰ª•‰∏é‰∏ä‰∏ÄÂ±ÇÁ∫øÊÄßÂ±ÇÂêàÂπ∂Ôºöno cost Áî®‰∫Ü‰∏§Ê¨° Á¨¨‰∏Ä‰∏™Áî®Âú®main path‰∏äÁî®Êù•Êõø‰ª£LNÔºöÂàùÂÄº‰∏∫identity transform(1,0) Á¨¨‰∫å‰∏™Âú®residual pathÈáåÈù¢Ôºådown scale to boostÔºåÁî®‰∏Ä‰∏™small valueÂàùÂßãÂåñ given inputÔºö $d\times N^2$ matrix $X$ affineÂú®d-dim‰∏äÂÅö Á¨¨‰∏Ä‰∏™Linear layerÂú®$N^2-dim$‰∏äÂÅöÔºöÂèÇÊï∞Èáè$N^2 \times N^2$ Á¨¨‰∫å„ÄÅ‰∏â‰∏™Linear layerÂú®$d-dim$‰∏äÂÅöÔºöÂèÇÊï∞Èáè$d \times 4d$ &amp; $4d \times d$]]></content>
      <tags>
        <tag>mlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[torch-note]]></title>
    <url>%2F2021%2F05%2F24%2Ftorch-note%2F</url>
    <content type="text"><![CDATA[torch.cuda.amp Ëá™Âä®Ê∑∑ÂêàÁ≤æÂ∫¶ÔºöFloatTensor &amp; HalfTensor torch.jit.script Â∞ÜÊ®°Âûã‰ªéÁ∫ØPythonÁ®ãÂ∫èËΩ¨Êç¢‰∏∫ËÉΩÂ§üÁã¨Á´ã‰∫éPythonËøêË°åÁöÑTorchScriptÁ®ãÂ∫è torch.nn.DataParallel torch.flatten(input, start_dim=0, end_dim=-1) Â±ïÂºÄstart_dimÂà∞end_dim‰πãÈó¥ÁöÑdimÊàê‰∏ÄÁª¥]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LV-ViT]]></title>
    <url>%2F2021%2F05%2F21%2FLV-ViT%2F</url>
    <content type="text"><![CDATA[[LV-ViT 2021] Token Labeling: Training an 85.4% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNetÔºåÊñ∞Âä†Âù°ÂõΩÁ´ã&amp;Â≠óËäÇÔºå‰∏ª‰ΩìÁªìÊûÑËøòÊòØViTÔºådeeper+narrower+multi-layer-cnn-patch-projection+auxiliary label&amp;loss ÂêåÁ≠âÂèÇÊï∞Èáè‰∏ãÔºåËÉΩÂ§üËææÂà∞‰∏éCNNÁõ∏ÂΩìÁöÑÂàÜÁ±ªÁ≤æÂ∫¶ 26M‚Äî‚Äî84.4% ImageNet top1 acc 56M‚Äî‚Äî85.4% ImageNet top1 acc 150M‚Äî‚Äî86.2% ImageNet top1 acc ImageNet &amp; ImageNet-1kÔºöThe ImageNet dataset consists of more than 14M images, divided into approximately 22k different labels/classes. However the ImageNet challenge is conducted on just 1k high-level categories (probably because 22k is just too much) Token Labeling: Training an 85.4% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNet Âä®Êú∫ develop a bag of training techniques on vision transformers slightly tune the structure introduce token labeling‚Äî‚Äîa new training objective ImageNet classificaiton task ËÆ∫ÁÇπ former ViTs ‰∏ªË¶ÅÈóÆÈ¢òÂ∞±ÊòØÈúÄË¶ÅÂ§ßÊï∞ÊçÆÈõÜpretrainÔºå‰∏çÁÑ∂Á≤æÂ∫¶‰∏ä‰∏çÂéª ÁÑ∂ÂêéÊ®°Âûã‰πüÊØîËæÉÂ§ßÔºåneed huge computation resources DeiTÂíåT2T-ViTÊé¢Á¥¢‰∫Üdata augmentation/ÂºïÂÖ•additional tokenÔºåËÉΩÂ§üÂú®ÊúâÈôêÁöÑÊï∞ÊçÆÈõÜ‰∏äÊãâÁ≤æÂ∫¶ our work rely on purely ImageNet-1k data rethink the way of performing patch embedding introduce inductive bias we add a token labeling objective loss beside cls token predition provide practical advice on adjusting vision transformer structures ÊñπÊ≥ï overview &amp; comparison ‰∏ª‰ΩìÁªìÊûÑ‰∏çÂèòÔºåÂ∞±ÊòØÂ¢ûÂä†‰∫Ü‰∏§È°π a MixToken method a token labeling objective review the vision transformer patch embedding Â∞ÜÂõ∫ÂÆöÂ∞∫ÂØ∏ÁöÑÂõæÁâáËΩ¨Êç¢Êàêpatch sequenceÔºå‰æãÂ¶Ç224x224ÁöÑÂõæÁâáÔºåpatch size=16ÔºåÈÇ£Â∞±ÊòØ14x14‰∏™small patches Â∞ÜÊØè‰∏™patch(16x16x3=768-dim) linear projectÊàê‰∏Ä‰∏™token(embedding-dim) concat a class tokenÔºåÊûÑÊàêÂÖ®ÈÉ®ÁöÑinput tokens position encoding added to input tokens fixed sinusoidal / learnable multi-head self-attention Áî®Êù•Âª∫Á´ãlong-range dependency multi-headsÔºöÊâÄÊúâattention headsÁöÑËæìÂá∫Âú®channel-dim‰∏äconcatÔºåÁÑ∂Âêélinear projectÂõûÂçï‰∏™headÁöÑchannel-dim feed-forward layers fc1-activation-fc2 score predition layer Âè™Áî®‰∫Ücls tokenÂØπÂ∫îÁöÑËæìÂá∫embeddingÔºåÂÖ∂‰ªñÁöÑdiscard training techniques network depth add more transformer blocks ÂêåÊó∂decrease the hidden dim of FFN explicit inductive bias CNNÈÄêÊ≠•Êâ©Â§ßÊÑüÂèóÈáéÔºåÊìÖÈïøÊèêÂèñÂ±ÄÈÉ®ÁâπÂæÅÔºåÂÖ∑ÊúâÂ§©ÁÑ∂ÁöÑÂπ≥Áßª‰∏çÂèòÊÄßÁ≠â transformerË¢´ÂèëÁé∞failed to capture the low-level and local structures we use convolutions with a smaller stride to provide an overlapped information for each nearby tokens Âú®patch embeddingÁöÑÊó∂ÂÄô‰∏çÊòØindependent cropÔºåËÄåÊòØÊúâoverlap ÁÑ∂ÂêéÁî®Â§öÂ±ÇconvÔºåÈÄêÊ≠•Êâ©Â§ßÊÑüÂèóÈáéÔºåsmaller kernel sizeÂêåÊó∂Èôç‰Ωé‰∫ÜËÆ°ÁÆóÈáè rethinking residual connection ÁªôÊÆãÂ∑ÆÂàÜÊîØadd a smaller ratio $\alpha$ enhance the residual connection since less information will go to the residual branch improve the generalization ability re-labeling label is not always accurate after cropping situations are worse on smaller images re-assign each image with a K-dim score mapÔºåÂú®1kÁ±ªÊï∞ÊçÆÈõÜ‰∏äK=1000 cheap operation compared to teacher-student Ëøô‰∏™labelÊòØÈíàÂØπwhole imageÁöÑlabelÔºåÊòØÈÄöËøáÂè¶‰∏Ä‰∏™È¢ÑËÆ≠ÁªÉÊ®°ÂûãËé∑Âèñ token-labeling based on the dense score map provided by re-labelingÔºåwe can assign each patch an individual label auxiliary token labeling loss ÊØè‰∏™tokenÈÉΩÂØπÂ∫î‰∫Ü‰∏Ä‰∏™K-dim score map ÂèØ‰ª•ËÆ°ÁÆó‰∏Ä‰∏™ce given outputs of the transformer $[X^{cls}, X^1, ‚Ä¶, X^N]$ K-dim score map $[y^1, y^2, ‚Ä¶, y^N]$ whole image label $y^{cls}$ loss auxiliary token labeling lossÔºö$L_{aux} = \frac{1}{N} \sum_1^N CE(X^i, y^i)$ cls lossÔºö$L_{cls} = CE(X^{cls}, y^{cls})$ total lossÔºö$L_{total} = L_{cls}+\beta L_{aux}$Ôºå$\beta=0.5$ MixToken ‰ªéMixup&amp;CutMixÂêØÂèëÊù•ÁöÑ ‰∏∫‰∫ÜÁ°Æ‰øùeach token have clear contentÔºåÊàë‰ª¨Âü∫‰∫étoken embeddingËøõË°åmixup given token sequence $T_1=[t^1_1, t^2_1, ‚Ä¶, t^N_1]$ &amp; $T_2=[t^1_2, t^2_2, ‚Ä¶, t^N_2]$ token labels $y_1=[y^1_1, y^2_1, ‚Ä¶, y^N_1]$ &amp; $Y_2=[y^1_2, y^2_2, ‚Ä¶, y^N_2]$ binary mask M MixToken mixed token sequenceÔºö$\hat T = T_1 \odot M + T_2 \odot (1-M)$ mixed labelsÔºö$\hat Y = Y_1 \odot M + Y_2 \odot (1-M)$ mixed cls labelÔºö$\hat {Y^{cls}} = \overline M y_1^{cls} + (1-\overline M) y_2^{cls}$Ôºå$\overline M$ is the average of $M$ ÂÆûÈ™å training details AdamW linear lr scalingÔºölarger when use token labeling weight decay dropoutÔºöhurts small modelsÔºåuse Stochastic Depth instead Training Technique Analysis more convs in patch embedding enhanced residual smaller scaling factor the weight get larger gradients in residual branch more information can be preserved in main branch better performance faster convergence re-labeling use NFNet-F6 to re-label the ImageNet dataset and obtain the 1000-dimensional score map for each image NFNet-F6 is trained from scratch given input 576x576ÔºåËé∑ÂæóÁöÑscore mapÊòØ18x18x1000Ôºàs32Ôºâ store the top5 probs for each position to save storage MixToken ÊØîbaselineÁöÑCutMix methodË¶ÅÂ•Ω ÂêåÊó∂ÁúãÂà∞token labelingÊØîrelabelingË¶ÅÂ•Ω token labeling relabelingÊòØÂú®whole image‰∏ä token labelingÊòØËøõ‰∏ÄÊ≠•Âú∞ÔºåÂú®token levelÊ∑ªÂä†labelÂíåloss augmentation techniques ÂèëÁé∞MixUp‰ºöhurt Model Scaling Ë∂äÂ§ßË∂äÂ•Ω]]></content>
      <tags>
        <tag>visual transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[memory bank]]></title>
    <url>%2F2021%2F05%2F19%2Fmemory-bank%2F</url>
    <content type="text"><![CDATA[2018Âπ¥ÁöÑpaper official codeÔºöhttps://github.com/zhirongw/lemniscate.pytorch memory bank NCE Unsupervised Feature Learning via Non-Parametric Instance Discrimination Âä®Êú∫ unsupervised learning can we learn good feature representation that captures apparent similarity among instances instead of classes formulate a non-parametric classification problem at instance-level use noise contrastive estimation our non-parametric model highly compactÔºö128-d feature per imageÔºåonly 600MB storage in total enable fast nearest neighbour retrieval „ÄêQUESTION„ÄëÊó†Á±ªÂà´Ê†áÁ≠æÔºåÂçïÈù†similarityÔºåÊúÄÁªàÁöÑÂàÜÁ±ªÊ®°ÂûãÊòØÂ¶Ç‰ΩïÂª∫Á´ãÁöÑÔºü verified on ImageNet 1K classification semi-supervised learning object detection tasks ËÆ∫ÁÇπ observations ImageNet top-5 errËøúÊØîtop-1 errÂ∞è second highest responding class is more likely to be visually related ËØ¥ÊòéÊ®°ÂûãÈöêÂºèÂú∞Â≠¶Âà∞‰∫Üsimilarity apparent similarity is learned not from se- mantic annotations, but from the visual data themselves Â∞Üclass-wise supervisionÊé®Âà∞‰∏Ä‰∏™ÊûÅÈôê Â∞±ÂèòÊàê‰∫Üinstance-level Á±ªÂà´Êï∞ÂèòÊàê‰∫Üthe whole training setÔºösoftmax to many more classes becomes infeasible approximate the full softmax distribution with noise-contrastive estimation(NCE) use a proximal regularization to stablize the learning process train &amp; test ÈÄöÂ∏∏ÁöÑÂÅöÊ≥ïÊòØlearned representationsÂä†‰∏Ä‰∏™Á∫øÊÄßÂàÜÁ±ªÂô® e.g. SVMÔºö‰ΩÜÊòØtrainÂíåtestÁöÑfeature spaceÊòØ‰∏ç‰∏ÄËá¥ÁöÑ Êàë‰ª¨Áî®‰∫ÜKNNÔºösame metric space ÊñπÊ≥ï overview to learn a embedding function $f_{\theta}$ distance metric $d_{\theta}(x,y) = ||f_{\theta}(x)-f_{\theta}(y)||$ to map visually similar images closer instance-levelÔºöto distinct between instances Non-Parametric Softmax Classifier common parametric classifier givenÁΩëÁªúÈ¢ÑÊµãÁöÑN-dim representation $v=f_{\theta}(x)$ Ë¶ÅÈ¢ÑÊµãC-classesÁöÑÊ¶ÇÁéáÔºåÈúÄË¶Å‰∏Ä‰∏™$W^{NC}$ÁöÑprojectionÔºö$P(i|v) = \frac{exp (W^T_iv)}{\sum exp (W^T_jv)}$ Non-Parametric version enforce $||v||=1$ via L2 norm replace $W^T$ with $v^T$ then the probabilityÔºö$P(i|v) = \frac{exp (v^T_iv/\tau)}{\sum exp (v^T_jv / \tau)}$ temperature param $\tau$Ôºöcontrols the concentration level of the distribution the goal is to minimize the negative log-likelihood ÊÑè‰πâÔºöL2 normÂ∞ÜÊâÄÊúâÁöÑrepresentationÊò†Â∞ÑÂà∞‰∫Ü‰∏Ä‰∏™128-d unit sphere‰∏äÈù¢Ôºå$v_i^T v_j$Â∫¶Èáè‰∫Ü‰∏§‰∏™projection vecÁöÑsimilarityÔºåÊàë‰ª¨Â∏åÊúõÂêåÁ±ªÁöÑvecÂ∞ΩÂèØËÉΩÈáçÂêàÔºå‰∏çÂêåÁ±ªÁöÑvecÂ∞ΩÂèØËÉΩÊ≠£‰∫§ class weights $W$ are not generalized to new classes but feature representations $V$ does memory bank Âõ†‰∏∫ÊòØinstance levelÔºåC-classesÂØπÂ∫îÊï¥‰∏™training setÔºå‰πüÂ∞±ÊòØËØ¥${v_i}$ for all the images are needed for loss Let $V={v_i}$ Ë°®Á§∫memory bankÔºåÂàùÂßã‰∏∫unit random vectors every learning iterations $f_\theta$ is optimized by SGD ËæìÂÖ•$x_i$ÊâÄÂØπÂ∫îÁöÑ$f_i$Êõ¥Êñ∞Âà∞$v_i$‰∏ä ‰πüÂ∞±ÊòØÂè™Êúâmini-batch‰∏≠ÂåÖÂê´ÁöÑÊ†∑Êú¨ÔºåÂú®Ëøô‰∏Ä‰∏™stepÔºåÊõ¥Êñ∞projection vec Noise-Contrastive Estimation non-parametric softmaxÁöÑËÆ°ÁÆóÈáèÈöèÁùÄÊ†∑Êú¨ÈáèÁ∫øÊÄßÂ¢ûÈïøÔºåmillions levelÊ†∑Êú¨ÈáèÁöÑÊÉÖÂÜµ‰∏ãÔºåËÆ°ÁÆóÂ§™heavy‰∫Ü we use NCE to approximate the full softmax assume noise samplesÁöÑuniform distributionÔºö$P_n =\frac{1}{n}$ noise samples are $m$ times frequent than data samples ÈÇ£‰πàsample $i$ matches vec $v$ÁöÑÂêéÈ™åÊ¶ÇÁéáÊòØÔºö$h(i,v)=\frac{P(i|v)}{P(i|v)+mP_n}$ approximated training object is to minimize the negative log-likelihood of $h(i,v)$ normalizing constant $Z$ÁöÑËøë‰ºº ‰∏ªË¶ÅÂ∞±ÊòØÂàÜÊØçËøô‰∏™$Z_i$ÁöÑËÆ°ÁÆóÊØîËæÉheavyÔºåÊàë‰ª¨Áî®Monte CarloÈááÊ†∑Êù•Ëøë‰ººÔºö ${j_k}$ is a random subset of indicesÔºöÈöèÊú∫ÊäΩ‰∫Ümemory bankÁöÑ‰∏Ä‰∏™Â≠êÈõÜÊù•approxÂÖ®ÈõÜÁöÑÂàÜÊØçÔºåÂÆûÈ™åÂèëÁé∞Âèñbatch sizeÂ§ßÂ∞èÁöÑÂ≠êÈõÜÂ∞±ÂèØ‰ª•Ôºåm=4096 Proximal Regularization the learning process oscillates a lot we have one instance per class during each training epoch each class is only visited once we introduce an additional term overall workflowÔºöÂú®ÊØè‰∏Ä‰∏™iteration tÔºåfeature representationÊòØ$v_i^t=f_{\theta}(x_i)$ÔºåËÄåmemory bankÈáåÈù¢ÁöÑrepresentationsÊù•Ëá™‰∏ä‰∏Ä‰∏™iteration step $V={v^{t-1}}$ÔºåÊàë‰ª¨‰ªémemory bankÈáåÈù¢ÈááÊ†∑ÔºåÂπ∂ËÆ°ÁÆóNCE lossÔºåÁÑ∂ÂêébpÊõ¥Êñ∞ÁΩëÁªúÊùÉÈáçÔºåÁÑ∂ÂêéÂ∞ÜËøô‰∏ÄËΩÆfpÁöÑrepresentations updateÂà∞memory bankÁöÑÊåáÂÆöÊ†∑Êú¨‰∏äÔºåÁÑ∂Âêé‰∏ã‰∏ÄËΩÆ ÂèØ‰ª•ÂèëÁé∞ÔºåÂú®ÂàùÂßãrandomÈò∂ÊÆµÔºåÊ¢ØÂ∫¶Êõ¥Êñ∞‰ºöÊØîËæÉÂø´ËÄå‰∏î‰∏çÁ®≥ÂÆö Êàë‰ª¨Áªôpositive sampleÁöÑloss‰∏äÈ¢ùÂ§ñÂä†‰∫Ü‰∏Ä‰∏™$\lambda ||v_i^t-v_i^{t-1}||^2_2$ÔºåÊúâÁÇπÁ±ª‰ººweight decayÈÇ£Áßç‰∏úË•øÔºåÂºÄÂßãÈò∂ÊÆµl2 loss‰ºöÂç†‰∏ªÂØºÔºåÂºïÂØºÁΩëÁªúÊî∂Êïõ stabilize speed up convergence improve the learned representations Weighted k-Nearest Neighbor Classifier a test timeÔºåÂÖàËÆ°ÁÆófeature representationÔºåÁÑ∂ÂêéË∑ümemory bankÁöÑvectorsÂàÜÂà´ËÆ°ÁÆócosine similarity $s_i=cos(v_i, f)$ÔºåÈÄâÂá∫topk neighbours $N_k$ÔºåÁÑ∂ÂêéËøõË°åweighted voting weighted votingÔºö ÂØπÊØè‰∏™class cÔºåËÆ°ÁÆóÂÆÉÂú®topk neighboursÁöÑtotal weightÔºå$w_c =\sum_{i \in N_k} \alpha_i 1(c_i=c)$ $\alpha_i = exp(s_i/\tau)$ k = 200 $\tau = 0.07$]]></content>
      <tags>
        <tag>Unsupervised Learning, NCE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MoCoÁ≥ªÂàó]]></title>
    <url>%2F2021%2F04%2F30%2FMoCo%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[papersÔºö [2019 MoCo v1] Momentum Contrast for Unsupervised Visual Representation LearningÔºåkaiming [2020 SimCLR] A Simple Framework for Contrastive Learning of Visual RepresentationsÔºåGoogle BrainÔºåÊ∑∑ËøõÊù•ÊòØÂõ†‰∏∫ÂÆÉimprove based on MoCo v1ÔºåËÄåMoCo v2/v3ÂèàÈÉΩÊòØÂü∫‰∫éÂÆÉÊîπËøõ [2020 MoCo v2] Improved Baselines with Momentum Contrastive LearningÔºåkaiming [2021 MoCo v3] An Empirical Study of Training Self-Supervised Visual TransformersÔºåkaiming preview: Ëá™ÁõëÁù£Â≠¶‰π† Self-supervised Learning referenceÔºöhttps://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html overview Â∞±ÊòØÊó†ÁõëÁù£ ÈíàÂØπÁöÑÁóõÁÇπÔºàÊúâÁõëÁù£ËÆ≠ÁªÉÊ®°ÂûãÔºâ Ê†áÊ≥®ÊàêÊú¨È´ò ËøÅÁßªÊÄßÂ∑Æ ‰ºöÂü∫‰∫éÊï∞ÊçÆÁâπÁÇπÔºåËÆæÁΩÆPretext tasksÔºàÊúÄÂ∏∏ËßÅÁöÑ‰ªªÂä°Â∞±ÊòØÁîüÊàê/ÈáçÂª∫ÔºâÔºåÊûÑÈÄ†Pesdeo LabelsÊù•ËÆ≠ÁªÉÁΩëÁªú ÈÄöÂ∏∏Ê®°ÂûãÁî®Êù•‰Ωú‰∏∫ÂÖ∂‰ªñÂ≠¶‰π†‰ªªÂä°ÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°Âûã Ë¢´ËÆ§‰∏∫ÊòØÁî®Êù•Â≠¶‰π†ÂõæÂÉèÁöÑÈÄöÁî®ËßÜËßâË°®Á§∫ methods ‰ªéÁªìÊûÑ‰∏äÂå∫ÂàÜ‰∏ªË¶ÅÂ∞±ÊòØ‰∏§Â§ßÁ±ªÊñπÊ≥ï ÁîüÊàêÂºèÔºöÈÄöËøáencoder-decoderÁªìÊûÑËøòÂéüËæìÂÖ•ÔºåÁõëÁù£‰ø°Âè∑ÊòØËæìÂÖ•ËæìÂá∫Â∞ΩÂèØËÉΩÁõ∏‰ºº ÈáçÂª∫‰ªªÂä°ÂºÄÈîÄÂ§ß Ê≤°ÊúâÂª∫Á´ãÁõ¥Êé•ÁöÑËØ≠‰πâÂ≠¶‰π† Â§ñÂä†GANÁöÑÂà§Âà´Âô®‰ΩøÂæó‰ªªÂä°Êõ¥Âä†Â§çÊùÇÈöæËÆ≠ Âà§Âà´ÂºèÔºöËæìÂÖ•‰∏§Âº†ÂõæÁâáÔºåÈÄöËøáencoderÁºñÁ†ÅÔºåÁõëÁù£‰ø°Âè∑ÊòØÂà§Êñ≠‰∏§Âº†ÂõæÊòØÂê¶Áõ∏‰ººÔºåÂà§Âà´ÂºèÊ®°Âûã‰πüÂè´Contrastive Learning ‰ªéPretext tasks‰∏äÂàíÂàÜ‰∏ªË¶ÅÂàÜ‰∏∫‰∏âÁ±ª Âü∫‰∫é‰∏ä‰∏ãÊñáÔºàContext basedÔºâ ÔºöÂ¶ÇbertÁöÑMLMÔºåÂú®Âè•Â≠ê/ÂõæÁâá‰∏≠ÈöèÊú∫Êâ£Êéâ‰∏ÄÈÉ®ÂàÜÔºåÁÑ∂ÂêéÊé®Âä®Ê®°ÂûãÂü∫‰∫é‰∏ä‰∏ãÊñá/ËØ≠‰πâ‰ø°ÊÅØÈ¢ÑÊµãËøôÈÉ®ÂàÜ/Áõ∏ÂØπ‰ΩçÁΩÆÂÖ≥Á≥ª Âü∫‰∫éÊó∂Â∫èÔºàTemporal BasedÔºâÔºöÂ¶ÇbertÁöÑNSPÔºåËßÜÈ¢ë/ËØ≠Èü≥ÔºåÂà©Áî®Áõ∏ÈÇªÂ∏ßÁöÑÁõ∏‰ººÊÄßÔºåÊûÑÂª∫‰∏çÂêåÊéíÂ∫èÁöÑÂ∫èÂàóÔºåÂà§Êñ≠BÊòØÂê¶ÊòØAÁöÑ‰∏ã‰∏ÄÂè•/ÊòØÂê¶Áõ∏ÈÇªÂ∏ß Âü∫‰∫éÂØπÊØîÔºàContrastive BasedÔºâÔºöÊØîËæÉÊ≠£Ë¥üÊ†∑Êú¨ÔºåÊúÄÂ§ßÂåñÁõ∏‰ººÂ∫¶ÁöÑlossÂú®ËøôÈáåÈù¢Ë¢´Âè´ÂÅöInfoNCE memory-bank Contrastive BasedÊñπÊ≥ïÊúÄÂ∏∏ËßÅÁöÑÊñπÂºèÊòØÂú®‰∏Ä‰∏™batch‰∏≠ÊûÑÂª∫Ê≠£Ë¥üÊ†∑Êú¨ËøõË°åÂØπÊØîÂ≠¶‰π† end-to-end ÊØè‰∏™mini-batch‰∏≠ÁöÑÂõæÂÉèÂ¢ûÂº∫ÂâçÂêéÁöÑ‰∏§Âº†ÂõæÁâá‰∫í‰∏∫Ê≠£Ê†∑Êú¨ Â≠óÂÖ∏Â§ßÂ∞èÂ∞±ÊòØminibatchÂ§ßÂ∞è memory bankÂåÖÂê´Êï∞ÊçÆÈõÜ‰∏≠ÊâÄÊúâÊ†∑Êú¨ÁºñÁ†ÅÂêéÁâπÂæÅ ÈöèÊú∫ÈááÊ†∑‰∏ÄÈÉ®ÂàÜ‰Ωú‰∏∫keys ÊØè‰∏™Ëø≠‰ª£Âè™Êõ¥Êñ∞Ë¢´ÈááÊ†∑ÁöÑÊ†∑Êú¨ÁºñÁ†Å Âõ†‰∏∫Ê†∑Êú¨ÁºñÁ†ÅÊù•Ëá™‰∏çÂêåÁöÑtraining stepÔºå‰∏ÄËá¥ÊÄßÂ∑Æ MoCo Âä®ÊÄÅÁºñÁ†ÅÂ∫ìÔºöout-of-dateÁöÑÁºñÁ†ÅÂá∫Âàó momentum updateÔºö‰∏ÄËá¥ÊÄßÊèêÂçá InfoNCE deep mindÂú®CPC(Contrastive Predictive Coding)ÊèêÂá∫ÔºåËÆ∫Êñá‰ª•ÂêéÊúâÊú∫‰ºöÂÜçÂ±ïÂºÄ unsupervised encoderÔºöencode x into latent space representations zÔºåresnet blocks autoregressive modelÔºösummarize each time-step set of {z} into a context representation cÔºåGRUs probabilistic contrastive loss Noise-Contrastive Estimation Importance Sampling ËÆ≠ÁªÉÁõÆÊ†áÊòØËæìÂÖ•Êï∞ÊçÆxÂíåcontext vector c‰πãÈó¥ÁöÑmutual information ÊØèÊ¨°‰ªé$p(x_{t+k}|c_t)$‰∏≠ÈááÊ†∑‰∏Ä‰∏™Ê≠£Ê†∑Êú¨ÔºöÊ≠£Ê†∑Êú¨ÊòØËøô‰∏™Â∫èÂàóÊé•‰∏ãÊù•È¢ÑÊµãÁöÑ‰∏úË•øÔºåÂíåcÁöÑÁõ∏‰ººÊÄßËÇØÂÆöË¶ÅÈ´ò‰∫é‰∏çÊÉ≥Âπ≤ÁöÑtoken ‰ªé$p(x_{t+k})$‰∏≠ÈááÊ†∑N-1‰∏™Ë¥üÊ†∑Êú¨ÔºöË¥üÊ†∑Êú¨ÊòØÂà´ÁöÑÂ∫èÂàóÈáåÈù¢ÈöèÊú∫ÈááÊ†∑ÁöÑ‰∏úË•ø ÁõÆÊ†áÊòØËÆ©Ê≠£Ê†∑Êú¨‰∏écontextÁõ∏ÂÖ≥ÊÄßÈ´òÔºåË¥üÊ†∑Êú¨‰Ωé MoCo v1: Momentum Contrast for Unsupervised Visual Representation Learning Âä®Êú∫ unsupervised visual representation learning contrastive learning dynamic dictionary large consisitent verified on 7 down-stream tasks ImageNet classification VOC &amp; COCO det/seg ËÆ∫ÁÇπ Unsupervised representation learning highly successful in NLPÔºåin CV supervised is still the main-stream ‰∏§‰∏™Ê†∏ÂøÉ pretext tasks loss functions loss functions ÁîüÊàêÂºèÊñπÊ≥ïÁöÑlossÊòØÂü∫‰∫épredictionÂíå‰∏Ä‰∏™fix targetÊù•ËÆ°ÁÆóÁöÑ contrastive-basedÁöÑkey targetÂàôÊòØvary on-the-fly during training Adversarial lossesÊ≤°Â±ïÂºÄ pretext tasks tasks involving recoverÔºöauto-encoder task involving pseudo-labelsÔºöÈÄöÂ∏∏Êúâ‰∏™exemplar/anchorÔºåÁÑ∂ÂêéËÆ°ÁÆócontrastive loss contrastive learning VS pretext tasks Â§ßÈáèpretext tasksÂèØ‰ª•ÈÄöËøáËÆæËÆ°‰∏Ä‰∫õcontrastive lossÊù•ÂÆûÁé∞ recent approaches using contrastive loss dynamic dictionaries Áî±keysÁªÑÊàêÔºösampled from data &amp; represented by an encoder train the encoder to perform dictionary look-up given an encoded query similar to its matching key and dissimilar to others desirable dictionary largeÔºöbetter sample consistentÔºötraining target consistent MoCoÔºöMomentum Contrast queue ÊØè‰∏™it stepÁöÑmini-batchÁöÑÁºñÁ†ÅÂÖ•Â∫ì the oldest are dequeued EMAÔºö a slowly progressing key encoder momentum-based moving average of the query encoder similarÁöÑÂÆö‰πâÔºöq &amp; k are from the same image ÊñπÊ≥ï contrastive learning a encoded query $q$ a set of encoded samples $\{k_0, k_1, ‚Ä¶\}$ assumeÔºöthere is a single key $k_+$ in the dictionary that $q$ matches similarity measurementÔºödot product InfoNCEÔºö $L_q = -log \frac{exp(qk_+/\tau)}{\sum_0^K exp(qk/\tau)}$ 1 positive &amp; K negtive samples Êú¨Ë¥®‰∏äÊòØ‰∏™softmax-based classifierÔºåÂ∞ùËØïÂ∞Ü$q$ÂàÜÁ±ªÊàê$k_+$ unsupervised workflow with a encoder network $f_q$ &amp; $f_k$ thus we have query &amp; sample representation $q=f_q(x^q)$ &amp; $k=f_k(x^k)$ inputs $x$ can be images/patches/context(patches set) $f_q$ &amp; $f_k$ can be identical/partially shared/different momentum contrast dictionary as a key the dictionary always represents a sampled subset of all data the current mini-batchÂÖ•Âàó the oldest mini-batchÂá∫Âàó momentum update large dictionaryÊ≤°Ê≥ïÂØπkeysËøõË°åback-propagationÔºöÂõ†‰∏∫sampleÂ§™Â§ö‰∫Ü only $f_q$ are updated by back-propagationÔºömini-batch naive solutionÔºöcopy $f_q$ÁöÑÂèÇÊï∞Áªô$f_k$Ôºåyields poor resultsÔºåÂõ†‰∏∫key encoderÂèÇÊï∞ÂèòÂåñÂ§™È¢ëÁπÅ‰∫ÜÔºårepresentation inconsistent issue momentum updateÔºö$f_k = mf_k + (1-m)f_q$Ôºå$m=0.999$ ‰∏âÁßçÊõ¥Êñ∞ÊñπÂºèÂØπÊØî Á¨¨‰∏ÄÁßçend-to-end methodÔºö use samples in current mini-batch as the dictionary keys are consistently encoded dictionary size is limited Á¨¨‰∫åÁßçmemory bank A memory bank consists of the representations of all samples in the dataset the dictionary for each mini-batch is randomly sampled from the memory bankÔºå‰∏çËøõË°åbpÔºåthus enables large dictionary key representation is updated when it was last seenÔºöinconsistent Êúâ‰∫õ‰πüÁî®momentum updateÔºå‰ΩÜÊòØÊòØÁî®Âú®representation‰∏äÔºåËÄå‰∏çÊòØencoderÂèÇÊï∞ pretext task define positive pairÔºöif the query and the key come from the same image Êàë‰ª¨‰ªéÂõæ‰∏ätake two random views under random augmentation to form a positive pair ÁÑ∂ÂêéÁî®ÂêÑËá™ÁöÑencoderÁºñÁ†ÅÊàêq &amp; k ÊØè‰∏ÄÂØπËÆ°ÁÆósimilarityÔºöpos similarity ÁÑ∂ÂêéÂÜçËÆ°ÁÆóinput queriesÂíådictionaryÁöÑsimilarityÔºöneg similarity ËÆ°ÁÆóceÔºåupdate $f_q$ Áî®$f_q$ update $f_k$ ÊääkÂä†ÂÖ•dictionaryÈòüÂàó ÊääÊúÄÊó©ÁöÑmini-batchÂá∫Âàó ÊäÄÊúØÁªÜËäÇ resnetÔºölast fc dim=128ÔºåL2 norm temperature $\tau=0.07$ augmentation random resize + random(224,224) crop random color jittering random horizontal flip random grayscale conversion shuffling BN ÂÆûÈ™åÂèëÁé∞‰ΩøÁî®resnetÈáåÈù¢ÁöÑBN‰ºöÂØºËá¥‰∏çÂ•ΩÁöÑÁªìÊûúÔºöÁåúÊµãÊòØintra-batch communicationÂºïÂØºÊ®°ÂûãÂ≠¶‰π†‰∫Ü‰∏ÄÁßçcheatingÁöÑlow-loss solution ÂÖ∑‰ΩìÂÅöÊ≥ïÊòØÁªô$f_k$ÁöÑËæìÂÖ•mini-batchÂÖàshuffle the orderÔºåÁÑ∂ÂêéËøõË°åfpÔºåÁÑ∂ÂêéÂÜçshuffle backÔºåËøôÊ†∑$f_q$Âíå$f_k$ÁöÑBNËÆ°ÁÆóÁöÑmini-batchÁöÑstaticsÂ∞±ÊòØ‰∏çÂêåÁöÑ ÂÆûÈ™å SimCLR: A Simple Framework for Contrastive Learning of Visual Representations Âä®Êú∫ simplify recently proposed contrastive self-supervised learning algorithms systematically study the major components data augmentations learnable unlinear prediction head larger batch size and more training steps outperform previous self-supervised &amp; semi-supervised learning methods on ImageNet ËÆ∫ÁÇπ discriminative approaches based on contrastive learning maximizing agreement between differently augmented views of the same data sample via a contrastive loss in the latent space major components &amp; conclusions Êï∞ÊçÆÂ¢ûÂº∫ÂæàÈáçË¶ÅÔºåunsupervisedÊØîsupervised benefits more ÂºïÂÖ•ÁöÑlearnable nonlinear transformationÊèêÂçá‰∫Ürepresentation quality contrastive cross entropy lossÂèóÁõä‰∫énormalized embeddingÂíåadjusted temperature parameter larger batch size and more training stepsÂæàÈáçË¶ÅÔºåunsupervisedÊØîsupervised benefits more ÊñπÊ≥ï common framework 4 major components ÈöèÊú∫Êï∞ÊçÆÂ¢ûÂº∫ results in two views of the same sampleÔºåÊûÑÊàêpositive pair crop + resize back + color distortions + gaussian blur base encoder Áî®Âï•ÈÉΩË°åÔºåÊú¨ÊñáÁî®‰∫Üresnet including the GAP a projection head Â∞Ürepresentation dimÊò†Â∞ÑÂà∞the space where contrastive loss is appliedÔºàgiven 1 pos pair &amp; N neg pairÔºåÂ∞±ÊòØN+1 dimÔºâ ‰πãÂâçÊúâÊñπÊ≥ïÁõ¥Êé•Áî®linear projection Êàë‰ª¨Áî®‰∫ÜÂ∏¶‰∏Ä‰∏™hidden layerÁöÑMLPÔºöfc-bn-relu-fc a contrastive loss overall workflow random sample a minibatch of N random augmentation results in 2N data points ÂØπÊØè‰∏™Ê†∑Êú¨Êù•ËÆ≤ÔºåÊúâ1‰∏™positive pairÔºåÂÖ∂‰Ωô2(N-1)‰∏™data pointsÈÉΩÊòØnegative samples set cosine similarity $sim(u,v)=u^Tv/|u||v|$ given positive pair $(i,j)$ then the loss is $l_{i,j} = -log \frac{exp(s_{i,j}/\tau)}{\sum_{k\neq i}^{2N} exp(s_{i,k}/\tau)}$ ÂØπÊØè‰∏™positive pairÈÉΩËÆ°ÁÆóÔºåÂåÖÊã¨$(i,j)$ Âíå$(j,i)$ÔºåÂè´ÈÇ£‰∏™symmetrized loss update encoder training with large batch size batch 8192Ôºånegatives 16382 Â§ßbatchÊó∂Ôºålinear learning rate scalingÂèØËÉΩ‰∏çÁ®≥ÂÆöÔºåÊâÄ‰ª•Áî®‰∫ÜLARS optmizer global BNÔºåaggregate BN mean &amp; variance over all devices TPU MoCo v2: Improved Baselines with Momentum Contrastive Learning Âä®Êú∫ still working on contrastive unsupervised learning simple modifications on MoCo introduce two effective SimCLR‚Äôs designsÔºö an MLP head more data augmentation requires smaller batch size than SimCLRÔºåmaking it possible to run on GPU verified on ImageNet classification VOC detection ËÆ∫ÁÇπ MoCo &amp; SimCLR contrastive unsupervised learning frameworks MoCo v1 shows promising SimCLR further reduce the gap we found two design imrpovements in SimCLR Âú®‰∏§‰∏™ÊñπÊ≥ï‰∏≠ÈÉΩworkÔºåËÄå‰∏îÁî®Âú®MoCo‰∏≠shows better transfer learning results an MLP projection head stronger data augmentation ÂêåÊó∂MoCo frameworkÁõ∏ÊØîËæÉ‰∫éSimCLR ÔºåËøú‰∏çÈúÄË¶Ålarge training batches SimCLR based on end-to-end mechanismÔºåÈúÄË¶ÅÊØîËæÉÂ§ßÁöÑbatch sizeÔºåÊù•Êèê‰æõË∂≥Â§üÂ§öÁöÑnegative pair MoCoÂàôÁî®‰∫ÜÂä®ÊÄÅÈòüÂàóÔºåÊâÄ‰ª•‰∏çÈôêÂà∂batch size SimCLR improves the end-to-end method larger batchÔºöto provide more negative samples output layerÔºöreplace fc with a MLP head stronger data augmentation MoCo a large number of negative samples are readily available ÊâÄ‰ª•Â∞±ÊääÂêé‰∏§È°πÂºïÂÖ•ËøõÊù•‰∫Ü ÊñπÊ≥ï MLP head 2-layer MLP(hidden dim=2048, ReLU) ‰ªÖÂΩ±Âìçunsupervised trainingÔºåÊúâÁõëÁù£transfer learningÁöÑÊó∂ÂÄôÊç¢Â§¥ temperature paramË∞ÉÊï¥Ôºö‰ªédefault 0.07 Ë∞ÉÊï¥Êàêoptimal value 0.2 augmentation add blur SimCLRËøòÁî®‰∫Üstronger color distortionÔºöwe found stronger color distortion in SimCLR hurts in our MoCoÔºåÊâÄ‰ª•Ê≤°Âä† ÂÆûÈ™å ablation MLPÔºöÂú®ÂàÜÁ±ª‰ªªÂä°‰∏äÁöÑÊèêÂçáÊØîÊ£ÄÊµãÂ§ß augmentationÔºöÂú®Ê£ÄÊµã‰∏äÁöÑÊèêÂçáÊØîÂàÜÁ±ªÂ§ß comparison large batches are not necessary for good accÔºöSimCLR longer trainingÈÇ£‰∏™ÁâàÊú¨Á≤æÂ∫¶Êõ¥È´ò end-to-endÁöÑÊñπÊ≥ïËÇØÂÆömore costly in memory and timeÔºöÂõ†‰∏∫Ë¶Åbp‰∏§‰∏™encoder MoCo v3: An Empirical Study of Training Self-Supervised Visual Transformers Âä®Êú∫ self-supervised frameworks that based on Siamese network, including MoCo ViTÔºöstudy the fundamental components for training self-supervised ViT MoCo v3Ôºöan incremental improvement of MoCo v1/2Ôºåstriking for a better balance of simplicity &amp; accuracy &amp; scalability instability is a major issue scaling up ViT models ViT-Large ViT-Huge ËÆ∫ÁÇπ we go back to the basics and investigate the fundamental components of training deep neural networks batch size learning rate optmizer instability instability is a major issue that impacts self-supervised ViT training but may not result in catastrophic failureÔºåÂè™‰ºöÂØºËá¥Á≤æÂ∫¶ÊçüÂ§± ÊâÄ‰ª•Áß∞‰πã‰∏∫hidden degradation use a simple trick to improve stabilityÔºöfreeze the patch projection layer in ViT and observes increasement in acc NLPÈáåÈù¢Âü∫‰∫émasked auto-encodingÁöÑframeworkÊïàÊûúË¶ÅÊØîÂü∫‰∫écontrastvieÁöÑframeworkÂ•ΩÔºåÂõæÂÉèÊ≠£Â•ΩÂèçËøáÊù• ÊñπÊ≥ï MoCo v3 take two crops for each image under random augmentation encoded by two encoders $f_q$ &amp; $f_k$ into vectors $q$ &amp; $k$ we use the keys that naturally co-exist in the same batch abandon the memory queueÔºöÂõ†‰∏∫ÂèëÁé∞batch sizeË∂≥Â§üÂ§ßÔºà4096ÔºâÁöÑÊó∂ÂÄôÔºåmemory queueÂ∞±Ê≤°Âï•acc gain‰∫Ü ÂõûÂΩíÂà∞batch-based sample pair ‰ΩÜÊòØencoder k‰ªçÊóß‰∏çÂõû‰º†Ê¢ØÂ∫¶ÔºåËøòÊòØÂü∫‰∫éencoder qËøõË°åÂä®ÈáèÊõ¥Êñ∞ symmetrized lossÔºö $ctr(q_1, k_2) + ctr(q_2,k_1)$ InfoNCE temperature ‰∏§‰∏™cropsÂàÜÂà´ËÆ°ÁÆóctr encoder encoder $f_q$ a backbone a projection head an extra prediction head encoder $f_k$ a backbone a projection head encoder $f_k$ is updated by the moving average of $f_q$Ôºåexcluding the prediction head baseline acc basic settingsÔºå‰∏ªË¶ÅÂèòÂä®Â∞±ÊòØ‰∏§‰∏™Ôºö dynamic queueÊç¢Êàêlarge batch encoder $f_q$ÁöÑextra prediction head use ViT Áõ¥Êé•Áî®ViTÊõøÊç¢resnet back met instability issue batch size ViTÈáåÈù¢ÁöÑ‰∏Ä‰∏™ËßÇÁÇπÂ∞±ÊòØÔºåmodelÊú¨Ë∫´ÊØîËæÉheavyÔºåÊâÄ‰ª•large batch is desirable ÂÆûÈ™åÂèëÁé∞ a batch of 1k &amp; 2k produces reasonably smooth curvesÔºöIn this regime, the larger batch improves accuracy thanks to more negative samples a batch of 4k ÊúâÊòéÊòæÁöÑuntable dipsÔºö a batch of 6k has worse failure patternsÔºöÊàë‰ª¨Ëß£ËØª‰∏∫Âú®Ë∑≥Ê∞¥ÁÇπÔºåtraining is partially restarted and jumps out of the current local optimum learning rate lrËæÉÂ∞èÔºåtrainingÊØîËæÉÁ®≥ÂÆöÔºå‰ΩÜÊòØÂÆπÊòìÊ¨†ÊãüÂêà lrËøáÂ§ßÔºå‰ºöÂØºËá¥unstableÔºå‰πü‰ºöÂΩ±Âìçacc ÊÄª‰ΩìÊù•ËØ¥Á≤æÂ∫¶ËøòÊòØÂÜ≥ÂÆö‰∫éstability optimizer default adamWÔºåbatch size 4096 Êúâ‰∫õÊñπÊ≥ïÁî®‰∫ÜLARS &amp; LAMB for large-batch training LAMB sensitive to lr optmal lr achieves slightly better accuracy than AdamW ‰ΩÜÊòØlr‰∏ÄÊó¶ËøáÂ§ßÔºåaccÊûÅÈÄüdrop ‰ΩÜÊòØtraining curves still smoothÔºåËôΩÁÑ∂‰∏≠Èó¥ËøáÁ®ãÊúâdropÔºöÊàë‰ª¨Ëß£ËØª‰∏∫LAMB can avoid sudden change in the gradientsÔºå‰ΩÜÊòØÈÅøÂÖç‰∏ç‰∫Ünegative compactÔºåËøòÊòØ‰ºöÁ¥ØÂä† a trick for improving stability we found a spike in gradient causes a dip in the training curve we also observe that gradient spikes happen earlier in the first layer (patch projection) ÊâÄ‰ª•Â∞ùËØïfreezing the patch projection layer during trainingÔºå‰πüÂ∞±ÊòØ‰∏Ä‰∏™randomÁöÑpatch projection layer This stability benefits the final accuracy The improvement is bigger for a larger lr Âú®Âà´ÁöÑViT-back-framework‰∏ä‰πüÊúâÊïàÔºàSimCLR„ÄÅBYOLÔºâ we also tried BNÔºåWNÔºågradient clip BN/WN does not improve gradient clipÂú®thresholdË∂≥Â§üÂ∞èÁöÑÊó∂ÂÄôÊúâÁî®ÔºåÊé®Âà∞ÊûÅÈôêÂ∞±ÊòØfreezing‰∫Ü implementation details AdamW batch size 4096 lrÔºöwarmup 40 eps then cosine decay MLP heads projection headÔºö3-layersÔºå4096-BN-ReLU-4096-BN-ReLU-256 prediction headÔºö2-layersÔºå4096-BN-ReLU-256 loss ctrÈáåÈù¢Êúâ‰∏™scaleÁöÑÂèÇÊï∞Ôºå$2\tau$ makes it less sensitive to $\tau$ value $\tau=0.2$ ViT architecture Ë∑üÂéüËÆ∫Êñá‰øùÊåÅ‰∏ÄËá¥ ËæìÂÖ•ÊòØ224x244ÁöÑimageÔºåÂàíÂàÜÊàê16x16/14x14ÁöÑpatch sequenceÔºåprojectÊàê256d/196dÁöÑembedding Âä†‰∏äsine-cosine-2DÁöÑPE ÂÜçconcat‰∏Ä‰∏™cls token ÁªèËøá‰∏ÄÁ≥ªÂàótransformer blocks The class token after the last block (and after the final LayerNorm) is treated as the output of the backboneÔºåand is the input to the MLP heads]]></content>
      <tags>
        <tag>self-supervised learning, transformer, contrastive loss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[optimizers‰ºòÂåñÂô®]]></title>
    <url>%2F2021%2F03%2F15%2Foptimizers%E4%BC%98%E5%8C%96%E5%99%A8%2F</url>
    <content type="text"><![CDATA[0. overviewkeywordsÔºöSGD, moment, Nesterov, adaptive, ADAM, Weight decay ‰ºòÂåñÈóÆÈ¢òOptimization to minimizeÁõÆÊ†áÂáΩÊï∞ grandient decent gradient numericalÔºöÊï∞ÂÄºÊ≥ïÔºåapproxÔºåslow analyticalÔºöËß£ÊûêÊ≥ïÔºåexactÔºåfast Stochastic Áî®minibatchÁöÑÊ¢ØÂ∫¶Êù•approximateÂÖ®ÈõÜ $\theta_{k+1} = \theta_k - v_{t+1}(x_i,y_i)$ classic optimizersÔºöSGDÔºåMomentumÔºåNesterov‚Äòs momentum adaptive optimizersÔºöAdaGradÔºåAdadeltaÔºåRMSPropÔºåAdam Newton modern optimizers for large-batch * AdamW LARS LAMB common updating steps for current step tÔºö step1ÔºöËÆ°ÁÆóÁõ¥Êé•Ê¢ØÂ∫¶Ôºå$g_t = \nabla f(w_t)$ step2ÔºöËÆ°ÁÆó‰∏ÄÈò∂Âä®ÈáèÂíå‰∫åÈò∂Âä®ÈáèÔºå$m_t \&amp; V_t$ step3ÔºöËÆ°ÁÆóÂΩìÂâçÊó∂ÂàªÁöÑ‰∏ãÈôçÊ¢ØÂ∫¶Ôºå$\eta_t = \alpha m_t/\sqrt {V_t}$ step4ÔºöÂèÇÊï∞Êõ¥Êñ∞Ôºå$w_{t+1} = w_t - \eta_t$ ÂêÑÁßç‰ºòÂåñÁÆóÊ≥ïÁöÑ‰∏ªË¶ÅÂ∑ÆÂà´Âú®step1Âíåstep2‰∏ä ÊªëÂä®Âπ≥Âùá/ÊåáÊï∞Âä†ÊùÉÂπ≥Âùá/moving average/EMA Â±ÄÈÉ®ÂùáÂÄºÔºå‰∏é‰∏ÄÊÆµÊó∂Èó¥ÂÜÖÁöÑÂéÜÂè≤Áõ∏ÂÖ≥ $v_t = \beta v_{t-1}+(1-\beta)\theta_t$ÔºåÂ§ßËá¥Á≠â‰∫éËøáÂéª$1/(1-\beta)$‰∏™Êó∂ÂàªÁöÑ$\theta$ÁöÑÂπ≥ÂùáÂÄºÔºå‰ΩÜÊòØÂú®Ëµ∑ÂßãÁÇπÈôÑËøëÂÅèÂ∑ÆËæÉÂ§ß $v_{tbiased} = \frac{v_t}{1-\beta^t}$ÔºåÂÅö‰∫Übias correction tË∂äÂ§ßÔºåË∂ä‰∏çÈúÄË¶Å‰øÆÊ≠£Ôºå‰∏§‰∏™ÊªëÂä®ÂùáÂÄºÁöÑÁªìÊûúË∂äÊé•Ëøë ‰ºòÁº∫ÁÇπÔºö‰∏çÁî®‰øùÂ≠òÂéÜÂè≤Ôºå‰ΩÜÊòØËøë‰ºº SGD SGDÊ≤°ÊúâÂä®ÈáèÁöÑÊ¶ÇÂøµÔºå$m_t=g_t$Ôºå$V_t=I^2$Ôºå$w_{t+1} = w_t - \alpha g_t$ ‰ªÖ‰æùËµñÂΩìÂâçËÆ°ÁÆóÁöÑÊ¢ØÂ∫¶ Áº∫ÁÇπÔºö‰∏ãÈôçÈÄüÂ∫¶ÊÖ¢ÔºåÂèØËÉΩÈô∑Âú®local optima‰∏äÊåÅÁª≠ÈúáËç° SGDW (with weight decay) Âú®ÊùÉÈáçÊõ¥Êñ∞ÁöÑÂêåÊó∂ËøõË°åÊùÉÈáçË°∞Âáè $w_{t+1} = (1-\lambda)w_t - \alpha g_t$ Âú®SGD formÁöÑ‰ºòÂåñÂô®‰∏≠weight decayÁ≠â‰ª∑‰∫éÂú®loss‰∏äL2 regularization ‰ΩÜÊòØÂú®adaptive formÁöÑ‰ºòÂåñÂô®‰∏≠ÊòØ‰∏çÁ≠â‰ª∑ÁöÑÔºÅÔºÅÂõ†‰∏∫historical funcÔºàERMÔºâ‰∏≠regularizerÂíågradient‰∏ÄËµ∑Ë¢´downscale‰∫ÜÔºåÂõ†Ê≠§not as much as they would get regularized in SGDW SGD with Momentum ÂºïÂÖ•‰∏ÄÈò∂Âä®ÈáèÔºå$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$Ôºå‰ΩøÁî®ÊªëÂä®ÂùáÂÄºÔºåÊäëÂà∂ÈúáËç° Ê¢ØÂ∫¶‰∏ãÈôçÁöÑ‰∏ªË¶ÅÊñπÂêëÊòØÊ≠§ÂâçÁ¥ØÁßØÁöÑ‰∏ãÈôçÊñπÂêëÔºåÁï•ÂæÆÂêëÂΩìÂâçÊó∂ÂàªÁöÑÊñπÂêëË∞ÉÊï¥ SGD with Nesterov Acceleration look ahead SGD-momentum Âú®local minimaÁöÑÊó∂ÂÄôÔºåÂõõÂë®Ê≤°Êúâ‰∏ãÈôçÁöÑÊñπÂêëÔºå‰ΩÜÊòØÂ¶ÇÊûúËµ∞‰∏ÄÊ≠•ÂÜçÁúãÔºåÂèØËÉΩÂ∞±‰ºöÊâæÂà∞‰ºòÂåñÊñπÂêë ÂÖàË∑üÁùÄÁ¥ØÁßØÂä®ÈáèËµ∞‰∏ÄÊ≠•ÔºåÊ±ÇÊ¢ØÂ∫¶Ôºö$g_t = \nabla f(w_t-\alpha m_{t-1}/\sqrt {V_{t-1}})$ Áî®Ëøô‰∏™ÁÇπÁöÑÊ¢ØÂ∫¶ÊñπÂêëÊù•ËÆ°ÁÆóÊªëÂä®Âπ≥ÂùáÔºåÂπ∂Êõ¥Êñ∞Ê¢ØÂ∫¶ Adagrad ÂºïÂÖ•‰∫åÈò∂Âä®ÈáèÔºåÂºÄÂêØ‚ÄúËá™ÈÄÇÂ∫îÂ≠¶‰π†Áéá‚ÄùÔºå$V_t = \sum_0^t g_k^2$ÔºåÂ∫¶ÈáèÂéÜÂè≤Êõ¥Êñ∞È¢ëÁéá ÂØπ‰∫éÁªèÂ∏∏Êõ¥Êñ∞ÁöÑÂèÇÊï∞ÔºåÊàë‰ª¨Â∑≤ÁªèÁßØÁ¥Ø‰∫ÜÂ§ßÈáèÂÖ≥‰∫éÂÆÉÁöÑÁü•ËØÜÔºå‰∏çÂ∏åÊúõË¢´Âçï‰∏™Ê†∑Êú¨ÂΩ±ÂìçÂ§™Â§ßÔºåÂ∏åÊúõÂ≠¶‰π†ÈÄüÁéáÊÖ¢‰∏Ä‰∫õÔºõÂØπ‰∫éÂÅ∂Â∞îÊõ¥Êñ∞ÁöÑÂèÇÊï∞ÔºåÊàë‰ª¨‰∫ÜËß£ÁöÑ‰ø°ÊÅØÂ§™Â∞ëÔºåÂ∏åÊúõËÉΩ‰ªéÊØè‰∏™ÂÅ∂ÁÑ∂Âá∫Áé∞ÁöÑÊ†∑Êú¨Ë∫´‰∏äÂ§öÂ≠¶‰∏Ä‰∫õÔºåÂç≥Â≠¶‰π†ÈÄüÁéáÂ§ß‰∏Ä‰∫õ $\eta_t = \alpha m_t / \sqrt{V_t}$ÔºåÊú¨Ë¥®‰∏ä‰∏∫ÊØè‰∏™ÂèÇÊï∞ÔºåÂØπÂ≠¶‰π†ÁéáÂàÜÂà´rescale Áº∫ÁÇπÔºö‰∫åÈò∂Âä®ÈáèÂçïË∞ÉÈÄíÂ¢ûÔºåÂØºËá¥Â≠¶‰π†ÁéáÂçïË∞ÉË°∞ÂáèÔºåÂèØËÉΩ‰ºö‰ΩøÂæóËÆ≠ÁªÉËøáÁ®ãÊèêÂâçÁªìÊùü AdaDelta/RMSProp ÂèÇËÄÉmomentumÔºåÂØπ‰∫åÈò∂Âä®Èáè‰πüËÆ°ÁÆóÊªëÂä®Âπ≥ÂùáÔºå$V_t = \beta_2 V_{t-1} + (1-\beta_2)g_t^2$ ÈÅøÂÖç‰∫Ü‰∫åÈò∂Âä®ÈáèÊåÅÁª≠Á¥ØÁßØ„ÄÅÂØºËá¥ËÆ≠ÁªÉËøáÁ®ãÊèêÂâçÁªìÊùü Adam ÈõÜÂ§ßÊàêËÄÖÔºöÊää‰∏ÄÈò∂Âä®ÈáèÂíå‰∫åÈò∂Âä®ÈáèÈÉΩÁî®Ëµ∑Êù•ÔºåAdaptive Momentum SGD-MÂú®SGDÂü∫Á°Ä‰∏äÂ¢ûÂä†‰∫Ü‰∏ÄÈò∂Âä®Èáè AdaGradÂíåAdaDeltaÂú®SGDÂü∫Á°Ä‰∏äÂ¢ûÂä†‰∫Ü‰∫åÈò∂Âä®Èáè ‰∏ÄÈò∂Âä®ÈáèÊªëÂä®Âπ≥ÂùáÔºö$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$ ‰∫åÈò∂Âä®ÈáèÊªëÂä®Âπ≥ÂùáÔºö$V_t = \beta_2 V_{t-1} + (1-\beta_2)g_t^2$ Nadam look ahead Adam ÊääNesterovÁöÑone step tryÂä†‰∏äÔºö$g_t = \nabla f(w_t-\alpha m_{t-1}/\sqrt {V_{t-1}})$ ÂÜçAdamÊõ¥Êñ∞‰∏§‰∏™Âä®Èáè ÁªèÈ™åË∂ÖÂèÇ $momentum=0.9$ $\beta_1=0.9$ $\beta_2=0.999$ $m_0 = 0$ $V_0 = 0$ ‰∏äÈù¢ÁöÑÂõæ‰∏äÂèØ‰ª•ÁúãÂá∫ÔºåÂàùÊúüÁöÑ$m_t$Âíå$V_t$‰ºöÊó†ÈôêÊé•Ëøë‰∫é0ÔºåÊ≠§Êó∂ÂèØ‰ª•ËøõË°åËØØÂ∑Æ‰øÆÊ≠£Ôºö$factor=\frac{1}{1-\beta^t}$ AdamW Âú®adaptive methods‰∏≠ÔºåËß£ËÄ¶weight-decayÂíåloss-based gradientÂú®ERMËøáÁ®ã‰∏≠ÁöÑÁªëÂÆödownscaleÁöÑÂÖ≥Á≥ª ÂÆûË¥®Â∞±ÊòØÂ∞ÜÂØºÊï∞È°πÂêéÁßª]]></content>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[regnet]]></title>
    <url>%2F2021%2F03%2F11%2Fregnet%2F</url>
    <content type="text"><![CDATA[RegNet: Designing Network Design Spaces Âä®Êú∫ study the network design principles design RegNet outperforms efficientNet and 5x faster top1 errorÔºö20.1 Ôºàeff-b5Ôºö21.5Ôºâ larger batch size 1/4 ÁöÑ train/test latency ËÆ∫ÁÇπ manual network design AlexNet, ResNet family, DenseNet, MobileNet focus on discovering new design choices that improve acc the recent popular approach NAS search the best in a fixed search space of possible networks limitationsÔºögeneralize to new settingsÔºålack of interpretability network scaling ‰∏äÈù¢‰∏§‰∏™focus on ÊâæÂá∫‰∏Ä‰∏™basenet for a specific regime scaling rules aims at tuning the optimal network in any target regime comparing networks the reliable comparison metric to guide the design process our method combines the disadvantages of manual design and NAS first AnyNet then RegNet ÊñπÊ≥ï]]></content>
  </entry>
  <entry>
    <title><![CDATA[mongodb]]></title>
    <url>%2F2021%2F03%2F09%2Fmongodb%2F</url>
    <content type="text"><![CDATA[downloadÔºöhttps://www.mongodb.com/try/download/enterprise install 123456789101112# Â∞ÜËß£Âéã‰ª•ÂêéÁöÑÊñá‰ª∂Â§πÊîæÂú®/usr/local‰∏ãsudo mv mongodb-osx-x86_64-4.0.9/ /usr/local/sudo ln -s mongodb-macos-x86_64-4.4.4 mongodb# ENV PATHexport PATH=/usr/local/mongodb/bin:$PATH# ÂàõÂª∫Êó•ÂøóÂèäÊï∞ÊçÆÂ≠òÊîæÁöÑÁõÆÂΩïsudo mkdir -p /usr/local/var/mongodbsudo mkdir -p /usr/local/var/log/mongodbsudo chown [amber] /usr/local/var/mongodbsudo chown [amber] /usr/local/var/log/mongodb configuration 12345678# ÂêéÂè∞ÂêØÂä®mongod --dbpath /usr/local/var/mongodb --logpath /usr/local/var/log/mongodb/mongo.log --fork# ÊéßÂà∂Âè∞ÂêØÂä®mongod --config /usr/local/etc/mongod.conf# Êü•ÁúãÁä∂ÊÄÅps aux | grep -v grep | grep mongod run 123# Âú®dbÁéØÂ¢É‰∏ãÂêØÂä®‰∏Ä‰∏™ÁªàÁ´Øcd /usr/local/mongodb/bin ./mongo original settings 123456789101112131415161718192021# ÊòæÁ§∫ÊâÄÊúâÊï∞ÊçÆÁöÑÂàóË°®&gt; show dbsadmin 0.000GBconfig 0.000GBlocal 0.000GB# ‰∏â‰∏™Á≥ªÁªü‰øùÁïôÁöÑÁâπÊÆäÊï∞ÊçÆÂ∫ì# ËøûÊé•/ÂàõÂª∫‰∏Ä‰∏™ÊåáÂÆöÁöÑÊï∞ÊçÆÂ∫ì&gt; use localswitched to db local# ÊòæÁ§∫ÂΩìÂâçÊï∞ÊçÆÂ∫ì, Â¶ÇÊûúÊ≤°useÈªòËÆ§‰∏∫test&gt; dbtest# „ÄêÔºÅÔºÅÈáçË¶Å„ÄëÂÖ≥Èó≠ÊúçÂä°‰πãÂâçÊúçÂä°Âô®Ë¢´kill -9Âº∫Âà∂ÂÖ≥Èó≠ÔºåÊï∞ÊçÆÂ∫ì‰∏¢Â§±‰∫Ü&gt; use adminswitched to db admin&gt; db.shutdownServer()server should be down... concepts ÊñáÊ°£document ‰∏ÄÁªÑkey-valueÂØπÔºåÂ¶Ç‰∏äÈù¢Â∑¶Âõæ‰∏≠ÁöÑ‰∏ÄË°åËÆ∞ÂΩïÔºåÂ¶Ç‰∏äÈù¢Âè≥Âõæ‰∏≠ÁöÑ‰∏Ä‰∏™dict ÈõÜÂêàcollection ‰∏ÄÂº†Ë°®ÔºåÂ¶Ç‰∏äÈù¢Â∑¶ÂõæÂíå‰∏äÈù¢Âè≥Âõæ ‰∏ªÈîÆprimary key ÂîØ‰∏Ä‰∏ªÈîÆÔºåObjectIdÁ±ªÂûãÔºåËá™ÂÆöÁîüÊàêÔºåÊúâÊ†áÂáÜÊ†ºÂºè Â∏∏Áî®ÂëΩ‰ª§ 10.1 ÂàõÂª∫/Âà†Èô§/ÈáçÂëΩÂêçdb 12345678910111213141516171819202122232425262728# ÂàáÊç¢Ëá≥Êï∞ÊçÆÂ∫ìtest1&gt; use test1# ÊèíÂÖ•‰∏ÄÊù°doc, db.COLLECTION_NAME.insert(document)# dbË¶ÅÂåÖÂê´Ëá≥Â∞ë‰∏ÄÊù°ÊñáÊ°£ÔºåÊâçËÉΩÂú®show dbsÁöÑÊó∂ÂÄôÊòæÁ§∫ÔºàÊâçÁúüÊ≠£ÂàõÂª∫Ôºâ&gt; db.sheet1.insert(&#123;'name': img0&#125;)# ÊòæÁ§∫ÂΩìÂâçÂ∑≤ÊúâÊï∞ÊçÆÂ∫ì&gt; show dbs# Âà†Èô§ÊåáÂÆöÊï∞ÊçÆÂ∫ì&gt; use test1&gt; db.dropDatabase()# ÊóßÁâàÊú¨(before4.0)ÈáçÂëΩÂêçÔºöÂÖàÊã∑Ë¥ù‰∏Ä‰ªΩÔºåÂú®Âà†Èô§ÊóßÁöÑ&gt; db.copyDatabase('OLDNAME', 'NEWNAME');&gt; use old_name&gt; db.dropDatabase()# Êñ∞ÁâàÊú¨ÈáçÂëΩÂêçÔºödump&amp;restoreÔºåËøô‰∏™‰∏úË•øÂú®mongodb toolsÈáåÈù¢ÔºåË¶ÅÂè¶Â§ñ‰∏ãËΩΩÔºåÂèØÊâßË°åÊñá‰ª∂ÊîæÂú®bin‰∏ã# mongodump # Â∞ÜÊâÄÊúâÊï∞ÊçÆÂ∫ìÂØºÂá∫Âà∞bin/dump/‰ª•ÊØè‰∏™dbÂêçÂ≠óÂëΩÂêçÁöÑÊñá‰ª∂Â§π‰∏ã# mongodump -h dbhost -d dbname -o dbdirectory# -h: ÊúçÂä°Âô®Âú∞ÂùÄ:Á´ØÂè£Âè∑# -d: ÈúÄË¶ÅÂ§á‰ªΩÁöÑÊï∞ÊçÆÂ∫ì# -o: Â≠òÊîæ‰ΩçÁΩÆÔºàÈúÄË¶ÅÂ∑≤Â≠òÂú®Ôºâmongodump -d test -o tmp/# Âú®ÊÅ¢Â§çÂ§á‰ªΩÊï∞ÊçÆÂ∫ìÁöÑÊó∂ÂÄôÊç¢‰∏™ÂêçÂ≠óÔºömongorestore -h dbhost -d dbname pathmongorestore -d test_bkp tmp/test# ËøôÊó∂ÂÄôÂèØ‰ª•ÁúãÂà∞‰∏Ä‰∏™Êñ∞Â¢û‰∫Ü‰∏Ä‰∏™Âè´test_bkpÁöÑdb 10.2 ÂàõÂª∫/Âà†Èô§/ÈáçÂëΩÂêçcollection 12345678910111213141516# ÂàõÂª∫Ôºödb.createCollection(name, options)&gt; db.createCollection('case2img')# ÊòæÁ§∫Â∑≤Êúâtables&gt; show collections# ‰∏çÁî®ÊòæÁ§∫ÂàõÂª∫ÔºåÂú®db insertÁöÑÊó∂ÂÄô‰ºöËá™Âä®ÂàõÂª∫ÈõÜÂêà&gt; db.sheet2.insert(&#123;"name" : "img2"&#125;)# Âà†Èô§Ôºödb.COLLECTION_NAME.drop()&gt; db.sheet2.drop()# ÈáçÂëΩÂêçÔºödb.COLLECTION_NAME.renameCollection('NEWNAME')&gt; db.sheet2.renameCollection('sheet3')# Â§çÂà∂Ôºödb.COLLECTION_NAME.aggregate(&#123;$out: 'NEWNAME'&#125;)&gt; db.sheet2.aggregate(&#123; $out : "sheet3" &#125;) 10.3 ÊèíÂÖ•/ÊòæÁ§∫/Êõ¥Êñ∞/Âà†Èô§document 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# ÊèíÂÖ•db.COLLECTION_NAME.insert(document)db.COLLECTION_NAME.save(document)db.COLLECTION_NAME.insertOne()db.COLLECTION_NAME.insertMany()# ÊòæÁ§∫Â∑≤Êúâdocdb.COLLECTION_NAME.find()# Êõ¥Êñ∞docÁöÑÈÉ®ÂàÜÂÜÖÂÆπdb.COLLECTION_NAME.update( &lt;query&gt;, # Êü•ËØ¢Êù°‰ª∂ &lt;update&gt;, # Êõ¥Êñ∞Êìç‰Ωú &#123; upsert: &lt;boolean&gt;, # if true Â¶ÇÊûú‰∏çÂ≠òÂú®ÂàôÊèíÂÖ• multi: &lt;boolean&gt;, # find fist/all match writeConcern: &lt;document&gt; &#125;)&gt; db.case2img.insert(&#123;"case": "s0", "name": "img0"&#125;)&gt; db.case2img.insert(&#123;"case": "s1", "name": "img1"&#125;)&gt; db.case2img.find()&gt; db.case2img.update(&#123;'case': 's1'&#125;, &#123;$set: &#123;'case': 's2', 'name': 'img2'&#125;&#125;)&gt; db.case2img.find()# ÁªôdocÁöÑÊüê‰∏™keyÈáçÂëΩÂêçdb.COLLECTION_NAME.updateMany(&#123;&#125;,&#123;'$rename': &#123;"old_key": "new_key"&#125;&#125;)# Êõ¥Êñ∞Êï¥Êù°ÊñáÊ°£by object_iddb.COLLECTION_NAME.save( &lt;document&gt;, &#123; writeConcern: &lt;document&gt; &#125;)&gt; db.case2img.save(&#123;"_id": ObjectId("60474e4b77e21bad9bd4655a"), "case":"s3", "name":"img3"&#125;)# Âà†Èô§Êª°Ë∂≥Êù°‰ª∂ÁöÑdocdb.COLLECTION_NAME.remove( &lt;query&gt;, &#123; justOne: &lt;boolean&gt;, # find fist/all match writeConcern: &lt;document&gt; &#125;)&gt; db.case2img.remove(&#123;"case": "s0"&#125;)# Âà†Èô§ÊâÄÊúâdoc&gt; db.case2img.remove(&#123;&#125;) 10.4 ÁÆÄÂçïÊü•ËØ¢find 1234567891011121314151617&gt; db.case2img.insert(&#123;"case": "s0", "name": "img0"&#125;)&gt; db.case2img.insert(&#123;"case": "s1", "name": "img1"&#125;)&gt; db.case2img.insert(&#123;"case": "s2", "name": "img2"&#125;)&gt; db.case2img.insert(&#123;"case": "s2", "name": "img3"&#125;)# Êü•ËØ¢Ë°®‰∏≠ÁöÑdocÔºödb.COLLECTION_NAME.find(&#123;query&#125;)&gt; db.case2img.find(&#123;'case': s2&#125;)&gt; db.case2img.find(&#123;'case': 's1'&#125;, &#123;"name":1&#125;) # projectionÁöÑvalueÂú®ÂØπÂ∫îÁöÑkey-valueÊòØlistÁöÑÊó∂ÂÄôÊúâÊÑè‰πâ# Ê†ºÂºèÂåñÊòæÁ§∫Êü•ËØ¢ÁªìÊûúÔºödb.COLLECTION_NAME.find(&#123;query&#125;).pretty()&gt; db.case2img.find(&#123;'case': s2&#125;).pretty()# ËØªÂèñÊåáÂÆöÊï∞ÈáèÁöÑÊï∞ÊçÆËÆ∞ÂΩïÔºödb.COLLECTION_NAME.find(&#123;query&#125;).limit(NUMBER)&gt; db.case2img.find(&#123;'case': &#123;$type: 'string'&#125;&#125;).limit(1)# Ë∑≥ËøáÊåáÂÆöÊï∞ÈáèÁöÑÊï∞ÊçÆÔºödb.COLLECTION_NAME.find(&#123;query&#125;).skip(NUMBER)&gt; db.case2img.find(&#123;'case': &#123;$type: 'string'&#125;&#125;).skip(1) 10.5 Êù°‰ª∂Êìç‰ΩúÁ¨¶ 123456789101112131415(&gt;) Â§ß‰∫é - $gt(&lt;) Â∞è‰∫é - $lt(&gt;=) Â§ß‰∫éÁ≠â‰∫é - $gte(&lt;=) Â∞è‰∫éÁ≠â‰∫é - $lte(or) Êàñ - $or&gt; db.case2img.update(&#123;'case':'s1'&#125;, &#123;$set: &#123;"name":'img1', 'size':100&#125;&#125;)WriteResult(&#123; "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 &#125;)&gt; db.case2img.update(&#123;'case':'s2'&#125;, &#123;$set: &#123;"name":'img2', 'size':200&#125;&#125;)# Êü•ËØ¢size&gt;150ÁöÑdoc&gt; db.case2img.find(&#123;'size': &#123;$gt: 150&#125;&#125;)# Êü•ËØ¢Êª°Ë∂≥‰ªªÊÑè‰∏Ä‰∏™Êù°‰ª∂ÁöÑdoc&gt; db.case2img.find(&#123;'$or': [&#123;'case':'s1'&#125;, &#123;'size': &#123;$gt: 150&#125;&#125;]&#125;) 10.6 Êï∞ÊçÆÁ±ªÂûãÊìç‰ΩúÁ¨¶ 12345type(KEY)Á≠â‰∫é - $type# ÊØîËæÉÂØπË±°ÂèØ‰ª•ÊòØÂ≠óÁ¨¶‰∏≤/ÂØπÂ∫îÁöÑreflect NUM&gt; db.case2img.find(&#123;'case': &#123;$type: 'string'&#125;&#125;)&gt; db.case2img.find(&#123;'case': &#123;$type: '0'&#125;&#125;) 10.7 ÊéíÂ∫èfind().sort 1234# ÈÄöËøáÊåáÂÆöÂ≠óÊÆµ&amp;ÊåáÂÆöÂçáÂ∫è/ÈôçÂ∫èÊù•ÂØπÊï∞ÊçÆÊéíÂ∫èÔºödb.COLLECTION_NAME.find().sort(&#123;KEY:1/-1&#125;)&gt; db.case2img.find().sort(&#123;'name':1&#125;)# skip(), limilt(), sort()‰∏â‰∏™ÊîæÂú®‰∏ÄËµ∑ÊâßË°åÁöÑÊó∂ÂÄôÔºåÊâßË°åÁöÑÈ°∫Â∫èÊòØÂÖà sort(), ÁÑ∂ÂêéÊòØ skip()ÔºåÊúÄÂêéÊòØÊòæÁ§∫ÁöÑ limit()„ÄÇ 10.8 Á¥¢Âºï skip 10.9 ËÅöÂêàaggregate 123456789101112131415161718# Áî®‰∫éÁªüËÆ° db.COLLECTION_NAME.aggregate(AGGREGATE_OPERATION)# by group&gt; db.case2img.aggregate([&#123;$group: &#123;_id: '$case', img_num:&#123;$sum:1&#125;&#125;&#125;])group by key value 'case'count number of items in each grouprefer to the number as img_num&gt; db.case2img.aggregate([&#123;$group: &#123;_id: '$case', img_num:&#123;$sum:'$size'&#125;&#125;&#125;])ËÆ°ÁÆóÊØè‰∏Ä‰∏™groupÂÜÖÔºåsizeÂÄºÁöÑÊÄªÂíå# by match&gt; db.case2img.aggregate([&#123;$match: &#123;'size': &#123;$gt:150&#125;&#125;&#125;, &#123;$group:&#123;_id: null, totalsize: &#123;$sum: '$size'&#125;&#125;&#125;])Á±ª‰ººshellÁöÑÁÆ°ÈÅìÔºåmatchÁî®Êù•Á≠õÈÄâÊù°‰ª∂ÔºåÁ¨¶ÂêàÊù°‰ª∂ÁöÑÈÄÅÂÖ•‰∏ã‰∏ÄÊ≠•ÁªüËÆ°&gt; db.case2img.aggregate([&#123;$skip: 4&#125;, &#123;$group:&#123;_id: null, totalsize: &#123;$sum: '$size'&#125;&#125;&#125;]) Âø´ÈÄüÁªüËÆ°distinct 12db.case2img.distinct(TAG_NAME)# Ê≥®ÊÑèÂ¶ÇÊûúdistinctÁöÑÂÜÖÂÆπÂ§™ÈïøÔºåË∂ÖËøá16MÔºå‰ºöÊä•distinct too bigÁöÑerrorÔºåÊé®ËçêÁî®ËÅöÂêàÊù•ÂÅöÁªüËÆ° 123456789101112 12. pymongo Áî®python‰ª£Á†ÅÊù•Êìç‰ΩúÊï∞ÊçÆÂ∫ì ÂÖàÂÆâË£ÖÔºöpip install pymongo 11.1 ËøûÊé•client ```python from pymongo import MongoClient Client = MongoClient() 11.2 Ëé∑ÂèñÊï∞ÊçÆÂ∫ì 12db = Client.DB_NAMEdb = Client['DB_NAME'] 11.3 Ëé∑Âèñcollection 12collection = db.COLLECTION_NAMEcollection = db['COLLECTION_NAME'] 11.4 ÊèíÂÖ•doc 123456789101112# insert onedocument1 = &#123;'x':1&#125;document2 = &#123;'x':2&#125;post_1 = collection.insert_one(document1).inserted_idpost_2 = collection.insert_one(document2).inserted_idprint(post_1)# insert manynew_document = [&#123;'x':1&#125;,&#123;'x':2&#125;]# new_document = [document1,document2] Ê≥®ÊÑèdocÊòØÁ•ûÊã∑Ë¥ùÔºåÂè™ËÉΩ‰Ωú‰∏∫‰∏ÄÊù°docË¢´ÊèíÂÖ•‰∏ÄÊ¨°result = collection.insert_many(new_document).inserted_idsprint(result) 11.5 Êü•Êâæ 12345678910from bson.objectid import ObjectId# find one ËøîÂõû‰∏ÄÊù°docresult = collection.find_one()result = collection.find_one(&#123;'case': 's0'&#125;)result = collection.find_one(&#123;'_id': ObjectId('604752f277e21bad9bd46560')&#125;)# find ËøîÂõû‰∏Ä‰∏™Ëø≠‰ª£Âô®for _, item in enumerate(collection.find()): print(item) 11.6 Êõ¥Êñ∞ 123456# update onecollection.update_one(&#123;'case':'s1'&#125;,&#123;'$set':&#123;'size':300&#125;&#125;)collection.update_one(&#123;'case':'s1'&#125;,&#123;'$push':&#123;'add':1&#125;&#125;) # ËøΩÂä†Êï∞ÁªÑÂÜÖÂÆπ# update manycollection.update_many(&#123;'case':'s1'&#125;,&#123;'$set':&#123;'size':300&#125;&#125;) 11.7 Âà†Èô§ 123# Âú®mongo shellÈáåÈù¢ÊòØremoveÊñπÊ≥ïÔºåÂú®pymongoÈáåÈù¢Ë¢´deprecatedÊàêdeleteÊñπÊ≥ïcollection.delete_one(&#123;"case": "s2"&#125;)collection.delete_many(&#123;"case": "s1"&#125;) 11.8 ÁªüËÆ° 12345# ËÆ°Êï∞ÔºöcountÊñπÊ≥ïÂ∑≤ÁªèË¢´ÈáçÊûÑprint(collection.count_documents(&#123;'case':'s0'&#125;))# uniqueÔºödistinctÊñπÊ≥ïprint(collection.distinct('case')) ‚Äã 11.9 Ê≠£Âàô ‚Äã mongo shellÂëΩ‰ª§Ë°åÈáåÁöÑÊ≠£ÂàôÂíåpymongoËÑöÊú¨ÈáåÁöÑÊ≠£ÂàôÂÜôÊ≥ïÊòØ‰∏ç‰∏ÄÊ†∑ÁöÑÔºåÂõ†‰∏∫pythonÈáåÈù¢ÊúâÂ∞ÅË£ÖÊ≠£ÂàôÊñπÊ≥ïÔºåÁÑ∂ÂêéÈÄöËøábsonÂ∞ÜpythonÁöÑÊ≠£ÂàôËΩ¨Êç¢ÊàêÊï∞ÊçÆÂ∫ìÁöÑÊ≠£Âàô 12345678910# pymongoimport reimport bsonpattern = re.compile(r'(.*)-0[345]-(.*)')regex = bson.regex.Regex.from_native(pattern)result = collection.aggregate([&#123;'$match': &#123;'date': regex&#125;&#125;])# mongo shell&gt; db.collection.find(&#123;date:&#123;$regex:"(.*)-0[345]-(.*)"&#125;&#125;)]]></content>
      <tags>
        <tag>Êï∞ÊçÆÂ∫ìÔºåNoSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker]]></title>
    <url>%2F2021%2F03%2F04%2Fdocker%2F</url>
    <content type="text"><![CDATA[shartup ÈÉ®ÁΩ≤ÊñπÊ°à Âè§Êó©Âπ¥‰ª£ ËôöÊãüÊú∫ docker imageÈïúÂÉè &amp; containerÂÆπÂô® &amp; registry‰ªìÂ∫ì ÈïúÂÉèÔºöÁõ∏ÂΩì‰∫éÊòØ‰∏Ä‰∏™ root Êñá‰ª∂Á≥ªÁªüÔºåÊèê‰æõÂÆπÂô®ËøêË°åÊó∂ÊâÄÈúÄÁöÑÁ®ãÂ∫è„ÄÅÂ∫ì„ÄÅËµÑÊ∫ê„ÄÅÈÖçÁΩÆÁ≠â ÂÆπÂô®ÔºöÈïúÂÉèËøêË°åÊó∂ÁöÑÂÆû‰ΩìÔºåÂèØ‰ª•Ë¢´ÂàõÂª∫„ÄÅÂêØÂä®„ÄÅÂÅúÊ≠¢„ÄÅÂà†Èô§„ÄÅÊöÇÂÅúÁ≠â ‰ªìÂ∫ìÔºöÁî®Êù•‰øùÂ≠òÈïúÂÉè ÂÆòÊñπ‰ªìÂ∫ìÔºödocker hubÔºöhttps://hub.docker.com/r/floydhub/tensorflow/tags?page=1&amp;ordering=last_updated Â∏∏Áî®ÂëΩ‰ª§ ÊãâÈïúÂÉè docker pull [ÈÄâÈ°π] [Docker Registry Âú∞ÂùÄ[:Á´ØÂè£Âè∑]/]‰ªìÂ∫ìÂêç[:Ê†áÁ≠æ] Âú∞ÂùÄÂèØ‰ª•ÊòØÂÆòÊñπÂú∞ÂùÄÔºå‰πüÂèØ‰ª•ÊòØÁ¨¨‰∏âÊñπÔºàÂ¶ÇHarborÔºâ ‰ªìÂ∫ìÂêçÁî±‰ΩúËÄÖÂêçÂíåËΩØ‰ª∂ÂêçÁªÑÊàêÔºàÂ¶Çzhangruiming/skinÔºâ Ê†áÁ≠æÁî®Êù•ÊåáÂÆöÊüê‰∏™ÁâàÊú¨ÁöÑimageÔºåÁúÅÁï•ÂàôÈªòËÆ§latest ÂàóÂá∫ÊâÄÊúâÈïúÂÉè docker images Âà†Èô§ÈïúÂÉè docker rmi [-f] [ÈïúÂÉèid] Âà†Èô§ÈïúÂÉè‰πãÂâçË¶Åkill/rmÊâÄÊúâ‰ΩøÁî®ËØ•ÈïúÂÉèÁöÑcontainerÔºödocker rm [ÂÆπÂô®id] ËøêË°åÈïúÂÉèÂπ∂ÂàõÂª∫‰∏Ä‰∏™ÂÆπÂô® docker run [-it] [‰ªìÂ∫ìÂêç] [ÂëΩ‰ª§] ÈÄâÈ°π -itÔºö‰∏∫ÂÆπÂô®ÈÖçÁΩÆ‰∏Ä‰∏™‰∫§‰∫íÁªàÁ´Ø ÈÄâÈ°π -dÔºöÂêéÂè∞ËøêË°åÂÆπÂô®ÔºåÂπ∂ËøîÂõûÂÆπÂô®IDÔºà‰∏çÁõ¥Êé•ËøõÂÖ•ÁªàÁ´ØÔºâ ÈÄâÈ°π ‚Äîname=‚Äôxxx‚ÄôÔºö‰∏∫ÂÆπÂô®ÊåáÂÆö‰∏Ä‰∏™ÂêçÁß∞ ÈÄâÈ°π-v /host_dir:/container_dirÔºöÂ∞Ü‰∏ªÊú∫‰∏äÊåáÂÆöÁõÆÂΩïÊò†Â∞ÑÂà∞ÂÆπÂô®ÁöÑÊåáÂÆöÁõÆÂΩï [ÂëΩ‰ª§]ÂèÇÊï∞ÂøÖÈ°ªË¶ÅÂä†ÔºåËÄå‰∏îË¶ÅÊòØÈÇ£Áßç‰∏ÄÁõ¥ÊåÇËµ∑ÁöÑÂëΩ‰ª§Ôºà/bin/bashÔºâÔºåÂ¶ÇÊûúÊòØls/cd/Áõ¥Êé•‰∏çÂ°´ÔºåÈÇ£‰πàÂëΩ‰ª§ËøêË°åÂÆåÂÆπÂô®Â∞±‰ºöÂÅúÊ≠¢ËøêË°åÔºådocker ps -aÊü•ÁúãÁä∂ÊÄÅÔºåÂèëÁé∞ÈÉΩÊòØExited ÂàõÂª∫ÂÆπÂô® docker run Êü•ÁúãÊâÄÊúâÂÆπÂô® docker ps ÂêØÂä®‰∏Ä‰∏™Â∑≤ÁªèÂÅúÊ≠¢ÁöÑÂÆπÂô®/ÂÅúÊ≠¢Ê≠£Âú®ËøêË°åÁöÑÂÆπÂô® docker start [ÂÆπÂô®id] docker stop [ÂÆπÂô®id] ËøõÂÖ•ÂÆπÂô® docker exec -it [ÂÆπÂô®id] [linuxÂëΩ‰ª§] Âà†Èô§ÂÆπÂô® docker rm [ÂÆπÂô®id] Âà†Èô§ÊâÄÊúâ‰∏çÊ¥ªË∑ÉÁöÑÂÆπÂô® docker container prune Êèê‰∫§ÈïúÂÉèÂà∞ËøúÁ´Ø‰ªìÂ∫ì docker tag [ÈïúÂÉèid] [Áî®Êà∑Âêç]/[‰ªìÂ∫ì]:[Ê†áÁ≠æ] # ÈáçÂëΩÂêç docker login # ÁôªÈôÜÁî®Êà∑ docker push Ê°à‰æã 123456789101112131415161718192021222324252627282930313233# ÊãâÈïúÂÉèdocker pull Âú∞ÂùÄ/‰ªìÂ∫ì:Ê†áÁ≠æ# ÊòæÁ§∫ÈïúÂÉèdocker images# ËøêË°åÊåáÂÆöÈïúÂÉèdocker run -itd --name='test' Âú∞ÂùÄ/‰ªìÂ∫ì:Ê†áÁ≠æ# Êü•ÁúãËøêË°åÁöÑÂÆπÂô®docker ps# ËøõÂÖ•ÂÆπÂô®docker exec -it ÂÆπÂô®idÊàñname /bin/bash# ‰∏ÄÈ°øÊìç‰ΩúÂÆåÈÄÄÂá∫ÂÆπÂô®exit# Â∞Ü‰øÆÊîπÂêéÁöÑÂÆπÂô®‰øùÂ≠ò‰∏∫ÈïúÂÉèdocker commit ÂÆπÂô®idÊàñname Êñ∞ÈïúÂÉèÂêçÂ≠ódocker imagesÂèØ‰ª•ÁúãÂà∞Ëøô‰∏™ÈïúÂÉè‰∫Ü# ‰øùÂ≠òÈïúÂÉèÂà∞Êú¨Âú∞docker save -o tf_torch.rar tf_torch# ËøòÂéüÈïúÂÉèdocker load --input tf_torch.tar# ÈáçÂëΩÂêçÈïúÂÉèdocker tag 3db0b2f40a70 amberzzzz/tf1.14_torch1.4_cuda10.0:v1# Êèê‰∫§ÈïúÂÉèdocker push amberzzzz/tf1.14-torch0.5-cuda10.0:v1 dockerfile Dockerfile ÊòØÁî®Êù•ËØ¥ÊòéÂ¶Ç‰ΩïËá™Âä®ÊûÑÂª∫ docker image ÁöÑÊåá‰ª§ÈõÜÊñá‰ª∂ Â∏∏Áî®ÂëΩ‰ª§ FROM image_nameÔºåÊåáÂÆö‰æùËµñÁöÑÈïúÂÉè RUN commandÔºåÂú® shell ÊàñËÄÖ exec ÁöÑÁéØÂ¢É‰∏ãÊâßË°åÁöÑÂëΩ‰ª§ ADD srcfile_path_inhost dstfile_incontainerÔºåÂ∞ÜÊú¨Êú∫Êñá‰ª∂Â§çÂà∂Âà∞ÂÆπÂô®‰∏≠ CMD [‚Äúexecutable‚Äù,‚Äùparam1‚Äù,‚Äùparam2‚Äù]ÔºåÊåáÂÆöÂÆπÂô®ÂêØÂä®ÈªòËÆ§ÊâßË°åÁöÑÂëΩ‰ª§ WORKDIR path_incontainerÔºåÊåáÂÆö RUN„ÄÅCMD ‰∏é ENTRYPOINT ÂëΩ‰ª§ÁöÑÂ∑•‰ΩúÁõÆÂΩï VOLUME [‚Äú/data‚Äù]ÔºåÊéàÊùÉËÆøÈóÆ‰ªéÂÆπÂô®ÂÜÖÂà∞‰∏ªÊú∫‰∏äÁöÑÁõÆÂΩï basic image ‰ªénvidia dockerÂºÄÂßãÔºöhttps://hub.docker.com/r/nvidia/cuda/tags?page=1&amp;ordering=last_updated&amp;name=10. ÈÄâ‰∏Ä‰∏™ÂñúÊ¨¢ÁöÑÔºöÂ¶Çdocker pull nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04 ÁÑ∂ÂêéÁºñËæëdockerfile 1234567891011121314151617181920FROM nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04MAINTAINER amber &lt;amber.zhang@tum.de&gt;# install basic dependenciesRUN apt-get update RUN apt-get install -y wget vim cmake# install Anaconda3RUN wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.2.0-Linux-x86_64.sh -O ~/anaconda3.shRUN bash ~/anaconda3.sh -b -p /home/anaconda3 &amp;&amp; rm ~/anaconda3.sh ENV PATH /home/anaconda3/bin:$PATH# RUN echo "export PATH=/home/anaconda3/bin:$PATH" &gt;&gt; ~/.bashrc &amp;&amp; /bin/bash -c "source /root/.bashrc" # change mirrorRUN mkdir ~/.pip \ &amp;&amp; cd ~/.pip RUN echo '[global]\nindex-url = https://pypi.tuna.tsinghua.edu.cn/simple/' &gt;&gt; ~/.pip/pip.conf# install tensorflowRUN /home/anaconda3/bin/pip install tensorflow-gpu==1.8.0 ÁÑ∂Âêébuild dockerfile 1docker build -t &lt;docker_name&gt; .]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[layer norm]]></title>
    <url>%2F2021%2F03%2F02%2Flayer-norm%2F</url>
    <content type="text"><![CDATA[ÁªºËø∞ papers [batch norm 2015] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate ShiftÔºåinceptionV2ÔºåGoogle TeamÔºåÂΩí‰∏ÄÂåñÂ±ÇÁöÑÂßãÁ•ñÔºåÂä†ÈÄüËÆ≠ÁªÉ&amp;Ê≠£ÂàôÔºåBNË¢´ÂêéËæàËøΩÁùÄÊâìÁöÑ‰∏ªË¶ÅÁóõÁÇπÔºöapproximation by mini-batchÔºåtest phase frozen [layer norm 2016] Layer NormalizationÔºåToronto+GoogleÔºåÈíàÂØπBN‰∏çÈÄÇÁî®small batchÂíåRNNÁöÑÈóÆÈ¢òÔºå‰∏ªË¶ÅÁî®‰∫éRNNÔºåÂú®CNN‰∏ä‰∏çÂ•ΩÔºåÂú®testÁöÑÊó∂ÂÄô‰πüÊòØactiveÁöÑÔºåÂõ†‰∏∫mean&amp;varianceÁî±‰∫éÂΩìÂâçÊï∞ÊçÆÂÜ≥ÂÆöÔºåÊúâË¥üË¥£rescaleÂíåreshiftÁöÑlayer params [weight norm 2016] Weight normalization: A simple reparameterization to accelerate training of deep neural networksÔºåOpenAIÔºå [cosine norm 2017] Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural NetworksÔºå‰∏≠ÁßëÈô¢Ôºå [instance norm 2017] Instance Normalization: The Missing Ingredient for Fast StylizationÔºåÈ´òÊ†°reportÔºåÈíàÂØπÈ£éÊ†ºËøÅÁßªÔºåINÂú®testÁöÑÊó∂ÂÄô‰πüÊòØactiveÁöÑÔºåËÄå‰∏çÊòØfreezeÁöÑÔºåÂçïÁ∫ØÁöÑinstance-independent normÔºåÊ≤°Êúâlayer params [group norm 2018] Group NormalizationÔºåFAIR KaimingÔºåÈíàÂØπBNÂú®small batch‰∏äÊÄßËÉΩ‰∏ãÈôçÁöÑÈóÆÈ¢òÔºåÊèêÂá∫batch-independentÁöÑ [weight standardization 2019] Weight StandardizationÔºåJohns HopkinsÔºå [batch-channel normalization &amp; weight standardization 2020] BCN&amp;WS: Micro-Batch Training with Batch-Channel Normalization and Weight StandardizationÔºåJohns HopkinsÔºå why Normalization Áã¨Á´ãÂêåÂàÜÂ∏ÉÔºöindependent and identically distribute ÁôΩÂåñÔºöwhiteningÔºà[PCA whitening][http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/]Ôºâ ÂéªÈô§ÁâπÂæÅ‰πãÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄß ‰ΩøÊâÄÊúâÁâπÂæÅÂÖ∑ÊúâÁõ∏ÂêåÁöÑÂùáÂÄºÂíåÊñπÂ∑Æ Ê†∑Êú¨ÂàÜÂ∏ÉÂèòÂåñÔºöInternal Covariate Shift ÂØπ‰∫éÁ•ûÁªèÁΩëÁªúÁöÑÂêÑÂ±ÇËæìÂÖ•ÔºåÁî±‰∫éstacking internel byproductÔºåÊØèÂ±ÇÁöÑÂàÜÂ∏ÉÊòæÁÑ∂ÂêÑ‰∏çÁõ∏ÂêåÔºå‰ΩÜÊòØÂØπ‰∫éÊüê‰∏™ÁâπÂÆöÁöÑÊ†∑Êú¨ËæìÂÖ•Ôºå‰ªñ‰ª¨ÊâÄÊåáÁ§∫ÁöÑlabelÊòØ‰∏çÂèòÁöÑ Âç≥Ê∫êÁ©∫Èó¥ÂíåÁõÆÊ†áÁ©∫Èó¥ÁöÑÊù°‰ª∂Ê¶ÇÁéáÊòØ‰∏ÄËá¥ÁöÑÔºå‰ΩÜÊòØËæπÁºòÊ¶ÇÁéáÊòØ‰∏çÂêåÁöÑ P_s(Y|X=x) = P_t(Y|X=x) \\ P_s(X) \neq P_t(X) ÊØè‰∏™Á•ûÁªèÂÖÉÁöÑÊï∞ÊçÆ‰∏çÂÜçÊòØÁã¨Á´ãÂêåÂàÜÂ∏ÉÔºåÁΩëÁªúÈúÄË¶Å‰∏çÊñ≠ÈÄÇÂ∫îÊñ∞ÁöÑÂàÜÂ∏ÉÔºå‰∏äÂ±ÇÁ•ûÁªèÂÖÉÂÆπÊòìÈ•±ÂíåÔºöÁΩëÁªúËÆ≠ÁªÉÂèàÊÖ¢Âèà‰∏çÁ®≥ÂÆö how to Normalization preparation unitÔºö‰∏Ä‰∏™Á•ûÁªèÂÖÉÔºà‰∏Ä‰∏™opÔºâÔºåËæìÂÖ•[b,N,C_in]ÔºåËæìÂá∫[b,N,1] layerÔºö‰∏ÄÂ±ÇÁöÑÁ•ûÁªèÂÖÉÔºà‰∏ÄÁ≥ªÂàóopÔºå$W\in R^{M*N}$ÔºâÔºåÂú®channel-dim‰∏äconcatÂΩìÂâçÂ±ÇÊâÄÊúâunitÁöÑËæìÂá∫[b,N,C_out] dims bÔºöbatch dimension NÔºöspatial dimensionÔºå1/2/3-dims CÔºöchannel dimension unified representationÔºöÊú¨Ë¥®‰∏äÈÉΩÊòØÂØπÊï∞ÊçÆÂú®ËßÑËåÉÂåñ $h = f(g*\frac{x-\mu}{\sigma}+b)$ÔºöÂÖàÂΩí‰∏ÄÂåñÔºåÂú®rescale &amp; reshift $\mu$ &amp; $\sigma$Ôºöcompute from‰∏ä‰∏ÄÂ±ÇÁöÑÁâπÂæÅÂÄº $g$ &amp; $b$Ôºölearnable paramsÂü∫‰∫éÂΩìÂâçÂ±Ç $f$Ôºöneurons‚Äô weighting operation ÂêÑÊñπÊ≥ïÁöÑ‰∏ªË¶ÅÂå∫Âà´Âú®‰∫émean &amp; varianceÁöÑËÆ°ÁÆóÁª¥Â∫¶ ÂØπÊï∞ÊçÆ BNÔºö‰ª•‰∏ÄÂ±ÇÊØè‰∏™Á•ûÁªèÂÖÉÁöÑËæìÂá∫‰∏∫Âçï‰ΩçÔºåÂç≥ÊØè‰∏™channelÁöÑmean&amp;varÁõ∏‰∫íÁã¨Á´ã LNÔºö‰ª•‰∏ÄÂ±ÇÊâÄÊúâÁ•ûÁªèÂÖÉÁöÑËæìÂá∫‰∏∫Âçï‰ΩçÔºåÂç≥ÊØè‰∏™sampleÁöÑmean&amp;varÁõ∏‰∫íÁã¨Á´ã INÔºö‰ª•ÊØè‰∏™sampleÂú®ÊØè‰∏™Á•ûÁªèÂÖÉÁöÑËæìÂá∫‰∏∫Âçï‰ΩçÔºåÊØè‰∏™sampleÂú®ÊØè‰∏™channelÁöÑmean&amp;varÈÉΩÁõ∏‰∫íÁã¨Á´ã GNÔºö‰ª•ÊØè‰∏™sampleÂú®‰∏ÄÁªÑÁ•ûÁªèÂÖÉÁöÑËæìÂá∫‰∏∫Âçï‰ΩçÔºå‰∏ÄÁªÑÂåÖÂê´‰∏Ä‰∏™Á•ûÁªèÂÖÉÁöÑÊó∂ÂÄôÂèòÊàêINÔºå‰∏ÄÁªÑÂåÖÂê´‰∏ÄÂ±ÇÊâÄÊúâÁ•ûÁªèÂÖÉÁöÑÊó∂ÂÄôÂ∞±ÊòØLN Á§∫ÊÑèÂõæÔºö ÂØπÊùÉÈáç WNÔºöÂ∞ÜÊùÉÈáçÂàÜËß£‰∏∫Âçï‰ΩçÂêëÈáèÂíå‰∏Ä‰∏™Âõ∫ÂÆöÊ†áÈáèÔºåÁõ∏ÂΩì‰∫éÁ•ûÁªèÂÖÉÁöÑ‰ªªÊÑèËæìÂÖ•vecÁÇπ‰πò‰∫Ü‰∏Ä‰∏™Âçï‰ΩçvecÔºàdownscaleÔºâÔºåÂÜçrescaleÔºåËøõ‰∏ÄÊ≠•Âú∞Áõ∏ÂΩì‰∫éÊ≤°ÊúâÂÅöshiftÂíåreshiftÁöÑÊï∞ÊçÆnormalization WSÔºöÂØπÊùÉÈáçÂÅöÂÖ®Â•óÔºàÂΩí‰∏ÄÂåñÂÜçrecaleÔºâÔºåÊØîWNÂ§ö‰∫ÜshiftÔºå‚Äúzero-center is the key‚Äù ÂØπop CosNÔºö Â∞ÜÁ∫øÊÄßÂèòÊç¢opÊõøÊç¢Êàêcos opÔºö$f_w(x) = cos = \frac{w \cdot x}{|w||x|}$ Êï∞Â≠¶Êú¨Ë¥®‰∏äÂèàÈÄÄÂåñÊàê‰∫ÜÂè™ÊúâdownscaleÁöÑÂèòÊç¢ÔºåË°®ÂæÅËÉΩÂäõ‰∏çË∂≥ WhiteningÁôΩÂåñ purpose imagesÁöÑadjacent pixel values are highly correlatedÔºåthus redundant linearly move the origin distributionÔºåmaking the inputs share the same mean &amp; variance method È¶ñÂÖàËøõË°åPCAÈ¢ÑÂ§ÑÁêÜÔºåÂéªÊéâcorrelation mean on sampleÔºàÊ≥®ÊÑè‰∏çÊòØmean on imageÔºâ \overline x = \frac{1}{N}\sum_{i=1}^N x_i\\ x^{'} = x - \overline x ÂçèÊñπÂ∑ÆÁü©Èòµ X \in R^{d*N}\\ S = \frac{1}{N}XX^T Â•áÂºÇÂÄºÂàÜËß£svd(S) S = U \Sigma V^T $\Sigma$‰∏∫ÂØπËßíÁü©ÈòµÔºåÂØπËßí‰∏äÁöÑÂÖÉÁ¥†‰∏∫Â•áÂºÇÂÄº $U=[u_1,u_2,‚Ä¶u_N]$‰∏≠ÊòØÂ•áÂºÇÂÄºÂØπÂ∫îÁöÑÊ≠£‰∫§ÂêëÈáè ÊäïÂΩ±ÂèòÊç¢ X^{'} = U_p^T X ÂèñÊäïÂΩ±Áü©Èòµ$U_p$ from $U$Ôºå$U_p \in R^{N*d}$Ë°®Á§∫Â∞ÜÊï∞ÊçÆÁ©∫Èó¥‰ªéNÁª¥ÊäïÂΩ±Âà∞$U_p$ÊâÄÂú®ÁöÑdÁª¥Á©∫Èó¥‰∏ä recoverÔºàÊäïÂΩ±ÈÄÜÂèòÊç¢Ôºâ X^{''} = U_p^T X^{'} * ÂèñÊäïÂΩ±Áü©Èòµ$U_r=U_p^T$ÔºåÂ∞±ÊòØÂ∞Ü Êï∞ÊçÆÁ©∫Èó¥‰ªédÁª¥Á©∫Èó¥ÂÜçÊäïÂΩ±ÂõûNÁª¥Á©∫Èó¥‰∏ä * PCAÁôΩÂåñÔºö * ÂØπPCAÊäïÂΩ±ÂêéÁöÑÊñ∞ÂùêÊ†áÔºåÂÅöÂΩí‰∏ÄÂåñÂ§ÑÁêÜÔºöÂü∫‰∫éÁâπÂæÅÂÄºËøõË°åÁº©Êîæ $$ X_{PCAwhite} = \Sigma^{-\frac{1}{2}}X^{&#39;} = \Sigma^{-\frac{1}{2}}U^TX $$ * $X_{PCAwhite}$ÁöÑÂçèÊñπÂ∑ÆÁü©Èòµ$S_{PCAwhite} = I$ÔºåÂõ†Ê≠§ÊòØÂéª‰∫ÜcorrelationÁöÑ * ZCAÁôΩÂåñÔºöÂú®‰∏ä‰∏ÄÊ≠•ÂÅöÂÆå‰πãÂêéÔºåÂÜçÊääÂÆÉÂèòÊç¢Âà∞ÂéüÂßãÁ©∫Èó¥ÔºåÊâÄ‰ª•ZCAÁôΩÂåñÂêéÁöÑÁâπÂæÅÂõæÊõ¥Êé•ËøëÂéüÂßãÊï∞ÊçÆ * ÂØπPCAÁôΩÂåñÂêéÁöÑÊï∞ÊçÆÔºåÂÜçÂÅö‰∏ÄÊ≠•recover $$ X_{ZCAwhite} = U X_{PCAwhite} $$ * ÂçèÊñπÂ∑ÆÁü©Èòµ‰ªçÊóßÊòØIÔºåÂêàÊ≥ïÁôΩÂåñ Layer Normalization Âä®Êú∫ BN reduces training time compute by each neuron require moving average depend on mini-batch size how to apply to recurrent neural nets propose layer norm [unlike BN] compute by each layer [like BN] with adaptive bias &amp; gain [unlike BN] perform the same computation at training &amp; test time [unlike BN] straightforward to apply to recurrent nets work well for RNNs ËÆ∫ÁÇπ BN reduce training time &amp; serves as regularizer require moving averageÔºöintroduce dependencies between training cases the approxmation of mean &amp; variance expectations constraints on the size of a mini-batch intuition norm layerÊèêÂçáËÆ≠ÁªÉÈÄüÂ∫¶ÁöÑÊ†∏ÂøÉÊòØÈôêÂà∂Á•ûÁªèÂÖÉËæìÂÖ•ËæìÂá∫ÁöÑÂèòÂåñÂπÖÂ∫¶ÔºåÁ®≥ÂÆöÊ¢ØÂ∫¶ Âè™Ë¶ÅÊéßÂà∂Êï∞ÊçÆÂàÜÂ∏ÉÔºåÂ∞±ËÉΩ‰øùÊåÅËÆ≠ÁªÉÈÄüÂ∫¶ ÊñπÊ≥ï compute over all hidden units in the same layer different training cases have different normalization terms Ê≤°Âï•Â•ΩËØ¥ÁöÑÔºåÂ∞±ÊòØÂú®channelÁª¥Â∫¶ËÆ°ÁÆónorm furtherÁöÑGNÊäächannelÁª¥Â∫¶ÂàÜÁªÑÂÅönormÔºåINÂú®Áõ¥Êé•ÊØè‰∏™ÁâπÂæÅËÆ°ÁÆónorm gain &amp; bias ‰πüÊòØÂú®ÂØπÂ∫îÁª¥Â∫¶Ôºö(hwd)c-dim https://tobiaslee.top/2019/11/21/understanding-layernorm/ ÂêéÁª≠ÊúâÂÆûÈ™åÂèëÁé∞ÔºåÂéªÊéâ‰∏§‰∏™learnable rescale paramsÂèçËÄåÊèêÁÇπ ËÄÉËôëÊòØÂú®training set‰∏äÁöÑËøáÊãüÂêà ÂÆûÈ™å RNN‰∏äÊúâÁî® CNN‰∏äÊØîÊ≤°Êúânorm layerÂ•ΩÔºå‰ΩÜÊòØÊ≤°ÊúâBNÂ•ΩÔºöÂõ†‰∏∫channelÊòØÁâπÂæÅÁª¥Â∫¶ÔºåÁâπÂæÅÁª¥Â∫¶‰πãÈó¥ÊúâÊòéÊòæÁöÑÊúâÁî®/Ê≤°Áî®Ôºå‰∏çËÉΩÁÆÄÂçïÁöÑnorm Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks Âä®Êú∫ reparameterizing the weights decouple length &amp; direction no dependency between samples which suits well for recurrent reinforcement generative no additional memory and computation testified on MLP with CIFAR generative model VAE &amp; DRAW reinforcement DQN ËÆ∫ÁÇπ a neuronÔºö get inputs from former layers(neurons) weighted sum over the inputs add a bias elementwise nonlinear transformation batch outputsÔºöone value per sample intuition of normalizationÔºö give gradients that are more like whitened natural gradients BNÔºömake the outputs of each neuronÊúç‰ªéstd norm our WNÔºö inspired by BN does not share BN‚Äôs across-sample property no addition memory and tiny addition computation Instance Normalization: The Missing Ingredient for Fast Stylization Âä®Êú∫ stylizationÔºöÈíàÂØπÈ£éÊ†ºËøÅÁßªÁΩëÁªú with a small changeÔºöswapping BN with IN achieve qualitative improvement ËÆ∫ÁÇπ stylized image a content image + a style image both style and content statistics are obtained from a pretrained CNN for image classification methods optimization-basedÔºöiterative thus computationally inefficient generator-basedÔºösingle pass but never as good as our work revisit the feed-forward method replace BN in the generator with IN keep them at test time as opposed to freeze ÊñπÊ≥ï formulation given a fixed stype image $x_0$ given a set of content images $x_t, t= 1,2,‚Ä¶,n$ given a pre-trained CNN with a variable z controlling the generation of stylization results compute the stylied image g($x_t$, z) compare the statisticsÔºö$min_g \frac{1}{n} \sum^n_{t=1} L(x_0, x_t, g(x_t, z))$ comparing targetÔºöthe contrast of the stylized image is similar to the constrast of the style image observations the more training examples, the poorer the qualitive results the result of stylization still depent on the constrast of the content image intuition È£éÊ†ºËøÅÁßªÊú¨Ë¥®‰∏äÂ∞±ÊòØÂ∞Üstyle imageÁöÑcontrastÁî®Âú®content imageÁöÑÔºö‰πüÂ∞±ÊòØrescale content imageÁöÑcontrast constrastÊòØper sampleÁöÑÔºö$\frac{pixel}{\sum pixels\ on\ the\ map}$ BNÂú®normÁöÑÊó∂ÂÄôÂ∞Übatch samplesÊêÖÂêàÂú®‰∫Ü‰∏ÄËµ∑ IN instance-specfic normalization also known as contrast normalization Â∞±ÊòØper imageÂÅöÊ†áÂáÜÂåñÔºåÊ≤°Êúâtrainable/frozen paramsÔºåÂú®test phase‰πü‰∏ÄÊ†∑Áî® Group Normalization Âä®Êú∫ for small batch size do normalization in channel groups batch-independent behaves stably over different batch sizes approach BN‚Äôs accuracy ËÆ∫ÁÇπ BN requires sufficiently large batch size (e.g. 32) Mask R-CNN frameworks use a batch size of 1 or 2 images because of higher resolution, where BN is ‚Äúfrozen‚Äù by transforming to a linear layer synchronized BN „ÄÅBR LN &amp; IN effective for training sequential models or generative models but have limited success in visual recognition GNËÉΩËΩ¨Êç¢ÊàêLNÔºèIN WN normalize the filter weights, instead of operating on features ÊñπÊ≥ï group it is not necessary to think of deep neural network features as unstructured vectors Á¨¨‰∏ÄÂ±ÇÂç∑ÁßØÊ†∏ÈÄöÂ∏∏Â≠òÂú®‰∏ÄÁªÑÂØπÁß∞ÁöÑfilterÔºåËøôÊ†∑Â∞±ËÉΩÊçïËé∑Âà∞Áõ∏‰ººÁâπÂæÅ Ëøô‰∫õÁâπÂæÅÂØπÂ∫îÁöÑchannel can be normalized together normalization transform the feature xÔºö$\hat x_i = \frac{1}{\sigma}(x_i-\mu_i)$ the mean and the standard deviationÔºö \mu_i=\frac{1}{m}\sum_{k\in S_i}x_k\\ \sigma_i=\sqrt {\frac{1}{m}\sum_{k\in S_i}(x_k-\mu_i)^2+\epsilon} the set $S_i$ BNÔºö $S_i=\{k|k_C = i_C\}$ pixels sharing the same channel index are normalized together for each channel, BN computes Œº and œÉ along the (N, H, W) axes LN $S_i=\{k|k_N = i_N\}$ pixels sharing the same batch index (per sample) are normalized together LN computes Œº and œÉ along the (C,H,W) axes for each sample IN $S_i=\{k|k_N = i_N, k_C=i_C\}$ pixels sharing the same batch index and the same channel index are normalized together LN computes Œº and œÉ along the (H,W) axes for each sample GN $S_i=\{k|k_N = i_N, [\frac{k_C}{C/G}]=[\frac{i_C}{C/G}]\}$ computes Œº and œÉ along the (H, W ) axes and along a group of C/G channels linear transform to keep representational ability per channel scale and shiftÔºö$y_i = \gamma \hat x_i + \beta$ relation to LN LN assumes all channels in a layer make ‚Äúsimilar contributions‚Äù which is less valid with the presence of convolutions GN improved representational power over LN to IN IN can only rely on the spatial dimension for computing the mean and variance it misses the opportunity of exploiting the channel dependence „ÄêQUESTION„ÄëBN‰πüÊ≤°ËÄÉËôëÈÄöÈÅìÈó¥ÁöÑËÅîÁ≥ªÂïäÔºå‰ΩÜÊòØËÆ°ÁÆómeanÂíåvarianceÊó∂Ë∑®‰∫Üsample implementation reshape learnable $\gamma \&amp; \beta$ computable mean &amp; var ÂÆûÈ™å GNÁõ∏ÊØî‰∫éBNÔºåtraining errorÊõ¥‰ΩéÔºå‰ΩÜÊòØval errorÁï•È´ò‰∫éBN GN is effective for easing optimization loses some regularization ability it is possible that GN combined with a suitable regularizer will improve results ÈÄâÂèñ‰∏çÂêåÁöÑgroupÊï∞ÔºåÊâÄÊúâÁöÑgroup&gt;1ÂùáÂ•Ω‰∫égroup=1ÔºàLNÔºâ ÈÄâÂèñ‰∏çÂêåÁöÑchannelÊï∞ÔºàCÔºèGÔºâÔºåÊâÄÊúâÁöÑchannel&gt;1ÂùáÂ•Ω‰∫échannel=1ÔºàINÔºâ Object Detection frozenÔºöÂõ†‰∏∫higher resolutionÔºåbatch sizeÈÄöÂ∏∏ËÆæÁΩÆ‰∏∫2/GPUÔºåËøôÊó∂ÁöÑBN frozenÊàê‰∏Ä‰∏™Á∫øÊÄßÂ±Ç$y=\gamma(x-\mu)/\sigma+beta$ÔºåÂÖ∂‰∏≠ÁöÑ$\mu$Âíå$sigma$ÊòØload‰∫Üpre-trained model‰∏≠‰øùÂ≠òÁöÑÂÄºÔºåÂπ∂‰∏îfrozenÊéâÔºå‰∏çÂÜçÊõ¥Êñ∞ denote as BN* replace BN* with GN during fine-tuning use a weight decay of 0 for the Œ≥ and Œ≤ parameters WS: Weight Standardization Âä®Êú∫ accelerate training micro-batchÔºö ‰ª•BN with large-batch‰∏∫Âü∫ÂáÜ ÁõÆÂâçBN with micro-batchÂèäÂÖ∂‰ªñnormalization methodsÈÉΩ‰∏çËÉΩmatchËøô‰∏™baseline operates on weights instead of activations ÊïàÊûú match or outperform BN smooth the loss ËÆ∫ÁÇπ two facts BNÁöÑperformance gain‰∏éreduction of internal covariate shiftÊ≤°‰ªÄ‰πàÂÖ≥Á≥ª BN‰ΩøÂæóoptimization landscape significantly smoother Âõ†Ê≠§our target is to find another technique achieves smooth landscape work with micro-batch normalization methods focus on activations ‰∏çÂ±ïÂºÄ focus on weights WNÔºöjust length-direction decoupling ÊñπÊ≥ï Lipschitz constants BN reduces the Lipschitz constants of the loss function makes the gradient more Lipschitz BN considers the Lipschitz constants with respect to activationsÔºånot the weights that the optimizer is directly optimizing our inspiration standardize the weights‰πüÂêåÊ†∑ËÉΩÂ§üsmooth the landscape Êõ¥Áõ¥Êé• smoothing effects on activations and weightsÊòØÂèØ‰ª•Á¥ØÁßØÁöÑÔºåÂõ†‰∏∫ÊòØÁ∫øÊÄßËøêÁÆó Weight Standardization reparameterize the original weights $W$ ÂØπÂç∑ÁßØÂ±ÇÁöÑÊùÉÈáçÂèÇÊï∞ÂÅöÂèòÊç¢Ôºåno bias $W \in R^{O * I}$ $O=C_{out}$ $I=C_{in}*kernel_size$ optimize the loss on $\hat W$ compute mean &amp; var on I-dim Âè™ÂÅöÊ†áÂáÜÂåñÔºåÊó†ÈúÄaffineÔºåÂõ†‰∏∫ÈªòËÆ§ÂêéÁª≠ËøòË¶ÅÊé•‰∏Ä‰∏™normalization layerÂØπÁ•ûÁªèÂÖÉËøõË°årefine WS normalizes gradients ÊãÜËß£Ôºö eq5Ôºö$W$ to $\dot W$ÔºåÂáèÂùáÂÄºÔºåzero-centered eq6Ôºö$\dot W$ to $\hat W$ÔºåÈô§ÊñπÂ∑ÆÔºåone-varianced eq8Ôºö$\delta \hat W$Áî±Ââç‰∏ÄÊ≠•ÁöÑÊ¢ØÂ∫¶normalizeÂæóÂà∞ eq9Ôºö$\delta \dot W$‰πüÁî±Ââç‰∏ÄÊ≠•ÁöÑÊ¢ØÂ∫¶normalize ÊúÄÁªàÁî®‰∫éÊ¢ØÂ∫¶Êõ¥Êñ∞ÁöÑÊ¢ØÂ∫¶ÊòØzero-centered WS smooths landscape Âà§ÂÆöÊòØÂê¶smoothÂ∞±ÁúãLipschitz constantÁöÑÂ§ßÂ∞è eq5Âíåeq6ÈÉΩËÉΩreduce the Lipschitz constant ÂÖ∂‰∏≠eq5 makes the major improvements eq6 slightly improvesÔºåÂõ†‰∏∫ËÆ°ÁÆóÈáè‰∏çÂ§ßÔºåÊâÄ‰ª•‰øùÁïô ÂÆûÈ™å ImageNet BNÁöÑbatchsizeÊòØ64ÔºåÂÖ∂‰ΩôÈÉΩÊòØ1ÔºåÂÖ∂‰ΩôÁöÑÊ¢ØÂ∫¶Êõ¥Êñ∞iterationsÊîπÊàê64‚Äî‚Äî‰ΩøÂæóÂèÇÊï∞Êõ¥Êñ∞Ê¨°Êï∞ÂêåÊ≠• ÊâÄÊúâÁöÑnormalization methodsÂä†‰∏äWSÈÉΩÊúâÊèêÂçá Ë£∏ÁöÑnormalization methodsÈáåÈù¢batchsize1ÁöÑGNÊúÄÂ•ΩÔºåÊâÄ‰ª•ÈÄâÁî®GN+WSÂÅöËøõ‰∏ÄÊ≠•ÂÆûÈ™å GN+WS+AFÔºöÂä†‰∏äconv weightÁöÑaffine‰ºöharm code 123456# official release# ÊîæÂú®WSConv2DÂ≠êÁ±ªÁöÑcallÈáåÈù¢kernel_mean = tf.math.reduce_mean(kernel, axis=[0, 1, 2], keepdims=True, name='kernel_mean')kernel = kernel - kernel_meankernel_std = tf.keras.backend.std(kernel, axis=[0, 1, 2], keepdims=True)kernel = kernel / (kernel_std + 1e-5)]]></content>
      <tags>
        <tag>CNN, layer, normalization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NFNet]]></title>
    <url>%2F2021%2F02%2F22%2FNFNet%2F</url>
    <content type="text"><![CDATA[NFNet: High-Performance Large-Scale Image Recognition Without Normalization Âä®Êú∫ NFÔºö normalization-free aims to match the test acc of batch-normalized networks attain new SOTA 86.5% pre-training + fine-tuning‰∏ä‰πüË°®Áé∞Êõ¥Â•Ω89.2% batch normalization ‰∏çÊòØÂÆåÁæéËß£ÂÜ≥ÊñπÊ°à depends on batch size non-normalized networks accuracy instabilitiesÔºödevelop adaptive gradient clipping ËÆ∫ÁÇπ vast majority models variants of deep residual + BN allow deeper, stable and regularizing disadvantages of batch normalization computational expensive introduces discrepancy between training &amp; testing models &amp; increase params breaks the independence among samples methods seeks to replace BN alternative normalizers study the origin benefits of BN train deep ResNets without normalization layers key theme when removing normalization suppress the scale of the residual branch simplest wayÔºöapply a learnable scalar recent workÔºösuppress the branch at initialization &amp; apply Scaled Weight StandardizationÔºåËÉΩËøΩ‰∏äResNetÂÆ∂ÊóèÔºå‰ΩÜÊòØÊ≤°ËøΩ‰∏äEffÂÆ∂Êóè our NFNets‚Äô main contributions propose AGCÔºöËß£ÂÜ≥unstableÈóÆÈ¢òÔºåallow larger batch size and stronger augmentatons NFNetsÂÆ∂ÊóèÂà∑Êñ∞SOTAÔºöÂèàÂø´ÂèàÂáÜ pretraining + finetuningÁöÑÊàêÁª©‰πüÊØîbatch normed modelsÂ•Ω ÊñπÊ≥ï Understanding Batch Normalization four main benefits downscale the residual branchÔºö‰ªéinitializationÂ∞±‰øùËØÅÊÆãÂ∑ÆÂàÜÊîØÁöÑscaleÊØîËæÉÂ∞èÔºå‰ΩøÂæóÁΩëÁªúhas well-behaved gradients early in trainingÔºå‰ªéËÄåefficient optimization eliminates mean-shiftÔºöReLUÊòØ‰∏çÂØπÁß∞ÁöÑÔºåstacking layers‰ª•ÂêéÊï∞ÊçÆÂàÜÂ∏É‰ºöÁ¥ØÁßØÂÅèÁßª regularizing effectÔºömini-batch‰Ωú‰∏∫subsetÂØπ‰∫éÂÖ®ÈõÜÊòØÊúâÂÅèÁöÑÔºåËøôÁßçnoiseÂèØ‰ª•Áúã‰ΩúÊòØregularizer allows efficient large-batch trainingÔºöÊï∞ÊçÆÂàÜÂ∏ÉÁ®≥ÂÆöÊâÄ‰ª•lossÂèòÂåñÁ®≥ÂÆöÔºåÂêåÊó∂Â§ßbatchÊõ¥Êé•ËøëÁúüÂÆûÂàÜÂ∏ÉÔºåÂõ†Ê≠§Êàë‰ª¨ÂèØ‰ª•‰ΩøÁî®Êõ¥Â§ßÁöÑlearning rateÔºå‰ΩÜÊòØËøô‰∏™property‰ªÖÂú®‰ΩøÁî®Â§ßbatch sizeÁöÑÊó∂ÂÄôÊúâÊïà NF-ResNets recovering the benefits of BNÔºöÂØπresidual branchËøõË°åscaleÂíåmean-shift residual blockÔºö$h_{i+1} = h_i + \alpha f_i (h_i/\beta_i)$ $\beta_i = Var(h_i)$ÔºöÂØπËæìÂÖ•ËøõË°åÊ†áÂáÜÂåñÔºàÊñπÂ∑Æ‰∏∫1ÔºâÔºåËøôÊòØ‰∏™expected valueÔºå‰∏çÊòØÁÆóÂá∫Êù•ÁöÑÔºåÁªìÊûÑÂÆöÊ≠ªÂ∞±ÂÆöÊ≠ª‰∫Ü Scaled Weight Standardization &amp; scaled activation ÊØîÂéüÁâàÁöÑWSÂ§ö‰∫Ü‰∏Ä‰∏™$\sqrt N$ÁöÑÂàÜÊØç Ê∫êÁ†ÅÂÆûÁé∞‰∏≠ÊØîÂéüÁâàWSËøòÂ§ö‰∫Ülearnable affine gain ‰ΩøÂæóconv-relu‰ª•ÂêéËæìÂá∫ËøòÊòØÊ†áÂáÜÂàÜÂ∏É $\alpha=0.2$Ôºörescale residual branch‰∏äÔºåÊúÄÁªàÁöÑËæìÂá∫‰∏∫$\alpha*$Ê†áÂáÜÂàÜÂ∏ÉÔºåÊñπÂ∑ÆÊòØ$\alpha^2$ id path‰∏äÔºåËæìÂá∫ËøòÊòØ$h_{i}$ÔºåÊñπÂ∑ÆÊòØ$Var(h_i)$ updateËøô‰∏™blockËæìÂá∫ÁöÑÊñπÂ∑Æ‰∏∫$Var(h_{i+1}) = Var(h_i)+\alpha^2$ÔºåÊù•Êõ¥Êñ∞‰∏ã‰∏Ä‰∏™blockÁöÑ $\beta$ variance reset ÊØè‰∏™transition block‰ª•ÂêéÔºåÊäävarianceÈáçÊñ∞ËÆæÂÆö‰∏∫$1+\alpha^2$ Âú®Êé•‰∏ãÊù•ÁöÑnon-transition block‰∏≠ÔºåÁî®‰∏äÈù¢ÁöÑupdateÂÖ¨ÂºèÊõ¥Êñ∞expected std ÂÜçÂä†‰∏äadditional regularizationÔºàDropoutÂíåStochastic Depth‰∏§ÁßçÊ≠£ÂàôÊâãÊÆµÔºâÔºåÂ∞±Êª°Ë∂≥‰∫ÜBN benefitsÁöÑÂâç‰∏âÊù° Âú®batch sizeËæÉÂ∞èÁöÑÊó∂ÂÄôËÉΩÂ§ücatch upÁîöËá≥Ë∂ÖË∂äbatch normalized models ‰ΩÜÊòØlarge batch sizeÁöÑÊó∂ÂÄôperform worse ÂØπ‰∫é‰∏Ä‰∏™Ê†áÂáÜÁöÑconv-bn-reluÔºå‰ªéworkflow‰∏äÁúã originÔºöinput‚Äî‚Äî‰∏Ä‰∏™freeÁöÑconv weighting‚Äî‚ÄîBNÔºànorm &amp; rescaleÔºâ‚Äî‚Äîactivation NFNetÔºöinput‚Äî‚Äîstandard norm‚Äî‚Äînormed weighting &amp; activation‚Äî‚Äîrescale Adaptive Gradient Clipping for Efficient Large-Batch Training Ê¢ØÂ∫¶Ë£ÅÂâ™Ôºö clip by normÔºöÁî®‰∏Ä‰∏™clipping threshold $\lambda$ ËøõË°årescaleÔºåtraining stability was extremely sensitive to Ë∂ÖÂèÇÁöÑÈÄâÊã©ÔºåsettingsÔºàmodel depth, the batch size, or the learning rateÔºâ‰∏ÄÂèòË∂ÖÂèÇÂ∞±Ë¶ÅÈáçÊñ∞Ë∞É clip by valueÔºöÁî®‰∏Ä‰∏™clipping valueËøõË°å‰∏ä‰∏ãÈôêÊà™Êñ≠ AGC given ÊüêÂ±ÇÁöÑÊùÉÈáç$W \in R^{NM}$ Âíå ÂØπÂ∫îÊ¢ØÂ∫¶$G \in R^{NM}$ ratio $\frac{||G||_F}{||W||_F}$ ÂèØ‰ª•Áúã‰ΩúÊòØÊ¢ØÂ∫¶ÂèòÂåñÂ§ßÂ∞èÁöÑmeasurement ÊâÄ‰ª•Êàë‰ª¨Áõ¥ËßÇÂú∞ÊÉ≥Âà∞Â∞ÜËøô‰∏™ratioËøõË°åÈôêÂπÖÔºöÊâÄË∞ìÁöÑadaptiveÂ∞±ÊòØÂú®Ê¢ØÂ∫¶Ë£ÅÂâ™ÁöÑÊó∂ÂÄô‰∏çÊòØÂØπÊâÄÊúâÊ¢ØÂ∫¶‰∏ÄÂàÄÂàáÔºåËÄåÊòØËÄÉËôëÂÖ∂ÂØπÂ∫îÊùÉÈáçÂ§ßÂ∞èÔºå‰ªéËÄåËøõË°åÊõ¥ÂêàÁêÜÁöÑË∞ÉËäÇ ‰ΩÜÊòØÂÆûÈ™å‰∏≠ÂèëÁé∞unit-wiseÁöÑgradient normË¶ÅÊØîlayer-wiseÁöÑÂ•ΩÔºöÊØè‰∏™unitÂ∞±ÊòØÊØèË°åÔºåÂØπ‰∫éconv weightsÂ∞±ÊòØ(hxwxCin)‰∏≠ÁöÑ‰∏Ä‰∏™ scalar hyperparameter $\lambda$ * the optimal value may depend on the choice of optimizer, learning rate and batch size * empirically we found $\lambda$ should be smaller for larger batches ablations for AGC Áî®pre-activation NF-ResNet-50 Âíå NF-ResNet-200 ÂÅöÂÆûÈ™åÔºåbatch sizeÈÄâÊã©‰ªé256Âà∞4096ÔºåÂ≠¶‰π†Áéá‰ªé0.1ÂºÄÂßãÂü∫‰∫ébatch sizeÁ∫øÊÄßÂ¢ûÈïøÔºåË∂ÖÂèÇ$\lambda$ÁöÑÂèñÂÄºËßÅÂè≥Âõæ Â∑¶ÂõæÁªìËÆ∫1ÔºöÂú®batch sizeËæÉÂ∞èÁöÑÊÉÖÂÜµ‰∏ãÔºåNF-NetsËÉΩÂ§üËøΩ‰∏äÁîöËá≥Ë∂ÖË∂änormed modelsÁöÑÁ≤æÂ∫¶Ôºå‰ΩÜÊòØbatch size‰∏ÄÂ§ßÔºà2048ÔºâÊÉÖÂÜµÂ∞±ÊÅ∂Âåñ‰∫ÜÔºå‰ΩÜÊòØÊúâAGCÁöÑNF-NetsÂàôËÉΩÂ§ümaintaining performance comparable or better thanÔΩûÔΩûÔΩû Â∑¶ÂõæÁªìËÆ∫2Ôºöthe benefits of using AGC are smaller when the batch size is small Âè≥ÂõæÁªìËÆ∫1ÔºöË∂ÖÂèÇ$\lambda$ÁöÑÂèñÂÄºÊØîËæÉÂ∞èÁöÑÊó∂ÂÄôÔºåÊàë‰ª¨ÂØπÊ¢ØÂ∫¶ÁöÑclippingÊõ¥strongÔºåËøôÂØπ‰∫é‰ΩøÁî®Â§ßbatch sizeËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄßÊù•ËØ¥ÈùûÂ∏∏ÈáçË¶Å whether or not AGC is beneficial for all layers * it is always better to not clip the final linear layer * ÊúÄÂºÄÂßãÁöÑÂç∑ÁßØ‰∏çÂÅöÊ¢ØÂ∫¶Ë£ÅÂâ™‰πüËÉΩÁ®≥ÂÆöËÆ≠ÁªÉ ÊúÄÁªàwe apply AGC to every layer except for the final linear layer Normalizer-Free Architectures begin with SE-ResNeXt-D model about group width * set group width to 128 the reduction in compute density means that Âè™ÂáèÂ∞ë‰∫ÜÁêÜËÆ∫‰∏äÁöÑFLOPsÔºåÊ≤°ÊúâÂÆûÈôÖÂä†ÈÄü about stages * RÁ≥ªÂàóÊ®°ÂûãÂä†Ê∑±ÁöÑÊó∂ÂÄôÊòØÈùûÁ∫øÊÄßÂ¢ûÈïøÔºåÁñØÁãÇÂè†Âä†stage3ÁöÑblockÊï∞ÔºåÂõ†‰∏∫Ëøô‰∏ÄÂ±Çresolution‰∏çÂ§ßÔºåchannel‰πü‰∏çÊòØÊúÄÂ§öÔºåÂÖºÈ°æ‰∫Ü‰∏§‰æßËÆ°ÁÆóÈáè Êàë‰ª¨ÁªôF0ËÆæÁΩÆ‰∏∫[1,2,6,3]ÔºåÁÑ∂ÂêéÂú®deeper variants‰∏≠ÂØπÊØè‰∏™stageÁöÑblockÊï∞Áî®‰∏Ä‰∏™scalar NÁ∫øÂΩ¢Â¢ûÈïø about width * ‰ªçÊóßÂØπstage3‰∏ãÊâãÔºå[256,512,1536,1536] * roughly preserves the training speed ‰∏Ä‰∏™ËÆ∫ÁÇπÔºöstage3 is the best place to add capacityÔºåÂõ†‰∏∫deeper enoughÂêåÊó∂have access to deeper levelsÂêåÊó∂ÂèàÊØîÊúÄÂêé‰∏ÄÂ±ÇÊúâslightly higher resolution about block * ÂÆûÈ™åÂèëÁé∞ÊúÄÊúâÁî®ÁöÑÊìç‰ΩúÊòØadding an additional 3 √ó 3 grouped conv after the first * overview about scaling variants * effÁ≥ªÂàóÈááÁî®ÁöÑÊòØR„ÄÅW„ÄÅD‰∏ÄËµ∑Â¢ûÈïøÔºåÂõ†‰∏∫effÁöÑblockÊØîËæÉËΩªÈáè ‰ΩÜÊòØÂØπRÁ≥ªÂàóÊù•ËØ¥ÔºåÂè™Â¢ûÈïøDÂíåRÂ∞±Â§ü‰∫Ü Ë°•ÂÖÖÁªÜËäÇ * Âú®inferenceÈò∂ÊÆµ‰ΩøÁî®ÊØîËÆ≠ÁªÉÈò∂ÊÆµslightly higher resolution ÈöèÁùÄÊ®°ÂûãÂä†Â§ßincrease the regularization strengthÔºö scale the drop rate of Dropout Ë∞ÉÊï¥stochastic depth rateÂíåweight decayÂàônot effective se-blockÁöÑscale‰πò‰∏™2 SGD params: Nesterov=True, momentum=0.9, clipnorm=0.01 lrÔºö ÂÖàwarmupÂÜç‰ΩôÂº¶ÈÄÄÁÅ´Ôºöincrease from 0 to 1.6 over 5 epochs, then decay to zero with cosine annealing ‰ΩôÂº¶ÈÄÄÁÅ´cosine annealing summary ÊÄªÁªìÊù•ËØ¥ÔºåÂ∞±ÊòØÊãøÊù•‰∏Ä‰∏™SE-ResNeXt-D ÂÖàÂÅöÁªìÊûÑ‰∏äÁöÑË∞ÉÊï¥Ôºåmodified width and depth patterns‰ª•Âèäa second spatial convolutionÔºåËøòÊúâdrop rateÔºåresolution ÂÜçÂÅöÂØπÊ¢ØÂ∫¶ÁöÑË∞ÉÊï¥ÔºöÈô§‰∫ÜÊúÄÂêé‰∏Ä‰∏™Á∫øÂΩ¢ÂàÜÁ±ªÂ±Ç‰ª•Â§ñÔºåÂÖ®Áî®AGCÔºå$\lambda=0.01$ ÊúÄÂêéÊòØËÆ≠ÁªÉ‰∏äÁöÑtrickÔºöstrong regularization and data augmentation detailed view of NFBlocks transition blockÔºöÊúâ‰∏ãÈááÊ†∑ÁöÑblock ÊÆãÂ∑Æbranch‰∏äÔºåbottleneckÁöÑnarrow ratioÊòØ0.5 ÊØè‰∏™stageÁöÑ3x3 convÁöÑgroup widthÊ∞∏ËøúÊòØ128ÔºåËÄågroupÊï∞ÁõÆÊòØÂú®ÈöèÁùÄblock widthÂèòÁöÑ skip pathÊé•Âú® $\beta$ downscaling ‰πãÂêé skip path‰∏äÊòØavg pooling + 1x1 conv non-transition blockÔºöÊó†‰∏ãÈááÊ†∑ÁöÑblock bottleneck-ratio‰ªçÊóßÊòØ0.5 3x3convÁöÑgroup width‰ªçÊóßÊòØ128 skip pathÊé•Âú®$\beta$ downscaling ‰πãÂâç skip pathÂ∞±ÊòØid ÂÆûÈ™å]]></content>
      <tags>
        <tag>CNN, classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[repVGG]]></title>
    <url>%2F2021%2F02%2F09%2FrepVGG%2F</url>
    <content type="text"><![CDATA[RepVGG: Making VGG-style ConvNets Great Again Âä®Êú∫ plain ConvNets simply efficient but poor performance propose a CNN architecture RepVGG ËÉΩÂ§üdecouple‰∏∫training-timeÂíåinference-time‰∏§‰∏™ÁªìÊûÑ ÈÄöËøástructure re-paramterization technique inference-time architecture has a VGG-like plain body faster 83% faster than ResNet-50 or 101% faster than ResNet-101 accuracy-speed trade-off reaches over 80% top-1 accuracy outperforms ResNets by a large margin verify on classification &amp; semantic segmentation tasks ËÆ∫ÁÇπ well-designed CNN architectures InceptionÔºåResNetÔºåDenseNetÔºåNAS models deliver higher accuracy drawbacks multi-branch designsÔºöslow down inference and reduce memory utilizationÔºåÂØπÈ´òÂπ∂Ë°åÂåñÁöÑËÆæÂ§á‰∏çÂèãÂ•Ω some componentsÔºödepthwise &amp; channel shuffleÔºåincrease memory access cost MAC(memory access cost) constitutes a large time usage in groupwise convolutionÔºöÊàëÁöÑgroupconvÂÆûÁé∞ÈáåcardinalityÁª¥Â∫¶‰∏äËÆ°ÁÆó‰∏çÂπ∂Ë°å FLOPsÂπ∂‰∏çËÉΩprecisely reflect actual speedÔºå‰∏Ä‰∫õÁªìÊûÑÁúã‰ººÊØîold fashioned VGG/resnetÁöÑFLOPsÂ∞ëÔºå‰ΩÜÂÆûÈôÖÂπ∂Ê≤°ÊúâÂø´ multi-branch ÈÄöÂ∏∏multi-branch modelË¶ÅÊØîplain modelË°®Áé∞Â•Ω Âõ†‰∏∫makes the model an implicit ensemble of numerous shallower models so that avoids gradient vanishing benefits are all for training drawbacks are undesired for inference the proposed RepVGG advantages plain architectureÔºöno branches 3x3 conv &amp; ReLUÁªÑÊàê Ê≤°ÊúâËøáÈáçÁöÑ‰∫∫Â∑•ËÆæËÆ°ÁóïËøπ training time use identity &amp; 1x1 conv branches at inference time identity ÂèØ‰ª•ÁúãÂÅödegraded 1x1 conv 1x1 conv ÂèØ‰ª•ÁúãÂÅödegraded 3x3 conv ÊúÄÁªàÊï¥‰∏™conv-bn branchesËÉΩÂ§üÊï¥ÂêàÊàê‰∏Ä‰∏™3x3 conv inference-time modelÂè™ÂåÖÂê´convÂíåReLUÔºöÊ≤°Êúâmax poolingÔºÅÔºÅ fewer memory unitsÔºöÂàÜÊîØ‰ºöÂç†ÂÜÖÂ≠òÔºåÁõ¥Âà∞ÂàÜÊîØËÆ°ÁÆóÁªìÊùüÔºåplainÁªìÊûÑÁöÑmemoryÂàôÊòØimmediately released ÊñπÊ≥ï training-time ResNet-like block id + 1x1 conv + 3x3 conv multi-branches use BN in each branch with n blocks, the model can be interpreted as an ensemble of $3^n$ models stride2ÁöÑblockÂ∫îËØ•Ê≤°Êúâid pathÂêßÔºüÔºü simply stack serveral blocks to construct the training model inference-time re-param inference-time BN‰πüÊòØ‰∏Ä‰∏™Á∫øÊÄßËÆ°ÁÆó ‰∏§‰∏™1x1 convÈÉΩÂèØ‰ª•ËΩ¨Êç¢Êàê‰∏≠ÈÄöÁöÑ3x3 kernelÔºåÊúâÊùÉ/Êó†ÊùÉ Ë¶ÅÊ±ÇÂêÑbranch has the same strides &amp; padding pixelË¶ÅÂØπÈΩê architectural specification varietyÔºödepth and width does not use maxpoolingÔºöÂè™Êúâ‰∏ÄÁßçoperatorÔºö3x3 conv+relu headÔºöGAP + fc / task specific 5 stages Á¨¨‰∏Ä‰∏™stageÂ§ÑÁêÜhigh resolutionÔºåstride2 Á¨¨‰∫î‰∏™stage shall have more channelsÔºåÊâÄ‰ª•Âè™Áî®‰∏ÄÂ±ÇÔºåsave parameters ÁªôÂÄíÊï∞Á¨¨‰∫å‰∏™stageÊúÄÂ§öÂ±ÇÔºåËÄÉËôëparamsÂíåcomputationÁöÑbalance RepVGG-AÔºö[1,2,4,14,1]ÔºåÁî®Êù•compete againstËΩªÈáèÂíå‰∏≠ÈáèÁ∫ßmodel RepVGG-BÔºödeeper in s2,3,4Ôºå[1,4,6,16,1]ÔºåÁî®Êù•compete against high-performance ones basic widthÔºö[64, 128, 256, 512] width multiplier a &amp; b aÊéßÂà∂Ââç4‰∏™stageÂÆΩÂ∫¶ÔºåbÊéßÂà∂ÊúÄÂêé‰∏Ä‰∏™stage [64a, 128a, 256a, 512b] Á¨¨‰∏Ä‰∏™stageÁöÑÂÆΩÂ∫¶Âè™Êé•ÂèóÂèòÂ∞è‰∏çÊé•ÂèóÂèòÂ§ßÔºåÂõ†‰∏∫Â§ßresolutionÂΩ±ÂìçËÆ°ÁÆóÈáèÔºåmin(64,64a) further reduce params &amp; computation groupwise 3x3 conv Ë∑≥ÁùÄÂ±ÇÊç¢Ôºö‰ªéÁ¨¨‰∏âÂºÄÂßãÔºåÁ¨¨‰∏â„ÄÅÁ¨¨‰∫î„ÄÅ number of groupsÔºö1Ôºå2Ôºå4 globally ÂÆûÈ™å ÂàÜÊîØÁöÑ‰ΩúÁî® ÁªìÊûÑ‰∏äÁöÑÂæÆË∞É id pathÂéªÊéâBN ÊääÊâÄÊúâÁöÑBNÁßªÂä®Âà∞addÁöÑÂêéÈù¢ ÊØè‰∏™pathÂä†‰∏ärelu ImageNetÂàÜÁ±ª‰ªªÂä°‰∏äÂØπÊ†áÂÖ∂‰ªñÊ®°Âûã simple augmentation strongÔºöAutoaugment, label smoothing and mixup]]></content>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[transform in CNN]]></title>
    <url>%2F2021%2F02%2F03%2Ftransform-in-CNN%2F</url>
    <content type="text"><![CDATA[ÁªºËø∞ Âá†‰ΩïÂèòÊç¢ STNÔºö ÊôÆÈÄöÁöÑCNNËÉΩÂ§üÈöêÂºèÁöÑÂ≠¶‰π†‰∏ÄÂÆöÁöÑÂπ≥Áßª„ÄÅÊóãËΩ¨‰∏çÂèòÊÄßÔºåËÆ©ÁΩëÁªúËÉΩÂ§üÈÄÇÂ∫îËøôÁßçÂèòÊç¢ÔºöÈôçÈááÊ†∑ÁªìÊûÑÊú¨Ë∫´ËÉΩÂ§ü‰ΩøÂæóÁΩëÁªúÂØπÂèòÊç¢‰∏çÊïèÊÑü ‰ªéÊï∞ÊçÆËßíÂ∫¶Âá∫ÂèëÔºåÊàë‰ª¨Ëøò‰ºöÂºïÂÖ•ÂêÑÁßçaugmentationÔºåÂº∫ÂåñÁΩëÁªúÂØπÂèòÂåñÁöÑ‰∏çÂèòËÉΩÂäõ deepMind‰∏∫ÁΩëÁªúËÆæËÆ°‰∫Ü‰∏Ä‰∏™ÊòæÂºèÁöÑÂèòÊç¢Ê®°ÂùóÊù•Â≠¶‰π†ÂêÑÁßçÂèòÂåñÔºåÂ∞ÜdistortedÁöÑËæìÂÖ•ÂèòÊç¢ÂõûÂéªÔºåËÆ©ÁΩëÁªúÂ≠¶‰π†Êõ¥ÁÆÄÂçïÁöÑ‰∏úË•ø ÂèÇÊï∞ÈáèÔºöÂ∞±ÊòØÂèòÊç¢Áü©ÈòµÁöÑÂèÇÊï∞ÔºåÈÄöÂ∏∏ÊòØ2x3ÁöÑÁ∫∫Â∞ÑÂèòÂåñÁü©ÈòµÔºå‰πüÂ∞±ÊòØ6‰∏™ÂèÇÊï∞ deformable convÔºö based on STN ÈíàÂØπÂàÜÁ±ªÂíåÊ£ÄÊµãÂàÜÂà´ÊèêÂá∫deformable convolutionÂíådeformable RoI poolingÔºö ÊÑüËßâdeformable RoI poolingÂíåguiding anchorÈáåÈù¢ÁöÑfeature adaptionÊòØ‰∏Ä‰∏™‰∏úË•ø ÂèÇÊï∞ÈáèÔºöregular kernel params 3x3 + deformable offsets 3x3x2 what‚Äôs newÔºü ‰∏™‰∫∫ËÆ§‰∏∫ÂºïÂÖ•Êõ¥Â§öÁöÑÂèÇÊï∞ÂºïÂÖ•ÁöÑÂèòÂåñ È¶ñÂÖàSTNÊòØ‰ªéoutputÂà∞inputÁöÑÊò†Â∞ÑÔºå‰ΩøÁî®ÂèòÊç¢Áü©ÈòµMÈÄöÂ∏∏Âè™ËÉΩË°®Á§∫depictable transformationÔºå‰∏îÂÖ®ÂõæÂè™Êúâ1‰∏™transformation ÂÖ∂Ê¨°STNÁöÑsampling kernel‰πüÊòØÈ¢ÑÂÆö‰πâÁöÑÁÆóÊ≥ïÔºåÂØπkernelÂÜÖÁöÑÊâÄÊúâpixel‰ΩøÁî®Áõ∏ÂêåÁöÑÂèòÂåñÔºå‰πüÂ∞±ÊòØ1‰∏™weight factor deformable convÊòØ‰ªéinputÂà∞outputÁöÑÊò†Â∞ÑÔºåÊò†Â∞ÑÂèØ‰ª•ÊòØ‰ªªÊÑèÁöÑtransformationÔºå‰∏î3x3x2ÁöÑÂèÇÊï∞ÊúÄÂ§öÂèØ‰ª•ÂåÖÂê´3x3Áßçtransformation sampling kernelÂØπkernelÂÜÖÁöÑÊØè‰∏™ÁÇπÔºå‰πüÂèØ‰ª•Êúâ‰∏çÂêåÁöÑÊùÉÈáçÔºå‰πüÂ∞±ÊòØ3x3‰∏™weight factor ËøòÊúâÂï•Ë∑üÂΩ¢ÂèòÁõ∏ÂÖ≥ÁöÑ attentionÊú∫Âà∂ spatial attentionÔºöSTNÔºåsSE channel attentionÔºöSENet ÂêåÊó∂‰ΩøÁî®Á©∫Èó¥attentionÂíåÈÄöÈÅìattentionÊú∫Âà∂ÔºöCBAM papers [STN] STN: Spatial Transformer NetworksÔºåSTNÁöÑÂèòÊç¢ÊòØpre-definedÁöÑÔºåÊòØÈíàÂØπÂÖ®Â±ÄfeaturemapÁöÑÂèòÊç¢ [DCN 2017] Deformable Convolutional Networks ÔºåDCNÁöÑÂèòÊç¢ÊòØÊõ¥ÈöèÊú∫ÁöÑÔºåÊòØÈíàÂØπÂ±ÄÈÉ®kernelÂàÜÂà´ËøõË°åÁöÑÂèòÂåñÔºåÂü∫‰∫éÂç∑ÁßØÊ†∏Ê∑ªÂä†location-specific shift [DCNv2 2018] Deformable ConvNets v2: More Deformable, Better ResultsÔºåËøõ‰∏ÄÊ≠•Ê∂àÈô§irrelevant contextÔºåÂü∫‰∫éÂç∑ÁßØÊ†∏Ê∑ªÂä†weighted-location-specific shiftÔºåÊèêÂçáperformance [attentionÁ≥ªÂàópaper] [SENet &amp;SKNet &amp; CBAM &amp; GC-Net][https://amberzzzz.github.io/2020/03/13/attention%E7%B3%BB%E5%88%97/] STN: Spatial Transformer Networks Âä®Êú∫ ‰º†ÁªüÂç∑ÁßØÔºölack the ability of spacially invariant propose a new learnable module can be inserted into CNN spatially manipulate the data without any extra supervision models learn to be invariant to transformations ËÆ∫ÁÇπ spatially invariant the ability of being invariant to large transformations of the input data max-pooling Âú®‰∏ÄÂÆöÁ®ãÂ∫¶‰∏äspatially invariant Âõ†‰∏∫receptive fields are fixed and local and small ÂøÖÈ°ªÂè†Âä†Âà∞ÊØîËæÉÊ∑±Â±ÇÁöÑÊó∂ÂÄôÊâçËÉΩÂÆûÁé∞Ôºåintermediate feature layersÂØπlarge transformations‰∏çÂ§™Ë°å ÊòØ‰∏ÄÁßçpre-defined mechanismÔºåË∑üsampleÊó†ÂÖ≥ spatial transformation module conditioned on individual samples dynamic mechanism produce a transformation and perform it on the entire feature map taskÂú∫ÊôØ distorted digitsÂàÜÁ±ªÔºöÂØπËæìÂÖ•ÂÅötranformËÉΩÂ§üsimplifyÂêéÈù¢ÁöÑÂàÜÁ±ª‰ªªÂä° co-localisationÔºö spatial attention related work ÁîüÊàêÂô®Áî®Êù•ÁîüÊàêtransformed imagesÔºå‰ªéËÄåÂà§Âà´Âô®ËÉΩÂ§üÂ≠¶‰π†ÂàÜÁ±ª‰ªªÂä°from transformation supervision ‰∏Ä‰∫õmethodsËØïÂõæ‰ªéÁΩëÁªúÁªìÊûÑ„ÄÅfeature extractorsÁöÑËßíÂ∫¶ÁöÑËé∑Âæóinvariant representationsÔºåwhile STN aims to achieve this by manipulating the data manipulating the dataÈÄöÂ∏∏Â∞±ÊòØÂü∫‰∫éattention mechanismÔºåcropÊ∂âÂèädifferentiableÈóÆÈ¢ò ÊñπÊ≥ï formulation localisation networkÔºöpredict transform parameters grid generatorÔºöÂü∫‰∫épredicted paramsÁîüÊàêsampling grid samplerÔºöelement-multiply localisation network input feature map $U \in R^{hwc}$ same transformation is applied to each channel generate parameters of transformation $\theta$Ôºö1-d vector fc / conv + final regression layer parameterised sampling grid sampling kernel applied by pixel general affine transformationÔºöcroppingÔºåtranslationÔºårotationÔºåscaleÔºåskew ouput map‰∏ä‰ªªÊÑè‰∏ÄÁÇπ‰∏ÄÂÆöÊù•Ëá™ÂèòÊç¢ÂâçÁöÑÊüê‰∏ÄÁÇπÔºåÂèç‰πã‰∏ç‰∏ÄÂÆöÔºåinput map‰∏äÊüê‰∏ÄÁÇπÂèØËÉΩÊòØbgÔºåË¢´cropÊéâ‰∫ÜÔºåÊâÄ‰ª•pointwise transformationÂÜôÊàêÂèçËøáÊù•ÁöÑÔºö target pointsÊûÑÊàêÁöÑÁÇπÈõÜÂ∞±ÊòØsampling points on the input feature map differentiable image sampling ÈÄöËøá‰∏ä‰∏ÄÊ≠•ÁöÑÁü©ÈòµtransformationÔºåÂæóÂà∞input map‰∏äÈúÄË¶Å‰øùÁïôÁöÑsource point set ÂØπÁÇπÈõÜ‰∏≠ÊØè‰∏ÄÁÇπapply kernel ÈÄöÁî®ÁöÑÊèíÂÄºË°®ËææÂºèÔºö ÊúÄËøëÈÇªkernelÊòØ‰∏™pulseÂáΩÊï∞ bilinear kernelÊòØ‰∏™distance&gt;1ÁöÑÂÖ®muteÊéâÔºåÂàÜÊÆµÂèØÂØº STNÔºöSpatial Transformer Networks Êääspatial transformerÂµåËøõCNNÂéªÔºölearn how to actively transform the features to help minimize the overall cost computationally fast Âá†ÁßçÁî®Ê≥ï feed the output of the localization network $\theta$ to the rest of the networkÔºöÂõ†‰∏∫transformÂèÇÊï∞explicitly encodesÁõÆÊ†áÁöÑ‰ΩçÁΩÆÂßøÊÄÅ‰ø°ÊÅØ place multiple spatial transformers at increasing depthÔºö‰∏≤Ë°åËÉΩÂ§üËÆ©Ê∑±Â±ÇÁöÑtransformerÂ≠¶‰π†Êõ¥ÊäΩË±°ÁöÑÂèòÊç¢ place multiple spatial transformers in parallelÔºöÂπ∂Ë°åÁöÑÂèòÊç¢‰ΩøÂæóÊØè‰∏™ÂèòÊç¢ÈíàÂØπ‰∏çÂêåÁöÑobject ÂÆûÈ™å R„ÄÅRTS„ÄÅP„ÄÅEÔºödistortion ahead aff„ÄÅproj„ÄÅTPSÔºötransformer predefined affÔºöÁªôÂÆöËßíÂ∫¶ÔºüÔºü TPSÔºöËñÑÊùøÊ†∑Êù°ÊèíÂÄº Deformable Convolutional Networks Âä®Êú∫ CNNÔºöfixed geometric structures enhance the transformation modeling capability deformable convolution deformable RoI pooling without additional supervision share similiar spirit with STN ËÆ∫ÁÇπ to accommodate geometric variations data augmentation is limited to model large, unknown transformations fixed receptive fields is undesirable for high level CNN layers that encode the semantics ‰ΩøÁî®Â§ßÈáèÂ¢ûÂπøÁöÑÊï∞ÊçÆÔºåÊûö‰∏æ‰∏çÂÖ®ÔºåËÄå‰∏îÊî∂ÊïõÊÖ¢ÔºåÊâÄÈúÄÁΩëÁªúÂèÇÊï∞ÈáèÂ§ß ÂØπ‰∫éÊèêÂèñËØ≠‰πâÁâπÂæÅÁöÑÈ´òÂ±ÇÁΩëÁªúÊù•ËÆ≤ÔºåÂõ∫ÂÆöÁöÑÊÑüÂèóÈáéÂØπ‰∏çÂêåÁõÆÊ†á‰∏çÂèãÂ•Ω introduce two new modules deformable convolution learning offsets for each kernel via additional convolutional layers deformable RoI pooling learning offset for each bin partition of the previous RoI pooling ÊñπÊ≥ï overview operate on the 2D spatial domain remains the same across the channel dimension deformable convolution Ê≠£Â∏∏ÁöÑÂç∑ÁßØÔºö $y(p_0) = \sum w(p_n)*x(p_0 + p_n)$ $p_n \in R\{(-1,-1),(-1,0),‚Ä¶, (0,0), (1,1)\}$ deformable convÔºöwith offsets $\Delta p_n$ $y(p_0) = \sum w(p_n)*x(p_0 + p_n + \Delta p_n)$ offset value is typically fractional bilinear interpolationÔºö $x(p) = \sum_q G(q,p)x(q)$ ÂÖ∂‰∏≠$G(q,p)$ÊòØÊù°‰ª∂Ôºö$G(q,p)=max(0, 1-|q_x-p_x|)*max(0, 1-|q_y-p_y|)$ Âè™ËÆ°ÁÆóÂíåoffsetÁÇπË∑ùÁ¶ªÂ∞è‰∫é1‰∏™Âçï‰ΩçÁöÑÈÇªËøëÁÇπ ÂÆûÁé∞ offsets convÂíåÁâπÂæÅÊèêÂèñconvÊòØ‰∏ÄÊ†∑ÁöÑkernelÔºösame spatial resolution and dilationÔºàN‰∏™positionÔºâ the channel dimension 2NÔºöÂõ†‰∏∫ÊòØxÂíåy‰∏§‰∏™ÊñπÂêëÁöÑoffset deformable RoI pooling RoI pooling converts an input feature map of arbitrary size into fixed size features Â∏∏ËßÑÁöÑRoI pooling divides ROI into k*k bins and for each binÔºö$y(i,j) = \sum_{p \in bin(i,j)} x(p_0+p)/n_{ij}$ ÂØπfeature map‰∏äÂàíÂàÜÂà∞ÊØè‰∏™binÈáåÈù¢ÊâÄÊúâÁöÑÁÇπ deformable RoI poolingÔºöwith offsets $\Delta p_{ij}$ $y(i,j) = \sum_{p \in bin(i,j)} x(p_0+p+\Delta p_{ij})/n_{ij}$ scaled normalized offsetsÔºö$\Delta p_{ij} = \gamma \Delta p_{ij} (w,h) $ normalized offset value is fractional bilinear interpolation on the pooled map as above ÂÆûÁé∞ fc layerÔºök*k*2‰∏™elementÔºàsigmoidÔºüÔºâ position sensitive RoI Pooling fully convolutional input feature mapÂÖàÈÄöËøáÂç∑ÁßØÊâ©Â±ïÊàêk*k*(C+1)ÈÄöÈÅì ÂØπÊØè‰∏™C+1(ÂåÖÂê´kk‰∏™feature map)ÔºåconvÂá∫ÂÖ®ÂõæÁöÑoffset(2\k*k‰∏™) deformable convNets initialized with zero weights learning rates are set to $\beta$ times of the learning rate for the existing layers $\beta=1.0$ for conv $\beta=0.01$ for fc feature extraction backÔºöResNet-101 &amp; Aligned-Inception-ResNet withoutTopÔºöA randomly initialized 1x1 conv is added at last to reduce the channel dimension to 1024 last block stride is changed from 2 to 1 the dilation of all the convolution filters with kernel size&gt;1 is changed from 1 to 2 Optionally last block use deformable conv in res5a,b,c segmentation and detection deeplab predicts 1x1 score maps Category-Aware RPN run region proposal with specific class modified faster R-CNNÔºöadd ROI pooling at last conv optional faster R-CNNÔºöuse deformable ROI pooling R-FCNÔºöstate-of-the-art detector optional R-FCNÔºöuse deformable ROI pooling ÂÆûÈ™å Accuracy steadily improves when more deformable convolution layers are usedÔºö‰ΩøÁî®Ë∂äÂ§öÂ±Çdeform convË∂äÂ•ΩÔºåÁªèÈ™åÂèñ‰∫Ü3 the learned offsets are highly adaptive to the image contentÔºöÂ§ßÁõÆÊ†áÁöÑÈó¥Ë∑ùÂ§ßÔºåÂõ†‰∏∫reception fieldÂ§ßÔºåconsistent in different layers atrous convolution also improvesÔºödefault networks have too small receptive fieldsÔºå‰ΩÜÊòØdilationÈúÄË¶ÅÊâãË∞ÉÂà∞ÊúÄ‰ºò using deformable RoI pooling alone already produces noticeable performance gains, using both obtains significant accuracy improvements Deformable ConvNets v2: More Deformable, Better Results Âä®Êú∫ DCNËÉΩÂ§üadapt‰∏ÄÂÆöÁöÑgeometric variationsÔºå‰ΩÜÊòØ‰ªçÂ≠òÂú®extend beyond image contentÁöÑÈóÆÈ¢ò to focus on pertinent image regions increased modeling power more deformable layers updated DCNv2 modules stronger training propose feature mimicking scheme verified on incorporated into Faster-RCNN &amp; Mask RCNN COCO for det &amp; set still lightweight and easy to incorporate ËÆ∫ÁÇπ DCNv1 deformable convÔºöÂú®standard convÁöÑÂü∫Á°Ä‰∏ägenerate location-specific offsets which are learned from the preceding feature maps deformable poolingÔºöoffsets are learned for the bin positions in RoIpooling ÈÄöËøáÂèØËßÜÂåñÊï£ÁÇπÂõæÂèëÁé∞ÊúâÈÉ®ÂàÜÊï£ÁÇπËêΩÂú®ÁõÆÊ†áÂ§ñÂõ¥ propose DCNv2 equip more convolutional layers with offset modified module each sample not only undergoes a learned offset but also a learned feature amplitude effective trainin use RCNN as the teacher network since RCNN learns features unaffected by irrelevant info outside the ROI feature mimicking loss ÊñπÊ≥ï stacking more deformable conv layers replace more regular conv layers by their deformable counterpartsÔºö resnet50ÁöÑstage3„ÄÅ4„ÄÅ5ÁöÑ3x3convÈÉΩÊõøÊç¢Êàêdeformable convÔºö13‰∏™conv layer DCNv1ÊòØÊäästage5ÁöÑ3‰∏™resblockÁöÑ3x3 convÊõøÊç¢Êàêdeformable convÔºö3‰∏™deconv layer Âõ†‰∏∫DCNv1ÈáåÈù¢Âú®PASCAL‰∏äÈù¢ÂÆûÈ™åÂèëÁé∞ÂÜçÂ§öÁöÑdeconvÁ≤æÂ∫¶Â∞±È•±Âíå‰∫ÜÔºå‰ΩÜÊòØDCNv2ÊòØÂú®harder dataset COCO‰∏äÈù¢ÁöÑbest-acc-efficiency-tradeoff modulated deformable conv modulate the input feature amplitudes from different spacial locations/bins set the learnable offset &amp; scalar for the k-th locationÔºö$\Delta p_k$Âíå$\Delta m_k$ set the conv kernel dilationÔºö$p_k$ÔºåresnetÈáåÈù¢ÈÉΩÊòØ1 the value for location p isÔºö$y(p) = \sum_{k=1}^K w_k x(p+p_k+\Delta p_k)\Delta m_k$Ôºåbilinear interpolation ÁõÆÁöÑÊòØÊäëÂà∂Êó†ÂÖ≥‰ø°Âè∑ learnable offset &amp; scalar obtained via a separate conv layer over the same input feature map x ËæìÂá∫Êúâ3K‰∏™channelÔºö2K for xy-offsetÔºåK for scalar offsetÁöÑconvÂêéÈù¢Ê≤°ÊøÄÊ¥ªÂáΩÊï∞ÔºåÂõ†‰∏∫ËåÉÂõ¥Êó†Èôê scalarÁöÑconvÂêéÈù¢Êúâ‰∏™sigmoidÔºåÂ∞ÜrangeÊéßÂà∂Âú®[0,1] ‰∏§‰∏™convÂÖ®0ÂàùÂßãÂåñ ‰∏§‰∏™conv layerÁöÑlearning rateÊØîexisting layersÂ∞è‰∏Ä‰∏™Êï∞ÈáèÁ∫ß modulated deformable RoIpooling given an input ROI split into K(7x7) spatial bins average pooling over the sampling points for each binËÆ°ÁÆóbinÁöÑvalue the bin value isÔºö$y(k) = \sum_{j=1}^{n_k} x(p_{kj}+\Delta p_k)\Delta m_k /n_k$Ôºåbilinear interpolation a sibling branch 2‰∏™1024d-fcÔºögaussian initialization with 0.01 std dev 1‰∏™3Kd-fcÔºöÂÖ®0ÂàùÂßãÂåñ last K channels + sigmoid learning rateË∑üexisting layers‰øùÊåÅ‰∏ÄËá¥ RCNN feature mimicking ÂèëÁé∞Êó†ËÆ∫ÊòØconvËøòÊòØdeconvÔºåerror-boundÈÉΩÂæàÂ§ß Â∞ΩÁÆ°‰ªéËÆæËÆ°ÊÄùË∑Ø‰∏äÔºåDCNv2ÊòØÂ∏¶Êúâmute irrelevantÁöÑËÉΩÂäõÁöÑÔºå‰ΩÜÊòØ‰∫ãÂÆû‰∏äÂπ∂Ê≤°ÂÅöÂà∞ ËØ¥Êòésuch representation cannot be learned well through standard FasterRCNN training procedureÔºö ËØ¥ÁôΩ‰∫ÜÂ∞±ÊòØsupervisionÂäõÂ∫¶‰∏çÂ§ü ÈúÄË¶Åadditional guidance feature mimic loss enforced only on positive ROIsÔºöÂõ†‰∏∫ËÉåÊôØÁ±ªÂæÄÂæÄÈúÄË¶ÅÊõ¥ÈïøË∑ùÁ¶ª/Êõ¥Â§ßËåÉÂõ¥ÁöÑcontext‰ø°ÊÅØ architecture add an additional RCNN branch RCNN input cropped imagesÔºågenerate 14x14 featuremapsÔºåÁªèËøá‰∏§‰∏™fcÂèòÊàê1024-d ÂíåFasterRCNNÈáåÂØπÂ∫îÁöÑcounterpartÔºåËÆ°ÁÆócosine similarity Ëøô‰∏™Â§™ÊâØ‰∫Ü‰∏çÂ±ïÂºÄ‰∫Ü]]></content>
      <tags>
        <tag>Âá†‰ΩïÂèòÊç¢</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spineNet]]></title>
    <url>%2F2021%2F01%2F28%2FspineNet%2F</url>
    <content type="text"><![CDATA[SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization Âä®Êú∫ object detection task requiring simultaneous recognition and localization solely encoder performs not well while encoder-decoder architectures are ineffective propose SpineNet scale-permuted intermediate features cross-scale connections searched by NAS on detection COCO can transfer to classification tasks Âú®ËΩªÈáèÂíåÈáçÈáèbackÁöÑ‰∏ÄÈò∂ÊÆµÁΩëÁªú‰∏≠ÈÉΩÊ∂®ÁÇπÈ¢ÜÂÖà ËÆ∫ÁÇπ scale-decreasing backbone throws away the spatial information by down-sampling challenging to recover Êé•‰∏Ä‰∏™ËΩªÈáèÁöÑFPNÔºö scale-permuted model scales of features can increase/decrease anytimeÔºöretain the spacial information connections go across scalesÔºömulti-scale fusion searched by NAS ÊòØ‰∏Ä‰∏™ÂÆåÊï¥ÁöÑFPNÔºå‰∏çÊòØencoder-decoderÈÇ£ÁßçÂèØÂàÜÁöÑÂΩ¢Âºè directly connect to classification and bounding box regression subnets base on ResNet50 use bottleneck feature blocks two inputs for each feature blocks roughly the same computation ÊñπÊ≥ï formulation overall architecture stemÔºöscale-decreased architecture scale-permuted network blocks in the stem network can be candidate inputs for the following scale-permuted network scale-permuted network building blocksÔºö$B_k$ feature levelÔºö$L_3 - L_7$ output featuresÔºö1x1 convÔºå$P_3 - P_7$ search space scale-permuted networkÔºö blockÂè™ËÉΩ‰ªéÂâçÂæÄÂêéconnect based on resNet blocks channel 256 for $L_5, L_6, L_7$ cross-scale connectionsÔºö two input connections for each block from lower ordering block / stem resampling narrow factor $\alpha$Ôºö1x1 conv ‰∏äÈááÊ†∑Ôºöinterpolation ‰∏ãÈááÊ†∑Ôºö3x3 s2 conv element-wise add block adjustment intermediate blocks can adjust its scale level &amp; type level from {-1, 0, 1, 2} select from bottleneck / residual block family of models R[N] - SP[M]ÔºöN feature layers in stem &amp; M feature layers in scale-permuted layers gradually shift from stem to SP with size decreasing spineNet family basicÔºöspineNet-49 spineNet-49SÔºöchannelÊï∞scaled down by 0.65 spineNet-96Ôºödouble the number of blocks spineNet-143Ôºörepeat 3 timesÔºåfusion narrow factor $\alpha=1$ spineNet-190Ôºörepeat 4 timesÔºåfusion narrow factor $\alpha=1$ÔºåchannelÊï∞scaled up by 1.3 ÂÆûÈ™å Âú®mid/heavyÈáèÁ∫ß‰∏äÔºåÊØîresnet-family-FPNÊ∂®Âá∫‰∏§‰∏™ÁÇπ Âú®lightÈáèÁ∫ß‰∏äÔºåÊØîmobileNet-family-FPNÊ∂®Âá∫‰∏Ä‰∏™ÁÇπ]]></content>
      <tags>
        <tag>backbone</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[guided anchoring]]></title>
    <url>%2F2021%2F01%2F27%2Fguided-anchoring%2F</url>
    <content type="text"><![CDATA[Âéü‰ΩúËÄÖÁü•‰πéreferenceÔºöhttps://zhuanlan.zhihu.com/p/55854246 ‰∏çÂÆåÂÖ®ÊòØanchor-freeÔºåÂõ†‰∏∫ËøòÊòØÊúâdecision grid to choose fromÁöÑÔºåÂ∫îËØ•ËØ¥ÊòØadaptive anchor instead of hand-picked ‰∏∫‰∫ÜÁâπÂæÅÂíåadaptive anchorÂØπÈΩêÔºåÂºïÂÖ•deformable conv Region Proposal by Guided Anchoring Âä®Êú∫ most methods predefined anchors do a uniformed dense prediction our method use sematic features to guide the anchoring anchor size‰πüÊòØÁΩëÁªúÈ¢ÑÊµãÂèÇÊï∞Ôºåcompute from feature map arbitrary aspect ratios feature inconsistency ‰∏çÂêåÁöÑanchor locÈÉΩÊòØÂØπÂ∫îfeature map‰∏äÊüê‰∏Ä‰∏™ÁÇπ ÂèòÂåñÁöÑanchor sizeÂíåÂõ∫ÂÆöÁöÑ‰ΩçÁΩÆÂêëÈáè‰πãÈó¥Â≠òÂú®inconsistency ÂºïÂÖ•feature adaption module use high-quality proposals GA-RPNÊèêÂçá‰∫ÜproposalÁöÑË¥®Èáè Âõ†Ê≠§Êàë‰ª¨ÂØπproposalËøõÂÖ•stage2ÁöÑÊù°‰ª∂Êõ¥‰∏•Ê†º adopt in Fast R-CNN, Faster R-CNN and RetinaNetÂùáÊ∂®ÁÇπ RPNÊèêÂçáÊòæËëóÔºö9.1 MAP‰πüÊúâÊ∂®ÁÇπÔºö1.2-2.7 ËøòÂèØ‰ª•boosting trained models boosting a two-stage detector by a fine-tuning schedule ËÆ∫ÁÇπ alignment &amp; consistency Êàë‰ª¨Áî®feature mapÁöÑpixels‰Ωú‰∏∫anchor representationsÔºåÈÇ£‰πàanchor centersÂøÖÈ°ªË∑üfeature pixels‰øùÊåÅalign ‰∏çÂêåpixelÁöÑreception fieldÂøÖÈ°ªË∑üÂØπÂ∫îÁöÑanchor size‰øùÊåÅÂåπÈÖç previous sliding window schemeÂØπÊØè‰∏™pixelÈÉΩÂÅö‰∏ÄÊ†∑ÁöÑÊìç‰ΩúÔºåÁî®ÂêåÊ†∑‰∏ÄÁªÑanchorÔºåÂõ†Ê≠§ÊòØalignÂíåconsistÁöÑ previous progressly refining schemeÂØπanchorÁöÑ‰ΩçÁΩÆÂ§ßÂ∞èÂÅö‰∫ÜrefinementÔºåignore the alignment &amp; consistency issueÔºåÊòØ‰∏çÂØπÁöÑÔºÅÔºÅ disadvantage of predefined anchors hard hyperparams huge pos/neg imbalance &amp; computation we propose GA-RPN learnable anchor shapes to mitigate the hand-picked issue feature adaptation to solve the consistency issue key concerns in this paper learnable anchors joint anchor distribution alignment &amp; consistency high-quality proposals ÊñπÊ≥ï formulation $p(x,y,w,h|I) = p(x,y|I)p(w,h|x,y,I)$ Â∞ÜÈóÆÈ¢òËß£ËÄ¶Êàê‰ΩçÁΩÆÂíåÂ∞∫ÂØ∏ÁöÑÈ¢ÑÊµãÔºåÈ¶ñÂÖàanchorÁöÑlocÊúç‰ªéfull imageÁöÑÂùáÂåÄÂàÜÂ∏ÉÔºåanchorÁöÑsizeÂª∫Á´ãÂú®locÂ≠òÂú®ÁöÑÂü∫Á°Ä‰∏ä two branches for loc &amp; shape prediction locÔºöbinary classificationÔºåhxwx1 shapeÔºölocation-dependent shapesÔºåhxwx2 anchorsÔºöloc probabilities above a certain threshold &amp; correponding ‚Äòmost probable‚Äô anchor shape multi-scale the anchor generation parameters are shared feature adaptation module adapts the feature according to the anchor shape anchor location prediction indicates the probability of an object‚Äôs center ‰∏ÄÂ±ÇÂç∑ÁßØÔºö1x1 convÔºåchannel1Ôºåsigmoid transform backÔºöeach grid(i,j) corresponds to coords ((i+0.5)*s, (j+0.5)*s) on the origin map filter out 90% of the regions thus replace the ensuing conv layers by masked convs groud truth binary label map each levelÔºöcenter region &amp; ignore region &amp; outside regionÔºåÂü∫‰∫éobject centerÁöÑÊñπÊ°Ü $\sigma_1=0.2Ôºå\sigma_2=0.5$Ôºöregion boxÁöÑÈïøÂÆΩÁ≥ªÊï∞ ÔºüÔºüÔºüÁî®centerNetÁöÑheatmap‰ºö‰∏ç‰ºöÊõ¥Â•ΩÔºüÔºüÔºü focal loss $L_{loc}$ anchor shape prediction predicts the best shape for each location best shapeÔºöa shape that lead to best iou with the nearest gt box ‰∏ÄÂ±ÇÂç∑ÁßØÔºö1x1 convÔºåchannel2Ôºå[-1,1] transform layerÔºötransform direct [-1,1] outputs to real box shape $w = \sigma s e^{dw}$ $h = \sigma s e^{dh}$ sÔºöstride $\sigma$ÔºöÁªèÈ™åÂèÇÊï∞Ôºå8 in experiments set 9 pairs of (w,h) as RetinaNetÔºåcalculate the IoU of these sampled anchors with gtÔºåtake the max as target value bounded iou lossÔºö$L_{shape} = L_1(1-min(\frac{w}{w_g}, \frac{w_g}{w})) + L_1(1-min(\frac{h}{h_g}, \frac{h_g}{h}))$ feature adaptation intuitionÔºöthe feature corresponding to different size of anchor shapesÂ∫îËØ•encode different content region inputsÔºöfeature map &amp; anchor shape location-dependent transformationÔºö3x3 deformable conv deformable convÁöÑoffsetÊòØanchor shapeÂæóÂà∞ÁöÑ outputsÔºöadapted features with adapted features then perform further classification and bounding-box regression training jointly optimizeÔºö$L = \lambda_1 L_{loc} + \lambda_2 L_{shape} + L_{cls} + L_{reg}$ $\lambda_1=0.2Ôºå\lambda_2=0.5$ each level of feature map should only target objects of a specific scale rangeÔºö‰ΩÜÊòØASFFËÆ∫Êñá‰∏ªÂº†ËØ¥ËøôÁßçarrange by scaleÁöÑÊ®°Âºè‰ºöÂºïÂÖ•ÂâçËÉåÊôØinconsistencyÔºüÔºü High-quality Proposals set a higher positive/negative threshold use fewer samples]]></content>
      <tags>
        <tag>ÁõÆÊ†áÊ£ÄÊµãÔºåone/two-stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ASFF]]></title>
    <url>%2F2021%2F01%2F25%2FASFF%2F</url>
    <content type="text"><![CDATA[Learning Spatial Fusion for Single-Shot Object Detection Âä®Êú∫ inconsistency when fuse across different feature scales propose ASFF suppress the inconsistency spatially filter conflictive informationÔºöÊÉ≥Ê≥ïÂ∫îËØ•Ë∑üSSE-blockÁ±ª‰ºº build on yolov3 introduce a bag of tricks anchor-free pipeline ËÆ∫ÁÇπ ssd is one of the first to generate pyramidal feature representations deeper layers reuse the formers bottom-up path small instances suffers low acc because containing insufficient semanic info FPN use top-down path shares rich semantics at all levels improvementÔºömore strengthening feature fusion Âú®‰ΩøÁî®FPNÊó∂ÔºåÈÄöÂ∏∏‰∏çÂêåscaleÁöÑÁõÆÊ†áÁªëÂÆöÂà∞‰∏çÂêåÁöÑlevel‰∏äÈù¢ inconsistencyÔºöÂÖ∂‰ªñlevelÁöÑfeature mapÂØπÂ∫î‰ΩçÁΩÆÁöÑ‰ø°ÊÅØÂàô‰∏∫ËÉåÊôØ some methods set ignore region in adjacent features ÊñπÊ≥ï introduce advanced techniques mixup cosine learning rate schedule sync-bn an anchor-free branch to run jointly with anchor-based ones L1 loss + IoU loss fusion ÂÖ®ËÅîÊé•ËÄåÈùûadjacent mergeÔºö‰∏â‰∏™levelÁöÑfuse mapÈÉΩÊù•Ëá™‰∏â‰∏™levelÁöÑfeature map ‰∏äÈááÊ†∑Ôºö 1x1 convÔºöÂØπÈΩêchannel upsamp with interpolation ‰∏ãÈááÊ†∑Ôºö s2Ôºö3x3 s2 conv s4Ôºömaxpooling + 3x3 s2 conv adaptive fusion pixel levelÁöÑreweight shared across channelsÔºöhxwx1 ÂØπÊù•Ëá™‰∏â‰∏™levelÁöÑfeature mapÔºåresolutionÂØπÈΩê‰ª•ÂêéÔºåÂàÜÂà´1x1convÔºåchannel 1 norm the weightsÔºösoftmax ‰∏∫Âï•ËÉΩsuppress inconsistencyÔºö‰∏â‰∏™levelÁöÑÂÉèÁ¥†ÁÇπÔºåÂè™ÊøÄÊ¥ª‰∏Ä‰∏™Âè¶Â§ñ‰∏§‰∏™ÊòØ0ÁöÑÊÉÖÂÜµÊòØÁªùÂØπ‰∏çharmÁöÑÔºåÁõ∏ÂΩì‰∫é‰∏äÈù¢ignoreÈÇ£‰∏™ÊñπÊ≥ïÊãìÂ±ïÊàêadaptive training apply mixup on the classification pretraining of D53 turn off mixup augmentation for the last 30 epochs. inference the detection header at each level first predicts the shape of anchorsÔºüÔºüÔºüËøô‰∏™‰∏çÂ§™ÊáÇ ASFF &amp; ASFF* enhanced version of ASFF by integrating other lightweight modules dropblock &amp; RFB ÂÆûÁé∞ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768class ASFF(nn.Module): def __init__(self, level, activate, rfb=False, vis=False): super(ASFF, self).__init__() self.level = level self.dim = [512, 256, 128] self.inter_dim = self.dim[self.level] if level == 0: self.stride_level_1 = conv_bn(256, self.inter_dim, kernel=3, stride=2) self.stride_level_2 = conv_bn(128, self.inter_dim, kernel=3, stride=2) self.expand = conv_bn(self.inter_dim, 512, kernel=3, stride=1) elif level == 1: self.compress_level_0 = conv_bn(512, self.inter_dim, kernel=1) self.stride_level_2 = conv_bn(128, self.inter_dim, kernel=3, stride=2) self.expand = conv_bn(self.inter_dim, 256, kernel=3, stride=1) elif level == 2: self.compress_level_0 = conv_bn(512, self.inter_dim, kernel=1, stride=1) self.compress_level_1= conv_bn(256,self.inter_dim,kernel=1,stride=1) self.expand = conv_bn(self.inter_dim, 128, kernel=3, stride=1) compress_c = 8 if rfb else 16 self.weight_level_0 = conv_bn(self.inter_dim, compress_c, 1, 1, 0) self.weight_level_1 = conv_bn(self.inter_dim, compress_c, 1, 1, 0) self.weight_level_2 = conv_bn(self.inter_dim, compress_c, 1, 1, 0) self.weight_levels = conv_bias(compress_c * 3, 3, kernel=1, stride=1, padding=0) self.vis = vis def forward(self, x_level_0, x_level_1, x_level_2): # Ë∑üËÆ∫ÊñáÊèèËø∞‰∏ÄÊ†∑Ôºö‰∏äÈááÊ†∑ÂÖà1x1convÂØπÈΩêÔºåÂÜçupinterpÔºå‰∏ãÈááÊ†∑3x3 s2 conv if self.level == 0: level_0_resized = x_level_0 level_1_resized = self.stride_level_1(x_level_1) level_2_downsampled_inter = F.max_pool2d(x_level_2, 3, stride=2, padding=1) level_2_resized = self.stride_level_2(level_2_downsampled_inter) elif self.level == 1: level_0_compressed = self.compress_level_0(x_level_0) sh = torch.tensor(level_0_compressed.shape[-2:])*2 level_0_resized = F.interpolate(level_0_compressed, tuple(sh), 'nearest') level_1_resized = x_level_1 level_2_resized = self.stride_level_2(x_level_2) elif self.level == 2: level_0_compressed = self.compress_level_0(x_level_0) sh = torch.tensor(level_0_compressed.shape[-2:])*4 level_0_resized = F.interpolate(level_0_compressed, tuple(sh), 'nearest') level_1_compressed = self.compress_level_1(x_level_1) sh = torch.tensor(level_1_compressed.shape[-2:])*2 level_1_resized = F.interpolate(level_1_compressed, tuple(sh),'nearest') level_2_resized = x_level_2 # ËøôÈáåÂæóÂà∞ÁöÑresizedÁâπÂæÅÂõæ‰∏çÁõ¥Êé•ËΩ¨Êç¢Êàê‰∏ÄÈÄöÈÅìÁöÑweighting mapÔºå # ËÄåÊòØÂÖà1x1convÈôçÁª¥Âà∞8/16ÔºåÁÑ∂ÂêéconcatÔºåÁÑ∂Âêé3x3ÁîüÊàê3ÈÄöÈÅìÁöÑweighting map # weighting mapÁõ∏ÂΩì‰∫é‰∏Ä‰∏™prediction headÔºåÊâÄ‰ª•ÊòØconv_bias_softmaxÔºåÊó†bn level_0_weight_v = self.weight_level_0(level_0_resized) level_1_weight_v = self.weight_level_1(level_1_resized) level_2_weight_v = self.weight_level_2(level_2_resized) levels_weight_v = torch.cat((level_0_weight_v, level_1_weight_v, level_2_weight_v), 1) levels_weight = self.weight_levels(levels_weight_v) levels_weight = F.softmax(levels_weight, dim=1) # reweighting fused_out_reduced = level_0_resized * levels_weight[:, 0:1, :, :] + \ level_1_resized * levels_weight[:, 1:2, :, :] + \ level_2_resized * levels_weight[:, 2:, :, :] # 3x3ÁöÑconvÔºåÊòØÁâπÂæÅÂõæÂπ≥Êªë out = self.expand(fused_out_reduced) if self.vis: return out, levels_weight, fused_out_reduced.sum(dim=1) else: return out]]></content>
      <tags>
        <tag>ÁõÆÊ†áÊ£ÄÊµãÔºåyolov3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VoVNet]]></title>
    <url>%2F2021%2F01%2F22%2FVoVNet%2F</url>
    <content type="text"><![CDATA[An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection Âä®Êú∫ denseNet dense pathÔºödiverse receptive fields heavy memory cost &amp; low efficiency we propose a backbone preserve the benefit of concatenation improve denseNet efficiency VoVNet comprised of One-Shot Aggregation (OSA) apply to one/two stage object detection tasks outperforms denseNet &amp; resNet based ones better small object detection performance ËÆ∫ÁÇπ main difference between resNet &amp; denseNet aggregationÔºösummation &amp; concatenation summation would washed out the early features concatenation last as it preserves GPU parallel computation computing utilization is maximized when operand tensor is larger many 1x1 convs for reducing dimension dense connections in intermediate layers are inducing the inefficiencies VoVNet hypothesize that the dense connections are redundant OSAÔºöaggregates intermediate features at once test as object detection backboneÔºöoutperforms DenseNet &amp; ResNet with better energy efficiency and speed factors for efficiency FLOPS and model sizes are indirect metrics energy per image and frame per second are more practical MACÔºö memory accesses costÔºå$hw(c_i+c_o) + k^2 c_ic_o$ memory usage‰∏çÊ≠¢Ë∑üÂèÇÊï∞ÈáèÊúâÂÖ≥ÔºåËøòË∑üÁâπÂæÅÂõæÂ∞∫ÂØ∏Áõ∏ÂÖ≥ MAC can be minimized when input channel size equals the output FLOPs/s splitting a large convolution operation into several fragmented smaller operations makes GPU computation inefficient as fewer computations are processed in parallel ÊâÄ‰ª•depthwise/bottleneckÁêÜËÆ∫‰∏äÈôç‰Ωé‰∫ÜËÆ°ÁÆóÈáèFLOPÔºå‰ΩÜÊòØ‰ªéGPUÂπ∂Ë°åÁöÑËßíÂ∫¶efficiencyÈôç‰ΩéÔºåÂπ∂Ê≤°ÊúâÊòæËëóÊèêÈÄüÔºöcause more sequential computations ‰ª•Êó∂Èó¥‰∏∫Âçï‰ΩçÁöÑFLOPsÊâçÊòØfairÁöÑ ÊñπÊ≥ï hypothesize dense connection makes similar between neighbor layers redundant OSA dense connectionÔºöformer features concats in every following features one-shot connectionÔºöformer features concats once in the last feature ÊúÄÂºÄÂßãË∑üdense block‰øùÊåÅÂèÇÊï∞‰∏ÄËá¥Ôºö‰∏Ä‰∏™blockÈáåÈù¢12‰∏™layersÔºåchannel20ÔºåÂèëÁé∞Ê∑±Â±ÇÁâπÂæÅcontributes lessÔºåÊâÄ‰ª•Êç¢ÊàêÊµÖÂ±ÇÔºå5‰∏™layersÔºåchannel43ÔºåÂèëÁé∞ÊúâÊ∂®ÁÇπÔºöimplies that building deep intermediate feature via dense connection is less effective than expected in/out channelÊï∞Áõ∏Âêå much less MACÔºö denseNet40Ôºö3.7M OSAÔºö5layersÔºåchannel43Ôºå2.5M ÂØπ‰∫éhigher resolutionÁöÑdetection‰ªªÂä°impies more fast and energy efficient GPU efficiency ‰∏çÈúÄË¶ÅÈÇ£Â•ΩÂá†ÂçÅ‰∏™1x1 architecture stemÔºö3‰∏™3x3conv downsampÔºös2ÁöÑmaxpooling stagesÔºöincreasing channels enables more rich semantic high-level informationÔºåbetter feature representation deeperÔºömakes more modules in stage3/4 ÂÆûÈ™å one-stageÔºörefineDet two-stageÔºöMask-RCNN]]></content>
  </entry>
  <entry>
    <title><![CDATA[GCN]]></title>
    <url>%2F2021%2F01%2F18%2FGCN%2F</url>
    <content type="text"><![CDATA[referenceÔºöhttps://mp.weixin.qq.com/s/SWQHgogAP164Kr082YkF4A Âõæ $G = (V,E)$ÔºöËäÇÁÇπ &amp; ËæπÔºåËøûÈÄöÂõæ &amp; Â≠§Á´ãÁÇπ ÈÇªÊé•Áü©ÈòµAÔºöNxNÔºåÊúâÂêë &amp; Êó†Âêë Â∫¶Áü©ÈòµDÔºöNxNÂØπËßíÁü©ÈòµÔºåÊØè‰∏™ËäÇÁÇπËøûÊé•ÁöÑËäÇÁÇπ ÁâπÂæÅÁü©ÈòµXÔºöNxFÔºåÊØè‰∏™1-dim FÊòØÊØè‰∏™ËäÇÁÇπÁöÑÁâπÂæÅÂêëÈáè ÁâπÂæÅÂ≠¶‰π† ÂèØ‰ª•Á±ªÊØîCNNÔºöÂØπÂÖ∂ÈÇªÂüüÔºàkernelÔºâÂÜÖÁâπÂæÅËøõË°åÁ∫øÊÄßÂèòÊç¢ÔºàwÂä†ÊùÉÔºâÔºåÁÑ∂ÂêéÊ±ÇÂíåÔºåÁÑ∂ÂêéÊøÄÊ¥ªÂáΩÊï∞ $H^{k+1} = f(H^{k},A) = \sigma(AH^{k}W^{k})$ HÔºörunning updating ÁâπÂæÅÁü©ÈòµÔºåNxFk AÔºö0-1ÈÇªÊé•Áü©ÈòµÔºåNxN WÔºöÊùÉÈáçÔºå$F_k$x$F_{k+1}$ ÊùÉÈáçÊâÄÊúâËäÇÁÇπÂÖ±‰∫´ ËäÇÁÇπÁöÑÈÇªÊé•ËäÇÁÇπÂèØ‰ª•ÁúãÂÅöÊÑüÂèóÈáé ÁΩëÁªúÂä†Ê∑±ÔºåÊÑüÂèóÈáéÂ¢ûÂ§ßÔºöËäÇÁÇπÁöÑÁâπÂæÅËûçÂêà‰∫ÜÊõ¥Â§öËäÇÁÇπÁöÑ‰ø°ÊÅØ ÂõæÂç∑ÁßØ A‰∏≠Ê≤°ÊúâËÄÉËôëËá™Â∑±ÁöÑÁâπÂæÅÔºöÊ∑ªÂä†Ëá™ËøûÊé• A = A + I Âä†Ê≥ïËßÑÂàôÂØπÂ∫¶Â§ßÁöÑËäÇÁÇπÔºåÁâπÂæÅ‰ºöË∂äÊù•Ë∂äÂ§ßÔºöÂΩí‰∏ÄÂåñ ‰ΩøÂæóÈÇªÊé•Áü©ÈòµÊØèË°åÂíå‰∏∫1ÔºöÂ∑¶‰πòÂ∫¶Áü©ÈòµÁöÑÈÄÜ Êï∞Â≠¶ÂÆûË¥®ÔºöÊ±ÇÂπ≥Âùá one step furtherÔºö‰∏çÂçïÂØπË°åÂÅöÂπ≥ÂùáÔºåÂØπÂ∫¶ËæÉÂ§ßÁöÑÈÇªÊé•ËäÇÁÇπ‰πüÂÅöpunish GCNÁΩëÁªú ÂÆûÁé∞ weightsÔºöin x outÔºåkaiming_uniform_initialize biasÔºöoutÔºåzero_initialize activationÔºörelu A x H x WÔºöÂ∑¶‰πòÊòØÁ≥ªÊï∞Áü©Èòµ‰πòÊ≥ï ÈÇªÊé•Áü©ÈòµÁöÑÁªìÊûÑ‰ªéËæìÂÖ•ÂºÄÂßãÂ∞±‰∏çÂèò‰∫ÜÔºåÂíåÊØèÂ±ÇÁöÑÁâπÂæÅÁü©Èòµ‰∏ÄËµ∑‰Ωú‰∏∫ËæìÂÖ•Ôºå‰º†ÂÖ•GCN ÂàÜÁ±ªÂ§¥ÔºöÊúÄÂêé‰∏ÄÂ±ÇÈ¢ÑÊµãNxn_classÁöÑÁâπÂæÅÂêëÈáèÔºåÊèêÂèñÊÑüÂÖ¥Ë∂£ËäÇÁÇπF(n_class)ÔºåÁÑ∂ÂêésoftmaxÔºåÂØπÂÖ∂ÂàÜÁ±ª ÂΩí‰∏ÄÂåñ 123456789101112131415161718# ÂØπÁß∞ÂΩí‰∏ÄÂåñdef normalize_adj(adj): """compute L=D^-0.5 * (A+I) * D^-0.5""" adj += sp.eye(adj.shape[0]) degree = np.array(adj.sum(1)) d_hat = sp.diags(np.power(degree, -0.5).flatten()) norm_adj = d_hat.dot(adj).dot(d_hat) return norm_adj # ÂùáÂÄºÂΩí‰∏ÄÂåñdef normalize_adj(adj): """compute L=D^-1 * (A+I)""" adj += sp.eye(adj.shape[0]) degree = np.array(adj.sum(1)) d_hat = sp.diags(np.power(degree, -1).flatten()) norm_adj = d_hat.dot(adj) return norm_adj Â∫îÁî®Âú∫ÊôØ [ÂçäÁõëÁù£ÂàÜÁ±ªGCN]ÔºöSEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKSÔºåÊèêÂá∫GCN [skin GCN]ÔºöLearning Differential Diagnosis of Skin Conditions with Co-occurrence Supervision using Graph Convolutional NetworksÔºå‰ΩìÁ¥†Ôºå‰∏Ä‰∏™ÂçïÁã¨ÁöÑÂü∫‰∫éÂõæÁöÑÁõ∏ÂÖ≥ÊÄßÂàÜÊîØÔºåÁªôfeatureÂä†ÊùÉ [Graph Attention]ÔºöGraph Attention NetworksÔºåÂõæÊ≥®ÊÑèÂäõÁΩëÁªú Learning Differential Diagnosis of Skin Conditions with Co-occurrence Supervision using Graph Convolutional Networks Âä®Êú∫ ÁöÆËÇ§ÁóÖÔºöÂèëÁóÖÁéáÈ´òÔºåexpertsÂ∞ë differential diagnosisÔºöÈâ¥Âà´ËØäÊñ≠ÔºåÂ∞±ÊòØ‰ªé‰ºóÂ§öÁñæÁóÖÁ±ªÂà´‰∏≠Ë∑≥Âá∫Ê≠£Á°ÆÁ±ªÂà´ still challengingÔºötimely and accurate propose a DLS(deep learning system) clinical images multi-label classification 80 conditionsÔºåË¶ÜÁõñÁóÖÁßç labels incompletenessÔºöÁî®GCNÂª∫Ê®°ÊàêCo-occurrence supervisionÔºåbenefit top5 ËÆ∫ÁÇπ googleÁöÑDLS 26‰∏≠ÁñæÁóÖ Âª∫Ê®°Êàêmulti-class classification problemÔºöÈùû0Âç≥1ÁöÑÂ§öÊ†áÁ≠æË°®ËææÁ†¥Âùè‰∫ÜÁ±ªÂà´Èó¥ÁöÑcorrelation our DLSÔºöGCN-CNN multi-label classification task over 80 conditions incomplete image labelsÔºöGCN that characterizes label co-occurrence supervision combine the classification network with the GCN Êï∞ÊçÆÈáèÔºö136,462 clinical images Á≤æÂ∫¶Ôºötest on 12,378 user taken imagesÔºåtop-5 acc 93.6% GCN original applicationÔºö nodes classificationÔºåonly a small subset of nodes had their labels availableÔºöÂçäÁõëÁù£ÊñáÊú¨ÂàÜÁ±ªÈóÆÈ¢òÔºåÂè™Êúâ‰∏ÄÈÉ®ÂàÜËäÇÁÇπÁî®‰∫éËÆ≠ÁªÉ the graph structure is contructed from data ML-GCNÔºö multi-label classification task correlation mapÔºàÂõæÁªìÊûÑÔºâÂàôÊòØÈÄöËøáÊï∞ÊçÆÁõ¥Êé•Âª∫Á´ã ÂõæËäÇÁÇπÊòØÊØè‰∏™Á±ªÂà´ÁöÑsemantic embeddings ÊñπÊ≥ï overview ‰∏Ä‰∏™trainableÁöÑCNNÔºåÂ∞ÜÂõæÁâáËΩ¨ÂåñÊàêfeature vector ‰∏Ä‰∏™GCN branchÔºö‰∏§Â±ÇÂõæÂç∑ÁßØÔºåÈÉΩÊòØorder-1ÔºåÂõæÁªìÊûÑÊòØÂü∫‰∫éËÆ≠ÁªÉÈõÜËÆ°ÁÆóÔºåÊó†ÂêëÂõæÔºåencodingÁöÑÊòØÂõæÂÉèlabels‰πãÈó¥ÁöÑdependencyÔºåÁî®ÂÆÉ implicitly supervises the classification task ÁÑ∂Âêé‰∏§‰∏™feature vectorÁõ∏‰πòÔºåÁªôÂá∫ÊúÄÁªàÁªìÊûú GCN branch two graph convolutional (GC) layers ‰∏ÄÁßçestimatedÂõæÁªìÊûÑÔºöbuild co-occurence graph using only training data node embed semantic meaning to labels ËæπÁöÑÂÄºÂÆö‰πâÊúâÁÇπÂÉèÁ±ªÂà´Èó¥ÁöÑÁõ∏ÂÖ≥ÊÄßÂº∫Â∫¶Ôºö$e_{ij} = 1(\frac{C(i,j)}{C(i)+C(j)} \geq t)$ÔºåÂàÜÂ≠êÊòØÊúâ‰∏§ÁßçÊ†áÁ≠æÁöÑÊ†∑Êú¨ÈáèÔºåÂàÜÊØçÊòØÂêÑËá™Ê†∑Êú¨Èáè ‰∏ÄÁßçdesignedÂõæÁªìÊûÑÔºöintial valueÊòØÂü∫‰∫éÊúâÁªèÈ™åÁöÑ‰∏ìÂÆ∂ÊûÑÂª∫ node representation graph branchÁöÑËæìÂÖ• label embedding Áî®‰∫ÜBioSentVecÔºå‰∏Ä‰∏™Âü∫‰∫éÁîüÁâ©ÂåªÂ≠¶ËØ≠ÊñôÂ∫ìËÆ≠ÁªÉÁöÑword bag GCN randomly initialize GCN-0Ôºödim 700 GCN-1Ôºödim 1024 GCN-2Ôºödim 2048 ÊúÄÁªàÂæóÂà∞(cls,2048)ÁöÑnode features cls branch inputÔºödownsized to 448x448 resnet101ÔºöÊâßË°åÂà∞FC-2048Ôºå‰Ωú‰∏∫image features ÂÖàËÆ≠ÁªÉ300 epochsÔºålr 0.1Ôºåstep decay GCN-CNN ÂÖàÈ¢ÑËÆ≠ÁªÉresnet backboneÔºå ÁÑ∂ÂêéÊï¥‰Ωì‰∏ÄËµ∑ËÆ≠ÁªÉ300 epochsÔºålr 0.0003Ôºå image featureÂíånode featuresÈÄöËøádot productËûçÂêàÔºåÂæóÂà∞(cls, )ÁöÑcls vecÔºå ÂÆûÈ™å ÂõæÁªìÊûÑ‰∏çËÉΩrandom initializationÔºå‰ºö‰ΩøÁªìÊûúÂèòÂ∑Æ Âü∫‰∫éÊï∞ÊçÆÈõÜ‰º∞ËÆ°ÁöÑgraph initializationÊúâÊòæËëóÊèêÂçá Âü∫‰∫é‰∏ìÂÆ∂ËÆæËÆ°ÁöÑgraph initializationÊúâËøõ‰∏ÄÊ≠•ÊèêÂçáÔºå‰ΩÜÊòØ‰∏çÊòéÊòæÔºåËÄÉËôëÂà∞Ê†áÊ≥®Â∑•‰ΩúÁπÅÈáç‰∏çÂ§™Êé®Ëçê SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS reference http://tkipf.github.io/graph-convolutional-networks/ÔºåÂÆòÊñπÂçöÂÆ¢ https://zhuanlan.zhihu.com/p/35630785ÔºåÁü•‰πéÁ¨îËÆ∞ ËÆ∫ÁÇπ Âú∫ÊôØ semi-supervised learning on graph-structured data ÊØîÂ¶ÇÔºöÂú®‰∏Ä‰∏™citation networkÔºåclassifying nodes (such as documents)Ôºålabels are only available for a small subset of nodesÔºå‰ªªÂä°ÁöÑÁõÆÊ†áÊòØÂØπÂ§ßÈÉ®ÂàÜÊú™Ê†áËÆ∞ÁöÑËäÇÁÇπÈ¢ÑÊµãÁ±ªÂà´ previous approach Standard Approach lossÁî±‰∏§ÈÉ®ÂàÜÁªÑÊàêÔºöÂçï‰∏™ËäÇÁÇπÁöÑfitting errorÔºåÂíåÁõ∏ÈÇªËäÇÁÇπÁöÑdistance error Âü∫‰∫é‰∏Ä‰∏™ÂÅáËÆæÔºöÁõ∏ÈÇªËäÇÁÇπÈó¥ÁöÑlabelÁõ∏‰ºº ÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑË°®ËææËÉΩÂäõ Embedding-based Approach ÂàÜ‰∏§Ê≠•ËøõË°åÔºöÂÖàÂ≠¶‰π†ËäÇÁÇπÁöÑembeddingÔºåÂÜçÂü∫‰∫éembeddingËÆ≠ÁªÉÂàÜÁ±ªÂô® ‰∏çend-to-endÔºå‰∏§‰∏™taskÂàÜÂà´ÊâßË°åÔºå‰∏çËÉΩ‰øùËØÅÂ≠¶Âà∞ÁöÑembeddingÊòØÈÄÇÂêàÁ¨¨‰∫å‰∏™‰ªªÂä°ÁöÑ ÊÄùË∑Ø train on a supervised target for nodes with labels ÁÑ∂ÂêéÈÄöËøáÂõæÁöÑËøûÈÄöÊÄßÔºåtrainable adjacency matrixÔºå‰º†ÈÄíÊ¢ØÂ∫¶Áªôunlabeled nodes ‰ΩøÂæóÂÖ®ÂõæÂæóÂà∞ÁõëÁù£‰ø°ÊÅØ contributions introduce a layer-wise propagation ruleÔºå‰ΩøÂæóÁ•ûÁªèÁΩëÁªúËÉΩÂ§üoperate on graphÔºåÂÆûÁé∞end-to-endÁöÑÂõæÁªìÊûÑÂàÜÁ±ªÂô® use this graph-based neural network modelÔºåËÆ≠ÁªÉ‰∏Ä‰∏™semi-supervised classification of nodesÁöÑ‰ªªÂä° ÊñπÊ≥ï fast approximate convolutions on graphs givenÔºö layer inputÔºö$H^l$ layer outputÔºö$H^{l+1}$ kernel patternÔºö$A$ÔºåÂú®Âç∑ÁßØÈáåÈù¢ÊòØfixed kxk ÊñπÊ†ºÔºåÂú®ÂõæÈáåÈù¢Â∞±ÊòØËá™Áî±Â∫¶Êõ¥È´òÁöÑÈÇªÊé•Áü©Èòµ kernel weightsÔºö$W$ general layer formÔºö$H^{l+1}=f(H^l,A)$ inspirationÔºöÂç∑ÁßØÂÖ∂ÂÆûÊòØ‰∏ÄÁßçÁâπÊÆäÁöÑÂõæÔºåÊØè‰∏™gridÁúã‰Ωú‰∏Ä‰∏™ËäÇÁÇπÔºåÊØè‰∏™ËäÇÁÇπÈÉΩÂä†‰∏äÂÖ∂ÈÇªÂ±ÖËäÇÁÇπÁöÑ‰ø°ÊÅØÔºå‰πüÂ∞±ÊòØÔºö H^{l+1}=\sigma(AH^lW) WÊòØÂú®ÂØπgridsÂä†ÊùÉ AÊòØÂú®ÂØπÊØè‰∏™gridsÂä†‰∏ä‰ªñÁöÑÈÇªÊé•ËäÇÁÇπ details in practice Ëá™ÁéØÔºö‰øùÁïôËá™Ë∫´ËäÇÁÇπ‰ø°ÊÅØÔºå$\hat A=A+I$ Ê≠£ÂàôÂåñÔºöstabilize the scaleÔºå$H^{l+1}=\sigma(\hat D^{-\frac{1}{2}}\hat A\hat D^{-\frac{1}{2}}H^lW)$ ‰∏Ä‰∏™ÂÆûÈ™åÔºöÂè™Âà©Áî®ÂõæÁöÑÈÇªÊé•Áü©ÈòµÔºåÂ∞±ËÉΩÂ§üÂ≠¶ÂæóÊïàÊûú‰∏çÈîô semi-supervised node classification ÊÄùË∑ØÂ∞±ÊòØÂú®ÊâÄÊúâÊúâÊ†áÁ≠æËäÇÁÇπ‰∏äËÆ°ÁÆó‰∫§ÂèâÁÜµloss Ê®°ÂûãÁªìÊûÑ inputÔºöXÔºå(b,N,D) ‰∏§Â±ÇÂõæÂç∑ÁßØ GCN1-reluÔºöhidden FÔºå(b,N,F) GCN2-softmaxÔºöoutput ZÔºå(b,N,cls) ËÆ°ÁÆó‰∫§ÂèâÁÜµ code torch/keras/tfÂÆòÊñπÈÉΩÊúâÔºö https://github.com/tkipf/gcnÔºåËÆ∫ÊñáÈáåÁªôÁöÑtfËøô‰∏™ÈìæÊé• torchÂíåkerasÁöÑreadmeÈáåÈù¢ÊúâËØ¥ÊòéÔºåinitialization scheme, dropout scheme, and dataset splitsÂíåtfÁâàÊú¨‰∏çÂêåÔºå‰∏çÊòØÁî®Êù•Â§çÁé∞ËÆ∫Êñá python setup.py bdist_wheel Êï∞ÊçÆÈõÜÔºöCora datasetÔºåÊòØ‰∏Ä‰∏™ÂõæÊï∞ÊçÆÈõÜÔºåÁî®‰∫éÂàÜÁ±ª‰ªªÂä°ÔºåÊï∞ÊçÆÈõÜ‰ªãÁªçhttps://blog.csdn.net/yeziand01/article/details/93374216 cora.contentÊòØÊâÄÊúâËÆ∫ÊñáÁöÑÁã¨Ëá™ÁöÑ‰ø°ÊÅØÔºåÊÄªÂÖ±2708‰∏™Ê†∑Êú¨ÔºåÊØè‰∏ÄË°åÈÉΩÊòØËÆ∫ÊñáÁºñÂè∑+ËØçÂêëÈáè1433-dim+ËÆ∫ÊñáÁ±ªÂà´ cora.citesÊòØËÆ∫Êñá‰πãÈó¥ÁöÑÂºïÁî®ËÆ∞ÂΩïÔºåA to BÁöÑreflect pairÔºå5429Ë°åÔºåÁî®‰∫éÂàõÂª∫ÈÇªÊé•Áü©Èòµ]]></content>
      <tags>
        <tag>ÂõæÂç∑ÁßØÔºågraph-conv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[transformers]]></title>
    <url>%2F2021%2F01%2F18%2Ftransformers%2F</url>
    <content type="text"><![CDATA[startupreference1Ôºöhttps://mp.weixin.qq.com/s/Rm899vLhmZ5eCjuy6mW_HA reference2Ôºöhttps://zhuanlan.zhihu.com/p/308301901 NLP &amp; RNN ÊñáÊú¨Ê∂âÂèä‰∏ä‰∏ãÊñáÂÖ≥Á≥ª RNNÊó∂Â∫è‰∏≤Ë°åÔºåÂª∫Á´ãÂâçÂêéÂÖ≥Á≥ª Áº∫ÁÇπÔºöÂØπË∂ÖÈïø‰æùËµñÂÖ≥Á≥ªÂ§±ÊïàÔºå‰∏çÂ•ΩÂπ∂Ë°åÂåñ NLP &amp; CNN ÊñáÊú¨ÊòØ1Áª¥Êó∂Èó¥Â∫èÂàó 1D CNNÔºåÂπ∂Ë°åËÆ°ÁÆó Áº∫ÁÇπÔºöCNNÊìÖÈïøÂ±ÄÈÉ®‰ø°ÊÅØÔºåÂç∑ÁßØÊ†∏Â∞∫ÂØ∏ÂíåÈïøË∑ùÁ¶ª‰æùËµñÁöÑbalance NLP &amp; transformer ÂØπÊµÅÂÖ•ÁöÑÊØè‰∏™ÂçïËØçÔºåÂª∫Á´ãÂÖ∂ÂØπËØçÂ∫ìÁöÑÊùÉÈáçÊò†Â∞ÑÔºåÊùÉÈáç‰ª£Ë°®attention Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ Âª∫Á´ãÈïøË∑ùÁ¶ª‰æùËµñ put in CV ÊèíÂÖ•Á±ª‰ººÁöÑËá™Ê≥®ÊÑèÂäõÂ±Ç ÂÆåÂÖ®ÊäõÂºÉÂç∑ÁßØÂ±ÇÔºå‰ΩøÁî®Transformers RNN &amp; LSTM &amp; GRU cell Ê†áÂáÜË¶ÅÁ¥†ÔºöËæìÂÖ•x„ÄÅËæìÂá∫y„ÄÅÈöêÂ±ÇÁä∂ÊÄÅh RNN RNN cellÊØèÊ¨°Êé•Êî∂‰∏Ä‰∏™ÂΩìÂâçËæìÂÖ•$x_t$ÔºåÂíåÂâç‰∏ÄÊ≠•ÁöÑÈöêÂ±ÇËæìÂá∫$h_{t-1}$ÔºåÁÑ∂Âêé‰∫ßÁîü‰∏Ä‰∏™Êñ∞ÁöÑÈöêÂ±ÇÁä∂ÊÄÅ$h_t$Ôºå‰πüÊòØÂΩìÂâçÁöÑËæìÂá∫$y_t$ formulationÔºö$y_t, h_t = f(x_t, h_{t-1})$ same parameters for each time stepÔºöÂêå‰∏Ä‰∏™cellÊØè‰∏™time stepÁöÑÊùÉÈáçÂÖ±‰∫´ ‰∏Ä‰∏™ÈóÆÈ¢òÔºöÊ¢ØÂ∫¶Ê∂àÂ§±/ÁàÜÁÇ∏ ËÄÉËôëhidden states‚Äô chainÁöÑÁÆÄÂåñÂΩ¢ÂºèÔºö$h_t = \theta^t h_0$Ôºå‰∏Ä‰∏™sequence forward‰∏ãÂéªÂ∞±ÊòØsame weights multiplied over and over again Âè¶Â§ñtanh‰πüÊòØ‰ºöËÆ©Á•ûÁªèÂÖÉÊ¢ØÂ∫¶Ê∂àÂ§±/ÁàÜÁÇ∏ LSTM key ingredient cellÔºöÂ¢ûÂä†‰∫Ü‰∏ÄÊù°cell state workflowÔºå‰ºòÂåñÊ¢ØÂ∫¶ÊµÅ gateÔºöÈÄöËøáÈó®ÁªìÊûÑÂà†ÈÄâÊê∫Â∏¶‰ø°ÊÅØÔºå‰ºòÂåñÈïøË∑ùÁ¶ªÂÖ≥ËÅî ÂèØ‰ª•ÁúãÂà∞LSTMÁöÑÂæ™ÁéØÁä∂ÊÄÅÊúâ‰∏§‰∏™ÔºöÁªÜËÉûÁä∂ÊÄÅ$c_t$ÂíåÈöêÂ±ÇÁä∂ÊÄÅ$h_t$ÔºåËæìÂá∫ÁöÑ$y_t$‰ªçÊóßÊòØ$h_t$ GRU LSTMÁöÑÂèò‰ΩìÔºå‰ªçÊóßÊòØÈó®ÁªìÊûÑÔºåÊØîLSTMÁªìÊûÑÁÆÄÂçïÔºåÂèÇÊï∞ÈáèÂ∞èÔºåÊçÆËØ¥Êõ¥Â•ΩËÆ≠ÁªÉ papers [‰∏Ä‰∏™Âàó‰∫ÜÂæàÂ§öËÆ∫ÊñáÁöÑ‰∏ªÈ°µ] https://github.com/dk-liang/Awesome-Visual-Transformer [ÁªèÂÖ∏ËÄÉÂè§] ‚Äã * [Seq2Seq 2014] Sequence to Sequence Learning with Neural NetworksÔºåGoogleÔºåÊúÄÊó©ÁöÑencoder-decoder stacking LSTMÁî®‰∫éÊú∫Áøª ‚Äã * [self-attention/Transformer 2017] Transformer: Attention Is All You NeedÔºåGoogleÔºå ‚Äã * [bert 2019] BERT: Pre-training of Deep Bidirectional Transformers for Language UnderstandingÔºåGoogleÔºåNLPÔºåËæìÂÖ•single sentence/patched sentencesÔºåÁî®Transformer encoderÊèêÂèñbidirectional cross sentence representationÔºåÁî®ËæìÂá∫ÁöÑÁ¨¨‰∏Ä‰∏™logitËøõË°åÂàÜÁ±ª [ÁªºËø∞] ‚Äã * [ÁªºËø∞2020] Efficient Transformers: A SurveyÔºåGoogleÔºå ‚Äã * [ÁªºËø∞2021] Transformers in Vision: A SurveyÔºåËø™ÊãúÔºå ‚Äã * [ÁªºËø∞2021] A Survey on Visual TransformerÔºåÂçé‰∏∫Ôºå [classification] ‚Äã * [ViT 2020] AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALEÔºåGoogleÔºåÂàÜÁ±ª‰ªªÂä°ÔºåÁî®transformerÁöÑencoderÊõøÊç¢CNNÂÜçÂä†ÂàÜÁ±ªÂ§¥ÔºåÊØè‰∏™feature patch‰Ωú‰∏∫‰∏Ä‰∏™input embeddingÔºåchannel dimÊòØvector dimÔºåÂèØ‰ª•ÁúãÂà∞Ë∑übertÂü∫Êú¨‰∏ÄÊ†∑ÔºåÂ∞±ÊòØinput sequenceÊç¢ÊàêpatchÔºåÂêéÁª≠Âü∫‰∫éÂÆÉÁöÑÊèêÂçáÊúâDeiT„ÄÅLV-ViT ‚Äã * [BotNet 2021] Bottleneck Transformers for Visual RecognitionÔºåGoogleÔºåÂ∞ÜCNN backboneÊúÄÂêéÂá†‰∏™stageÊõøÊç¢ÊàêMSA ‚Äã * [CvT 2021] CvT: Introducing Convolutions to Vision TransformersÔºåÂæÆËΩØÔºå [detection] ‚Äã * [DeTR 2020] DeTR: End-to-End Object Detection with TransformersÔºåFacebookÔºåÁõÆÊ†áÊ£ÄÊµãÔºåCNN+transformer(en-de)+È¢ÑÊµãÂ§¥ÔºåÊØè‰∏™feature pixel‰Ωú‰∏∫‰∏Ä‰∏™input embeddingÔºåchannel dimÊòØvector dim ‚Äã * [Swin 2021] Swin Transformer: Hierarchical Vision Transformer using Shifted WindowsÔºåÂæÆËΩØ [segmentation] ‚Äã * [SETR] Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with TransformersÔºåÂ§çÊó¶ÔºåÊ∞¥ÔºåÊÑüËßâÂ∞±ÊòØÊääFCNÁöÑbackÊç¢Êàêtransformer [Unet+Transformer]Ôºö ‚Äã * [UNETR 2021] UNETR: Transformers for 3D Medical Image SegmentationÔºåËã±‰ºüËææÔºåÁõ¥Êé•‰ΩøÁî®transformer encoderÂÅöunet encoder ‚Äã * [TransUNet 2021] TransUNet: Transformers Make Strong Encoders for Medical Image SegmentationÔºåencoder streamÈáåÈù¢Âä†transformer block ‚Äã * [TransFuse 2021] TransFuse: Fusing Transformers and CNNs for Medical Image SegmentationÔºåÂ§ßÂ≠¶ÔºåCNN featureÂíåTransformer featureËøõË°åbifusion Sequence to Sequence [a keras tutorial][https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html] general case extract the information of the entire input sequence then start generate the output sequence seq2seq model workflow a (stacking of) RNN layer acts as encoder processes the input sequence returns its own internal stateÔºö‰∏çË¶ÅRNNÁöÑoutputsÔºåÂè™Ë¶Åinternal states encoderÁºñÁ†ÅÂæóÂà∞ÁöÑ‰∏úË•øÂè´Context Vector a (stacking of) RNN layer acts as decoder given previous characters of the target sequence it is trained to predict the next characters of the target sequence teacher forcingÔºö ËæìÂÖ•ÊòØtarget sequenceÔºåËÆ≠ÁªÉÁõÆÊ†áÊòØ‰ΩøÊ®°ÂûãËæìÂá∫offset by one timestepÁöÑtarget sequence ‰πüÂèØ‰ª•‰∏çteacher forcingÔºöÁõ¥Êé•ÊääÈ¢ÑÊµã‰Ωú‰∏∫next stepÁöÑËæìÂÖ• Context VectorÁöÑÂêåË¥®ÊÄßÔºöÊØè‰∏™stepÔºådecoderÈÉΩËØªÂèñ‰∏ÄÊ†∑ÁöÑContext Vector‰Ωú‰∏∫initial_state when inference Á¨¨‰∏ÄÊ≠•Ëé∑Âèñinput sequenceÁöÑstate vectors repeat ÁªôdecoderËæìÂÖ•input statesÂíåout sequence(begin with a Ëµ∑ÂßãÁ¨¶) ‰ªéprediction‰∏≠ÊãøÂà∞next character append the character to the output sequence untilÔºöÂæóÂà∞end character / hit the character limit implementation https://github.com/AmberzzZZ/transformer/blob/master/seq2seq.py one step further ÊîπËøõÊñπÂêë bi-directional RNNÔºöÁ≤óÊö¥ÂèçËΩ¨Â∫èÂàóÔºåÊúâÊïàÊ∂®ÁÇπ attentionÔºöÊú¨Ë¥®ÊòØÂ∞ÜencoderÁöÑËæìÂá∫Context VectorÂä†ÊùÉ ConvS2SÔºöËøòÊ≤°Áúã ‰∏ªË¶ÅÈÉΩÊòØÈíàÂØπRNNÁöÑÁº∫Èô∑ÊèêÂá∫ Âä®Êú∫ present a general end-to-end sequence learning approach multi-layered LSTMs encode the input seq to a fix-dim vector decode the target seq from the fix-dim vector LSTM did not have difficulty on long sentences reversing the order of the words improved performance ÊñπÊ≥ï standard RNN given a sequence $(x_1, x_2, ‚Ä¶, x_T)$ iteratingÔºö h_t = sigm(W^{hx} x_t + W^{hh}h_{t-1})\\ y_t = W^{yh} h_t Â¶ÇÊûúËæìÂÖ•„ÄÅËæìÂá∫ÁöÑÈïøÂ∫¶‰∫ãÂÖàÂ∑≤Áü•‰∏îÂõ∫ÂÆöÔºå‰∏Ä‰∏™RNNÁΩëÁªúÂ∞±ËÉΩÂª∫Ê®°seq2seq model‰∫Ü Â¶ÇÊûúËæìÂÖ•„ÄÅËæìÂá∫ÁöÑÈïøÂ∫¶‰∏çÂêå„ÄÅÂπ∂‰∏îÊúç‰ªé‰∏Ä‰∫õÊõ¥Â§çÊùÇÁöÑÂÖ≥Á≥ªÔºüÂ∞±ÂæóÁî®‰∏§‰∏™RNNÁΩëÁªúÔºå‰∏Ä‰∏™Â∞Üinput seqÊò†Â∞ÑÊàêfixed-sized vectorÔºåÂè¶‰∏Ä‰∏™Â∞ÜvectorÊò†Â∞ÑÊàêoutput seqÔºåbut long-term-dependency issue LSTM LSTMÊòØÂßãÁªàÂ∏¶ÁùÄÂÖ®ÈÉ®seqÁöÑ‰ø°ÊÅØÁöÑÔºåÂ¶Ç‰∏äÂõæÈÇ£Ê†∑ our actual model use two LSTMsÔºöencoder-decoderËÉΩÂ§üÂ¢ûÂä†ÂèÇÊï∞Èáè an LSTM with four layersÔºödeeper input sequenceÂÄíÂ∫èÔºöÁúüÊ≠£ÁöÑÂè•È¶ñÊõ¥Êé•ËøëtransÁöÑÂè•È¶ñÔºåmakes it easy for SGD to establish communication training details LSTMÔºö4 layersÔºå1000 cells word-embeddingÔºö1000-dimÔºå(input vocab 160,000, output vocab 80,000) naive softmax uniform initializationÔºö(-0.08, 0.08) SGDÔºålr=0.7Ôºåhalf by every half epochÔºåtotal 7.5 epochs gradient norm [10, 25] all sentences in a minibatch are roughly of the same length Transformer: Attention Is All You Need Âä®Êú∫ sequence2sequence models encoder + decoder RNN / CNN + an attention path we propose Transformer base solely on attention mechanisms more parallelizable and less training time ËÆ∫ÁÇπ sequence modeling ‰∏ªÊµÅÔºöRNNÔºåLSTMÔºågated align the positions to computing time steps sequentialÊú¨Ë¥®ÈòªÁ¢çÂπ∂Ë°åÂåñ Attention mechanisms acts as a integral part in previous work used in conjunction with the RNN ‰∏∫‰∫ÜÂπ∂Ë°åÂåñ some methods use CNN as basic building blocks difficult to learn dependencies between distant positions we propose Transformer rely entirely on an attention mechanism draw global dependencies self-attention relating different positions of a single sequence to generate a overall representation of the sequence ÊñπÊ≥ï encoder-decoder encoderÔºödoc2emb given an input sequence of symbol representation $(x_1, x_2, ‚Ä¶, x_n)$ map to a sequence of continuous representations $(z_1, z_2, ‚Ä¶, z_n)$Ôºå(embeddings) decoderÔºöhidden layers given embeddings z generate an output sequence $(y_1, y_2, ‚Ä¶, y_m)$ one element at a time the previous generated symbols are served as additional input when computing the current time step Transformer Architecture Transformer use for both encoder and decoder stacked self-attention and point-wise fully-connected layers encoder N=6 identical layers each layer has 2 sub-layers multi-head self-attention mechanism postision-wise fully connected layer residual for two sub-layers independently add &amp; layer norm d=512 decoder N=6 identical layers 3 sub-layers [new] masked multi-head self-attentionÔºöcombine‰∫ÜÂÖàÈ™åÁü•ËØÜÔºåoutput embeddingÂè™ËÉΩÂü∫‰∫éÂú®ÂÆÉ‰πãÂâçÁöÑtime-stepÁöÑembeddingËÆ°ÁÆó multi-head self-attention mechanism postision-wise fully connected layer residual attention referenceÔºöhttps://bbs.cvmart.net/articles/4032 step1Ôºöproject embedding to query-key-value pairs $Q = W_Q^{dd} A^{dN}$ $K = W_K^{dd} A^{dN}$ $V = W_V^{dd} A^{dN}$ step2Ôºöscaled dot-product attention $A^{N*N}=softmax(K^TQ/\sqrt{d})$ $B^{dN} = V^{dN}A^{N*N}$ multi-head attention ‰ª•‰∏äÁöÑstep1&amp;step2Êìç‰Ωúperforms a single attention function ‰∫ãÂÆû‰∏äÊàë‰ª¨ÂèØ‰ª•Áî®Â§öÁªÑprojectionÂæóÂà∞Â§öÁªÑ$\{Q,K,V\}^h$Ôºåin parallelÂú∞ÊâßË°åattentionËøêÁÆóÔºåÂæóÂà∞Â§öÁªÑ$\{B^{d*N}\}^h$ concat &amp; project concat in d-dimÔºö$B\in R^{(dh)N}$ linear projectÔºö$out = W^{d(dh)} B$ h=8 $d_{in}/h=64$ÔºöembeddingÁöÑdim $d_{out}=64$Ôºöquery-key-valueÁöÑdim positional encoding Êï∞Â≠¶Êú¨Ë¥®ÊòØ‰∏Ä‰∏™hand-craftedÁöÑÊò†Â∞ÑÁü©Èòµ$W^P$Âíåone-hotÁöÑÁºñÁ†ÅÂêëÈáè$p$Ôºö \left[ \begin{array}{ccc} a\\ e \end{array} \right ] = [W^I, W^P] \left[ \begin{array}{ccc} x\\ p \end{array} \right ] Áî®PEË°®Á§∫e posÊòØsequence x‰∏äÁöÑposition 2iÂíå2i+1ÊòØembedding a‰∏äÁöÑidx point-wise feed-forward network fc-ReLU-fc dim_fc=2048 dim_in &amp; dim_out = 512 ËøêË°åËøáÁ®ã encoderÊòØÂèØ‰ª•Âπ∂Ë°åËÆ°ÁÆóÁöÑ ËæìÂÖ•ÊòØsequence embeddingÂíåpositional embeddingÔºö$A\in R^{d*N}$ ÁªèËøárepeated blocks ËæìÂá∫ÊòØÂè¶Â§ñ‰∏Ä‰∏™sequenceÔºö$B\in R^{d*N}$ self-attentionÔºöQ„ÄÅK„ÄÅVÊòØ‰∏Ä‰∏™‰∏úË•ø encoderÁöÑÊú¨Ë¥®Â∞±ÊòØÂú®Ëß£ÊûêËá™Ê≥®ÊÑèÂäõÔºö Âπ∂Ë°åÁöÑÂÖ®Â±Ä‰∏§‰∏§ÊØîËæÉÔºå‰∏ÄÊ≠•Âà∞‰Ωç RNNË¶Åby step CNNË¶Åstack layers decoderÊòØÂú®ËÆ≠ÁªÉÈò∂ÊÆµÊòØÂèØ‰ª•Âπ∂Ë°åÁöÑÔºåÂú®inferenceÈò∂ÊÆµby step ËæìÂÖ•ÊòØencoderÁöÑËæìÂá∫Âíå‰∏ä‰∏Ä‰∏™time-step decoderÁöÑËæìÂá∫embedding ËæìÂá∫ÊòØÂΩìÂâçtime-stepÂØπÂ∫îpositionÁöÑËæìÂá∫ËØçÁöÑÊ¶ÇÁéá Á¨¨‰∏Ä‰∏™attention layerÊòØout embeddingÁöÑself-attentionÔºöË¶ÅÂÆûÁé∞ÂÉèRNN‰∏ÄÊ†∑‰æùÊ¨°Ëß£Á†ÅÂá∫Êù•ÔºåÊØè‰∏™time stepË¶ÅÁî®Âà∞‰∏ä‰∏Ä‰∏™‰ΩçÁΩÆÁöÑËæìÂá∫‰Ωú‰∏∫ËæìÂÖ•‚Äî‚Äîmasking givenËæìÂÖ•sequenceÊòØ\ I have a catÔºå5‰∏™ÂÖÉÁ¥† ÈÇ£‰πàmaskÂ∞±ÊòØ$R^{5*5}$ÁöÑ‰∏ã‰∏âËßíÁü©Èòµ ËæìÂÖ•embeddingÁªèËøátransformationÂèòÊàêQ„ÄÅK„ÄÅV‰∏â‰∏™Áü©Èòµ ‰ªçÊóßÊòØ$A=K^TQ$ËÆ°ÁÆóattention ËøôÈáåÊúâ‰∏Ä‰∫õattentionÊòØÈùûÊ≥ïÁöÑÔºö‰ΩçÁΩÆÈù†ÂâçÁöÑqueryÂè™ËÉΩÁî®Âà∞ÊØî‰ªñ‰ΩçÁΩÆÊõ¥Èù†ÂâçÁöÑqueryÔºåÂõ†Ê≠§Ë¶Å‰πò‰∏ämaskÁü©ÈòµÔºö$A=M A$ softmaxÔºö$A=softmax(A)$ scaleÔºö$B = VA$ concat &amp; projection Á¨¨‰∫å‰∏™attention layerÊòØin &amp; out sequenceÁöÑÊ≥®ÊÑèÂäõÔºåÂÖ∂keyÂíåvalueÊù•Ëá™encoderÔºåqueryÊù•Ëá™‰∏ä‰∏Ä‰∏™decoder blockÁöÑËæìÂá∫ why self-attention Ë°°ÈáèÁª¥Â∫¶ total computational complexity per layer amount of computation that can be parallelized path-length between long-range dependencies given input sequence with length N &amp; dim $d_{in}$Ôºåoutput sequence with dim $d_{out}$ RNN need N sequencial operations of $W\in R^{d_{in} * d_{out}}$ CNN need N/k stacking layers of $d_{in}d_{out}$ sequence operations of $W\in R^{kk}$ÔºågenerallyÊòØRNNÁöÑkÂÄç training optimizerÔºö$Adam(lr, \beta_1=0.9, \beta_2=0.98, \epsilon=10^{-9})$ lrscheduleÔºöwarmup by 4000 stepsÔºåthen decay dropout residual dropoutÔºöÂ∞±ÊòØstochastic depth dropout to the sum of embeddings &amp; PE for both encoder and decoder drop_rate = 0.1 label smoothingÔºösmooth_factor = 0.1 ÂÆûÈ™å AÔºövary the number of attention headsÔºåÂèëÁé∞Â§ö‰∫ÜÂ∞ë‰∫ÜÈÉΩhurts BÔºöreduce the dim of attention keyÔºåÂèëÁé∞hurts C &amp; DÔºöÂ§ßÊ®°Âûã+dropout helps EÔºölearnable &amp; sincos PEÔºönearly identical ÊúÄÂêéÊòØbig modelÁöÑÂèÇÊï∞ BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Âä®Êú∫ BERTÔºöBidirectional Encoder Representations from Transformers Bidirectional Encoder Representations Transformers workflow pretrain bidirectional representations from unlabeled text tune with one additional output layer to obtain the model SOTA GLUE score 80.5% ËÆ∫ÁÇπ pretraining is effective in NLP tasks feature-based methodÔºöuse task-specfic architecturesÔºå‰ªÖ‰ΩøÁî®pretrained modelÁöÑÁâπÂæÅ fine-tuining methodÔºöÁõ¥Êé•fine-tuneÈ¢ÑËÆ≠ÁªÉÊ®°Âûã ‰∏§ÁßçÊñπÊ≥ïÂú®È¢ÑËÆ≠ÁªÉÈò∂ÊÆµËÆ≠ÁªÉÁõÆÊ†á‰∏ÄËá¥Ôºöuse unidirectional language models to learn general language representations reduce the need for many heavily-engineered task- specific architectures current methods‚Äô limitations unidirectionalÔºö limit the choice of architectures ‰∫ãÂÆû‰∏ätokenÁöÑ‰∏ä‰∏ãÊñáÈÉΩÂæàÈáçË¶ÅÔºå‰∏çËÉΩÂè™Áúã‰∏äÊñá ÁÆÄÂçïÁöÑconcat‰∏§‰∏™independentÁöÑL2RÂíåR2LÊ®°ÂûãÔºàbiRNNÔºâ independent shallow concat BERT masked language modelÔºöÂú®‰∏Ä‰∏™sequence‰∏≠È¢ÑÊµãË¢´ÈÅÆÊå°ÁöÑËØç next sentence predictionÔºötrains text-pair representations ÊñπÊ≥ï two steps pre-training unlabeled data different pretraining tasks fine-tuning labeled data of the downstream tasks fine-tune all the params ‰∏§‰∏™Èò∂ÊÆµÁöÑÊ®°ÂûãÔºåÂè™ÊúâËæìÂá∫Â±Ç‰∏çÂêå ‰æãÂ¶ÇÈóÆÁ≠îÊ®°Âûã pretrainingÈò∂ÊÆµÔºåËæìÂÖ•ÊòØ‰∏§‰∏™sentenceÔºåËæìÂÖ•ÁöÑËµ∑ÂßãÊúâ‰∏Ä‰∏™CLS symbolÔºå‰∏§‰∏™Âè•Â≠êÁöÑÂàÜÈöîÊúâ‰∏Ä‰∏™SEP symbol fine-tuningÈò∂ÊÆµÔºåËæìÂÖ•ÂàÜÂà´ÊòØÈóÆÂíåÁ≠îÔºå„ÄêËæìÂá∫ÊòØÂï•Ôºü„Äë architecture multi-layer bidirectional Transformer encoder number of transfomer blocks L hidden size H number of self-attention heads A FFN dim 4H Bert baseÔºöL=12ÔºåH=768ÔºåA=12 Bert largeÔºöL=24ÔºåH=1024ÔºåA=16 input/output representations a single sentence / two packed up sentenceÔºö ÊãºÊé•ÁöÑsentenceÁî®ÁâπÊÆätoken SEPË°îÊé• segment embeddingÔºöÂêåÊó∂add a learned embedding to every token indicating who it belongs use WordPiece embeddings with 30000 token vocabulary ËæìÂÖ•sequenceÁöÑÁ¨¨‰∏Ä‰∏™tokenÊ∞∏ËøúÊòØ‰∏Ä‰∏™ÁâπÊÆäÁ¨¶Âè∑CLSÔºåÂÆÉÂØπÂ∫îÁöÑfinal stateËæìÂá∫‰Ωú‰∏∫sentenceÊï¥‰ΩìÁöÑrepresentationÔºåÁî®‰∫éÂàÜÁ±ª‰ªªÂä° overallÁΩëÁªúÁöÑinput representationÊòØÈÄöËøáÂ∞Ütoken embeddingsÊãºÊé•‰∏ä‰∏äÁâπÊÆäÁ¨¶Âè∑ÔºåÂä†‰∏äSEÂíåPEÂæóÂà∞ pre-training two unsupervised tasks Masked LM (MLM) mask some percentage of the input tokens at randomÔºö15% 80%ÁöÑÊ¶ÇÁéáÁî®MASK tokenÊõøÊç¢ 10%ÁöÑÊ¶ÇÁéáÁî®random tokenÊõøÊç¢ 10%ÁöÑÊ¶ÇÁéáunchanged then predict those masked tokens the final hidden states corresponding to the masked tokens are fed into a softmax Áõ∏ÊØîËæÉ‰∫é‰º†ÁªüÁöÑleft2right/right2left/concatÊ®°Âûã Êó¢ÊúâÂâçÊñáÂèàÊúâÂêéÊñá Âè™È¢ÑÊµãmasked tokenÔºåËÄå‰∏çÊòØÂÖ®Âè•È¢ÑÊµã Next Sentence Prediction (NSP) ÂØπ‰∫érelationship between sentencesÔºö ‰æãÂ¶Çquestion&amp;answerÔºåÂè•Â≠êÊé®Êñ≠ not direatly captured by language modelingÔºåÊ®°ÂûãÁõ¥ËßÇÂ≠¶‰π†ÁöÑÊòØtoken relationship binarized next sentence prediction task ÈÄâÂèñsentence A&amp;BÔºö 50%ÁöÑÊ¶ÇÁéáÊòØÁúüÁöÑ‰∏ä‰∏ãÊñáÔºàIsNextÔºâ 50%ÁöÑÊ¶ÇÁéáÊòØrandomÔºàNotNextÔºâ ÊûÑÊàê‰∫Ü‰∏Ä‰∏™‰∫åÂàÜÁ±ªÈóÆÈ¢òÔºö‰ªçÊóßÁî®CLS tokenÂØπÂ∫îÁöÑhidden state CÊù•È¢ÑÊµã fine-tuning BERTÂÖºÂÆπmany downstream tasksÔºösingle text or text pairs Áõ¥Êé•ÁªÑÂ•ΩËæìÂÖ•Ôºåend-to-end fine-tuningÂ∞±Ë°å ËæìÂá∫ËøòÊòØÁî®CLS tokenÂØπÂ∫îÁöÑhidden state CÊù•È¢ÑÊµãÔºåÊé•ÂàÜÁ±ªÂ§¥ A Survey on Visual Transformer Âä®Êú∫ provide a comprehensive overview of the recent advances in visual transformers discuss the potential directions for further improvement develop timeline ÊåâÁÖßÂ∫îÁî®Âú∫ÊôØÂàÜÁ±ª backboneÔºöÂàÜÁ±ª high/mid-level visionÔºöÈÄöÂ∏∏ÊòØËØ≠‰πâÁõ∏ÂÖ≥ÁöÑÔºåÊ£ÄÊµã/ÂàÜÂâ≤/ÂßøÊÄÅ‰º∞ËÆ° low-level visionÔºöÂØπÂõæÂÉèÊú¨Ë∫´ËøõË°åÊìç‰ΩúÔºåË∂ÖÂàÜ/ÂõæÂÉèÁîüÊàêÔºåÁõÆÂâçÂ∫îÁî®ËæÉÂ∞ë video processing revisiting transformer key-conceptsÔºösentence„ÄÅembedding„ÄÅpositional encoding„ÄÅencoder„ÄÅdecoder„ÄÅself-attention layer„ÄÅencoder-decoder attention layer„ÄÅmulti-head attention„ÄÅfeed-forward neural network self-attention layer input vector is transformed into 3 vectors input vector is embedding+PE(pos,i)ÔºöposÊòØwordÂú®sequence‰∏≠ÁöÑ‰ΩçÁΩÆÔºåiÊòØPE-elementÂú®embedding vec‰∏≠ÁöÑ‰ΩçÁΩÆ query vec q key vec k value vec v $d_q = d_k = d_v = d_{model} = 512$ then calculateÔºö$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$ encoder-decoder attention layer KÂíåVÊòØ‰ªéencoder‰∏≠ÊãøÂà∞ QÊòØ‰ªéÂâç‰∏ÄÂ±ÇÊãøÂà∞ ËÆ°ÁÆóÊòØÁõ∏‰ººÁöÑ multi-head attention ‰∏Ä‰∏™attentionÊòØ‰∏Ä‰∏™softmaxÔºåÂØπÂ∫î‰∫Ü‰∏ÄÂØπÂº∫Áõ∏ÂÖ≥ÔºåÂêåÊó∂ÊäëÂà∂‰∫ÜÂÖ∂‰ªñwordÁöÑÁõ∏ÂÖ≥ÊÄß ËÄÉËôë‰∏Ä‰∏™ËØçÂæÄÂæÄ‰∏éÂá†‰∏™ËØçÂº∫Áõ∏ÂÖ≥ÔºåËøôÂ∞±ÈúÄË¶ÅÂ§ö‰∏™attention multi-headÔºödifferent QKV matrices are used for different heads given a input vectorÔºåthe number of heads h ÂÖà‰∫ßÁîüh‰∏™ pairs $d_q=d_k=d_v=d_{model}/h=64$ Ëøôh‰∏™pairÔºåÂàÜÂà´ËÆ°ÁÆóattention vectorÔºåÂæóÂà∞h‰∏™[b,d]ÁöÑcontext vector concat along-d-axis and linear projection to final [b,d] vector residual &amp; layer-normÔºölayer-normÂú®residual-add‰ª•Âêé feed-forward network fc-GeLU-fc $d_h=2048$ final-layer in decoder dense+softmax $d_{words}=$ number of words in the vocabulary when applied in CV tasks most transformers adopt the original transformer‚Äôs encoder module used as a feature selector Áõ∏ÊØîËæÉ‰∫éCNNÔºåËÉΩÂ§ücapture long-distance characteristicsÔºåderive global information Áõ∏ÊØîËæÉ‰∫éRNNÔºåËÉΩÂ§üÂπ∂Ë°åËÆ°ÁÆó ËÆ°ÁÆóÈáè È¶ñÂÖàÊòØ‰∏â‰∏™Á∫øÊÄßÂ±ÇÔºöÁ∫øÊÄßÊó∂Èó¥Â§çÊùÇÂ∫¶O(n)ÔºåËÆ°ÁÆóÈáè‰∏é$d_{model}$ÊàêÊ≠£ÊØî ÁÑ∂ÂêéÊòØself-attentionÂ±ÇÔºöQKVÁü©Èòµ‰πòÊ≥ïËøêÁÆóÔºåÂπ≥ÊñπÊó∂Èó¥Â§çÊùÇÂ∫¶O(n^2) multi-headÁöÑËØùÔºåËøòÊúâ‰∏Ä‰∏™Á∫øÊÄßÂ±ÇÔºöÂπ≥ÊñπÊó∂Èó¥Â§çÊùÇÂ∫¶O(n^2) revisiting transformers for NLP ÊúÄÊó©ÊúüÁöÑRNN + attentionÔºörnnÁöÑsequentialÊú¨Ë¥®ÂΩ±Âìç‰∫ÜÈïøË∑ùÁ¶ª/Âπ∂Ë°åÂåñ/Â§ßÊ®°Âûã transformerÁöÑsolely attentionÁªìÊûÑÔºöËß£ÂÜ≥‰ª•‰∏äÈóÆÈ¢òÔºå‰øÉËøõ‰∫Ülarge pre-trained models (PTMs) for NLP BERT and its variants are a series of PTMs built on the multi-layer transformer encoder architecture pre-trained Masked language modeling Next sentence prediction fine-tuned add an output layer Generative Pre-trained Transformer models (GPT) are another type of PTMs based on the transformer decoder architecture masked self-attention mechanisms pre-trained ‰∏éBERTÊúÄÂ§ßÁöÑ‰∏çÂêåÊòØÊúâÂêëÊÄß visual transformer „Äêcategory1„Äë: backbone for image classification transformerÁöÑËæìÂÖ•ÊòØtokensÔºåÂú®NLPÈáåÊòØembeddingÂΩ¢ÂºèÁöÑÂàÜËØçÂ∫èÂàóÔºåÂú®CVÈáåÂ∞±ÊòØrepresenting a certain semantic conceptÁöÑvisual token visual tokenÂèØ‰ª•Êù•Ëá™CNNÁöÑfeature ‰πüÂèØ‰ª•Áõ¥Êé•Êù•Ëá™imageÁöÑÂ∞èpatch purely use transformerÊù•ÂÅöimage classification‰ªªÂä°ÁöÑÊ®°ÂûãÊúâiGPT„ÄÅViT„ÄÅDeiT iGPT pretraining stage + finetuning stage pre-training stage self-supervisedÔºöËá™ÁõëÁù£ÔºåÊâÄ‰ª•ÁªìÊûúËæÉÂ∑Æ given an unlabeled dataset train the model by minimizing the -log(density)ÔºåÊÑüËßâÊòØÂú®forceÂÖâÊ†ÖÊéíÂ∫èÊ≠£Á°Æ fine-tuning stage average pool + fc + softmax jointly train with L_gen &amp; L_CE ViT pre-trained on large datasets standard transformer‚Äôs encoder + MLP head treats all patches equally Êúâ‰∏Ä‰∏™Á±ª‰ººBERT class tokenÁöÑ‰∏úË•ø ‰ªéËÆ≠ÁªÉÁöÑËßíÂ∫¶Ôºågather knowledge of the entire class inferenceÁöÑÊó∂ÂÄôÔºåÂè™Êãø‰∫ÜËøôÁ¨¨‰∏Ä‰∏™logitÁî®Êù•ÂÅöÈ¢ÑÊµã fine-tuning Êç¢‰∏Ä‰∏™zero-initializedÁöÑMLP head use higher resolution &amp; ÊèíÂÄºpe DeiT Data-efficient image transformer better performance with a more cautious training strategy and a token-based distillation „Äêcategory2„Äë: High/Mid-level Vision „Äêcategory3„Äë: Low-level Vision „Äêcategory4„Äë: Video Processing efficient transformerÔºöÁò¶Ë∫´&amp;Âä†ÈÄü Pruning and Decomposition Knowledge Distillation Quantization Compact Architecture Design ViT: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE Âä®Êú∫ attention in vision either in conjunction with CNN or replace certain part of a CNN overallÈÉΩËøòÊòØCNN-based use a pure transformer to sequence of image patches verified on image classification tasks in supervised fashion ËÆ∫ÁÇπ transformer lack some inductive biases inherent to CNNsÔºåÊâÄ‰ª•Âú®insufficient data‰∏änot generalize well however large scale training trumps inductive biasÔºåÂ§ßÊï∞ÊçÆÈõÜ‰∏äViTÊõ¥Â•Ω naive application of self-attention Âª∫Á´ãpixel‰πãÈó¥ÁöÑ‰∏§‰∏§ÂÖ≥ËÅîÔºöËÆ°ÁÆóÈáèÂ§™Â§ß‰∫Ü ÈúÄË¶ÅapproximationÔºölocal/ÊîπÂèòsize we use transformer wih global self-attention to full-sized images ÊñπÊ≥ï input 1D-embedding sequence Â∞Üimage $x\in R^{HWC}$ Â±ïÂºÄÊàêpatches $\{x_p \in R^{P^2C}\}$ thus sequence length $N=HW/P^2$ patch embeddingÔºö use a trainable linear projection fixed dimension size through-all position embeddingÔºö add to patch embedding standard learnable 1D position embedding prepended embeddingÔºö ÂâçÁΩÆÁöÑlearnable embedding $x_{class}$ similar to BERT‚Äôs class token ‰ª•‰∏ä‰∏â‰∏™embeddingÁªÑÂêàËµ∑Êù•Ôºå‰Ωú‰∏∫ËæìÂÖ•sequence transformer encoder follow the original Transformer ‰∫§ÊõøÁöÑMSAÂíåMLP layer norm LN residual GELU hybrid architecture input sequence‰πüÂèØ‰ª•Êù•Ê∫ê‰∫éCNNÁöÑfeature maps patch sizeÂèØ‰ª•ÊòØ1x1 classification head attached to $z_L^0$ÔºöÊòØclass tokenÁî®Êù•ÂÅöÈ¢ÑÊµã pre-trainingÁöÑÊó∂ÂÄôÊòØMLP fine-tuningÁöÑÊó∂ÂÄôÊç¢‰∏Ä‰∏™zero-initializedÁöÑsingle linear layer workflow typicallyÂÖàpre-train on large datasets ÂÜçfine-tune to downstream tasks fine-tuneÁöÑÊó∂ÂÄôÊõøÊç¢‰∏Ä‰∏™zero-initializedÁöÑÊñ∞Á∫øÊÄßÂàÜÁ±ªÂ§¥ when feeding images with higher resolution keep the patch size results in larger sequence length ËøôÊó∂ÂÄôpre-trained PEÂ∞±no longer meaningful‰∫Ü we therefore perform 2D interpolationÂü∫‰∫éÂÆÉÂú®ÂéüÂõæ‰∏äÁöÑ‰ΩçÁΩÆ training details AdamÔºö$\beta_1=0.9Ôºå\beta_2=0.999$ batch size 4096 high weight decay 0.1 linear lr warmup &amp; decay fine-tuning details SGDM cosine LR no weight decay „ÄêÔºüÔºüÔºüÔºü„Äëaverage 0.9999 Swin Transformer: Hierarchical Vision Transformer using Shifted Windows Âä®Êú∫ use Transformer as visual tasks‚Äô backbone challenges of Transformer in vision domain large variations of scales of the visual entities high resolution of pixels we propose hierarchical Transformer shifted windows self-attention in local windows cross-window connection verified on classificationÔºöImageNet top1 acc 86.4 detectionÔºöCOCO box-MAP 58.7 segmentationÔºöADE20K ËÆ∫ÁÇπ when transfer Transformer‚Äôs high performance in NLP domain to CV domain differences between the two modalities scaleÔºöNLPÈáåÈù¢Ôºåword tokens serves as the basic elementÔºå‰ΩÜÊòØCVÈáåÈù¢ÔºåpatchÁöÑÂΩ¢ÊÄÅÂ§ßÂ∞èÈÉΩÊòØÂèØÂèòÁöÑÔºåprevious methodsÈáåÈù¢ÔºåÈÉΩÊòØÁªü‰∏ÄËÆæÂÆöÂõ∫ÂÆöÂ§ßÂ∞èÁöÑpatch token resolutionÔºö‰∏ªË¶ÅÈóÆÈ¢òÂ∞±ÊòØself-attentionÁöÑËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåÊòØimage sizeÁöÑÂπ≥Êñπ we propose Swin Transformer hierarchial feature maps linear computatoinal complexity to image size hierarchical start from small patches merge in deeper layers ÊâÄ‰ª•ÂØπ‰∏çÂêåÂ∞∫Â∫¶ÁöÑÁâπÂæÅpatchËøõË°å‰∫ÜËûçÂêà linear complexity compute self-attention locally in each window ÊØè‰∏™windowÁöÑnumber of patchesÊòØËÆæÂÆöÂ•ΩÁöÑÔºåwindowÊï∞ÊòØ‰∏éimage sizeÊàêÊ≠£ÊØîÁöÑ ÊâÄ‰ª•ÊòØÁ∫øÊÄß shifted window approach Ë∑®Â±ÇÁöÑwindow shiftÔºåÂª∫Á´ãËµ∑Áõ∏ÈÇªwindowÈó¥ÁöÑÊ°•Ê¢Å „ÄêQUESTION„Äëall query patches within a window share the same key set previous attemptations of Transformer self-attention based backbone architectures Â∞ÜÈÉ®ÂàÜ/ÂÖ®ÈÉ®conv layersÊõøÊç¢Êàêself-attention Ê®°Âûã‰∏ª‰ΩìÊû∂ÊûÑËøòÊòØResNet slightly better acc larger latency caused by self-att self-attention complement CNNs ‰Ωú‰∏∫additional blockÔºåÁªôÂà∞backbone/headÔºåÊèê‰æõÈïøË∑ùÁ¶ª‰ø°ÊÅØ Êúâ‰∫õÊ£ÄÊµã/ÂàÜÂâ≤ÁΩëÁªú‰πüÂºÄÂßãÁî®‰∫ÜtransformerÁöÑencoder-decoderÁªìÊûÑ transformer-based vision backbones ‰∏ªË¶ÅÂ∞±ÊòØViTÂèäÂÖ∂Ë°çÁîüÂìÅ ViT requires large-scale training sets DeiT introduces training strategies ‰ΩÜÊòØËøòÂ≠òÂú®high resolutionËÆ°ÁÆóÈáèÁöÑÈóÆÈ¢ò ÊñπÊ≥ï overview Swin-TÔºötiny version Á¨¨‰∏ÄÊ≠•ÊòØpatch partitionÔºö Â∞ÜRGBÂõæÂàáÊàênon-overlapping patches patchesÔºötokenÔºåbasic element feature input dimÔºöwith patch size 4x4Ôºådim=4x4x3=48 ÁÑ∂ÂêéÊòØlinear embedding layer Â∞Üraw feature re-projectionÂà∞ÊåáÂÆöÁª¥Â∫¶ ÊåáÂÆöÁª¥Â∫¶CÔºödefault=96 Êé•‰∏ãÊù•ÊòØSwin Transformer blocks the number of tokens maintain patch merging layersË¥üË¥£reduce the number of tokens Á¨¨‰∏Ä‰∏™patch merging layer concat ÊâÄÊúâ2x2ÁöÑneighbor patchesÔºö4C-dim vec each ÁÑ∂ÂêéÁî®‰∫Ü‰∏Ä‰∏™Á∫øÊÄßÂ±Çre-projection number of tokensÔºàresolutionÔºâÔºöÔºàH/4*W/4Ôºâ/4 = ÔºàH/8*W/8ÔºâÔºåË∑üÂ∏∏ËßÑÁöÑCNN‰∏ÄÊ†∑ÂèòÂåñÁöÑ token dimsÔºö2C ÂêéÈù¢Êé•‰∏ä‰∏Ä‰∏™Transformer blocks ÂêàËµ∑Êù•Âè´stage2Ôºàstage3„ÄÅstage4Ôºâ Swin Transformer blocks Ë∑üÂéüÂßãÁöÑTransformer blockÊØîÔºåÂ∞±ÊòØÊääÂéüÂßãÁöÑMSAÊõøÊç¢Êàê‰∫Üwindow-basedÁöÑMSA ÂéüÂßãÁöÑattentionÔºöglobal computation leads to quadratic complexity window-based attentionÔºö attentionÁöÑËÆ°ÁÆóÂè™ÂèëÁîüÂú®ÊØè‰∏™windowÂÜÖÈÉ® non-overlapping partition ÂæàÊòæÁÑ∂lacks connections across windows shifted window partitioning in successive blocks ‰∏§‰∏™attention block Á¨¨‰∏Ä‰∏™Áî®Â∏∏ËßÑÁöÑwindow partitioning strategyÔºö‰ªéÂ∑¶‰∏äËßíÂºÄÂßãÔºåtake M=4Ôºåwindow size 4x4Ôºà‰∏Ä‰∏™windowÈáåÈù¢ÂåÖÂê´4x4‰∏™patchÔºâ Á¨¨‰∫åÂ±ÇÁöÑwindowÔºåÂü∫‰∫éÂâç‰∏ÄÂ±ÇÔºåÂêÑÂπ≥ÁßªM/2 introduce connections between neighbor non-overlapping windows in the previous layer efficient computation shifted window‰ºöÂØºËá¥windowÂ∞∫ÂØ∏‰∏ç‰∏ÄËá¥Ôºå‰∏çÂà©‰∫éÂπ∂Ë°åËÆ°ÁÆó relative position bias Êàë‰ª¨Âú®MxMÁöÑwindowÂÜÖÈÉ®ËÆ°ÁÆólocal attentionÔºö‰πüÂ∞±ÊòØinput sequenceÁöÑtime-stepÊòØ$M^2$ Q„ÄÅK„ÄÅV $\in R ^ {M^2 d}$ $Attention(Q,K,V)=Softmax(QK^T/\sqrt{d}+B)V$ Ëøô‰∏™B‰Ωú‰∏∫localÁöÑposition biasÔºåÂú®‰∫åÁª¥‰∏äÔºåÂú®ÊØè‰∏™ËΩ¥‰∏äÁöÑÂèòÂåñËåÉÂõ¥[-M+1,M-1] we parameterized a smaller-sized bias matrix $\hat B\in R ^{(2M-1)*(2M-1)}$ values in $B \in R ^ {M^2*M^2}$ are taken from $\hat B$ the learnt relative position biasÂèØ‰ª•Áî®Êù•initialize fine-tuned model Architecture variants base modelÔºöSwin-BÔºåÂèÇÊï∞ÈáèÂØπÊ†áViT-B Swin-TÔºö0.25xÔºåÂØπÊ†áResNet-50 (DeiT-S) Swin-SÔºö0.5xÔºåÂØπÊ†áResNet-101 Swin-LÔºö2x window sizeÔºöM=7 query dimÔºöd=32ÔºåÔºàÊØè‰∏™stageÁöÑinput sequence dimÈÄêÊ∏êx2Ôºåheads numÈÄêÊ∏êx2Ôºâ MLPÔºöexpansion ratio=4 channel number CÔºöÁ¨¨‰∏Ä‰∏™stageÁöÑembdding dimÔºåÔºàÂêéÁª≠ÈÄêÊ∏êx2Ôºâ hypersÔºö acc DETR: End-to-End Object Detection with Transformers Âä®Êú∫ new task formulationÔºöa direct set prediction problem main gradients a set-based global loss a transformer en-de architecture remove the hand-designed componets like nms &amp; anchor acc &amp; run-time on par with Faster R-CNN on COCO significantly better performance on large objects lower performances on small objects ËÆ∫ÁÇπ modern detectors run object detection in an indirect way Âü∫‰∫éÊ†ºÂ≠ê/anchor/proposalsËøõË°åÂõûÂΩíÂíåÂàÜÁ±ª ÁÆóÊ≥ïÊÄßËÉΩÂèóÂà∂‰∫énmsÊú∫Âà∂„ÄÅanchorËÆæËÆ°„ÄÅtarget-anchorÁöÑÂåπÈÖçÊú∫Âà∂ end-to-end approach transformerÁöÑself-attentionÊú∫Âà∂Ôºåexplicitly model all pairwise interactions between elementsÔºöÂÜÖÂê´‰∫ÜÂéªÈáçÔºànmsÔºâÁöÑËÉΩÂäõ bipartite matchingÔºöset loss functionÔºåÂ∞ÜÈ¢ÑÊµãÂíågtÁöÑbox‰∏Ä‰∏ÄÂåπÈÖçÔºårun in parallel DETR does not require any customized layers, thus can be reproduced easily expand to segmentation taskÔºöa simple segmentation head trained on top of a pre-trained DETR set predictionÔºöto predict a set of bounding boxes and the categories for each basicÔºömultilabel classification detection task has near-duplicates issues set predictionÊòØpostprocessing-freeÁöÑÔºåÂÆÉÁöÑglobal inference schemesËÉΩÂ§üavoid redundancy usual lossÔºöbipartite match object detection set-based loss modern detectors use non-unique assignment rules together with NMS bipartite matchingÊòØtargetÂíåpred‰∏Ä‰∏ÄÂØπÂ∫î ÊñπÊ≥ï overall three main components a CNN backbone an encoder-decoder transformer a simple FFN backbone conventional r50 inputÔºö$[H_0, W_0, 3]$ outputÔºö$[H,W,C], H=\frac{H_0}{32}, W=\frac{W_0}{32}, C=2048$ transformer encoder reduce channel dim to $d$Ôºö1x1 convÔºå$d=512$ collapse the spatial dimensionsÔºöfeature sequence [d, HW]ÔºåÊØè‰∏™spatial pixel‰Ωú‰∏∫‰∏Ä‰∏™feature fixed positional encodingsÔºö added to the input of each attention layer „ÄêQUESTION„ÄëÂä†Âú®KÂíåQ‰∏äËøòÊòØembedding‰∏äÔºü transformer decoder ËæìÂÖ•N‰∏™dim=dÁöÑembedding Âè´object queriesÔºöË°®Á§∫Êàë‰ª¨È¢ÑÊµãÂõ∫ÂÆöÂÄºN‰∏™ÁõÆÊ†á Âõ†‰∏∫decoder‰πüÊòØpermutation-invariantÁöÑÔºàÂõ†‰∏∫all sharedÔºâÔºåÊâÄ‰ª•Ë¶ÅËæìÂÖ•N‰∏™‰∏ç‰∏ÄÊ†∑ÁöÑembedding learnt positional encodings add them to the input of each attention layer decodes the N objects in parallel prediction FFN 3 layerÔºåReLUÔºå box predictionÔºönormalized center coords &amp; height &amp; width class predictionÔºö an additional class label $\varnothing$ Ë°®Á§∫no object auxiliary losses each decoder layerÂêéÈù¢ÈÉΩÊé•‰∏Ä‰∏™FFN predictionÂíåHungarian loss shared FFN an additional shared LN to norm the inputs of FFN three components of the loss class lossÔºöCE loss box loss GIOU loss L1 loss technical details AdamWÔºö initial transformer lr=10e-4 initial backbone lr=10e-5 weight decay=10e-4 Xavier init imagenet-pretrained resnet weights with frozen batchnorm layersÔºör50 &amp; r101ÔºåDETR &amp; DETR-R101 a variantÔºö increase feature resolution version remove stage5‚Äôs stride and add a dilation DETR-DC5 &amp; DETR-DC5-R101 improve performance for small objects overall 2x computation increase augmentation resize input random cropÔºöwith 0.5 prob then resize transformer default dropout 0.1 lr schedule 300 epochs drop by factor 10 after 200 epochs 4 images per GPUÔºåtotal batch 64 for segmentation taskÔºöÂÖ®ÊôØÂàÜÂâ≤ Áªôdecoder outputsÂä†mask head compute multi-head attention among decoder box predictions encoder outputs generate M attention heatmaps per object add a FPN styled CNN to recover resolution pixel-wise argmax UNETR: Transformers for 3D Medical Image Segmentation Âä®Êú∫ unetÁªìÊûÑÁî®‰∫éÂåªÂ≠¶ÂàÜÂâ≤ encoder learns global context decoder utilize the representations to predict the semanic ouputs the locality of CNN limits long-range spatial dependency our method use a pure transformer as the encoder learn sequence representations of the input volume global multi-scale encoder directly connects to decoder with skip connections ËÆ∫ÁÇπ unetÁªìÊûÑ encoderÁî®Êù•ÊèêÂèñÂÖ®ÂõæÁâπÂæÅ decoderÁî®Êù•recover skip connectionsÁî®Êù•Ë°•ÂÖÖspatial information that is lost during downsampling localized receptive fieldsÔºö disadvantage in capturing multi-scale contextual information Â¶Ç‰∏çÂêåÂ∞∫ÂØ∏ÁöÑËÑëËÇøÁò§ ÁºìÂíåÊâãÊÆµÔºöatrous convsÔºåstill limited transformer self-attention mechanism in NLP highlight the important features of word sequences learn its long-range dependencies in ViT an image is represented as a patch embedding sequence our method formulation 1D seq2seq problem use embedded patches the first completely transformer-based encoder other unet- transformer methods 2D (ours 3D) employ only in the bottleneck (ours pure transformer) CNN &amp; transformer in separate streams and fuse ÊñπÊ≥ï overview transformer encoder inputÔºö1D sequence of input embeddings given 3D volume $x \in R^{HWDC}$ divide into flattened uniform non-overlapping patches $x\in R^{LCN^3}$ $L=HWD/N^3$Ôºöthe sequence length $N^3$Ôºöpatch dimension linear projection to K-dim $E \in R^{LCK}$Ôºöremain constant through transformer 1D learnable positional embedding $E_{pos} \in R^LD$ 12 self-att blocksÔºöMSA + MLP decoder &amp;skip connections ÈÄâÂèñencoderÁ¨¨{3,6,9,12}‰∏™blockÁöÑËæìÂá∫ reshape back to 3D volume $[\frac{H}{N},\frac{W}{N},\frac{D}{N},C]$ consecutive 3x3x3 conv+BN+ReLU bottleneck deconv by 2 to increase resolution then concat with the previous resized feature then jointly consecutive conv then upsample with deconv‚Ä¶ concatÂà∞ÂéüÂõæresolution‰ª•ÂêéÔºåconsecutive conv‰ª•ÂêéÔºåÂÜç1x1x1 conv+softmax loss dice loss diceÔºöfor each class channelÔºåËÆ°ÁÆódiceÔºåÁÑ∂ÂêéÊ±ÇÁ±ªÂπ≥Âùá 1-dice ce loss for each pixelÔºåÊ±ÇbceÔºåÁÑ∂ÂêéÊ±ÇÊâÄÊúâpixelÁöÑÂπ≥Âùá]]></content>
      <tags>
        <tag>transformer, self-attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pre-training & self-training]]></title>
    <url>%2F2021%2F01%2F17%2Fpre-training-self-training%2F</url>
    <content type="text"><![CDATA[[pre-training] Rethinking ImageNet Pre-trainingÔºåHe KaimingÔºåimageNet pre-trainingÂπ∂Ê≤°ÊúâÁúüÊ≠£helps accÔºåÂè™ÊòØspeedupÔºårandom initializationËÉΩÂ§üreach no worseÁöÑÁªìÊûúÔºåÂâçÊèêÊòØÊï∞ÊçÆÂÖÖË∂≥Â¢ûÂº∫Â§üÁåõÔºåÂØπÂ∞èÈó®Â∞èÊà∑ËøòÊòØÊ≤°Âï•Áî®ÔºåÊàë‰ª¨Â∏åÊúõspeedup [pre-training &amp; self-training] Rethinking Pre-training and Self-trainingÔºåGoogle BrainÔºåÊèêÂá∫task-specificÁöÑpseudo labelË¶ÅÊØîpre-training‰∏≠ÊêûÂá∫Êù•ÁöÑÂêÑÁßçÊ†áÁ≠æË¶ÅÂ•ΩÔºåÂâçÊèêËøòÊòØÂ†ÜÊï∞ÊçÆÔºåÂØπÂ∞èÈó®Â∞èÊà∑Ê≤°Âï•Áî®Ôºålow-data‰∏ãËøòÊòØpre-train‰øùÂπ≥ÂÆâ ÊÄª‰Ωì‰∏äÈÉΩÊòØÈíàÂØπË∑®‰ªªÂä°‰∏ãÔºåimageNet pre-trainingÊÑè‰πâÁöÑÊé¢ËÆ®Ôºå ÂàÜÁ±ªÈóÆÈ¢òËøòÊòØÂèØ‰ª•ÁªßÁª≠pretrained kaimingËøô‰∏™Âè™ÊòØfactÔºåÊ≤°ÊúâÁé∞ÂÆûÊåáÂØºÊÑè‰πâ googleËøô‰∏™one step furtherÔºåÊèêÂá∫‰∫Üself-trainingÂú®Áé∞ÂÆûÊù°‰ª∂‰∏≠ÂèØ‰ª•‰∏ÄËØï Rethinking Pre-training and Self-training Âä®Êú∫ given factÔºöImageNet pre-training has limited impact on COCO object detection investigate self-training to utilize the additional data ËÆ∫ÁÇπ common practice pre-training supervised pre-training È¶ñÂÖàË¶ÅÊ±ÇÊï∞ÊçÆÊúâÊ†áÁ≠æ pre-train the backbone on ImageNet as a classification task Âº±ÁõëÁù£Â≠¶‰π† with pseudo/noisy label kaimingÔºöExploring the limits of weakly supervised pretraining self-supervised pre-training Êó†Ê†áÁ≠æÁöÑÊµ∑ÈáèÊï∞ÊçÆ ÊûÑÈÄ†Â≠¶‰π†ÁõÆÊ†áÔºöautoencoderÔºåcontrastiveÔºå‚Ä¶ https://zhuanlan.zhihu.com/p/108906502 self-training paradigm on COCO train an object detection model on COCO generate pseudo labels on ImageNet both labeled data are combined to train a new model Âü∫Êú¨Âü∫‰∫énoisy studentÁöÑÊñπÊ≥ï observations with stronger data augmentation, pre-training hurts the accuracy, but helps in self-training both supervised and self-supervised pre-training methods fails the benefit of pre-training does not cancel out the gain by self-training flexible about unlabeled data sources, model architectures and computer vision tasks ÊñπÊ≥ï data augmentation vary the strength of data augmentation as 4 levels pre-training efficientNet-B7 AutoAugment weights &amp; noisy student weights self-training noisy student scheme ÂÆûÈ™åÂèëÁé∞self-training with this standard loss function can be unstable implement a loss normalization technique experimental settings object detection COCO dataset for supervised learning unlabeled ImageNet and OpenImages dataset for self-trainingÔºöscore thresh 0.5 to generate pesudo labels retinaNet &amp; spineNet batchÔºöhalf supervised half pesudo semantic segmentation PASCAL VOC 2012 for supervised learning augmented PASCAL &amp; COCO &amp; ImageNet for self-trainingÔºöscore thresh 0.5 to generate pesudo masks &amp; multi-scale NAS-FPN ÂÆûÈ™å pre-training Pre-training hurts performance when stronger data augmentation is usedÔºöÂõ†‰∏∫‰ºösharpenÊï∞ÊçÆÂ∑ÆÂºÇÔºü More labeled data diminishes the value of pre-trainingÔºöÈÄöÂ∏∏Êàë‰ª¨ÁöÑÂÆûÈ™åÊï∞ÊçÆfractionÈÉΩÊØîËæÉÂ∞èÁöÑÁõ∏ÂØπimageNetÔºåÊâÄ‰ª•ÁêÜËÆ∫‰∏ä‰∏ç‰ºöharmÔºü self-supervised pre-training‰πü‰ºö‰∏ÄÊ†∑harmÔºåÂú®augmentÂä†Âº∫ÁöÑÊó∂ÂÄô self-training Self-training helps in high data/strong augmentation regimes, even when pre-training hurtsÔºö‰∏çÂêåÁöÑaugment levelÔºåself-trainingÂØπÊúÄÁªàÁªìÊûúÈÉΩÊúâÂä†Êàê Self-training works across dataset sizes and is additive to pre-trainingÔºö‰∏çÂêåÁöÑÊï∞ÊçÆÈáèÔºå‰πüÈÉΩÊúâÂä†ÊàêÔºå‰ΩÜÊòØlow data regime‰∏ãenjoys the biggest gain discussion weak performance of pre-training is that pre-training is not aware of the task of interest and can fail to adapt jointly training also helpsÔºöaddress the mismatch between two dataset noisy labeling is worse than targeted pseudo labeling ÊÄª‰ΩìÁªìËÆ∫ÔºöÂ∞èÊ†∑Êú¨ÈáèÁöÑÊó∂ÂÄôÔºåpre-trainingËøòÊòØÊúâÂä†ÊàêÁöÑÔºåÂÜçÂä†‰∏äself-trainingËøõ‰∏ÄÊ≠•ÊèêÂçáÔºåÊ†∑Êú¨Â§öÁöÑÊó∂ÂÄôÂ∞±Áõ¥Êé•self-training Rethinking ImageNet Pre-training Âä®Êú∫ thinking random initialization &amp; pre-training ImageNet pre-training speed up but not necessarily improving random initialization can achieve no worse result robust to data size, models, tasks and metrics rethink current paradigm of ‚Äòpre- training and fine-tuning‚Äô ËÆ∫ÁÇπ no fundamental obstacle preventing us from training from scratch if use normalization techniques appropriately if train sufficiently long pre-training speed up when fine-tuning on small dataset new hyper-parameters must be selected to avoid overfitting localization-sensitive task benefits limited from pre-training aimed at communities that don‚Äôt have enough data or computational resources ÊñπÊ≥ï normalization form normalized parameter initialization normalization layers BN layers makes training from scratch difficult small batch size degrade the acc of BN fine-tuningÂèØ‰ª•freeze BN alternatives GNÔºöÂØπbatch size‰∏çÊïèÊÑü syncBN with appropriately normalized initializationÂèØ‰ª•train from scratch VGGËøôÁßç‰∏çÁî®BNÂ±ÇÁöÑ convergence pre-training model has learned low-level features that do not need to be re-learned during random-initial training need more iterations to learn both low-level and semantic features ÂÆûÈ™å investigate maskRCNN ÊõøÊç¢BNÔºöGN/sync-BN learning rateÔºö training longer for the first (large) learning rate is useful but training for longer on small learning rates often leads to overfitting 10k COCOÂæÄ‰∏äÔºåtrain from scratch resultsËÉΩÂ§ücatch up pretraining resultsÔºåÂè™Ë¶ÅËÆ≠ÁöÑÂ§ü‰πÖ 1kÂíå3.5kÁöÑCOCOÔºåconverges show no worseÔºå‰ΩÜÊòØÂú®È™åËØÅÈõÜ‰∏äÂ∑Æ‰∏Ä‰∫õÔºöstrong overfitting due to lack of data PASCALÁöÑÁªìÊûú‰πüÂ∑Æ‰∏ÄÁÇπÔºåÂõ†‰∏∫instanceÂíåcategoryÈÉΩÊõ¥Â∞ëÔºånot directly comparable to the same number of COCO imagesÔºöfewer instances and categories has a similar negative impact as insufficient training data]]></content>
  </entry>
  <entry>
    <title><![CDATA[long-tailed]]></title>
    <url>%2F2021%2F01%2F11%2Flong-tailed%2F</url>
    <content type="text"><![CDATA[[bag of tricks] Bag of Tricks for Long-Tailed Visual Recognition with Deep Convolutional Neural NetworksÔºöÁªìËÆ∫Â∞±ÊòØ‰∏§Èò∂ÊÆµÔºåinput mixup + CAM-based DRS + muted mixup fine-tuningÁªÑÂêà‰ΩøÁî®ÊúÄÂ•Ω [balanced-meta softmax] Balanced Meta-Softmax for Long-Tailed Visual RecognitionÔºöÂïÜÊ±§ [eql] Equalization Loss for Long-Tailed Object Recognition [eql2] Equalization Loss v2: A New Gradient Balance Approach for Long-tailed Object Detection [Class Rectification Loss] Imbalanced Deep Learning by Minority Class Incremental RectificationÔºöÊèêÂá∫CRL‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üËØÜÂà´ÂàÜÂ∏ÉÁ®ÄÁñèÁöÑÂ∞èÁ±ª‰ª¨ÁöÑËæπÁïåÔºå‰ª•Ê≠§ÈÅøÂÖçÂ§ßÁ±ª‰∏ªÂØºÁöÑÂΩ±Âìç Bag of Tricks for Long-Tailed Visual Recognition with Deep Convolutional Neural Networks Âä®Êú∫ to give a detailed experimental guideline of common tricks to obtain the effective combinations of these tricks propose a novel data augmentation approach ËÆ∫ÁÇπ long-tailed datasets poor accuray on the under-presented minority long-tailed CIFARÔºö ÊåáÊï∞ÂûãË°∞Âáè imbalance factorÔºö50/100 test set unchanged ImageNet-LT sampling the origin set follow the pareto distribution test set is balanced iNaturalist extremely imbalanced real world dataset fine-grained problem different learning paradigms metric learning meta learning knowledge transfer suffer from high sensitivity to hyper-parameters training tricks re-weighting re-sample mixup two-stage training different tricks might hurt each other propose a novel data augmentation approach based on CAMÔºögenerate images with transferred foreground and unchanged background ÊñπÊ≥ï start from baseline re-weighting baselineÔºöCE re-weighting methodsÔºö cost-sensitive CEÔºöÊåâÁÖßÊ†∑Êú¨ÈáèÁ∫øÊÄßÂä†ÊùÉ$\frac{n_c}{n_{min}}$ focal lossÔºöÂõ∞ÈöæÊ†∑Êú¨Âä†ÊùÉ class-balanced lossÔºö effective number rather than Ê†∑Êú¨Èáè$n_c$ hyperparameter $\beta$ and weighting factorÔºö$\frac{1-\beta}{1-\beta^{n_c}}$ Âú®cifar10‰∏äÊúâÊïàÔºå‰ΩÜÊòØcifar100‰∏äÂ∞±‰∏çÂ•Ω‰∫Ü directly application in training procedure is not a proper choice especially whenÁ±ªÂà´Â¢ûÂ§öÔºåimbalanceÂä†ÂâßÁöÑÊó∂ÂÄô re-sampling re-sampling methods over-samplingÔºö ÈöèÊú∫Â§çÂà∂minority might leads to overfitting under-sampling ÈöèÊú∫ÂéªÊéâ‰∏Ä‰∫õmajority be preferable to over-sampling ÊúâËßÑÂæãÂú∞sampling Â§ß‰ΩìÈÉΩÊòØimbalancedÂêëÁùÄlighter imbalancedÂêëÁùÄbalancedÊé®Âä® artificial sampling methods create artificial samples sample based on gradients and features likely to introduce noisy data ËßÇÂØüÂà∞ÊèêÂçáÊïàÊûú‰∏çÊòéÊòæ mixup input mixupÔºöinput mixup can be further improved if we remove the mixup in last several epochs manifold mixupÔºöon only one layer ËßÇÂØüÂà∞‰∏§ÁßçmixupÂäüÊïàÂ∑Æ‰∏çÂ§öÔºåÂêéÈù¢ÂèëÁé∞input mixupÊõ¥Â•Ω‰∫õ input mixupÂéªÊéâÂÜçfinetuningÂá†‰∏™epochÁªìÊûúÂèàÊèêÂçáÔºåmanifoldÂàô‰ºöÂèòÂ∑Æ two-stage training imbalanced training + balanced fine-tuning vanilla training schedule on imbalanced data ÂÖàÂ≠¶ÁâπÂæÅ fine-tune on balanced subsets ÂÜçË∞ÉÊï¥recognition accuracy deferred re-balancing by re-sampling (DRS) Ôºöpropose CAM-based sampling deferred re-balancing by re-weighting (DRW) proposed CAM-based sampling DRS only replicate or remove for each sampled image, apply the trained model &amp; its ground truth label to generate CAM Áî®heatmapÁöÑÂπ≥ÂùáÂÄº‰Ωú‰∏∫ÈòàÂÄºÊù•Âå∫ÂàÜÂâçËÉåÊôØ ÂØπÂâçÊôØapply transformations horizontal flipping translation rotating scaling ÂèëÁé∞fine-tuningÊó∂ÂÄôÂÜçresampleÊØîÁõ¥Êé•resampleÁöÑÁªìÊûúÂ•Ω proposed CAM-based samplingÂ•Ω‰∫éÂÖ∂‰ªñsamplingÔºåÂÖ∂‰∏≠CAM-based balance- samplingÊúÄÂ•Ω ImageTrans balance-samplingÂè™ÂÅöÂèòÊç¢Ôºå‰∏çÁî®CAMÂå∫ÂàÜÂâçËÉåÊôØÔºåÁªìÊûú‰∏çÂ¶ÇCAM-basedÔºåËØÅÊòéCAMÊúâÁî® ÂèëÁé∞fine-tuningÊó∂ÂÄôÂÜçreweightÊØîÁõ¥Êé•reweightÁöÑÁªìÊûúÂ•Ω ÂÖ∂‰∏≠CSCEÔºàÊåâÁÖßÊ†∑Êú¨ÈáèÁ∫øÊÄßÂä†ÊùÉÔºâÊúÄÂ•Ω Êï¥‰ΩìÊù•ÁúãDRSÁöÑÁªìÊûúÁ®çÂæÆÊØîDRWÂ•Ω‰∏ÄÁÇπ trick combinations two-stageÁöÑCAM-based DRSÁï•Â•Ω‰∫éDRWÔºå‰∏§‰∏™ÂêåÊó∂Áî®‰∏ç‰ºöfurther improve ÂÜçÂä†‰∏ämixupÁöÑËØùÔºåinputÊØîmanifoldÂ•Ω‰∏Ä‰∫õ ÁªìËÆ∫Â∞±ÊòØÔºöinput mixup + CAM-based DRS + mute fine-tuningÔºåapply the tricks incrementally Balanced Meta-Softmax for Long-Tailed Visual Recognition Âä®Êú∫ long-tailedÔºömismatch between training and testing distributions softmaxÔºöbiased gradient estimation under the long-tailed setup propose Balanced SoftmaxÔºöan elegant unbiased extension of Softmax apply a complementary Meta SamplerÔºöoptimal sample rate classification &amp; segmentation ËÆ∫ÁÇπ raw baselineÔºöa model that minimizes empirical risk on long-tailed training datasets often underperforms on a class-balanced test set most methods use re-sampling or re-weighting to simulate a balanced dataset may under-class the majority or have gradient issue meta-learning optimize the weight per sample need a clean and unbiased dataset decoupled training Â∞±ÊòØ‰∏äÈù¢‰∏ÄÁØáËÆ∫Êñá‰∏≠ÁöÑ‰∏§Èò∂ÊÆµÔºåÁ¨¨‰∏ÄÈò∂ÊÆµÂÖàÂ≠¶Ë°®ÂæÅÔºåÁ¨¨‰∫åÈò∂ÊÆµË∞ÉÊï¥ÂàÜÂ∏Éfine-tuning not adequate for datasets with extremely high imbalance factor LDAM Label-Distribution-Aware Margin Loss larger generalization error bound for minority suit for binary classification we propose BALMS Balanced Meta-Softmax theoretically equivalent with generalization error bound for datasets with high imbalance factors should combine Meta Sampler ÊñπÊ≥ï balanced softmax biasedÔºö‰ªéË¥ùÂè∂ÊñØÊù°‰ª∂Ê¶ÇÁéáÂÖ¨ÂºèÁúãÔºåstandard softmax‰∏äÈªòËÆ§‰∫ÜÂùáÂåÄÈááÊ†∑ÁöÑp(y)ÔºåÂú®ÈïøÂ∞æÂàÜÂ∏ÉÁöÑÊó∂ÂÄôÔºåÂ∞±ÊòØÊúâÂÅèÁöÑ Âä†ÊùÉÔºö Âä†Âú®softmaxÈ°πÈáåÈù¢ Âü∫‰∫éÊ†∑Êú¨ÈáèÁ∫øÊÄßÂä†ÊùÉ Êï∞Â≠¶ÊÑè‰πâ‰∏äÔºöwe need to focus on minimizing the training loss of the tail classes meta sampler resampleÂíåreweightÁõ¥Êé•combineÂèØËÉΩ‰ºöworsen performance class balance resampleÂèØËÉΩÊúâover-balance issue combination procedures ÂØπÂΩìÂâçÂàÜÂ∏ÉÔºåÂÖàËÆ°ÁÆóbalanced-softmaxÔºå‰øùÂ≠ò‰∏Ä‰∏™Ê¢ØÂ∫¶Êõ¥Êñ∞ÂêéÁöÑÊ®°Âûã ËÆ°ÁÆóËøô‰∏™‰∏¥Êó∂Ê®°ÂûãÂú®meta set‰∏äÁöÑCEÔºåÂØπÂàÜÂ∏ÉembeddingËøõË°åÊ¢ØÂ∫¶Êõ¥Êñ∞ÔºöËØÑ‰º∞ÂΩìÂâçÂàÜÂ∏ÉÂíãÊ†∑ÔºåÂæÄ‰∏ÄÂÆöÊñπÂêëÁü´Ê≠£ ÂØπÁúüÊ≠£ÁöÑÊ®°ÂûãÔºåÁî®ÊúÄÊñ∞ÁöÑÂàÜÂ∏ÉÔºåËÆ°ÁÆóbalanced-softmaxÔºåËøõË°åÊ¢ØÂ∫¶Êõ¥Êñ∞ÔºöÁî®‰ºòÂåñÂêéÁöÑÂàÜÂ∏ÉÔºåÂºïÂØºÊ®°ÂûãÂ≠¶‰π† ÂÆûÈ™å CEÁöÑÁªìÊûúÂëàÁé∞ÊòéÊòæÁöÑÈïøÂ∞æÂêåÂàÜÂ∏ÉË∂ãÂäø CBSÊúâÁºìËß£ BSÊõ¥Â•Ω BS+CBS‰ºöover sample BS+metaÊúÄÂ•Ω Imbalanced Deep Learning by Minority Class Incremental Rectification Âä®Êú∫ significantly imbalanced training data propose batch-wise incremental minority class rectification model Class Rectification Loss (CRL) bring benefits to both minority and majority class boundary learning ËÆ∫ÁÇπ Most methods produce learning bias towards the majority classes to eliminate bias lifting the importance of minority classesÔºöover-sampling can easily cause model overfittingÔºåÂèØËÉΩÈÄ†ÊàêÂØπÂ∞èÁ±ªÂà´ÁöÑËøáÂàÜÂÖ≥Ê≥®ÔºåËÄåÂØπÂ§ßÁ±ªÂà´‰∏çÂ§üÈáçËßÜÔºåÂΩ±ÂìçÊ®°ÂûãÊ≥õÂåñËÉΩÂäõ cost-sensitive learningÔºödifficult to optimise threshold-adjustment techniqueÔºögiven by experts previous methods mainly investigate single-label binary-class with small imbalance ratio real data large ratioÔºöpower-law distributions Subtle appearance discrepancy hard sample mining hard negatives are more informative than easy negatives as they violate a model class boundary we only consider hard mining on the minority classes for efficiency our batch-balancing hard mining strategyÔºöeliminating exhaustive searching LMLE ÂîØ‰∏ÄÁöÑÁ´ûÂìÅÔºöËÄÉËôë‰∫Üdata imbalanceÁöÑÁªÜÁ≤íÂ∫¶ÂàÜÁ±ª not end-to-end global hard mining computationally complex and expensive ÊñπÊ≥ï CRL overview explicitly imposing structural discrimination of minority classes batch-wise operate on CE forcus on minority class onlyÔºöthe conventional CE loss can already model the majority classes well limitations of CE CE treat the individual samples and classes as equally important the learned model is suboptimal boundaries are biased towards majority classes profile the class distribution for each class hard mining overview minority class hard sample mining selectively ‚Äúborrowing‚Äù majority class samples from class decision boundary to minority class‚Äôs perspectiveÔºömining both hard-positive and hard-negative samples define minority classÔºöselected in each mini-batch Incremental refinementÔºö eliminates the LMLE‚Äôs drawback in assuming that local group structures of all classes can be estimated reliably by offline global clustering mini-batchÁöÑdata distributionÂíåËÆ≠ÁªÉÈõÜ‰∏çÊòØÂÆåÂÖ®‰∏ÄËá¥ÁöÑ steps profile the minority and majority classes per label in each training mini-batch for each sampleÔºåfor each class $j$Ôºåfor each pred class $k$Ôºåwe have $h^j=[h_1^j, ‚Ä¶, h_k^j, ‚Ä¶, h_{n_cls}^j]$ sort $h_k^j$ in descent orderÔºådefine the minority classes for each class with $C_{min}^j = \sum_{k\in C_{min}^j}h_k^j \leq \rho * n_{bs}$Ôºåwith $\rho=0.5$ hard mining hardness score basedÔºöprediction scoreÔºåclass-level feature basedÔºöfeature distanceÔºåinstance-level class-levelÔºåfor class c hard-positivesÔºösame gt classÔºåbut low prediction hard-negativeÔºödifferent gt classÔºåwith high prediction instance-levelÔºåfor each sample in class c hard-positivesÔºösame gt classÔºålarge distance with current sample hard-negativeÔºödifferent gt classÔºåsmall distance with current sample top-k mining hard-positivesÔºöbottom-k scored on c/top-k distance on c hard-negativeÔºötop-k scored on c/bottom-k distance on c score-based yields superior to distance-based CRL final weighted lossÔºö$L = \alpha L_{crl}+(1-\alpha)L_{ce}$Ôºå$\alpha=\eta\Omega_{imbalance}$ class imbalance measure $\Omega$Ôºömore weighting is assigned to more imbalanced labels form triplet lossÔºöÁ±ªÂÜÖ+Á±ªÈó¥ contrastive lossÔºöÁ±ªÂÜÖ modelling the distribution relationship of positive and negative pairsÔºöÊ≤°ÁúãÊáÇ ÊÄªÁªì Â∞±ÊòØÂ•óÁî®Áé∞ÊúâÁöÑmetric learningÔºåÂÆö‰πâ‰∫Ü‰∏Ä‰∏™ÂèòÂåñÁöÑminority classÔºåÂûÉÂúæ„ÄÇ ËØ¥Âà∞Â∫ïÂ∞±ÊòØÂ§ßÊï∞ÊçÆ‚Äî‚ÄîCEÔºåÂ∞èÊï∞ÊçÆ‚Äî‚Äîmetric learning„ÄÇ]]></content>
      <tags>
        <tag>ÈïøÂ∞æÂàÜÂ∏É</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[refineDet]]></title>
    <url>%2F2021%2F01%2F08%2FrefineDet%2F</url>
    <content type="text"><![CDATA[ÂíårefineNetÊ≤°Êúâ‰ªª‰ΩïÂÖ≥Á≥ª RefineDet: Single-Shot Refinement Neural Network for Object Detectio Âä®Êú∫ inherit the merits of both two-stage and one-stageÔºöaccuracy and efficiency single-shot multi-task refineDet anchor refinement module (ARM) object detection module (ODM) transfer connection block (TCB) ËÆ∫ÁÇπ three advantages that two-stage superior than one-stage RPNÔºöhandle class imbalance two step regressÔºöcoarse to refine two stage featureÔºöRPN‰ªªÂä°Âíåregression‰ªªÂä°ÊúâÂêÑËá™ÁöÑfeature Ê®°Êãü‰∫åÈò∂ÊÆµÊ£ÄÊµãÁöÑRPNÔºåÊääclassifier‰ªªÂä°‰∏≠ÁöÑÂ§ßÈáèÈò¥ÊÄßÊ°ÜÂÖàÊéíÊéâÔºå‰ΩÜ‰∏çÊòØ‰ª•‰∏§‰∏™Èò∂ÊÆµÁöÑÂΩ¢ÂºèÔºåËÄåÊòØmulti-taskÂπ∂Ë°å Â∞Ü‰∏ÄÈò∂ÊÆµÊ£ÄÊµãÁöÑobjectnessÂíåbox regression‰ªªÂä°Ëß£ËÄ¶Ôºå‰∏§‰∏™‰ªªÂä°ÈÄöËøátransfer blockËøûÊé• ARM remove negative anchors to reduce search space for the classifier coarsely adjust the locations and sizes of anchors to provide better initialization for regression ODM further improve the regression predict multi labels TCB transfer the features in the ARM to handle the more challenging tasks in the ODM ÊñπÊ≥ï Transfer Connection Block Ê≤°‰ªÄ‰πàÊñ∞ÁöÑ‰∏úË•øÔºå‰∏äÈááÊ†∑Áî®‰∫ÜdeconvÔºåconv-reluÔºåelement-wise add Two-Step Cascaded Regression fisrt step ARM prediction for each cellÔºåfor each predefined anchor boxesÔºåpredict 4 offsets and 2 scores obtain refined anchor boxes second step ODM prediction with justified feature mapÔºåwith refined anchor boxes generate accurate boxes offset to refined boxes and multi-class scoresÔºåc+4 Negative Anchor Filtering reject well-classified negative anchors if the negative confidence is larger than 0.99Ôºådiscard it in training the ODM ODMÊé•Êî∂ÊâÄÊúâpred positiveÂíåhard negative Training and Inference details backÔºöVGG16 &amp; resnet101 fc6 &amp; fc7ÂèòÊàê‰∏§‰∏™conv different feature scales L2 norm two extra convolution layers and one extra residual block 4 feature strides each levelÔºö1 scale &amp; 3 ratios ensures that different scales of anchors have the same tiling density on the image matching ÊØè‰∏™GT box match‰∏Ä‰∏™scoreÊúÄÈ´òÁöÑanchor box ‰∏∫ÊØè‰∏™anchor boxÊâæÂà∞ÊúÄÂåπÈÖçÁöÑiouÂ§ß‰∫é0.5ÁöÑgt box Áõ∏ÂΩì‰∫éÊääignoreÈÇ£ÈÉ®ÂàÜ‰πü‰Ωú‰∏∫Ê≠£Ê†∑Êú¨‰∫Ü Hard Negative Mining select negative anchor boxes with top loss values n &amp; p ratioÔºö3:1 Loss Function ARM loss binary classÔºöÂè™ËÆ°ÁÆóÊ≠£Ê†∑Êú¨ÔºüÔºüÔºü boxÔºöÂè™ËÆ°ÁÆóÊ≠£Ê†∑Êú¨ ODM loss pass the refined anchors with the negative confidence less than the threshold multi-classÔºöËÆ°ÁÆóÂùáË°°ÁöÑÊ≠£Ë¥üÊ†∑Êú¨ boxÔºöÂè™ËÆ°ÁÆóÊ≠£Ê†∑Êú¨ Ê≠£Ê†∑Êú¨Êï∞‰∏∫0ÁöÑÊó∂ÂÄôÔºålossÂùá‰∏∫0ÔºöÁ∫ØÈò¥ÊÄßÊ†∑Êú¨Êó†ÊïàÔºüÔºü]]></content>
      <tags>
        <tag>ÁõÆÊ†áÊ£ÄÊµã</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CPNDet]]></title>
    <url>%2F2021%2F01%2F05%2FCPNDet%2F</url>
    <content type="text"><![CDATA[Corner Proposal Network for Anchor-free, Two-stage Object Detection Âä®Êú∫ anchor-free two-stage ÂÖàÊâæpotential corner keypoints classify each proposal corner-basedÊñπÊ≥ïÔºöÂØπ‰∫éobjects of various scalesÊúâÊïàÔºåÂú®ËÆ≠ÁªÉ‰∏≠ÈÅøÂÖç‰∫ßÁîüËøáÂ§öÁöÑÂÜó‰Ωôfalse-positive proposalsÔºå‰ΩÜÊòØÂú®ÁªìÊûú‰∏ä‰ºöÂá∫Áé∞Êõ¥Â§öÁöÑfp ÂæóÂà∞ÁöÑÊòØcompetitive results ËÆ∫ÁÇπ anchor-based methodsÂØπÂΩ¢Áä∂Â•áÊÄ™ÁöÑÁõÆÊ†áÂÆπÊòìÊºèÊ£Ä anchor-free methodsÂÆπÊòìÂºïÂÖ•ÂÅáÈò≥caused by mistakely grouping thus an individual classifier is strongly required Corner Proposal Network (CPN) use key-point detection in CornerNet ‰ΩÜÊòØgroupÈò∂ÊÆµ‰∏çÂÜçÁî®embedding distanceË°°ÈáèÔºåËÄåÊòØÁî®a binary classifier ÁÑ∂ÂêéÊòØmulti-class classifierÔºåoperate on the survived objects ÊúÄÂêésoft-NMS]]></content>
      <tags>
        <tag>ÁõÆÊ†áÊ£ÄÊµãÔºåtwo-stageÔºåanchor-free</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[refineNet]]></title>
    <url>%2F2021%2F01%2F05%2FrefineNet%2F</url>
    <content type="text"><![CDATA[RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic SegmentationÂºïÁî®Èáè1452Ôºå‰ΩÜÊòØÊ≤°ÊúâÂá†ÁØáÊäÄÊúØÂçöÂÆ¢ÔºüÔºü Âä®Êú∫ ËØ≠‰πâÂàÜÂâ≤ dense classification on every single pixel refineNet long-range residual connections chained residual pooling ËÆ∫ÁÇπ pooling/conv strideÔºö losing finer image structure deconv is not able to recover the lost info atrous high resoÔºölarge computation dilated convÔºöcoarse sub-sampling of feature FCN fuse features from all levels stage-wise rather than end-to-end???Â≠òÁñë this paper main ideaÔºöeffectively exploit middle layer features RefineNet fuse all level feature residual connections with identity skip chained residual pooling to capture background contextÔºöÁúãÊèèËø∞ÊÑüËßâÂÉèinception downsamp end-to-end ÊòØÊï¥‰∏™ÂàÜÂâ≤ÁΩëÁªú‰∏≠ÁöÑ‰∏Ä‰∏™component ÊñπÊ≥ï backbone pretrained resnet 4 blocksÔºöx4 - x32Ôºåeach blockÔºöpool-residual connectionÔºöÊØè‰∏™ËæìÂá∫ËøûÊé•‰∏Ä‰∏™RefineNet unit 4-cascaded architecture final ouputÔºö high-resolution feature maps dense soft-max bilinear interpolation to origin resolution cascade inputs output from backbone block ouput from previous refineNet block refineNet block adapt convÔºö to adapt the dimensionality and refine special task BN layers are removed channel 512 for R4Ôºåchannel 256 for the rest fusionÔºö ÂÖàÁî®conv to adapt dimension and recale the paths ÁÑ∂Âêéupsamp summation Â¶ÇÊûúsingle inputÔºöwalk through and stay unchanged chained residual poolingÔºö aims to capture background context from a large image region chainedÔºöefficiently pool features with multiple window sizes pooling blocksÔºös1 maxpooling+conv in practiceÁî®‰∫Ü‰∏§‰∏™pooling blocks use one ReLU in the chained residual pooling block output convÔºö ‰∏Ä‰∏™residualÔºöto employ non-linearity dimension remains unchanged final levelÔºötwo additional RCUs before the final softmax prediction residual identity mappings a clean information path not block by any non-linearityÔºöÊâÄÊúâreluÈÉΩÂú®residual pathÈáåÈù¢ Âè™Êúâchained residual poolingÊ®°ÂùóËµ∑ÂßãÊó∂ÂÄôÊúâ‰∏™ReLUÔºöone single ReLU in each RefineNet block does not noticeably reduce the effectiveness of gradient flow linear operationsÔºö within the fusion block dimension reduction operations upsamp operations ÂÖ∂‰ªñÁªìÊûÑ Á∫ßËÅîÁöÑÂ∞±Âè´cascaded ‰∏Ä‰∏™blockÂ∞±Âè´single Â§ö‰∏™input resolutionÂ∞±Âè´mult-scale ÂÆûÈ™å 4-cascaded works better than 1-cas &amp; 2-cas 2-scale works better than 1-scale]]></content>
      <tags>
        <tag>ËØ≠‰πâÂàÜÂâ≤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centerNet]]></title>
    <url>%2F2020%2F12%2F29%2FcenterNet%2F</url>
    <content type="text"><![CDATA[[papers] [centerNet] ÁúücenterNet: Objects as PointsÔºåutexasÔºåËøô‰∏™ÊòØÁúüÁöÑcenterNetÔºåÂü∫‰∫éÂàÜÂâ≤Êû∂ÊûÑÔºåÈ¢ÑÊµã‰∏≠ÂøÉÁÇπÁöÑheatmapÔºå‰ª•Âèä2-N‰∏™channelÂÖ∂‰ªñÁõ∏ÂÖ≥ÂèÇÊï∞ÁöÑÂõûÂΩí [cornet-centerNet] centerNet: Keypoint Triplets for Object DetectionÔºåËøô‰∏™Êä¢ÂÖàÂè´‰∫ÜcenterNetÔºå‰ΩÜÊòØÊàëËßâÂæóÂè´corner-centerNetÊõ¥ÂêàÈÄÇÔºåÂÆÉÊòØÂü∫‰∫écornerNetË°çÁîüÁöÑÔºåÂú®cornerNetÁöÑÂü∫Á°Ä‰∏äÂÜçÂä†‰∏ÄÂàÄÂà§ÂÆöÔºåÂü∫‰∫éËßíÁÇπpairÁöÑ‰∏≠ÂøÉÁÇπÊòØÂê¶ÊòØÂâçÊôØÊù•ÂÜ≥ÂÆöÊòØÂê¶‰øùÁïôËøô‰∏™Ê°Ü [centerNet2] Probabilistic two-stage detectionÔºåutexasÔºå centerNet: Objects as Points Âä®Êú∫ anchor-based exhaustive list of potential locations wasteful, inefficient, requires additional post-processing our detector centerÔºöuse keypoint estimation to find center points other propertiesÔºöregress tasks object detection 3d object detection multi-person human pose estimation ËÆ∫ÁÇπ Áõ∏ÊØîËæÉ‰∫é‰º†Áªü‰∏ÄÈò∂ÊÆµ„ÄÅ‰∫åÈò∂ÊÆµÊ£ÄÊµã anchorÔºö box &amp; kpÔºö‰∏Ä‰∏™ÊòØÊ°ÜÔºå‰∏Ä‰∏™ÊòØÂáª‰∏≠Ê†ºÂ≠ê nmsÔºötake local peaksÔºåno need of nms larger resolutionÔºöhourglassÊû∂ÊûÑÔºåËæìÂá∫x4ÁöÑheatmapÔºåeliminates the need for multiple anchors Áõ∏ÊØîËæÉ‰∫ékey point estimantion network themÔºörequire grouping stage ourÔºöÂè™ÂÆö‰Ωç‰∏Ä‰∏™center pointÔºåno need for group or post-processing ÊñπÊ≥ï loss ÂÖ≥ÈîÆÁÇπloss center pointÂÖ≥ÈîÆÁÇπÂÆö‰πâÔºöÊØè‰∏™ÁõÆÊ†áÁöÑgt pointÂè™Êúâ‰∏Ä‰∏™Ôºå‰ª•ÂÆÉ‰∏∫‰∏≠ÂøÉÔºåÂÅöobject size-adaptiveÁöÑÈ´òÊñØpenalty reductionÔºåoverlapÁöÑÂú∞ÊñπÂèñmax focal lossÔºöÂü∫Êú¨‰∏écornetNet‰∏ÄËá¥ L_k = \frac{-1}{N}\sum_{x,y,c} \begin{cases} (1-\hat Y)^\alpha log(\hat Y), if Y=1\\ (1-Y)^\beta \hat Y^\alpha log(1-\hat Y), otherwise \end{cases} $\alpha=2, \beta=4$ background pointsÊúâpenaltyÔºåÊ†πÊçÆgtÁöÑÈ´òÊñØË°∞ÂáèÊù•ÁöÑ offset loss Âè™Êúâ‰∏§‰∏™ÈÄöÈÅì(x_offset &amp; y_offset)Ôºöshared among categories gtÁöÑoffsetÊòØÂéüÂßãresolution/output strideÂêë‰∏ãÂèñÊï¥ÂæóÂà∞ L1 loss centerNet output Á¨¨‰∏Ä‰∏™ÈÉ®ÂàÜÔºö‰∏≠ÂøÉÁÇπÔºå[h,w,c]Ôºåbinary mask for each category Á¨¨‰∫å‰∏™ÈÉ®ÂàÜÔºöoffsetÔºå[h,w,2]Ôºåshared among Á¨¨‰∏â‰∏™ÈÉ®ÂàÜÔºösizeÔºå[h,w,2]Ôºåshared among L1 lossÔºåuse raw pixel coordinates overall C+4 channelsÔºåË∑ü‰º†ÁªüÊ£ÄÊµãÁöÑformulationÊòØ‰∏ÄËá¥ÁöÑÔºåÂè™‰∏çËøá‰º†ÁªüÊ£ÄÊµãgtÊòØÂü∫‰∫éanchorËÆ°ÁÆóÁöÑÁõ∏ÂØπÂÄºÔºåÊú¨ÊñáÁõ¥Êé•ÂõûÂΩíÁªùÂØπÂÄº $L_{det} = L_k + \lambda_{size} L_{size} + \lambda_{off} L_{off}$ ÂÖ∂‰ªñtaskÁöÑformulationÁúãÁ¨¨‰∏ÄÂº†Âõæ inference workflow local peaksÔºö for each category channel all responses greater or equal to its 8-connected neighborsÔºö3x3 max pooling keep the top100 generate bounding boxes ÁªÑÂêàoffset &amp; size predictions ÔºüÔºüÔºüÔºüÊ≤°ÊúâÂêéÂ§ÑÁêÜ‰∫ÜÔºüÔºüÔºüÂÅáÈò≥ÔºüÔºüÔºüÔºü encoder-decoder backboneÔºöx4 hourglass104 stemÔºöx4 modulesÔºö‰∏§‰∏™ resnet18/101+deformable conv upsampling 3x3 deformable conv, 256/128/64 bilinear interpolation DLA34+deformable conv upsampling heads independent heads one 3x3 convÔºå256 1x1 conv for prediction ÊÄªÁªì ‰∏™‰∫∫ÊÑüËßâÔºåcenterNetÂíåanchor-basedÁöÑformulationÂÖ∂ÂÆûÊòØ‰∏ÄÊ†∑ÁöÑÔºå centerÁöÑÂõûÂΩíÂØπÊ†áconfidenceÁöÑÂõûÂΩíÔºåÂå∫Âà´Âú®‰∫éÈ´òÊñØ/[0,1]/[0,-1,1] sizeÁöÑÂõûÂΩíÂèòÊàê‰∫Üraw pixelÔºå‰∏çÂÜçÂü∫‰∫éanchor hourglassÁªìÊûÑÂ∞±ÊòØfpnÔºåÁ∫ßËÅîÁöÑhourglassÂèØ‰ª•ÂØπÊ†ábi-fpn Â§öÂ∞∫Â∫¶ÂèòÊàê‰∫ÜÂçï‰∏ÄÂ§ßresolutionÁâπÂæÅÂõæÔºå‰πüÂèØ‰ª•Áî®Â§öÂ∞∫Â∫¶È¢ÑÊµãÔºåÈúÄË¶ÅÂä†NMS centerNet2: Probabilistic two-stage detection Âä®Êú∫ two-stage probabilistic interpretation the suggested pipeline stage1Ôºöinfer proper object-backgroud likelihoodÔºå‰∏ìÊ≥®ÂâçËÉåÊôØÂàÜÁ¶ª stage2Ôºöinform the overall score verified on COCO faster and more accurate than both one and two stage detectors outperform yolov4 extreme large modelÔºö56.4 mAP standard ResNeXt- 32x8d-101-DCN backÔºö50.2 mAP ËÆ∫ÁÇπ one-stage detectors dense predict jointly predict class &amp; location anchor-basedÔºöRetinaNetÁî®focal lossÊù•deal with ÂâçËÉåÊôØimbalance anchor-freeÔºöFCOS &amp; CenterNet‰∏çÂü∫‰∫éanchorÂü∫‰∫égridÔºåÁºìËß£imbalance deformable convÔºöAlignDetÂú®outputÂâçÈù¢Âä†‰∏ÄÂ±Çdeformable conv to get richer features sound probablilistic interpretation heavier separate classification and regression branches than two-stage modelsÔºöÂ¶ÇÊûúÁ±ªÂà´ÁâπÂà´Â§öÁöÑÊÉÖÂÜµÔºåÂ§¥‰ºöÈùûÂ∏∏ÈáçÔºå‰∏•ÈáçÂΩ±ÂìçÊÄßËÉΩ misaligned issueÔºö‰∏ÄÈò∂ÊÆµÈ¢ÑÊµãÊòØÂü∫‰∫élocal featureÔºåÊÑüÂèóÈáé„ÄÅanchor settingsÈÉΩ‰ºöÂΩ±Âìç‰∏éÁõÆÊ†áÁöÑÂØπÈΩêÁ®ãÂ∫¶ two-stage detectors first RPN generates coarse object proposals then per-region head to classify and refine ROI headsÔºöFaster-RCNNÁî®‰∫Ü‰∏§‰∏™fcÂ±Ç‰Ωú‰∏∫ROI heads cascadedÔºöCascadeRCNNÁî®‰∫Ü‰∏â‰∏™ËøûÁª≠ÁöÑFaster-RCNNÔºåwith a different positive threshold semantic branchÔºöHTCÁî®‰∫ÜÈ¢ùÂ§ñÁöÑÂàÜÂâ≤ÂàÜÊîØenhance the inter-stage feature flow decoupleÔºöTSDÂ∞Ücls&amp;pos‰∏§‰∏™ROI headsËß£ËÄ¶ weak RPNÔºöÂõ†‰∏∫Â∞ΩÂèØËÉΩÊèêÂçáÂè¨ÂõûÁéáÔºåproposal score‰πü‰∏çÂáÜÔºå‰∏ßÂ§±‰∫Ü‰∏Ä‰∏™clearÁöÑprobabilistic interpretation independent probabilistic interpretationÔºö‰∏§‰∏™Èò∂ÊÆµÂêÑËÆ≠ÂêÑÁöÑÔºåÊúÄÂêéÁöÑcls score‰ªÖÁî®Á¨¨‰∫åÈò∂ÊÆµÁöÑ slowÔºöproposalsÂ§™Â§ö‰∫ÜÊâÄ‰ª•slow down other detectors point-basedÔºöcornetNetÈ¢ÑÊµã&amp;ÁªÑÂêà‰∏§‰∏™ËßíÁÇπÔºåcenterNetÈ¢ÑÊµã‰∏≠ÂøÉÁÇπÂπ∂Âü∫‰∫éÂÆÉÂõûÂΩíÈïøÂÆΩ transformerÔºöDETRÁõ¥Êé•È¢ÑÊµãa set of bounding boxesÔºåËÄå‰∏çÊòØ‰º†ÁªüÁöÑÁªìÊûÑÂåñÁöÑdense output ÁΩëÁªúÁªìÊûÑ one/two-stage detectorsÔºöimage classification network + lightweight upsampling layers + heads point-basedÔºöFCNÔºåÊúâsymmetric downsampling and upsampling layerÔºåÈ¢ÑÊµã‰∏Ä‰∏™Â∞èstrideÁöÑheatmap DETRÔºöfeature extraction + transformer decoder our method Á¨¨‰∏Ä‰∏™Èò∂ÊÆµ ÂÅö‰∫åÂàÜÁ±ªÁöÑone-stage detectorÔºåÊèêÂâçÊôØÔºå ÂÆûÁé∞‰∏äÂ∞±Áî®region-level feature+classifierÔºàFCN-basedÔºâ Á¨¨‰∫åÈò∂ÊÆµ ÂÅöposition-basedÁ±ªÂà´È¢ÑÊµã ÂÆûÁé∞‰∏äÊó¢ÂèØ‰ª•Áî®‰∏Ä‰∏™Faster-RCNNÔºå‰πüÂèØ‰ª•Áî®classifier ÊúÄÁªàÁöÑlossÁî±‰∏§‰∏™Èò∂ÊÆµÂêàÂπ∂ÂæóÂà∞ÔºåËÄå‰∏çÊòØÂàÜÈò∂ÊÆµËÆ≠ÁªÉ Ë∑üformer two-stage frameworkÁöÑ‰∏ªË¶Å‰∏çÂêåÊòØ Âä†‰∫Üjoint probabilistic objective over both stages ‰ª•ÂâçÁöÑ‰∫åÈò∂ÊÆµRPNÁöÑÁî®ÈÄî‰∏ªË¶ÅÊòØÊúÄÂ§ßÂåñrecallÔºådoes not produce accurate likelihoods faster and more accurate È¶ñÂÖàÊòØÁ¨¨‰∏Ä‰∏™Èò∂ÊÆµÁöÑproposalÊõ¥Â∞ëÊõ¥ÂáÜ ÂÖ∂Ê¨°ÊòØÁ¨¨‰∫å‰∏™Èò∂ÊÆµmakes full use of years of progress in two-stage detectionÔºå‰∫åÈò∂ÊÆµÁöÑËÆæËÆ°Á´ôÂú®‰ºü‰∫∫ÁöÑËÇ©ËÜÄ‰∏ä ÊñπÊ≥ï joint class distributionÔºöÂÖà‰ªãÁªçÊÄé‰πàÂ∞Ü‰∏Ä‰∫åÈò∂ÊÆµËÅîÂä® „ÄêÁ¨¨‰∏ÄÈò∂ÊÆµÁöÑÂâçËÉåÊôØscore„Äë ‰πò‰∏ä „ÄêÁ¨¨‰∫åÈò∂ÊÆµÁöÑclass score„Äë $P(C_k) = \sum_o P(C_k|O_k=o)P(O_k=o)$ maximum likelihood estimation for annotated objects ÈÄÄÈò∂Êàêindependent maximum-likelihood $log P(C_k) = log P(C_k|O_k=1) + log P(O_k=1)$ for background class ‰∏çÂàÜËß£ $log P(bg) = log( P(bg|O_k=1) * P(O_k=1) + P(O_k=0))$ lower boundsÔºåÂü∫‰∫éjensen‰∏çÁ≠âÂºèÂæóÂà∞‰∏§‰∏™‰∏çÁ≠âÂºè $log P(bg) \ge P(O_k=1) * log( P(bg|O_k=1))$ÔºöÂ¶ÇÊûú‰∏ÄÈò∂ÊÆµÂâçÊôØÁéáË¥ºÂ§ßÔºåÈÇ£‰πàÂ∞± $log P(bg) \ge P(O_k=0)$Ôºö optimize both bounds jointly works better network designÔºö‰ªãÁªçÊÄé‰πàÂú®one-stage detectorÁöÑÂü∫Á°Ä‰∏äÊîπÈÄ†Âá∫‰∏Ä‰∏™two-stage probabilistic detector experiment with 4 different designs for first-stage RPN RetinaNet RetinaNetÂÖ∂ÂÆûÂíåtwo-stageÁöÑRPNÈ´òÂ∫¶Áõ∏‰ººÊ†∏ÂøÉÂå∫Âà´Âú®‰∫éÔºö a heavier head designÔºö4-conv vs 1-conv RetinaNetÊòØbackbone+fpn+individual heads RPNÊòØbackbone+fpn+shared convs+individual heads a stricter positive and negative anchor definitionÔºöÈÉΩÊòØIoU-based anchor selectionÔºåthresh‰∏ç‰∏ÄÊ†∑ focal loss first-stage design ‰ª•‰∏ä‰∏âÁÇπÈÉΩÂú®probabilistic modelÈáåÈù¢‰øùÁïô ÁÑ∂ÂêéÂ∞Üseparated headsÊîπÊàêshared heads centerNet Ê®°ÂûãÂçáÁ∫ß ÂçáÁ∫ßÊàêmulti-scaleÔºöuse ResNet-FPN backÔºåP3-P7 Â§¥ÊòØFCOSÈÇ£ÁßçÂ§¥Ôºöindividual headsÔºå‰∏çshare convÔºåÁÑ∂Âêécls branchÈ¢ÑÊµãcenterness+clsÔºåreg branchÈ¢ÑÊµãregress params Ê≠£Ê†∑Êú¨‰πüÊòØÊåâÁÖßFCOSÁ≠ñÁï•Ôºöposition &amp; scale-based ÂçáÁ∫ßÊ®°ÂûãËøõË°åone-stage &amp; two-stageÂÆûÈ™åÔºöcenterNet* ATSS ÊòØ‰∏Ä‰∏™adaptive IoU threshÁöÑÊñπÊ≥ïÔºåcenternessÊù•Ë°®Á§∫‰∏Ä‰∏™Ê†ºÂ≠êÁöÑscore Êàë‰ª¨Â∞Ücenterness*classification scoreÂÆö‰πâ‰∏∫Ëøô‰∏™Ê®°ÂûãÁöÑproposal score Âè¶Â§ñÂ∞±ÊòØtwo-stage‰∏ãËøòÊòØÂ∞ÜRPNÁöÑcls &amp; reg headsÂêàÂπ∂ GFLÔºöËøòÊ≤°ÁúãËøáÔºåÂÖàË∑≥ËøáÂêß second-stage designsÔºöFasterRCNN &amp; CascadeR- CNN deformable convÔºöËøô‰∏™Âú®centerNetv1ÁöÑResNetÂíåDLA backÈáåÈù¢ÈÉΩÁî®‰∫ÜÔºåÂú®v2ÈáåÈù¢Ôºå‰∏ªË¶ÅÊòØÁî®ResNeXt-32x8d-101-DCNÔºå hyperparameters for two-stage probabilistic model ‰∏§Èò∂ÊÆµÊ®°ÂûãÈÄöÂ∏∏ÊòØÁî®P2-P6Ôºå‰∏ÄÈò∂ÊÆµÈÄöÂ∏∏Áî®P3-P7ÔºöÊàë‰ª¨Áî®P3-P7 increase the positive IoU thresholdÔºö0.5 to [0.6,0.7,0.8] maximum of 256 proposals ÔºàÂØπÊØîoriginÁöÑ1kÔºâ increase nms threshold from 0.5 to 0.7 SGDÔºå90K iterations base learning rateÔºö0.02 for two-stage &amp; 0.01 for one-stageÔºå0.1 decay multi-scale trainingÔºöÁü≠Ëæπ[640,800]ÔºåÈïøËæπ‰∏çË∂ÖËøá1333 fix-scale testingÔºöÁü≠ËæπÁî®800ÔºåÈïøËæπ‰∏çË∂ÖËøá1333 first stage loss weightÔºö0.5ÔºåÂõ†‰∏∫one-stage detectorÈÄöÂ∏∏Áî®0.01 lrÂºÄÂßãËÆ≠ÁªÉ ÂÆûÈ™å 4ÁßçdesignÁöÑÂØπÊØî ÊâÄÊúâÁöÑprobabilistic modelÈÉΩÊØîone-stage modelÂº∫ÔºåÁîöËá≥ËøòÂø´ÔºàÂõ†‰∏∫ÁÆÄÂåñ‰∫ÜËÑëË¢ãÔºâ ÊâÄÊúâÁöÑprobabilistic FasterRCNNÈÉΩÊØîÂéüÂßãÁöÑRPN-based FasterRCNNÂº∫Ôºå‰πüÂø´ÔºàÂõ†‰∏∫P3-P7ÊØîP2-P6ÁöÑËÆ°ÁÆóÈáèÂ∞è‰∏ÄÂçäÔºåËÄå‰∏îÁ¨¨‰∫åÈò∂ÊÆµfewer proposalsÔºâ CascadeRCNN-CenterNet design performs the bestÔºöÊâÄ‰ª•‰ª•ÂêéÂ∞±ÊääÂÆÉÂè´CenterNet2 ÂíåÂÖ∂‰ªñreal-time modelsÂØπÊØî Â§ßÂ§öÊòØreal-time modelsÈÉΩÊòØ‰∏ÄÈò∂ÊÆµÊ®°Âûã ÂèØ‰ª•ÁúãÂà∞‰∫åÈò∂ÊÆµ‰∏ç‰ªÖËÉΩÂ§üÊØî‰∏ÄÈò∂ÊÆµÊ®°ÂûãËøòÂø´ÔºåÁ≤æÂ∫¶ËøòÊõ¥È´ò SOTAÂØπÊØî Êä•‰∫Ü‰∏Ä‰∏™56.4%ÁöÑsotaÔºå‰ΩÜÊòØÂ§ßÂÆ∂Âè£Á¢ë‰∏äÂ•ΩÂÉèÊïàÊûúÂæàÂ∑Æ ‰∏çÊîæÂõæ‰∫Ü corner-centerNet: Keypoint Triplets for Object Detection Âä®Êú∫ based on cornerNet triplet corner keypointsÔºöweak grouping ability cause false positives correct predictions can be determined by checking the central parts cascade corner pooling and center poolling ËÆ∫ÁÇπ whats new in CenterNet triplet inference workflow after a proposal is generated as a pair of corner keypoints checking if there is a center keypoint of the same class center pooling for predicting center keypoints by making the center keypoints on feature map having the max sum Hori+Verti responses cascade corner pooling equips the original corner pooling module with the ability of perceiving internal information not only consider the boundary but also the internal directions CornetNetÁóõÁÇπ fp rateÈ´ò small objectÁöÑfp rateÂ∞§ÂÖ∂È´ò ‰∏Ä‰∏™ideaÔºöcornerNet based RPN ‰ΩÜÊòØÂéüÁîüRPNÈÉΩÊòØÂ§çÁî®ÁöÑ ËÆ°ÁÆóÊïàÁéáÔºü ÊñπÊ≥ï center pooling geometric centers &amp; semantic centers center poolingËÉΩÂ§üÊúâÊïàÂú∞Â∞ÜËØ≠‰πâ‰ø°ÊÅØÊúÄ‰∏∞ÂØåÁöÑÁÇπÔºàsemantic centersÔºâ‰º†ËææÂà∞Áâ©ÁêÜ‰∏≠ÂøÉÁÇπÔºàgeometric centersÔºâÔºå‰πüÂ∞±ÊòØcentral region]]></content>
      <tags>
        <tag>ÁõÆÊ†áÊ£ÄÊµãÔºåanchor-free, one-stage&amp;two-stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[equlization loss]]></title>
    <url>%2F2020%2F12%2F21%2Fequlization-loss%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[megDet]]></title>
    <url>%2F2020%2F12%2F18%2FmegDet%2F</url>
    <content type="text"><![CDATA[MegDet: A Large Mini-Batch Object Detector Âä®Êú∫ past methods mainly come from novel framework or loss design this paper studies the mini-batch size enable training with a large mini-batch size warmup learning rate policy cross-gpu batch normalization faster &amp; better acc ËÆ∫ÁÇπ potential drawbacks with small mini-batch sizes long training time inaccurate statistics for BNÔºöprevious methods use fixed statistics from ImageNet which is a sub-optimal trade-off positive &amp; negative training examples are more likely imblanced Âä†Â§ßbatch size‰ª•ÂêéÔºåÊ≠£Ë¥üÊ†∑Êú¨ÊØî‰æãÊúâÊèêÂçáÔºåÊâÄ‰ª•yolov3‰ºöÂÖàÈîÅÁùÄbackÂºÄÂ§ßbatchsizeÂÅöwarmup learning rate dilemma large min-batch size usually requires large learning rate large learning rate is likely leading to convergence failure a smaller learning rate often obtains inferior results solution of the paper linear scaling rule warmup Cross-GPU Batch Normalization (CGBN) ÊñπÊ≥ï warmup set up the learning rate small enough at the be- ginning then increase the learning rate with a constant speed after every iteration, until fixed Cross-GPU Batch Normalization ‰∏§Ê¨°ÂêåÊ≠• tensorpackÈáåÈù¢Êúâ ‰∏ÄÊ¨°ÂêåÊ≠• ÂºÇÊ≠•BNÔºöbatch size ËæÉÂ∞èÊó∂ÔºåÊØèÂº†Âç°ËÆ°ÁÆóÂæóÂà∞ÁöÑÁªüËÆ°ÈáèÂèØËÉΩ‰∏éÊï¥‰ΩìÊï∞ÊçÆÊ†∑Êú¨ÂÖ∑ÊúâËæÉÂ§ßÂ∑ÆÂºÇ ÂêåÊ≠•Ôºö ÈúÄË¶ÅÂêåÊ≠•ÁöÑÊòØÊØèÂº†Âç°‰∏äËÆ°ÁÆóÁöÑÁªüËÆ°ÈáèÔºåÂç≥BNÂ±ÇÁî®Âà∞ÁöÑÂùáÂÄº$\mu$ÂíåÊñπÂ∑Æ$\sigma^2$ ËøôÊ†∑Â§öÂç°ËÆ≠ÁªÉÁªìÊûúÊâç‰∏éÂçïÂç°ËÆ≠ÁªÉÊïàÊûúÁõ∏ÂΩì ‰∏§Ê¨°ÂêåÊ≠•Ôºö Á¨¨‰∏ÄÊ¨°ÂêåÊ≠•ÂùáÂÄºÔºöËÆ°ÁÆóÂÖ®Â±ÄÂùáÂÄº Á¨¨‰∫åÊ¨°ÂêåÊ≠•ÊñπÂ∑ÆÔºöÂü∫‰∫éÂÖ®Â±ÄÂùáÂÄºËÆ°ÁÆóÂêÑËá™ÊñπÂ∑ÆÔºåÂÜçÂèñÂπ≥Âùá ‰∏ÄÊ¨°ÂêåÊ≠•Ôºö Ê†∏ÂøÉÂú®‰∫éÊñπÂ∑ÆÁöÑËÆ°ÁÆó È¶ñÂÖàÂùáÂÄºÔºö$\mu = \frac{1}{m} \sum_{i=1}^m x_i$ ÁÑ∂ÂêéÊòØÊñπÂ∑ÆÔºö \sigma^2 = \frac{1}{m} \sum_{i=1}^m (x_i-\mu)^2 = \frac{1}{m} \sum_{i=1}^m x_i^2 - \mu^2\\ =\frac{1}{m} \sum_{i=1}^m x_i^2 - (\frac{1}{m} \sum_{i=1}^m x_i)^2 * ËÆ°ÁÆóÊØèÂº†Âç°ÁöÑ$\sum x_i$Âíå$\sum x_i^2$ÔºåÂ∞±ÂèØ‰ª•‰∏ÄÊ¨°ÊÄßÁÆóÂá∫ÊÄªÂùáÂÄºÂíåÊÄªÊñπÂ∑Æ]]></content>
      <tags>
        <tag>ÁõÆÊ†áÊ£ÄÊµãÔºålarge mini-batch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RFB]]></title>
    <url>%2F2020%2F12%2F16%2FRFB%2F</url>
    <content type="text"><![CDATA[RFB: Receptive Field Block Net for Accurate and Fast Object Detection Âä®Êú∫ RF blockÔºöReceptive Fields strengthen the lightweight features using a hand-crafted mechanismÔºöËΩªÈáèÔºåÁâπÂæÅË°®ËææËÉΩÂäõÂº∫ assemble RFB to the top of SSD ËÆ∫ÁÇπ lightweight enhance feature representation ‰∫∫Á±ª Áæ§Êô∫ÊÑüÂèóÈáéÔºàpRFÔºâÁöÑÂ§ßÂ∞èÊòØÂÖ∂ËßÜÁΩëËÜúÂõæ‰∏≠ÂÅèÂøÉÁéáÁöÑÂáΩÊï∞ ÊÑüÂèóÈáéÈöèÁùÄÂÅèÂøÉÁéáËÄåÂ¢ûÂä† Êõ¥Èù†Ëøë‰∏≠ÂøÉÁöÑÂå∫ÂüüÂú®ËØÜÂà´Áâ©‰ΩìÊó∂Êã•ÊúâÊõ¥È´òÁöÑÊØîÈáçÊàñ‰ΩúÁî® Â§ßËÑëÂú®ÂØπ‰∫éÂ∞èÁöÑÁ©∫Èó¥ÂèòÂåñ‰∏çÊïèÊÑü fixed sampling grid (conv) probably induces some loss in the feature discriminability as well as robustness inception RFs of multiple sizes but at the same center ASPP with different atrous rates the resulting feature tends to be less distinctive Deformable CNN sampling grid is flexible but all pixels in an RF contribute equally RFB varying kernel sizes applies dilated convolution layers to control their eccentricities ÁªÑÂêàÊù•Ê®°Êãühuman visual system concat 1x1 conv for fusion main contributions RFB module: enhance deep features of lightweight CNN networks RFB Net: gain on SSD assemble on MobileNet ÊñπÊ≥ï Receptive Field Block Á±ª‰ººinceptionÁöÑmulti-branch dilated pooling or convolution layer RFB Net SSD-base Â§¥‰∏äÊúâËæÉÂ§ßÂàÜËæ®ÁéáÁöÑÁâπÂæÅÂõæÁöÑconvÂ±Çare replaced by the RFB module ÁâπÂà´Â§¥‰∏äÁöÑconvÂ±ÇÂ∞±‰øùÁïô‰∫ÜÔºåÂõ†‰∏∫their feature maps are too small to apply filters with large kernels like 5 √ó 5 stride2 moduleÔºöÊØè‰∏™conv stride2ÔºåÈÇ£id pathÂæóÂèòÊàê1x1 convÔºü]]></content>
      <tags>
        <tag>ÁõÆÊ†áÊ£ÄÊµã</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PANet]]></title>
    <url>%2F2020%2F12%2F02%2FPANet%2F</url>
    <content type="text"><![CDATA[PANet: Path Aggregation Network for Instance Segmentation Âä®Êú∫ boost the information flow bottom-up path shorten information path enhance accurate localization adaptive feature pooling aggregate all levels avoiding arbitrarily assigned results mask prediction head fcn + fc captures different views, possess complementary properties subtle extra computational ËÆ∫ÁÇπ previous skills: fcn, fpn, residual, dense findings È´òÂ±ÇÁâπÂæÅÁ±ªÂà´ÂáÜÔºåÂ∫ïÂ±ÇÁâπÂæÅÂÆö‰ΩçÂáÜÔºå‰ΩÜÊòØÈ´òÂ±ÇÂíåÂ∫ïÂ±ÇÁâπÂæÅ‰πãÈó¥ÁöÑpathÂ§™Èïø‰∫ÜÔºå‰∏çÂà©‰∫éÂèåÈ´ò past proposals make predictions based on one level PANet bottom-up path shorten information path enhance accurate localization adaptive feature pooling aggregate all levels avoiding arbitrarily assigned results mask prediction head fcn + fc captures different views, possess complementary properties ÊñπÊ≥ï framework b: bottom-up path c: adaptive feature pooling e: fusion mask branch bottom-up path fpn‚Äôs top-down path: to propagate strong semantical information to ensure reasonable classification capability long path: red line, 100+ layers bottom-up path: enhances the localization capability short path: green line, less than 10 layers for each level $N_l$ input: $N_{l+1}$ &amp; $P_l$ $N_{l+1}$ 3x3 conv &amp; $P_l$ id path - add - 3x3 conv channel 256 ReLU after conv adaptive feature pooling pool features from all levels, then fuse, then predict steps map each proposal to all feature levels roi align go through one layer of the following sub-networks independently fusion operation (element-wise max or sum) ‰æãÂ¶ÇÔºåbox branchÊòØ‰∏§‰∏™fcÂ±ÇÔºåÊù•Ëá™ÂêÑ‰∏™levelÁöÑroi align‰πãÂêéÁöÑproposal featuresÔºåÂÖàÂêÑËá™ÁªèËøá‰∏Ä‰∏™fcÂ±ÇÔºåÂÜçshare the following till the headÔºåmask branchÊòØ4‰∏™convÂ±ÇÔºåÊù•Ëá™ÂêÑ‰∏™levelÁöÑroi align‰πãÂêéÁöÑproposal featuresÔºåÂÖàÂêÑËá™ÁªèËøá‰∏Ä‰∏™convÂ±ÇÔºåÂÜçshare the following till the head fusion mask branch fc layers are location sensitive helpful to differentiate instances and recognize separate parts belonging to the same object convÂàÜÊîØ 4‰∏™ËøûÁª≠conv+1‰∏™deconvÔºö3x3 convÔºåchannel256Ôºådeconv factor=2 predict mask of each classÔºöoutput channel n_classes fcÂàÜÊîØ from convÂàÜÊîØÁöÑconv3ËæìÂá∫ 2‰∏™ËøûÁª≠convÔºåchannel256Ôºåchannel128 fcÔºådim=28x28ÔºåÁâπÂæÅÂõæÂ∞∫ÂØ∏ÔºåÁî®‰∫éÂâçËÉåÊôØÂàÜÁ±ª final maskÔºöadd ÂÆûÈ™å heavier head 4 consecutive 3x3 convs shared among reg &amp; cls Âú®multi-taskÁöÑÊÉÖÂÜµ‰∏ãÔºåÂØπboxÁöÑÈ¢ÑÊµãÊúâÊïà]]></content>
      <tags>
        <tag>ÂÆû‰æãÂàÜÂâ≤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSPNet]]></title>
    <url>%2F2020%2F11%2F17%2FCSPNet%2F</url>
    <content type="text"><![CDATA[CSPNET: A NEW BACKBONE THAT CAN ENHANCE LEARNING CAPABILITY OF CNN Âä®Êú∫ propose a network from the respect of the variability of the gradients reduces computations superior accuracy while being lightweightening ËÆ∫ÁÇπ CNN architectures design ResNeXtÔºöcardinality can be more effective than width and depth DenseNetÔºöreuse features partial ResNetÔºöhigh cardinality and sparse connectionÔºåthe concept of gradient combination introduce Cross Stage Partial Network (CSPNet) strengthening learning ability of a CNNÔºösufficient accuracy while being lightweightening removing computational bottlenecksÔºöhoping evenly distribute the amount of computation at each layer in CNN reducing memory costsÔºöadopt cross-channel pooling during fpn ÊñπÊ≥ï ÁªìÊûÑ Partial Dense BlockÔºöËäÇÁúÅ‰∏ÄÂçäËÆ°ÁÆó Partial Transition LayerÔºöfusion lastËÉΩÂ§üsave computationÂêåÊó∂Á≤æÂ∫¶‰∏çÊéâÂ§™Â§ö ËÆ∫ÊñáËØ¥fusion first‰ΩøÂæóÂ§ßÈáèÊ¢ØÂ∫¶ÂæóÂà∞ÈáçÁî®Ôºåcomputation cost is significantly droppedÔºåfusion last‰ºöÊçüÂ§±ÈÉ®ÂàÜÊ¢ØÂ∫¶ÈáçÁî®Ôºå‰ΩÜÊòØÁ≤æÂ∫¶ÊçüÂ§±‰πüÊØîËæÉÂ∞è(0.1)„ÄÇ it is obvious that if one can effectively reduce the repeated gradient information, the learning ability of a network will be greatly improved. Apply CSPNet to Other Architectures Âõ†‰∏∫Âè™Êúâ‰∏ÄÂçäÁöÑchannelÂèÇ‰∏éresnet blockÁöÑËÆ°ÁÆóÔºåÊâÄ‰ª•Êó†ÈúÄÂÜçÂºïÂÖ•bottleneckÁªìÊûÑ‰∫Ü ÊúÄÂêé‰∏§‰∏™pathÁöÑËæìÂá∫concat EFM fusion * ÁâπÂæÅÈáëÂ≠óÂ°îÔºàFPNÔºâÔºöËûçÂêàÂΩìÂâçÂ∞∫Â∫¶Âíå‰ª•ÂâçÂ∞∫Â∫¶ÁöÑÁâπÂæÅ„ÄÇ * ÂÖ®Â±ÄËûçÂêàÊ®°ÂûãÔºàGFMÔºâÔºöËûçÂêàÊâÄÊúâÂ∞∫Â∫¶ÁöÑÁâπÂæÅ„ÄÇ * Á≤æÁ°ÆËûçÂêàÊ®°ÂûãÔºàEFMÔºâÔºöËûçÂêàanchorÂ∞∫ÂØ∏‰∏äÁöÑÁâπÂæÅ„ÄÇ EFM assembles features from the three scalesÔºöÂΩìÂâçÂ∞∫Â∫¶&amp;Áõ∏ÈÇªÂ∞∫Â∫¶ ÂêåÊó∂ÂèàÂä†‰∫Ü‰∏ÄÁªÑbottom-upÁöÑËûçÂêà Maxout techniqueÂØπÁâπÂæÅÊò†Â∞ÑËøõË°åÂéãÁº© ÁªìËÆ∫ ‰ªéÂÆûÈ™åÁªìÊûúÊù•ÁúãÔºå ÂàÜÁ±ªÈóÆÈ¢ò‰∏≠Ôºå‰ΩøÁî®CSPNetÂèØ‰ª•Èôç‰ΩéËÆ°ÁÆóÈáèÔºå‰ΩÜÊòØÂáÜÁ°ÆÁéáÊèêÂçáÂæàÂ∞èÔºõ Âú®ÁõÆÊ†áÊ£ÄÊµãÈóÆÈ¢ò‰∏≠Ôºå‰ΩøÁî®CSPNet‰Ωú‰∏∫BackboneÂ∏¶Êù•ÁöÑÊèêÂçáÊØîËæÉÂ§ßÔºåÂèØ‰ª•ÊúâÊïàÂ¢ûÂº∫CNNÁöÑÂ≠¶‰π†ËÉΩÂäõÔºåÂêåÊó∂‰πüÈôç‰Ωé‰∫ÜËÆ°ÁÆóÈáè„ÄÇÊú¨ÊñáÊâÄÊèêÂá∫ÁöÑEFMÊØîGFMÊÖ¢2fpsÔºå‰ΩÜAPÂíåAP50ÂàÜÂà´ÊòæËëóÊèêÈ´ò‰∫Ü2.1%Âíå2.4%„ÄÇ]]></content>
  </entry>
  <entry>
    <title><![CDATA[nms]]></title>
    <url>%2F2020%2F10%2F29%2Fnms%2F</url>
    <content type="text"><![CDATA[Non-maximum suppressionÔºöÈùûÊûÅÂ§ßÂÄºÊäëÂà∂ÁÆóÊ≥ïÔºåÊú¨Ë¥®ÊòØÊêúÁ¥¢Â±ÄÈÉ®ÊûÅÂ§ßÂÄºÔºåÊäëÂà∂ÈùûÊûÅÂ§ßÂÄºÂÖÉÁ¥† [nms]Ôºöstandard nmsÔºåÂΩìÁõÆÊ†áÊØîËæÉÂØÜÈõÜ„ÄÅÂ≠òÂú®ÈÅÆÊå°Êó∂ÔºåÊºèÊ£ÄÁéáÈ´ò [soft nms]ÔºöÊîπÂèònmsÁöÑhard thresholdÔºåÁî®ËæÉ‰ΩéÁöÑÂàÜÊï∞Êõø‰ª£0ÔºåÊèêÂçárecall [softer nms]ÔºöÂºïÂÖ•box position confidenceÔºåÈÄöËøáÂêéÂ§ÑÁêÜÊèêÈ´òÂÆö‰ΩçÁ≤æÂ∫¶ [DIoU nms]ÔºöÈááÁî®DIoUÁöÑËÆ°ÁÆóÊñπÂºèÊõøÊç¢IoUÔºåÂõ†‰∏∫DIoUÁöÑËÆ°ÁÆóËÄÉËôëÂà∞‰∫Ü‰∏§Ê°Ü‰∏≠ÂøÉÁÇπ‰ΩçÁΩÆÁöÑ‰ø°ÊÅØÔºåÊïàÊûúÊõ¥‰ºò [fast nms]ÔºöYOLOACTÂºïÂÖ•Áü©Èòµ‰∏âËßíÂåñÔºå‰ºöÊØîTraditional NMSÊäëÂà∂Êõ¥Â§öÁöÑÊ°ÜÔºåÊÄßËÉΩÁï•ÂæÆ‰∏ãÈôç [cluster nms]ÔºöCIoUÊèêÂá∫ÔºåÂº•Ë°•Fast NMSÁöÑÊÄßËÉΩ‰∏ãÈôçÔºåËøêÁÆóÊïàÁéáÊØîFast NMS‰∏ãÈôç‰∫Ü‰∏Ä‰∫õ [mask nms]Ôºömask iouËÆ°ÁÆóÊúâ‰∏çÂèØÂøΩÁï•ÁöÑÂª∂ËøüÔºåÂõ†Ê≠§ÊØîbox nmsÊõ¥ËÄóÊó∂ [matrix nms]ÔºöSOLOÂ∞Ümask IoUÂπ∂Ë°åÂåñÔºåÊØîFAST-NMSËøòÂø´ÔºåÊÄùË∑ØÂíåFAST-NMS‰∏ÄÊ†∑‰ªé‰∏ä‰∏âËßíIoUÁü©ÈòµÂá∫ÂèëÔºåÂèØËÉΩÈÄ†ÊàêËøáÂ§öÊäëÂà∂„ÄÇ [WBF]ÔºöÂä†ÊùÉÊ°ÜËûçÂêàÔºåKaggleËÉ∏ÁâáÂºÇÁâ©ÊØîËµõclaimÊúâÁî®ÔºåÈÄüÂ∫¶ÊÖ¢ÔºåÂ§ßÊ¶ÇÊØîÊ†áÂáÜNMSÊÖ¢3ÂÄçÔºåWBFÂÆûÈ™å‰∏≠ÊòØÂú®Â∑≤ÁªèÂÆåÊàêNMSÁöÑÊ®°Âûã‰∏äËøõË°åÁöÑ nms ËøáÊª§+Ëø≠‰ª£+ÈÅçÂéÜ+Ê∂àÈô§ È¶ñÂÖàËøáÊª§ÊéâÂ§ßÈáèÁΩÆ‰ø°Â∫¶ËæÉ‰ΩéÁöÑÊ°ÜÔºåÂ§ß‰∫éconfidence threshÁöÑbox‰øùÁïô Â∞ÜÊâÄÊúâÊ°ÜÁöÑÂæóÂàÜÊéíÂ∫èÔºåÈÄâ‰∏≠ÊúÄÈ´òÂàÜÁöÑÊ°Ü ÈÅçÂéÜÂÖ∂‰ΩôÁöÑÊ°ÜÔºåÂ¶ÇÊûúÂíåÂΩìÂâçÊúÄÈ´òÂàÜÊ°ÜÁöÑIOUÂ§ß‰∫é‰∏ÄÂÆöÈòàÂÄº(nms thresh)ÔºåÂ∞±Â∞ÜÊ°ÜÂà†Èô§(score=0) ‰ªéÊú™Â§ÑÁêÜÁöÑÊ°Ü‰∏≠ÁªßÁª≠ÈÄâ‰∏Ä‰∏™ÂæóÂàÜÊúÄÈ´òÁöÑÔºåÈáçÂ§ç‰∏äËø∞ËøáÁ®ã when evaluation iou threshÔºöÁïô‰∏ãÁöÑboxÈáåÈù¢Ôºå‰∏égt boxÁöÑiouÂ§ß‰∫éiou threshÁöÑbox‰Ωú‰∏∫Ê≠£‰æãÔºåÁî®‰∫éËÆ°ÁÆóÂá∫APÂíåmAPÔºåÈÄöËøáË∞ÉÊï¥confidence threshÂèØ‰ª•ÁîªÂá∫PRÊõ≤Á∫ø softnms Âü∫Êú¨ÊµÅÁ®ãËøòÊòØnmsÁöÑË¥™Â©™ÊÄùË∑ØÔºåËøáÊª§+Ëø≠‰ª£+ÈÅçÂéÜ+Ë°∞Âáè re-score functionÔºöhigh overlap decays more linearÔºö for each $iou(M,b_i)&gt;th$Ôºå $s_i=s_i(1-iou)$ not continuousÔºåsudden penalty gaussianÔºö for all remaining detection boxesÔºå$s_i=s_i e^{-\frac{iou(M,b_i)}{\sigma}}$ ÁÆóÊ≥ïÊµÅÁ®ã‰∏äÊú™ÂÅö‰ºòÂåñÔºåÊòØÈíàÂØπÁ≤æÂ∫¶ÁöÑ‰ºòÂåñ softer nms Ë∑üsoft nmsÊ≤°ÂÖ≥Á≥ª ÂÖ∑ÊúâÈ´òÂàÜÁ±ªÁΩÆ‰ø°Â∫¶ÁöÑËæπÊ°ÜÂÖ∂‰ΩçÁΩÆÂπ∂‰∏çÊòØÊúÄÁ≤æÂáÜÁöÑ Êñ∞Â¢ûÂä†‰∫Ü‰∏Ä‰∏™ÂÆö‰ΩçÁΩÆ‰ø°Â∫¶ÁöÑÈ¢ÑÊµãÔºå‰ΩøÂÖ∂Êúç‰ªéÈ´òÊñØÂàÜÂ∏É inferÈò∂ÊÆµËæπÊ°ÜÁöÑÊ†áÂáÜÂ∑ÆÂèØ‰ª•Ë¢´ÁúãÂÅöËæπÊ°ÜÁöÑ‰ΩçÁΩÆÁΩÆ‰ø°Â∫¶Ôºå‰∏éÂàÜÁ±ªÁΩÆ‰ø°Â∫¶ÂÅöÂä†ÊùÉÂπ≥ÂùáÔºå‰Ωú‰∏∫total score ÁÆóÊ≥ïÊµÅÁ®ã‰∏äÊú™ÂÅö‰ºòÂåñÔºåÂÆåÂÖ®ÊòØÁ≤æÂ∫¶ÁöÑ‰ºòÂåñ DIoU nms ‰πüÊòØ‰∏∫‰∫ÜËß£ÂÜ≥hard nmsÂú®ÂØÜÈõÜÂú∫ÊôØ‰∏≠ÊºèÊ£ÄÁéáÈ´òÁöÑÈóÆÈ¢ò ‰ΩÜÊòØ‰∏çÂêå‰∫ésoft nmsÁöÑÊòØÔºåDÁöÑÊîπËøõÂú®iouËÆ°ÁÆó‰∏äÔºåËÄå‰∏çÊòØÂú®score diouÁöÑËÆ°ÁÆóÔºö$diou = iou-\frac{\rho^2(b_1, b_2)}{c^2}$ ÁÆóÊ≥ïÊµÅÁ®ã‰∏äÊú™ÂÅö‰ºòÂåñÔºå‰ªçÊóßÊòØÁ≤æÂ∫¶ÁöÑ‰ºòÂåñ fast nms yoloactÊèêÂá∫ ‰∏ªË¶ÅÊïàÁéáÊèêÂçáÂú®‰∫éÁî®Áü©ÈòµÊìç‰ΩúÊõøÊç¢ÈÅçÂéÜÔºåÊâÄÊúâÊ°ÜÂêåÊó∂Ë¢´filterÊéâÔºåËÄåÈùû‰æùÊ¨°ÈÅçÂéÜÂà†Èô§ iou‰∏ä‰∏âËßíÁü©Èòµ iou‰∏ä‰∏âËßíÁü©ÈòµÁöÑÊØè‰∏Ä‰∏™ÂÖÉÁ¥†ÈÉΩÊòØË°åÂè∑Â∞è‰∫éÂàóÂè∑ iou‰∏ä‰∏âËßíÁü©ÈòµÁöÑÊØè‰∏Ä‰∏™Ë°åÔºåÂØπÂ∫î‰∏Ä‰∏™bnd boxÔºå‰∏éÂÖ∂‰ªñÊâÄÊúâscoreÂ∞è‰∫éÂÆÉÁöÑbnd boxÁöÑiou iou‰∏ä‰∏âËßíÁü©ÈòµÁöÑÊØè‰∏Ä‰∏™ÂàóÔºåÂØπÂ∫î‰∏Ä‰∏™bnd boxÔºå‰∏éÂÖ∂‰ªñÊâÄÊúâscoreÂ§ß‰∫éÂÆÉÁöÑbnd boxÁöÑiou fast nmsÂú®iouÁü©ÈòµÊØè‰∏ÄÂàó‰∏äÊ±ÇÊúÄÂ§ßÂÄºÔºåÂ¶ÇÊûúËøô‰∏™ÊúÄÂ§ßÂÄºÂ§ß‰∫éiou threshÔºåËØ¥ÊòéÂΩìÂâçÂàóÂØπÂ∫îÁöÑbnd boxÔºåÂ≠òÂú®‰∏Ä‰∏™scoreÂ§ß‰∫éÂÆÉÔºå‰∏îÂíåÂÆÉÈáçÂè†Â∫¶ËæÉÈ´òÁöÑbnd boxÔºåÂõ†Ê≠§Ë¶ÅÊääËøô‰∏™boxËøáÊª§Êéâ ÊúâÁ≤æÂ∫¶ÊçüÂ§± Âú∫ÊôØÔºö Â¶ÇÊûúÊòØhard nmsÁöÑËØùÔºåÈ¶ñÂÖàÈÅçÂéÜb1ÁöÑÂÖ∂‰ªñboxÔºåb2Â∞±Ë¢´Âà†Èô§‰∫ÜÔºåËøôÊòØb3Â∞±‰∏çÂ≠òÂú®È´òÈáçÂè†Ê°Ü‰∫ÜÔºåb3Â∞±‰ºöË¢´Áïô‰∏ãÔºå‰ΩÜÊòØÂú®fast nmsÂú∫ÊôØ‰∏ãÔºåÊâÄÊúâÊ°ÜË¢´ÂêåÊó∂Âà†Èô§ÔºåÂõ†Ê≠§b2„ÄÅb3ÈÉΩÊ≤°‰∫Ü„ÄÇ cluster nms ÈíàÂØπfast nmsÊÄßËÉΩ‰∏ãÈôçÁöÑÂº•Ë°• fast nmsÊÄßËÉΩ‰∏ãÈôçÔºå‰∏ªË¶ÅÈóÆÈ¢òÂú®‰∫éËøáÂ∫¶ÊäëÂà∂ÔºåÂπ∂Ë°åÊìç‰ΩúÊó†Ê≥ïÂèäÊó∂Ê∂àÈô§high scoreÊ°ÜÊäπÊéâÂØπÂêéÁª≠low scoreÊ°ÜÂà§Êñ≠ÁöÑÂΩ±Âìç ÁÆóÊ≥ïÊµÅÁ®ã‰∏äÔºåÂ∞Üfast nmsÁöÑ‰∏ÄÊ¨°ÈòàÂÄºÊìç‰ΩúÔºåËΩ¨Êç¢ÊàêÂ∞ëÊï∞Âá†Ê¨°ÁöÑËø≠‰ª£Êìç‰ΩúÔºåÊØèÊ¨°ÈÉΩÊòØ‰∏Ä‰∏™fast nms Âõæ‰∏≠XË°®Á§∫iouÁü©ÈòµÔºåbË°®Á§∫nmsÈòàÂÄº‰∫åÂÄºÂåñ‰ª•ÂêéÁöÑÂêëÈáèÔºå‰πüÂ∞±ÊòØfast nmsÈáåÈù¢ÈÇ£‰∏™‰øùÁïôÔºèÊäëÂà∂ÂêëÈáè ÊØèÊ¨°Ëø≠‰ª£ÔºåÁÆóÊ≥ïÂ∞ÜbÂ±ïÂºÄÊàê‰∏Ä‰∏™ÂØπËßíÁü©ÈòµÔºåÁÑ∂ÂêéÂ∑¶‰πòiouÁü©Èòµ Áõ¥Âà∞Âá∫Áé∞Êüê‰∏§Ê¨°Ëø≠‰ª£ÂêéÔºå b‰øùÊåÅ‰∏çÂèò‰∫ÜÔºåÈÇ£‰πàËøôÂ∞±ÊòØÊúÄÁªàÁöÑb cluster nmsÁöÑËø≠‰ª£Êìç‰ΩúÔºåÂÖ∂ÂÆûÂ∞±ÊòØÂú®ÁúÅÁï•‰∏ä‰∏ÄÊ¨°Fast NMSËø≠‰ª£‰∏≠Ë¢´ÊäëÂà∂ÁöÑÊ°ÜÂØπÂÖ∂‰ªñÊ°ÜÁöÑÂΩ±Âìç Êï∞Â≠¶ÂΩíÁ∫≥Ê≥ïËØÅÊòéÔºåcluster nmsÁöÑÁªìÊûú‰∏éhard nmsÂÆåÂÖ®‰∏ÄËá¥ÔºåËøêÁÆóÊïàÁéáÊØîfast nms‰∏ãÈôç‰∫Ü‰∏Ä‰∫õÔºå‰ΩÜÊòØÊØîhard nmsÂø´ÂæóÂ§ö cluster nmsÁöÑËøêÁÆóÊïàÁéá‰∏ç‰∏éclusterÊï∞ÈáèÊúâÂÖ≥ÔºåÂè™‰∏éÈúÄË¶ÅËø≠‰ª£Ê¨°Êï∞ÊúÄÂ§öÁöÑÈÇ£‰∏Ä‰∏™clusterÊúâÂÖ≥ mask nms ‰ªéÊ£ÄÊµãÊ°ÜÂΩ¢Áä∂ÁöÑËßíÂ∫¶ÊãìÂ±ïÂá∫Êù•ÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫émask nms„ÄÅpolygon nms‰ª•Âèäinclined nms iouÁöÑËÆ°ÁÆóÊñπÂºèÊúâ‰∏ÄÁßçÊòØmmiÔºö$mmi=max(\frac{I}{I_A}, \frac{I}{I_B})$ matrix nms Â≠¶‰π†soft nmsÔºödecay factor one step furtherÔºöËø≠‰ª£ÊîπÂπ∂Ë°å ÂØπ‰∫éÊüê‰∏™object $m_j$ÁöÑscoreËøõË°åpenaltyÁöÑÊó∂ÂÄôËÄÉËôë‰∏§ÈÉ®ÂàÜÂΩ±Âìç Ëø≠‰ª£Êüê‰∏™$m_i$Êó∂ÔºåÂØπÂêéÁª≠lower scoreÁöÑ$m_j$ÁöÑÂΩ±Âìç ‰∏ÄÊòØÊ≠£Èù¢ÂΩ±Âìç$f(iou_{i,j})\ linear/guassian$ÔºöËøô‰∏™Ê°Ü‰øùÁïôÔºåÈÇ£‰πàÂêéÁª≠Ê°ÜÈÉΩË¶ÅÂü∫‰∫é‰∏éÂÖ∂ÁöÑiouÂÅödecay ‰∫åÊòØÂèçÂêëÂΩ±Âìç$f(iou_{*,i})=max_{\forall s_k&gt;s_i}f(iou_{k,i})$ÔºöÂ¶ÇÊûúËøô‰∏™Ê°Ü‰∏ç‰øùÁïôÔºåÈÇ£‰πàÂØπ‰∫éÂêéÁª≠Ê°ÜÊù•ËÆ≤ÔºåÂ∫îËØ•Ê∂àÈô§Ëøô‰∏™Ê°ÜÂØπÂÖ∂ÁöÑdecayÔºåÈÄâÊúÄÂ§ßÂÄºÁöÑÊÑè‰πâÊòØÂΩìÂâçmaskË¢´ÊäëÂà∂ÊúÄÊúâÂèØËÉΩÂ∞±ÊòØÂíå‰ªñÈáçÂè†Â∫¶ÊúÄÂ§ßÁöÑÈÇ£‰∏™maskÂπ≤ÁöÑÔºàÂõ†‰∏∫ÂØπÂ∫îÁöÑÊ≠£Èù¢ÂΩ±Âìç1-iouÊúÄÂ∞èÔºâ final decay factorÔºö$decay_j=min_{\forall s_i &gt; s_j}\frac{f(iou_{i,j})}{f(iou_{*,i})}$ ÁÆóÊ≥ïÊµÅÁ®ã &lt;img src=&quot;nms/matrixnms.png&quot; width=&quot;50%;&quot; /&gt; * ÊåâÁÖßÂéüËÆ∫ÊñáÁöÑÂÆûÁé∞ÔºådecayÊ∞∏ËøúÂ§ß‰∫éÁ≠â‰∫é1ÔºåÂõ†‰∏∫ÊØè‰∏ÄÂàóÁöÑiou_cmaxÊ∞∏ËøúÂ§ß‰∫éÁ≠â‰∫éiouÔºå‰ªéËÆ∫ÊñáÁöÑÊÄùË∑ØÊù•ÁúãÔºåÊØè‰∏™maskÁöÑdecayÊòØÂÆÉ‰πãÂâçÊâÄÊúâmaskÁöÑÂΩ±ÂìçÂè†Âä†Âú®‰∏ÄËµ∑ÔºåÊâÄ‰ª•Â∫îËØ•ÊòØ‰πòÁßØËÄå‰∏çÊòØminÔºö 123456789101112# ÂéüËÆ∫ÊñáÂÆûÁé∞if method=='gaussian': decay = np.exp(-(np.square(iou)-np.square(iou_cmax))/sigma)else: decay = (1-iou)/(1-iou_cmax)decay = np.min(decay, axis=0)# ÊîπËøõÂÆûÁé∞if method=='gaussian': decay = np.exp(-(np.sum(np.square(iou),axis=0)-np.square(iou_cmax))/sigma)else: decay = np.prod(1-iou)/(1-iou_cmax)]]></content>
      <tags>
        <tag>ÁõÆÊ†áÊ£ÄÊµã</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MimicDet]]></title>
    <url>%2F2020%2F10%2F14%2FMimicDet%2F</url>
    <content type="text"><![CDATA[[MimicDet] ResNeXt-101 backbone on the COCO: 46.1 mAP MimicDet: Bridging the Gap Between One-Stage and Two-Stage Object Detection Âä®Êú∫ mimic taskÔºöknowledge distillation mimic the two-stage features a shared backbone two heads for mimicking end-to-end training specialized designs to facilitate mimicking dual-path mimicking staggered feature pyramid reach two-stage accuracy ËÆ∫ÁÇπ one-stage detectors adopt a straightforward fully convolutional architecture two-stage detectors use RPN + R-CNN advantages of two-stage detectors avoid class imbalance less proposals enables larger cls net and richer features RoIAlign extracts location consistent feature -&gt; better represenation regress the object location twice -&gt; better refined one-stage detectors‚Äô imitation RefineDetÔºöcascade detection flow AlignDetÔºöRoIConv layer still leaves a big gap network mimicking knowledge distillation use a well-trained large teacher model to supervise difference mimic in heads instead of backbones teacher branch instead of model trained jointly this method not only mimic the structure design, but also imitate in the feature level contains both one-stage detection head and two-stage detection head during training share the same backbone two-stage detection head, called T-head one-stage detection head, called S-head similarity loss for matching featureÔºöguided deformable conv layer together with detection losses specialized designs decomposed detection heads conduct mimicking in classification and regression branches individually staggered feature pyramid ÊñπÊ≥ï overview back &amp; fpn RetinaNet fpnÔºöwith P6 &amp; P7 crucial modificationÔºöP2 ÔΩû P7 staggered feature pyramid high-res set {P2 to P6}Ôºöfor T-head &amp; accuray low-res set {P3 to P7}Ôºöfor S-head &amp; computation speed refinement module filter out easy negativesÔºömitigate the class imbalance issue adjust the location and size of pre-defined anchor boxesÔºöanchor initialization module on top of the feature pyramid one 3x3 conv two sibling 1x1 convs binary classificationÔºöbce loss bounding box regressionÔºöthe same as Faster R-CNNÔºåL1 loss top-ranked boxes transferred to T-head and S-head one anchor on each positionÔºöavoid feature sharing among proposals assign the objects to feature pyramid according to their scale positive areaÔºö0.3 times shrinking of gt boxes from center positive sampleÔºö valid scale rangeÔºögt target belongs to this level central point of anchor lies in the positive area detection heads T-head heavy head run on a sparse set of anchor boxes use the staggered feature pyramid generate 7x7 location-sensitive features for each anchor box cls branch two 1024-d fc layers one 81-d fc layer + softmaxÔºöce loss reg branch four 3x3 convsÔºåch256 flatten 1024-d fc 4-d fcÔºöL1 loss mimicking target 81-d classification logits 1024-d regression feature S-head light-weight directly dense detection on fpn „Äê‰∏çÂ§™ÁêÜËß£„Äëintroducing the refinement module will break the location consistency between the anchor box and its corresponding featuresÔºöÊàëÁöÑÁêÜËß£ÊòØrefine‰ª•ÂêéÁöÑanchorÂíåÂéüÂßãanchorÂØπÂ∫îÁöÑÁâπÂæÅÂõæmisalign‰∫ÜÔºåT-headÁî®ÁöÑÊòØrefined anchorÔºåS-headÁî®ÁöÑÊòØoriginal gridÔºåÊâÄ‰ª•misalign use deformable convolution to capture the misaligned feature deformation offset is computed by a micro-network takes the regression output of the refinement module as input three 1x1 convsÔºåch64/128Ôºè18(50) 3x3 Dconv for P3 and 5x5 for othersÔºåch256 two sibling 1x1 convsÔºåch1024 cls branchÔºö1x1 convÔºåch80 reg branchÔºö1x1 convÔºåch4 head mimicking cosine similarity cls logits &amp; refine params To get the S-head feature of an adjusted anchor box trace back to its initial position extract the pixel at that position in the feature map lossÔºö$L_{mimic} = 1 - cosine(F_i^T, F_i^S)$ multi-task training loss $L = L_R + L_S + L_T + L_{mimic}$ $L_R$Ôºörefine module lossÔºåbce+L1 $L_S$ÔºöS-head lossÔºåce+L1 $L_T$ÔºöT-head lossÔºåce+L1 $L_{mimic}$Ôºömimic loss training details networkÔºöresnet50/101Ôºåresize image with shorter side 800 refinement module run NMS with 0.8 IoU threshold on anchor boxes select top 2000 boxes T-head sample 128 boxes from proposal pÔºènÔºö1/3 S-head hard miningÔºöselect 128 boxes with top loss value inference take top 1000 boxes from refine module NMS with 0.6 IoU threshold and 0.005 score threshold „ÄêÔºüÔºü„Äëfinally top 100 scoring boxesÔºöËøôÂùó‰∏çÂ§™ÁêÜËß£ÔºåÊúÄÂêéÂ∫îËØ•‰∏çÊòØÁªìÊûÑÂåñËæìÂá∫‰∫ÜÂïäÔºåÂ∫îËØ•ÊòØ‰∏ÄÈò∂ÊÆµÊ£ÄÊµãÂ§¥ÁöÑre-refineËæìÂá∫Âïä]]></content>
      <tags>
        <tag>ÁõÆÊ†áÊ£ÄÊµã</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[metrics]]></title>
    <url>%2F2020%2F10%2F09%2Fmetrics%2F</url>
    <content type="text"><![CDATA[ÂàÜÁ±ªÊåáÊ†á recallÔºöÂè¨ÂõûÁéá precisionÔºöÂáÜÁ°ÆÁéá accuracyÔºöÊ≠£Á°ÆÁéá F-Measure sensitivityÔºöÁÅµÊïèÂ∫¶ specificityÔºöÁâπÂºÇÂ∫¶ TPR FPR ROC AUC Ê∑∑Ê∑ÜÁü©Èòµ | | gt is p | gt is n | | :‚Äî‚Äî‚Äî-: | :‚Äî‚Äî‚Äî‚Äî: | :‚Äî‚Äî‚Äî‚Äî‚Äî: | | pred is p | tp | fpÔºàÂÅáÈò≥ÊÄßÔºâ | | pred is n | fnÔºàÊºèÊ£ÄÔºâ | tn | Ê≥®ÊÑèÂå∫ÂàÜfpÂíåfn fpÔºöË¢´ÈîôËØØÂú∞ÂàíÂàÜ‰∏∫Ê≠£‰æãÁöÑ‰∏™Êï∞ÔºåÂç≥ÂÆûÈôÖ‰∏∫Ë¥ü‰æã‰ΩÜË¢´ÂàÜÁ±ªÂô®ÂàíÂàÜ‰∏∫Ê≠£‰æãÁöÑÂÆû‰æãÊï∞ fnÔºöË¢´ÈîôËØØÂú∞ÂàíÂàÜ‰∏∫Ë¥ü‰æãÁöÑ‰∏™Êï∞ÔºåÂç≥ÂÆûÈôÖ‰∏∫Ê≠£‰æã‰ΩÜË¢´ÂàÜÁ±ªÂô®ÂàíÂàÜ‰∏∫Ë¥ü‰æãÁöÑÂÆû‰æãÊï∞ recall Ë°°ÈáèÊü•ÂÖ®Áéá ÂØπgt is pÂÅöÁªüËÆ° $recall = \frac{tp}{tp+fn}$ precision Ë°°ÈáèÊü•ÂáÜÁéá ÂØπpred is pÂÅöÁªüËÆ° $precision = \frac{tp}{tp+fp}$ accuracy ÂØπÁöÑÈô§‰ª•ÊâÄÊúâ $accuracy = \frac{tp+tn}{p+n}$ sensitivity Ë°°ÈáèÂàÜÁ±ªÂô®ÂØπÊ≠£‰æãÁöÑËØÜÂà´ËÉΩÂäõ ÂØπgt is pÂÅöÁªüËÆ° $sensitivity = \frac{tp}{p}=\frac{tp}{tp+fn}$ specificity Ë°°ÈáèÂàÜÁ±ªÂô®ÂØπË¥ü‰æãÁöÑËØÜÂà´ËÉΩÂäõ ÂØπgt is nÂÅöÁªüËÆ° $specificity =\frac{tn}{n}= \frac{tn}{fp+tn}$ F-measure ÁªºÂêàËÄÉËôëPÂíåRÔºåÊòØPrecisionÂíåRecallÂä†ÊùÉË∞ÉÂíåÂπ≥Âùá $F = \frac{(a^2+1)PR}{a^2*P+R}$ $F_1 = \frac{2PR}{P+R}$ TPR Â∞ÜÊ≠£‰æãÂàÜÂØπÁöÑÊ¶ÇÁéá ÂØπgt is tÂÅöÁªüËÆ° $TPR = \frac{tp}{tp+fn}$ FPR Â∞ÜË¥ü‰æãÈîôÂàÜ‰∏∫Ê≠£‰æãÁöÑÊ¶ÇÁéá ÂØπgt is nÂÅöÁªüËÆ° $FPR = \frac{fp}{fp+tn}$ FPR = 1 - ÁâπÂºÇÂ∫¶ ROC ÊØè‰∏™ÁÇπÁöÑÊ®™ÂùêÊ†áÊòØFPRÔºåÁ∫µÂùêÊ†áÊòØTPR ÊèèÁªò‰∫ÜÂàÜÁ±ªÂô®Âú®TPÔºàÁúüÊ≠£ÁöÑÊ≠£‰æãÔºâÂíåFPÔºàÈîôËØØÁöÑÊ≠£‰æãÔºâÈó¥ÁöÑtrade-off ÈÄöËøáÂèòÂåñÈòàÂÄºÔºåÂæóÂà∞‰∏çÂêåÁöÑÂàÜÁ±ªÁªüËÆ°ÁªìÊûúÔºåËøûÊé•Ëøô‰∫õÁÇπÂ∞±ÂΩ¢ÊàêROC curve Êõ≤Á∫øÂú®ÂØπËßíÁ∫øÂ∑¶‰∏äÊñπÔºåÁ¶ªÂæóË∂äËøúËØ¥ÊòéÂàÜÁ±ªÊïàÊûúÂ•Ω P/RÂíåROCÊòØ‰∏§‰∏™‰∏çÂêåÁöÑËØÑ‰ª∑ÊåáÊ†áÂíåËÆ°ÁÆóÊñπÂºèÔºå‰∏ÄËà¨ÊÉÖÂÜµ‰∏ãÔºåÊ£ÄÁ¥¢Áî®ÂâçËÄÖÔºåÂàÜÁ±ª„ÄÅËØÜÂà´Á≠âÁî®ÂêéËÄÖ AUC AUCÁöÑÂÄºÂ∞±ÊòØÂ§Ñ‰∫éROC curve‰∏ãÊñπÁöÑÈÇ£ÈÉ®ÂàÜÈù¢ÁßØÁöÑÂ§ßÂ∞è ÈÄöÂ∏∏ÔºåAUCÁöÑÂÄº‰ªã‰∫é0.5Âà∞1.0‰πãÈó¥]]></content>
      <tags>
        <tag>ËØÑ‰ª∑ÊåáÊ†á</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[metric learningÁ≥ªÂàó]]></title>
    <url>%2F2020%2F09%2F25%2Fmetric-learning%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[ÂèÇËÄÉÔºöhttps://gombru.github.io/2019/04/03/ranking_loss/ÔºåÂçö‰∏ªÂÆûÈ™å‰∏ãÊù•ËßâÂæóTriplet Loss outperforms Cross-Entropy Loss ÁªºËø∞ metric learning Â∏∏ËßÑÁöÑcls lossÁ≥ªÂàó(CE„ÄÅBCE„ÄÅMSE)ÁöÑÁõÆÊ†áÊòØpredict a label metric lossÁ≥ªÂàóÁöÑÁõÆÊ†áÂàôÊòØpredict relative distances between inputs Â∏∏Áî®Âú∫ÊôØÔºö‰∫∫ËÑ∏ &amp; fine-grained relation between samples first get the embedded representation then compute the similarity score binary (similar / dissimilar) regression (euclidian distance) Â§ßÁ±ªÔºö‰∏çÁÆ°Âè´Âï•Ôºå‰∏ª‰Ωì‰∏äÂ∞±‰∏§Á±ªÔºå‰∫åÂÖÉÁªÑÂíå‰∏âÂÖÉÁªÑ common targetÔºöÊãâËøëÁ±ªÂÜÖË∑ùÁ¶ªÔºåÊãâÂ§ßÁ±ªÈó¥Ë∑ùÁ¶ª pairs anchor + sample positive pairsÔºödistance ‚Äî&gt; 0 negative pairsÔºödisctance &gt; a margin triplets anchor + pos sample + neg sample targetÔºö(dissimilar distance - similar distance) ‚Äî&gt; a margin papers [siamese network] Signature Verification using a ‚ÄòSiamese‚Äô Time Delay Neural NetworkÔºö1993ÔºålecunÔºåÂ≠™ÁîüÁΩëÁªúÂßãÁ•ñÔºå‰ø©‰∏™Â≠êÁΩëÁªúsharing weightsÔºåË∑ùÁ¶ªÁî®ÁöÑÊòØcosine distanceÔºålossÁõ¥Êé•‰ºòÂåñË∑ùÁ¶ªÔºå‰ºòÂåñtargetÊòØ‰∏™ÂÆöÂÄºcosine=1.0/-1.0 [contrastive loss] Dimensionality Reduction by Learning an Invariant MappingÔºö2006ÔºålecunÔºåcontrastive lossÂßãÁ•ñÔºåÁ†îÁ©∂ÁöÑÊòØÈ´òÁª¥ÁâπÂæÅÂêëÈáèÂêë‰ΩéÁª¥Êò†Â∞ÑÁöÑÈùûÁ∫øÊÄßÂ±ÇÔºåË∑ùÁ¶ªÁî®ÁöÑÊòØeuclidean distanceÔºåloss‰ºòÂåñÁöÑÊòØsquared distanceÔºå‰ºòÂåñtargetÊòØ0ÂíåmÔºåsimilar pairs‰ªçÊóß‰ºöË¢´Êé®Âêë‰∏Ä‰∏™ÂÆöÁÇπÔºåÊ≤°ÊúâËß£ÂÜ≥ËÆ∫ÊñáÂ£∞Áß∞ÁöÑuniform distribution [triplet-loss] Learning Fine-grained Image Similarity with Deep RankingÔºö2014ÔºåGoogleÔºåÁî®‰∫Ü‰∏âÂÖÉÁªÑÔºåÊèêÂá∫‰∫Ütriplet-loss [facenet] FaceNet: A Unified Embedding for Face Recognition and ClusteringÔºö2015ÔºåGoogleÔºåÁî®Êù•ËØÜÂà´‰∫∫ËÑ∏ÔºåÁî®‰∫Ü‰∏âÂÖÉÁªÑÂíåtriplet-lossÔºåsquared euclidean distanceÔºå‰ºòÂåñÁõÆÊ†áÊòØÂêåÁ±ªÂíåÂºÇÁ±ªpair‰πãÈó¥ÁöÑÁõ∏ÂØπË∑ùÁ¶ªÔºåÂõ∞ÈöæÊ†∑Êú¨Ôºàsemi-hard &amp; hardÔºâÂØπÊî∂ÊïõËµ∑‰ΩúÁî®ÔºàÂä†ÈÄüÔºèlocal minimaÔºâÔºåtriplet-lossËÄÉËôë‰∫ÜÁ±ªÈó¥ÁöÑÁ¶ªÊï£ÊÄßÔºå‰ΩÜÊ≤°ÊúâËÄÉËôëÁ±ªÂÜÖÁöÑÁ¥ßÂáëÊÄß [center-loss] A Discriminative Feature Learning Approach for Deep Face RecognitionÔºö2016Ôºå‰πüÊòØÁî®Âú®‰∫∫ËÑ∏‰ªªÂä°‰∏äÔºå‰ºòÂåñÁõÆÊ†áÊòØÁ±ªÂÜÖÁöÑÁªùÂØπË∑ùÁ¶ªÔºåËÄå‰∏çÊòØÂª∫Ê®°Áõ∏ÂØπÂÖ≥Á≥ªÔºåcenter-lossÁõ¥Êé•‰ºòÂåñÁöÑÊòØÁ±ªÈó¥ÁöÑÈó¥ÂáëÊÄßÔºåÁ±ªÈó¥ÁöÑÁ¶ªÊï£ÊÄßÈù†ÁöÑÊòØsoftmax loss [triplet-center-loss] Triplet-Center Loss for Multi-View 3D Object RetrievalÔºö2018Ôºå‰∏úÊãºË•øÂáëÊ∞¥ËÆ∫Êñá [Hinge-loss] SVM margin [circle-loss] Circle Loss: A Unified Perspective of Pair Similarity OptimizationÔºö2020CVPRÔºåÊó∑ËßÜÔºåÊèêÂá∫‰∫Ücls lossÂíåmetric lossÁöÑÁªü‰∏ÄÂΩ¢Âºè$minimize(s_n - s_p+m)$ÔºåÂú®Ê≠§Âü∫Á°Ä‰∏äÊèêÂá∫circle loss‰Ωú‰∏∫‰ºòÂåñÁõÆÊ†á$(\alpha_n s_n - \alpha_p s_p) = m$ÔºåÂú®toy scenario‰∏ãÂ±ïÁ§∫‰∫ÜÂàÜÁ±ªËæπÁïåÂíåÊ¢ØÂ∫¶ÁöÑÊîπÂñÑ„ÄÇ ÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩûÔΩû [Hierarchical Similarity] Learning Hierarchical Similarity MetricsÔºö2012CVPRÔºå [Hierarchical Triplet Loss] Deep Metric Learning with Hierarchical Triplet LossÔºö2018ECCVÔºå Hierarchical classicificationÂ∫îËØ•ÂçïÁã¨ÂÅö‰∏Ä‰∏™Á≥ªÂàóÔºåtobeadded ‰∏Ä‰∫õÂæÖÊòéÁ°ÆÁöÑÈóÆÈ¢ò anchorÊÄé‰πàÈÄâÔºöfacenet‰∏≠ËØ¥ÊòéÔºåÊØè‰∏™mini-batch‰∏≠ÊØè‰∏™Á±ªÂà´ÂøÖÈ°ªÈÉΩÊúâ pairsÊÄé‰πàÂÆûÁé∞ÔºàÂõ∞ÈöæÁöÑÂÆö‰πâÔºâÔºöfacenet‰∏≠ËØ¥ÊòéÔºåhard distance sample in mini-batch hingeloss &amp; SVMÊé®ÂØº Â∏∏ËßÑ‰ΩøÁî®ÔºüÁªìÂêàcls lossÂíåmetric lossËøòÊòØÂè™Áî®metric lossÔºöcls lossÂíåmetric lossÊú¨Ë¥®‰∏äÊòØ‰∏ÄÊ†∑ÁöÑÔºåÈÉΩÊòØÂ∏åÊúõÂêåÁ±ªÊ†∑Êú¨ËæìÂá∫‰∏ÄÊ†∑Ôºå‰∏çÂêåÁ±ªÊ†∑Êú¨ËæìÂá∫‰∏ç‰∏ÄÊ†∑ÔºåÂè™‰∏çËøáÂâçËÄÖÂÖ∑ÊúâÊ¶ÇÁéáÊÑè‰πâÔºåÂêéËÄÖÂÖ∑ÊúâË∑ùÁ¶ªÊÑè‰πâ„ÄÇ‰∏äÈù¢ÂàóÂá∫Êù•ÁöÑÂè™Êúâcenter lossÊòØË¶ÅË∑ücls lossÁªìÂêàËµ∑Êù•Áî®ÁöÑÔºåÂõ†‰∏∫‰ªñÂè™ÈíàÂØπÁ±ªÂÜÖÔºå‰∏çË∂≥‰ª•Êé®Âä®Êï¥‰∏™Ê®°Âûã„ÄÇ Signature Verification using a ‚ÄòSiamese‚Äô Time Delay Neural Network Âä®Êú∫ verification of written signatures propose Siamese two identical sub-networks joined at their outputs measure the distance verification process a stored feature vector a chosen threshold ÊñπÊ≥ï network two inputsÔºöextracting features two sub-networksÔºöshare the same weights one outputÔºöcosine of the angle between two feature vectors target two real signaturesÔºöcosine=1.0 with one forgeryÔºöcosine=-0.9 and cosine=-1.0 dataset 50% genuine:genuine pairs 40% genuine:forgery pairs 10% genuine:zero-effort pairs Dimensionality Reduction by Learning an Invariant Mapping Âä®Êú∫ dimensionality reduction propose Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) globally co- herent non-linear function relies solely on neighbor- hood relationships invariant to certain transformations of the inputs ËÆ∫ÁÇπ most existing dimensionality reduction techniques they do not produce a function (or a mapping) from input to manifold new points with unknown relationships with training samples cannot be processed they tend to cluster points in output space a uniform distribution in the outer manifolds is desirable proposed DrLIM globally coherent non-linear function neighborhood relationships that are independent from any distance metric invariant to complicated non-linear trnasformations lighting changes geometric distortions can be used to map new samples empoly contrastive loss neighbors are pulled together non-neighbors are pushed apart energy based model euclidean distance approximates the ‚Äúsemantic similarity‚Äùof the inputs in input space ÊñπÊ≥ï contrastive loss conventional loss sum over samples contrastive loss sum over pairs $(X_1, X_2, Y)$ similar pairsÔºö$Y=0$ dissimilarÔºö$Y=1$ euclidean distance $L = (1-Y)\sum L_s ||G_w(X_1)-G_w(X_2)||_2 + Y\sum L_d ||G_w(X_1)-G_w(X_2)||_2$ $L_s$ should results in low values for similar pairs $L_d$ should results in high values for dissimilar pairs exact formÔºö$L(W,Y,X_1,X_2) = (1-Y)\frac{1}{2}(D^2) + (Y)\frac{1}{2} \{max(0,m-D)\}^2$ spring model analogy similar partial lossÁõ∏ÂΩì‰∫éÁªôÂºπÁ∞ßÊñΩÂä†‰∫Ü‰∏Ä‰∏™ÊÅíÂÆöÁöÑÂäõÔºåÂêë‰∏≠ÂøÉÁÇπÊå§Âéã dissimilar partial lossÂè™ÂØπÂúàÂÜÖÁöÑÁÇπÊñΩÂäõÔºåÊé®Âá∫ÂúàÂ§ñÂ∞±‰∏çÁÆ°‰∫Ü FaceNet: A Unified Embedding for Face Recognition and Clustering Âä®Êú∫ face tasks face verification: is this the same person face recognition: who is the person clustering: find common people among the faces learn a mapping compact Euclidean space where the Euclidean distance directly correspond to face similarity ËÆ∫ÁÇπ traditionally training classification layer generalizes well to new facesÔºü indirectness large dimension feature representation inefficiency use siamese pairs the loss encourages all faces of one identity to project onto a single point this paper employ triplet loss targetÔºöseparate the positive pair from the negative by a distance margin allows the faces of one identity to live on a manifold obtain face embedding l2 norm a fixed d-dims hypersphere large dataset to attain the appropriate invariances to pose, illumination, and other variational conditions architecture explore two different deep network ÊñπÊ≥ï inputÔºö‰∏âÂÖÉÁªÑÔºåconsist of two matching face thumbnails and a non-matching one ouputÔºöÁâπÂæÅÊèèËø∞Ôºåa compact 128-D embedding living on the fixed hypersphere $||f(x)||_2=1$ triple-loss targetÔºöall anchor-pos distances are smaller than any anchor-neg distances with a least margin $\alpha$ $L = \sum_i^N [||f(x_i^a) - f(x_i^p)||_2^2 - ||f(x_i^a) - f(x_i^n)||_2^2 + \alpha]$ hard triplets hard samples $argmax_{x_i^p}||f(x_i^a) - f(x_i^p)||_2^2$ $argmin_{x_i^n}||f(x_i^a) - f(x_i^n)||_2^2$ infeasible to compute over the whole setÔºömislabelled and poorly imaged faces would dominate the hard positives and negatives off-lineÔºöuse recent checkpoint to compute on a subset onlineÔºöselect in mini-batch mini-batchÔºö ÊØè‰∏™Á±ªÂà´ÈÉΩÂøÖÈ°ªÊúâÊ≠£Ê†∑Êú¨ Ë¥üÊ†∑Êú¨ÊòØrandomly sampled hard sample use all anchor-positive pairs selecting the hard negatives hardest negatives can lead to bad local minima in early stage ÂÖàpick semi-hardÔºö$||f(x_i^a) - f(x_i^p)||_2^2 &lt; ||f(x_i^a) - f(x_i^n)||_2^2$ network ‰∏ÄÁßçstraightÁöÑÁΩëÁªúÔºåÂºïÂÖ•‰∫Ü1x1 convÂÖàÂéãÁº©ÈÄöÈÅì Inception modelsÔºö20x fewer paramsÔºå5x fewer FLOPS metric sameÔºèdifferentÊòØÁî±a squared L2 distanceÂÜ≥ÂÆö Âõ†Ê≠§ÊµãËØïÁªìÊûúÊòØdÁöÑÂáΩÊï∞ ÂÆö‰πâtrue acceptsÔºöÂúàÂÜÖÂØπÁöÑÔºå$TA(d)=\{(i,j)\in P_{same}, with D(x_i,x_j)\leq d\}$ ÂÆö‰πâfalse acceptsÔºöÂúàÂÜÖÈîôÁöÑÔºå$FA(d)=\{(i,j)\in P_{diff}, with D(x_i,x_j)\leq d\}$ ÂÆö‰πâvalidation rateÔºö$VAL(d) = \frac{|TA(d)|}{|P_{same}|}$ ÂÆö‰πâfalse accept rateÔºö$FAR(d) = \frac{|FA(d)|}{|P_{diff}|}$ A Discriminative Feature Learning Approach for Deep Face Recognition Âä®Êú∫ enhance the discriminationative power of the deeply learned features joint supervision softmax loss center loss two key learning objectives inter-class dispension intra-class compactness ËÆ∫ÁÇπ face recognition task requirement the learned features need to be not only separable but also discriminative generalized enough for the new unseen samples the softmax loss only encourage the separability of features ÂØπÂàÜÁ±ªËæπÁïå„ÄÅÁ±ªÂÜÖÁ±ªÈó¥ÂàÜÂ∏ÉÊ≤°ÊúâÁõ¥Êé•Á∫¶Êùü contrastive loss &amp; triplet loss training pairs or triplets dramatically grows slow convergence and instability we propose learn a center simultaneously update the center and optimize the distances joint supervision softmax loss forces the deep features of different classes staying apart center loss efficiently pulls the deep features of the same class to their centers to be more discriminationative the inter-class features differences are enlarged the intra-class features variations are reduced ÊñπÊ≥ï softmax vis ÊúÄÂêé‰∏ÄÂ±Çhidden layer‰ΩøÁî®‰∏§‰∏™Á•ûÁªèÂÖÉ so that we can directly plot separable but still show significant intra-class variations center loss $L_c = \frac{1}{2} \sum_1^m ||x_i - c_{y_i}||_2^2$ update class center on mini-batchÔºö \frac{\partial L_c}{\partial x_i} = x_i - c_{y_i}\\ \Delta c_j = \frac{\sum_i^m \delta (y_i=j) * (c_j - x_i)}{1+\sum_i^m \delta (y_i=j)} joint supervisionÔºö L = L_{CE} + \lambda L_c discussion necessity of joint supervision solely softmax loss ‚Äî-&gt; large intra-class variations solely center loss ‚Äî-&gt; features and centers will degraded to zeros compared to contrastive loss and triplet loss using pairsÔºösuffer from dramatic data expansion hard miningÔºöcomplex recombination optimizing targetÔºö center lossÁõ¥Êé•ÈíàÂØπintra-class compactnessÔºåÁ±ªÂÜÖÁî®Ë∑ùÁ¶ªÊù•Á∫¶ÊùüÔºåÁ±ªÈó¥Áî®softmaxÊù•Á∫¶Êùü contrastive loss‰πüÊòØÁõ¥Êé•‰ºòÂåñÁªùÂØπË∑ùÁ¶ªÔºåÁ±ªÂÜÖ&amp;Á±ªÈó¥ÈÉΩÁî®Ë∑ùÁ¶ªÊù•Á∫¶Êùü triplet lossÊòØÂª∫Ê®°Áõ∏ÂØπÂÖ≥Á≥ªÔºåÁ±ªÂÜÖ&amp;Á±ªÈó¥ÈÉΩÁî®Ë∑ùÁ¶ªÊù•Á∫¶Êùü architecture local convolution layerÔºöÂΩìÊï∞ÊçÆÈõÜÂú®‰∏çÂêåÁöÑÂå∫ÂüüÊúâ‰∏çÂêåÁöÑÁâπÂæÅÂàÜÂ∏ÉÊó∂ÔºåÈÄÇÂêàÁî®local-ConvÔºåÂÖ∏ÂûãÁöÑ‰æãÂ≠êÂ∞±ÊòØ‰∫∫ËÑ∏ËØÜÂà´Ôºå‰∏ÄËà¨‰∫∫ÁöÑÈù¢ÈÉ®ÈÉΩÈõÜ‰∏≠Âú®ÂõæÂÉèÁöÑ‰∏≠Â§ÆÔºåÂõ†Ê≠§Êàë‰ª¨Â∏åÊúõÂΩìconvÁ™óÂè£ÊªëËøáËøôÂùóÂå∫ÂüüÁöÑÊó∂ÂÄôÔºåÊùÉÈáçÂíåÂÖ∂‰ªñËæπÁºòÂå∫ÂüüÊòØ‰∏çÂêåÁöÑ ÂèÇÊï∞ÈáèÊö¥Â¢ûÔºökernel_size kernel_size output_size output_size input_channel * output_channel ÂÆûÈ™å hyperparamÔºö$\lambda$ and $\alpha$ fix $\alpha=0.5$ and vary $\lambda$ from 0-0.1 fix $\lambda=0.003$ and vary $\alpha$ from 0.01-1 ÁªìËÆ∫ÊòØremains stable across a large rangeÔºåÊ≤°ÊúâÁªôÂá∫ÊúÄ‰Ω≥ÔºèÂª∫ËÆÆ ÊàëÁöÑÂÆûÈ™å Âä†ÊØî‰∏çÂä†ËÆ≠ÁªÉÊÖ¢ÂæóÂ§ö Âú®Mnist‰∏äÊµãËØïÂêåÊ†∑ÁöÑepochÂä†ÊØî‰∏çÂä†ÂáÜÁ°ÆÁéá‰Ωé ‰πãÊâÄ‰ª•Center LossÊòØÈíàÂØπ‰∫∫ËÑ∏ËØÜÂà´ÁöÑLossÊòØÊúâÂéüÂõ†ÁöÑÔºå‰∏™‰∫∫ËÆ§‰∏∫‰∫∫ËÑ∏ÁöÑ‰∏≠ÂøÉÊÄßÊõ¥Âº∫‰∏Ä‰∫õÔºå‰πüÂ∞±ÊòØËØ¥‰∏Ä‰∏™‰∫∫ÁöÑÊâÄÊúâËÑ∏ÂèñÂπ≥ÂùáÂÄº‰πãÂêéÁöÑ‰∫∫ËÑ∏Êàë‰ª¨ËøòÊòØÂèØ‰ª•Ëæ®ËØÜÊòØ‰∏çÊòØËøô‰∏™‰∫∫ÔºåÊâÄ‰ª•Center LossÊâçËÉΩÂèëÊå•‰ΩúÁî® Circle Loss: A Unified Perspective of Pair Similarity Optimization Âä®Êú∫ pair similarity circular decision boundary unify cls-based &amp; metric-based data class-level labels pair-wise labels ËÆ∫ÁÇπ there is no intrinsic difference between softmax loss &amp; metric loss minimize between-class similarity $s_n$ maximize within- class similarity $s_p$ reduce $s_n - s_p$ short-commings lack of flexibilityÔºö$s_p$Âíå$s_n$ÁöÑ‰ºòÂåñÈÄüÂ∫¶ÂèØËÉΩ‰∏çÂêåÔºå‰∏Ä‰∏™Âø´Êî∂Êïõ‰∫Ü‰∏Ä‰∏™ËøòÂæàÂ∑ÆÔºåËøôÊó∂ÂÄôÁî®ÂêåÊ†∑ÁöÑÊ¢ØÂ∫¶ÂéªÊõ¥Êñ∞Â∞±ÈùûÂ∏∏inefficient and irrationalÔºåÂ∞±Â∑¶ÂõæÊù•ËØ¥Ôºå‰∏ãÈù¢ÁöÑÁÇπÁõ∏ÂØπ‰∏äÈù¢ÁöÑÁÇπÔºå$s_n$Êõ¥Â∞èÔºàÊõ¥Êé•ËøëopÔºâÔºå$s_p$Êõ¥Â∞èÔºàÊõ¥ËøúÁ¶ªopÔºâÔºåvice versaÔºå‰ΩÜÊòØÂÜ≥Á≠ñÂπ≥Èù¢ÂØπ‰∏â‰∏™ÁÇπÁõ∏ÂØπ‰∫é$s_n$Âíå$s_p$ÁöÑÊ¢ØÂ∫¶ÈÉΩÊòØ‰∏ÄÊ†∑ÁöÑÔºà1Âíå-1Ôºâ„ÄÇ ambiguous convergence statusÔºöÁî®‰∏Ä‰∏™hard distance marginÊù•ÊèèËø∞decision boundaryËøò‰∏çÂ§üdiscriminativeÔºåhard decision boundary‰∏äÂêÑÁÇπÂÖ∂ÂÆûËøòÊòØÊúâÂ∑ÆÂà´ÁöÑÔºåÂÅáËÆæÂ≠òÂú®‰∏Ä‰∏™optimumÔºà$s_p=1 \ \&amp; \ s_n=0$ÔºâÔºåÈÇ£‰πàÂ∑¶ÂõæÂÜ≥Á≠ñÂπ≥Èù¢‰∏ä‰∏§‰∏™ÁÇπÔºåÁõ∏ÂØπoptimumÁöÑÊÑè‰πâÊòéÊòæ‰∏ç‰∏ÄÊ†∑ÔºåÂÜ≥Á≠ñÂπ≥Èù¢Â∫îËØ•ÊòØ‰∏™Âõ¥ÁªïoptimumÁöÑÂúÜÂúà„ÄÇ propose circle loss independent weighting factorsÔºöÁ¶ªoptimumË∂äËøúÁöÑpenalty strengthË∂äÂ§ßÔºåËøô‰∏ÄÈ°πÁõ¥Êé•‰ª•Ë∑ùÁ¶ª‰∏∫‰ºòÂåñÁõÆÊ†áÁöÑlossÈÉΩÊòØÊª°Ë∂≥ÁöÑ different penalty strengthÔºö$s_p$Âíå$s_n$ learn at different pacesÔºåÁ±ªÂÜÖÂä†ÊùÉÔºåÂä†ÊùÉÁ≥ªÊï∞ÊòØlearnable params $(\alpha_n s_n - \alpha_p s_p) = m$Ôºöyielding a circle shape ÊñπÊ≥ï Ê†∏ÂøÉÔºö$(\alpha_n s_n - \alpha_p s_p) = m$ self-paced weighting given optimum $O_p$ and $O_n$Ôºåfor each similarity scoreÔºö \begin{cases} a_p^i = [O_p - s_p^i]_+ \\ a_n^j = [s_n^j - O_n]_+ \end{cases} cut-off at zero ÂØπ‰∫éËøúÁ¶ªoptimumÁöÑÁÇπÊ¢ØÂ∫¶ÊîæÂ§ßÔºåÊé•ËøëoptimumÁöÑÁÇπÔºàÂø´Êî∂ÊïõÔºâÊ¢ØÂ∫¶Áº©Â∞è softmaxÈáåÈù¢ÈÄöÂ∏∏‰∏ç‰ºöÂØπÂêåÁ±ªÊ†∑Êú¨Èó¥ÂÅöËøôÁßçrescalingÁöÑÔºåÂõ†‰∏∫ÂÆÉÂ∏åÊúõÊâÄÊúâÊ†∑Êú¨valueÈÉΩËææÂà∞Ë¥ºÂ§ß Circle loss abandons the interpretation of classifying a sample to its target class with a large probability margin adding a margin m reinforces the optimization take toy scenario ÊúÄÁªàÊï¥ÁêÜÊàêÔºö$(s_n-0)^2 + (s_p-1)^2 = 2m^2$ op targetÔºö$s_p &gt; 1-m$Ôºå$s_n &lt; m$ relaxation factor $m$Ôºöcontrols the radius of the decision boundary unified perspective tranverse all the similarity pairsÔºö$\{s_p^i\}^K$Âíå$\{s_n^j\}^N$ to reduce $(s_n^j - s_p^i)$Ôºö$L_{uni}=log[1+\sum^K_i \sum^N_j exp(\lambda (s_n^j - s_p^i + m))]$ Ëß£ËÄ¶Ôºà‰∏ç‰ºöÂêåÊó∂ÊòØ$s_p$Âíå$s_n$ÔºâÔºö$L_{uni}=log[1+\sum^N_j exp(\lambda (s_n^j + m))\sum^K_i exp(\lambda (-s_p^i))]$ given class labelsÔºö we get $(N-1)$ between-class similarity scores and $(1)$ within-class similarity score ÂàÜÊØçÁøª‰∏äÂéªÔºö$L = -log \frac{exp(\lambda (s_p-m))}{exp(\lambda (s_p-m)) + \sum^{N-1}_j exp(\lambda (s_n^j))}$ Â∞±ÊòØsoftmax given pair-wise labelsÔºö triplet loss with hard miningÔºöfind pairs with large $s_n$ and low $s_p$ use infiniteÔºö$L=lim_{\lambda \to \inf} \frac{1}{\lambda} L_{uni}$ ÂÆûÈ™å Face recognition noisy and long-tailed dataÔºöÂéªÂô™Âπ∂‰∏îÂéªÊéâÁ®ÄÁñèÊ†∑Êú¨ resnet &amp; 512-d feature embeddings &amp; cosine distance $\lambda=256$Ôºå$m=0.25$ Person re-identification $\lambda=128$Ôºå$m=0.25$ Fine-grained image retrieval ËΩ¶ÈõÜÂíåÈ∏üÈõÜ bn-inception &amp; 512-d embeddings P-K sampling $\lambda=80$Ôºå$m=0.4$ hyper-params the scale factor $\lambda$Ôºö determines the largest scale of each similarity score Circle loss exhibits high robustness on $\lambda$ the other two becomes unstable with larger $\lambda$ owing to the decay factor the relaxation factor mÔºö determines the radius of the circular decision boundary surpasses the best performance of the other two in full range robustness inference ÂØπ‰∫∫ËÑ∏Á±ª‰ªªÂä°ÔºåÈÄöÂ∏∏Áî®ËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãÁîüÊàê‰∏Ä‰∏™‰∫∫ËÑ∏Ê†áÂáÜÂ∫ïÂ∫ìÔºåÁÑ∂ÂêéÊØèÊ¨°Êé®ÁêÜÁöÑÊó∂ÂÄôÂæóÂà∞ÊµãËØïÊï∞ÊçÆÁöÑÁâπÂæÅÂêëÈáèÔºåÂπ∂Âú®Ê†áÂáÜÂ∫ïÂ∫ì‰∏≠ÊêúÁ¥¢Áõ∏‰ººÂ∫¶ÊúÄÈ´òÁöÑÁâπÂæÅÔºåÂÆåÊàê‰∫∫ËÑ∏ËØÜÂà´ËøáÁ®ã„ÄÇ]]></content>
      <tags>
        <tag>Â∫¶ÈáèÂ≠¶‰π†Ôºåloss &amp; network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[efficientÂë®Ëæπ]]></title>
    <url>%2F2020%2F09%2F23%2Fefficient%E5%91%A8%E8%BE%B9%2F</url>
    <content type="text"><![CDATA[Âõ†‰∏∫‰∏çÊòØgooglenetÂÆ∂ÊóèÂÆòÊñπÂá∫ÂìÅÔºåÊâÄ‰ª•ÊîæÂú®Â§ñÈù¢ [EfficientFCN] EfficientFCN: Holistically-guided Decoding for Semantic SegmentationÔºöÂïÜÊ±§Ôºå‰∏ªË¶ÅÈíàÂØπupsamplingÊòØÂ±ÄÈÉ®ÊÑüÂèóÈáéÔºåÈáçÂª∫Â§±ÁúüÂ§öÔºåÂàÜÂâ≤Á≤æÂ∫¶Â∑ÆÁöÑÈóÆÈ¢òÔºåÊèêÂá∫‰∫ÜHolistically-guided Decoder (HGD) ÔºåÁî®Êù•recover the high-resolution (OS=8) feature mapsÔºåÊÉ≥Ê≥ï‰∏äÊé•ËøëSCSE-blockÔºåÊï∞Â≠¶Ë°®Ëææ‰∏äÊé•Ëøëbilinear-CNNÔºåÊÄßËÉΩÊèêÂçá‰∏ªË¶ÅÂΩíÂõ†‰∫éeff backÂêß„ÄÇ EfficientFCN: Holistically-guided Decoding for Semantic Segmentation Âä®Êú∫ Semantic Segmentation dilatedFCNÔºöcomputational complexity encoder-decoderÔºöperformance proposed EfficientFCN common back without dilated convolution holistically-guided decoder balance performance and efficiency ËÆ∫ÁÇπ key elements for semantic segmentation high-resolution feature maps pre-trained weights OS32 feature mapÔºöthe fine-grained structural information is discarded dilated convolutionÔºöno extra parameters introduced but equire high computational complexity and memory consumption encoder-decoder based methods repeated upsampling + skip connection procedure upsampling concatÔºèadd successive convs Even with the skip connections, lower-level high-resolution feature maps cannot provide abstractive enough features for achieving high- performance segmentation The bilinear upsampling or deconvolution operations are conducted in a local manner(from a limited receptive filed) improvements reweightÔºöSE-block scales each feature channel but maintains the original spatial size and structuresÔºö„Äêscse blockÂØπspacialÊúâÂä†ÊùÉÂïä„Äë propose EfficientFCN widely used classification model Holistically-guided Decoder (HGD) take OS8, OS16, OS32 feature maps from backbone OS8ÂíåOS16Áî®Êù•spatially guiding the feature upsampling process OS32Áî®Êù•encode the global contextÁÑ∂ÂêéÂü∫‰∫éguidanceËøõË°å‰∏äÈááÊ†∑ linear assembly at each high-resolution spatial locationÔºöÊÑüËßâÂ∞±ÊòØÂØπ‰∏äÈááÊ†∑ÁâπÂæÅÂõæÂÅö‰∫ÜÂä†ÊùÉ ÊñπÊ≥ï Holistically-guided Decoder multi-scale feature fusion holistic codebook generation from high-level feature maps holistic codewordsÔºöwithout any spatial order codeword assembly multi-scale feature fusion we observe the fusion of multi-scale feature maps generally result in better performance compressÔºöseparate 1x1 convs bilinear downsampÔºèupsamp concatenate fused OS32 $m_{32}$ &amp; fused OS8 $m_8$ holistic codebook generation from $m_{32}$ two separate 1x1 conv a codeword based map $B \in R^{1024(H/32)(W/32)}$ÔºöÊØè‰∏™‰ΩçÁΩÆÁî®‰∏Ä‰∏™1024-dimÁöÑvectorÊù•ÊèèËø∞ n spatial weighting map $A\in R^{n(H/32)(W/32)}$Ôºöhighlight ÁâπÂæÅÂõæ‰∏ä‰∏çÂêåÂå∫Âüü softmax norm in spatial-dim $\widetilde A_i(x,y)=\frac{exp(A_i(x,y))}{\sum_{p,q} exp(A_i(p,q))}, i\in [0,n)$ codeword $c_i \in R^{1024}$ global description for each weighting map weighted average of B on all locations $c_i = \sum_{p,q} \widetilde A_i(p,q) B(p,q)$ each codeword captures certain aspect of the global context orderless high-level global features $C \in R^{1024*n}$ $C = [c_1, ‚Ä¶, c_n]$ codeword assembly raw guidance map $G \in R^{1024(H/8)(W/8)}$Ôºö1x1 conv on $m_8$ fuse semantic-rich feature map $\overline B \in R^{1024}$Ôºöglobal average vector novel guidance feature map $\overline G = G \oplus \overline B $Ôºölocation-wise addition„ÄêÔºüÔºüÔºüÔºü„Äë linear assembly weights of the n codewords $W \in R^{n(H/8)(W/8)}$Ôºö1x1 conv on $\overline G$ holistically-guided upsampled feature $\tilde f_8 = W^T C$Ôºöreshape &amp; dot final feature map $f_8$Ôºöconcat $\tilde f_8$ and $G$ final segmentation 1x1 conv further upsampling ÂÆûÈ™å numer of holistic codewords 32-512Ôºöincrease 512-1024Ôºöslight drop we observe the number of codewords needed is approximately 4 times than the number of classes]]></content>
      <tags>
        <tag>ËØ≠‰πâÂàÜÂâ≤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[data aug]]></title>
    <url>%2F2020%2F09%2F18%2Fdata-aug%2F</url>
    <content type="text"><![CDATA[[mixup] mixup: BEYOND EMPIRICAL RISK MINIMIZATIONÔºöÂØπ‰∏çÂêåÁ±ªÂà´ÁöÑÊ†∑Êú¨Ôºå‰∏ç‰ªÖÂèØ‰ª•‰Ωú‰∏∫Êï∞ÊçÆÂ¢ûÂπøÊâãÊÆµÔºåËøòÂèØ‰ª•Áî®‰∫ésemi-supervised learningÔºàMixMatchÔºâ [mixmatch] MixMatch: A Holistic Approach to Semi-Supervised LearningÔºöÈíàÂØπÂçäÁõëÁù£Êï∞ÊçÆÁöÑÊï∞ÊçÆÂ¢ûÂπø [mosaic] from YOLOv4 [AutoAugment] AutoAugment: Learning Augmentation Policies from DataÔºögoogle [RandAugment] RandAugment: Practical automated data augmentation with a reduced search spaceÔºögoogle RandAugment: Practical automated data augmentation with a reduced search space Âä®Êú∫ AutoAugment separate search phase run on a subset of a huge dataset unable to adjust the regularization strength based on model or dataset size RandAugment significantly reduced search space can be used uniformly across tasks and datasets match or exceeds the previous val acc ÊñπÊ≥ï formulation always select a transformation with uniform prob $\frac{1}{K}$ given N transformations for an imageÔºöthere are $K^N$ potential policies fixied magnitude schedule MÔºöwe choose ConstantÔºåÂõ†‰∏∫Âè™Ë¶Å‰∏Ä‰∏™hyper run naive grid search ÁñëÈóÆÔºöËøôÊ†∑ÊØè‰∏™opÁ≠âÊ¶ÇÁéáÔºåÂ∞±‰∏çÂÜçdata-specific‰∫ÜÔºå‰πüÁúã‰∏çÂá∫Ëá™ÁÑ∂ÂõæÂÉèÊõ¥prefer color transformationËøôÁßçÁªìËÆ∫‰∫Ü AutoAugment: Learning Augmentation Policies from Data Âä®Êú∫ search for data augmentation policies propose AutoAugment create a search space composed of augmentation sub-policies one sub-policy is randomly choosed per image per mini-batch a sub-policy consists of two base operations find the best policyÔºöyields the highest val acc on the target dataset the learned policy can transfer ËÆ∫ÁÇπ data augmentation to teach a model about invariance in data domain is easier than hardcoding it into model architecture currently dataset-specific and often do not transferÔºö MNISTÔºöelastic distortions, scale, translation, and rotation CIFAR &amp; ImageNetÔºörandom cropping, image mirroring and color shifting / whitening GANÔºöÁõ¥Êé•ÁîüÊàêÂõæÂÉèÔºåÊ≤°ÊúâÂΩíÁ∫≥policy we aim to automate the process of finding an effective data augmentation policy for a target dataset each policyÔºö operations in certain order probabilities after applying magnitudes use reinforcement learning as the search algorithm contributions SOTA on CIFAR &amp; ImageNet &amp; SVHN new insight on transfer learningÔºö‰ΩøÁî®È¢ÑËÆ≠ÁªÉÊùÉÈáçÊ≤°ÊúâÊòæËëóÊèêÂçáÁöÑdataset‰∏äÔºå‰ΩøÁî®ÂêåÊ†∑ÁöÑaug policiesÂàô‰ºöÊ∂®ÁÇπ ÊñπÊ≥ï formulation search space of policies policyÔºöa policy consists of 5 sub-policies sub-policyÔºöeach sub-policy consisting of two image operations operationÔºöeach operation is also associated with two hyperparameters probabilityÔºöof applying the operationÔºåuniformly discrete into 11 values magnitudeÔºöof the operationÔºåuniformly discrete into 10 values a mini-batch share the same chosen sub-policy operationsÔºö16 in totalÔºåmainly use PIL https://blog.csdn.net/u011583927/article/details/104724419ÊúâÂêÑÁßçoperationÁöÑÂèØËßÜÂåñÊïàÊûú shearÊòØÁ†çÊéâÂõæÂÉè‰∏Ä‰∏™ËßíÁöÑÁï∏Âèò equalizeÊòØÁõ¥ÊñπÂõæÂùáË°°Âåñ solarizeÊòØÂü∫‰∫é‰∏ÄÂÆöÈòàÂÄºÁöÑinvertÔºåÈ´ò‰∫éÈòàÂÄºinvertÔºå‰Ωé‰∫éÈòàÂÄº‰∏çÂèò posterize‰πüÊòØ‰∏ÄÁßçÂÉèÁ¥†ÂÄºÊà™Êñ≠Êìç‰Ωú colorÊòØË∞ÉÊï¥È•±ÂíåÂ∫¶Ôºåmag&lt;1Ë∂ãËøëÁÅ∞Â∫¶Âõæ sharpnessÂÜ≥ÂÆöÂõæÂÉèÊ®°Á≥ä/ÈîêÂåñ sample pairingÔºö‰∏§Âº†ÂõæÂä†ÊùÉÊ±ÇÂíåÔºå‰ΩÜÊòØ‰∏çÊîπÂèòÊ†áÁ≠æ searching goal with $(161011)^2$ choices of sub-policies we want 5 example ‰∏Ä‰∏™sub-policyÂåÖÂê´‰∏§‰∏™operation ÊØè‰∏™operationÊúâ‰∏ÄÂÆöÁöÑpossibilityÂÅö/‰∏çÂÅö ÊØè‰∏™operationÊúâ‰∏ÄÂÆöÁöÑmagnitudeÂÜ≥ÂÆöÂÅöÂêéÁöÑÊïàÊûú ÁªìËÆ∫ On CIFAR-10, AutoAugment picks mostly color-based transformations on ImageNet, AutoAugment focus on color-based transformations as well, besides geometric transformation and rotate is commonly used one of the best policy overall results mixup: BEYOND EMPIRICAL RISK MINIMIZATION Âä®Êú∫ classification task memorization and sensitivity issue reduces the memorization of corrupt labels increases the robustness to adversarial examples improves the generalization can be used to stabilize the training of GANs propose convex combinations of pairs of examples and their labels ËÆ∫ÁÇπ ERM(Empirical Risk Minimization)Ôºöissue of generalization allows large neural networks to memorize (instead of generalize from) the training data even in the presence of strong regularization neural networks change their predictions drastically when evaluated on examples just outside the training distribution VRM(Vicinal Risk Minimization)Ôºöintroduce data augmentation e.g. define the vicinity of one image as the set of its horizontal reflections, slight rotations, and mild scalings vicinity share the same class does not model the vicinity relation across examples of different classes ERM‰∏≠ÁöÑtraining setÂπ∂‰∏çÊòØÊï∞ÊçÆÁöÑÁúüÂÆûÂàÜÂ∏ÉÔºåÂè™ÊòØÁî®ÊúâÈôêÊï∞ÊçÆÊù•Ëøë‰ººÁúüÂÆûÂàÜÂ∏ÉÔºåmemorization‰πü‰ºöÊúÄÂ∞èÂåñtraining errorÔºå‰ΩÜÊòØÂØπtraining seg‰ª•Â§ñÁöÑsampleÂ∞±leads to undesirable behaviour mixupÂ∞±ÊòØVRMÁöÑ‰∏ÄÁßçÔºåpropose a generic vicinal distributionÔºåË°•ÂÖÖvicinity relation across examples of different classes ÊñπÊ≥ï mixup constructs virtual training examples x = \lambda x_i + (1-\lambda)x_j \\ y = \lambda y_i + (1-\lambda)y_j use two examples drawn at randomÔºöraw inputs &amp; raw one-hot labels ÁêÜËÆ∫Âü∫Á°ÄÔºölinear interpolations of feature vectors should lead to linear interpolations of the associated targets hyper-parameter $\alpha$ $\lambda = np.random.beta(\alpha, \alpha)$ controls the strength of interpolation ÂàùÊ≠•ÁªìËÆ∫ three or more examples mixup does not provide further gain but more computation interpolating only between inputs with equal label did not lead to the performance gains key elemets‚Äî‚Äîtwo inputs with different label vis decision boundariesÊúâ‰∫Ü‰∏Ä‰∏™Á∫øÊÄßËøáÊ∏° Êõ¥ÂáÜÁ°Æ &amp; Ê¢ØÂ∫¶Êõ¥Â∞èÔºöerrorÂ∞ëÊâÄ‰ª•lossÂ∞èÊâÄ‰ª•Ê¢ØÂ∫¶Â∞èÔºüÔºü ÂÆûÈ™å ÂàùÊ≠•ÂàÜÁ±ªÂÆûÈ™å $\alpha \in [0.1, 0.4]$ leads to improved performanceÔºålargers leads to underfitting models with higher capacities and/or longer training runs are the ones to benefit the most from mixup memorization of corrupted labels Â∞ÜÊï∞ÊçÆÈõÜ‰∏≠‰∏ÄÈÉ®ÂàÜlabelÊç¢Êàêrandom noise ERMÁõ¥Êé•ËøáÊãüÂêàÔºåÂú®corrupted sample‰∏äÈù¢training errorÊúÄÂ∞èÔºåÊµãËØïÈõÜ‰∏ätest errorÊúÄÂ§ß dropoutÊúâÊïàÈò≤Ê≠¢ËøáÊãüÂêàÔºå‰ΩÜÊòØmixup outperformsÂÆÉ corrupted labelÂ§öÁöÑÊÉÖÂÜµ‰∏ãÔºådropout+mixup performs the best robustness to adversarial examples Adversarial examples are obtained by adding tiny (visually imperceptible) perturbations Â∏∏ËßÑÊìç‰Ωúdata augmentationÔºöproduce and train on adversarial examples add significant computationalÔºöÊ†∑Êú¨Êï∞ÈáèÂ¢ûÂ§öÔºåÊ¢ØÂ∫¶ÂèòÂåñÂ§ß mixup results in a smaller loss and gradient normÔºöÂõ†‰∏∫mixupÁîüÊàêÁöÑÂÅáÊ†∑Êú¨‚ÄúÊõ¥ÂêàÁêÜ‰∏ÄÁÇπ‚ÄùÔºåÊ¢ØÂ∫¶ÂèòÂåñÊõ¥Â∞è ablation study mixup is the bestÔºöÁªùÂØπÈ¢ÜÂÖàÁ¨¨‰∫åmix input + label smoothing the effect of regularization ERMÈúÄË¶ÅÂ§ßweight decayÔºåmixupÈúÄË¶ÅÂ∞èÁöÑ‚Äî‚ÄîËØ¥ÊòémixupÊú¨Ë∫´ÁöÑregularization effectsÊõ¥Âº∫ È´òÂ±ÇÁâπÂæÅmixupÈúÄË¶ÅÊõ¥Â§ßÁöÑweight decay‚Äî‚ÄîÈöèÁùÄÂ±ÇÊï∞Âä†Ê∑±regularization effectsÂáèÂº± AC+RPÊúÄÂº∫ label smoothingÂíåadd Gaussian noise to inputs Áõ∏ÂØπÊØîËæÉÂº± mix inputs only(SMOTE) shows no gain MixMatch: A Holistic Approach to Semi-Supervised Learning Âä®Êú∫ semi-supervised learning unify previous methods proposed mixmatch guessing low-entropy labels mixup labeled and unlabeled data useful for differentially private learning ËÆ∫ÁÇπ semi-supervised learning add a loss term computed on unlabeled data and encourages the model to generalize better to unseen data the loss term entropy minimizationÔºödecision boundaryÂ∫îËØ•Â∞ΩÂèØËÉΩËøúÁ¶ªÊï∞ÊçÆÁ∞áÔºåÂõ†Ê≠§prediction on unlabeled data‰πüÂ∫îËØ•ÊòØhigh confidence consistency regularizationÔºöÂ¢ûÂº∫ÂâçÂêéÁöÑunlabeled dataËæìÂá∫ÂàÜÂ∏É‰∏ÄËá¥ generic regularizationÔºöweight decay &amp; mixup MixMatch unified all above introduces a unified loss term for unlabeled data ÊñπÊ≥ï overview givenÔºöa batch of labeled examples $X$ and a batch of labeled examples $U$ augment+label guessÔºöa batch of augmented labeled examples $X^{‚Äò}$ and a batch of augmented labeled examples $U^{‚Äò}$ computeÔºöseparate labeled and unlabeled loss terms $L_X$ and $L_U$ combineÔºöweighted sum MixMatch data augmentation Â∏∏ËßÑaugmentation ‰ΩúÁî®‰∫éÊØè‰∏Ä‰∏™$x_b$Âíå$u_b$ $u_b$ÂÅö$K$Ê¨°Â¢ûÂº∫ label guessing ÂØπÂ¢ûÂº∫ÁöÑ$K$‰∏™$u_b$ÂàÜÂà´È¢ÑÊµãÔºåÁÑ∂ÂêéÂèñÂπ≥Âùá average class prediction sharpening reduce the entropy of the label distribution ÊãâÈ´òÊúÄÂ§ßpredictionÔºåÊãâÂ∞èÂÖ∂‰ªñÁöÑ $Sharpen (p, T)_i =\frac{p_i^{\frac{1}{T}}}{\sum^{N}_j p_j^{\frac{1}{T}}} $ $T$Ë∂ãËøë‰∫é0ÁöÑÊó∂ÂÄôÔºåprocessed labelÂ∞±Êé•Ëøëone-hot‰∫Ü mixup slightly modified form of mixup to make the generated sample being more closer to the original loss function labeled lossÔºötypical cross-entropy loss unlabeled lossÔºösquared L2Ôºåbounded and less sensitive to completely incorrect predictions hyperparameters sharpening temperature $T$Ôºöfixed 0.5 number of unlabeled augmentations $K$Ôºöfixed 2 MixUp Beta parameter $\alpha$Ôºö0.75 for start unsupervised loss weight $\lambda_U$Ôºö100 for start Algorithm ÂÆûÈ™å [mosaic] from YOLOv4]]></content>
      <tags>
        <tag>Êï∞ÊçÆÂ¢ûÂº∫ÔºåÊ†∑Êú¨‰∏çÂπ≥Ë°°ÔºåÂçäÁõëÁù£Â≠¶‰π†ÔºåÂ∫¶ÈáèÂ≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hrnet]]></title>
    <url>%2F2020%2F09%2F18%2Fhrnet%2F</url>
    <content type="text"><![CDATA[papers [v1 2019] Deep High-Resolution Representation Learning for Human Pose EstimationÔºöbase HRNetÔºåÊèêÂá∫parallel multi-resolution subnetworksÔºåhighest resolution output‰Ωú‰∏∫ËæìÂá∫ [v2 2019] High-Resolution Representations for Labeling Pixels and RegionsÔºösimple modificationÔºåÂú®Êú´Á´ØËæìÂá∫ÁöÑÊó∂ÂÄôÂä†‰∫Ü‰∏ÄÊ≠•ËûçÂêàÔºåÂ∞ÜÊâÄÊúâresolution-levelÁöÑfeature‰∏äÈááÊ†∑Âà∞output-levelÁÑ∂Âêéconcat Deep High-Resolution Representation Learning for Human Pose Estimation Âä®Êú∫ human pose estimation high-resolution representations through existing methods recover high-res feature from the lowÔºåÂ§ßÂ§öÊï∞ÊñπÊ≥ïÊòØrecoverÁ≥ª this methods maintain the high-res from start to the endÔºåÊú¨ÊñáÊòØmaintainingÁ≥ª add high-to-low resolution subnetworks repeated multi-scale fusions more accurate and spatially more precise estimate on the high-res outputÔºåÊúÄÂêéÁöÑhigh-res representation‰Ωú‰∏∫ËæìÂá∫ÔºåÊé•ÂêÑÁßçtask heads ËÆ∫ÁÇπ in parallel rather than in seriesÔºöpotentially spatially more preciseÔºåÁõ∏ÊØîËæÉ‰∫érecoverÁ±ªÁöÑÊû∂ÊûÑÔºå‰∏ç‰ºöÂØºËá¥ËøáÂ§öÁöÑspatial resolution lossÔºårecoverÁ±ªÁöÑÊû∂ÊûÑÊúâÊó∂‰ºöÁî®Á©∫Ê¥ûÂç∑ÁßØÊù•Áª¥ÊåÅresolutionÊù•Èôç‰Ωéspatial resolution loss repeated multi- scale fusionsÔºöboost both high&amp;low representationsÔºåmore accurate pose estimation probabilistic graphical model regression heatmap High-to-low and low-to-high frameworks Symmetric high-to-low and low-to-highÔºöHourglass Heavy high-to-low and light low-to-highÔºöResNet back + simple bilinear upsampling Heavy high-to-low with dilated convolutions and further lighter low-to-highÔºöResNet with atrous conv + fewer bilinear upsampling high-to-low partÂíålow-to-hight partÔºöÊúâÂØπÁß∞Âíå‰∏çÂØπÁß∞‰∏§ÁßçÔºåÂØπÁß∞Â∞±Â¶ÇHourglassÔºå‰∏çÂØπÁß∞Â∞±ÊòØdown-path‰ΩøÁî®heavy classification backboensÔºåup-path‰ΩøÁî®ËΩªÈáèÁöÑ‰∏äÈááÊ†∑ fusionÔºö aÂíåbÈÉΩÊúâskip-connectionsÔºåÂ∞Üdown-pathÂíåup-pathÁöÑÁâπÂæÅËûçÂêàÔºåÁõÆÁöÑÊòØËûçÂêàlow-levelÂíåhigh-levelÁöÑÁâπÂæÅ aÈáåÈù¢ËøòÊúâ‰∏çÂêåresolution levelÁöÑËûçÂêà fusionÊñπÂºèÊúâsum/concat refinenetÔºö‰πüÂ∞±ÊòØup-pathÔºåÂèØ‰ª•Áî®upSampling/transpose convs ÊñπÊ≥ï task description human pose estimation = keypoint detection detect K keypoints from an Image (H,W,3) state-of-the art methodsÔºöpredict K heatmapsÔºåeach indicates one of the keypoint a stem with 2 strided conv a body outputting features with the same input resolution a regressor estimating heatmaps we focus on the design of the main body sequential &amp; parallel multi-resolution networks notationÔºö$N_{sr}$ s is the stage r is the resolution indexÔºådenotes $\frac{1}{2^{r-1}}$ of the resolution of the first subnetwork sequential parallel overview four stages channels double when halve the res 1st stage Á¨¨‰∏Ä‰∏™stageÊòØ‰∏Ä‰∏™high-resolution subnetworkÔºåÊ≤°Êúâ‰∏ãÈááÊ†∑ÔºåÊ≤°ÊúâparallelÂàÜÊîØ 4 residual unitsÔºåbottleneck resblock width=64 3x3 conv reducing width to C 2„ÄÅ3„ÄÅ4 stages Êé•‰∏ãÊù•ÁöÑstage gradually add high-to-low subnetwork ÊòØmulti-resolution subnetworks ÊØè‰∏™subnetworkÈÉΩÊØîÂâç‰∏Ä‰∏™Â§ö‰∏Ä‰∏™extra lower one resolution contain 1, 4, 3 exchange blocks respectively exchange block convÔºö4 residual unitsÔºåtwo 3x3 conv exchange unit width CÔºöwidth of the high-resolution subnetworks in last three stages other three parallel subnetworks HRNet-W32Ôºö64, 128, 256 HRNet-W48Ôºö96, 192, 384 repeated multi-scale fusion exchange blocksÔºöÊØè‰∏™high-to-low subnetworkÂåÖÂê´Â§ö‰∏™parallelÂàÜÊîØÔºåÊØèÊù°pathÁß∞‰∏∫exchange blockÔºåÊØè‰∏™exchange blockÂåÖÂê´‰∏ÄÁ≥ªÂàó3-conv-units + a exchange unit 3-conv-unitsÔºöÂ†ÜÂè†Âç∑ÁßØÊ†∏ÔºåÊèêÂèñÁâπÂæÅÔºåÂä†Ê∑±ÁΩëÁªú exchange unitÔºö‰∫§Êç¢‰∏çÂêåresolution levelÁöÑ‰ø°ÊÅØ notationsÔºö‰∏ÄÁ≥ªÂàóËæìÂÖ•$\{X_1,X_2, ‚Ä¶, X_r\}$Ôºå‰∏ÄÁ≥ªÂàóËæìÂá∫$\{Y_1,Y_2, ‚Ä¶, Y_r\}$ÔºåÂ¶ÇÊûúË∑®stageËøòÊúâ‰∏Ä‰∏™$Y_{r+1}$ ÊØè‰∏™$Y_k$ÈÉΩÊòØ‰∏Ä‰∏™aggregation of the input mapsÔºö$Y_k=\sum^s_i a(X_i,k)$ i&lt;kÔºöÈúÄË¶Å‰∏ãÈááÊ†∑ÔºåÊØè‰∏ãÈááÊ†∑‰∏ÄÂÄçÈÉΩÊòØ‰∏Ä‰∏™stride2-3x3-conv i=kÔºöidentify connection i&gt;kÔºöÈúÄË¶Å‰∏äÈááÊ†∑Ôºånearest neighbor upsamp + 1x1-align-conv k=$r+1$ÔºöÈúÄË¶ÅÂú®$Y_r$ÁöÑÂü∫Á°Ä‰∏äÔºåÂú®ÊâßË°å‰∏ÄÊ¨°stride2-3x3-conv‰∏ãÈááÊ†∑ÂæóÂà∞ fusionÔºösumÔºåÊâÄ‰ª•‰∏ä/‰∏ãÈááÊ†∑ÈÉΩÈúÄË¶ÅÈÄöÈÅìÂØπÈΩêÔºåËæìÂá∫mapÂíåÂØπÂ∫îlevelÁöÑËæìÂÖ•map‰øùÊåÅÂ∞∫ÂØ∏‰∏çÂèò heatmap estimation from the last high-res exchange unit mse gt gassian mapÔºöstd=1 network instantiation stem + 4 stages ÊØè‰∏™new stage inputÔºöres halved and channel doubled stem ‰∏§‰∏™s2-conv-bn-reluÔºåchannel 64 first stageÔºö ‰ΩøÁî®ÂíåResNet-50‰∏≠‰∏ÄÊ†∑ÁöÑ4‰∏™residual unitsÔºåchannel 64 ÁÑ∂ÂêéÁî®‰∏Ä‰∏™3x3-convË∞ÉÊï¥channelÂà∞‰∏Ä‰∏™Ëµ∑Âßãchannel C 2/3/4 stage Â†ÜÂè†exchange blocksÔºåÂàÜÂà´Êúâ1/4/3‰∏™exchange block ÊØè‰∏™exchange block‰ΩøÁî®4‰∏™residual unitsÂíå1‰∏™exchange unit ‰πüÂ∞±ÊòØÊÄªÂÖ±Êúâ8Ê¨°multi-scale fusion channel C/2C/4C HRNet-32ÔºöC=64 HRNet-48ÔºöC=96 HRNet v2: High-Resolution Representations for Labeling Pixels and Regions Âä®Êú∫ High-resolution representationÂæàÈáçË¶Å HRNet v1Â∑≤ÁªèÊúâ‰∏çÈîôÁöÑÁªìÊûú a further study on high resolution representations a small modificationÔºö‰πãÂâçÂè™ÂÖ≥Ê≥®high-resolution representationsÔºåÁé∞Âú®ÂÖ≥Ê≥®ÊâÄÊúâlevelÁöÑoutput representations ËÆ∫ÁÇπ Ëé∑Âæóhigh resolution representationÁöÑ‰∏§Â§ßÊñπÂºè recoverÁ≥ªÔºöÂÖà‰∏ãÈááÊ†∑ÔºåÁÑ∂ÂêéÁî®low-resolutionÈáçÂª∫ÔºåHourglassÔºåU-netÔºåencoder-decoder maintainÁ≥ªÔºöÂßãÁªà‰øùÁïôhigh-resolutionÁöÑrepresentationÔºåÂêåÊó∂‰∏çÊñ≠Áî®parallel low-resolution representationsÊù•strengthenÔºåHRNet HRNet maintains high-resolution representations connecting high-to-low resolution convolutions in parallel repeatedly conducting multi-scale fusions across levels ÁÆÄÂçïÊù•ËØ¥ÔºåÂ∞±ÊòØÂú®ÊØè‰∏™Èò∂ÊÆµÔºå‰øùÁïôÁé∞Êúâresolution levelÔºåÂêåÊó∂ ‰∏ç‰ªÖrepresentationË∂≥Â§üÂº∫Â§ßÔºàËûçÂêà‰∫Ülow-level high semantic infoÔºâÔºåËøòspatially precise our modification HRNetV2 HRNet ÈáåÈù¢Êàë‰ª¨Âè™ÂÖ≥Ê≥®ÊúÄ‰∏äÈù¢ÁöÑhigh-resolution representation HRNet V2ÈáåÈù¢Êàë‰ª¨Êé¢Á¥¢ÊâÄÊúâhigh-to-low parallel paths‰∏äÈù¢ÁöÑrepresentations Âú®ËØ≠ÊÑèÂàÜÂâ≤‰ªªÂä°‰∏≠Êàë‰ª¨‰ΩøÁî®output high resolution representationsÊù•ÁîüÊàêheatmaps Âú®Ê£ÄÊµã‰ªªÂä°‰∏≠Êàë‰ª¨Â∞Ümulti-levelÁöÑrepresentationsÁªôÂà∞FastRCNN ÊñπÊ≥ï Architecture multi-resolution block multi-resolution group convolutionÔºöÂú®ÊØè‰∏™representation levelÂàÜÂà´ÊâßË°åÂàÜÁªÑÂç∑ÁßØÔºådeeper multi-resolution convolutionÔºöÂèëÁîüÂú®ÊâÄÊúârepresentation level‰∏ä ‰∏ãÈááÊ†∑Ôºöstride-2 3x3 conv ‰∏äÈááÊ†∑Ôºöbilinear /nearest neighbor Modification HRNetV1ÔºöÂè™ÊääÊúÄÂêé‰∏Ä‰∏™Èò∂ÊÆµ highest resolutionÁöÑrepresentation‰Ωú‰∏∫ËæìÂá∫ HRNetV2ÔºöÊúÄÂêé‰∏Ä‰∏™Èò∂ÊÆµÔºåÊØè‰∏™resolution levelÁöÑrepresentationsÈÉΩ‰∏äÈááÊ†∑Âà∞highestÔºåÁÑ∂Âêéconcat‰Ωú‰∏∫ËæìÂá∫ÔºåÁîöËá≥ËøòÂ∞ÜËøô‰∏™ËæìÂá∫Ëøõ‰∏ÄÊ≠•‰∏ãÈááÊ†∑ÂæóÂà∞feature pyramid HRNet for classificationÔºö‰πüÂèØ‰ª•ÂèçÂêëÊìç‰ΩúÔºåÂ∞ÜÊúÄÂêé‰∏Ä‰∏™Èò∂ÊÆµÊØè‰∏™resolution levelÁöÑrepresentationsÈÉΩ‰∏ãÈááÊ†∑Âà∞lowestÔºåÁÑ∂ÂêésumÔºåÊúÄÂêéoutput 2048-dim representation is fed into the classifier ÂÆûÈ™å]]></content>
      <tags>
        <tag>È´òÂàÜËæ®ÁéáÔºå‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bilinear CNN]]></title>
    <url>%2F2020%2F09%2F18%2Fbilinear-CNN%2F</url>
    <content type="text"><![CDATA[17Âπ¥ÁöÑpaperÔºåÂºïÁî®Èáè15ÔºåÊèêÂá∫‰∫ÜÁΩëË∑ØÁªìÊûÑÔºå‰ΩÜÊòØÊ≤°ÂàÜÊûê‰∏∫Âï•ÊúâÊïàÔºåÂûÉÂúæ Bilinear CNNs for Fine-grained Visual Recognition Âä®Êú∫ fine-grained classification propose a pooled outer product of features derived from two CNNs 2 CNNs a bilinear layer a pooling layer outperform existing models and fairly efficient effective at other image classification tasks such as material, texture, and scene recognition ËÆ∫ÁÇπ fine-grained classification tasks require recognition of highly localized attributes of objects while being invariant to their pose and location in the image previous techniques part-based models construct representations by localizing parts more accurate but requires part annotations holistic models construct a representation of the entire image texture descriptorsÔºöFVÔºåSIFT STNÔºöaugment CNNs with parameterized image transformations attentionÔºöuse segmentation as a weakly-supervised manner Our key insight is that several widely-used texture representations can be written as a pooled outer product of two suitably designed features several widely-used texture representations two suitably designed features the bilinear features are highly redundant dimensionality reduction trade-off between accuracy We also found that feature normalization and domain-specific fine-tuning offers additional benefits combination concatenateÔºöadditional parameters to fuse an outer productÔºöno parameters sum productÔºöcan achieve similar approximations ‚Äútwo-stream‚Äù architectures one used to model two- factor variations such as ‚Äústyle‚Äù and ‚Äúcontent‚Äù for images in our case is to model two factor variations in location and appearance of partsÔºö‰ΩÜÂπ∂‰∏çÊòØexplicit modelingÂõ†‰∏∫ÊúÄÁªàÊòØ‰∏™ÂàÜÁ±ªÂ§¥ one used to analyze videos modeling the temporal aspect and the spatial aspect dimension reduction two 512-dim feature results in 512x512-dim earlier work projects one feature to a lower-dimensional space, e.g. 64-dim‚Äî&gt;512x64-dim we use compact bilinear pooling to generate low-dimensional embeddings (8-32x) ÊñπÊ≥ï architecture input $(l,I)$Ôºötakes an image and a locationÔºålocation generally contains position and scale quadruple $B=(f_A, f_B, P, C)$ A„ÄÅB‰∏§‰∏™CNNÔºöconv+pooling layersÔºå PÔºöpooling function combined A&amp;B outputs using the matrix outer product average pooling CÔºölogistic regression or linear SVM we found that linear models are effective on top of bilinear features CNN independentÔºèpartial sharedÔºèfully shared bilinear combination for each location $bilinear(l,I,f_A,f_B)=f_A(l,I)^T f_B(l,I)$ pooling function combines bilinear features across all locations $\Phi (I) = \sum_{l\in L} bilinear(l,I,f_A,f_B)$ same feature dimension K for A &amp; BÔºåe.g. KxM &amp; KxN respectivelyÔºå$\Phi(I)$ is size MxN Normalization a signed square rootÔºö$y=sign(x)\sqrt {|x|}$ follow a l2 normÔºö$z = \frac{y}{||y||_2}$ improves performance in practice classification logistic regression or linear SVM we found that linear models are effective on top of bilinear features back propagation $\frac{dl}{dA}=B(\frac{dl}{dx})^T$Ôºå$\frac{dl}{dB}=A(\frac{dl}{dx})^T$ Relation to classical texture representationsÔºöÊîæÂú®Ëøô‰∏ÄËäÇÊíëÁØáÂπÖÔºüÔºü texture representations can be defined by the choice of the local features, the encoding function, the pooling function, and the normalization function choice of local featuresÔºöorderless aggregation with sumÔºèmax operation encoding functionÔºöA non-linear encoding is typically applied to the local feature before aggregation normalizationÔºönormalization of the aggregated feature is done to increase invariance end-to-end trainable]]></content>
      <tags>
        <tag>ÁªÜÁ≤íÂ∫¶ÔºåÁâπÂæÅËûçÂêà</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[label smoothing]]></title>
    <url>%2F2020%2F09%2F14%2Flabel-smoothing%2F</url>
    <content type="text"><![CDATA[Âä®Êú∫ to understand label smoothing improving generalization improves model calibration changes the representations learned by the penultimate layer of the network effect on knowledge distillation of a student network soft targetsÔºöa hard target and the uniform distribution of other classes ËÆ∫ÁÇπ label smoothing implicitly calibrates the learned models ËÉΩËÆ©confidencesÊõ¥ÊúâËß£ÈáäÊÄß‚Äî‚Äîmore aligned with the accuracies of their predictions label smoothing impairs distillation‚Äî‚ÄîteacherÁî®‰∫Ülabel smoothingÔºåstudent‰ºöË°®Áé∞ÂèòÂ∑ÆÔºåthis adverse effect results from loss of information in the digits ÊñπÊ≥ï modeling penultimate layerÔºöfc with activation $p_k = \frac{e^{wx}}{\sum e^{wx}}$ outputsÔºöloss $H(y,p)=\sum_{k=1}^K -y_klog(p_k)$ hard targetsÔºö$y_k$ is 1 for the correct class and 0 for the rest label smoothingÔºö$y_k^{LS} = y_k(1-\alpha)+ \alpha /K$ visualization schem Â∞ÜdimK activation vectorÊäïÂΩ±Âà∞Ê≠£‰∫§Âπ≥Èù¢‰∏äÔºåa dim2 vector per example clusters are much tighter because label smoothing encourages that each example in training set to be equidistant from all the other class‚Äôs templates 3 classes shows triangle structure since ‚Äòequidistant‚Äô predictions‚Äò absolute values are much bigger without LM, representing over-confident semantically similar classes are harder to separateÔºå‰ΩÜÊòØÊÄª‰Ωì‰∏äclusterÂΩ¢ÊÄÅËøòÊòØÂ•Ω‰∏ÄÁÇπ training without label smoothing there is continuous degree of change between two semantically similar classesÔºåÁî®‰∫ÜLM‰ª•ÂêéÂ∞±ËßÇÂØü‰∏çÂà∞‰∫Ü‚Äî‚ÄîÁõ∏‰ººclass‰πãÈó¥ÁöÑËØ≠‰πâÁõ∏ÂÖ≥ÊÄßË¢´Á†¥Âùè‰∫ÜÔºå‚Äôerasure of information‚Äô have similar accuracies despite qualitatively different clusteringÔºåÂØπÂàÜÁ±ªÁ≤æÂ∫¶ÁöÑÊèêÂçá‰∏çÊòéÊòæÔºå‰ΩÜÊòØ‰ªéclusterÂΩ¢ÊÄÅ‰∏äÁúãÊõ¥Â•ΩÁúã model calibration making the confidence of its predictions more accurately represent their accuracy metricÔºöexpected calibration error (ECE) reliability diagram better calibration compared to the unscaled network Despite trying to collapse the training examples to tiny clusters, these networks generalize and are calibratedÔºöÂú®ËÆ≠ÁªÉÈõÜ‰∏äÁöÑclusterÂàÜÂ∏ÉÈùûÂ∏∏Á¥ßÂáëÔºåencourageÊØè‰∏™Ê†∑Êú¨ÈÉΩÂíåÂÖ∂‰ªñÁ±ªÂà´ÁöÑcluster‰øùÊåÅÁõ∏ÂêåÁöÑË∑ùÁ¶ªÔºå‰ΩÜÊòØÂú®ÊµãËØïÈõÜ‰∏äÔºåÊ†∑Êú¨ÁöÑÂàÜÂ∏ÉÂ∞±ÊØîËæÉÊùæÊï£‰∫ÜÔºå‰∏ç‰ºöÈôêÂÆöÂú®Â∞èÂ∞èÁöÑ‰∏ÄÂù®ÂÜÖÔºåËØ¥ÊòéÁΩëÁªúÊ≤°Êúâover-confidentÔºårepresenting the full range of confidences for each prediction knowledge distillation even when label smoothing improves the accuracy of the teacher network, teachers trained with label smoothing produce inferior student networks As the representations collapse to small clusters of points, much of the information that could have helped distinguish examples is lost Áúãtraining setÁöÑscatterÔºåLM‰ºöÂÄæÂêë‰∫éÂ∞Ü‰∏ÄÁ±ªsampleÈõÜ‰∏≠Êàê‰∏∫Áõ∏‰ººÁöÑË°®ÂæÅÔºåsample‰πãÈó¥ÁöÑÂ∑ÆÂºÇÊÄß‰ø°ÊÅØ‰∏¢‰∫ÜÔºöTherefore a teacher with better accuracy is not necessarily the one that distills better]]></content>
      <tags>
        <tag>ÂàÜÁ±ªÔºåloss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[noisy student]]></title>
    <url>%2F2020%2F09%2F11%2Fnoisy-student%2F</url>
    <content type="text"><![CDATA[Self-training with Noisy Student improves ImageNet classification Âä®Êú∫ semi-supervised learningÔºàSSLÔºâ semi-supervised approach when labeled data is abundant use unlabeled images to improve SOTA model improve self-training and distillation accuracy and robustness better acc, mCE, mFR EfficientNet model on labeled images student even or larger student model on labeled &amp; pseudo labeled images noise, stochastic depth, data augmentation generalizes better process iteration by putting back the student as the teacher ËÆ∫ÁÇπ supervised learning which requires a large corpus of labeled images to work well robustness noisy dataÔºöunlabeled images that do not belong to any category in ImageNet large margins on much harder test sets training process teacher EfficientNet model on labeled images student even or larger student model on labeled &amp; pseudo labeled images noise, stochastic depth, data augmentation generalizes better process iteration by putting back the student as the teacher improve in two ways it makes the student largerÔºöÂõ†‰∏∫Áî®‰∫ÜÊõ¥Â§öÊï∞ÊçÆ noised student is forced to learn harderÔºöÂõ†‰∏∫labelÊúâpseudo labelsÔºåinputÊúâÂêÑÁ±ªaugmentationÔºåÁΩëÁªúÊúâdropoutÔºèstochastic depth main difference compared with Knowledge Distillation use noise ‚Äî‚Äî‚Äî KD do not use use equal/larger student ‚Äî‚Äî‚Äî KD use smaller student to learn faster think of as Knowledge Expansion giving the student model enough capacity and difficult environments want the student to be better than the teacher ÊñπÊ≥ï algorithm train teacher use labeled images use teacher to inference unlabedled images, generating pseudo labels, soft/one-hot train student model use labeled &amp; unlabeld images make student the new teacher, jump to the inter step noise enforcing invariancesÔºöË¶ÅÊ±ÇstudentÁΩëÁªúËÉΩÂ§üÂØπÂêÑÁßçÂ¢ûÂº∫ÂêéÁöÑÊï∞ÊçÆÈ¢ÑÊµãlabel‰∏ÄÊ†∑Ôºåensure consistency required to mimic a more powerful ensemble modelÔºöteacherÁΩëÁªúÂú®inferenceÈò∂ÊÆµËøõË°ådropoutÂíåstochastic depthÔºåbehaves like an ensembleÔºåwhereas the student behaves like a single modelÔºåËøôÂ∞±push studentÁΩëÁªúÂéªÂ≠¶‰π†‰∏Ä‰∏™Êõ¥Âº∫Â§ßÁöÑÊ®°Âûã other techniques data filtering we filter images that the teacher model has low confidences ËøôÈÉ®ÂàÜdata‰∏étraining dataÁöÑÂàÜÂ∏ÉËåÉÂõ¥ÂÜÖ data balancing duplicate images in classes where there are not enough images take the images with the highest confidence when there are too many softÔºèhard pseudo labels both work soft slightly better ÂÆûÈ™å dataset benchmarked datasetÔºöImageNet 2012 ILSVRC unlabeled datasetÔºöJFT fillter &amp; balancingÔºö use EfficientNet-B0 trained on ImageNetÔºåinference over JFT take images with confidence over 0.3 130M at most per class models EfficientNet-L2 further scale up EfficientNet-B7 wider &amp; deeper lower resolution train-test resolution discrepancy first perform normal training with a smaller resolution for 350 epochs then finetune the model with a larger resolution for 1.5 epochs on unaugmented labeled images shallow layers are fixed during finetuning noise stochastic depthÔºöstochastic depth 0.8 for the final layer and follow the linear decay rule for other layers dropoutÔºödropout 0.5 for the final layer RandAugmentÔºötwo random operations with magnitude set to 27 iterative training „Äêteacher„Äëfirst trained an EfficientNet-B7 on ImageNet „Äêstudent„Äëthen trained an EfficientNet-L2 with the unlabeled batch size set to 14 times the labeled batch size „Äênew teacher„Äëtrained a new EfficientNet-L2 „Äênew student„Äëtrained an EfficientNet-L2 with the unlabeled batch size set to 28 times the labeled batch size „Äêiteration„Äë‚Ä¶ robustness test difficult images common corruptions and perturbations FGSM attack metrics improves the top-1 accuracy reduces mean corruption error (mCE) reduces mean flip rate (mFR) ablation study noisy Â¶ÇÊûú‰∏çnoise the studentÔºåÂΩìstudent modelÁöÑÈ¢ÑÊµãÂíåteacherÈ¢ÑÊµãÁöÑunlabeledÊï∞ÊçÆÂÆåÂÖ®‰∏ÄÊ†∑ÁöÑÊÉÖÂÜµ‰∏ãÔºåloss‰∏∫0Ôºå‰∏çÂÜçÂ≠¶‰π†ÔºåËøôÊ†∑studentÂ∞±‰∏çËÉΩoutperform teacher‰∫Ü injecting noise to the student model enables the teacher and the student to make different predictions The student performance consistently drops with noise function removed removing noise leads to a smaller drop in training lossÔºåËØ¥ÊòénoiseÁöÑ‰ΩúÁî®‰∏çÊòØ‰∏∫‰∫Üpreventing overfittingÔºåÂ∞±ÊòØ‰∏∫‰∫Üenhance model iteration iterative training is effective in producing increas- ingly better models larger batch size ratio for latter iteration]]></content>
      <tags>
        <tag>classification, semi-supervised, teacher-student</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[complement cross entropy]]></title>
    <url>%2F2020%2F09%2F08%2Fcomplement-cross-entropy%2F</url>
    <content type="text"><![CDATA[summary ‰ΩøÁî®complement lossÁöÑ‰∏ªË¶ÅÂä®Êú∫ÊòØone-hotÁöÑlabel‰∏ãÔºåceÂè™ÂÖ≥Ê≥®ÊãâÈ´òÊ≠£Ê†∑Êú¨Ê¶ÇÁéáÔºå‰∏ßÂ§±Êéâ‰∫ÜÂÖ∂‰ªñincorrectÁ±ªÂà´ÁöÑ‰ø°ÊÅØ ‰∫ãÂÆû‰∏äÂØπ‰∫éincorrectÁ±ªÂà´ÔºåÂèØ‰ª•ËÆ©ÂÖ∂ËæìÂá∫Ê¶ÇÁéáÂÄºÂàÜÂ∏ÉÁöÑÁÜµÂ∞ΩÂèØËÉΩÁöÑÂ§ß‚Äî‚Äî‰πüÂ∞±ÊòØÂ∞ÜËøô‰∏™ÂàÜÂ∏ÉÂ∞ΩÂèØËÉΩÊé®ÂêëÂùáÂåÄÂàÜÂ∏ÉÔºåËÆ©ÂÆÉ‰ª¨‰πãÈó¥‰∫íÁõ∏ÈÅèÂà∂‰ªéËÄåÂá∏ÊòæÂá∫ground truthÁöÑÊ¶ÇÁéá ‰ΩÜËøôÊòØÂª∫Á´ãÂú®‚ÄúÂêÑ‰∏™Ê†áÁ≠æ‰πãÈó¥Áõ∏‰∫íÁã¨Á´ã‚ÄùËøô‰∏™ÂÅáËÆæ‰∏äÔºåÂ¶ÇÊûúÁ±ªÂà´Èó¥ÊúâhierarchicalÁöÑÂÖ≥Á≥ªÔºèmulti-labelÔºåÂ∞±‰∏çË°å‰∫Ü„ÄÇ Âú®Êï∞Â≠¶Ë°®Ëææ‰∏äÔºå È¶ñÂÖà‰ªçÁÑ∂ÊòØÁî®ce‰ΩúÁî®‰∫écorrect labelÔºåÂ∏åÊúõÊ≠£Ê†∑Êú¨Ê¶ÇÁéágt_predÂ∞ΩÂèØËÉΩÊèêÈ´òÔºåÊé•ËøëÁúüÂÆûÂÄº ÁÑ∂ÂêéÊòØ‰ΩúÁî®‰∫éincorrect labelÁöÑcceÔºåÂú®Èô§‰∫ÜÊ≠£‰æãpred possibility‰ª•Â§ñÁöÑÂá†‰∏™Ê¶ÇÁéá‰∏äÔºåËÆ°ÁÆó‰∫§ÂèâÁÜµÔºåÂ∏åÊúõËøôÂá†‰∏™Ê¶ÇÁéáÂ∞ΩÂèØËÉΩÊúç‰ªéÂùáÂåÄÂàÜÂ∏ÉÔºåÊ¶ÇÁéáÊé•Ëøë$\frac{1-gt_pred}{K-1}$ ÊàëÊÑüËßâËøôÂ∞±ÊòØlabel smoothingÔºå‰∏ªË¶ÅÂå∫Âà´Â∞±ÊòØcce‰∏äÊúâ‰∏™normÈ°πÔºålabel smoothinÂú®ËÆ°ÁÆóceÁöÑÊó∂ÂÄôÔºåvector‰∏≠ÊØè‰∏Ä‰∏™incorrect labelÁöÑÁÜµÈÉΩ‰∏écorrect labelÁ≠âÊùÉÈáçÔºåcceÂØπÊï¥‰∏™incorrect vectorÁöÑÊùÉÈáç‰∏écorrect labelÁ≠âÂêåÔºå‰∏îÂèØ‰ª•Ë∞ÉÊï¥„ÄÇ Imbalanced Image Classification with Complement Cross Entropy Âä®Êú∫ class-balanced datasets motivated by COT(complement objective training) suppressing softmax probabilities on incorrect classes during training propose cce keep ground truth probability overwhelm the other classes neutralizing predicted probabilities on incorrect classes ËÆ∫ÁÇπ class imbalace limits generalization resample oversampling on minority classes undersampling on majority classes reweight neglect the fact that samples on minority classes may have noise or false annotations might cause poor generalization observed degradation in imbalanced datasets using CE cross entropy mostly ignores output scores on wrong classes neutralizing predicted probabilities on incorrect classes helps improve accuracy of prediction for imbalanced image classification ÊñπÊ≥ï complement entropy calculated on incorrect classes N samplesÔºåK-dims class vector $C(y,\hat y)=-\frac{1}{N}\sum_{i=1}^N\sum_{j=1,j \neq g}^K \frac{\hat y^j}{1-\hat y^g}log\frac{\hat y^j}{1-\hat y^g} $ the purpose is to encourage larger gap between ground truth and other classes ‚Äî‚Äî when the incorrect classes obey normal distribution it reaches optimal balanced complement entropy add balancing factor $C^{‚Äò}(y,\hat y) = \frac{1}{K-1}C(y,\hat y)$ forming COTÔºö twice back-propagation per each iteration first cross entropy second complement entropy CCE (Complement Cross Entropy) add modulating factorÔºö$\tilde C(y, \hat y) = \frac{\gamma}{K-1}C(y, \hat y)$Ôºå$\gamma=-1$ combinationÔºöCE+CCE ÂÆûÈ™å datasetÔºö cifar class-balanced originally construct imbalanced variants with imbalance ratio $\frac{N_{min}}{N_{max}}$ test acc ËÆ∫ÊñáÁöÑÂÆûÈ™åÁªìÊûúÈÉΩÊòØÂú®cifar‰∏äcceÂ•Ω‰∫écotÂ•Ω‰∫éfocal lossÔºåÂú®road‰∏äcceÂ•Ω‰∫écotÔºåÊ≤°Êîæfl Âí±‰πü‰∏çÁü•ÈÅì„ÄÇ„ÄÇ„ÄÇ]]></content>
      <tags>
        <tag>loss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[regression loss]]></title>
    <url>%2F2020%2F09%2F07%2Fregression-loss%2F</url>
    <content type="text"><![CDATA[ÊçüÂ§±ÂáΩÊï∞Áî®Êù•ËØÑ‰ª∑Ê®°ÂûãÈ¢ÑÊµãÂÄºÂíåÁúüÂÆûÂÄºÁöÑ‰∏ç‰∏ÄÊ†∑Á®ãÂ∫¶ ‰∏§Á≥ªÊçüÂ§±ÂáΩÊï∞Ôºö ÁªùÂØπÂÄºloss $L(Y,f(x))=|Y-f(x)|$ Âπ≥ÂùáÁªùÂØπÂÄºÊçüÂ§±ÔºåMAEÔºåL1 ÂØπÂºÇÂ∏∏ÁÇπÊúâÊõ¥Â•ΩÁöÑÈ≤ÅÊ£íÊÄß Êõ¥Êñ∞ÁöÑÊ¢ØÂ∫¶ÂßãÁªàÁõ∏ÂêåÔºåÂØπ‰∫éÂæàÂ∞èÁöÑÊçüÂ§±ÂÄºÔºåÊ¢ØÂ∫¶‰πüÂæàÂ§ßÔºå‰∏çÂà©‰∫éÊ®°ÂûãÂ≠¶‰π†‚Äî‚ÄîÊâãÂä®Ë°∞ÂáèÂ≠¶‰π†Áéá Âπ≥ÊñπÂ∑Æloss $L(Y, f(x)) = (Y-f(x))^2$ ÂùáÊñπËØØÂ∑ÆÊçüÂ§±ÔºåMSEÔºåL2 Âõ†‰∏∫Âèñ‰∫ÜÂπ≥ÊñπÔºå‰ºöËµã‰∫àÂºÇÂ∏∏ÁÇπÊõ¥Â§ßÁöÑÊùÉÈáçÔºå‰ºö‰ª•Áâ∫Áâ≤ÂÖ∂‰ªñÊ†∑Êú¨ÁöÑËØØÂ∑Æ‰∏∫‰ª£‰ª∑ÔºåÊúùÁùÄÂáèÂ∞èÂºÇÂ∏∏ÁÇπËØØÂ∑ÆÁöÑÊñπÂêëÊõ¥Êñ∞ÔºåÈôç‰ΩéÊ®°ÂûãÁöÑÊï¥‰ΩìÊÄßËÉΩ Huber loss $L = \begin{cases} \frac{1}{2}(y-f(x))^2,\text{ for }|y-f(x)|&lt;\delta,\\ \delta |y-f(x)|-\frac{1}{2}\delta^2, \text{ otherwise} \end{cases} $ Ë∂ÖÂèÇÂÜ≥ÂÆö‰∫ÜÂØπ‰∏éÂºÇÂ∏∏ÁÇπÁöÑÂÆö‰πâÔºåÂè™ÂØπËæÉÂ∞èÁöÑÂºÇÂ∏∏ÂÄºÊïèÊÑü ÂØπÊï∞loss L(Y, P(Y|X)) = -log(P(Y|X)) cross-entropy loss ‰∫åÂàÜÁ±ªÂèåËæπËÆ°ÁÆóÔºö L = ylna + (1-y)ln(1-a) Â§öÂàÜÁ±ªÂçïËæπËÆ°ÁÆóÔºö L = ylna ÊåáÊï∞loss L(Y, f(x)) = exp[-yf(x)] Hinge loss L(Y, f(x)) = max(0, 1-yf(x)) perceptron loss L(Y, f(x)) = max(0, -yf(x)) cross-entropy loss ‰∫åÂàÜÁ±ªÂèåËæπËÆ°ÁÆóÔºö L = ylna + (1-y)ln(1-a) Â§öÂàÜÁ±ªÂçïËæπËÆ°ÁÆóÔºö L = ylna]]></content>
      <tags>
        <tag>ÂõûÂΩí</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pseudo-3d]]></title>
    <url>%2F2020%2F09%2F02%2Fpseudo-3d%2F</url>
    <content type="text"><![CDATA[[3d resnet] Learning Spatio-Temporal Features with 3D Residual Networks for Action RecognitionÔºöÁúü3dÔºåfor comparisonÔºåÂàÜÁ±ª [C3d] Learning Spatiotemporal Features with 3D Convolutional NetworksÔºöÁúü3dÔºåfor comparisonÔºåÂàÜÁ±ª [Pseudo-3D resnet] Learning Spatio-Temporal Representation with Pseudo-3D Residual NetworksÔºö‰º™3dÔºåresblockÔºåSÂíåTËä±ÂºèËøûÊé•ÔºåÂàÜÁ±ª [2.5d Unet] Automatic Segmentation of Vestibular Schwannoma from T2-Weighted MRI by Deep Spatial Attention with Hardness-Weighted LossÔºöpatchËæìÂÖ•ÔºåÂÖà2dÂêé3dÔºåÈíàÂØπÂêÑÂêëÂºÇÊÄßÔºåÂàÜÂâ≤ [two-pathway U-Net] Combining analysis of multi-parametric MR images into a convolutional neural network: Precise target delineation for vestibular schwannoma treatment planningÔºöpatchËæìÂÖ•Ôºå3dÁΩëÁªúÔºåxyÂíåzÂπ≥Èù¢ÂàÜÂà´conv &amp; concatÔºåÂàÜÂâ≤ [Projection-Based 2.5D U-net] Projection-Based 2.5D U-net Architecture for Fast Volumetric SegmentationÔºömipÔºå2dÁΩëÁªúÔºåÂàÜÂâ≤ÔºåÈáçÂª∫ [New 2.5D Representation] A New 2.5D Representation for Lymph Node Detection using Random Sets of Deep Convolutional Neural Network ObservationsÔºöÊ®™ÂÜ†Áü¢‰∏â‰∏™Âπ≥Èù¢‰Ωú‰∏∫‰∏â‰∏™channelËæìÂÖ•Ôºå2dÁΩëÁªúÔºåÊ£ÄÊµã Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks Âä®Êú∫ spatio-temporal video the development of a very deep 3D CNN from scratch results in expensive computational cost and memory demand new framework 1x3x3 &amp; 3x1x1 Pseudo-3D Residual Net which exploits all the variants of blocks outperforms 3D CNN and frame-based 2D CNN ËÆ∫ÁÇπ 3d CNNÁöÑmodel sizeÔºömaking it extremely difficult to train a very deep model fine-tuning 2d Â•Ω‰∫é train from scrach 3d RNN builds only the temporal connections on the high-level featuresÔºåleaving the correlations in the low-level forms not fully exploited we propose 1x3x3 &amp; 3x1x1 in parallel or cascaded ÂÖ∂‰∏≠ÁöÑ3x3 convÂèØ‰ª•Áî®2d convÊù•ÂàùÂßãÂåñ a family of bottleneck building blocksÔºöenhance the structural diversity ÊñπÊ≥ï P3D Blocks directÔºèindirect influenceÔºöSÂíåT‰πãÈó¥ÊòØ‰∏≤ËÅîËøòÊòØÂπ∂ËÅî directÔºèindirect connected to the final outputÔºöSÂíåTÁöÑËæìÂá∫ÊòØÂê¶Áõ¥Êé•‰∏éidentity pathÁõ∏Âä† bottleneckÔºö Â§¥Â∞æÂêÑÊé•‰∏Ä‰∏™1x1x1ÁöÑconv Â§¥Áî®Êù•narrow channelÔºåÂ∞æÁî®Êù•widen back Â§¥ÊúâreluÔºåÂ∞æÊ≤°Êúârelu Pseudo-3D ResNet mixing blocksÔºöÂæ™ÁéØABC better performance &amp; small increase in model size fine-tuning resnet50Ôºö randomly cropped 224x224 freeze all BN except for the first one add an extra dropout layer with 0.9 dropout rate further fine-tuning P3D resnetÔºö initialize with r50 in last step randomly cropped 16x160x160 horizontally flipped mini-batch as 128 frames future work attention mechanism will be incorporated Projection-Based 2.5D U-net Architecture for Fast Volumetric Segmentation Âä®Êú∫ MIPÔºö2D images containing information of the full 3D image faster, less memory, accurate ÊñπÊ≥ï 2d unet MIPÔºö$\alpha=36$ 3x3 conv, s2 pooling, transpose conv, concat, BN, relu, filtersÔºöbegin with 32, end with 512 dropoutÔºö0.5 in the deepest convolutional block and 0.2 in the second deepest blocks 3d unet overfitting &amp; memory space filtersÔºöbegin with 4, end with 16 dropoutÔºö0.5 in the deepest convolutional block and 0.4 in the second deepest blocks Projection-Based 2.5D U-net 2d sliceÔºöloss of connection 2d mipÔºödisappointing results 2d volumeÔºölong training time the proposed 2.5D U-netÔºö N(x) = T R_p F_p \left[ \begin{matrix} U M_{\alpha_1}(x) \\ ... \\ U M_{\alpha_p}(x) \end{matrix} \right] $M_{i}$ÔºöMIPÔºåp=12 $U$Ôºö2d-Unet like above $F_p$Ôºölearnable filtrationÔºå1x3 convÔºåfor each projectionÔºåÊäëÂà∂ÈáçÂª∫‰º™ÂΩ± $R_p$Ôºöreconstruction operator $T$Ôºöfine-tuning operatorÔºåshift &amp; scale back to 0-1 mask Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition Âä®Êú∫ 3D kernels tend to overfit 3D CNNs is relatively shallow propose a 3D CNNs based on ResNets better performance not overfit deeper than C3D ËÆ∫ÁÇπ two-stream architectureÔºöconsists of RGB and optical flow streams is often used to represent spatio-temporal information 3D CNNsÔºötrained on relatively small video datasets performs worse than 2D CNNs pretrained on large datasets Very deep 3D CNNsÔºönot explored yet due to training difficulty ÊñπÊ≥ï Network Architecture main differenceÔºökernel dimensions stemÔºöstride2 for SÔºåstride1 for T resblockÔºöconv_bn_relu&amp;conv + id identity shortcutsÔºöuse zero-padding for increasing dimensionsÔºåto avoid increasing the number of parameters stride2 convÔºöconv3_1„ÄÅ conv4_1„ÄÅ conv5_1 input clipsÔºö3x16x112x112 large learning rate and batch size was important ÂÆûÈ™å Âú®Â∞èÊï∞ÊçÆÈõÜ‰∏ä3d-r18‰∏çÂ¶ÇC3DÔºåoverfit‰∫ÜÔºöshallow architecture of the C3D and pretraining on the Sports-1M dataset prevent the C3D from overfitting Âú®Â§ßÊï∞ÊçÆÈõÜ‰∏ä3d-r34Â•Ω‰∫éC3DÔºåÂêåÊó∂C3DÁöÑval accÊòéÊòæÈ´ò‰∫étrain acc‚Äî‚ÄîÂ§™shallowÊ¨†ÊãüÂêà‰∫ÜÔºår34ÂàôË°®Áé∞Êõ¥Â•ΩÔºåËÄå‰∏î‰∏çÈúÄË¶ÅÈ¢ÑËÆ≠ÁªÉ RGB-I3D achieved the best performance 3d-r34ÊòØÊõ¥deeperÁöÑ RGB-I3DÁî®‰∫ÜÊõ¥Â§ßÁöÑbatch sizeÔºöLarge batch size is important to train good models with batch normalization High resolutionsÔºö3x64x224x224 Learning Spatiotemporal Features with 3D Convolutional Networks Âä®Êú∫ generic efficient simple 3d ConvNet with 3x3x3 conv &amp; a simple linear classifier ËÆ∫ÁÇπ 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets 2D ConvNets lose temporal information of the input signal right after every convolution operation 2d convÂú®channelÁª¥Â∫¶‰∏äÊùÉÈáçÈÉΩÊòØ‰∏ÄÊ†∑ÁöÑÔºåÁõ∏ÂΩì‰∫étemporal dims‰∏äÊ≤°ÊúâÈáçË¶ÅÊÄßÁâπÂæÅÊèêÂèñ ÊñπÊ≥ï basic network settings 5 conv layers + 5 pooling layers + 2 fc layers + softmax filtersÔºö[64Ôºå128Ôºå256Ôºå256Ôºå256] fc dimsÔºö[2048Ôºå2048] conv kernelÔºödx3x3 pooling kernelÔºö2x2x2Ôºås2 except for the first layer with the intention of not to merge the temporal signal too early also to satisfy the clip length of 16 frames varing settings temporal kernel depth homogeneousÔºödepth-1/3/5/7 throughout varyingÔºöincreasing-3-3-5-5-7 &amp; decreasing-7- 5-5-3-3 depth-3 throughout performs the best depth-1 is significantly worse We also verify that 3D ConvNet consistently performs better than 2D ConvNet on a large-scale internal dataset C3D 8 conv layers + 5 pooling layers + 2 fc layers + softmax homogeneousÔºö3x3x3 s1 conv thtoughout pool1Ôºö1x2x2 kernel size &amp; strideÔºårest 2x2x2 fc dimsÔºö4096 C3D video descriptorÔºöfc6 activations + L2-norm deconvolution visualizingÔºö conv5b feature maps starts by focusing on appearance in the first few frames tracks the salient motion in the subsequent frames compactness PCA ÂéãÁº©Âà∞50-100dim‰∏çÂ§™ÊçüÂ§±acc ÂéãÁº©Âà∞10dim‰ªçÊóßÊòØÊúÄÈ´òacc projected to 2-dimensional space using t-SNE C3D features are semantically separable compared to Imagenet quantitatively observe that C3D is better than Imagenet Action Similarity Labeling predicting action similarity extract C3D features: prob, fc7, fc6, pool5 for each clip L2 normalization compute the 12 different distances for each featureÔºö48 in total linear SVM is trained on these 48-dim feature vectors C3D significantly outperforms the others]]></content>
      <tags>
        <tag>3d CNN, 2.5d CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSD]]></title>
    <url>%2F2020%2F08%2F13%2FSSD%2F</url>
    <content type="text"><![CDATA[SSD: Single Shot MultiBox Detector Âä®Êú∫ single network speed &amp; accuracy 59 FPS / 74.3% mAP ËÆ∫ÁÇπ prev methods two-stageÔºöÁîüÊàêÁ®ÄÁñèÁöÑÂÄôÈÄâÊ°ÜÔºåÁÑ∂ÂêéÂØπÂÄôÈÄâÊ°ÜËøõË°åÂàÜÁ±ª‰∏éÂõûÂΩí one-stageÔºöÂùáÂåÄÂú∞Âú®ÂõæÁâáÁöÑ‰∏çÂêå‰ΩçÁΩÆÔºåÈááÁî®‰∏çÂêåÂ∞∫Â∫¶ÂíåÈïøÂÆΩÊØîÔºåËøõË°åÂØÜÈõÜÊäΩÊ†∑ÔºåÁÑ∂ÂêéÂà©Áî®CNNÊèêÂèñÁâπÂæÅÂêéÁõ¥Êé•ËøõË°åÂàÜÁ±ª‰∏éÂõûÂΩí fundamental speed improvement eliminating bounding box proposals eliminating feature resampling other improvements small convolutional filter for bbox categories and offsetsÔºàÈíàÂØπyolov1ÁöÑÂÖ®ËøûÊé•Â±ÇËØ¥Ôºâ separate predictors by aspect ratio multiple scales Ëøô‰∫õÊìç‰ΩúÈÉΩ‰∏çÊòØÂéüÂàõ The core of SSD is predicting category scores and box offsets for a fixed set of default bounding boxes using small convolutional filters applied to feature maps. ÊñπÊ≥ï Model Multi-scale feature maps for detectionÔºöÈááÁî®‰∫ÜÂ§öÂ∞∫Â∫¶ÁöÑÁâπÂæÅÂõæÔºåÈÄêÊ∏êÁî®s2ÈôçÁª¥ÔºåÂ§ßÂ∞∫Â∫¶ÁâπÂæÅÂõæ‰∏äÊúâÊõ¥Â§öÁöÑÂçïÂÖÉÔºåÁî®Êù•ÂõûÂΩíÂ∞èÁâ©‰Ωì Convolutional predictors for detectionÔºöÈíàÂØπyolov1ÈáåÈù¢ÁöÑfcÂ±Ç Default boxes and aspect ratiosÔºö‰∏Ä‰∏™ÂçïÂÖÉ4ÁßçsizeÁöÑÂÖàÈ™åÊ°ÜÔºåÂØπÊØè‰∏™ÂÖàÈ™åÊ°ÜÈÉΩÈ¢ÑÊµã‰∏ÄÁªÑ4+(c+1)ÔºåÂÖ∂‰∏≠ÁöÑ1ÂèØ‰ª•Áúã‰ΩúËÉåÊôØÁ±ªÔºå‰πüÂèØ‰ª•ÁúãÂÅöÊòØÊúâÊó†ÁõÆÊ†áÁöÑÁΩÆ‰ø°Â∫¶ÔºåÂêÑÁî®‰∏Ä‰∏™conv3x3ÁöÑhead backbone ÂèÇËÄÉÔºöhttps://www.cnblogs.com/sddai/p/10206929.html VGG16ÂâçÂõõ‰∏™conv block‰øùÁïô Êó†dropoutÂíåfc conv5ÁöÑÊ±†ÂåñÁî±2x2-s2ÂèòÊàê3x3-s1 conv6Âíåconv7ÊòØ3x3x1024Âíå1x1x1024ÁöÑÁ©∫Ê¥ûÂç∑ÁßØÔºåËæìÂá∫19x19x1024 conv8ÊòØ1x1x256Âíå3x3x512 s2ÁöÑconvÔºåËæìÂá∫10x10x512 conv9ÈÉΩÊòØ1x1x128Âíå3x3x256 s2ÁöÑconvÔºåËæìÂá∫5x5x256 conv10„ÄÅconv11ÈÉΩÊòØ1x1x128Âíå3x3x256 s1 p0ÁöÑconvÔºåËæìÂá∫3x3x256„ÄÅ1x1x256 Training Matching strategyÔºömatch default boxÂíågt box È¶ñÂÖà‰∏∫ÊØè‰∏Ä‰∏™gt boxÊâæÂà∞‰∏Ä‰∏™overlapÊúÄÂ§ßÁöÑdefault box ÁÑ∂ÂêéÊâæÂà∞ÊâÄÊúâ‰∏égt boxÁöÑoverlapÂ§ß‰∫é0.5ÁöÑdefault box ‰∏Ä‰∏™gt boxÂèØËÉΩÂØπÂ∫îÂ§ö‰∏™default box ‰∏Ä‰∏™default boxÂè™ËÉΩÂØπÂ∫î‰∏Ä‰∏™gt boxÔºàoverlapÊúÄÂ§ßÁöÑÔºâ Objective loss loc lossÔºösmooth L1Ôºåoffsets like Faster R-CNN cls lossÔºösoftmax loss weighted sumÔºö$L = \frac{1}{N} (L_{cls} + \alpha L_{loc})$Ôºå N is the number of matched default boxes loss=0 when N=0 Choosing scales and aspect ratios for default boxes ÊØè‰∏™levelÁöÑfeature mapÊÑüÂèóÈáé‰∏çÂêåÔºådefault boxÁöÑÂ∞∫ÂØ∏‰πü‰∏çÂêå Êï∞Èáè‰πü‰∏çÂêåÔºåconv4„ÄÅconv10Âíåconv11ÊòØ4‰∏™Ôºåconv7„ÄÅconv8„ÄÅconv9ÊòØ6‰∏™ ratioÔºö{1,2,3,1/2,1/3}Ôºå4‰∏™ÁöÑÊ≤°Êúâ3Âíå1/3 L2 normalization for conv4Ôºö $y_i = \frac{x_i}{\sqrt{\sum_{k=1}^n x_k^2}}$ ‰ΩúÁî®ÊòØÂ∞Ü‰∏çÂêåÂ∞∫Â∫¶ÁöÑÁâπÂæÅÈÉΩÂΩí‰∏ÄÂåñÊàêÊ®°‰∏∫1ÁöÑÂêëÈáè scaleÔºöÂèØ‰ª•ÊòØÂõ∫ÂÆöÂÄºÔºå‰πüÂèØ‰ª•ÊòØÂèØÂ≠¶‰π†ÂèÇÊï∞ ‰∏∫Âï•Âè™ÈíàÂØπconv4Ôºü‰ΩúËÄÖÁöÑÂè¶‰∏ÄÁØápaper(ParseNet)‰∏≠ÂèëÁé∞conv4ÂíåÂÖ∂‰ªñÂ±ÇÁâπÂæÅÁöÑscaleÊòØ‰∏ç‰∏ÄÊ†∑ÁöÑ predictions all default boxes with different scales and aspect ratio from all locations of many feature maps significant imbalance for positive/negative Hard negative mining sort using the highest confidence loss pick the top ones with n/p at most 3:1 faster optimization and a more stable training Data augmentation sample a patch with specific IoU resize ÊÄßË¥® much worse performance on smaller objects, increasing the input size can help improve Data augmentation is crucial, resulting in a 8.8% mAP improvement Atrous is faster, ‰øùÁïôpool5‰∏çÂèòÁöÑËØùÔºåthe result is about the same while the speed is about 20% slower]]></content>
      <tags>
        <tag>ÁõÆÊ†áÊ£ÄÊµã</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pythonÂ§öÁ∫øÁ®ã&Â§öËøõÁ®ã]]></title>
    <url>%2F2020%2F08%2F04%2Fpython%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E5%A4%9A%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[ReferenceÔºö https://www.cnblogs.com/kaituorensheng/p/4465768.html https://zhuanlan.zhihu.com/p/46368084 https://www.runoob.com/python3/python3-multithreading.html ÂêçËØç ËøõÁ®ã(process)ÂíåÁ∫øÁ®ã(thread) cpuÂú®Â§ÑÁêÜ‰ªªÂä°Êó∂ÔºåÊääÊó∂Èó¥ÂàÜÊàêËã•Âπ≤‰∏™Â∞èÊó∂Èó¥ÊÆµÔºåËøô‰∫õÊó∂Èó¥ÊÆµÂæàÂ∞èÁöÑÔºåÁ≥ªÁªü‰∏≠ÊúâÂæàÂ§öËøõÁ®ãÔºåÊØè‰∏™ËøõÁ®ã‰∏≠ÂèàÂåÖÂê´ÂæàÂ§öÁ∫øÁ®ãÔºåÂú®Âêå‰∏ÄÊó∂Èó¥ÊÆµ ÂÜÖÔºåÁîµËÑëCPUÂè™ËÉΩÂ§ÑÁêÜ‰∏Ä‰∏™Á∫øÁ®ãÔºå‰∏ã‰∏Ä‰∏™Êó∂Èó¥ÊÆµÔºåÂèØËÉΩÂèàÂéªÊâßË°åÂà´ÁöÑÁ∫øÁ®ã‰∫ÜÔºàÊó∂Èó¥ÁâáËΩÆËΩ¨Ôºå‰ªéËÄåÂÆûÁé∞‰º™Â§ö‰ªªÂä°ÔºâÔºåÂÖ∑‰ΩìÈ°∫Â∫èÂèñÂÜ≥‰∫éÂÖ∂Ë∞ÉÂ∫¶ÈÄªËæë Â§öÊ†∏cpuÂèØ‰ª•ÂÆûÁé∞ÁúüÊ≠£ÁöÑÂπ∂Ë°åÔºåÂêå‰∏Ä‰∏™Êó∂ÂàªÊØè‰∏™cpu‰∏äÈÉΩÂèØ‰ª•Ë∑ë‰∏Ä‰∏™‰ªªÂä° Â§öËøõÁ®ãÔºöÊØè‰∏™ËøõÁ®ãÂàÜÂà´ÊâßË°åÊåáÂÆö‰ªªÂä°ÔºåËøõÁ®ãÈó¥‰∫íÁõ∏Áã¨Á´ãÔºåÊØè‰∏™Êó∂ÂàªÂπ∂Ë°åÁöÑÂÆûÈôÖËøõÁ®ãÊï∞ÂèñÂÜ≥‰∫écpuÊï∞Èáè Â§öÁ∫øÁ®ãÔºöÂçï‰∏™cpuÂêå‰∏ÄÊó∂ÂàªÂè™ËÉΩÂ§ÑÁêÜ‰∏Ä‰∏™Á∫øÁ®ãÔºå‰∏Ä‰∏™‰ªªÂä°ÂèØËÉΩÁî±Â§ö‰∏™Â∑•‰∫∫Êù•ÂÆåÊàêÔºåÂ∑•‰∫∫‰ª¨Áõ∏‰∫íÂçèÂêåÔºåËøôÂàôÊòØÂ§öÁ∫øÁ®ã pythonÁöÑÂ§öËøõÁ®ãÔºömultiprocessÊ®°Âùó pythonÁöÑÂ§öÁ∫øÁ®ãÔºöthreadingÊ®°Âùó ÊØè‰∏™ËøõÁ®ãÂú®ÊâßË°åËøáÁ®ã‰∏≠Êã•ÊúâÁã¨Á´ãÁöÑÂÜÖÂ≠òÂçïÂÖÉÔºåËÄå‰∏Ä‰∏™ËøõÁ®ãÁöÑÂ§ö‰∏™Á∫øÁ®ãÂú®ÊâßË°åËøáÁ®ã‰∏≠ÂÖ±‰∫´ÂÜÖÂ≠ò„ÄÇ Â§öËøõÁ®ãmultiprocess ÊØçËøõÁ®ãÔºöÂΩìÊàë‰ª¨ÊâßË°å‰∏Ä‰∏™pythonËÑöÊú¨Ôºåif main‰∏ãÈù¢ÂÆûÈôÖËøêË°åÁöÑ‰∏ª‰ΩìÂ∞±ÊòØÊØçËøõÁ®ã Â≠êËøõÁ®ãÔºöÊàë‰ª¨‰ΩøÁî®multiprocessÊòæÂºèÂàõÂª∫ÁöÑËøõÁ®ãÔºåÈÉΩÊòØÂ≠êËøõÁ®ã join()ÊñπÊ≥ïÔºöÁî®Êù•ËÆ©ÊØçËøõÁ®ãÈòªÂ°ûÔºåÁ≠âÂæÖÊâÄÊúâÂ≠êËøõÁ®ãÊâßË°åÂÆåÊàêÂÜçÁªìÊùü ‰ΩøÁî®multiprocessÁöÑÂ§öËøõÁ®ãÔºåÂèØ‰ª•ÈÄöËøáprocessÊñπÊ≥ïÂíåpoolÊñπÊ≥ï processÊñπÊ≥ïÔºöÈÄÇÁî®ËøõÁ®ãËæÉÂ∞ëÊó∂ÂÄôÔºåÊó†Ê≥ïÊâπÈáèÂºÄÂêØ/ÂÖ≥Èó≠ poolÊñπÊ≥ïÔºöÊâπÈáèÁÆ°ÁêÜ ÂèÇÊï∞ÔºöËæìÂÖ•ÂèÇÊï∞ÈÉΩÂ∑Æ‰∏çÂ§öÔºåÁ¨¨‰∏Ä‰∏™ÊòØË¶ÅÊâßË°åÁöÑÂáΩÊï∞ÊñπÊ≥ïtarget/funcÔºåÁ¨¨‰∫å‰∏™ÊòØËæìÂÖ•ÂèÇÊï∞args üå∞ProcessÊñπÊ≥ïÔºö 1234567891011121314151617181920212223from multiprocessing import Processimport osimport timedef long_time_task(i): print('Â≠êËøõÁ®ã: &#123;&#125; - ‰ªªÂä°&#123;&#125;'.format(os.getpid(), i)) time.sleep(2) print("ÁªìÊûú: &#123;&#125;".format(8 ** 20))if __name__=='__main__': print('ÂΩìÂâçÊØçËøõÁ®ã: &#123;&#125;'.format(os.getpid())) start = time.time() p1 = Process(target=long_time_task, args=(1,)) p2 = Process(target=long_time_task, args=(2,)) print('Á≠âÂæÖÊâÄÊúâÂ≠êËøõÁ®ãÂÆåÊàê„ÄÇ') p1.start() p2.start() p1.join() p2.join() end = time.time() print("ÊÄªÂÖ±Áî®Êó∂&#123;&#125;Áßí".format((end - start))) processÊñπÊ≥ï‰ΩøÁî®ProcessÂÆû‰æãÂåñ‰∏Ä‰∏™ËøõÁ®ãÂØπË±°ÔºåÁÑ∂ÂêéË∞ÉÁî®ÂÆÉÁöÑstartÊñπÊ≥ïÂºÄÂêØËøõÁ®ã üå∞PoolÊñπÊ≥ïÔºö 123456789101112131415161718192021222324252627282930from multiprocessing import Pool, cpu_countimport osimport timedef long_time_task(i): print('Â≠êËøõÁ®ã: &#123;&#125; - ‰ªªÂä°&#123;&#125;'.format(os.getpid(), i)) time.sleep(2) print("ÁªìÊûú: &#123;&#125;".format(8 ** 20)) return True # Áî®‰∫éÊºîÁ§∫poolÈÄÇÁî®‰∫éÊúâËøîÂõûÂÄºif __name__=='__main__': print("CPUÂÜÖÊ†∏Êï∞:&#123;&#125;".format(cpu_count())) # 4 print('ÂΩìÂâçÊØçËøõÁ®ã: &#123;&#125;'.format(os.getpid())) start = time.time() p = Pool(4) results = [] for i in range(5): # p.apply_async(long_time_task, args=(i,)) results.append(p.apply_async(long_time_task, args=(i,))) print('Á≠âÂæÖÊâÄÊúâÂ≠êËøõÁ®ãÂÆåÊàê„ÄÇ') p.close() p.join() end = time.time() print("ÊÄªÂÖ±Áî®Êó∂&#123;&#125;Áßí".format((send - start))) # Êü•ÁúãËøîÂõûÂÄº for res in results: print(res.get()) apply_async(func, args=(), kwds={}, callback=None)ÔºöÂêëËøõÁ®ãÊ±†Êèê‰∫§ÈúÄË¶ÅÊâßË°åÁöÑÂáΩÊï∞ÂèäÂèÇÊï∞ÔºåÂêÑ‰∏™ËøõÁ®ãÈááÁî®ÈùûÈòªÂ°ûÔºàÂºÇÊ≠•ÔºâÁöÑË∞ÉÁî®ÊñπÂºèÔºåÂç≥ÊØè‰∏™Â≠êËøõÁ®ãÂè™ÁÆ°ËøêË°åËá™Â∑±ÁöÑÔºå‰∏çÁÆ°ÂÖ∂ÂÆÉËøõÁ®ãÊòØÂê¶Â∑≤ÁªèÂÆåÊàê„ÄÇ close()ÔºöÂÖ≥Èó≠ËøõÁ®ãÊ±†ÔºàpoolÔºâÔºå‰∏çÂÜçÊé•ÂèóÊñ∞ÁöÑ‰ªªÂä°„ÄÇ join()Ôºö‰∏ªËøõÁ®ãÈòªÂ°ûÁ≠âÂæÖÂ≠êËøõÁ®ãÁöÑÈÄÄÂá∫Ôºå Ë∞ÉÁî®join()‰πãÂâçÂøÖÈ°ªÂÖàË∞ÉÁî®close()Êàñterminate()ÊñπÊ≥ïÔºå‰ΩøÂÖ∂‰∏çÂÜçÊé•ÂèóÊñ∞ÁöÑProcess„ÄÇ Â§öÁ∫øÁ®ãthreading pythonÁöÑÂ§öÁ∫øÁ®ãÊòØ‰º™Â§öÁ∫øÁ®ãÔºåÂõ†‰∏∫‰∏ªËøõÁ®ãÂè™Êúâ‰∏Ä‰∏™ÔºåÊâÄ‰ª•Âè™Áî®‰∫ÜÂçïÊ†∏ÔºåÂè™ÊòØÈÄöËøáÁ¢éÁâáÂåñËøõÁ®ã„ÄÅË∞ÉÂ∫¶„ÄÅÂÖ®Â±ÄÈîÅÁ≠âÊìç‰ΩúÔºåcpuÂà©Áî®ÁéáÊèêÂçá‰∫Ü ÊâÄ‰ª•ÊàëÊÉ≥Âπ∂Ë°åÂ§ÑÁêÜÁôæ‰∏áÈáèÁ∫ßÁöÑÊï∞ÊçÆÂÖ•Â∫ìÊìç‰ΩúÊó∂ÔºåÂ§öËøõÁ®ãÁöÑÊïàÁéáÊòéÊòæÈ´ò‰∫éÂ§öÁ∫øÁ®ã „ÄêÈóÆÈ¢ò„Äë‰ªéÊàëËßÇÂØü‰∏äÁúãÂ§öÁ∫øÁ®ãÂü∫Êú¨Â∞±ÊòØ‰∏≤Ë°åÔºüÔºü üå∞threading 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import threadingimport timedef long_time_task(): print('ÂΩìÂ≠êÁ∫øÁ®ã: &#123;&#125;'.format(threading.current_thread().name)) time.sleep(2) print("ÁªìÊûú: &#123;&#125;".format(8 ** 20))if __name__=='__main__': start = time.time() print('ËøôÊòØ‰∏ªÁ∫øÁ®ãÔºö&#123;&#125;'.format(threading.current_thread().name)) for i in range(5): t = threading.Thread(target=long_time_task, args=()) t.setDaemon(True) t.start() t.join() end = time.time() print("ÊÄªÂÖ±Áî®Êó∂&#123;&#125;Áßí".format((end - start))) # ÁªßÊâø&amp;ÊúâËøîÂõûÂÄºÁöÑÂÜôÊ≥ïdef long_time_task(i): time.sleep(2) return 8**20class MyThread(threading.Thread): def __init__(self, func, args , name='', ): threading.Thread.__init__(self) self.func = func self.args = args self.name = name self.result = None def run(self): print('ÂºÄÂßãÂ≠êËøõÁ®ã&#123;&#125;'.format(self.name)) self.result = self.func(self.args[0],) print("ÁªìÊûú: &#123;&#125;".format(self.result)) print('ÁªìÊùüÂ≠êËøõÁ®ã&#123;&#125;'.format(self.name)) def get_result(self): threading.Thread.join(self) # Á≠âÂæÖÁ∫øÁ®ãÊâßË°åÂÆåÊØï return self.resultif __name__=='__main__': start = time.time() threads = [] for i in range(1, 3): t = MyThread(long_time_task, (i,), str(i)) threads.append(t) for t in threads: t.start() for t in threads: t.join() end = time.time() print("ÊÄªÂÖ±Áî®Êó∂&#123;&#125;Áßí".format((end - start))) joinÊñπÊ≥ïÔºöÁ≠âÂæÖÊâÄÊúâËøõÁ®ãÊâßË°åÂÆåÔºå‰∏ªËøõÁ®ãÂÜçÊâßË°åÂÆå setDaemon(True)Ôºö‰∏ªÁ∫øÁ®ãÊâßË°åÂÆåÂ∞±ÈÄÄÂá∫]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IoU]]></title>
    <url>%2F2020%2F08%2F03%2FIoU%2F</url>
    <content type="text"><![CDATA[reference: https://bbs.cvmart.net/articles/1396 IoU IoU = Intersection / Union $Loss_{IoU} = 1 - IoU$ [0,1] Êó†Ê≥ïÁõ¥Êé•‰ºòÂåñÊ≤°ÊúâÈáçÂè†ÁöÑÈÉ®ÂàÜÔºöÂ¶ÇÊûú‰∏§‰∏™Ê°ÜÊ≤°Êúâ‰∫§ÈõÜÔºåIoU=0ÔºåÊ≤°ÊúâÊ¢ØÂ∫¶Âõû‰º†ÔºåÊó†Ê≥ïËøõË°åÂ≠¶‰π†ËÆ≠ÁªÉ Â∞∫Â∫¶‰∏çÊïèÊÑü Êó†Ê≥ïÁ≤æÁ°ÆÁöÑÂèçÊò†‰∏§ËÄÖÁöÑÈáçÂêàË¥®Èáè GIoU(Generalized Intersection over Union) $GIoU = IoU - \frac{|A_c - U|}{|A_c|}$Ôºå$A_c$ÊòØÂåÖÂê´‰∏§‰∏™Ê°ÜÁöÑÊúÄÂ∞èÂ§ñÊé•Ê°Ü $Loss_{GIoU} = 1 - GIoU$ GIoUÂÄæÂêë‰∫éÂÖàÂ¢ûÂ§ßbboxÁöÑÂ§ßÂ∞èÊù•Â¢ûÂ§ß‰∏éGTÁöÑ‰∫§ÈõÜÔºåÁÑ∂ÂêéÈÄöËøáIoUÈ°πÂºïÂØºÊúÄÂ§ßÂåñbboxÁöÑÈáçÂè†Âå∫Âüü [-1,1] ËÉΩÂ§üÂÖ≥Ê≥®Âà∞ÈùûÈáçÂêàÂå∫Âüü Â∞∫Â∫¶‰∏çÊïèÊÑü ‰∏§‰∏™Ê°Ü‰∏∫ÂåÖÂê´ÂÖ≥Á≥ªÊó∂ÔºåÈÄÄÂåñ‰∏∫IoU Â¶ÇÊûú‰πãÈó¥Áî®Êù•ÊõøÊç¢mseÔºåÂâçÊúüÊî∂Êïõ‰ºöÊØîËæÉÊÖ¢ ‰∏ÄËà¨Âú∞ÔºåGIoU loss‰∏çËÉΩÂæàÂ•ΩÂú∞Êî∂ÊïõSOTAÁÆóÊ≥ïÔºåÂèçËÄåÈÄ†Êàê‰∏çÂ•ΩÁöÑÁªìÊûú DIoU (Distance-IoU) $DIoU = IoU - \frac{d^2}{c^2}$ÔºådÊòØ‰∏§‰∏™‰∏≠ÂøÉÁÇπÈó¥ÁöÑÊ¨ßÂºèË∑ùÁ¶ªÔºåcÊòØ‰∏§‰∏™Ê°ÜÁöÑÊúÄÂ∞èÂ§ñÊé•Ê°ÜÁöÑÂØπËßíÁ∫øË∑ùÁ¶ª $Loss_{DIoU} = 1 - DIoU$ * Áõ¥Êé•ÊúÄÂ∞èÂåñ‰∏§‰∏™ÁõÆÊ†áÊ°ÜÁöÑË∑ùÁ¶ªÔºåÊî∂ÊïõÂø´ÂæóÂ§ö * ËÉΩÂ§üÂÖ≥Ê≥®Âà∞ÈùûÈáçÂêàÂå∫Âüü * ÂØπ‰∫éÂåÖÂê´ÂÖ≥Á≥ªÁöÑ‰∏§‰∏™Ê°ÜÔºå‰ªçÊóßÊúâË∑ùÁ¶ªÊçüÂ§±Ôºå‰∏ç‰ºöÈÄÄÂåñ‰∏∫IoU * ÂèØ‰ª•ÊõøÊç¢NMS‰∏≠ÁöÑIoUÔºöÂéüÂßãÁöÑIoU‰ªÖËÄÉËôë‰∫ÜÈáçÂè†Âå∫ÂüüÔºåÂØπÂåÖÂê´ÁöÑÊÉÖÂÜµÊ≤°ÊúâÂæàÂ•ΩÁöÑÂ§ÑÁêÜ $$ score = score\text{ if }IoU - dis(box_{max}, box)&gt;\epsilon \text{, else } 0 $$ * Ê≤°ÊúâËÄÉËôëÂΩ¢Áä∂ÔºàÈïøÂÆΩÊØîÔºâ CIoU (Complete-IoU) $CIoU = IoU - \frac{d^2}{c^2}-av$ÔºåÂú®DIoUÁöÑÂü∫Á°Ä‰∏äÊñ∞Â¢û‰∫ÜÊÉ©ÁΩöÈ°πavÔºåaÊòØÊùÉÈáçÁ≥ªÊï∞ÔºåvÁî®Êù•ËØÑ‰ª∑ÈïøÂÆΩÊØîÔºö v = \frac{4}{\pi ^ 2}(arctan\frac{w^{gt}}{h^{gt}}-arctan\frac{w}{h})^2\\ a = \frac{v}{1-IoU+v} $Loss_{CIoU} = 1 - CIoU$ vÁöÑÊ¢ØÂ∫¶‰∏≠Êúâ$\frac{1}{w^2+h^2}$ÔºåÈïøÂÆΩÂú®[0,1]‰πãÈó¥ÔºåÂèØËÉΩÂæàÂ∞èÔºå‰ºöÂØºËá¥Ê¢ØÂ∫¶ÁàÜÁÇ∏ÔºåÁî®ÁöÑÊó∂ÂÄô clamp‰∏Ä‰∏ã‰∏ä‰∏ãÈôê ÂàÜÊØç‰∏≠ÁöÑ$w^2+h^2$ÊõøÊç¢Êàê1 \frac{\partial v }{\partial w} = \frac{8}{\pi ^ 2}(arctan\frac{w^{gt}}{h^{gt}}-arctan\frac{w}{h})\frac{h}{w^2+h^2}\\ \frac{\partial v }{\partial w} = \frac{8}{\pi ^ 2}(arctan\frac{w^{gt}}{h^{gt}}-arctan\frac{w}{h})\frac{w}{w^2+h^2}]]></content>
      <tags>
        <tag>ÁõÆÊ†áÊ£ÄÊµãÔºåloss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YOLOACT]]></title>
    <url>%2F2020%2F07%2F17%2FYOLOACT%2F</url>
    <content type="text"><![CDATA[[YOLACT] Real-time Instance SegmentationÔºö33 FPS/30 mAP [YOLACT++] Better Real-time Instance SegmentationÔºö33.5 FPS/34.1 mAP YOLACT: Real-time Instance Segmentation Âä®Êú∫ create a real-time instance segmentation base on fast, one-stage detection model forgoes an explicit localization step (e.g., feature repooling) doesn‚Äôt depend on repooling (RoI Pooling) produces very high-quality masks set two parallel subtasks prototypes‚Äî‚Äîconv mask coefficients‚Äî‚Äîfc ‰πãÂêéÂ∞ÜÊ®°ÊùømaskÂíåÂÆû‰æãmaskÁ≥ªÊï∞ËøõË°åÁ∫øÊÄßÁªÑÂêàÊù•Ëé∑ÂæóÂÆû‰æãÁöÑmask ‚Äòprototypes‚Äô: vocabulary fully-convolutional localization is still translation variant Fast NMS ËÆ∫ÁÇπ State-of-the-art approaches to instance segmentation like Mask R- CNN and FCIS directly build off of advances in object detection like Faster R-CNNand R-FCN focus primarily on performance over speed these methods ‚Äúre-pool‚Äù features in some bounding box region inherently sequential therefore difficult to accelerate One-stage instance segmentation methods generate position sensitive maps still require repooling or other non-trivial computations prototypes related works use prototypes to represent features (Bag of Feature) we use them to assemble masks for instance segmentation we learn prototypes that are specific to each image, rather than global prototypes shared across the entire dataset Bag of Feature BOFÂÅáËÆæÂõæÂÉèÁõ∏ÂΩì‰∫é‰∏Ä‰∏™ÊñáÊú¨ÔºåÂõæÂÉè‰∏≠ÁöÑ‰∏çÂêåÂ±ÄÈÉ®Âå∫ÂüüÊàñÁâπÂæÅÂèØ‰ª•Áúã‰ΩúÊòØÊûÑÊàêÂõæÂÉèÁöÑËØçÊ±á(codebook) ÊâÄÊúâÁöÑÊ†∑Êú¨ÂÖ±‰∫´‰∏Ä‰ªΩËØçÊ±áÊú¨ÔºåÈíàÂØπÊØè‰∏™ÂõæÂÉèÔºåÁªüËÆ°ÊØè‰∏™ÂçïËØçÁöÑÈ¢ëÊ¨°ÔºåÂç≥ÂèØÂæóÂà∞ÂõæÁâáÁöÑÁâπÂæÅÂêëÈáè ÊñπÊ≥ï parallel tasks The first branch uses an FCN to produce a set of image-sized ‚Äúprototype masks‚Äù that do not depend on any one instance. The second adds an extra head to the object detection branch to predict a vector of ‚Äúmask coefficients‚Äù for each anchor that encode an instance‚Äôs rep- resentation in the prototype space. linearly combining Rationale masks are spatially coherentÔºömashÊòØÁ©∫Èó¥Áõ∏ÂÖ≥ÁöÑÔºåÁõ∏ÈÇªÂÉèÁ¥†ÂæàÂèØËÉΩÊòØ‰∏ÄÁ±ª Âç∑ÁßØÂ±ÇËÉΩÂ§üÂà©Áî®Âà∞ËøôÁßçÁ©∫Èó¥Áõ∏ÂÖ≥ÊÄßÔºå‰ΩÜÊòØfcÂ±Ç‰∏çËÉΩ ËÄåone-stageÊ£ÄÊµãÂô®ÁöÑÊ£ÄÊµãÂ§¥ÈÄöÂ∏∏ÊòØfcÂ±ÇÔºüÔºü making use of fc layers, which are good at producing semantic vectors and conv layers, which are good at producing spatially coherent masks Prototype Âú®backbone feature layer P3‰∏äÊé•‰∏Ä‰∏™FCN taking protonet from deeper backbone features produces more robust masks higher resolution prototypes result in both higher quality masks and better performance on smaller objects upsampleÂà∞x4ÁöÑÂ∞∫Â∫¶to increase performance on small objects headÂåÖÂê´k‰∏™channels Ê¢ØÂ∫¶Âõû‰º†Êù•Ê∫ê‰∫éÊúÄÁªàÁöÑfinal assembled maskÔºå‰∏çÊòØÂΩìÂâçËøô‰∏™Â§¥ unboundedÔºöReLU or no nonlinearity We choose ReLU for more interpretable prototypes Mask Coefficients a third branch in parallel with detection heads nonlinearityÔºöË¶ÅÊúâÊ≠£Ë¥üÔºåÊâÄ‰ª•tanh Mask Assembly linear combination + sigmoid: $M=\sigma(PC^T)$ loss cls lossÔºöw=1, Âíåssd‰∏ÄÊ†∑Ôºåc+1 softmax box reg lossÔºöw=1.5, Âíåssd‰∏ÄÊ†∑Ôºåsmooth-L1 mask lossÔºöw=6.125Ôºå BCE crop mask evalÔºöÁî®predict boxÂéªcrop trainÔºöÁî®gt boxÂéªcropÔºåÂêåÊó∂ËøòË¶ÅÁªômask lossÈô§‰ª•gt boxÁöÑÈù¢ÁßØÔºåto preserve small objects Emergent Behavior ‰∏çcrop‰πüËÉΩÂàÜÂâ≤‰∏≠Â§ßÁõÆÊ†áÔºö YOLACT learns how to localize instances on its own via different activations in its prototypes ËÄå‰∏çÊòØÈù†ÂÆö‰ΩçÁªìÊûú translation variant the consistent rim of padding in modern FCNs like ResNet gives the network the ability to tell how far away from the image‚Äôs edge a pixel isÔºåÊâÄ‰ª•Áî®‰∏ÄÂº†Á∫ØËâ≤ÁöÑÂõæËÉΩÂ§üÁúãÂá∫kernelÂÆûÈôÖhighlightÁöÑÊòØÂì™ÈÉ®ÂàÜÁâπÂæÅ Âêå‰∏ÄÁßçkernelÔºåÂêå‰∏ÄÁßç‰∫îËßíÊòüÔºåÂú®ÁîªÈù¢‰∏çÂêå‰ΩçÁΩÆÔºåÂØπÂ∫îÁöÑÂìçÂ∫îÂÄºÊòØ‰∏çÂêåÁöÑÔºåËØ¥ÊòéfcnÊòØËÉΩÂ§üÊèêÂèñÁâ©‰Ωì‰ΩçÁΩÆËøôÊ†∑ÁöÑËØ≠‰πâ‰ø°ÊÅØÁöÑ prototypes are compressibleÔºö Â¢ûÂä†Ê®°ÁâàÊï∞ÁõÆÂèçËÄå‰∏çÂ§™ÊúâÊïàÔºåbecause predicting coefficients is difficultÔºå the network has to play a balancing act to produce the right coef- ficients, and adding more prototypes makes this harder, We choose 32 for its mix of performance and speed Network speed as well as feature richness backboneÂèÇËÄÉRetinaNetÔºåResNet-101 + FPN 550x550 inputÔºåresize ÂéªÊéâP2Ôºåadd P6&amp;P7 3 anchors per levelÔºå[1, 1/2, 2] P3ÁöÑanchorÂ∞∫ÂØ∏ÊòØ24x24ÔºåÊé•‰∏ãÊù•ÊØèÂ±Çdouble the scale Ê£ÄÊµãÂ§¥Ôºöshared conv+parallel conv OHEM single GPUÔºöbatch size 8 using ImageNet weightsÔºåno extra bn layers Fast NMS ÊûÑÈÄ†cxnxnÁöÑÁü©ÈòµÔºåc‰ª£Ë°®ÊØè‰∏™class ÁÑ∂ÂêéÊêûÊàê‰∏ä‰∏âËßíÔºåÊ±Çcolumn-wise max ÂÜçIoU threshold 15.0 ms faster with a performance loss of 0.3 mAP Semantic Segmentation Loss using modules not executed at test time P3‰∏ä1x1 convÔºåsigmoid and c channels w=1 +0.4 mAP boost YOLACT++: Better Real-time Instance Segmentation]]></content>
      <tags>
        <tag>ÂÆû‰æãÂàÜÂâ≤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cornerNet]]></title>
    <url>%2F2020%2F07%2F17%2FcornerNet%2F</url>
    <content type="text"><![CDATA[CornerNet: Detecting Objects as Paired Keypoints Âä®Êú∫ corner formulation top-left corner bottom-right corner anchor-free corner pooling no multi-scale ËÆ∫ÁÇπ anchor box drawbacks huge set of anchors boxes to ensure sufficient overlapÔºåcause huge imbalance hyperparameters and design choices cornerNet detect and group heatmap to predict corners ‰ªéÊï∞Â≠¶Ë°®Ëææ‰∏äÁúãÔºåÂÖ®Âõæwh‰∏™tl cornerÔºåwh‰∏™bt cornerÔºåÂèØ‰ª•Ë°®Ëææwwhh‰∏™Ê°Ü anchor-basedÔºåÂÖ®Âõæwh‰∏™‰∏≠ÂøÉÁÇπÔºå9‰∏™anchor sizeÔºåÂè™ËÉΩË°®ËææÊúâÈôêÁöÑÊ°ÜÔºå‰∏îÂèØËÉΩmatch‰∏ç‰∏ä embeddings to group pairs of corners corner pooling better localize corners which are usually out of the foreground modifid hourglass architecture add our novel variant of focal loss ÊñπÊ≥ï two prediction modules heatmaps C channels, C for number of categories binary mask each corner has only one ground-truth positive penalty the neighbored negatives within a radius that still hold high iou (0.3 iou) determine the radius penalty reduction $=e^{-\frac{x^2+y^2}{2\sigma^2}}$ variant focal loss L_{det} = \frac{-1}{N} \sum^C \sum^H \sum^W \begin{cases} (1-p_{i,j})^\alpha log(p_{i,j}), \ \ if y_{ij}=1\\ (1-y_{ij})^\beta (p_{i,j})^\alpha log(1-p_{i,j}), \ \ otherwise \end{cases} $\alpha=2, \beta=4$ N is the number of gts embeddings associative embedding use 1-dimension embedding pull and push loss on gt positives $L_{pull} = \frac{1}{N} \sum^N [(e_{tk}-e_k)^2 + (e_{bk}-e_k)^2]$ $L_{push} = \frac{1}{N(N-1)} \sum_j^N\sum_{k\neq j}^N max(0, \Delta -|e_k-e_j|)$ $e_k$ is the average of $e_{tk}$ and $e{bk}$ $\Delta$ = 1 offsets ‰ªéheatmap resolution remappingÂà∞origin resolutionÂ≠òÂú®Á≤æÂ∫¶ÊçüÂ§± o_k = Ôºà\frac{x_k}{n} - \lfloor \frac{x_k}{n} \rfloorÔºå \frac{y_k}{n} - \lfloor \frac{y_k}{n} \rfloorÔºâ greatly affect the IoU of small bounding boxes shared among all categories smooth L1 loss on gt positives $$ L_{off} = \frac{1}{N} \sum^N SmoothL1(o_k, \hat o_k) $$ corner pooling top-left pooling layerÔºö * ‰ªéÂΩìÂâçÁÇπ(i,j)ÂºÄÂßãÔºå * Âêë‰∏ãelementwise maxÊâÄÊúâfeature vecorÔºåÂæóÂà∞$t_{i,j}$ * ÂêëÂè≥elementwise maxÊâÄÊúâfeature vecorÔºåÂæóÂà∞$l_{i,j}$ * ÊúÄÂêé‰∏§‰∏™vectorÁõ∏Âä† bottom-right cornerÔºöÂêëÂ∑¶Âêë‰∏ä Hourglass Network hourglass modules series of convolution and max pooling layers series of upsampling and convolution layers skip layers multiple hourglass modules stackedÔºöreprocess the features to capture higher-level information intermediate supervision Â∏∏ËßÑÁöÑ‰∏≠ÁªßÁõëÁù£Ôºö ‰∏ã‰∏ÄÁ∫ßhourglass moduleÁöÑËæìÂÖ•ÂåÖÊã¨‰∏â‰∏™ÈÉ®ÂàÜ Ââç‰∏ÄÁ∫ßËæìÂÖ• Ââç‰∏ÄÁ∫ßËæìÂá∫ ‰∏≠ÁªßÁõëÁù£ÁöÑËæìÂá∫ Êú¨Êñá‰ΩøÁî®‰∫Ü‰∏≠ÁªßÁõëÁù£Ôºå‰ΩÜÊòØÊ≤°ÊääËøô‰∏™ÁªìÊûúÂä†ÂõûÂéª hourglass2 inputÔºö1x1 conv-BN to both input and output of hourglass1 + add + relu Our backbone 2 hourglasses 5 times downsamp with channels [256,384,384,384,512] use stride2 conv instead of max-pooling upsampÔºö2 residual modules + nearest neighbor upsampling skip connection: 2 residual modulesÔºåadd mid connection: 4 residual modules stem: 7x7 stride2, ch128 + residual stride2, ch256 hourglass2 inputÔºö1x1 conv-BN to both input and output of hourglass1 + add + relu ÂÆûÈ™å training details randomly initialized, no pretrained biasÔºöset the biases in the convolution layers that predict the corner heatmaps inputÔºö511x511 outputÔºö128x128 apply PCA to the input image full lossÔºö$L = L_{det} + \alpha L_{pull} + \beta L_{push} + \gamma L_{off}$ ÈÖçÂØπlossÔºö$\alpha=\beta=0.1$ offset lossÔºö$\gamma=1$ batch size = 49 = 4+5x9 test details NMSÔºö3x3 max pooling on heatmaps pickÔºötop100 top-left corners &amp; top100 bottom-right corners filter pairsÔºö L1 distance greater than 0.5 from different categories fusionÔºöcombine the detections from the original and flipped images + soft nms Ablation Study corner pooling is especially helpful for medium and large objects penalty reduction especially benefits medium and large objects CornerNet achieves a much higher AP at 0.9 IoU than other detectorsÔºöÊõ¥ÊúâËÉΩÂäõÁîüÊàêÈ´òË¥®ÈáèÊ°Ü error analysisÔºöthe main bottleneck is detecting corners CornerNet-Lite: Efficient Keypoint-Based Object Detection Âä®Êú∫ keypoint-based methods detecting and grouping accuary but with processing cost propose CornerNet-Lite CornerNet-SaccadeÔºöattention mechanism CornerNet-SqueezeÔºöa new compact backbone performance ËÆ∫ÁÇπ main drawback of cornerNet inference speed reducing the number of scales or the image resolution cause a large accuracy drop two orthogonal directions reduce the number of pixels to processÔºöCornerNet-Saccade reduce the amount of processing per pixelÔºö CornerNet-Saccade downsized attention map select a subset of crops to examine in high resolution for off-lineÔºöAP of 43.2% at 190ms per image CornerNet-Squeeze inspired by squeezeNet and mobileNet 1x1 convs bottleneck layers depth-wise separable convolution for real-timeÔºöAP of 34.4% at 30ms combined?? CornerNet-Squeeze-Saccade turns out slower and less accurate than CornerNet- Squeeze SaccadesÔºöÊâ´ËßÜ to generate interesting crops RCNNÁ≥ªÂàóÔºösingle-type &amp; single object AutoFocusÔºöadd a branchË∞ÉÁî®faster-RCNNÔºåthus multi-type &amp; mixed-objectsÔºåÊúâsingle branchÊúâmulti branch CornerNet-SaccadeÔºö single-type &amp; multi object crops can be much smaller than number of objects ÊñπÊ≥ï CornerNet-Saccade step1Ôºöobtain possible locations downsizeÔºötwo scalesÔºå255 &amp; 192Ôºåzero-padding predicts 3 attention maps small objectÔºölonger side&lt;32 pixels medium objectÔºö32-96 large objectÔºö&gt;96 so that we can control the zoom-in factorÔºözoom-in more for smaller objects feature mapÔºödifferent scales from the upsampling layers attention mapÔºö3x3 conv-relu + 1x1 conv-sigmoid process locations where scores &gt; 0.3 step2Ôºöfiner detection zoom-in scalesÔºö4Ôºå2Ôºå1 for small„ÄÅmedium„ÄÅlarge objects apply CornerNet-Saccade on the ROI 255x255 window centered at the location step3ÔºöNMS soft-nms remove the bounding boxes which touch the crop boundary CornerNet-Saccade uses the same network for attention maps and bounding boxes Âú®Á¨¨‰∏ÄÊ≠•ÁöÑÊó∂ÂÄôÔºåÂØπ‰∏Ä‰∫õÂ§ßÁõÆÊ†áÂ∑≤ÁªèÊúâ‰∫ÜÊ£ÄÊµãÊ°Ü ‰πüË¶Åzoom-inÔºåÁü´Ê≠£‰∏Ä‰∏ã efficiency regions/croped imagesÈÉΩÊòØprocessed in batch/parallel resize/cropÊìç‰ΩúÂú®GPU‰∏≠ÂÆûÁé∞ suppress redundant regions using a NMS-similar policy before prediction new hourglass backbone 3 hourglass moduleÔºådepth 54 downsize twice before hourglass modules downsize 3 times in each moduleÔºåwith channels [384,384,512] one residual in both encoding path &amp; skip connection mid connectionÔºöone residualÔºåwith channels 512 CornerNet-Squeeze to replace the heavy hourglass104 use fire module to replace residuals downsizes 3 times before hourglass modules downsize 4 times in each module replace the 3x3 conv in prediction head with 1x1 conv replace the nearest neighboor upsampling with 4x4 transpose conv]]></content>
      <tags>
        <tag>ÁõÆÊ†áÊ£ÄÊµãÔºåanchor-free</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SOLO]]></title>
    <url>%2F2020%2F07%2F17%2FSOLO%2F</url>
    <content type="text"><![CDATA[[SOLO] SOLO: Segmenting Objects by LocationsÔºöÂ≠óËäÇÔºåÁõÆÂâçÁªùÂ§ßÂ§öÊï∞ÊñπÊ≥ïÂÆû‰æãÂàÜÂâ≤ÁöÑÁªìÊûÑÈÉΩÊòØÈó¥Êé•ÂæóÂà∞‚Äî‚ÄîÊ£ÄÊµãÊ°ÜÂÜÖËØ≠‰πâÂàÜÂâ≤ÔºèÂÖ®ÂõæËØ≠‰πâÂàÜÂâ≤ËÅöÁ±ªÔºå‰∏ªË¶ÅÂéüÂõ†ÊòØformulation issueÔºåÂæàÈöæÊääÂÆû‰æãÂàÜÂâ≤ÂÆö‰πâÊàê‰∏Ä‰∏™ÁªìÊûÑÂåñÁöÑÈóÆÈ¢ò [SOLOv2] SOLOv2: Dynamic, Faster and StrongerÔºöbest 41.7% AP SOLO: Segmenting Objects by Locations Âä®Êú∫ challengingÔºöarbitrary number of instances form the task into a classification-solvable problem direct &amp; end-to-end &amp; one-stage &amp; using mask annotations solely on par accuracy with Mask R-CNN outperforming recent single-shot instance segmenters ËÆ∫ÁÇπ formulating Objects in an image belong to a fixed set of semantic categories‚Äî‚Äîsemantic segmentation can be easily formulated as a dense per-pixel classification problem the number of instances varies existing methods Ê£ÄÊµãÔºèËÅöÁ±ªÔºöstep-wise and indirect Á¥ØÁßØËØØÂ∑Æ core idea in most cases two instances in an image either have different center locations or have different object sizes locationÔºö think image as a divided grid of cells an object instance is assigned to one of the grid cells as its center location category encode center location categories as the channel axis size FPN assign objects of different sizes to different levels of feature maps SOLO converts coordinate regression into classification by discrete quantization One feat of doing so is the avoidance of heuristic coordination normalization and log-transformation typically used in detectors„ÄêÔºüÔºüÔºü‰∏çÊáÇËøôÂè•ËØùÊÉ≥Ë°®ËææÂï•„Äë ÊñπÊ≥ï problem formulation divided grids simultaneous task category-aware prediction instance-aware mask generation category prediction predict instance for each gridÔºö$SSC$ grid sizeÔºö$S*S$ number of classesÔºö$C$ based on the assumption that each cell must belong to one individual instance C-dim vec indicates the class probability for each object instance in each grid mask prediction predict instance mask for each positive cellÔºö$HWS^2$ the channel corresponding to the location position sensitiveÔºöÂõ†‰∏∫ÊØè‰∏™grid‰∏≠ÂàÜÂâ≤ÁöÑmaskÊòØË¶ÅÊò†Â∞ÑÂà∞ÂØπÂ∫îÁöÑchannelÁöÑÔºåÂõ†Ê≠§Êàë‰ª¨Â∏åÊúõÁâπÂæÅÂõæÊòØspatially variant ËÆ©ÁâπÂæÅÂõæspatially variantÁöÑÊúÄÁõ¥Êé•ÂäûÊ≥ïÂ∞±ÊòØÂä†‰∏ÄÁª¥spatially variantÁöÑ‰ø°ÊÅØ inspired by CoordConvÔºöÊ∑ªÂä†‰∏§‰∏™ÈÄöÈÅìÔºånormed_xÂíånormed_yÔºå[-1,1] original feature tensor $HWD$ becomes $HW(D+2)$ final results gather category prediction &amp; mask prediction NMS network backboneÔºöresnet FCNÔºö256-d headsÔºöweights are shared across different levels except for the last 1x1 conv learning positive gridÔºöfalls into a center region maskÔºömask center $(c_x, c_y)$Ôºåmask size $(h,w)$ center regionÔºö$(c_x,c_y,\epsilon w, \epsilon h)$Ôºåset $\epsilon = 0.2$ lossÔºö$L = L_{cate} + \lambda L_{seg}$ cate lossÔºöfocal loss seg lossÔºödiceÔºå$L_{mask} = \frac{1}{N_{pos}}\sum_k 1_{p^_{i,j}&gt;0} dice(m_k, m^_k) $ÔºåÂ∏¶ÊòüÂè∑ÁöÑÊòØgroud truth inference use a confidence threshold of 0.1 to filter out low spacial predictions use a threshold of 0.5 to binary the soft masks select the top 500 scoring masks NMS Only one instance will be activated at each grid and one in- stance may be predicted by multiple adjacent mask channels keep top 100 ÂÆûÈ™å grid number ÈÄÇÂΩìÂ¢ûÂä†ÊúâÊèêÂçáÔºå‰∏ªË¶ÅÊèêÂçáËøòÊòØÂú®FPN fpn ‰∫î‰∏™FPN pyramids Â§ßÁâπÂæÅÂõæÔºåÂ∞èÊÑüÂèóÈáéÔºåÁî®Êù•ÂàÜÈÖçÂ∞èÁõÆÊ†áÔºågridÊï∞ÈáèË¶ÅÂ¢ûÂ§ß feature alignment Âú®ÂàÜÁ±ªbranchÔºå$HW$ÁâπÂæÅÂõæË¶ÅËΩ¨Êç¢Êàê$SS$ÁöÑÁâπÂæÅÂõæ interpolationÔºöbilinear interpolating adaptive-poolÔºöapply a 2D adaptive max-pool region-grid- interpolationÔºöÂØπÊØè‰∏™cellÔºåÈááÊ†∑Â§ö‰∏™ÁÇπÂÅöÂèåÁ∫øÊÄßÊèíÂÄºÔºåÁÑ∂ÂêéÂèñÂπ≥Âùá is no noticeable performance gap between these variants ÔºàÂèØËÉΩÂõ†‰∏∫ÊúÄÁªàÊòØÂàÜÁ±ª‰ªªÂä° head depth 4-7ÊúâÊ∂®ÁÇπ ÊâÄ‰ª•Êú¨ÊñáÈÄâ‰∫Ü7 decoupled SOLO mask branchÈ¢ÑÊµãÁöÑchannelÊï∞ÊòØ$S^2$ÔºåÂÖ∂‰∏≠Â§ßÈÉ®ÂàÜchannelÂÖ∂ÂÆûÊòØÊ≤°ÊúâË¥°ÁåÆÁöÑÔºåÁ©∫Âç†ÂÜÖÂ≠ò prediction is somewhat redundant as in most cases the objects are located sparsely in the image element-wise multiplication ÂÆûÈ™å‰∏ãÊù• achieves the same performance efficient and equivalent variant SOLOv2: Dynamic, Faster and Stronger Âä®Êú∫ take one step further on the mask head dynamically learning the mask head decoupled into mask kernel branch and mask feature branch propose Matrix NMS faster &amp; better results try object detection and panoptic segmentation ËÆ∫ÁÇπ SOLO develop pure instance segmentation instance segmentation requires instance-level and pixel-level predictions simultaneously most existing instance segmentation methods build on the top of bounding boxes SOLO develop pure instance segmentation SOLOv2 improve SOLO mask learningÔºödynamic scheme mask NMSÔºöparallel matrix operationsÔºåoutperforms Fast NMS Dynamic Convolutions STNÔºöadaptively transform feature maps conditioned on the input Deformable Convolutional NetworksÔºölearn location ÊñπÊ≥ï revisit SOLOv1 redundant mask prediction decouple dynamicÔºödynamically pick the valid ones from predicted $s^2$ classifiers and perform the convolution SOLOv2 dynamic mask segmentation head mask kernel branch mask feature branch mask kernel branch prediction headsÔºö4 convs + 1 final convÔºåshared across scale no activation on the output concat normalized coordinates in two additional input channels at start ouputs D-dims kernel weights for each gridÔºöe.g. for 3x3 conv with E input channels, outputs $SS9E$ mask feature branch predict instance-aware featureÔºö$F \in R^{HWE}$ unified and high-resolution mask featureÔºöÂè™ËæìÂá∫‰∏Ä‰∏™Â∞∫Â∫¶ÁöÑÁâπÂæÅÂõæÔºåencoded x32 feature with coordinates info we feed normalized pixel coordinates to the deepest FPN level (at 1/32 scale) repeated „Äê3x3 conv, group norm, ReLU, 2x bilinear upsampling„Äë element-wise sum last layerÔºö1x1 conv, group norm, ReLU instance mask mask feature branch conved by the mask kernel branchÔºöfinal conv $HWS^2$ mask NMS train lossÔºö$L = L_{cate} + \lambda L_{seg}$ cate lossÔºöfocal loss seg lossÔºödiceÔºå$L_{mask} = \frac{1}{N_{pos}}\sum_k 1_{p^_{i,j}&gt;0} dice(m_k, m^_k) $ÔºåÂ∏¶ÊòüÂè∑ÁöÑÊòØgroud truth inference category scoreÔºöfirst use a confidence threshold of 0.1 to filter out predictions with low confidence mask branchÔºörun convolution based on the filtered category map sigmoid use a threshold of 0.5 to convert predicted soft masks to binary masks Matrix NMS Matrix NMS decremented functions linearÔºö$f(iou_{i,j}=1-iou_{i,j})$ gaussianÔºö$f(iou_{i,j}=exp(-\frac{iou_{i,j}^2}{\sigma})$ the most overlapped prediction for $m_i$Ôºömax iou $f(iou_{*,i}) = min_{s_k}f(iou_{k,i})$ decay factor $decay_i = min \frac{f(iou_{i,j})}{f(iou_{*,i})}$]]></content>
      <tags>
        <tag>ÂÆû‰æãÂàÜÂâ≤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[polarMask]]></title>
    <url>%2F2020%2F06%2F29%2FpolarMask%2F</url>
    <content type="text"><![CDATA[PolarMask: Single Shot Instance Segmentation with Polar Representation Âä®Êú∫ instance segmentation anchor-free single-shot modified on FCOS ËÆ∫ÁÇπ two-stage methods FCIS, Mask R-CNN bounding box detection then semantic segmentation within each box single-shot method formulate the task as instance center classification and dense distance regression in a polar coordinate FCOS can be regarded as a special case that the contours has only 4 directions this paper two parallel taskÔºö instance center classification dense distance regression Polar IoU Loss can largely ease the optimization and considerably improve the accuary Polar Centerness improves the original idea of ‚ÄúCentreness‚Äù in FCOS, leading to further performance boost ÊñπÊ≥ï architecture back &amp; fpn are the same as FCOS model the instance mask as one center and n rays conclude that mass-center is more advantageous than box center the angle interval is pre-fixed, thus only the length of the rays is to be regressed positive samplesÔºöfalls into 1.5xstrides of the area around the gt mass-centerÔºåthat is 9-16 pixels around gt grid distance regression Â¶ÇÊûú‰∏ÄÊù°Â∞ÑÁ∫ø‰∏äÂ≠òÂú®Â§ö‰∏™‰∫§ÁÇπÔºåÂèñÊúÄÈïøÁöÑ Â¶ÇÊûú‰∏ÄÊù°Â∞ÑÁ∫ø‰∏äÊ≤°Êúâ‰∫§ÁÇπÔºåÂèñÊúÄÂ∞èÂÄº$\epsilon=10^{-6}$ potential issuse of the mask regression branch dense regression task with such as 36 rays, may cause imbalance between regression loss and classification loss n rays are relevant and should be trained as a whole rather than a set of independent values‚Äî-&gt;iou loss inference multiply center-ness with classification to obtain final confidence scores, conf thresh=0.05 take top-1k predictions per fpn level use the smallest bounding boxes to run NMS, nms thresh=0.5 polar centerness to suppress low quality detected centers $polar\ centerness=\sqrt{\frac{min(\{d_1,d_2, ‚Ä¶, d_n\})}{max(\{d_1,d_2, ‚Ä¶, d_n\})}}$ $d_{min}$Âíå$d_{max}$Ë∂äÊé•ËøëÔºåËØ¥Êòé‰∏≠ÂøÉÁÇπË¥®ÈáèË∂äÂ•Ω Experiments show that Polar Centerness improves accuracy especially under stricter localization metrics, such as $AP_{75}$ polar IoU loss polar IoUÔºö$IoU=lim_{N\to\inf}\frac{\sum_{i=1}^N\frac{1}{2} d_{min}^2 \Delta \theta}{\sum_{i=1}^N\frac{1}{2} d_{max}^2 \Delta \theta}$ empirically observe that ÂéªÊéâÂπ≥ÊñπÈ°πÊïàÊûúÊõ¥Â•ΩÔºö$polar\ IoU=\frac{\sum_{i=1}^n d_{min}}{\sum_{i=1}^n d_{max}}$ polar iou lossÔºöbce of polar IoUÔºå$-log(\frac{\sum_{i=1}^n d_{min}}{\sum_{i=1}^n d_{max}})$ advantage differentiable, enable bp regards the regression targets as a whole keep balance with classification loss]]></content>
      <tags>
        <tag>ÂÆû‰æãÂàÜÂâ≤ÔºåÊûÅÂùêÊ†áÔºåone-stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FCOS]]></title>
    <url>%2F2020%2F06%2F23%2FFCOS%2F</url>
    <content type="text"><![CDATA[FCOS: Fully Convolutional One-Stage Object Detection Âä®Êú∫ anchor free proposal free avoids the complicated computation related to anchor boxes calculating overlapping during training avoid all hyper-parameters related to anchor boxes size &amp; shape positiveÔºèignoredÔºènegative leverage as many foreground samples as possible ËÆ∫ÁÇπ anchor-based detectors detection performance is sensitive to anchor settings encounter difficulties in cases with large shape variations hamper the generalization ability of detectors dense proposeÔºöthe excessive number of negative samples aggravates the imbalance involve complicated computationÔºösuch as calculating the IoU with gt boxes FCN-based detector predict a 4D vector plus a class category at each spatial location on a level of feature maps do not work well when applied to overlapped bounding boxes with FPN this ambiguity can be largely eliminated anchor-free detector yolov1Ôºöonly the points near the center are usedÔºålow recall CornerNetÔºöcomplicated post-processing to match the pairs of corners DenseBoxÔºödifficulty in handling overlapping bounding boxes this methos use FPN to deal with ambiguity dense predictÔºöuse all points in a ground truth bounding box to predict the bounding box introduce ‚Äúcenter-ness‚Äù branch to predict the deviation of a pixel to the center of its corresponding bounding box can be used as a RPN in two-stage detectors and can achieve significantly better performance ÊñπÊ≥ï ground truth boxesÔºå$B_i=(x_0, y_0, x_1, y_1, c)$Ôºåcorners + cls anchor-freeÔºöeach location (x,y)Ôºåmap into abs input image (xs+[s/2], ys+[s/2]) positive sampleÔºöif a location falls into any ground-truth box ambiguous sampleÔºölocation falls into multiple gt boxesÔºåchoose the box with minimal area regression targetÔºöl t r b distanceÔºålocation to the four sides cls branch C binary classifiers C-dims vector p focal loss $\frac{1}{N_{pos}} \sum_{x,y}L_{cls}(p_{x,y}, c_{x,y}^*)$ calculate on both positive/negative samples box reg branch 4-dims vector t IOU loss $\frac{1}{N_{pos}} \sum_{x,y}1_{\{c_{x,y}^&gt;0\}}L_{reg}(t_{x,y}, t_{x,y}^)$ calculate on positive samples inference choose the location with p &gt; 0.05 as positive samples two possible issues large stride makes BPR low, which is actually not a problem in FCOS overlaps gt boxes cause ambiguity, which can be greatly resolved with multi-level prediction FPN P3, P4, P5Ôºö1x1 conv from C3, C4, C5, top-down connections P6, P7: stride2 conv from P5, P6 limit the bbox regression for each level $m_i$Ôºömaximum distance for each level if a location‚Äôs gt bbox satifiesÔºö$max(l^,t^,r^,b^)&gt;m_i$ or $max(l^,t^,r^,b^)&lt;m_{i-1}$Ôºåit is set as a negative sampleÔºånot regress at current level objects with different sizes are assigned to different feature levelsÔºölargely alleviate‰∏ÄÈÉ®ÂàÜbox overlappingÈóÆÈ¢ò for other overlapping casesÔºösimply choose the gt box with minimal area sharing heads between different feature levels to regress different size rangeÔºöuse $exp(s_ix)$ trainable scalar $s_i$ slightly improve center-ness low-quality predicted bounding boxes are produced by locations far away from the center of an object predict the ‚Äúcenter-ness‚Äù of a location normalized distance centerness^* = \sqrt {\frac{min(l^*,r^*)}{max(l^*,r^*)}* \frac{min(t^*,b^*)}{max(t^*,b^*)}} sqrt to slow down the decay [0,1] use bce loss when inference center-ness is mutiplied with the class scoreÔºöcan down-weight the scores of bounding boxes far from the center of an object, then filtered out by NMS an alternative of the center-nessÔºöuse of only the central portion of ground-truth bounding box as positive samplesÔºåÂÆûÈ™åËØÅÊòé‰∏§ÁßçÊñπÊ≥ïÁªìÂêàÊïàÊûúÊúÄÂ•Ω architecture two minor differences from the standard RetinaNet use Group Normalization in the newly added convolutional layers except for the last prediction layers use P5 instead of C5 to produce P6&amp;P7 ‚Äã]]></content>
      <tags>
        <tag>ÁõÆÊ†áÊ£ÄÊµãÔºåÂÖ®Âç∑ÁßØÔºåone-stageÔºåcenternessÔºåanchor free</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FCIS]]></title>
    <url>%2F2020%2F06%2F22%2FFCIS%2F</url>
    <content type="text"><![CDATA[Fully Convolutional Instance-aware Semantic Segmentation Âä®Êú∫ instance segmentationÔºö ÂÆû‰æãÂàÜÂâ≤ÊØîËµ∑Ê£ÄÊµãÔºåÈúÄË¶ÅÂæóÂà∞ÁõÆÊ†áÊõ¥Á≤æÁ°ÆÁöÑËæπÁïå‰ø°ÊÅØ ÊØîËµ∑ËØ≠‰πâÂàÜÂâ≤ÔºåÈúÄË¶ÅÂå∫ÂàÜ‰∏çÂêåÁöÑÁâ©‰Ωì detects and segments simultanously FCN + instance mask proposal ËÆ∫ÁÇπ FCNs do not work for the instance-aware semantic segmentation task convolution is translation invariantÔºöÊùÉÂÄºÂÖ±‰∫´Ôºå‰∏Ä‰∏™ÂÉèÁ¥†ÂÄºÂØπÂ∫î‰∏Ä‰∏™ÂìçÂ∫îÂÄºÔºå‰∏é‰ΩçÁΩÆÊó†ÂÖ≥ instance segmentation operates on region level the same pixel can have different semantics in different regions Certain translation-variant property is required prevalent method step1: an FCN is applied on the whole image to generate shared feature maps step2: a pooling layer warps each region of interest into fixed-size per-ROI feature maps step3: use fc layers to convert the per-ROI feature maps to per-ROI masks the translation-variant property is introduced in the fc layer(s) in the last step drawbacks the ROI pooling step losses spatial details the fc layers over-parametrize the task InstanceFCN position-sensitive score maps sliding windows sub-tasks are separated and the solution is not end-to-end blind to the object categoriesÔºöÂâçËÉåÊôØÂàÜÂâ≤ In this work extends InstanceFCN end-to-end fully convolutional operates on box proposals instead of sliding windows per-ROI computation does not involve any warping or resizing operations ÊñπÊ≥ï position-sensitive score map FCN predict a single score map predict each pixel‚Äôs likelihood score of belonging to each category at instance level the same pixel can be foreground on one object but background on another a single score map per-category is insufficient to distinguish these two cases a fully convolutional solution for instance mask proposal k x k evenly partitioned cells of object thus obtain k x k position-sensitive score maps Each score represents ÂΩìÂâçÂÉèÁ¥†Âú®ÂΩìÂâç‰ΩçÁΩÆÔºàscore mapÂú®cells‰∏≠ÁöÑ‰ΩçÁΩÆÔºâ‰∏äÂ±û‰∫éÊüê‰∏™Áâ©‰ΩìÂÆû‰æãÁöÑ‰ººÁÑ∂ÂæóÂàÜ assembling (copy-paste) jointly and simultaneously The same set of score maps are shared for the two sub-tasks For each pixel in a ROI, there are two tasks: detectionÔºöwhether it belongs to an object bounding box segmentationÔºöwhether it is inside an object instance‚Äôs boundary separateÔºötwo 1x1 conv heads fuseÔºöinside and outside high inside score and low outside scoreÔºödetection+, segmentation+ low inside score and high outside scoreÔºödetection+, segmentation- low inside score and low outside scoreÔºödetection-, segmentation- detection score average pooling over all pixels‚Äò likelihoods for each class max(detection score) represent the object segmentation softmax(inside, outside) for each pixel to distinguish fgÔºèbg All the per-ROI components are implemented through convs local weight sharing propertyÔºöa regularization mechanism without involving any feature warping, resizing or fc layers the per-ROI computation cost is negligible architecture ResNet back produce features with 2048 channels a 1x1 conv reduces the dimension to 1024 x16 output strideÔºöconv5 stride is decreased from 2 to 1, the dilation is increased from 1 to 2 head1Ôºöjoint det conf &amp; segmentation 1x1 convÔºågenerates $2k^2(C+1)$ score maps 2 for insideÔºèoutside $k^2$ for $k^2$‰∏™position $(C+1)$ for fgÔºèbg head2Ôºöbbox regression 1x1 convÔºå$4k^2$ channels RPN to generate ROIs inference 300 ROIs pass through the bbox regression obtaining another 300 ROIs pass through joint head to obtain detection score&amp;fg mask for all categories mask votingÔºöÊØè‰∏™ROI (with max det score) Âè™ÂåÖÂê´ÂΩìÂâçÁ±ªÂà´ÁöÑÂâçÊôØÔºåËøòË¶ÅË°•‰∏äÊ°ÜÂÜÖÂÖ∂‰ªñÁ±ªÂà´ËÉåÊôØ for current ROI, find all the ROIs (from the 600) with IoU scores higher than 0.5 their fg masks are averaged per-pixel and weighted by the classification score training ROI positiveÔºènegativeÔºöIoU&gt;0.5 loss softmax detection loss over C+1 categories softmax segmentation loss over the gt fg mask, on positive ROIs bbox regression loss, , on positive ROIs OHEMÔºöamong the 300 proposed ROIs on one image, 128 ROIs with the highest losses are selected to back-propagate their error gradients RPNÔºö 9 anchors sharing feature between FCIS and RPN ÂÆûÈ™å metricÔºömAP FCIS (translation invariant)Ôºö set k=1Ôºåachieve the worst mAP indicating the position sensitive score map is vital for this method back 50-101Ôºöincrease 101-152Ôºösaturate tricks * r]]></content>
      <tags>
        <tag>ÂÆû‰æãÂàÜÂâ≤ÔºåÂÖ®Âç∑ÁßØÔºåÂ∏¶‰ΩçÁΩÆ‰ø°ÊÅØÁöÑscoremap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Â§©Ê±†ËÑäÊü±MRI]]></title>
    <url>%2F2020%2F06%2F11%2F%E5%A4%A9%E6%B1%A0%E8%84%8A%E6%9F%B1MRI%2F</url>
    <content type="text"><![CDATA[MRI T1Âä†ÊùÉÁõ∏‰∏äÈù¢ÂØÜÂ∫¶È´òÁöÑÈ™®Â§¥‰ºöÊØîËæÉ‰∫Æ(Â∞±ÊòØÈ´ò‰ø°Âè∑)ÔºåËøòÊúâËÑÇËÇ™ÂíåÁî≤Áä∂ËÖ∫‰πüÊòØÈ´ò‰ø°Âè∑ÔºåÊ∞¥‰ªΩ‰∏ÄËà¨ÈÉΩÊòØÊó†‰ø°Âè∑Ôºå T2Âä†ÊùÉÁõ∏ÈáåÊ∞¥ÊòØÈ´ò‰ø°Âè∑ÊâÄ‰ª•Ê∞¥ÊØîËæÉ‰∫ÆÔºåÂõ†‰∏∫ÂæàÂ§öÁöÑÁóÖÂèòÊúâÊ∞¥ËÇøÔºåÊâÄ‰ª•T2Âä†ÊùÉÁõ∏ÈÄö‰øóÂèØ‰ª•ËØ¥ÊòØÁúãÁóÖÂèò(ÊØïÁ´üÊØîËæÉÊòéÊòæ)Ôºå ËßÜËßâÁõ¥ËßÇ‰∏äÊù•ÁúãÔºåT1ÁúãËß£ÂâñÔºåT2ÁúãÁóÖÂèò ‚Äî‚ÄîÊÄé‰πàfusion‰∏Ä‰∏™caseÔºàÊ†áÊ≥®Âè™Êúâ‰∏ÄÂº†Ôºâ Êï∞ÊçÆÈõÜ T1„ÄÅT2Áü¢Áä∂‰ΩçÔºåT2ËΩ¥Áä∂‰ΩçÔºå ÂÖ≥ÈîÆÁÇπÔºöÂü∫‰∫éT2Áü¢Áä∂‰ΩçÁöÑ‰∏≠Èó¥Â∏ßÔºå Ê†áÊ≥®ËåÉÂõ¥Ôºö‰ªéËÉ∏12ÔºàT12ÔºâËÖ∞1ÔºàL1ÔºâÈó¥ÁöÑÊ§éÈó¥ÁõòÂºÄÂßãÔºåÂà∞ËÖ∞5ÔºàL5ÔºâÈ™∂1ÔºàS1ÔºâÈó¥ÁöÑÊ§éÈó¥ÁõòÁªìÊùü Á±ªÂà´ÔºöÊ§éÂùóÊúâÁºñÂè∑ÔºàT12Âà∞L5ÔºâÔºåÈó¥ÁõòÈÄöËøá‰∏ä‰∏ãÊ§éÂùóÁöÑÁºñÂè∑Ë°®Á§∫ÔºàT12-L1Âà∞L5-S1Ôºâ ÁóÖÁÅ∂Ôºö * Ê§éÂùóÊúâ‰∏§Á±ªÔºöÊ≠£Â∏∏V1ÂíåÈÄÄË°åÊÄßÁóÖÂèòV2Ôºå * Ê§éÈó¥ÁõòÊúâ7Á±ªÔºöÊ≠£Â∏∏V1ÔºåÈÄÄË°åÊÄßÊîπÂèòV2ÔºåÂº•Êº´ÊÄßËÜ®Âá∫ÔºåÈùûÂØπÁß∞ÊÄßËÜ®Âá∫ÔºåÁ™ÅÂá∫ÔºåËÑ±Âá∫ÔºåÁñùÂá∫V7 jsonÁªìÊûÑÔºö uidÔºådimÔºåspacingÁ≠â‰∏Ä‰∫õheader info annotationÔºö sliceÔºöÈöæÈÅì‰∏çÊòØT2Áü¢Áä∂‰ΩçÁöÑ‰∏≠Èó¥Â∏ßÂêóÔºü pointÔºöÂÖ≥ÈîÆÁÇπÂùêÊ†áÔºåÁóÖÁÅ∂Á±ªÂà´ÔºåÂÖ≥ÈîÆÁÇπÁ±ªÂà´ ËØÑ‰º∞ÊåáÊ†á distance&lt;8mm TPÔºöÂ§ö‰∏™ÂëΩ‰∏≠ÂèñÊúÄËøëÁöÑÔºåÂÖ∂‰ΩôÂøΩÁï• FPÔºöÂÅáÈò≥ÊÄßÔºådistanceË∂ÖÂá∫ÊâÄÊúâgtÁöÑ8mmÂúàÂúàÔºèËêΩËøõÂúàÂúà‰ΩÜÊòØÁ±ªÂà´Èîô‰∫Ü FNÔºöÂÅáÈò¥ÊÄßÔºågtÁÇπÊ≤°ÊúâË¢´TP precisionÔºöTP/(TP+FP) recallÔºöTP/(TP+FN) AP MAP]]></content>
      <tags>
        <tag>competition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[group normalization]]></title>
    <url>%2F2020%2F06%2F08%2Fgroup-normalization%2F</url>
    <content type="text"><![CDATA[Group Normalization Âä®Êú∫ for small batch size do normalization in channel groups batch-independent behaves stably over different batch sizes approach BN‚Äôs accuracy ËÆ∫ÁÇπ BN requires sufficiently large batch size (e.g. 32) Mask R-CNN frameworks use a batch size of 1 or 2 images because of higher resolution, where BN is ‚Äúfrozen‚Äù by transforming to a linear layer synchronized BN „ÄÅBR LN &amp; IN effective for training sequential models or generative models but have limited success in visual recognition GNËÉΩËΩ¨Êç¢ÊàêLNÔºèIN WN normalize the filter weights, instead of operating on features ÊñπÊ≥ï group it is not necessary to think of deep neural network features as unstructured vectors Á¨¨‰∏ÄÂ±ÇÂç∑ÁßØÊ†∏ÈÄöÂ∏∏Â≠òÂú®‰∏ÄÁªÑÂØπÁß∞ÁöÑfilterÔºåËøôÊ†∑Â∞±ËÉΩÊçïËé∑Âà∞Áõ∏‰ººÁâπÂæÅ Ëøô‰∫õÁâπÂæÅÂØπÂ∫îÁöÑchannel can be normalized together normalization transform the feature xÔºö$\hat x_i = \frac{1}{\sigma}(x_i-\mu_i)$ the mean and the standard deviationÔºö \mu_i=\frac{1}{m}\sum_{k\in S_i}x_k\\ \sigma_i=\sqrt {\frac{1}{m}\sum_{k\in S_i}(x_k-\mu_i)^2+\epsilon} the set $S_i$ BNÔºö $S_i=\{k|k_C = i_C\}$ pixels sharing the same channel index are normalized together for each channel, BN computes Œº and œÉ along the (N, H, W) axes LN $S_i=\{k|k_N = i_N\}$ pixels sharing the same batch index (per sample) are normalized together LN computes Œº and œÉ along the (C,H,W) axes for each sample IN $S_i=\{k|k_N = i_N, k_C=i_C\}$ pixels sharing the same batch index and the same channel index are normalized together LN computes Œº and œÉ along the (H,W) axes for each sample GN $S_i=\{k|k_N = i_N, [\frac{k_C}{C/G}]=[\frac{i_C}{C/G}]\}$ computes Œº and œÉ along the (H, W ) axes and along a group of C/G channels linear transform to keep representational ability per channel scale and shiftÔºö$y_i = \gamma \hat x_i + \beta$ relation to LN LN assumes all channels in a layer make ‚Äúsimilar contributions‚Äù which is less valid with the presence of convolutions GN improved representational power over LN to IN IN can only rely on the spatial dimension for computing the mean and variance it misses the opportunity of exploiting the channel dependence „ÄêQUESTION„ÄëBN‰πüÊ≤°ËÄÉËôëÈÄöÈÅìÈó¥ÁöÑËÅîÁ≥ªÂïäÔºå‰ΩÜÊòØËÆ°ÁÆómeanÂíåvarianceÊó∂Ë∑®‰∫Üsample implementation reshape learnable $\gamma \&amp; \beta$ computable mean &amp; var ÂÆûÈ™å GNÁõ∏ÊØî‰∫éBNÔºåtraining errorÊõ¥‰ΩéÔºå‰ΩÜÊòØval errorÁï•È´ò‰∫éBN GN is effective for easing optimization loses some regularization ability it is possible that GN combined with a suitable regularizer will improve results ÈÄâÂèñ‰∏çÂêåÁöÑgroupÊï∞ÔºåÊâÄÊúâÁöÑgroup&gt;1ÂùáÂ•Ω‰∫égroup=1ÔºàLNÔºâ ÈÄâÂèñ‰∏çÂêåÁöÑchannelÊï∞ÔºàCÔºèGÔºâÔºåÊâÄÊúâÁöÑchannel&gt;1ÂùáÂ•Ω‰∫échannel=1ÔºàINÔºâ Object Detection frozenÔºöÂõ†‰∏∫higher resolutionÔºåbatch sizeÈÄöÂ∏∏ËÆæÁΩÆ‰∏∫2/GPUÔºåËøôÊó∂ÁöÑBN frozenÊàê‰∏Ä‰∏™Á∫øÊÄßÂ±Ç$y=\gamma(x-\mu)/\sigma+beta$ÔºåÂÖ∂‰∏≠ÁöÑ$\mu$Âíå$sigma$ÊòØload‰∫Üpre-trained model‰∏≠‰øùÂ≠òÁöÑÂÄºÔºåÂπ∂‰∏îfrozenÊéâÔºå‰∏çÂÜçÊõ¥Êñ∞ denote as BN* replace BN* with GN during fine-tuning use a weight decay of 0 for the Œ≥ and Œ≤ parameters]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ê≠£ÂàôÂåñ]]></title>
    <url>%2F2020%2F06%2F02%2Fregularization%2F</url>
    <content type="text"><![CDATA[ÁªºËø∞ Ê≠£Âàô Ê≠£ÂàôÂåñÊòØÁî®Êù•Ëß£ÂÜ≥Á•ûÁªèÁΩëÁªúËøáÊãüÂêàÁöÑÈóÆÈ¢òÔºåÈÄöËøáÈôç‰ΩéÊ®°ÂûãÁöÑÂ§çÊùÇÊÄßÂíåÁ∫¶ÊùüÊùÉÂÄºÔºåËø´‰ΩøÁ•ûÁªèÁΩëÁªúÂ≠¶‰π†ÂèØÊ≥õÂåñÁöÑÁâπÂæÅ Ê≠£ÂàôÂåñÂèØ‰ª•ÂÆö‰πâ‰∏∫Êàë‰ª¨‰∏∫‰∫ÜÂáèÂ∞ëÊ≥õÂåñËØØÂ∑ÆËÄå‰∏çÊòØÂáèÂ∞ëËÆ≠ÁªÉËØØÂ∑ÆËÄåÂØπËÆ≠ÁªÉÁÆóÊ≥ïÊâÄÂÅöÁöÑ‰ªª‰ΩïÊîπÂèò ÂØπÊùÉÈáçËøõË°åÁ∫¶Êùü ÂØπÁõÆÊ†áÂáΩÊï∞Ê∑ªÂä†È¢ùÂ§ñÈ°πÔºàÈó¥Êé•Á∫¶ÊùüÊùÉÂÄºÔºâÔºöL1 &amp; L2Ê≠£Âàô Êï∞ÊçÆÂ¢ûÂº∫ Èôç‰ΩéÁΩëÁªúÂ§çÊùÇÂ∫¶ÔºödropoutÔºåstochastic depth early stopping Êàë‰ª¨Âú®ÂØπÁΩëÁªúËøõË°åÊ≠£ÂàôÂåñÊó∂‰∏çËÄÉËôëÁΩëÁªúÁöÑbiasÔºöÊ≠£ÂàôË°®ËææÂºèÂè™ÊòØÊùÉÂÄºÁöÑË°®ËææÂºèÔºå‰∏çÂåÖÂê´bias biasÊØîweightÂÖ∑ÊúâÊõ¥Â∞ëÁöÑÂèÇÊï∞Èáè ÂØπbiasËøõË°åÊ≠£ÂàôÂåñÂèØËÉΩÂºïÂÖ•Â§™Â§öÁöÑÊñπÂ∑ÆÔºåÂºïÂÖ•Â§ßÈáèÁöÑÊ¨†ÊãüÂêà L1 &amp; L2Ôºö Ë¶ÅÊÉ©ÁΩöÁöÑÊòØÁ•ûÁªèÁΩëÁªú‰∏≠ÊØè‰∏™Á•ûÁªèÂÖÉÁöÑÊùÉÈáçÂ§ßÂ∞è L2ÂÖ≥Ê≥®ÁöÑÊòØÊùÉÈáçÁöÑÂπ≥ÊñπÂíåÔºåÊòØË¶ÅÁΩëÁªú‰∏≠ÁöÑÊùÉÈáçÊé•Ëøë0‰ΩÜ‰∏çÁ≠â‰∫é0Ôºå‚ÄúÊùÉÈáçË°∞Âáè‚Äù \frac{d}{dW}(\frac{\lambda}{2m}W^2) = \frac{\lambda}{m} W L1ÂÖ≥Ê≥®ÁöÑÊòØÊùÉÈáçÁöÑÁªùÂØπÂÄºÔºåÊùÉÈáçÂèØËÉΩË¢´ÂéãÁº©Êàê0ÔºåÊùÉÈáçÊõ¥Êñ∞Êó∂ÊØèÊ¨°ÂáèÂéªÁöÑÊòØ‰∏Ä‰∏™Â∏∏Èáè \frac{d}{dW}(\frac{\lambda}{m}W) = \frac{\lambda}{m} sgn(W) L1‰ºöË∂ãÂêë‰∫é‰∫ßÁîüÂ∞ëÈáèÁöÑÁâπÂæÅÔºåËÄåÂÖ∂‰ªñÁöÑÁâπÂæÅÈÉΩÊòØ0ÔºåËÄåL2‰ºöÈÄâÊã©Êõ¥Â§öÁöÑÁâπÂæÅÔºåËøô‰∫õÁâπÂæÅÈÉΩ‰ºöÊé•Ëøë‰∫é0 dropout ÊØè‰∏™epochËÆ≠ÁªÉÁöÑÊ®°ÂûãÈÉΩÊòØÈöèÊú∫ÁöÑ Âú®testÁöÑÊó∂ÂÄôÁõ∏ÂΩì‰∫éensembleÂ§ö‰∏™Ê®°Âûã ÊùÉÈáçÂÖ±‰∫´ Êï∞ÊçÆÂ¢ûÂº∫ ÈöêÂºèÊ≠£ÂàôÂåñÔºöÂÖ∂Âá∫Áé∞ÁöÑÁõÆÁöÑ‰∏çÊòØ‰∏∫‰∫ÜÊ≠£ÂàôÂåñÔºåËÄåÊ≠£ÂàôÂåñÁöÑÊïàÊûúÊòØÂÖ∂ÂâØ‰∫ßÂìÅÔºåÂåÖÊã¨early stoppingÔºåBNÔºåÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôç dropout &amp; drop connectÔºà[Reference][https://zhuanlan.zhihu.com/p/108024434]Ôºâ dropoutÔºö 2012Âπ¥HintonÊèêÂá∫ÔºåÂú®Ê®°ÂûãËÆ≠ÁªÉÊó∂‰ª•Ê¶ÇÁéápÈöèÊú∫ËÆ©ÈöêÂ±ÇËäÇÁÇπÁöÑËæìÂá∫ÂèòÊàê0ÔºåÊöÇÊó∂ËÆ§‰∏∫Ëøô‰∫õËäÇÁÇπ‰∏çÊòØÁΩëÁªúÁªìÊûÑÁöÑ‰∏ÄÈÉ®ÂàÜÔºå‰ΩÜÊòØ‰ºöÊääÂÆÉ‰ª¨ÁöÑÊùÉÈáç‰øùÁïô‰∏ãÊù•Ôºà‰∏çÊõ¥Êñ∞Ôºâ„ÄÇ Ê†áÂáÜdropoutÁõ∏ÂΩì‰∫éÂú®‰∏ÄÂ±ÇÁ•ûÁªèÂÖÉ‰πãÂêéÂÜçÊ∑ªÂä†‰∏Ä‰∏™È¢ùÂ§ñÁöÑÂ±ÇÔºåËøô‰∫õÁ•ûÁªèÂÖÉÂú®ËÆ≠ÁªÉÊúüÈó¥‰ª•‰∏ÄÂÆöÁöÑÊ¶ÇÁéáÂ∞ÜÂÄºËÆæÁΩÆ‰∏∫Èõ∂ÔºåÂπ∂Âú®ÊµãËØïÊúüÈó¥Â∞ÜÂÆÉ‰ª¨‰πò‰ª•p„ÄÇ drop connectÔºö ‰∏çÊòØÈöèÊú∫ÁöÑÂ∞ÜÈöêÂ±ÇËäÇÁÇπÁöÑËæìÂá∫ÂèòÊàê0ÔºåËÄåÊòØÂ∞ÜËäÇÁÇπ‰∏≠ÁöÑÊØè‰∏™‰∏éÂÖ∂Áõ∏ËøûÁöÑËæìÂÖ•ÊùÉÂÄº‰ª•1-pÁöÑÊ¶ÇÁéáÂèòÊàê0„ÄÇÔºà‰∏Ä‰∏™ÊòØËæìÂá∫‰∏Ä‰∏™ÊòØËæìÂÖ•Ôºâ ËÆ≠ÁªÉÈò∂ÊÆµÔºåÂØπÊØè‰∏™exampleÔºèmini-batch, ÊØè‰∏™epochÈÉΩÈöèÊú∫sample‰∏Ä‰∏™maskÁü©Èòµ DropconnectÂú®ÊµãËØïÊúüÈó¥ÈááÁî®‰∫Ü‰∏éÊ†áÂáÜdropout‰∏çÂêåÁöÑÊñπÊ≥ï„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫ÜdropconnectÂú®ÊØè‰∏™Á•ûÁªèÂÖÉÂ§ÑÁöÑÈ´òÊñØËøë‰ººÔºåÁÑ∂Âêé‰ªéËøô‰∏™È´òÊñØÂáΩÊï∞‰∏≠ÊäΩÂèñ‰∏Ä‰∏™Ê†∑Êú¨Âπ∂‰º†ÈÄíÁªôÁ•ûÁªèÂÖÉÊøÄÊ¥ªÂáΩÊï∞„ÄÇËøô‰ΩøÂæódropconnectÂú®ÊµãËØïÊó∂ÂíåËÆ≠ÁªÉÊó∂ÈÉΩÊòØ‰∏ÄÁßçÈöèÊú∫ÊñπÊ≥ï„ÄÇ ‰ºØÂä™Âà©ÂàÜÂ∏ÉÔºö0-1ÂàÜÂ∏É dropout &amp; drop connect ÈÄöÂ∏∏Âè™‰ΩúÁî®‰∫éÂÖ®ËøûÊé•Â±Ç‰∏äÔºöËøô‰ø©ÊòØÁî®Êù•Èò≤Ê≠¢ËøáÂ§öÂèÇÊï∞ÂØºËá¥ËøáÊãüÂêà Âç∑ÁßØÂ±ÇÂèÇÊï∞Ë¥ºÂ∞ëÔºåÊâÄ‰ª•Ê≤°ÂøÖË¶ÅÔºå ÈíàÂØπÂç∑ÁßØÈÄöÈÅìÊúâspacial dropoutÔºöÊåâÁÖßchannelÈöèÊú∫Êâî dropblockÔºöÊòØÈíàÂØπÂç∑ÁßØÂ±ÇÁöÑÊ≠£ÂàôÂåñÊñπÊ≥ïÔºåÁõ∏ÊØîËæÉ‰∫édropoutÁöÑrandom muteÔºåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞removeÊéâÈÉ®ÂàÜËØ≠‰πâ‰ø°ÊÅØÔºåblock size=1ÁöÑÊó∂ÂÄôÈÄÄÂåñÊàêdropout papers [dropout] Improving neural networks by preventing co-adaptation of feature detectorsÔºå‰∏¢ËäÇÁÇπ [drop connect] Regularization of neural networks using dropconnectÔºå‰∏¢weight path [Stochastic Depth] Deep Networks with Stochastic DepthÔºå‰∏¢layer [DropBlock] A regularization method for convolutional networks dropÂ§ßÊ≥ï‰∏ÄÂè•ËØùÊ±áÊÄª dropoutÔºöÂêÑÁª¥Â∫¶ÂÆåÂÖ®ÈöèÊú∫Êâî spacial dropoutÔºöÊåâÁÖßchannelÈöèÊú∫Êâî stochastic depthÔºöÊåâÁÖßres blockÈöèÊú∫Êâî dropblockÔºöÂú®feature map‰∏äÊåâÁÖßspacialÂùóÈöèÊú∫Êâî cutoutÔºöÂú®input map‰∏äÊåâÁÖßspacialÂùóÈöèÊú∫Êâî dropconnectÔºöÊâîËøûÊé•‰∏çÊâîÁ•ûÁªèÂÖÉ Deep Networks with Stochastic Depth Âä®Êú∫ propose a training procedureÔºöstochastic depthÔºåtrain short and test deep for each mini-batch randomly drop a subset of layers and bypass them with the identity function shortÔºöreduces training time regÔºöimproves the test error can increase the network depth ËÆ∫ÁÇπ deeper expressiveness vanishing gradients diminishing feature reuse resnet skip connection whenËæìÂÖ•ËæìÂá∫channelÊï∞‰∏çmatchÔºöredefine id(¬∑) as a linear projection to reduce the dimensions dropout Dropout reduces the effect known as ‚Äúco- adaptation‚Äù of hidden nodes Dropout loses effectiveness when used in combination with Batch Normalization our approach higher diversity shorter instead of thinner work with Batch Normalization ÊñπÊ≥ï stochastic depth randomly dropping entire ResBlocks $H_l = ReLU(b_l Res_l(H_{l-1}) + id(H_{l-1}))$ survival probabilities $p_l = Pr(b_l=1)$ set uniformly / set following a linear decay rule set $p_0=1, p_L=0.5$Ôºö p_l = 1 - \frac{l}{L}(1-p_L) intuitionÔºöthe earlier layers extract low-level features that will be used by later layers and should therefore be more reliably present Expected network depth $E(L) \approx 3L/4$ approximately 25% of training time could be saved during testing all res path are active each res path is weighted by its survival probability $H_l^{Test} = ReLU(b_l Res_l(H_{l-1}, W_l) + id(H_{l-1}))$ Ë∑üdropout‰∏ÄÊ†∑]]></content>
      <tags>
        <tag>Ê≠£ÂàôÂåñÔºådropoutÔºådropconnect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RetinaNet]]></title>
    <url>%2F2020%2F05%2F30%2FRetinaNet%2F</url>
    <content type="text"><![CDATA[[det] RetinaNet: Focal Loss for Dense Object Detection [det+instance seg] RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free [det+semantic seg] Retina U-Net: Embarrassingly Simple Exploitation of Segmentation Supervision for Medical Object Detection Focal Loss for Dense Object Detection Âä®Êú∫ dense prediction(one-stage detector) focal lossÔºöaddress the class imbalance problem RetinaNetÔºödesign and train a simple dense detector ËÆ∫ÁÇπ accuracy trailed two-stageÔºöclassifier is applied to a sparse set of candidate one-stageÔºödense sampling of possible object locations the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause loss standard cross entropy lossÔºödown-weights the loss assigned to well-classified examples proposed focal lossÔºöfocuses training on a sparse set of hard examples R-CNNÁ≥ªÂàótwo-stage framework proposal-driven the first stage generates a sparse set of candidate object locations the second stage classifies each candidate location as one of the foreground classes or as background class imbalanceÔºöÂú®stage1Â§ßÈÉ®ÂàÜËÉåÊôØË¢´filter out‰∫ÜÔºåstage2ËÆ≠ÁªÉÁöÑÊó∂ÂÄôÂº∫Âà∂Âõ∫ÂÆöÂâçËÉåÊôØÊ†∑Êú¨ÊØî‰æãÔºåÂÜçÂä†‰∏äÂõ∞ÈöæÊ†∑Êú¨ÊåñÊéòOHEM fasterÔºöreducing input image resolution and the number of proposals ever fasterÔºöone-stage one-stage detectors One stage detectors are applied over a regular, dense sampling of object locations, scales, and aspect ratios denseÔºöregularly sampling(contrast to selection)ÔºåÂü∫‰∫égrid‰ª•Âèäanchor‰ª•ÂèäÂ§öÂ∞∫Â∫¶ the training procedure is still dominated by easily classified background examples class imbalanceÔºöÈÄöÂ∏∏ÂºïÂÖ•bootstrappingÂíåhard example miningÊù•‰ºòÂåñ Object Detectors ClassicÔºösliding-window+classifier based on HOGÔºådense predict Two-stageÔºöselective Search+classifier based on CNNÔºåshared network RPN One-stageÔºö‚Äòanchors‚Äô introduced by RPNÔºåFPN loss Huber lossÔºödown-weighting the loss of outliers (hard examples) focal lossÔºödown-weighting inliers (easy examples) ÊñπÊ≥ï focal loss CEÔºö$CE(p_t)=-log(p_t)$ even examples that are easily classified ($p_t&gt;0.5$) incur a loss with non-trivial magnitude summed CE loss over a large number of easy examples can overwhelm the rare class WCEÔºö$WCE(p_t)=-\alpha_t log(p_t)$ balances the importance of positive/negative examples does not differentiate between easy/hard examples FLÔºö$FL(p_t)=-\alpha_t(1-p_t)^\gamma log(p_t)$ as $\gamma$ increases the modulating factor is likewise increased $\gamma=2$ works best in our experiments ‚Äã two-stage detectorsÈÄöÂ∏∏‰∏ç‰ºö‰ΩøÁî®WCEÊàñFL cascade stage‰ºöËøáÊª§ÊéâÂ§ßÈÉ®ÂàÜeasy negatives Á¨¨‰∫åÈò∂ÊÆµËÆ≠ÁªÉ‰ºöÂÅöbiased minibatch sampling Online Hard Example Mining (OHEM) construct minibatches using high-loss examples scored by loss + nms completely discards easy examples RetinaNet composeÔºöbackbone network + two task-specific subnetworks backboneÔºöconvolutional feature map over the entire input image subnet1Ôºöobject classification subnet2Ôºöbounding box regression ResNet-FPN backbone rich, multi-scale feature pyramidÔºå‰∫åÈò∂ÊÆµÁöÑRPN‰πüÁî®‰∫ÜFPN each level can be used for detecting objects at a different scale P3 - P7Ôºö8x - 128x downsamp FPN channelsÔºö256 anchors anchor ratiosÔºö{1:2, 1:1, 2:1}ÔºåÈïøÂÆΩÊØî anchor scalesÔºö{$2^0$, $2^\frac{1}{3}$, $2^\frac{2}{3}$}ÔºåÂ§ßÂ∞èÔºåÂêå‰∏Ä‰∏™scaleÁöÑanchorÔºåÈù¢ÁßØÁõ∏ÂêåÔºåÈÉΩÊòØsize*sizeÔºåÈïøÂÆΩÈÄöËøáratioÊ±ÇÂæó anchor size per levelÔºö[32, 64, 128, 256, 512]ÔºåÂü∫Êú¨ÁöÑÊ≠£ÊñπÂΩ¢anchorÁöÑËæπÈïø total anchors per levelÔºöA=9 KAÔºöeach anchor is assigned a length K one-hot vector of classification targets 4AÔºöand a 4-vector of box regression targets anchors are assigned to ground-truth object boxes using an intersection-over-union (IoU) threshold of 0.5 anchors are assigned background if their IoU is in [0, 0.4) anchor is unassigned between [0.4, 0.5), which is ignored during training each anchor is assigned to at most one object box for each anchor classification targetsÔºöone-hot vector box regression targetsÔºöeach anchorÂíåÂÖ∂ÂØπÂ∫îÁöÑgt boxÁöÑoffset rpn offsetÔºö‰∏≠ÂøÉÁÇπ„ÄÅÂÆΩ„ÄÅÈ´ò $$ t_x = (x - x_a) / w_a\\ t_y = (y - y_a) / h_a\\ t_w = log(w/ w_a)\\ t_h = log(h/ h_a) $$ or omitted if there is no assignment „ÄêQUESTION„ÄëÊâÄË∞ìÁöÑanchor state {-1:ignore, 0:negative, 1:positive} ÊòØÈíàÂØπcls lossÊù•ËØ¥ÁöÑÔºåÁõ∏ÂΩì‰∫é‰∫∫‰∏∫‰∏¢ÂºÉ‰∫Ü‰∏ÄÈÉ®ÂàÜÂÅèÂêë‰∏≠Á´ãÁöÑÊ†∑Êú¨ÔºåËøôÂØπÂàÜÁ±ªÊïàÊûúÊúâÊèêÂçáÂêóÔºüÔºü classification subnet for each spatial positionÔºåfor each anchorÔºåpredict one among K classesÔºåone-hot inputÔºöC channels feature map from FPN structureÔºöfour 3x3 conv + ReLUÔºåeach with C filters headÔºö3x3 conv + sigmoidÔºåwith KA filters share across levels not share with box regression subnet focal lossÔºö sum over all ÔΩû100k anchors * and normalized by the number of anchors assigned to a ground-truth box * Âõ†‰∏∫ÊòØsumÔºåÊâÄ‰ª•Ë¶ÅnormailizeÔºånormÈ°πÁî®ÁöÑÊòØnumber of assigned anchorsÔºàËøôÊòØÂåÖÊã¨‰∫ÜÂâçËÉåÊôØÔºüÔºâ * vast majority of anchors are **easy negatives** and receive negligible loss values under the focal lossÔºàÁ°ÆÂÆûÂåÖÂê´ËÉåÊôØÊ°ÜÔºâ * $\alpha$ÔºöIn general $alpha$ should be decreased slightly as $\gamma$ is increased strong effect on negativesÔºöFL can effectively discount the effect of easy negatives, focusing all attention on the hard negative examples box regression subnet class-agnostic bounding box regressor same structureÔºöfour 3x3 conv + ReLUÔºåeach with C filters * headÔºö4A linear outputs * L1 loss inference keep top 1k predictions per FPN level * all levels are merged and non-maximum suppression with a threshold of 0.5 train initializationÔºö cls head bias initializationÔºåencourage more foreground prediction at the start of training prevents the large number of background anchors from generating a large, destabilizing loss network design anchors * one-stage detecors use fixed sampling grid to generate position * use multiple ‚Äòanchors‚Äô at each spatial position to cover boxes of various scales and aspect ratios * beyond 6-9 anchors did not shown further gains in AP * speed/accuracy trade-off * outperforms all previous methods * bigger resolution bigger AP * Retina-101-600‰∏éResNet101-FRCNNÁöÑAPÊåÅÂπ≥Ôºå‰ΩÜÊòØÊØî‰ªñÂø´ gradientÔºö Ê¢ØÂ∫¶ÊúâÁïå the derivative is small as soon as $x_t &gt; 0$ &lt;img src=&quot;RetinaNet/gradient.png&quot; width=&quot;70%;&quot; /&gt; ‚Äã RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free Âä®Êú∫ improve single-shot detectors to the same level as current two-stage techniques improve on RetinaNet integrating instance mask prediction adaptive loss additional hard examples Group Normalization same computational cost as the original RetinaNet but more accurateÔºöÂêåÊ†∑ÁöÑÂèÇÊï∞ÈáèÁ∫ßÊØîorgin RetinaNetÂáÜÔºåÊï¥‰ΩìÁöÑÂèÇÊï∞ÈáèÁ∫ßÂ§ß‰∫éyolov3ÔºåaccÂø´Ë¶ÅÊé•Ëøë‰∫åÈò∂ÊÆµÁöÑmask RCNN‰∫Ü ËÆ∫ÁÇπ part of improvements of two-stage detectors is due to architectures like Mask R-CNN that involves multiple prediction heads additional segmentation task had only been added to two-stage detectors in the past two-stage detectors have the cost of resampling(ROI-Align) issueÔºöRPN‰πãÂêéË¶ÅÁâπÂæÅÂØπÈΩê add addtional heads in training keeps the structure of the detector at test time unchanged potential improvement directions dataÔºöOHEM contextÔºöFPN additional taskÔºösegmentation branch this paper‚Äôs contribution add a mask prediction branch propose a new self-adjusting loss function include more of positive samples‚Äî&gt;those with low overlap ÊñπÊ≥ï best matching policy speical caseÔºöoutlier gt boxÔºåË∑üÊâÄÊúâÁöÑanchor iouÈÉΩ‰∏çÂ§ß‰∫é0.5ÔºåÊ∞∏Ëøú‰∏ç‰ºöË¢´ÂΩì‰ΩúÊ≠£Ê†∑Êú¨ use best matching anchor with any nonzero overlap to replace the threshold self-adjusting Smooth L1 loss bbox regression smooth L1Ôºö L1 loss is used beyond $\beta$ to avoid over-penalizing outliers the choice of control point is heuristic and is usually done by hyper parameter search f(x) = \begin{cases} 0.5 \frac{x^2}{\beta} \text{, if } |x| < \beta \\ |x| - 0.5\beta \text{, otherwise } \end{cases} self-adjusting control point running mean &amp; variance \mu_B = \frac{1}{n}\sum_{i=1}^n |x_i|\\ \sigma_B^2 = \frac{1}{n}\sum_{i=1}^n(|x_i|-\mu_B)^2 minibatch updateÔºöm=0.9 \mu_R = \mu_R * m + \mu_B*(1-m)\\ \sigma_R^2 = \sigma_R^2*m+\sigma_B^2*(1-m) control pointÔºö$[0, \hat \beta]$ clip to avoid unstable \beta = max(0, min(\hat \beta, \mu_R-\sigma_R^2)) mask module detection predictions are treated as mask proposals extract the top N scored predictions distribute the mask proposals to sample features from the appropriate layers k = [k_0 + log_2 \sqrt{wh}/224] $k_0=4$ÔºåÂ¶ÇÊûúsizeÂ∞è‰∫é224*224Ôºåproposal‰ºöË¢´ÂàÜÈÖçÁªôP3ÔºåÂ¶ÇÊûúÂ§ß‰∫é448*448Ôºåproposal‰ºöË¢´ÂàÜÈÖçÁªôP5 using more feature layers shows no performance boost architecture r50&amp;r101 backÔºöfreezing all of the Batch Nor- malization layers fpn feature channelÔºö256 classification branch 4 conv layersÔºöconv3x3+reluÔºåchannel256 headÔºöconv3x3+sigmoidÔºåchannel n_anchors*n_classes regression branch 4 conv layersÔºöconv3x3+reluÔºåchannel256 headÔºöconv3x3Ôºåchannel n_anchors*4 aggregate the boxes to the FPN layers ROI-Align yielding 14x14 resolution features mask head 4 conv layersÔºöconv3x3 a single transposed convolutional layerÔºöconvtranspose2d 2x2Ôºåto 28*28 resolution prediction headÔºöconv1x1 training min side &amp; max sideÔºö800&amp;1333 limited GPUÔºöreduce the batch sizeÔºåincreasing the number of training iterations and reducing the learning rate accordingly positive/ignore/negativeÔºö0.5Ôºå0.4 focal loss for classification gaussian initialization $\alpha=0.25, \lambda=2.0$ $FL=-\alpha_t(1-p_t)^\lambda log(p_t)$ FL = \left\{ \begin{array}{lr} -\alpha (1-p)^{\gamma}log(p), \ \ y=1\\ -(1-\alpha) p^{\gamma}log(1-p), \ \ y=0\\ \end{array} \right. gammaÈ°πÊéßÂà∂ÁöÑÊòØÁÆÄÂçïÊ†∑Êú¨ÁöÑË°∞ÂáèÈÄüÂ∫¶ÔºåalphaÈ°πÊéßÂà∂ÁöÑÊòØÊ≠£Ë¥üÊ†∑Êú¨ÊØî‰æãÔºåÂèØ‰ª•ÈªòËÆ§ÂÄº‰∏ãÊ≠£Ê†∑Êú¨ÁöÑÊùÉÈáçÊòØ0.25ÔºåË¥üÊ†∑Êú¨ÁöÑÊùÉÈáçÊòØ0.75ÔºåÂíåÊÉ≥Ë±°‰∏≠ÁöÑÁªôÊ≠£Ê†∑Êú¨Êõ¥Â§öÊùÉÈáç‰∏ç‰∏ÄÊ†∑ÔºåÂõ†‰∏∫alphaÂíågammaÊòØËÄ¶ÂêàËµ∑Êù•‰ΩúÁî®ÁöÑÔºåÔºàÂèØËÉΩÊ£ÄÊµãÂú∫ÊôØ‰∏ãÂõ∞ÈöæÁöÑË¥üÊ†∑Êú¨Áõ∏ÊØî‰∫éÊ≠£Ê†∑Êú¨Êõ¥Â∞ëÔºüËÉåÊôØÂ∞±ÊòØÊØîÂâçÊôØÂ•ΩÂ≠¶Ôºü‰∏çÁ°ÆÂÆö‰∏çÁ°ÆÂÆö„ÄÇ„ÄÇ„ÄÇÔºâ self-adjusting L1 loss for box regression limit running paramsÔºö[0, 0.11] mask loss top-100 predicted boxes + ground truth boxes inference box confidence threshold 0.05 nms threshold 0.4 use top-50 boxes for mask prediction Retina U-Net: Embarrassingly Simple Exploitation of Segmentation Supervision for Medical Object Detection Âä®Êú∫ localization pixel-level predict ad-hoc heuristics when mapping back to object-level scores semantic segmentation auxiliary task overall one-stage leveraging available supervision signals ËÆ∫ÁÇπ monitoring pixel-wise predictions are clinically required medical annotations is commonly performed in pixel- wise full semantic supervision fully exploiting the available semantic segmentation signal results in significant performance gains one-stage explicit scale variance enforced by the resampling operation in two-stage detectors is not helpful in the medical domain two-stage methods predict proposal-based segmentations mask loss is only evaluated on cropped proposalÔºöno context gradients ROI-AlignÔºönot suggested in medical image depends on the results of region proposalÔºöserial vs parallel gradients of the mask loss do not flow through the entire model ÊñπÊ≥ï model backÔºö ResNet50 fpnÔºö shift p3-p6 to p2-p5 change sigmoid to softmax 3d head channelsÔºö64 anchor sizeÔºö$\{P_2: 4^2, P_3: 8^2,, P_4: 16^2,, P_5: 32^2\}$ 3d z-scaleÔºö{1Ôºå2Ôºå4Ôºå8}ÔºåËÄÉËôëÂà∞zÊñπÂêëÁöÑlow resolution segmentation supervision p0 &amp; p1 with skip connections without detection heads segmentation loss calculates on p0 logits dice + ce h weighted box clustering patch crop tiling strategies &amp; model ensembling causes multi predictions per location nmsÈÄâ‰∫Ü‰∏ÄÁ±ª‰∏≠scoreÊúÄÂ§ßÁöÑboxÔºåÁÑ∂ÂêéÊäëÂà∂ÊâÄÊúâ‰∏éÂÆÉÂêåÁ±ªÁöÑIoUÂ§ß‰∫é‰∏ÄÂÆöÈòàÂÄºÁöÑbox weighted box‰ΩúÁî®‰∫éËøô‰∏ÄÁ±ªÊâÄÊúâÁöÑboxÔºåËÆ°ÁÆó‰∏Ä‰∏™ËûçÂêàÁöÑÁªìÊûú coordinates confidenceÔºö$o_c = \frac{\sum c_i s_i w_i}{\sum s_i w_i}$ score confidenceÔºö$o_s = \frac{\sum s_i w_i}{\sum w_i + n_{missing * \overline w}}$ $w_i$Ôºö$w=f a p$ overlap factor fÔºö‰∏éhighest scoring boxÁöÑoverlap area factor aÔºöhigher weights to larger boxesÔºåÁªèÈ™å patch center factor pÔºöÁõ∏ÂØπ‰∫épatch centerÁöÑÊ≠£ÊÄÅÂàÜÂ∏É score confidenceÁöÑÂàÜÊØç‰∏äÊúâ‰∏Ä‰∏™down-weightÈ°π$n_{missing}$ÔºöÂü∫‰∫éprior knowledgeÈ¢ÑÊúüpredictionÁöÑÊÄªÊï∞ÂæóÂà∞ ËÆ∫ÊñáÁªôÁöÑ‰æãÂ≠êËÆ©ÊàëÊÑüËßâÂ•ΩÊØînmsÁöÑÁÇπ ‰∏Ä‰∏™clusterÈáåÈù¢‰∏ÄÁ±ªÊúÄÁªàÂ∞±Áïô‰∏ã‰∏Ä‰∏™Ê°ÜÔºöËß£ÂÜ≥nms‰∏ÄÁ±ªÂ§ßÊ°ÜÂåÖÂ∞èÊ°ÜÁöÑÊÉÖÂÜµ Ëøô‰∏™location‰∏äpredictionÊòéÊòæÂ∞ë‰∫éprior knowledgeÁöÑÁ±ªÂà´confidence‰ºöË¢´ÊòæËëóÊãâ‰ΩéÔºöËß£ÂÜ≥‰∏Ä‰∏™‰ΩçÁΩÆÂá∫Áé∞Â§ßÊ¶ÇÁéáÂÅáÈò≥Ê°ÜÁöÑÊÉÖÂÜµ]]></content>
      <tags>
        <tag>ÁõÆÊ†áÊ£ÄÊµãÔºåfocallossÔºåÂÆû‰æãÂàÜÂâ≤ÔºåËá™ÈÄÇÂ∫îsmoothL1</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DCGAN]]></title>
    <url>%2F2020%2F05%2F27%2FDCGAN%2F</url>
    <content type="text"><![CDATA[UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS Âä®Êú∫ unsupervised learning learns a hierarchy of representations from object parts to scenes used for novel tasks ËÆ∫ÁÇπ GAN Learning reusable feature representations from large unlabeled datasets generator and discriminator networks can be later used as feature extractors for supervised tasks unstable to train we propose a set of constraints on the architectural topology making it stable to train use the trained discriminators for image classification tasks visualize the filters show that the generators have interesting vector arithmetic properties unsupervised representation learning clustering, hierarchical clustering auto-encoders learn good feature representations generative image models samples often suffer from being blurry, being noisy and incomprehensible further use for supervised tasks ÊñπÊ≥ï architecture all convolutional netÔºöÊ≤°ÊúâÊ±†ÂåñÔºåÁî®stride conv eliminating fully connected layersÔºö generatorÔºöËæìÂÖ•ÊòØ‰∏Ä‰∏™ÂêëÈáèÔºåreshape‰ª•ÂêéÊé•ÁöÑÂÖ®ÊòØÂç∑ÁßØÂ±Ç discriminatorÔºöÊúÄÂêé‰∏ÄÂ±ÇÂç∑ÁßØÂá∫Êù•Áõ¥Êé•flatten Batch Normalization generatorËæìÂá∫Â±Ç &amp; discriminatorËæìÂÖ•Â±Ç‰∏çÂä† resulted in sample oscillation and model instability ReLU generatorËæìÂá∫Â±ÇÁî®Tanh discriminatorÁî®leakyReLU train image preprocessÔºörescale to [-1,1] LeakyReLU(0.2) lrÔºö2e-4 momentum term $\beta 1$Ôºö0.5, default 0.9 ÂÆûÈ™å evaluate apply them as a feature extractor on supervised datasets evaluate the performance of linear models on top of these features model use the discriminator‚Äôs convolutional features from all layers maxpooling to 4x4 grids flattened and concatenated to form a 28672 dimensional vector regularized linear L2-SVM Áõ∏ÊØî‰πã‰∏ãÔºöthe discriminator has many less feature maps, but larger total feature vector size visualizing walking in the latent space Âú®vector Z‰∏äÂ∑ÆÂÄºÔºåÁîüÊàêÂõæÂÉèÂèØ‰ª•ËßÇÂØüÂà∞smooth transitions visualize the discriminator feature ÁâπÂæÅÂõæÂèØËßÜÂåñÔºåËÉΩËßÇÂØüÂà∞Â∫äÁªìÊûÑ manipulate the generator representation generator learns specific object representations for major scene components use logistic regression to find feature maps related with window, drop the spatial locations on feature-maps most result forgets to draw windows in the bedrooms, replacing them with other objects vector arithmetic averaging the Z vector for three examplars semantically obeyed the arithmetic]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[densenet]]></title>
    <url>%2F2020%2F05%2F27%2Fdensenet%2F</url>
    <content type="text"><![CDATA[Âä®Êú∫ embrace shorter connections the feature-maps of all preceding layers are used as inputs advantages alleviate vanishing-gradient encourage feature reuse reduce the number of parameters ËÆ∫ÁÇπ Dense each layer obtains additional inputs from all preceding lay- ers and passes on its own feature-maps to all subsequent layers feature reuse combine features by concatenatingÔºöthe summation in ResNet may impede the information flow in the network information preservation id shortcut/additive identity transformations fewer params DenseNet layers are very narrow add only a small set of feature-maps to the ‚Äúcollective knowledge‚Äù gradients flow each layer has direct access to the gradients from the loss function have regularizing effect ÊñπÊ≥ï architecture dense blocks concat BN-ReLU-3x3 conv $x_l = H_l([x_0, x_1, ‚Ä¶, x_{l-1}])$ transition layers change the size of feature-maps BN-1x1 conv-2x2 avg pooling growth rate k $H_l$ produces feature- maps narrowÔºöe.g., k = 12 One can view the feature-maps as the global state of the network The growth rate regulates how much new information each layer contributes to the global state bottleneck ‚Äî- DenseNet-B in dense block stage 1x1 conv reduce dimension first number of channelsÔºö4k compression ‚Äî- DenseNet-C in transition stage reduce the number of feature-maps number of channelsÔºö$\theta k$ structure configurations 1st conv channelsÔºöÁ¨¨‰∏ÄÂ±ÇÂç∑ÁßØÈÄöÈÅìÊï∞ number of dense blocks LÔºödense blockÈáåÈù¢ÁöÑlayerÊï∞ kÔºögrowth rate BÔºöbottleneck 4k CÔºöcompression 0.5k ËÆ®ËÆ∫ concat replace sumÔºö seemingly small modification lead to substantially different behaviors of the two network architectures feature reuseÔºöfeature can be accessed anywhere parameter efficientÔºöÂêåÊ†∑ÂèÇÊï∞ÈáèÔºåtest accÊõ¥È´òÔºåÂêåÊ†∑accÔºåÂèÇÊï∞ÈáèÊõ¥Â∞ë deep supervisionÔºöclassifiers attached to every hidden layer weight assign All layers spread their weights over multi inputs (include transition layers) least weight are assigned to the transition layer, indicating that transition layers contain many redundant features, thus can be compressed overall there seems to be concentration towards final feature-maps, suggesting that more high-level features are produced late in the network]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GANomaly]]></title>
    <url>%2F2020%2F05%2F25%2FGANomaly%2F</url>
    <content type="text"><![CDATA[GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training Âä®Êú∫ Anomaly detection highly biased towards one class (normal) insufficient sample size of the other class (abnormal) semi-supervised learning detecting the unknown/unseen anomaly case trained on normal samples tested on normal and abnormal samples encoder-decoder-encoder minimizing the distance between the images and the latent vectors a larger distance metric ËÆ∫ÁÇπ supervised approaches heavily depend on large, labeled datasets Generative Adversarial Networks (GAN) have emerged as a leading methodology across both unsupervised and semi-supervised problems reconstruction-based anomaly techniques Overall prior work strongly supports the hypothesis that the use of autoencoders and GAN ÊñπÊ≥ï GAN unsupervised to generate realistic images compete generator tries to generate an image, decoder- alike network, map input to latent space discriminator decides whether the generated image is a real or a fake, classical classification architecture, reading an input image, and determining its validity Adversarial Auto-Encoders (AAE) encoder + decoder reconstruction: maps the input to latent space and remaps back to input data space train autoencoders with adversarial setting inverse mapping with the additional use of an encoder, a vanilla GAN network is capable of learning inverse mapping model learns both the normal data distribution and minimizes the output anomaly score two encoder, one decoder, a discriminator encoder convolutional layers followed by batch-norm and leaky ReLU() activation compress to a vector z decoder convolutional transpose layers, ReLU() activation and batch-norm a tanh layer at the end 2nd encoder with the same architectural but different parametrization discriminator DCGAN discriminator Adversarial Loss ‰∏çÊòØÂü∫‰∫éGANÁöÑtraditional 0/1 ouput ËÄåÊòØÈÄâ‰∫Ü‰∏Ä‰∏™‰∏≠Èó¥Â±ÇÔºåËÆ°ÁÆórealÔºèfake(reconstructed)ÁöÑL2 distance Contextual Loss L1 yields less blurry results than L2 ËÆ°ÁÆóËæìÂÖ•ÂõæÂÉèÂíåÈáçÂª∫ÂõæÂÉèÁöÑL1 distance Encoder Loss an additional encoder loss to minimize the distance of the bottleneck features ËÆ°ÁÆó‰∏§‰∏™È´òÁª¥ÂêëÈáèÁöÑL2 distance Âú®ÊµãËØïÁöÑÊó∂ÂÄôÁî®ÂÆÉÊù•scoring the abnormality]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[resnets]]></title>
    <url>%2F2020%2F05%2F23%2Fresnets%2F</url>
    <content type="text"><![CDATA[overview papers [resnet] ResNet: Deep Residual Learning for Image Recognition [resnext] ResNext: Aggregated Residual Transformations for Deep Neural Networks [resnest] ResNeSt: Split-Attention Networks [revisiting resnets] Revisiting ResNets: Improved Training and Scaling Strategies ResNext: Aggregated Residual Transformations for Deep Neural Networks Âä®Êú∫ new network architecture new building blocks with the same topology propose cardinality increasing cardinality is able to improve classification accuracy is more effective than going deeper or wider classification task ËÆ∫ÁÇπ VGG &amp; ResNetsÔºö stacking building blocks of the same topology deeper reduces the free choices of hyper-parameters Inception models split-transform-merge strategy splitÔºö1x1conv spliting into a few lower-dimensional embeddings transformÔºöa set of specialized filters mergeÔºöconcat approach the representational power of large and dense layers, but at a considerably lower computational complexity modules are customized stage-by-stage our architecture adopts VGG/ResNets‚Äô repeating layers adopts Inception‚Äòs split-transform-merge strategy aggregated by summation cardinalityÔºöthe size of the set of transformationsÔºàsplit pathÊï∞Ôºâ Â§ö‰∫Ü1x1 convÁöÑËÆ°ÁÆóÈáè Â∞ë‰∫Ü3x3 convÁöÑËÆ°ÁÆóÈáè Ë¶ÅÁ¥† Multi-branch convolutional blocks Grouped convolutionsÔºöÈÄöÈÅìÂØπÈΩêÔºåÁ®ÄÁñèËøûÊé• Compressing convolutional networks Ensembling ÊñπÊ≥ï architecture a template module if producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes) when the spatial map is downsampled by a factor of 2, the width of the blocks is multiplied by a factor of 2 grouped convolutionsÔºöÁ¨¨‰∏Ä‰∏™1x1Âíå3x3convÁöÑwidthË¶ÅÊ†πÊçÆCËøõË°åsplit equivalent blocks BN after each conv ReLU after each BN except the last of block ReLU after add r Model Capacity improve accuracy when maintaining the model complexity and number of parameters adjust the width of bottleneck, the according C to maintain capacityÔºöC=1ÁöÑÊó∂ÂÄôÈÄÄÂåñÊàêResNet block ResNeSt: Split-Attention Networks Âä®Êú∫ propose a modular Split-Attention block enables attention across feature-map groups preserve the overall ResNet structure for downstream applications such as object detection and semantic segmentation prove improvement on detection &amp; segmentation tasks ËÆ∫ÁÇπ ResNet simple and modular design limited receptive-field size and lack of cross-channel interaction image classification networks have focused more on group or depth-wise convolution do not transfer well to other tasks isolated representations cannot capture cross-channel relationships a versatile backbone improving performance across multiple tasks at the same time a network with cross-channel representations is desirable a Split-Attention block divides the feature-map into several groups (along the channel dimension) finer-grained subgroups or splits weighted combination featuremap attention mechanismÔºöNiN‚Äôs 1x1 conv Multi-pathÔºöGoogleNet channel-attention mechanismÔºöSE-Net ÁªìÊûÑ‰∏äÔºåÂÖ®Â±Ä‰∏äÁúãÔºåÊ®°‰ªøResNextÔºåÂºïÂÖ•cardinalityÂíågroup convÔºåÂ±ÄÈÉ®‰∏äÁúãÔºåÊØè‰∏™groupÂÜÖÈÉ®ÁªßÁª≠ÂàÜÁªÑÔºåÁÑ∂ÂêéÊ®°‰ªøSK-NetÔºåËûçÂêàÂ§ö‰∏™ÂàÜÊîØÁöÑsplit-attentionÔºåÂ§ßgroup‰πãÈó¥concatÔºåËÄå‰∏çÊòØResNextÁöÑaddÔºåÂÜçÁªè1x1 convË∞ÉÊï¥Áª¥Â∫¶Ôºåadd id path ÊñπÊ≥ï Split-Attention block enables feature-map attention across different feature-map groups within a blockÔºöcontrolled by cardinality within a cardinal groupÔºöintroduce a new radix hyperparameter R indicating the number of splits split-attention Â§ö‰∏™in-group branchÁöÑinputËæìÂÖ•ËøõÊù• fusionÔºöÂÖàÂÅöelement-wise summation channel-wise global contextual informationÔºöÂÅöglobal average pooling ÈôçÁª¥ÔºöDense-BN-ReLU ÂêÑÂàÜÊîØDense(the attention weight function)ÔºöÂ≠¶‰π†ÂêÑËá™ÁöÑÈáçË¶ÅÊÄßÊùÉÈáç channel-wise soft attentionÔºöÂØπÂÖ®ÈÉ®ÁöÑdenseÂÅösoftmax Âä†ÊùÉÔºöÂéüÂßãÁöÑÂêÑÂàÜÊîØinput‰∏éÂä†ÊùÉÁöÑdenseÂÅö‰πòÊ≥ï ÂíåÔºöÂä†ÊùÉÁöÑÂêÑÂàÜÊîØadd r=1ÔºöÈÄÄÂåñÊàêSE-blockaverage pooling shortcut connection for blocks with a strided convolution or combined convolution-with-pooling can be applied to the id concat average pooling downsampling for dense prediction tasksÔºöit becomes essential to preserve spatial information former work tend to use strided 3x3 conv we use an average pooling layer with 3x3 kernel 2x2 average pooling applied to strided shortcut connection before 1x1 conv Revisiting ResNets: Improved Training and Scaling Strategies Âä®Êú∫ disentangle the three aspects model architecture training methodology scaling strategies improve ResNets to SOTA design a family of ResNet architectures, ResNet-RS use improved training and scaling strategies and combine minor architecture changes Âú®ImageNet‰∏äÊâìË¥•efficientNet Âú®ÂçäÁõëÁù£‰∏äÊâìË¥•efficientNet-noisystudent ËÆ∫ÁÇπ ImageNet‰∏äÊ¶úÂ§ßÊ≥ï Architecture ‰∫∫Â∑•Á≥ªÂàóÔºöAlexNetÔºåVGGÔºåResNetÔºåInceptionÔºåResNeXt NASÁ≥ªÂàóÔºöNasNet-AÔºåAmoebaNet-AÔºåEfficientNet Training and Regularization Methods regularization methods dropoutÔºålabel smoothingÔºåstochastic depthÔºådropblockÔºådata augmentation significantly improve generalization when training more epochs training learning rate schedules Scaling Strategies model dimensionÔºöwidthÔºådepthÔºåresolution efficientNetÊèêÂá∫ÁöÑÂùáË°°Â¢ûÈïøÔºåÂú®Êú¨Êñá‰∏≠shows sub-optimal for both resnet and efficientNet Additional Training Data pretraining on larger dataset semi-supervised the performance of a vision model architectureÔºömost research focus on training methods and scaling strategyÔºöless publicized but critical unfairÔºö‰ΩøÁî®modern training methodÁöÑÊñ∞Êû∂ÊûÑ‰∏é‰ΩøÁî®dated methodsÁöÑËÄÅÁΩëÁªúÁõ¥Êé•ÂØπÊØî we focus on the impact of training methods and scaling strategies training methodsÔºö We survey the modern training and regularization techniques ÂèëÁé∞ÂºïÂÖ•ÂÖ∂‰ªñÊ≠£ÂàôÊñπÊ≥ïÁöÑÊó∂ÂÄôÈôç‰Ωé‰∏ÄÁÇπweight decayÊúâÂ•ΩÂ§Ñ scaling strategiesÔºö We offer new perspectives and practical advice on scaling ÂèØËÉΩÂá∫Áé∞ËøáÊãüÂêàÁöÑÊó∂ÂÄôÂ∞±Âä†depthÔºåÂê¶ÂàôÂÖàÂä†ÂÆΩ resolutionÊÖ¢ÁÇπÂ¢ûÈïøÔºåmore slowly than prior works ‰ªéaccÂõæÂèØ‰ª•ÁúãÂà∞ÔºöÊàë‰ª¨ÁöÑscaling strategies‰∏éÁΩëÁªúÁªìÊûÑÁöÑlightweight changeÊ≠£‰∫§ÔºåÊòØadditiveÁöÑ re-scaled ResNets, ResNet-RS ‰ªÖimprove training &amp; scaling strategyÂ∞±ËÉΩÂ§ßÂπÖÂ∫¶Ê∂®ÁÇπ combine minor architectural changesËøõ‰∏ÄÊ≠•Ê∂®ÁÇπ ÊñπÊ≥ï architecture use ResNet with two widely used architecture changes ResNet-D stemÁöÑ7x7convÊç¢Êàê3‰∏™3x3conv stemÁöÑmaxpoolingÂéªÊéâÔºåÊØè‰∏™stageÁöÑÈ¶ñ‰∏™3x3convË¥üË¥£stride2 residual path‰∏äÂâç‰∏§‰∏™Âç∑ÁßØÁöÑstride‰∫íÊç¢ÔºàÂú®3x3‰∏ä‰∏ãÈááÊ†∑Ôºâ id path‰∏äÁöÑ1x1 s2convÊõøÊç¢Êàê2x2 s2ÁöÑavg pooling+1x1conv SE in bottleneck use se-ratio of 0.25 training methods match the efficientNet setup train for 350 epochs use cosine learning rate instead of exponential decay RandAugment instead of AutoAugment use Momentum optimizer instead of RMSProp regularization weight decay label smoothing dropout stochastic depth data augmentation we use RandAugment EfficientNet use AutoAugment which slightly outperforms RandAugment hyperÔºö droprate increase the regularization as the model size increase to limit overfitting label smoothing = 0.1 weight decay = 4e-5 improved training methods additive study ÊÄª‰Ωì‰∏äÁúãÈÉΩÊòØadditiveÁöÑ increase training epochsÂú®Ê∑ªÂä†regularization methodsÁöÑÂâçÊèê‰∏ãÊâç‰∏çhurtÔºåÂê¶Âàô‰ºöoverfitting dropoutÂú®‰∏çÈôç‰Ωéweight decayÁöÑÊÉÖÂÜµ‰∏ã‰ºöhurt weight decay Â∞ëÈáè/Ê≤°Êúâregularization methodsÁöÑÊÉÖÂÜµ‰∏ãÔºöÂ§ßweight decayÈò≤Ê≠¢ËøáÊãüÂêàÔºå1e-4 Â§ö/Âº∫regularization methodsÁöÑÊÉÖÂÜµ‰∏ãÔºöÈÄÇÂΩìÂáèÂ∞èweight decayËÉΩÊ∂®ÁÇπÔºå4e-5 improved scaling strategies search space width multiplierÔºö[0.25, 0.5, 1.0, 1.5, 2.0] depthÔºö[26, 50, 101, 200, 300, 350, 400] resolutionÔºö[128, 160, 224, 320, 448] increase regularization as model size increase observe 10/100/350 epoch regime we found that the best scaling strategies depends on training regime strategy1Ôºöscale depth Depth scaling outperforms width scaling for longer epoch regimes width scaling is preferable for shorter epoch regimes scaling widthÂèØËÉΩ‰ºöÂºïËµ∑overfittingÔºåÊúâÊó∂ÂÄô‰ºöhurt performance depth scalingÂºïÂÖ•ÁöÑÂèÇÊï∞Èáè‰πüÊØîwidthÂ∞è strategy2Ôºöslow resolution scaling efficientNets/resNeSt lead to very large images our experimentsÔºöÂ§ßÂèØ‰∏çÂøÖ ÂÆûÈ™å]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Receptive Field]]></title>
    <url>%2F2020%2F05%2F18%2FReceptive-Field%2F</url>
    <content type="text"><![CDATA[ÁªºËø∞ ÊÑüÂèóÈáé Èô§‰∫ÜÂç∑ÁßØÂíåÊ±†ÂåñÔºåÂÖ∂‰ªñÂ±ÇÂπ∂‰∏çÂΩ±ÂìçÊÑüÂèóÈáéÂ§ßÂ∞è ÊÑüÂèóÈáé‰∏éÂç∑ÁßØÊ†∏Â∞∫ÂØ∏kernel_sizeÂíåÊ≠•ÈïøstrideÊúâÂÖ≥ ÈÄíÂΩíËÆ°ÁÆóÔºö N\_RF = kernel\_size + (cur\_RF-1)*stride ÂÖ∂‰∏≠$cur_RF$ÊòØÂΩìÂâçÂ±ÇÔºàstart from 1ÔºâÔºå$kernel_size$„ÄÅ$stride$ÊòØÂΩìÂâçÂ±ÇÂèÇÊï∞Ôºå$N_RF$ÊòØ‰∏ä‰∏ÄÂ±ÇÁöÑÊÑüÂèóÈáé„ÄÇ ÊÑüÂèóÈáéËÆ°ÁÆóÂô® https://fomoro.com/research/article/receptive-field-calculator Understanding the Effective Receptive Field in Deep Convolutional Neural Networks Âä®Êú∫ effective receptive field the effect of nonlinear activations, dropout, sub-sampling and skip connections on it ËÆ∫ÁÇπ it is critical for each output pixel to have a big receptive field, such that no important information is left out when making the prediction deeper networkÔºöincrease the receptive field size linearly Sub-samplingÔºöincreases the receptive field size multiplicatively it is easy to see that pixels at the center of a receptive field have a much larger impact on an outputÔºöÂâçÂêë‰º†Êí≠ÁöÑÊó∂ÂÄôÔºå‰∏≠Èó¥‰ΩçÁΩÆÁöÑÂÉèÁ¥†ÁÇπÊúâÊõ¥Â§öÊù°pathÈÄöÂêëoutput ÊñπÊ≥ïÁúã‰∏çÊáÇÁõ¥Êé•ÁúãÁªìËÆ∫ dropout does not change the Gaussian ERF shape Subsampling and dilated convolutions turn out to be effective ways to increase receptive field size quickly Skip-connections make ERFs smaller ERFs are Gaussian distributed uniformlyÂíåÈöèÊú∫ÂàùÂßãÂåñÈÉΩÊòØperfect Gaus- sian shapes Âä†‰∏äÈùûÁ∫øÊÄßÊøÄÊ¥ªÂáΩÊï∞‰ª•ÂêéÊòØnear Gaussian shapes with different nonlinearities $\sqrt n$ absolute growth and $1/\sqrt n$ relative shrinkageÔºöRFÊòØÈöèÁùÄlayerÁ∫øÊÄßÂ¢ûÈïøÁöÑÔºåERFÂú®log‰∏ä0.56ÁöÑÊñúÁéáÔºåÁ∫¶Á≠â‰∫é$\sqrt n$ Subsampling &amp; dilated convolution increases receptive field The reference baseline is a convnet with 15 dense convolution layers SubsamplingÔºöreplace 3 of the 15 convolutional layers with stride-2 convolution dilatedÔºöreplace them with dilated convolution with factor 2,4 and 8Ôºårectangular ERF shape evolves during training as the networks learns, the ERF gets bigger, and at the end of training is significantly larger than the initial ERF classification 32*32 cifar 10 theoretical receptive field of our network is actually 74 √ó 74 segmentation CamVid dataset the theoretical receptive field of the top convolutional layer units is quite big at 505 √ó 505 ÂÆûÈôÖÁöÑERFÈÉΩÂæàÂ∞èÔºåÈÉΩÊ≤°Âà∞ÂéüÂõæÂ§ßÂ∞è increase the effective receptive field New InitializationÔºö makes the weights at the center of the convolution kernel to have a smaller scale, and the weights on the outside to be larger 30% speed-up of training ÂÖ∂‰ªñÊïàÊûú‰∏çÊòéÊòæ Architecturalchanges sparsely connect each unit to a larger area dilated convolution or even not grid-like g]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SqueezeNet]]></title>
    <url>%2F2020%2F05%2F18%2FSqueezeNet%2F</url>
    <content type="text"><![CDATA[SQUEEZENET: ALEXNET-LEVEL ACCURACY WITH 50X FEWER PARAMETERS AND &lt;0.5MB MODEL SIZE Âä®Êú∫ Smaller CNN achieve AlexNet-level accuracy model compression ËÆ∫ÁÇπ model compression SVD sparse matrix quantization (to 8 bits or less) CNN microarchitecture extensively 3x3 filters 1x1 filters higher level building blocks bypass connections automated designing approaches this paper eschew automated approaches propose and evaluate the SqueezeNet architecture with and without model compression explore the impact of design choices ÊñπÊ≥ï architectural design strategy Replace 3x3 filters with 1x1 filters Decrease the number of input channels to 3x3 filters ÔºàsqueezeÔºâ Downsample late in the network so that convolution layers have large activation mapsÔºölarge activation maps (due to delayed downsampling) can lead to higher classification accuracy the fire module squeezeÔºö1x1 convs expandÔºömix of 1x1 and 3x3 convs, same padding relu concatenate the SqueezeNet a standalone convolution layer (conv1) followed by 8 Fire modules (fire2-9) ending with a final conv layer (conv10) stride2 max-pooling after layers conv1, fire4, fire8, and conv10 dropout with a ratio of 50% is applied after the fire9 module GAP understand the impact each Fire module has three dimensional hyperparameters, to simplifyÔºö define $base_e$Ôºöthe number of expand filters in the first Fire module for layer iÔºö$e_i=base_e + (incr_e*[\frac{i}{freq}])$ expand ratio $pct_{3x3}$Ôºöthe percentage of 3x3 filters in expand layers squeeze ratio $SR$Ôºöthe number of filters in the squeeze layerÔºèthe number of filters in the expnad layer normal settingÔºö$base_e=128, incre_e=128, pct_{3x3}=0.5, freq=2, SR=0.125$ SR increasing SR leads to higher accuracy and larger model size Accuracy plateaus at 86.0% with SR=0.75 further increasing provides no improvement pct increasing pct leads to higher accuracy and larger model size Accuracy plateaus at 85.6% with pct=50% further increasing provides no improvement bypass Vanilla simple bypassÔºöwhen in &amp; out channels have the same dimensions complex bypassÔºöincludes a 1x1 convolution layer alleviate the representational bottleneck introduced by squeeze layers both yielded accuracy improvements simple bypass enabled higher accuracy]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hausdorff Distance]]></title>
    <url>%2F2020%2F05%2F14%2FHausdorff-Distance%2F</url>
    <content type="text"><![CDATA[Reducing the Hausdorff Distance in Medical Image Segmentation with Convolutional Neural Networks Âä®Êú∫ novel loss function to reduce HD directly propose three methods 2D&amp;3DÔºåultra &amp; MR &amp; CT lead to approximately 18 ‚àí 45% reduction in HD without degrading other segmentation performance criteria ËÆ∫ÁÇπ HD is one of the most informative and useful criteria because it is an indicator of the largest segmentation error current segmentation algorithms rarely aim at minimizing or reducing HD directly HD is determined solely by the largest error instead of the overall segmentation performance HD‚Äòs sensitivity to noise and outliers ‚Äî&gt; modified version the optimization diffculty thus we propose an ‚ÄúHD- inspired‚Äù loss function ÊñπÊ≥ï denotations probabilityÔºö$q$ binary maskÔºö$\bar p$„ÄÅ$\bar q$ boundaryÔºö$\delta p$„ÄÅ$\delta q$ single hdÔºö$hd(\bar p, \bar q)$„ÄÅ$hd(\bar q, \bar p)$ based on distance transforms distance map $d_p$Ôºödefine the distance map as the unsigned distance to the boundary $\delta p$ DT_X[i,j] = min_{[k,l]\in X}d([i,j], [k,l]) Ë∑ùÁ¶ªÂú∫ÂÆö‰πâ‰∏∫ÔºöÊØè‰∏™ÁÇπÂà∞ÁõÆÊ†áÂå∫Âüü(X)ÁöÑË∑ùÁ¶ªÁöÑÊúÄÂ∞èÂÄº HD based on DTÔºö hd_{DT}(\delta p, \delta q) = max((\bar p \triangle \bar q)\circ d_p)\\ \bar p \triangle \bar q = |\bar p - \bar q| finally haveÔºö HD_{DT}(\delta p, \delta q) = max(hd_{DT}(\delta p, \delta q), hd_{DT}(\delta q, \delta p)) modified loss version of HDÔºö Loss_{DT}(q,p) = \frac{1}{|\Omega|}\sum_{\Omega}((p-q)^2\circ(d_p^{\alpha}+d_q^{\alpha})) penalizely focus on areas instead of single point $\alpha$ determines how strongly we penalize larger errors use possibility instead of thresholded value use $(p-q)^2$ instead of $|p-q|$ correlations $HD_{DT}$ÔºöPearson correlation coefficient above 0.99 $Loss_{DT}$ÔºöPearson correlation coefficient above 0.93 drawback high computational cost especially in 3D $q$ changes along with training process thus $d_q$ changes while $d_p$ remains modified one-sided HD (OS)Ôºö Loss_{DT-OS}(q,p) = \frac{1}{|\Omega|}\sum_{\Omega}((p-q)^2\circ(d_p^{\alpha})) HD using Morphological Operations morphological erosionÔºö S \ominus B = \{z\in \Omega | B(z) \subseteq S\} ËÖêËöÄÊìç‰ΩúÂÆö‰πâ‰∏∫ÔºöÂú®ÂéüÂßã‰∫åÂÄºÂåñÂõæÁöÑÂâçÊôØÂå∫ÂüüÔºå‰ª•ÊØè‰∏™ÂÉèÁ¥†‰∏∫‰∏≠ÂøÉÁÇπÔºårun structure element block BÔºåÂ¶ÇÊûúBÂÆåÂÖ®Âú®ÂéüÂõæÂÜÖÔºåÂàôÂΩìÂâç‰∏≠ÂøÉÁÇπÂú®ËÖêËöÄÂêé‰πüÊòØÂâçÊôØ„ÄÇ HD based on erosionÔºö HD_{ER}(\delta p, \delta q)=2r^*\\ where\ r^* = min_r \{(\bar p \triangle \bar q) \ominus B_r = \varnothing\} $HD_{ER}$ is a lower bound of the true value can be computed more efficiently using convolutional operations modifid loss versionÔºö Loss_{ER}(q,p) = \frac{1}{|\Omega|}\sum_k \sum_{\Omega}((p-q)^2 \ominus_k B)k^{\alpha} k successive erosions cross-shaped kernel whose elements sum to one followed by a soft thresholding at 0.50 correlations $HD_{ER}$ÔºöPearson correlation coefficient above 0.91 $Loss_{ER}$ÔºöPearson correlation coefficient above 0.83 HD using circular-shaped convolutional kernel circular-shaped kernel HD based on circular-shaped kernelÔºö hd_{CV}(\delta p, \delta q)=max(r_1, r_2)\\ where \ r_1=max_r (max_{\Omega}f_h(\bar p ^C * B_r)\circ(\bar q \backslash \bar p))\\ where \ r_2=max_r (max_{\Omega}f_h(\bar p * B_r)\circ(\bar p \backslash \bar q))\\ $\bar p^C$Ôºöcomplement Ë°•ÈõÜ $f_h$Ôºöhard thresholding setting all values below 1 to zero modified loss versionÔºö Loss_{CV}(q,p)=\frac{1}{|\Omega|}\sum_{r\in R}r^{\alpha}\sum_{\Omega}[f_s(Br*\bar p^C)\circ f_{\bar q\backslash \bar p} + f_s(B_r * \bar p) \circ f_{\bar p \backslash \bar q}\\ +f_s(Br*\bar q^C)\circ f_{\bar p\backslash \bar q} + f_s(B_r * \bar q) \circ f_{\bar q \backslash \bar p}] soft thresholding f_{\bar p\backslash \bar q} = (p-q)^2*p correlations $HD_{CV}$ÔºöPearson correlation coefficient above 0.99 $Loss_{CV}$ÔºöPearson correlation coefficient above 0.88 computationÔºö kernel size $HD_{ER}$ is computed using small fixed convolutional kernels (of size 3) $Loss_{CV}$ require applying filters of increasing size(we use a maximum kernel radius of 18 pixels in 2D and 9 voxels in 3D) steps choose R based on the expected range of segmentation errors set R = {3, 6, . . . 18} for 2D images and R = {3,6,9} for 3D training standard Unet augment our HD-based loss term with a DSC loss term for more stable training reweight both loss after every epoch d]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SE block]]></title>
    <url>%2F2020%2F04%2F30%2FSE-block%2F</url>
    <content type="text"><![CDATA[ÁªºËø∞ÂõæÂÉèÁâπÂæÅÁöÑÊèêÂèñËÉΩÂäõÊòØCNNÁöÑÊ†∏ÂøÉËÉΩÂäõÔºåËÄåSE blockÂèØ‰ª•Ëµ∑Âà∞‰∏∫CNNÊ†°ÂáÜÈááÊ†∑ÁöÑ‰ΩúÁî®„ÄÇ Ê†πÊçÆÊÑüÂèóÈáéÁêÜËÆ∫ÔºåÁâπÂæÅÁü©Èòµ‰∏ªË¶ÅÊù•Ëá™‰∫éÊ†∑Êú¨ÁöÑ‰∏≠Â§ÆÂå∫ÂüüÔºåÂ§ÑÂú®ËæπÁºò‰ΩçÁΩÆÁöÑÈÖíÁì∂ÁöÑÂõæÂÉèÁâπÂæÅÂæàÂ§ßÊ¶ÇÁéá‰ºöË¢´poolingÂ±ÇÊäõÂºÉÊéâ„ÄÇËÄåSE blockÁöÑÂä†ÂÖ•Â∞±ÂèØ‰ª•ÈÄöËøáÊù•Ë∞ÉÊï¥ÁâπÂæÅÁü©ÈòµÔºåÂ¢ûÂº∫ÈÖíÁì∂ÁâπÂæÅÁöÑÊØîÈáçÔºåÊèêÈ´òÂÆÉÁöÑËØÜÂà´Ê¶ÇÁéá„ÄÇ [SE-Net] Squeeze-and-Excitation Networks [SC-SE] Concurrent Spatial and Channel ‚ÄòSqueeze &amp; Excitation‚Äô in Fully Convolutional Networks [CMPE-SE] Competitive Inner-Imaging Squeeze and Excitation for Residual Network SENet: Squeeze-and-Excitation Networks Âä®Êú∫ prior research has investigated the spatial component to achieve more powerful representations we focus on the channel relationship instead SE-blockÔºöadaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels enhancing the representational power in a computationally efficient manner ËÆ∫ÁÇπ stronger networkÔºö deeper NiN-like bocks cross-channel correlations in prior work mapped as new combinations of features through 1x1 conv concentrated on the objective of reducing model and computational complexity In contrast, we found this mechanism can ease the learning process and significantly enhance the representational power of the network Attention Attention can be interpreted as a means of biasing the allocation of available computational resources towards the most informative components Some works provide interesting studies into the combined use of spatial and channel attention ÊñπÊ≥ï SE-block The channel relationships modelled by convolution are inherently implicit and local we would like to provide it with access to global information squeezeÔºöusing global average pooling excitationÔºönonlinear &amp; non-mutually-exclusive using sigmoid bottleneckÔºöa dimensionality-reduction layer $W_1$ with reduction ratio $r$ and ReLU and a dimensionality-increasing layer $W_2$ $s = F_{ex}(z,W) = \sigma (W_2 \delta(W_1 z))$ integration insert after the non-linearity following each convolution inceptionÔºötake the transformation $F_{tr}$ to be an entire Inception module residualÔºötake the transformation $F_{tr}$ to be the non-identity branch of a residual module model and computational complexity ResNet50 vs. SE-ResNet50Ôºö0.26% relative increase GFLOPs approaching ResNet10‚Äôs accuracy the additional parameters result solely from the two FC layers, among which the final stage FC claims the majority due to being performed across the greatest number of channels the costly final stage of SE blocks could be removed at only a small cost in performance ablations FC removing the biases of the FC layers in the excitation facilitates the modelling of channel dependencies reduction ratio performance is robust to a range of reduction ratios In practice, using an identical ratio throughout a network may not be optimal due to the distinct roles performed by different layers squeeze global average pooling vs. global max poolingÔºöaverage pooling slightly better excitation Sigmoid vs. ReLU vs. tanhÔºö tanhÔºöslightly worse ReLUÔºödramatically worse stages each stages brings benefits combination make even better integration strategy fairly robust to their location, provided that they are applied prior to branch aggregation inside the residual unitÔºöfewer channels, fewer parameters, comparable accuracy primitive understanding squeeze the use of global information has a significant influence on the model performance excitation the distribution across different classes is very similar at the earlier layers (general features) the value of each channel becomes much more class-specific at greater depth SE_5_2 exhibits an interesting tendency towards a saturated state in which most of the activations are close to one SE_5_3 exhibits a similar pattern emerges over different classes, up to a modest change in scale suggesting that SE_5_2 and SE_5_3 are less important than previous blocks in providing recalibration to the network (thus can be removed) APPENDIX Âú®ImageNet‰∏äSOTAÁöÑÊ®°ÂûãÊòØSENet-154Ôºåtop1-errÊòØ18.68ÔºåË¢´Ê†áËÆ∞Âú®‰∫ÜefficientNetËÆ∫ÊñáÁöÑÊäòÁ∫øÂõæ‰∏ä SE-ResNeXt-152Ôºà64x4dÔºâ input=(224,224)Ôºötop1-errÊòØ18.68 input=320/299Ôºötop1-errÊòØ17.28 further difference each bottleneck building blockÁöÑÁ¨¨‰∏Ä‰∏™1x1 convsÁöÑÈÄöÈÅìÊï∞ÂáèÂçä stemÁöÑÁ¨¨‰∏Ä‰∏™7x7convÊç¢Êàê‰∫Ü3‰∏™ËøûÁª≠ÁöÑ3x3 conv 1x1ÁöÑs2 convÊç¢Êàê‰∫Ü3x3ÁöÑs2 conv fc‰πãÂâçÊ∑ªÂä†dropout layer label smoothing ÊúÄÂêéÂá†‰∏™training epochÂ∞ÜBNÂ±ÇÁöÑÂèÇÊï∞ÂÜª‰ΩèÔºå‰øùËØÅËÆ≠ÁªÉÂíåÊµãËØïÁöÑÂèÇÊï∞‰∏ÄËá¥ 64 GPUsÔºåbatch size=2048Ôºà32 per GPUÔºâ initial lr=1.0 SC-SE: Concurrent Spatial and Channel ‚ÄòSqueeze &amp; Excitation‚Äô in Fully Convolutional Networks Âä®Êú∫ image segmentation task ‰∏äÈù¢SE-NetÊèêÂá∫Êù•‰∏ªË¶ÅÊòØÈíàÂØπÂàÜÁ±ª three variants of SE modules squeezing spatially and exciting channel-wise (cSE) squeezing channel-wise and exciting spatially (sSE) concurrent spatial and channel squeeze &amp; excitation (scSE) integrate within three different state-of-the- art F-CNNs (DenseNet, SD-Net, U-Net) ËÆ∫ÁÇπ F-CNNs have become the tool of choice for many image segmentation tasks coreÔºöconvolutions that capturing local spatial pattern along all input channels jointly SE block factors out the spatial dependency by global average pooling to learn a channel specific descriptor (later refered to as cSE /channel-SE) while for image segmentation, we hypothesize that the pixel-wise spatial information is more informative thus we propose sSE(spatial SE) and scSE(spatial and channel SE) can be seamlessly integrated by placing after every encoder and decoder block ÊñπÊ≥ï cSE GAPÔºöembeds the global spatial information into a vector FC-ReLU-FC-SigmoidÔºöadaptively learns the importance recalibrate sSE 1x1 convÔºögenerating a projection tensor representing the linearly combined representation for all channels C for a spatial location (i,j) SigmoidÔºörescale recalibrate scSE by element-wise addition encourages the network to learn more meaningful feature maps ‚Äî‚Äî‚Äî- relevant both spatially and channel-wise ÂÆûÈ™å F-CNN architecturesÔºö 4 encoder blocks, one bottleneck layer, 4 decoder blocks and a classification layer class imbalanceÔºömedian frequency balancing ce dice cmpÔºöscSE &gt; sSE &gt; cSE &gt; vanilla Â∞èÂå∫ÂüüÁ±ªÂà´ÁöÑÂàÜÂâ≤ÔºåËßÇÂØüÂà∞‰ΩøÁî®cSEÂèØËÉΩ‰ºöÂ∑Æ‰∫évanillaÔºö might have got overlooked by only exciting the channels ÂÆöÊÄßÂàÜÊûêÔºö ‰∏Ä‰∫õunder segmentedÁöÑÂú∞ÊñπÔºåscSE improves with the inclusion ‰∏Ä‰∫õover segmentedÁöÑÂú∞ÊñπÔºåscSE rectified the result Competitive Inner-Imaging Squeeze and Excitation for Residual Network Âä®Êú∫ for residual network the residual architecture has been proved to be diverse and redundant model the competition between residual and identity mappings make the identity flow to control the complement of the residual feature maps ËÆ∫ÁÇπ For analysis of ResNet, with the increase in depth, the residual network exhibits a certain amount of redundancy with the CMPE-SE mechanism, it makes residual mappings tend to provide more efficient supplementary for identity mappings ÊñπÊ≥ï ‰∏ªË¶ÅÊèêÂá∫‰∫Ü‰∏âÁßçÂèò‰ΩìÔºö Á¨¨‰∏Ä‰∏™Âèò‰ΩìÔºö ‰∏§‰∏™ÂàÜÊîØidÂíåresÂàÜÂà´GAPÂá∫‰∏Ä‰∏™vectorÔºåÁÑ∂Âêéfc reduct by ratioÔºåÁÑ∂ÂêéconcatÔºåÁÑ∂Âêéchannel back Implicitly, we can believe that the winning of the identity channels in this competition results in less weights of the residual channels Á¨¨‰∫å‰∏™Âèò‰ΩìÔºö ‰∏§ÁßçÊñπÊ°à 2x1 convsÔºöÂØπ‰∏ä‰∏ãÁõ∏Â∫î‰ΩçÁΩÆÁöÑÂÖÉÁ¥†Ê±Çavg 1x1 convsÔºöÂØπÂÖ®ÈÉ®ÂÖÉÁ¥†Ê±ÇavgÔºåÁÑ∂Âêéflatten Á¨¨‰∏â‰∏™Âèò‰ΩìÔºö ‰∏§ËæπÁöÑchannel-wise vectorÂè†Ëµ∑Êù•ÔºåÁÑ∂ÂêéreshapeÊàêÁü©ÈòµÂΩ¢ÂºèÔºåÁÑ∂Âêé3x3 convÔºåÁÑ∂Âêéflatten ÊØîËæÉÊâØÔºå‰∏çÊµ™Ë¥πÊó∂Èó¥ÂàÜÊûê‰∫Ü„ÄÇ]]></content>
      <tags>
        <tag>papers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cv2&numpy&tobeadded]]></title>
    <url>%2F2020%2F04%2F19%2Fcv2-numpy-tobeadded%2F</url>
    <content type="text"><![CDATA[Áü©Èòµ‰πòÊ≥ï np.dot(A,B)ÔºöÁúüÊ≠£ÁöÑÁü©Èòµ‰πòÊ≥ï np.multiply(A,B) &amp; npÈáçËΩΩÁöÑ*Ôºöelement-wise productÔºåÁü©Èòµ‰∏≠ÂØπÂ∫îÂÖÉÁ¥†Áõ∏‰πò cvÁöÑA.dot(B) &amp; cvÈáçËΩΩÁöÑ*ÔºöÁúüÊ≠£ÁöÑÁü©Èòµ‰πòÊ≥ï cvÁöÑA.mul(B) Ôºöelement-wise productÔºåÁü©Èòµ‰∏≠ÂØπÂ∫îÂÖÉÁ¥†Áõ∏‰πò ÂõæÂÉèÊóãËΩ¨ ÈÄöËøá‰ªøÂ∞ÑÁü©Èòµcv2.getRotationMatrix2DÂíå‰ªøÂ∞ÑÂèòÊç¢ÂáΩÊï∞cv2.warpAffineÊù•ÂÆûÁé∞ srcÔºöËæìÂÖ•ÂõæÂÉè MÔºöÂèòÊç¢Áü©Èòµ dsizeÔºöËæìÂá∫ÂõæÂÉèÁöÑÂ§ßÂ∞èÔºàÂü∫‰∫éÂõæÂÉèÂéüÁÇπË£ÅÂâ™Ôºâ flagsÔºöÊèíÂÄºÊñπÊ≥ï borderModeÔºöËæπÁïåÂÉèÁ¥†Ê®°Âºè borderValueÔºöËæπÁïåÂ°´ÂÖÖÂÄºÔºåÈªòËÆ§‰∏∫0 cv2.getRotationMatrix2D(center, angle, scale)ÔºöËøîÂõû‰∏Ä‰∏™2x3ÁöÑÂèòÊç¢Áü©Èòµ centerÔºöÊóãËΩ¨‰∏≠ÂøÉ angleÔºöÊóãËΩ¨ËßíÂ∫¶ÔºåÊ≠£ÂÄºÊòØÈÄÜÊó∂ÈíàÊóãËΩ¨ scaleÔºöÁº©ÊîæÂõ†Â≠ê cv2.warpAffine(src, M, dsize[, dst[, flags[, borderMode[, borderValue]]]]))ÔºöËøîÂõûÂèòÊç¢ÂêéÁöÑÂõæÂÉè srcÔºöËæìÂÖ•ÂõæÂÉè MÔºöÂèòÊç¢Áü©Èòµ dsizeÔºöËæìÂá∫ÂõæÂÉèÁöÑÂ§ßÂ∞èÔºàÂü∫‰∫éÂõæÂÉèÂéüÁÇπË£ÅÂâ™Ôºâ flagsÔºöÊèíÂÄºÊñπÊ≥ï borderModeÔºöËæπÁïåÂÉèÁ¥†Ê®°Âºè borderValueÔºöËæπÁïåÂ°´ÂÖÖÂÄºÔºåÈªòËÆ§‰∏∫0 12345678910111213141516171819def rotate_img(angle, img, interpolation=cv2.INTER_LINEAR, points=[]): h, w = img.shape rotataMat = cv2.getRotationMatrix2D((w/2, h/2), math.degrees(angle), 1) # rotate_img1: ËæìÂá∫ÂõæÂÉèÂ∞∫ÂØ∏‰∏çÂèòÔºåË∂ÖÂá∫ÂéüÂõæÂÉèÈÉ®ÂàÜË¢´cutÊéâ rotate_img1 = cv2.warpAffine(img, rotataMat, dsize=(w, h), flags=interpolation, borderMode=cv2.BORDER_CONSTANT, borderValue=0) # rotate_img2: ËæìÂá∫ÂõæÂÉèÂ∞∫ÂØ∏ÂèòÂ§ßÔºå‰øùÁïôË∂ÖÂá∫ÂéüÂõæÂÉèÈÉ®ÂàÜÔºåÊñ∞ÁöÑÂùêÊ†áÂéüÁÇπ‰øùËØÅÊóãËΩ¨‰∏≠ÂøÉ‰ªçÊóß‰Ωç‰∫éÂõæÂÉè‰∏≠ÂøÉ new_h = int(w*math.fabs(math.sin(angle)) + h*math.fabs(math.cos(angle))) new_w = int(h*math.fabs(math.sin(angle)) + w*math.fabs(math.cos(angle))) rotataMat[0, 2] += (new_w - w) / 2 rotataMat[1, 2] += (new_h - h) / 2 rotate_img2 = cv2.warpAffine(img, rotataMat, dsize=(new_w, new_h), flags=interpolation, borderMode=cv2.BORDER_CONSTANT, borderValue=0) # ÂùêÊ†áÁÇπÁöÑÂèòÊç¢ rotated_points = [] for point in points: point = rotataMat.dot([[point[0]], [point[1]], [1]]) rotated_points.append((int(point[0]), int(point[1]))) return rotate_img2, rotated_points ‰ΩøÁî®tipsÔºö Â¶ÇÊûú‰∏ç‰øÆÊîπ‰ªøÂ∞ÑÂèòÊç¢Áü©ÈòµÁöÑÂπ≥ÁßªÂèÇÊï∞ÔºåÂùêÊ†áÂéüÁÇπÁöÑ‰ΩçÁΩÆ‰∏çÂèëÁîüÊîπÂèò dsizeÊåáÂÆöÁöÑËæìÂá∫ÂõæÂÉèÊòØ‰ªéÂéüÁÇπ‰ΩçÁΩÆÂºÄÂßãË£ÅÂâ™ ÂùêÊ†áÁÇπÁöÑÂèòÊç¢Êª°Ë∂≥ÂÖ¨ÂºèÔºö dst(x,y) = src(M_{11}x+M_{12}y+M_{13}, M_{21}x+M_{22}y+M_{23}) np.meshgrid(*xi,**kwargs) Ëøô‰∏™ÂáΩÊï∞Á•û‰ªñÂ¶àÂùëÔºå‰ΩúÁî®ÊòØReturn coordinate matrices from coordinate vectors. Make N-D coordinate arrays for vectorized evaluations of N-D scalar/vector fields over N-D grids, given one-dimensional coordinate arrays x1, x2,‚Ä¶, xn. ‰ΩÜÊòØÂ∞ùËØï‰∏Ä‰∏ã‰ºöÂèëÁé∞Ôºö 12345x = np.arange(0,10,1)y = np.arange(0,20,1)z = np.arange(0,30,1)x, y, z= np.meshgrid(x, y, z)print(x.shape) # (20, 10, 30) xyËΩ¥ÂùêÊ†áÊòØÂèçËøáÊù•ÁöÑÔºåËøôÊòØÂõ†‰∏∫optional argsÈáåÈù¢Êúâ‰∏Ä‰∏™indexingÔºö indexing : {‚Äòxy‚Äô, ‚Äòij‚Äô}, Cartesian (‚Äòxy‚Äô, default) or matrix (‚Äòij‚Äô) indexing of output. Êàë‰ª¨ÊÉ≥Ë¶ÅÂæóÂà∞ÁöÑÂùêÊ†áÁ≥ªÂíåËæìÂÖ•ÁöÑËΩ¥‰∏Ä‰∏ÄÂØπÂ∫îÔºåÂæóÊåáÂÆöÂèÇÊï∞indexing=&#39;ij&#39; 12345x = np.arange(0,10,1)y = np.arange(0,20,1)z = np.arange(0,30,1)x, y, z= np.meshgrid(x, y, z, indexing='ij')print(x.shape) # (10, 20, 30) ËøòÊúâ‰∏Ä‰∏™ÂèÇÊï∞sparseÔºåÂõ†‰∏∫ÊØèÊ†πËΩ¥ÁöÑÂùêÊ†áÈÉΩÊòØÂ§çÂà∂ÁöÑÔºåÊâÄ‰ª•ÂèØ‰ª•Á®ÄÁñèÂ≠òÂÇ®ÔºåÊ≠§Êó∂ÂáΩÊï∞ËøîÂõûÂÄºÂèòÂåñÔºö sparse : bool, If True a sparse grid is returned in order to conserve memory. Default is False. 12345678910x = np.arange(0,10,1)y = np.arange(0,20,1)xx, yy = np.meshgrid(x, y)print(xx) # a 20x10 listxx, yy = np.meshgrid(x, y, sparse=True)print(xx) # a 1*10 listprint(yy) # a 20*1 list# ÊâÄ‰ª•Êï¥‰Ωì‰∏äËøòÊòØ‰∏™20*10ÁöÑÁü©Èòµ ‰∫åÁª¥ÂèØËßÜÂåñÔºö 12345import matplotlib.pyplot as pltz = xx**2 + yy**2 # xxÂíåyyÊó¢ÂèØ‰ª•ÊòØdense convervation‰πüÂèØ‰ª•ÊòØsparse convervationh = plt.contourf(x,y,z)plt.show() np.tile(A,reps) Ëøô‰∏™ÂáΩÊï∞Êå∫ÊúâÁî®ÁöÑÔºåÊääÊï∞ÁªÑÊ≤øÁùÄÊåáÂÆöÁª¥Â∫¶Â§çÂà∂ÔºåÊØîstack„ÄÅconcatÂï•ÁöÑÈÉΩ‰ºòÈõÖÔºåËÉΩËá™Âä®ÂàõÂª∫Êñ∞ÁöÑÁª¥Â∫¶ AÔºöarray_like, The input array. repsÔºöarray_like, The number of repetitions of A along each axis. np.reshape(a, newshape, order=‚ÄôC‚Äô) Ëøô‰∏™ÂáΩÊï∞Ë¥ºÂ∏∏Áî®Ôºå‰ΩÜÊòØ‰∏ÄËà¨Áî®‰∫é‰∫åÁª¥ÁöÑÊó∂ÂÄôÊ≤°ËÄÉËôëÈáçÁªÑÈ°∫Â∫èËøô‰ª∂‰∫ã order: {‚ÄòC‚Äô, ‚ÄòF‚Äô, ‚ÄòA‚Äô}, optionalÔºåÁÆÄÂçïÁêÜËß£ÔºåreshapeÁöÑÈÄöÁî®ÂÆûÁé∞ÊñπÂºèÊòØÂÖàÂ∞ÜÁúü‰∏™arrayÊãâÁõ¥ÔºåÁÑ∂Âêé‰æùÊ¨°ÂèñÊï∞ÊçÆÂ°´ÂÖ•ÊåáÂÆöÁª¥Â∫¶ÔºåCÊòØ‰ªéÊúÄÈáåÈù¢ÁöÑÁª¥Â∫¶ÂºÄÂßãÊãâÁõ¥&amp;ÊûÑÈÄ†ÔºåFÊòØ‰ªéÊúÄÂ§ñÈù¢ÁöÑÁª¥Â∫¶ÂºÄÂßãÊãâÁõ¥&amp;ÊûÑÈÄ†ÔºåA for auto 123456789101112a = np.arange(6)array([0, 1, 2, 3, 4, 5])# C-like index orderingnp.reshape(a, (2, 3))array([[0, 1, 2], [3, 4, 5]])# Fortran-like index orderingnp.reshape(a, (2, 3), order='F')array([[0, 4, 3], [2, 1, 5]]) tfÂíåkerasÈáåÈù¢‰πüÊúâreshapeÔºåÊòØÊ≤°ÊúâorderÂèÇÊï∞ÁöÑÔºåÈªòËÆ§ÊòØ‚ÄôC‚Äô]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MobileNets]]></title>
    <url>%2F2020%2F04%2F16%2FMobileNets%2F</url>
    <content type="text"><![CDATA[preview Âä®Êú∫ ËÆ°ÁÆóÂäõÊúâÈôê Ê®°ÂûãÂéãÁº©Ôºè‰ΩøÁî®Â∞èÊ®°Âûã Ê∑±Â∫¶ÂèØÂàÜÁ¶ªÂç∑ÁßØ Depthwise Separable Convolution Â∞ÜÊ†áÂáÜÂç∑ÁßØÊãÜÂàÜ‰∏∫‰∏§‰∏™Êìç‰ΩúÔºöÊ∑±Â∫¶Âç∑ÁßØ(depthwise convolution) ÂíåÈÄêÁÇπÂç∑ÁßØ(pointwise convolution) Ê†áÂáÜÂç∑ÁßØÔºöÂèÇÊï∞Èáèk*k*input_channel*output_channel Ê∑±Â∫¶Âç∑ÁßØ(depthwise convolution) ÔºöÈíàÂØπÊØè‰∏™ËæìÂÖ•ÈÄöÈÅìÈááÁî®‰∏çÂêåÁöÑÂç∑ÁßØÊ†∏ÔºåÂèÇÊï∞Èáèk*k*input_channel ÈÄêÁÇπÂç∑ÁßØ(pointwise convolution)ÔºöÂ∞±ÊòØÊôÆÈÄöÁöÑÂç∑ÁßØÔºåÂè™‰∏çËøáÂÖ∂ÈááÁî®1x1ÁöÑÂç∑ÁßØÊ†∏ÔºåÂèÇÊï∞Èáè1*1*input_channel*output_channel with BN and ReLUÔºö DWÊ≤°ÊúâÊîπÂèòÈÄöÈÅìÊï∞ÁöÑËÉΩÂäõÔºåÂ¶ÇÊûúËæìÂÖ•Â±ÇÁöÑÈÄöÈÅìÊï∞ÂæàÂ∞ëÔºåDW‰πüÂè™ËÉΩÂú®‰ΩéÁª¥Á©∫Èó¥ÊèêÁâπÂæÅÔºåÂõ†Ê≠§V2ÊèêÂá∫ÂÖàÂØπÂéüÂßãËæìÂÖ•ÂÅöexpansionÔºåÁî®‰∏Ä‰∏™ÈùûÁ∫øÊÄßPWÂçáÁª¥ÔºåÁÑ∂ÂêéDWÔºåÁÑ∂ÂêéÂÜç‰ΩøÁî®‰∏Ä‰∏™PWÈôçÁª¥ÔºåÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÁ¨¨‰∫å‰∏™PW‰∏ç‰ΩøÁî®ÈùûÁ∫øÊÄßÊøÄÊ¥ªÂáΩÊï∞ÔºåÂõ†‰∏∫‰ΩúËÄÖËÆ§‰∏∫Ôºårelu‰ΩúÁî®Âú®‰ΩéÁª¥Á©∫Èó¥‰∏ä‰ºöÂØºËá¥‰ø°ÊÅØÊçüÂ§±„ÄÇ Ëøõ‰∏ÄÊ≠•Áº©ÂáèËÆ°ÁÆóÈáè ÈÄöÈÅìÊï∞Áº©ÂáèÔºöÂÆΩÂ∫¶Âõ†Â≠ê alpha ÂàÜËæ®ÁéáÁº©ÂáèÔºöÂàÜËæ®ÁéáÂõ†Â≠êrho papers [V1] MobileNets: Efficient Convolutional Neural Networks for Mobile Vision ApplicationsÔºå‰∏ªË¶ÅË¥°ÁåÆDepthwise Separable Convolution [V2] MobileNetV2: Inverted Residuals and Linear BottlenecksÔºå‰∏ªË¶ÅË¥°ÁåÆinverted residual with linear bottleneck [V3] Searching for MobileNetV3ÔºåÊ®°ÂûãÁªìÊûÑÂçáÁ∫ß(inverted-res-block + SE-block)ÔºåÈÄöËøáNASÔºåËÄåÈùûÊâãÂä®ËÆæËÆ° MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications Âä®Êú∫ efficient modelsÔºöuses depthwise separable convolutions and two simple global hyper-parameters resource and accuracy tradeoffs a class of network architectures that allows a model developer to specifically choose a small network that matches the resource restrictions (latency, size) for their application ËÆ∫ÁÇπÔºö the general trend has been to make deeper and more complicated networks in order to achieve higher accuracy not efficient on computationally limited platform building small and efficient neural networksÔºöeither compressing pretrained networks or training small networks directly Many papers on small networks focus only on size but do not consider speed speed &amp; size ‰∏çÂÆåÂÖ®ÂØπÁ≠â sizeÔºödepthwise separable convolutions, bottleneck approaches, compressing pretrained networks, distillation ÊñπÊ≥ï depthwise separable convolutions a form of factorized convolutionsÔºöa standard conv splits into 2 layers factorize the filtering and combination steps of standard conv drastically reducing computation and model size to $\frac{1}{N} + \frac{1}{D_k^2}$ use both batchnorm and ReLU nonlinearities for both layers MobileNet uses 3 √ó 3 depthwise separable convolutions which bring between 8 to 9 times less computation MobileNet the first layer is a full convolution, the rest depthwise separable convolutions down sampling is handled with strided convolution all layers are followed by a BN and ReLU nonlinearity a final average pooling reduces the spatial resolution to 1 before the fully connected layer. the final fully connected layer has no nonlinearity and feeds into a softmax layer for classification training so few parameters RMSprop less regularization and data augmentation techniques because small models have less trouble with overfitting it was important to put very little or no weight decay (l2 regularization) do not use side heads or label smoothing or image distortions Width Multiplier: Thinner Models thin a network uniformly at each layer the input channels $M$ and output channels $N$ becomes $\alpha M$ and $\alpha N$ $\alpha=1$Ôºöbaseline MobileNet $\alpha&lt;1$Ôºöreduced MobileNet reduce the parameters roughly by $\alpha^2$ Resolution Multiplier: Reduced Representation apply this to the input image the input resolution of the network is typically 224, 192, 160 or 128 $\rho=1$Ôºöbaseline MobileNet $\rho&lt;1$Ôºöreduced MobileNet reduce the parameters roughly by $\rho^2$ ÁªìËÆ∫ using depthwise separable convolutions compared to full convolutions only reduces accuracy by 1% on ImageNet but saving tremendously on mult-adds and parameters at similar computation and number of parameters, thinner MobileNets is 3% better than making them shallower trade-offs based on the two hyper-parameters MobileNetV2: Inverted Residuals and Linear Bottlenecks Âä®Êú∫ a new mobile architecture based on an inverted residual structure remove non-linearities in the narrow layers in order to maintain representational power prove on multiple tasks object detectionÔºöSSDLite semantic segmentationÔºöMobile DeepLabv3 ÊñπÊ≥ï Depthwise Separable Convolutions replace a full convolutional opera- tor with a factorized version depthwise convolution, it performs lightweight filtering per input channel pointwise convolution, computing linear combinations of the input channels Linear Bottlenecks ReLU results in information loss in lower dimension space expansion ratioÔºöif we have lots of channels, information might still be preserved in the other channels linearÔºöbottleneck‰∏äÈù¢‰∏çÂåÖÂê´ÈùûÁ∫øÊÄßÊøÄÊ¥ªÂçïÂÖÉ Inverted residuals bottlenecks actually contain all the necessary information expansion layer acts merely as an implementation detail that accompanies a non-linear transformation parameter countÔºö basic building block is a bottleneck depth-separable convolution with residuals * interpretation * provides a natural separation between the input/output * expansionÔºöcapacity * layer transformationÔºöexpressiveness * MobileNetV2 model architecture * initial filtersÔºö32 * ReLU6Ôºöuse ReLU6 as the non-linearity because of its robustness when used with low-precision computation * use constant expansion rate between 5 and 10 except the 1stÔºösmaller network inclines smaller and larger larger &lt;img src=&quot;MobileNets/MobileNetV2.png&quot; width=&quot;40%&quot; /&gt; * comparison with other architectures &lt;img src=&quot;MobileNets/cmpV2.png&quot; width=&quot;40%&quot; /&gt; ÂÆûÈ™å Object Detection evaluate the performance as feature extractors replace all the regular convolutions with separable convolutions in SSD prediction layersÔºöbackboneÊ≤°ÊúâÊîπÂä®ÔºåÂè™ÊõøÊç¢Â§¥ÈÉ®ÁöÑÂç∑ÁßØÔºåÈôç‰ΩéËÆ°ÁÆóÈáè achieves competitive accuracy with significantly fewer parameters and smaller computational complexity Semantic Segmentation build DeepLabv3 heads on top of the second last feature map of MobileNetV2 DeepLabv3 heads are computationally expensive and removing the ASPP module significantly reduces the MAdds ablation inverted residual connectionsÔºöshortcut connecting bottleneck perform better than shortcuts connecting the expanded layers Âú®Â∞ëÈÄöÈÅìÁöÑÁâπÂæÅ‰∏äËøõË°åÁü≠ËøûÊé• linear bottlenecksÔºölinear bottlenecks improve performance, providing support that non-linearity destroys information in low-dimensional space Searching for MobileNetV3 Âä®Êú∫ automated search algorithms and network design work together classification &amp; detection &amp; segmentation a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP) new efficient versions of nonlinearities ËÆ∫ÁÇπ reducing the number of parameters the number of operations (MAdds) inference latency related work SqueezeNetÔºö1x1 convolutions MobileNetV1Ôºöseparable convolution MobileNetV2Ôºöinverted residuals ShuffleNetÔºögroup convolutions CondenseNetÔºögroup convolutions ShiftNetÔºöshift operation MnasNetÔºöMobileNetV2+SE-blockÔºåattention modules are placed after the depthwise filters in the expansion ÊñπÊ≥ï base blocks combination of ideas from [MobileNetV1, MobileNetV2, MnasNet] inverted-res-block + SE-block swish nonlinearity hard sigmoid Network Search use platform-aware NAS to search for the global network structures use the NetAdapt algorithm to search per layer for the number of filters Network Improvements redesign the computionally-expensive layers at the beginning and the end of the network the last block of MobileNetV2‚Äôs inverted bottleneck structure move this layer past the final average poolingÔºöÁßªÂä®Âà∞GAPÂêéÈù¢ÂéªÔºå‰ΩúÁî®Âú®1x1ÁöÑfeaturemap‰∏äinstead of 7x7ÔºåÊõ≤Á∫øÊïëÂõΩ a new nonlinearity, h-swish the initial set of filters are also expensiveÔºöusually start with 32 filters in a full 3x3 convolution to build initial filter banks for edge detection reduce the number of filters to 16 and use the hard swish nonlinearity swish\ [x]=x*\sigma(x)\\ h\_swish\ [x]=x\frac{ReLU6(x+3)}{6} most of the benefits swish are realized by using them only in the deeper layersÔºöÂè™Âú®ÂêéÂçäÊÆµÁΩëÁªú‰∏≠Áî® SE-block ratioÔºöall to fixed to be 1/4 of the number of channels in expansion layer MobileNetV3 architecture ÂÆûÈ™å Detection use MobileNetV3 as replacement for the backbone feature extractor in SSDLiteÔºöÊîπÂÅöbackbone‰∫Ü reduce the channel counts of C4&amp;C5‚Äôs blockÔºöÂõ†‰∏∫MobileNetV3ÂéüÊú¨ÊòØË¢´Áî®Êù•ËæìÂá∫1000Á±ªÁöÑÔºåtransferÂà∞90Á±ªÁöÑcocoÊï∞ÊçÆÈõÜ‰∏äÊúâ‰∫õredundant Segmentation as network backbone compare two segmentation heads R-ASPPÔºöreduced design of the Atrous Spatial Pyramid Pooling module with only two branches Lite R-ASPPÔºöÁ±ªSE-blockÁöÑËÆæËÆ°ÔºåÂ§ßÂç∑ÁßØÊ†∏ÔºåÂ§ßÊ≠•Èïø]]></content>
      <tags>
        <tag>papers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[verseg]]></title>
    <url>%2F2020%2F04%2F15%2Fverseg%2F</url>
    <content type="text"><![CDATA[challenge Large Scale Vertebrae Segmentation Challenge task1:Vertebra LabellingÔºåÂÖ≥ÈîÆÁÇπÊ£ÄÊµã task2:Vertebra SegmentationÔºåÂ§öÁ±ªÂà´ÂàÜÂâ≤ data variationÔºöÊï∞ÊçÆaffineËΩ¥‰∏çÁªü‰∏ÄÔºåÂ∞∫ÂØ∏‰∏çÁªü‰∏ÄÔºåÊâ´ÊèèËåÉÂõ¥‰∏çÁªü‰∏ÄÔºåFOVÂå∫Âüü‰∏çÁªü‰∏Ä niiÁöÑ‰∏§Â§ßËß£ÊûêÂ∑•ÂÖ∑ÔºönibabelÂ∫ìload dataÁöÑxyzÈ°∫Â∫è‰∏éaxcodeÁöÑÈ°∫Â∫è‰∏ÄËá¥Ôºåe.g.[‚ÄòR‚Äô,‚ÄôA‚Äô,‚ÄôS‚Äô]ÁöÑorientation‰ºöÂæóÂà∞xyzÁöÑarrayÔºåËÄåsitkÁöÑËØªÂèñÂàöÂ•ΩÂèçËøáÊù•ÔºåsitkÁöÑarr‰ºöÊòØzyx„ÄÇÊàë‰ª¨‰πãÂâçÂú®Â∞ÜdicomÂÜôÂÖ•niiÊó∂Ôºå‰ºöÊåáÂÆö‰∏Ä‰∏™‰∏ç‰∏∫np.eye(4)ÁöÑaffineÔºåÂ∞±ÊòØ‰∏∫‰∫ÜtransposeËøô‰∏â‰∏™ËΩ¥„ÄÇ model team paper \ ‰∏âÈò∂ÊÆµÔºöÁ¨¨‰∏ÄÈò∂ÊÆµÔºådue to large variation FOV of the datasetÔºåÁ≤óÂàÜÂâ≤ÂÆö‰ΩçËÑäÊü±‰ΩçÁΩÆÔºåÁ¨¨‰∫åÈò∂ÊÆµÔºåhigher resolutionÂ§öÁ±ªÂà´ÂÖ≥ÈîÆÁÇπÂÆö‰ΩçcenterÔºåËé∑Âæóeach located vertebraÔºåÁ¨¨‰∏âÈò∂ÊÆµÔºå‰∫åÁ±ªÂàÜÂâ≤for each located vertebra„ÄÇ keywordsÔºö1. uniform voxel spacingÔºö‰∏çË¶ÅÈöèÊÑèresizeÔºåtodo: trilinear interpÔºõ2. on-the-fly data augmentationÔºöusing SimpleITK Á¨¨‰∏ÄÈò∂ÊÆµÔºöSpine Localization Unet regress the Gaussian heatmap of spinal centerline L2-loss uniform voxel spacing of 8mm input shapeÔºö[64,64,128]ÔºåpadÔºü Á¨¨‰∫åÈò∂ÊÆµÔºöVertebrae Localization SpatialConfiguration-Net regress each located vertebra‚Äòs heatmap in individual channel resamplingÔºöbi/tricubic interpolation normÔºömaxmin on the whole dataset uniform voxel spacing of 2mm input shapeÔºö[96,96,128]Ôºåz-axis random cropÔºåxy-plane use ROI from stage1 Á¨¨‰∏âÈò∂ÊÆµÔºöVertebrae Segmentation Unet binary segment the mask of each vertebrae sigmoid ce-loss uniform voxel spacing of 1mm input shapeÔºö[128,128,96]Ôºåcrop origin image &amp; heatmap image based on centroids reference paper\ Ê†∏ÂøÉË¥°ÁåÆÔºö1.MIPÔºöcombines the information across reformationsÔºå3D to 2DÔºå2. Âü∫‰∫éÂà§Âà´Âô®ÁöÑËÆ≠ÁªÉÊú∫Âà∂Ôºöencodes local spine structure as an anatomical priorÔºåÂä†Âõ∫Ê§éÂùóÈó¥Á±ªÂà´&amp;‰ΩçÁΩÆÁöÑspacial information MIPÔºö localisation and identification rely on a large context large receptive field in full-body scans where spine is not spatially centred or is obstructed by the ribcage, such cases are handled with a pre-processing stage detecting the occluded spine adversarial learningÔºö FCNÁî®‰∫éÂàÜÂâ≤ AEÁî®‰∫éËØÑ‰º∞ÂàÜÂâ≤ÁöÑÂ•ΩÂùè do not ‚Äòpre-train‚Äô it (the AE) lossÔºöan anatomically-inspired supervision instead of the usual binary adversarial supervision (vanilla GAN) ÂÖàËØ¥FCN‚Äî‚ÄîBtrfly Network Âª∫Ê®°ÊàêÂõûÂΩíÈóÆÈ¢òÔºåÊØè‰∏™ÂÖ≥ÈîÆÁÇπÂØπÂ∫î‰∏Ä‰∏™ÈÄöÈÅìÁöÑÈ´òÊñØheatmapÔºåËÉåÊôØchannel‰∏∫$1-max_i (y_i)$ ÂèåËæìÂÖ•ÂèåËæìÂá∫Ôºàsagittal &amp; coronalÔºâ ‰∏§‰∏™ËßÜËßíÁöÑfeature mapÂú®ÁΩëÁªúÊ∑±Â±ÇÂÅö‰∫ÜËûçÂêàÔºåto learn their inter-dependency Batch- normalisation is used after every convolution layer, along with 20% dropout in the fused layers of Btrfly lossÔºöl2 distance + weighted ce L_{sag} = ||Y_{sag} - \hat{Y}_{sag}||^2 + \omega CE(softmax(Y_{sag}, softmax(\hat{Y}_{sag})) $\omega$ is the median frequency weighing map, boosting the learning of less frequent classes(ECB) ÂÜçËØ¥Âà§Âà´Âô®‚Äî‚ÄîEnergy-based adversary for encoding prior fully-convolutionalÔºöits predictions across voxels are independent of each other owing to the spatial invariance of convolutions to impose the anatomical prior of the spine‚Äôs shape onto the Btrfly net look at $\hat{Y}_{sag}$ and $\hat{Y}_{cor}$ as a 3D volume and employ a 3D AE with a receptive field covering a part of the spine $\hat{Y}_{sag}$ consists of GaussiansÔºöless informative than an image, avoid using max-pooling by resorting to average pooling employ spatially dilated convolution kernels mission of AEÔºöpredict the l2 distance of input and its reconstruction, it learns to discriminate by predicting a low E for real annotations, while G learns to generate annotations that would trick D L = D(Y_x) + max(0, m-D(Y_g))\\ L_G = D(Y_g) + L_{fcn} inferenceÔºö The values below a threshold (T) are ignored in order to remove noisy predictions Áî®Â§ñÁßØÔºå$\hat{Y}=\hat{Y}_{sag}\otimes\hat{Y}_{cor}$ ÊØè‰∏™channelÁöÑÊúÄÂ§ßÂÄº‰Ωú‰∏∫centroids experiments „ÄêIMPORTANT„Äë10 MIPs are obtained from one 3D scan per view, each time randomly choosing half the slices of interest ÂØπ‰∫éÊØè‰∏™ËßÜËßíÔºåÊØèÊ¨°ÈöèÊú∫ÊäΩÂèñ‰∏ÄÂçäÊï∞ÁõÆÁöÑsliceÁî®‰∫éËÆ°ÁÆóMIP similar local appearanceÔºö strong spatial configurationÔºöÂá°ÊòØÊ∂âÂèäÂà∞Ê§éÂùó-wiseÁöÑ‰ø°ÊÅØÔºå‰ªéÂÖ®Â±Ä‰ø°ÊÅØÂÖ•Êâã]]></content>
      <tags>
        <tag>challenge</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GoogLeNetÁ≥ªÂàó]]></title>
    <url>%2F2020%2F04%2F13%2FGoogLeNet%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[ÁªºËø∞ papers [V1] Going Deeper with Convolutions, 6.67% test error [V2] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, 4.8% test error [V3] Rethinking the Inception Architecture for Computer Vision, 3.5% test error [V4] Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, 3.08% test error [Xception] Xception: Deep Learning with Depthwise Separable Convolutions [EfficientNet] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks [EfficientDet] EfficientDet: Scalable and Efficient Object Detection [EfficientNetV2] EfficientNetV2: Smaller Models and Faster Training Â§ß‰ΩìÊÄùË∑Ø inception V1ÔºöÊâìÁ†¥‰º†ÁªüÁöÑconv blockÔºåËÆæËÆ°‰∫ÜInception blockÔºåÂ∞Ü1*1„ÄÅ3*3„ÄÅ5*5ÁöÑÂç∑ÁßØÁªìÊûúconcatÔºåÂ¢ûÂä†ÁΩëÁªúÂÆΩÂ∫¶ inception V2ÔºöÂä†ÂÖ•‰∫ÜBNÂ±ÇÔºåÂáèÂ∞ëInternal Covariate ShiftÔºåÁî®‰∏§‰∏™3*3Êõø‰ª£5*5ÔºåÈôç‰ΩéÂèÇÊï∞Èáè inception V3ÔºöÊèêÂá∫ÂàÜËß£FactorizationÔºå7*7ÊîπÊàê7*1Âíå1*7ÔºåÂèÇÊï∞ÂáèÂ∞ëÂä†ÈÄüËÆ°ÁÆóÔºåÂ¢ûÂä†ÁΩëÁªúÊ∑±Â∫¶ÂíåÈùûÁ∫øÊÄß inception V4ÔºöÁªìÂêàResidual Connection XceptionÔºöÈíàÂØπinception V3ÁöÑÂàÜËß£ÁªìÊûÑÁöÑÊîπËøõÔºå‰ΩøÁî®ÂèØÂàÜÁ¶ªÂç∑ÁßØ EfficientNetÔºö‰∏ªË¶ÅÁ†îÁ©∂model scalingÔºåÈíàÂØπÁΩëÁªúÊ∑±Â∫¶„ÄÅÂÆΩÂ∫¶„ÄÅÂõæÂÉèÂàÜËæ®ÁéáÔºåÊúâÊïàÂú∞Êâ©Â±ïCNN EfficientDetÔºöÂ∞ÜEfficientNet‰ªéÂàÜÁ±ª‰ªªÂä°Êâ©Â±ïÂà∞ÁõÆÊ†áÊ£ÄÊµã‰ªªÂä° review review0122Ôºöconv-BNÂ±ÇÂêàÂπ∂ËøêÁÆó referenceÔºöhttps://nenadmarkus.com/p/fusing-batchnorm-and-conv/ freezed BNÂèØ‰ª•ÁúãÊàê1x1ÁöÑÂç∑ÁßØËøêÁÆó ‰∏§‰∏™Á∫øÊÄßËøêÁÆóÊòØÂèØ‰ª•ÂêàÂπ∂ÁöÑ given $W_{conv} \in R^{CC_{prev}kk}$Ôºå$b_{conv} \in R^C $Ôºå$W_{bn}\in R^{CC}$Ôºå$b_{bn}\in R^C$ F = W_{bn} * (W_{conv} * F_{prev} + b_{conv}) + b_{bn} V1: Going deeper with convolutions Âä®Êú∫ improved utilization of the computing resources increasing the depth and width of the network while keeping the computational budget ËÆ∫ÁÇπ the recent trend has been to increase the number of layers and layer size, while using dropout to address the problem of overfitting major bottleneckÔºölarge networkÔºålarge number of paramsÔºålimited datasetÔºåoverfitting methods use filters of different sizes in order to handle multiple scales NiN use 1x1 convolutional layers to easily integrate in the current CNN pipelines we use 1x1 convs with a dual purpose of dimension reduction ÊñπÊ≥ï Architectural 1x1 conv+ReLU for compute reductions an alternative parallel pooling path since pooling operations have been essential for the success overall architecture : ÁªÜËäÇÔºö rectified linear activation mean subtraction a move from fully connected layers to average pooling improves acc the use of dropout remained essential adding auxiliary classifiers(on 4c&amp;4d) with a discount weight 5x5 avg pool, stride 3 1x1 conv+relu, 128 filters 1024 fc+relu 70% dropout 1000 fc+softmax asynchronous stochastic gradient descent with 0.9 momentum fixed learning rate schedule (de- creasing the learning rate by 4% every 8 epochs photometric distortions useful to combat overfitting random interpolation methods for resizing V2: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Âä®Êú∫ use much higher learning rates be less careful about initialization also acts as a regularizer, eliminating the need for Dropout ËÆ∫ÁÇπ SGDÔºöoptimizes the parameters $\theta$ of the network, so as to minimize the loss \theta = argmin_{\theta} \frac{1}{N}\sum_{i=1}^N loss(x_i, \theta) Ê¢ØÂ∫¶Êõ¥Êñ∞Ôºö\theta_{next-timestep} = \theta - \alpha \frac{\partial[\frac{1}{N}\sum_Nloss(\theta)]}{\partial \theta}Ôºå$x_i$ is the full set batch approximationÔºöuse $\frac{1}{m} \sum_M \frac{\partial loss(\theta)}{\partial \theta}$Ôºå$x_i$ is the mini-batch set quality improves as the batch size increases computation over a batch is much more efficient than m computations for individual examples the learning rate and the initial values require careful tuning Internal covariate shift the input distribution of the layers changes consider a gradient descent step aboveÔºå$x$ÁöÑÊï∞ÊçÆÂàÜÂ∏ÉÊîπÂèò‰∫ÜÔºå$\theta$Â∞±Ë¶ÅÁõ∏Â∫îÂú∞ readjust to compensate for the change in the distribution of x activation ÂØπ‰∫éÁ•ûÁªèÂÖÉ$z = sigmoid(Wx+b)$ÔºåÂâçÈù¢Â±ÇÁöÑÂèÇÊï∞ÂèòÂåñÔºåÂæàÂÆπÊòìÂØºËá¥ÂΩìÂâçÁ•ûÁªèÂÖÉÁöÑÂìçÂ∫îÂÄº‰∏çÂú®ÊúâÊïàÊ¥ªÂä®Âå∫Èó¥Ôºå‰ªéËÄåÂØºËá¥Ëøá‰∫ÜÂΩìÂâçÊøÄÊ¥ªÂáΩÊï∞‰ª•ÂêéÊ¢ØÂ∫¶Ê∂àÂ§±Ôºåslow down the convergence In practice, using ReLU &amp; careful initialization &amp; small learning rates Â¶ÇÊûúÊàë‰ª¨ËÉΩ‰ΩøÂæóthe distribution of nonlinearity inputs remains more stable as the network trainsÔºåÂ∞±‰∏ç‰ºöÂá∫Áé∞Á•ûÁªèÂÖÉÈ•±ÂíåÁöÑÈóÆÈ¢ò whitening ÂØπtraining setÁöÑÈ¢ÑÂ§ÑÁêÜÔºölinearly transformed to have zero means and unit variances, and decorrelated ‰ΩøÂæóËæìÂÖ•Êï∞ÊçÆÁöÑÂàÜÂ∏É‰øùÊåÅÁ®≥ÂÆöÔºånormal distribution ÂêåÊó∂ÂéªÈô§‰∫ÜÊï∞ÊçÆÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄß batch normalization fixes the means and variances of layer inputs reducing the dependence of gradients on the scale of the parameters or of their initial values makes it possible to use saturating nonlinearities * full whitening of each layer is costly * so we normalize each layer independently, full set--&gt; mini-batch * standard normal distributionÂπ∂‰∏çÊòØÊØè‰∏™Á•ûÁªèÂÖÉÊâÄÈúÄÁöÑÔºàÂ¶Çidentity transformÔºâÔºöintroduce, for each activation $x(k)$ , a pair of parameters $\gamma(k)$, $\beta(k)$, which scale and shift the normalized value to maintain the representation ability of the neuron ‚Äã * for convolutional networks * we add the BN transform immediately before the nonlinearity, $z = g(Wx+b)$ to $z = g(BN(Wx))$ * since we normalize $Wx+b$, the bias b can be ignored * obey the convolutional property‚Äî‚Äîdifferent elements of the same feature map, at different locations, are normalized in the same way * We learn a pair of parameters $\gamma(k)$ and $\beta(k)$ **per feature map**, rather than per activation * properties * back-propagation through a layer is unaffected by the scale of its parameters * Moreover, larger weights lead to smaller gradients, thus stabilizing the parameter growth * regularizes the modelÔºöÂõ†‰∏∫ÁΩëÁªú‰∏≠mini-batchÁöÑÊï∞ÊçÆ‰πãÈó¥ÊòØÊúâ‰∫íÁõ∏ÂΩ±ÂìçÁöÑËÄåÈùûindependentÁöÑ ÊñπÊ≥ï batch normalization full whitening of each layer is costly so we normalize each layer independently, full set‚Äî&gt; mini-batch standard normal distributionÂπ∂‰∏çÊòØÊØè‰∏™Á•ûÁªèÂÖÉÊâÄÈúÄÁöÑÔºàÂ¶Çidentity transformÔºâÔºöintroduce, for each activation $x(k)$ , a pair of parameters $\gamma(k)$, $\beta(k)$, which scale and shift the normalized value to maintain the representation ability of the neuron bpÔºö inferenceÈò∂ÊÆµÔºö È¶ñÂÖà‰∏§‰∏™ÂèØÂ≠¶‰π†ÂèÇÊï∞$\gamma$Âíå$\beta$ÊòØÂÆö‰∏ãÊù•ÁöÑ ËÄåÂùáÂÄºÂíåÊñπÂ∑Æ‰∏çÂÜçÈÄöËøáËæìÂÖ•Êï∞ÊçÆÊù•ËÆ°ÁÆóÔºåËÄåÊòØËΩΩÂÖ•ËÆ≠ÁªÉËøáÁ®ã‰∏≠Áª¥Êä§ÁöÑÂèÇÊï∞Ôºàmoving averagesÔºâ ‚Äã for convolutional networks we add the BN transform immediately before the nonlinearity, $z = g(Wx+b)$ to $z = g(BN(Wx))$ since we normalize $Wx+b$, the bias b can be ignored obey the convolutional property‚Äî‚Äîdifferent elements of the same feature map, at different locations, are normalized in the same way We learn a pair of parameters $\gamma(k)$ and $\beta(k)$ per feature map, rather than per activation properties back-propagation through a layer is unaffected by the scale of its parameters Moreover, larger weights lead to smaller gradients, thus stabilizing the parameter growth regularizes the modelÔºöÂõ†‰∏∫ÁΩëÁªú‰∏≠mini-batchÁöÑÊï∞ÊçÆ‰πãÈó¥ÊòØÊúâ‰∫íÁõ∏ÂΩ±ÂìçÁöÑËÄåÈùûindependentÁöÑ V3: Rethinking the Inception Architecture for Computer Vision Âä®Êú∫ go deeper and widerÔºö enough labeled data computational efficiency parameter count to scale up networks utilizing the added computation as efficiently give general design principles and optimization ideas factorized convolutions aggressive regularization ËÆ∫ÁÇπ GoogleNet does not provide a clear description about the contributing factors that lead to the various design ÊñπÊ≥ï General Design Principles Avoid representational bottlenecksÔºöÁâπÂæÅÂõæÂ∞∫ÂØ∏Â∫îËØ•gently decreaseÔºåresolutionÁöÑ‰∏ãÈôçÂøÖÈ°ª‰º¥ÈöèÁùÄchannelÊï∞ÁöÑ‰∏äÂçáÔºåÈÅøÂÖç‰ΩøÁî®max poolingÂ±ÇËøõË°å‰∏ãÈááÊ†∑ÔºåÂõ†‰∏∫ËøôÊ†∑ÂØºËá¥‰ø°ÊÅØÊçüÂ§±ËæÉÂ§ß Higher dimensional representations are easier to process locally within a network. Increasing the activa- tions per tile in a convolutional network allows for more disentangled features. The resulting networks will train fasterÔºöÂâçÂçäÂè•ÊáÇ‰∫ÜÔºåhigh-resoÁöÑÁâπÂæÅÂõæfocusÂú®Â±ÄÈÉ®‰ø°ÊÅØÔºåÂêéÂçäÂè•‰∏çÊáÇÔºåÊ†πÊçÆ‰∏ä‰∏ÄÁØápaperÔºåÁî®‰∫Übatch norm‰ª•ÂêéÔºåscale upÁ•ûÁªèÂÖÉ‰∏çÂΩ±ÂìçbpÔºåÂêåÊó∂‰ºölead to smaller gradientsÔºå‰∏∫Âï•ËÉΩÂä†ÈÄüÔºü Spatial aggregation can be done over lower dimensional embeddingsÔºöadjacent unit‰πãÈó¥Êúâstrong correlationÔºåÊâÄ‰ª•ÂèØ‰ª•reduce the dimension of the input representation before the spatial aggregationÔºå‰∏ç‰ºöÊúâÂ§™Â§ßÁöÑ‰ø°ÊÅØÊçüÂ§±ÔºåÂπ∂‰∏îpromotes faster learning The computational budget should therefore be distributed in a balanced way between the depth and width of the network. Factorizing Convolutions Filter Size into smaller convolutions Â§ßfilterÈÉΩÂèØ‰ª•ÊãÜËß£ÊàêÂ§ö‰∏™3x3 ÂçïÁ∫ØÂéªÁ≠â‰ª∑Á∫øÊÄßÂàÜËß£ÂèØ‰ª•‰∏ç‰ΩøÁî®ÈùûÁ∫øÊÄßactivationÔºå‰ΩÜÊòØÊàë‰ª¨‰ΩøÁî®‰∫Übatch normÔºàincrease variatyÔºâÔºåÊâÄ‰ª•ËßÇÂØüÂà∞‰ΩøÁî®ReLU‰ª•ÂêéÊãüÂêàÊïàÊûúÊõ¥Â•Ω into Asymmetric Convolutions n*nÁöÑfilterÊãÜËß£Êàê1*nÂíån*1 this factorization does not work well on early layers, but gives very good results on medium grid-sizes (ranges between 12 and 20, using 1x7 and 7x1 Utility of Auxiliary Classifiers did not result in improved convergence early in the trainingÔºöËÆ≠ÁªÉÂºÄÂßãÈò∂ÊÆµÊ≤°Âï•Áî®ÔºåÂø´Êî∂ÊïõÊó∂ÂÄôÊúâÁÇπÁÇπaccÊèêÂçá removal of the lower auxiliary branch did not have any adverse effect on the final qualityÔºöÊãøÊéâÂØπÊúÄÁªàÁªìÊûúÊ≤°ÂΩ±Âìç ÊâÄ‰ª•ÊúÄÂàùÁöÑËÆæÊÉ≥Ôºàhelp evolving the low-level featuresÔºâ ÊòØÈîôÁöÑÔºå‰ªÖ‰ªÖact as regularizerÔºåauxiliary headÈáåÈù¢Âä†‰∏äbatch norm‰ºö‰ΩøÂæóÊúÄÁªàÁªìÊûúbetter Efficient Grid Size Reduction‰∏ãÈááÊ†∑Ê®°Âùó‰∏çÂÜç‰ΩøÁî®maxpooling dxdxk feature map expand to (d/2)x(d/2)x2kÔºö 1x1x2k convÔºåstride2 poolÔºökxdxdx2k computation 1x1x2k stride2 convÔºökx(d/2)x(d/2)x2k computationÔºåËÆ°ÁÆóÈáè‰∏ãÈôçÔºå‰ΩÜÊòØËøùÂèçprinciple1 parallel stride P and C blocksÔºökx(d/2)x(d/2)xk computationÔºåÁ¨¶Âêàprinciple1:reduces the grid-size while expands the filter banks Inception-v3 ÂºÄÂ§¥ÁöÑ7x7convÂ∑≤ÁªèÊç¢Êàê‰∫ÜÂ§ö‰∏™3x3 ‰∏≠Èó¥Â±ÇfeaturemapÈôçÁª¥Âà∞17x17ÁöÑÊó∂ÂÄôÔºåÂºÄÂßãÁî®Asymmetric Factorization block Âà∞8x8ÁöÑÊó∂ÂÄôÔºåÂÅö‰∫Üexpanding the filter bank outputs Label Smoothing Ôºàhttps://zhuanlan.zhihu.com/p/116466239Ôºâ used the uniform distribution $u(k)=1/K$ q(k) = (1-\epsilon)\delta(k) + \frac{\epsilon}{K} ÂØπ‰∫ésoftmaxÂÖ¨ÂºèÔºö$p(k)=\frac{exp(y_k)}{\sum exp(y_i)}$ÔºåËøô‰∏™lossËÆ≠ÁªÉÁöÑÁªìÊûúÂ∞±ÊòØ$y_k$Êó†ÈôêË∂ãËøë‰∫é1ÔºåÂÖ∂‰ªñ$y_i$Êó†ÈôêË∂ãËøë‰∫é0Ôºå ‰∫§ÂèâÁÜµlossÔºö$ce=\sum -y_{gt}log(y_k)$ÔºåÂä†‰∫Ülabel smoothing‰ª•ÂêéÔºåloss‰∏äÂ¢ûÂä†‰∫ÜÈò¥ÊÄßÊ†∑Êú¨ÁöÑregularizationÔºåÊ≠£Ë¥üÊ†∑Êú¨ÁöÑÊúÄ‰ºòËß£Ë¢´ÈôêÂÆöÂú®ÊúâÈôêÂÄºÔºåÈÄöËøáÊäëÂà∂Ê≠£Ë¥üÊ†∑Êú¨ËæìÂá∫Â∑ÆÂÄºÔºå‰ΩøÂæóÁΩëÁªúÊúâÊõ¥Âº∫ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ V4: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning Âä®Êú∫ residualÔºöwhether there are any benefit in combining the Inception architecture with residual connections inceptionV4Ôºösimplify the inception blocks ËÆ∫ÁÇπ residual connections seems to improve the training speed greatlyÔºö‰ΩÜÊòØÊ≤°Êúâ‰πüËÉΩËÆ≠ÁªÉÊ∑±Â±ÇÁΩëÁªú made uniform choices for the Inception blocks for each grid size Inception-A for 35x35 Inception-B for 17x17 Inception-C for 8x8 for residual versions use cheaper Inception blocks for residual versionsÔºöÁÆÄÂåñmoduleÔºåÂõ†‰∏∫identityÈÉ®ÂàÜÔºàÁõ¥Êé•Áõ∏ËøûÁöÑÁ∫øÔºâÊú¨Ë∫´ÂåÖÂê´‰∏∞ÂØåÁöÑÁâπÂæÅ‰ø°ÊÅØ Ê≤°Êúâ‰ΩøÁî®pooling replace the filter concatenation stage of the Inception architecture with residual connectionsÔºöÂéüÊù•blockÈáåÈù¢ÁöÑconcatenation‰∏ª‰ΩìÊîæÂú®ÊÆãÂ∑Æpath‰∏≠ Each Inception block is followed by filter-expansion layer (1 √ó 1 convolution without activation) to match the depth of the input for additionÔºöÁõ∏Âä†‰πãÂâç‰øùËØÅchannelÊï∞‰∏ÄËá¥ used batch-normalization only on top of the traditional layers, but not on top of the summationsÔºöÊµ™Ë¥πÂÜÖÂ≠ò number of filters exceeded 1000 causes instabilities scaling down the residuals before adding by factors between 0.1 and 0.3ÔºöÊÆãÂ∑ÆÈÄöÈÅìÂìçÂ∫îÂÄº‰∏çË¶ÅÂ§™Â§ß blocks V4 ABCÔºö Res ABCÔºö Xception: Deep Learning with Depthwise Separable Convolutions Âä®Êú∫ Inception modules have been replaced with depthwise separable convolutions significantly outperforms Inception V3 on a larger dataset due to more efficient use of model parameters ËÆ∫ÁÇπ early LeNet-style models simple stacks of convolutions for feature extraction and max-pooling operations for spatial sub-sampling increasingly deeper complex blocks Inception modules inspired by NiN be capable of learning richer repre- sentations with less parameters The Inception hypothesis a single convolution kernel is tasked with simultaneously mapping cross-channel correlations and spatial correlations while Inception factors it into a series of operations that independently look at cross-channel correlations(1x1 convs) and at spatial correlations(3x3/5x5 convs) suggesting that cross-channel correlations and spatial correlations are sufficiently decoupled that it is preferable not to map them jointly inception blockÂÖàÁî®1x1ÁöÑconvÂ∞ÜÂéüËæìÂá∫Êò†Â∞ÑÂà∞3-4‰∏™lower spaceÔºàcross-channel correlationsÔºâÔºåÁÑ∂ÂêéÂú®Ëøô‰∫õÂ∞èÁöÑ3d spaces‰∏äÂÅöregular convÔºàmaps all correlations Ôºâ‚Äî‚ÄîËøõ‰∏ÄÊ≠•ÂÅáËÆæÔºåÂΩªÂ∫ïËß£ËÄ¶ÔºåÁ¨¨‰∫åÊ≠•Âè™ÂÅöspatial correlations main differences between ‚Äúextreme ‚Äù Inception and depthwise separable convolution order of the operationsÔºö1x1 first or latter non-linearityÔºödepthwise separable convolutions are usually implemented without non-linearities„ÄêQUESTIONÔºöËøôÂíåMobileNetÈáåÈù¢ËØ¥ÁöÑ‰∏ç‰∏ÄÊ†∑ÂïäÔºåMÈáåÈù¢ÁöÑdepthwise‰πüÊòØÊØèÂ±ÇÈÉΩÂ∏¶‰∫ÜBNÂíåReLUÁöÑ„Äë Ë¶ÅÁ¥† Convolutional neural networks The Inception family Depthwise separable convolutions Residual connections ÊñπÊ≥ï architecture a linear stack of depthwise separable convolution layers with residual connections all conv are followed by BN kerasÁöÑseparableConvÂíådepthwiseConvÔºöÂâçËÄÖÁî±ÂêéËÄÖÂä†‰∏ä‰∏Ä‰∏™pointwiseConvÁªÑÊàêÔºåÊúÄÂêéÊúâactivationÔºå‰∏≠Èó¥Ê≤°Êúâ cmp Xception and Inception V3 have nearly the same number of parameters marginally better on ImageNet much larger performance increasement on JFT Residual connections are clearly essential in helping with convergence, both in terms of speed and final classification performance. Effect of intermediate activationÔºöthe absence of any non-linearity leads to both faster convergence and better final performance EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks Âä®Êú∫ common senseÔºöscaled up the network for better accuracy we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance propose a new scaling methodÔºöusing compound coefficient to uniformly scale all dimensions of depth/width/resolution on MobileNets and ResNet a new baseline network family EfficientNets much better accuracy and efficiency ËÆ∫ÁÇπ previous work scale up one of the three dimensions depthÔºömore layers widthÔºömore channels image resolutionÔºöhigher resolution arbitrary scaling requires tedious manual tuning and often yields sub-optimal accuracy and efficiency uniformly scalingÔºöOur empirical study shows that it is critical to balance all dimensions of network width/depth/resolution, and surprisingly such balance can be achieved by simply scaling each of them with constant ratio. neural architecture searchÔºöbecomes increasingly popular in designing efficient mobile-size ConvNets ÊñπÊ≥ï problem formulation ConvNetsÔºö$N = \bigodot_{i=1‚Ä¶s} F_i^{L_i}(X_{})$, s for stage, L for repeat times, F for function simplify the design problem fixing $F_i$ all layers must be scaled uniformly with constant ratio an optimization problemÔºöd for depth coefficients, w for width coefficients, r for resolution coefficients max_{d,w,r} \ \ \ Accuracy(N(d,w,r))\\ s.t. \ \ \ N(d,w,r) = \bigodot_{i=1...s} F_i^{d*L_i}(X_{}) observation 1 Scaling up any dimension of network (width, depth, or resolution) improves accuracy, but the accuracy gain diminishes for bigger models. ÂáÜÁ°ÆÁéáÈÉΩ‰ºöÊèêÂçáÔºåÊúÄÁªàÈÉΩ‰ºöÈ•±Âíå depthÔºödeeper ConvNet can capture richer and more complex features widthÔºöwider networks tend to be able to capture more fine-grained features and are easier to train Ôºàcommonly used for small size modelsÔºâ‰ΩÜÊòØÊ∑±Â∫¶ÂíåÂÆΩÂ∫¶ÊúÄÂ•ΩÂåπÈÖçÔºå‰∏ÄÂë≥Âä†ÂÆΩshallow network‰ºöËæÉÈöæÊèêÂèñÈ´òÁ∫ßÁâπÂæÅ resolutionÔºöhigher resolution input can potentially capture more fine-grained patterns observation 2 compound scalingÔºöit is critical to balance all dimensions of network width, depth, and resolution different scaling dimensions are not independent ËæìÂÖ•Êõ¥È´òÁöÑresolutionÔºåÂ∞±ÈúÄË¶ÅÊõ¥Ê∑±ÁöÑÁΩëÁªúÔºå‰ª•Ëé∑ÂèñÊõ¥Â§ßÁöÑÊÑüÂèóÈáéÔºåÂêåÊó∂ËøòÈúÄË¶ÅÊõ¥ÂÆΩÁöÑÁΩëÁªúÔºå‰ª•ÊçïËé∑Êõ¥Â§öÁöÑÁªÜÁ≤íÂ∫¶ÁâπÂæÅ compound coefficient $\phi$Ôºö d = \alpha ^ \phi\\ w = \beta ^ \phi\\ r = \gamma ^ \phi\\ s.t. \alpha * \beta^2 * \gamma^2 \approx 2, \ \alpha\geq1,\ \beta\geq1,\ \gamma\geq1 $\alpha, \beta, \gamma$ are constants determined by a small grid search, controling the assign among the 3 dimensions [d,w,r] $\phi$ controls how many more resources are available for model scaling the total FLOPS will approximately increase by $2^\phi$ efficientNet architecture having a good baseline network is also critical thus we developed a new mobile-size baseline called EfficientNet by leveraging a multi-objective neural architecture search that optimizes both accuracy and FLOPS compound scalingÔºöfix $\phi=1$ and grid search $\alpha, \beta, \gamma$, fix $\alpha, \beta, \gamma$ and use different $\phi$ ÂÆûÈ™å on MobileNets and ResNets compared to other single-dimension scaling methods compound scaling method improves the accuracy on all on EfficientNet model with compound scaling tends to focus on more relevant regions with more object details while other models are either lack of object details or unable to capture all objects in the images implementing details RMSProp: decay=0.9, momentum(rho)=0.9Ôºåtpu‰∏ä‰ΩøÁî®lars BN: momentum=0.99 weight decay = 1e-5 lr: initial=0.256, decays by 0.97 every 2.4 epochs SiLU activation AutoAugment Stochastic depth: survive_prob = 0.8 dropout rate: 0.2 to 0.5 for B0 to B7 EfficientDet: Scalable and Efficient Object Detection Âä®Êú∫ model efficiency for object detectionÔºöbased on one-stage detector ÁâπÂæÅËûçÂêàÔºöpropose a weighted bi-directional feature pyramid network (BiFPN) ÁΩëÁªúrescaleÔºöuniformly scales the resolution, depth, and width for all backbone achieve better accuracy with much fewer parameters and FLOPs also test on Pascal VOC 2012 semantic segmentation ËÆ∫ÁÇπ previous work tends to achieve better efficiency by sacrificing accuracy previous work fuse feature at different resolutions by simply summing up without distinction EfficientNet backboneÔºöcombine EfficientNet backbones with our propose BiFPN scale upÔºöjointly scales up the resolution/depth/width for all backbone, feature network, box/class prediction network Existing object detectors two-stageÔºöhave a region-of-interest proposal step one-stageÔºöhave not, use predefined anchors ÊñπÊ≥ï BiFPNÔºöefficient bidirectional cross-scale connec- tions and weighted feature fusion FPNÔºölimitÊòØÂè™Êúâtop-bottom‰∏ÄÊù°information flow PANetÔºöÂä†‰∏ä‰∫Ü‰∏ÄÊù°bottom-up pathÔºåbetter accuracy‰ΩÜÊòØmore parameters and computations NAS-FPNÔºöÂü∫‰∫éÁΩëÁªúÊêúÁ¥¢Âá∫ÁöÑÁªìÊûÑÔºåirregular and difficult to interpret or modify BiFPN remove those nodes that only have one input edgeÔºöÂè™Êúâ‰∏ÄÊù°ËæìÂÖ•ÁöÑËäÇÁÇπÔºåÊ≤°ÂÅöÂà∞‰ø°ÊÅØËûçÂêà add an extra edge from the original input to output node if they are at the same levelÔºöfuse more features without adding much cost repeat blocks Weighted Feature Fusion since different input features are at different resolutions, they usually contribute to the output feature unequally learnable weight that can be a scalar (per-feature), a vector (per-channel), or a multi-dimensional tensor (per-pixel) weight normalization Softmax-basedÔºö$O=\sum_i \frac{e^{w_i}}{\sum_j e^{w_j}}*I_i$ Fast normalizedÔºö$O=\sum_i \frac{w_i}{\epsilon + \sum_j w_j}*I_i$ÔºåRelu is applied after each $w_i$ to keep non-negative P_6^{td} = Conv(\frac{w_1P_6^{in}+w_2Resize(P_7^{in})}{\epsilon+w_1+w_2})\\ P_6^{out} = Conv(\frac{w_1P_6^{in}+w_2P_6^{td}+w_3Resize(P_5^{out})}{\epsilon+w_1+w_2+w_3}) EfficientDet ImageNet-pretrained Effi- cientNets as the backbone BiFPN serves as the feature network the fused features(level 3-7) are fed to a class and box network respectively compound scaling backboneÔºöreuse the same width/depth scaling coefficients of EfficientNet-B0 to B6 feature networkÔºö depth(layers)Ôºö$D=3+\phi$ width(channes)Ôºö$W=64 \cdot (1.35^{\phi}) $ box/class prediction networkÔºö depthÔºö$D=3+[\phi/3]$ widthÔºösame as FPN resolution use feature 3-7Ôºömust be dividable by $2^7$ $R=512+128*\phi$ EfficientDet-D0 ($\phi=0$) to D7 ($\phi=7$) ÂÆûÈ™å for object detection train Learning rate is linearly increased from 0 to 0.16 in the first training epoch and then annealed down employ commonly-used focal loss 3x3 anchors compare low-accuracy regimeÔºö‰ΩéÁ≤æÂ∫¶‰∏ãÔºåEfficientDet-D0ÂíåyoloV3Â∑Æ‰∏çÂ§ö ‰∏≠Á≠âÁ≤æÂ∫¶ÔºåEfficientDet-D1ÂíåMask-RCNNÂ∑Æ‰∏çÂ§ö EfficientDet-D7 achieves a new state-of-the-art for semantic segmentation modify keep feature level {P2,P3,‚Ä¶,P7} in BiFPN but only use P2 for the final per-pixel classification set the channel size to 128 for BiFPN and 256 for classification head Both BiFPN and classification head are repeated by 3 times compare Âíådeeplabv3ÊØîÁöÑÔºåCOCOÊï∞ÊçÆÈõÜ better accuracy and fewer FLOPs ablation study backbone improves accuracy v.s. resnet50 BiFPN improves accuracy v.s. FPN BiFPN achieves similar accuracy as repeated FPN+PANet BiFPN + weghting achieves the best accuracy NormalizedÔºösoftmaxÂíåfastÁâàÊú¨ÊïàÊûúÂ∑Æ‰∏çÂ§öÔºåÊØè‰∏™ËäÇÁÇπÁöÑweightÂú®ËÆ≠ÁªÉÂºÄÂßãËøÖÈÄüÂèòÂåñÔºàsuggesting different features contribute to the feature fusion unequallyÔºâ Compound ScalingÔºöËøô‰∏™ÊØîÂÖ∂‰ªñÂè™ÊèêÈ´ò‰∏Ä‰∏™ÊåáÊ†áÁöÑÊïàÊûúÂ•ΩÂ∞±‰∏çÁî®ËØ¥‰∫Ü Ë∂ÖÂèÇÔºö efficientNetÂíåefficientDetÁöÑresolutionÊòØ‰∏ç‰∏ÄÊ†∑ÁöÑÔºåÂõ†‰∏∫Ê£ÄÊµãËøòÊúâneckÂíåheadÔºåÂ±ÇÊï∞Êõ¥Ê∑±ÔºåÊâÄ‰ª•resolutionÊõ¥Â§ß EfficientNetV2: Smaller Models and Faster Training Âä®Êú∫ faster training speed and better parameter efficiency use a new op: Fused-MBConv propose progressive learningÔºöadaptively adjuts regularization &amp; image size ÊñπÊ≥ï review of EfficientNet large image size large memory usageÔºåsmall batch sizeÔºålong training time thus propose increasing image size gradually in V2 extensive depthwise conv often cannot fully utilize modern accelerators thus introduce Fused-MBConv in V2ÔºöWhen applied in early stage 1-3, Fused-MBConv can improve training speed with a small overhead on parameters and FLOPs equally scaling up proved sub-optimal in nfnets since the stages are not equally contributed to the efficiency &amp; accuracy thus in V2 use a non-uniform scaling strategyÔºögradually add more layers to later stages(s5 &amp; s6) restrict the max image size EfficientNet V2 Architecture basic ConvBlock use fused-MBConv in the early layers use MBConv in the latter layers expansion ratios use smaller expansion ratios Âõ†‰∏∫ÂêåÊ†∑ÁöÑÈÄöÈÅìÊï∞Ôºåfused-MBÊØîMBÁöÑÂèÇÊï∞ÈáèÂ§ß kernel size ÂÖ®Âõæ3x3ÔºåÊ≤°Êúâ5x5‰∫Ü add more layers to compensate the reduced receptive field last stride 1 stage effv1ÊòØ7‰∏™stage effv2Êúâ6‰∏™stage scaling policy compound scalingÔºöR„ÄÅW„ÄÅD‰∏ÄËµ∑scale ‰ΩÜÊòØÈôêÂà∂‰∫ÜÊúÄÂ§ßinference image size=480Ôºàtrain=384Ôºâ gradually add more layers to later stages (s5 &amp; s6) progressive learning large models require stronger regularization larger image size leads to more computations with larger capacityÔºåthus also needs stronger regularization training process in the early training epochs, we train the network with smaller images and weak regularization gradually increase image size but also making learning more difficult by adding stronger regularization adaptive params image size dropout rate randAug magnitude mixup alpha ÁªôÂÆöÊúÄÂ§ßÊúÄÂ∞èÂÄºÔºåstage NÔºå‰ΩøÁî®linear interpolation train&amp;test details RMSProp optimizer with decay 0.9 and momentum 0.9 batch norm momentum 0.99 weight decay 1e-5 trained for 350 epochs with total batch size 4096 Learning rate is first warmed up from 0 to 0.256, and then decayed by 0.97 every 2.4 epochs exponential moving average with 0.9999 decay rate stochastic depth with 0.8 survival probability 4 stages (87 epochs per stage)Ôºöearly stage with weak regularization &amp; later stronger maximum image size for training is about 20% smaller than inference &amp; no further finetuning]]></content>
      <tags>
        <tag>papers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FCN]]></title>
    <url>%2F2020%2F03%2F28%2FFCN%2F</url>
    <content type="text"><![CDATA[FCN: Fully Convolutional Networks for Semantic Segmentation Âä®Êú∫ take input of arbitrary size pixelwise prediction (semantic segmentation) efficient inference and learning end-to-end with superwised-pretraining ËÆ∫ÁÇπ fully connected layers brings heavy computation patchwise/proposals training with less efficiency (‰∏∫‰∫ÜÂØπ‰∏Ä‰∏™ÂÉèÁ¥†ÂàÜÁ±ªÔºåË¶ÅÊâ£ÂÆÉÂë®Âõ¥ÁöÑpatchÔºå‰∏ÄÂº†ÂõæÁöÑÂ≠òÂÇ®ÂÆπÈáè‰∏äÂçáÂà∞k*kÂÄçÔºåËÄå‰∏îÁõ∏ÈÇªpatchÈáçÂè†ÁöÑÈÉ®ÂàÜÂºïÂÖ•Â§ßÈáèÈáçÂ§çËÆ°ÁÆóÔºåÂêåÊó∂ÊÑüÂèóÈáéÂ§™Â∞èÔºåÊ≤°Ê≥ïÊúâÊïàÂà©Áî®ÂÖ®Â±Ä‰ø°ÊÅØ) fully convolutional structure are used to get a feature extractor which yield a localized, fixed-length feature Semantic segmentation faces an inherent tension between semantics and location: global information resolves what while local information resolves where. Deep feature hierarchies jointly encode location and semantics in a local-to-global pyramid. other semantic works (RCNN) are not end-to-end Ë¶ÅÁ¥† ÊääÂÖ®ËøûÊé•Â±ÇÊç¢Êàê1*1Âç∑ÁßØÔºåÁî®‰∫éÊèêÂèñÁâπÂæÅÔºåÂΩ¢ÊàêÁÉ≠ÁÇπÂõæ ÂèçÂç∑ÁßØÂ∞ÜÂ∞èÂ∞∫ÂØ∏ÁöÑÁÉ≠ÁÇπÂõæ‰∏äÈááÊ†∑Âà∞ÂéüÂ∞∫ÂØ∏ÁöÑËØ≠‰πâÂàÜÂâ≤ÂõæÂÉè a novel ‚Äúskip‚Äù architecture to combine deep, coarse, semantic information and shallow, fine, appearance information ÊñπÊ≥ï fully convolutional network receptive fields: Locations in higher layers correspond to the locations in the image they are path-connected to typical recognition nets: fixed-input patches the fully connected layers can be viewed as convolutions with kernels that cover their entire input regions our structure: arbitrary-input the computation is saved by computing the overlapping regions of those patches only once output size corresponds to the input(H/16, W/16) heatmap: the (H/16 * W/16) high-dims feature-map corresponds to the 1000 classes coarse predictions to dense OverFeat introduced ÂØπ‰∫éÈ´òÁª¥ÁâπÂæÅÂõæ‰∏ä‰∏Ä‰∏™ÂÖÉÁ¥†ÔºåÂØπÂ∫î‰∫ÜÂéüÂõæÊÑüÂèóÈáé‰∏ÄÁâáÂå∫ÂüüÔºåÂ∞Üreception field‰∏≠c‰ΩçÂ°´‰∏äËøô‰∏™ÂÖÉÁ¥†ÁöÑÂÄº ÁßªÂä®ÂéüÂõæÔºåÁõ∏Â∫îÁöÑÊÑüÂèóÈáéÂØπÂ∫îÁöÑÂõæÁâá‰πüÂèëÁîü‰∫ÜÁßªÂä®ÔºåÈ´òÁª¥ÁâπÂæÅÂõæÁöÑËæìÂá∫Âèò‰∫ÜÔºåc‰ΩçÂèò‰∫Ü ÁßªÂä®ËåÉÂõ¥stride*strideÔºåÂ∞±‰ºöÂæóÂà∞ÂéüÂõæÂ∞∫ÂØ∏ÁöÑËæìÂá∫‰∫Ü upsampling simplest: bilinear interpolation in-network upsampling: backwards convolution (deconvolution) with an output stride of f A stack of deconvolution layers and activation functions can even learn a nonlinear upsampling factor: FCNÈáåÈù¢inputsizeÂíåoutputsize‰πãÈó¥Â≠òÂú®Á∫øÊÄßÂÖ≥Á≥ªÔºåÂ∞±ÊòØÊâÄÊúâÂç∑ÁßØpoolingÂ±ÇÁöÑÁ¥ØÁßØÈááÊ†∑Ê≠•Èïø‰πòÁßØ kernelsizeÔºö$2 * factor - factor \% 2$ strideÔºö$factor$ paddingÔºö$ceil((factor - 1) / 2.)$ ËøôÂùóÁöÑËÆ°ÁÆóÊúâÁÇπÁªïÔºå$stride=factor$ÊØîËæÉÂ•ΩÁ°ÆÂÆöÔºåËøôÊòØÂ∞ÜÁâπÂæÅÂõæÊÅ¢Â§çÁöÑÂéüÂõæÂ∞∫ÂØ∏Ë¶ÅrescaleÁöÑÂ∞∫Â∫¶„ÄÇÁÑ∂ÂêéÂú®ËæìÂÖ•ÁöÑÁõ∏ÈÇªÂÖÉÁ¥†‰πãÈó¥ÊèíÂÖ•s-1‰∏™0ÂÖÉÁ¥†ÔºåÂéüÂõæÂ∞∫ÂØ∏Âèò‰∏∫$(s-1)(input_size-1)+input_size = sinput_size + (s-1)$Ôºå‰∏∫‰∫ÜÂæóÂà∞$output_size=s*input_size$ËæìÂá∫ÔºåÂÜçËá≥Â∞ë$padding=[(s-1)/2]_{ceil}$ÔºåÁÑ∂ÂêéÊ†πÊçÆÔºö (s-1) * (in-1) + in + 2p -k + 1 = outÊúâÔºö 2p-k+2 = sÂú®kerasÈáåÈù¢ÂèØ‰ª•Ë∞ÉÁî®Â∫ìÂáΩÊï∞Conv2DTransposeÊù•ÂÆûÁé∞Ôºö 123456789101112131415161718192021x = Input(shape=(64,64,16))y = Conv2DTranspose(filters=16, kernel_size=20, strides=8, padding='same')(x)model = Model(x, y)model.summary()# input: (None, 64, 64, 16) output: (None, 512, 512, 16) params: 102,416x = Input(shape=(32,32,16))y = Conv2DTranspose(filters=16, kernel_size=48, strides=16, padding='same')(x)# input: (None, 32, 32, 16) output: (None, 512, 512, 16) params: 589,840x = Input(shape=(16,16,16))y = Conv2DTranspose(filters=16, kernel_size=80, strides=32, padding='same')(x)# input: (None, 16, 16, 16) output: (None, 512, 512, 16) params: 1,638,416# ÂèÇÊï∞ÂèÇËÄÉÔºöorig unetÁöÑtotalÂèÇÊï∞Èáè‰∏∫36,605,042# ÂêÑÁ∫ßtransposeÁöÑÂèÇÊï∞Èáè‰∏∫Ôºö# (None, 16, 16, 512) 4,194,816# (None, 32, 32, 512) 4,194,816# (None, 64, 64, 256) 1,048,832# (None, 128, 128, 128) 262,272# (None, 256, 256, 32) 16,416 ÂèØ‰ª•ÁúãÂà∞kernel_sizeÂèòÂ§ßÔºåÂØπÂèÇÊï∞ÈáèÁöÑÂΩ±ÂìçÊûÅÂ§ß„ÄÇÔºàkernel_sizeËÆæÁΩÆÁöÑÂ∞è‰∫ÜÔºåÂè™ËÉΩÊèêÂèñÂà∞Âçï‰∏™ÂÖÉÁ¥†ÔºåÊàëËßâÂæókernel_sizeËá≥Â∞ëË¶ÅÂ§ß‰∫éstrideÔºâ Segmentation Architecture use pre-trained model convert all fully connected layers to convolutions append a 1*1 conv with channel dimension(including background) to predict scores followed by a deconvolution layer to upsample the coarse outputs to dense outputs skips the 32 pixel stride at the final prediction layer limits the scale of detail in the upsampled output ÈÄêÂ±ÇupsamplingÔºåËûçÂêàÂâçÂá†Â±ÇÁöÑfeature mapÔºåelement-wise add finer layers: ‚ÄúAs they see fewer pixels, the finer scale predictions should need fewer layers.‚Äù ËøôÊòØÈíàÂØπÂâçÈù¢ÁöÑÂç∑ÁßØÁΩëÁªúÊù•ËØ¥ÔºåÈöèÁùÄÁΩëÁªúÂä†Ê∑±ÔºåÁâπÂæÅÂõæ‰∏äÁöÑÊÑüÂèóÈáéÂèòÂ§ßÔºåÂ∞±ÈúÄË¶ÅÊõ¥Â§öÁöÑchannelÊù•ËÆ∞ÂΩïÊõ¥Â§öÁöÑ‰ΩéÁ∫ßÁâπÂæÅÁªÑÂêà add a 1*1 conv on top of pool4 (zero-initialized) adding a 2x upsampling layer on top of conv7 (We initialize this 2xupsampling to bilinear interpolation, but allow the parameters to be learned) sum the above two stride16 predictions (‚ÄúMax fusion made learning difficult due to gradient switching‚Äù) 16x upsampled back to the image ÂÅöÂà∞Á¨¨‰∏âË°åÂÜçÂæÄ‰∏ãÔºåÁªìÊûúÂèà‰ºöÂèòÂ∑ÆÔºåÊâÄ‰ª•ÂÅöÂà∞ËøôÈáåÂ∞±ÂÅú‰∏ã ÊÄªÁªì Âú®ÂçáÈááÊ†∑ËøáÁ®ã‰∏≠ÔºåÂàÜÈò∂ÊÆµÂ¢ûÂ§ßÊØî‰∏ÄÊ≠•Âà∞‰ΩçÊïàÊûúÊõ¥Â•Ω Âú®ÂçáÈááÊ†∑ÁöÑÊØè‰∏™Èò∂ÊÆµÔºå‰ΩøÁî®ÈôçÈááÊ†∑ÂØπÂ∫îÂ±ÇÁöÑÁâπÂæÅËøõË°åËæÖÂä© 8ÂÄç‰∏äÈááÊ†∑ËôΩÁÑ∂ÊØî32ÂÄçÁöÑÊïàÊûúÂ•Ω‰∫ÜÂæàÂ§öÔºå‰ΩÜÊòØÁªìÊûúËøòÊòØÊØîËæÉÊ®°Á≥äÂíåÂπ≥ÊªëÔºåÂØπÂõæÂÉè‰∏≠ÁöÑÁªÜËäÇ‰∏çÊïèÊÑüÔºåËÆ∏Â§öÁ†îÁ©∂ËÄÖÈááÁî®MRFÁÆóÊ≥ïÊàñCRFÁÆóÊ≥ïÂØπFCNÁöÑËæìÂá∫ÁªìÊûúÂÅöËøõ‰∏ÄÊ≠•‰ºòÂåñ x8‰∏∫Âï•Â•Ω‰∫éx32Ôºö1. x32ÁöÑÁâπÂæÅÂõæÊÑüÂèóÈáéËøáÂ§ßÔºåÂØπÂ∞èÁâ©‰Ωì‰∏çÊïèÊÑü 2. x32ÁöÑÊîæÂ§ßÊØî‰æãÈÄ†ÊàêÁöÑÂ§±ÁúüÊõ¥Â§ß unetÁöÑÂå∫Âà´Ôºö unetÊ≤°Áî®imagenetÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºåÂõ†‰∏∫ÊòØÂåªÂ≠¶ÂõæÂÉè unetÂú®ËøõË°åÊµÖÂ±ÇÁâπÂæÅËûçÂêàÁöÑÊó∂ÂÄôÁî®‰∫ÜconcatËÄåÈùûelement-wise add ÈÄêÂ±Ç‰∏äÈááÊ†∑Ôºåx2 vs. x8/x32 orig unetÊ≤°Áî®padÔºåËæìÂá∫Â∞è‰∫éËæìÂÖ•ÔºåFCNÂàôpad+crop Êï∞ÊçÆÂ¢ûÂº∫ÔºåFCNÊ≤°Áî®Ëøô‰∫õ‚Äòmachinery‚ÄôÔºåÂåªÂ≠¶ÂõæÂÉèÈúÄË¶ÅÂº∫augmentation Âä†ÊùÉloss ‚Äã]]></content>
      <tags>
        <tag>papers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cs231n-RNN-review]]></title>
    <url>%2F2020%2F03%2F15%2Fcs231n-RNN-review%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[attentionÁ≥ªÂàó]]></title>
    <url>%2F2020%2F03%2F13%2Fattention%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[0. ÁªºËø∞ attentionÁöÑÊñπÂºèÂàÜ‰∏∫‰∏§ÁßçÔºàReferenceÔºâ Â≠¶‰π†ÊùÉÈáçÂàÜÂ∏É ÈÉ®ÂàÜÂä†ÊùÉÔºàhard attentionÔºâÔºèÂÖ®ÈÉ®Âä†ÊùÉÔºàsoft attentionÔºâ ÂéüÂõæ‰∏äÂä†ÊùÉÔºèÁâπÂæÅÂõæ‰∏äÂä†ÊùÉ Á©∫Èó¥Â∞∫Â∫¶Âä†ÊùÉÔºèchannelÂ∞∫Â∫¶Âä†ÊùÉÔºèÊó∂Èó¥ÂüüÂä†ÊùÉÔºèÊ∑∑ÂêàÂüüÂä†ÊùÉ CAMÁ≥ªÂàó„ÄÅSE-blockÁ≥ªÂàóÔºöËä±ÂºèÂä†ÊùÉÔºåÂ≠¶‰π†ÊùÉÈáçÔºånon-localÁöÑÊ®°ÂùóÔºå‰ΩúÁî®‰∫éÊüê‰∏™Áª¥Â∫¶ ‰ªªÂä°ÂàÜËß£ ËÆæËÆ°‰∏çÂêåÁöÑÁΩëÁªúÁªìÊûÑÔºàÊàñÂàÜÊîØÔºâ‰∏ìÊ≥®‰∫é‰∏çÂêåÁöÑÂ≠ê‰ªªÂä°Ôºå ÈáçÊñ∞ÂàÜÈÖçÁΩëÁªúÁöÑÂ≠¶‰π†ËÉΩÂäõÔºå‰ªéËÄåÈôç‰ΩéÂéüÂßã‰ªªÂä°ÁöÑÈöæÂ∫¶Ôºå‰ΩøÁΩëÁªúÊõ¥Âä†ÂÆπÊòìËÆ≠ÁªÉ STN„ÄÅdeformable convÔºöÊ∑ªÂä†ÊòæÂºèÁöÑÊ®°ÂùóË¥üË¥£Â≠¶‰π†ÂΩ¢Âèò/receptive fieldÁöÑÂèòÂåñÔºålocalÊ®°ÂùóÔºåapply by pixel local / non-local localÊ®°ÂùóÁöÑÁªìÊûúÊòØpixel-specificÁöÑ non-localÊ®°ÂùóÁöÑÁªìÊûúÊòØÂÖ®Â±ÄÂÖ±ÂêåËÆ°ÁÆóÁöÑÁöÑ Âü∫‰∫éÊùÉÈáçÁöÑattentionÔºàReferenceÔºâ Ê≥®ÊÑèÂäõÊú∫Âà∂ÈÄöÂ∏∏Áî±‰∏Ä‰∏™ËøûÊé•Âú®ÂéüÁ•ûÁªèÁΩëÁªú‰πãÂêéÁöÑÈ¢ùÂ§ñÁöÑÁ•ûÁªèÁΩëÁªúÂÆûÁé∞ Êï¥‰∏™Ê®°Âûã‰ªçÁÑ∂ÊòØÁ´ØÂØπÁ´ØÁöÑÔºåÂõ†Ê≠§Ê≥®ÊÑèÂäõÊ®°ÂùóËÉΩÂ§üÂíåÂéüÊ®°Âûã‰∏ÄËµ∑ÂêåÊ≠•ËÆ≠ÁªÉ ÂØπ‰∫ésoft attentionÔºåÊ≥®ÊÑèÂäõÊ®°ÂùóÂØπÂÖ∂ËæìÂÖ•ÊòØÂèØÂæÆÁöÑÔºåÊâÄ‰ª•Êï¥‰∏™Ê®°Âûã‰ªçÂèØÁî®Ê¢ØÂ∫¶ÊñπÊ≥ïÊù•‰ºòÂåñ ËÄåhard attentionË¶ÅÁ¶ªÊï£Âú∞ÈÄâÊã©ÂÖ∂ËæìÂÖ•ÁöÑ‰∏ÄÈÉ®ÂàÜÔºåËøôÊ†∑Êï¥‰∏™Á≥ªÁªüÂØπ‰∫éËæìÂÖ•‰∏çÂÜçÊòØÂèØÂæÆÁöÑ papers [STN] Spatial Transformer Networks [deformable conv] Deformable Convolutional Networks [CBAM] CBAM: Convolutional Block Attention Module [SE-Net] Squeeze-and-Excitation Networks [SE-blockÁöÑ‰∏ÄÁ≥ªÂàóÂèò‰Ωì] SC-SEÔºàfor segmentationÔºâ„ÄÅCMPE-SEÔºàÂ§çÊùÇÂèàÊ≤°Áî®Ôºâ [SK-Net] Selective Kernel NetworksÔºöÊòØattension moduleÔºå‰ΩÜÊòØ‰∏ªË¶ÅÊîπËøõÁÇπÂú®receptive fieldÔºåtrickÂ§ßÊùÇÁÉ© [GC-Net] GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond CBAM: Convolutional Block Attention Module Âä®Êú∫ attention module lightweight and general improvements in classification and detection ËÆ∫ÁÇπ deeperÔºö can obtain richer representation increased widthÔºöcan outperform an extremely deep network cardinalityÔºöresults in stronger representation power than depth and width attentionÔºöimproves the representation of interests humans exploit a sequence of partial glimpses and selectively focus on salient parts Residual Attention NetworkÔºöcomputes 3d attention map we decompose the process that learns channel attention and spatial attention separately SE-blockÔºöuse global average-pooled features we suggest to use max-pooled features as well ÊñπÊ≥ï sequentially infers a 1D channel attention map and a 2D spatial attention map broadcast and element-wise multiplication Channel attention module focuses on ‚Äòwhat‚Äô is meaningful squeeze the spatial dimension use both average-pooled and max-pooled features simultaneously both descriptors are then forwarded to a shared MLP to reduce dimension „ÄêQUESTION„ÄëÁúãËÆ∫ÊñáMLPÊòØÁ∫øÊÄßÁöÑÂêóÔºåÊ≤°ÂÜôÊøÄÊ¥ªÂáΩÊï∞ then use element-wise summation sigmoid function Spatial attention module focuses on ‚Äòwhere‚Äô apply average-pooling and max-pooling along the channel axis and concatenate 7x7 conv sigmoid function Arrangement of attention modules in a parallel or sequential manner we found sequential better than parallel we found channel-first order slightly better than the spatial-first integration apply CBAM on the convolution outputs in each block in residual path before the add operation ÂÆûÈ™å Ablation studies Channel attentionÔºö‰∏§‰∏™pooling pathÈÉΩÊúâÊïàÔºå‰∏ÄËµ∑Áî®ÊúÄÂ•Ω Spatial attentionÔºö1x1convÁõ¥Êé•squeeze‰πüË°åÔºåavg+maxÊõ¥Â•ΩÔºå7x7convÁï•Â•Ω‰∫é3x3conv arrangementÔºöÂâçÈù¢ËØ¥‰∫ÜÔºåÊØîSEÁöÑÂçïspacial squeezeÂ•ΩÔºåchannelÂú®ÂâçÂ•Ω‰∫éÂú®ÂêéÔºå‰∏≤Ë°åÂ•Ω‰∫éÂπ∂Ë°å Classification resultsÔºöoutperform baselines and SE Network Visualization cover the target object regions better the target class scores also increase accordingly Object Detection results apply to detectorsÔºöright before every classifier apply to backbone SK-Net: Selective Kernel Networks Âä®Êú∫ ÁîüÁâ©ÁöÑÁ•ûÁªèÂÖÉÁöÑÊÑüÂèóÈáéÊòØÈöèÁùÄÂà∫ÊøÄÂèòÂåñËÄåÂèòÂåñÁöÑ propose a selective kernel unit adaptively adjust the RF multiple branches with different kernel sizes guided fusion Â§ßÊùÇÁÉ©Ôºömulti-branch&amp;kernelÔºågroup convÔºådilated convÔºåattention mechanism SKNet by stacking multiple SK units Âú®ÂàÜÁ±ª‰ªªÂä°‰∏äÈ™åËØÅ ËÆ∫ÁÇπ multi-scale aggregation inception blockÂ∞±Êúâ‰∫Ü but linear aggregation approach may be insufficient multi-branch network two-branchÔºö‰ª•resnet‰∏∫‰ª£Ë°®Ôºå‰∏ªË¶ÅÊòØ‰∏∫‰∫Üeasier to train multi-branchÔºö‰ª•inception‰∏∫‰ª£Ë°®Ôºå‰∏ªË¶Å‰∏∫‰∫ÜÂæóÂà∞multifarious features grouped/depthwise/dilated conv grouped convÔºöreduce computationÔºåÊèêÂçáÁ≤æÂ∫¶ depthwise convÔºöreduce computationÔºåÁâ∫Áâ≤Á≤æÂ∫¶ dilated convÔºöenlarge RFÔºåÊØîdense large kernelËäÇÁúÅÂèÇÊï∞Èáè attention mechanism Âä†ÊùÉÁ≥ªÂàóÔºö SENet&amp;CBAMÔºö Áõ∏ÊØî‰πã‰∏ãSKNetÂ§ö‰∫Üadaptive RF Âä®ÊÄÅÂç∑ÁßØÁ≥ªÂàóÔºö STN‰∏çÂ•ΩËÆ≠ÁªÉÔºåËÆ≠Â•Ω‰ª•ÂêéÂèòÊç¢Â∞±ÂÆöÊ≠ª‰∫Ü deformable convËÉΩÂ§üÂú®inferenceÁöÑÊó∂ÂÄô‰πüÂä®ÊÄÅÁöÑÂèòÂåñÂèòÊç¢Ôºå‰ΩÜÊòØÊ≤°Êúâmulti-scaleÂíånonlinear aggregation thus we propose SK convolution multi-kernelsÔºöÂ§ßsizeÁöÑconv kernelÊòØÁî®‰∫Üdilated conv nonlinear aggregation computationally lightweight could successfully embedded into small models workflow split fuse select main difference from inception less customized adaptive selection instead of equally addition ÊñπÊ≥ï selective kernel convolution split multi-branch with different kernel size grouped/depthwise conv + BN + ReLU 5x5 kernel can be further replaced with dilated conv fuse to learn the control of information flow from different branches element-wise summation global average pooling fc-BN-ReLUÔºöreduce dimensionÔºåat least 32 select channel-wise weighting factor A &amp; B &amp; moreÔºöA+B + more = 1 fc-softmax Âú®2ÂàÜÊîØÁöÑÊÉÖÂÜµ‰∏ãÔºå‰∏Ä‰∏™ÊùÉÈáçÁü©ÈòµAÂ∞±Â§ü‰∫ÜÔºåBÊòØÂÜó‰ΩôÁöÑÔºåÂõ†‰∏∫ÂèØ‰ª•Èó¥Êé•ÁÆóÂá∫Êù• reweighting network start from resnext repeated SK unitsÔºöÁ±ª‰ººbottleneck 1x1 conv SK conv 1x1 conv hyperparams number of branches M=2 group number G=32Ôºöcardinality of each path reduction ratio r=16Ôºöfuse operator‰∏≠dim-reductionÁöÑÂèÇÊï∞ ÂµåÂÖ•Âà∞ËΩªÈáèÁöÑÁΩëÁªúÁªìÊûÑ MobileNet/shuffleNet ÊääÂÖ∂‰∏≠ÁöÑ3x3 depthwiseÂç∑ÁßØÊõøÊç¢ÊàêSK conv ÂÆûÈ™å ÊØîsortÁöÑresnet„ÄÅdensenet„ÄÅresnextÁ≤æÂ∫¶ÈÉΩË¶ÅÂ•Ω GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond Âä®Êú∫ Non-Local Network (NLNet) capture long-range dependencies obtain query-specific global context but we found global contexts are almost the same for different query positions we produce query-independent formulation smiliar structure as SE-Net aims at global context modeling ËÆ∫ÁÇπ Capturing long-range dependency mainly by sdeeply stacking conv layersÔºöinefficient non-local network via self-attention mechanism computes the pairwise relations between the query position then aggregate ‰ΩÜÊòØ‰∏çÂêå‰ΩçÁΩÆqueryÂæóÂà∞ÁöÑattention mapÂü∫Êú¨‰∏ÄËá¥ we simply the non-local block query-independent maintain acc &amp; save computation our proposed GC-block unifies both the NL block and the SE block three steps global context modelingÔºö feature transform moduleÔºöcapture channel-wise interdependency fusion moduleÔºömerge into the original features Â§öÁßç‰ªªÂä°‰∏äÂùáÊúâÊ∂®ÁÇπ ‰ΩÜÈÉΩÊòØÂú®Ë∑üresnet50ÂØπÊØî revisit NLNet non-local block $f(x_i, x_j)$Ôºö encodes the relationship between position i &amp; j ËÆ°ÁÆóÊñπÂºèÊúâGaussian„ÄÅEmbedded Gaussian„ÄÅDot product„ÄÅConcat different instantiations achieve comparable performance $C(x)$Ôºönorm factor $x_i + \sum^{N_p} F(x_j)$Ôºöaggregates a specific global feature on $x_i$ widely-used Embedded GaussianÔºö ÂµåÂÖ•ÊñπÂºèÔºö Mask R-CNN with FPN and Res50 only add one non-local block right before the last residual block of res4 observations &amp; inspirations distances among inputs show that input features are discriminated outputs &amp; attention maps are almost the sameÔºöglobal context after training is actually independent of query position inspirations simplify the Non-local block no need of query-specific ÊñπÊ≥ï simplifying form of NL blockÔºöSNL Ê±Ç‰∏Ä‰∏™commonÁöÑglobal featureÔºåshareÁªôÂÖ®ÂõæÊØè‰∏™position Ëøõ‰∏ÄÊ≠•ÁÆÄÂåñÔºöÊää$x_j$ÁöÑ1x1 convÊèêÂà∞ÂâçÈù¢ÔºåFLOPsÂ§ßÂ§ßÂáèÂ∞ëÔºåÂõ†‰∏∫feature scale‰ªéHWÂèòÊàê‰∫Ü1x1 the SNL block achieves comparable performance to the NL block with significantly lower FLOPs global context modeling SNLÂèØ‰ª•ÊäΩË±°Êàê‰∏âÈÉ®ÂàÜÔºö global attention poolingÔºöÈÄöËøá$W_k$ &amp; softmaxËé∑Âèñattention weightsÔºåÁÑ∂ÂêéËøõË°åglobal pooling feature transformÔºö1x1 conv feature aggregationÔºöbroadcast element-wise add SE-block‰πüÂèØ‰ª•ÂàÜËß£ÊàêÁ±ª‰ººÁöÑÊäΩË±° global attention poolingÔºöÁî®‰∫ÜÁÆÄÂçïÁöÑglobal average pooling feature transformÔºöÁî®‰∫Üsqueeze &amp; exciteÁöÑfc-relu-fc-sigmoid feature aggregationÔºöbroadcast element-wise multiplication Global Context Block integrate the benefits of both SNL global attention poolingÔºöeffective modeling on long-range dependency SE bottleneck transformÔºölight computationÔºàÂè™Ë¶ÅratioÂ§ß‰∫é2Â∞±‰ºöËäÇÁúÅÂèÇÊï∞ÈáèÂíåËÆ°ÁÆóÈáèÔºâ ÁâπÂà´Âú∞ÔºåÂú®SE transformÁöÑsqueeze layer‰∏äÔºåÂèàÂä†‰∫ÜBN ease optimization benefit generalization fusionÔºöadd ÂµåÂÖ•ÊñπÂºèÔºö GC-ResNet50 add GC-block to all layers (c3+c4+c5) in resnet50 with se ratio of 16 relationship to SE-block È¶ñÂÖàÊòØfusion method reflects different goals SEÂü∫‰∫éÂÖ®Â±Ä‰ø°ÊÅØrescales the channelsÔºåÈó¥Êé•‰ΩøÁî® GCÁõ¥Êé•‰ΩøÁî®ÔºåÂ∞Ülong-range dependencyÂä†Âú®ÊØè‰∏™position‰∏ä ÂÖ∂Ê¨°ÊòØnorm layer ease optimization ÊúÄÂêéÊòØglobal attention pooling SEÁöÑGAPÊòØa special case weighting factors shows superior]]></content>
      <tags>
        <tag>Ê≥®ÊÑèÂäõÊú∫Âà∂</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeeplabÁ≥ªÂàó]]></title>
    <url>%2F2020%2F02%2F24%2FDeeplab%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[ÁªºËø∞ papers deeplabV1: SEMANTIC IMAGE SEGMENTATION WITH DEEP CONVOLUTIONAL NETS AND FULLY CONNECTED CRFSÔºå‰∏ªË¶ÅË¥°ÁåÆÊèêÂá∫‰∫ÜÁ©∫Ê¥ûÂç∑ÁßØÔºå‰ΩøÂæófeature extractionÈò∂ÊÆµËæìÂá∫ÁöÑÁâπÂæÅÂõæÁª¥ÊåÅËæÉÈ´òÁöÑresolution deeplabV2: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFsÔºå‰∏ªË¶ÅË¥°ÁåÆÊòØÂ§öÂ∞∫Â∫¶ASPPÁªìÊûÑ deeplabV3: Rethinking Atrous Convolution for Semantic Image SegmentationÔºåÊèêÂá∫‰∫ÜÂü∫‰∫éResNetÁöÑ‰∏≤Ë°å&amp;Âπ∂Ë°å‰∏§ÁßçÁªìÊûÑÔºåÁªÜËäÇ‰∏äÊèêÂà∞‰∫Ümulti-gridÔºåÊîπËøõ‰∫ÜASPPÊ®°Âùó deeplabV3+: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation ÂàÜÂâ≤ÁªìÊûúÊØîËæÉÁ≤óÁ≥ôÁöÑÂéüÂõ† Ê±†ÂåñÔºöÂ∞ÜÂÖ®ÂõæÊäΩË±°ÂåñÔºåÈôç‰ΩéÂàÜËæ®ÁéáÔºå‰ºö‰∏¢Â§±ÁªÜËäÇ‰ø°ÊÅØÔºåÂπ≥Áßª‰∏çÂèòÊÄßÔºå‰ΩøÂæóËæπÁïå‰ø°ÊÅØ‰∏çÊ∏ÖÊô∞ Ê≤°ÊúâÂà©Áî®Ê†áÁ≠æ‰πãÈó¥ÁöÑÊ¶ÇÁéáÂÖ≥Á≥ªÔºöCNNÁº∫Â∞ëÂØπÁ©∫Èó¥„ÄÅËæπÁºò‰ø°ÊÅØÁ≠âÁ∫¶Êùü ÂØπÊ≠§ÔºådeeplabV1ÂºïÂÖ•‰∫Ü Á©∫Ê¥ûÂç∑ÁßØÔºöVGG‰∏≠ÊèêÂá∫ÁöÑÂ§ö‰∏™Â∞èÂç∑ÁßØÊ†∏‰ª£ÊõøÂ§ßÂç∑ÁßØÊ†∏ÁöÑÊñπÊ≥ïÔºåÂè™ËÉΩ‰ΩøÊÑüÂèóÈáéÁ∫øÊÄßÂ¢ûÈïøÔºåËÄåÂ§ö‰∏™Á©∫Ê¥ûÂç∑ÁßØ‰∏≤ËÅîÔºåÂèØ‰ª•ÂÆûÁé∞ÊåáÊï∞Â¢ûÈïø„ÄÇ ÂÖ®ËøûÊé•Êù°‰ª∂ÈöèÊú∫Âú∫CRFÔºö‰Ωú‰∏∫stage2ÔºåÊèêÈ´òÊ®°ÂûãÊçïËé∑ÁªÜËäÇÁöÑËÉΩÂäõÔºåÊèêÂçáËæπÁïåÂàÜÂâ≤Á≤æÂ∫¶ Â§ßÂ∞èÁâ©‰ΩìÂêåÊó∂ÂàÜÂâ≤ deeplabV2ÂºïÂÖ• Â§öÂ∞∫Â∫¶ASPP(Atrous Spatial Pyramid Pooling)ÔºöÂπ∂Ë°åÁöÑÈááÁî®Â§ö‰∏™ÈááÊ†∑ÁéáÁöÑÁ©∫Ê¥ûÂç∑ÁßØÊèêÂèñÁâπÂæÅÔºåÂÜçËøõË°åÁâπÂæÅËûçÂêà backbone model changeÔºöVGG16Êîπ‰∏∫ResNet ‰ΩøÁî®‰∏çÂêåÁöÑÂ≠¶‰π†Áéá Ëøõ‰∏ÄÊ≠•ÊîπËøõÊ®°ÂûãÊû∂ÊûÑ deeplabV3ÂºïÂÖ• ASPPÂµåÂÖ•ResNetÂêéÂá†‰∏™block ÂéªÊéâ‰∫ÜCRF ‰ΩøÁî®ÂéüÂßãÁöÑConv/poolÊìç‰ΩúÔºåÂæóÂà∞ÁöÑlow resolution score mapÔºåpool stride‰ºö‰ΩøÂæóËøáÁ®ã‰∏≠‰∏¢ÂºÉ‰∏ÄÈÉ®ÂàÜ‰ø°ÊÅØÔºå‰∏äÈááÊ†∑‰ºöÂæóÂà∞ËæÉÂ§ßÁöÑÂ§±ÁúüÂõæÂÉèÔºå‰ΩøÁî®Á©∫Ê¥ûÂç∑ÁßØÔºå‰øùÁïôÁâπÂæÅÂõæ‰∏äÁöÑÂÖ®ÈÉ®‰ø°ÊÅØÔºåÂêåÊó∂keep resolutionÔºåÂáèÂ∞ë‰∫Ü‰ø°ÊÅØÊçüÂ§± DeeplabV3ÁöÑASPPÁõ∏ÊØîËæÉ‰∫éV2ÔºåÂ¢ûÂä†‰∫Ü‰∏ÄÊù°1x1 conv pathÂíå‰∏ÄÊù°image pooling pathÔºåÂä†GAPËøôÊù°pathÊòØÂõ†‰∏∫ÔºåÂÆûÈ™å‰∏≠ÂèëÁé∞ÔºåÈöèÁùÄrateÁöÑÂ¢ûÂ§ßÔºåÊúâÊïàÁöÑweightÊï∞ÁõÆÂºÄÂßãÂáèÂ∞ëÔºàÈÉ®ÂàÜË∂ÖÂá∫ËæπÁïåÊó†Ê≥ïÊúâÊïàÊçïÊçâËøúË∑ùÁ¶ª‰ø°ÊÅØÔºâÔºåÂõ†Ê≠§Âà©Áî®global average poolingÊèêÂèñ‰∫Üimage-levelÁöÑÁâπÂæÅÂπ∂‰∏éASPPÁöÑÁâπÂæÅÂπ∂Âú®‰∏ÄËµ∑ÔºåÊù•Ë°•ÂÖÖÂõ†‰∏∫dilation‰∏¢Â§±ÁöÑ‰ø°ÊÅØ Á©∫Ê¥ûÂç∑ÁßØÁöÑpathÔºåV2ÊòØÊØèÊù°pathÂàÜÂà´Á©∫Ê¥ûÂç∑ÁßØÁÑ∂ÂêéÊé•‰∏§‰∏™1x1convÔºàÊ≤°ÊúâBNÔºâÔºåV3ÊòØÁ©∫Ê¥ûÂç∑ÁßØÂíåBatchNormalizationÁªÑÂêà fusionÊñπÂºèÔºåV2ÊòØsum fusionÔºåV3ÊòØÊâÄÊúâpath concatÁÑ∂Âêé1x1 convÔºåÂæóÂà∞ÊúÄÁªàscore map DeeplabV3ÁöÑ‰∏≤Ë°åÁâàÊú¨Ôºå‚ÄúIn order to maintain original image size, convolutions are replaced with strous convolutions with rates that differ from each other with factor 2‚ÄùÔºåppt‰∏äËØ¥ÂêéÈù¢Âá†‰∏™blockÂ§çÂà∂‰∫Üblock4ÔºåÊØè‰∏™blockÈáåÈù¢‰∏âÂ±ÇconvÔºåÂÖ∂‰∏≠ÊúÄÂêé‰∏ÄÂ±Çconv stride2ÔºåÁÑ∂Âêé‰∏∫‰∫Ümaintain output sizeÔºåÁ©∫Ê¥ûrate*2ÔºåËøô‰∏™‰∏çÂ§™ÁêÜËß£„ÄÇ multi-grid methodÔºöÂØπÊØè‰∏™blockÈáåÈù¢ÁöÑ‰∏âÂ±ÇÂç∑ÁßØÈááÁî®‰∏çÂêåÁ©∫Ê¥ûÁéáÔºåunit rateÔºàe.g.(1,2,4)Ôºâ * rate Ôºàe.g. 2Ôºâ deeplabV1: SEMANTIC IMAGE SEGMENTATION WITH DEEP CONVOLUTIONAL NETS AND FULLY CONNECTED CRFS Âä®Êú∫ brings together methods from Deep Convolutional Neural Networks and probabilistic graphical models poor localization property of deep networks combine a fully connected Conditional Random Field (CRF) be able to localize segment boundaries beyond previous accuracies speed: atrous accuracy: simplicity: cascade modules ËÆ∫ÁÇπ DCNN learns hierarchical abstractions of data, which is desirable for high-level vision tasks (classification) but it hampers low-level tasks, such as pose estimation and semantic segmentation, where we want precise localization, rather than abstraction of spatial details two technical hurdles in DCNNs when applying to image labeling tasks pooling, loss of resolution: we employ the ‚Äòatrous‚Äô (with holes) for efficient dense computation spacial invariance: we use the fully connected pairwise CRF to capture fine edge details Our approach treats every pixel as a CRF node exploits long-range dependencies and uses CRF inference to directly optimize a DCNN-driven cost function ÊñπÊ≥ï structure fully convolutional VGG-16 keep the first 3 subsampling blocks for a target stride of 8 use hole algorithm conv filters for the last two blocks keep the pooling layers for the purpose of fine-tuingÔºåchange strides from 2 to 1 for dense map(h/8), the first fully convolutional 7*7*4096 is computational, thus change to 4*4 / 3*3 convs further computation decreasement: reduce the fc channels from 4096 to 1024 train labelÔºöground truth subsampled by 8 loss functionÔºöcross-entropy test x8Ôºösimply bilinear interpolation fcnÔºöstride32 forces them to use learned upsampling layers, significantly increasing the complexity and training time CRF short-rangeÔºöused to smooth noisy fully connected modelÔºöto recover detailed local structure rather than further smooth it energy function: E(x) = \sum_{i}\theta_i(x_i) + \sum_{ij}\theta_{ij}(x_i, x_j)\\ \theta_i(x_i) = -logP(x_i)\\ $P(x_i)$ is the bi-linear interpolated probability output of DCNN. \theta_{ij}(x_i, x_j) = \mu(x_i, x_j)\sum_{m=1}^K \omega_m k^m (f_i,f_j)\\ \mu(x_i, x_j) = \begin{cases} 1& \text{if }x_i \neq x_j\\ 0& \text{otherwise} \end{cases} $k^m(f_i, f_j)$ is the Gaussian kernel depends on features (involving pixel positions &amp; pixel color intensities) multi-scale prediction to increase the boundary localization accuracy we attach to the input image and the output of each of the first four max pooling layers a two-layer MLP (first layer: 128 3x3 convolutional filters, second layer: 128 1x1 convolutional filters) the feature maps above is concatenated to the main network‚Äôs last layer feature map the new outputs is enhanced by 128*5=640 channels we only adjust the newly added weights introducing these extra direct connections from fine-resolution layers improves localization performance, yet the effect is not as dramatic as the one obtained with the fully-connected CRF Á©∫Ê¥ûÂç∑ÁßØdilated convolution Á©∫Ê¥ûÂç∑ÁßØÁõ∏ÊØîËæÉ‰∫éÊ≠£Â∏∏Âç∑ÁßØÔºåÂ§ö‰∫Ü‰∏Ä‰∏™ hyper-parameter‚Äî‚Äîdilation rateÔºåÊåáÁöÑÊòØkernelÁöÑÈó¥ÈöîÊï∞Èáè(Ê≠£Â∏∏ÁöÑconvolution dilatation rateÊòØ1) fcnÔºöÂÖàpoolingÂÜçupsamplingÔºåËøáÁ®ã‰∏≠Êúâ‰ø°ÊÅØÊçüÂ§±ÔºåËÉΩ‰∏çËÉΩËÆæËÆ°‰∏ÄÁßçÊñ∞ÁöÑÊìç‰ΩúÔºå‰∏çÈÄöËøápooling‰πüËÉΩÊúâËæÉÂ§ßÁöÑÊÑüÂèóÈáéÁúãÂà∞Êõ¥Â§öÁöÑ‰ø°ÊÅØÂë¢Ôºü Â¶ÇÂõæ(b)ÁöÑ2-dilated convÔºåkernel sizeÂè™Êúâ3x3Ôºå‰ΩÜÊòØËøô‰∏™Âç∑ÁßØÁöÑÊÑüÂèóÈáéÂ∑≤ÁªèÂ¢ûÂ§ßÂà∞‰∫Ü7x7ÔºàÂÅáËÆæÂâç‰∏ÄÂ±ÇÊòØ3x3ÁöÑ1-dilated convÔºâ Â¶ÇÂõæ(c)ÁöÑ4-dilated convÔºåkernel sizeÂè™Êúâ3x3Ôºå‰ΩÜÊòØËøô‰∏™Âç∑ÁßØÁöÑÊÑüÂèóÈáéÂ∑≤ÁªèÂ¢ûÂ§ßÂà∞‰∫Ü15x15ÔºàÂÅáËÆæÂâç‰∏§Â±ÇÊòØ3x3ÁöÑ1-dilated convÂíå3x3ÁöÑ2-dilated convÔºâ ËÄå‰º†ÁªüÁöÑ‰∏â‰∏™3x3ÁöÑ1-dilated convÂ†ÜÂè†ÔºåÂè™ËÉΩËææÂà∞7x7ÁöÑÊÑüÂèóÈáé dilated‰ΩøÂæóÂú®‰∏çÂÅöpoolingÊçüÂ§±‰ø°ÊÅØÁöÑÊÉÖÂÜµ‰∏ãÔºåÂä†Â§ß‰∫ÜÊÑüÂèóÈáéÔºåËÆ©ÊØè‰∏™Âç∑ÁßØËæìÂá∫ÈÉΩÂåÖÂê´ËæÉÂ§ßËåÉÂõ¥ÁöÑ‰ø°ÊÅØ The Gridding EffectÔºöÂ¶Ç‰∏ãÂõæÔºåÂ§öÊ¨°Âè†Âä†3x3ÁöÑ2-dilated convÔºå‰ºöÂèëÁé∞Êàë‰ª¨Â∞ÜÊÑøËæìÂÖ•Á¶ªÊï£Âåñ‰∫Ü„ÄÇÂõ†Ê≠§Âè†Âä†Âç∑ÁßØÁöÑ dilation rate ‰∏çËÉΩÊúâÂ§ß‰∫é1ÁöÑÂÖ¨Á∫¶Êï∞„ÄÇ Long-ranged informationÔºöÂ¢ûÂ§ßdilation rateÂØπÂ§ßÁâ©‰ΩìÊúâÊïàÊûúÔºåÂØπÂ∞èÁâ©‰ΩìÂèØËÉΩÊúâÂºäÊó†Âà© HDC(Hybrid Dilated Convolution)ËÆæËÆ°ÁªìÊûÑ Âè†Âä†Âç∑ÁßØÁöÑ dilation rate ‰∏çËÉΩÊúâÂ§ß‰∫é1ÁöÑÂÖ¨Á∫¶Êï∞ÔºåÂ¶Ç[2,4,6] Â∞Ü dilation rate ËÆæËÆ°ÊàêÈîØÈΩøÁä∂ÁªìÊûÑÔºå‰æãÂ¶Ç [1, 2, 5, 1, 2, 5] Âæ™ÁéØÁªìÊûÑÔºåÈîØÈΩøÁä∂ËÉΩÂ§üÂêåÊó∂Êª°Ë∂≥Â∞èÁâ©‰ΩìÂ§ßÁâ©‰ΩìÁöÑÂàÜÂâ≤Ë¶ÅÊ±Ç(Â∞è dilation rate Êù•ÂÖ≥ÂøÉËøëË∑ùÁ¶ª‰ø°ÊÅØÔºåÂ§ß dilation rate Êù•ÂÖ≥ÂøÉËøúË∑ùÁ¶ª‰ø°ÊÅØ) Êª°Ë∂≥$M_i = max [M_{i+1}-2r_i, M_{i+1}-2(M_{i+1}-r_i), r_i]$Ôºå$M_i$ÊòØÁ¨¨iÂ±ÇÊúÄÂ§ßdilation rate ‰∏Ä‰∏™ÂèØË°åÊñπÊ°à[1,2,5]Ôºö deeplabV2: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs Âä®Êú∫ atrous convolutionÔºöcontrol the resolution atrous spatial pyramid pooling (ASPP) Ôºömultiple sampling rates fully connected Conditional Random Field (CRF) ËÆ∫ÁÇπ three challenges in the application of DCNNs to semantic image segmentation reduced feature resolutionÔºömax-pooling and downsampling (‚Äòstriding‚Äô) ‚Äî&gt; atrous convolution existence of objects at multiple scalesÔºömulti input scale ‚Äî&gt; ASPP reduced localization accuracy due to DCNN invarianceÔºöskip-layers ‚Äî&gt; CRF improvements compared to its first version better segment objects at multiple scales ResNet replaces VGG16 a more comprehensive experimental evaluation on models &amp; dataset related works jointly learning of the DCNN and CRF to form an end-to-end trainable feed-forward network while in our work still a 2 stage process use a series of atrous convolutional layers with increasing rates to aggregate multiscale context while in our structure using parallel instead of serial ÊñπÊ≥ï atrous convolution Âú®‰∏ãÈááÊ†∑‰ª•ÂêéÁöÑÁâπÂæÅÂõæ‰∏äÔºåËøêË°åÊôÆÈÄöÂç∑ÁßØÔºåÁõ∏ÂΩì‰∫éÂú®ÂéüÂõæ‰∏äËøêË°å‰∏äÈááÊ†∑ÁöÑfilter 1-DÁ§∫ÊÑèÂõæ‰∏äÂèØ‰ª•ÁúãÂá∫Ôºå‰∏§ËÄÖÊÑüÂèóÈáéÁõ∏Âêå ÂêåÊó∂ËÉΩ‰øùÊåÅhigh resolution while both the number of filter parameters and the number of operations per position stay constant Êääbackbone‰∏≠‰∏ãÈááÊ†∑ÁöÑÂ±Ç(pooling/conv)‰∏≠ÁöÑstrideÊîπÊàê1ÔºåÁÑ∂ÂêéÂ∞ÜÊé•‰∏ãÊù•ÁöÑconvÂ±ÇÈÉΩÊîπÊàê2-dilated convÔºöcould allow us to compute feature responses at the original image resolution efficiency/accuracy trade-offÔºöusing atrous convolution to increase the resolution by a factor of 4 followed by fast bilinear interpolation by a factor of 8 to the original image resolution Bilinear interpolation is sufficient in this setting because the class score maps are quite smooth unlike FCN Atrous convolution offers easily control of the field-of-view and finds the best trade-off between accurate localization (small field-of-view) and context assimilation (large field-of-view)ÔºöÂ§ßÊÑüÂèóÈáéÔºåÊäΩË±°ËûçÂêà‰∏ä‰∏ãÊñáÔºåÂ§ßÊÑüÂèóÈáéÔºålow-levelÂ±ÄÈÉ®‰ø°ÊÅØÂáÜÁ°Æ ÂÆûÁé∞ÔºöÔºà1ÔºâÊ†πÊçÆÂÆö‰πâÔºåÁªôfilter‰∏äÈááÊ†∑ÔºåÊèí0ÔºõÔºà2ÔºâÁªôfeature map‰∏ãÈááÊ†∑ÂæóÂà∞k*k‰∏™reduced resolution mapsÔºåÁÑ∂Âêérun orgin convÔºåÁªÑÂêà‰ΩçÁßªÁªìÊûú ASPP multi input scaleÔºö run parallel DCNN branches that share the same parameters fuse by taking at each position the maximum response across scales computing spatial pyramid pooling run multiple parallel filters with different rates multi-scale features are further processed in separate branchesÔºöfc7&amp;fc8 fuseÔºösum fusion CRFÔºökeep the same as V1 deeplabV3: Rethinking Atrous Convolution for Semantic Image Segmentation Âä®Êú∫ for segmenting objects at multiple scales employ atrous convolution in cascade or in parallel with multiple atrous rates augment ASPP with image-level features encoding global context and further boost performance without DenseCRF ËÆ∫ÁÇπ our proposed module consists of atrous convolution with various rates and batch normalization layers modules in cascade or in parallelÔºöwhen applying a 3*3 atrous convolution with an extremely large rate, it fails to capture long range information due to image boundary effects ÊñπÊ≥ï Atrous Convolution for each location $i$ on the output $y$ and a filter $w$, an $r$-rate atrous convolution is applied over the input feature map $x$Ôºö y[i] = \sum_k x[i+rk]w[k] in cascade duplicate several copies of the last ResNet block (block4) extra block5, block6, block7 as replicas of block4 multi-rates ASPP we include batch normalization within ASPP as the sampling rate becomes larger, the number of valid filter weights becomes smaller (beyond boundary) to incorporate global context informationÔºöwe adopt image-level features by GAP on the last feature map of the model GAP ‚Äî&gt; 1*1*256 conv ‚Äî&gt; BN ‚Äî&gt; bilinearly upsample fusion: concatenated + 1*1 conv segÔºöfinal 1*1*n_classes conv training details large crop size required to make sure the large atrous rates effective upsample the output: it is important to keep the groundtruths intact and instead upsample the final logits ÁªìËÆ∫ output stride=8 Â•ΩËøá16Ôºå‰ΩÜÊòØËøêÁÆóÈÄüÂ∫¶ÊÖ¢‰∫ÜÂá†ÂÄç deeplabV3+: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation Âä®Êú∫ spatial pyramid pooling module captures rich contextual information encode-decoder structure captures sharp object boundaries combine the above two methods propose a simple yet effective decoder module explore Xception backbone ËÆ∫ÁÇπ even though rich semantic information is encoded through ASPP, detailed information related to object boundaries is missing due to striding operations atrous convolution could alleviate but suffer the computational balance while encoder-decoder models lend themselves to faster computation (since no features are dilated) in the encoder path and gradually recover sharp object boundaries in the decoder path ÊâÄË∞ìencoder-decoder structureÔºåÂ∞±ÊòØÈÄöËøáencoderÂíådecoder‰πãÈó¥ÁöÑÁü≠ËøûÊé•Êù•Â∞Ü‰∏çÂêåÂ∞∫Â∫¶ÁöÑÁâπÂæÅÈõÜÊàêËµ∑Êù•ÔºåÂ¢ûÂä†ËøôÊ†∑ÁöÑshortcutÔºåÂêåÊó∂Â¢ûÂ§ßÁΩëÁªúÁöÑ‰∏ãÈááÊ†∑ÁéáÔºàencoder path‰∏ä‰∏ç‰ΩøÁî®Á©∫Ê¥ûÂç∑ÁßØÔºåÂõ†Ê≠§‰∏∫‰∫ÜËææÂà∞ÂêåÊ†∑ÁöÑÊÑüÂèóÈáéÔºåÂæóÂ¢ûÂä†poolingÔºåÁÑ∂Âêé‰øùÁïôÊúÄÂ∫ïÁ´ØÁöÑASPP blockÔºâÔºåÊó¢ÂáèÂ∞ë‰∫ÜËÆ°ÁÆóÔºåÂèàenrich‰∫Ülocal borderËøôÁßçÁªÜËäÇÁâπÂæÅ applying the atrous separable convolution to both the ASPP and decoder modulesÔºöÊúÄÂêéÂèàÂºïÂÖ•ÂèØÂàÜÁ¶ªÂç∑ÁßØÔºåËøõ‰∏ÄÊ≠•ÊèêÂçáËÆ°ÁÆóÊïàÁéá ÊñπÊ≥ï atrous separable convolution significantly reduces the computation complexity while maintaining similar (or better) performance DeepLabv3 as encoder output_stride=16/8Ôºöremove the striding of the last 1/2 blocks atrous convolutionÔºöapply atrous convolution to the blocks without striding ASPPÔºörun 1x1 conv in the end to set the output channel to 256 proposed decoder naive decoderÔºöbilinearly upsampled by 16 proposedÔºöfirst bilinearly upsampled by 4, then concatenated with the corresponding low-level features low-level featuresÔºö apply 1x1 conv on the low-level features to reduce the number of channels to avoid outweigh the importance the last feature map in res2x residual block before striding combined featuresÔºöapply 3x3 conv(2 layers, 256 channels) to obtain sharper segmentation results more shortcutÔºöobserved no significant improvement modified Xception backbone deeper all the max pooling operations are replaced with depthwise separable convolutions with striding DWconv-BN-ReLU-PWconv-BN-ReLU ÂÆûÈ™å decoder effect on border f Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation]]></content>
      <tags>
        <tag>papers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RCNNÁ≥ªÂàó]]></title>
    <url>%2F2020%2F01%2F08%2FRCNN%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[ÁªºËø∞ papers [R-CNN] R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation [SPP] SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition [Fast R-CNN] Fast R-CNN: Fast Region-based Convolutional Network [Faster R-CNN] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks [Mask R-CNN] Mask R-CNN [FPN] FPN: Feature Pyramid Networks for Object Detection [Cascade R-CNN] Cascade R-CNN: Delving into High Quality Object Detection R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation Âä®Êú∫ localizing objects with a deep network and training a high-capacity model with only a small quantity of annotated detection data apply CNN to region proposals: R-CNN represents ‚ÄòRegions with CNN features‚Äô supervised pre-training ËÆ∫ÁÇπ model as a regression problem: not fare well in practice build a sliding-window detector: have to maintain high spatial resolution what we do: our method gener- ates around 2000 category-independent region proposals for the input image, extracts a fixed-length feature vector from each proposal using a CNN, and then classifies each region with category-specific linear SVMs conventional solution to training a large CNN is ‚Äòusing unsupervised pre-training, followed by supervised fine-tuning‚Äô what we do: ‚Äòsupervised pre-training on a large auxiliary dataset (ILSVRC), followed by domain specific fine-tuning on a small dataset (PASCAL)‚Äô we also demonstrate: a simple bounding box regression method significantly reduces mislocalizations R-CNN operates on regions: it is natural to extend it to the task of semantic segmentation Ë¶ÅÁ¥† category-independent region proposals a large convolutional neural network that extracts a fixed-length feature vector from each region a set of class-specific linear SVMs ÊñπÊ≥ï Region proposals: we use selective search Feature extraction: we use Krizhevsky CNN, 227*227 RGB input, 5 convs, 2 fcs, 4096 output we first dilate the tight bounding box (padding=16) then warp the bounding box to the required size (ÂêÑÂêëÂºÇÊÄßÁº©Êîæ) Test-time detection: we score each extracted feature vector using the SVM trained for each class we apply a greedy non-maximum suppression (for each class independently) ÂØπÁïô‰∏ãÁöÑËøô‰∫õÊ°ÜËøõË°åcannyËæπÁºòÊ£ÄÊµãÔºåÂ∞±ÂèØ‰ª•ÂæóÂà∞bounding-box (then B-BoxRegression) Supervised pre-training: pre-trained the CNN on a large auxiliary dataset (ILSVRC 2012) with image-level annotations Domain-specific fine-tuning: continue SGD training of the CNN using only warped region proposals from VOC replace the 1000-way classification layer with a randomly initialized 21-way layer (20 VOC classes plus background) class label: all region proposals with ‚â• 0.5 IoU overlap with a ground-truth box as positives, else negatives 1/10th of the initial pre-training rate uniformly sample 32 positive windows (over all classes) and 96 background windows to construct a mini-batch of size 128 Object category classifiers: considering a binary classifier for a specific class class label: take IoU overlap threshold &lt;0.3 as negatives, take only regions tightly enclosing the object as positives take the ground-truth bounding boxes for each class as positives unexplained: the positive and negative examples are defined differently in CNN fine-tuning versus SVM training CNNÂÆπÊòìËøáÊãüÂêàÔºåÈúÄË¶ÅÂ§ßÈáèÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºåÊâÄ‰ª•Âú®CNNËÆ≠ÁªÉÈò∂ÊÆµÊàë‰ª¨ÂØπBounding boxÁöÑ‰ΩçÁΩÆÈôêÂà∂Êù°‰ª∂ÈôêÂà∂ÁöÑÊØîËæÉÊùæ(IOUÂè™Ë¶ÅÂ§ß‰∫é0.5ÈÉΩË¢´Ê†áÊ≥®‰∏∫Ê≠£Ê†∑Êú¨)ÔºåsvmÈÄÇÁî®‰∫éÂ∞ëÊ†∑Êú¨ËÆ≠ÁªÉÔºåÊâÄ‰ª•ÂØπ‰∫éËÆ≠ÁªÉÊ†∑Êú¨Êï∞ÊçÆÁöÑIOUË¶ÅÊ±ÇÊØîËæÉ‰∏•Ê†ºÔºåÊàë‰ª¨Âè™ÊúâÂΩìbounding boxÊääÊï¥‰∏™Áâ©‰ΩìÈÉΩÂåÖÂê´ËøõÂéª‰∫ÜÔºåÊàë‰ª¨ÊâçÊääÂÆÉÊ†áÊ≥®‰∏∫Áâ©‰ΩìÁ±ªÂà´„ÄÇ it‚Äôs necessary to train detection classifiers rather than simply use outputs of the fine-tuned CNN ‰∏ä‰∏Ä‰∏™ÂõûÁ≠îÂÖ∂ÂÆûÂêåÊó∂‰πüËß£Èáä‰∫ÜCNNÁöÑheadÂ∑≤ÁªèÊòØ‰∏Ä‰∏™ÂàÜÁ±ªÂô®‰∫ÜÔºåËøòË¶ÅÁî®SVMÂàÜÁ±ªÔºöÊåâÁÖß‰∏äËø∞Ê≠£Ë¥üÊ†∑Êú¨ÂÆö‰πâÔºåCNN softmaxÁöÑËæìÂá∫ÊØîÈááÁî®svmÁ≤æÂ∫¶‰Ωé„ÄÇ ÂàÜÊûê learned features: compute the units‚Äô activations on a large set of held-out region proposals sort from the highest to low perform non-maximum suppression display the top-scoring regions Ablation studies: without fine-tuning: features from fc7 generalize worse than features from fc6, indicating that most of the CNN‚Äôs representational power comes from its convolutional layers with fine-tuning: The boost from fine-tuning is much larger for fc6 and fc7 than for pool5, suggests that pool features learned from ImageNet are general and that most of the improvement is gained from learning domain-specific non-linear classifiers on top of them Detection error analysis: more of our errors result from poor localization rather than confusion CNN features are much more discriminative than HOG Loose localization likely results from our use of bottom-up region proposals and the positional invariance learned from pre-training the CNN for whole-image classification(Á≤óÊö¥ÁöÑIOUÂà§ÂÆöÂâçËÉåÊôØÔºå‰∫åÂÄºÂåñlabelÔºåÊó†Ê≥ï‰ΩìÁé∞ÂÆö‰ΩçÂ•ΩÂùèÂ∑ÆÂºÇ) Bounding box regressionÔºö a linear regression model use the pool5 features for a selective search region proposal as input ËæìÂá∫‰∏∫xyÊñπÂêëÁöÑÁº©ÊîæÂíåÂπ≥Áßª ËÆ≠ÁªÉÊ†∑Êú¨ÔºöÂà§ÂÆö‰∏∫Êú¨Á±ªÁöÑÂÄôÈÄâÊ°Ü‰∏≠ÂíåÁúüÂÄºÈáçÂè†Èù¢ÁßØÂ§ß‰∫é0.6ÁöÑÂÄôÈÄâÊ°Ü Semantic segmentationÔºö three strategies for computing features: ‚Äòfull ‚Äò ignores the region‚Äôs shape, two regions with different shape might have very similar bounding boxes(‰ø°ÊÅØ‰∏çÂÖÖÂàÜ) ‚Äòfg ‚Äò slightly outperforms full, indicating that the masked region shape provides a stronger signal ‚Äòfull+fg ‚Äò achieves the best, indicating that the context provided by the full features is highly informative even given the fg features(ÂΩ¢Áä∂Âíåcontext‰ø°ÊÅØÈÉΩÈáçË¶Å) SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition Âä®Êú∫Ôºö propose a new pooling strategy, ‚Äúspatial pyramid pooling‚Äù can generate a fixed-length representation regardless of image size/scale also robust to object deformations ËÆ∫ÁÇπÔºö existing CNNs require a fixed-size input reduce accuracy for sub-images of an arbitrary size/scale (need cropping/warping) cropped region lost content, while warped content generates unwanted distortion overlooks the issues involving scales convolutional layers do not require a fixed image size, whle the fully-connected layers need to have fixed- size/length input by their definition by introducing the SPP layer between the last convolutional layer and the first fully-connected layer pools the features and generates fixed- length outputs Spatial pyramid pooling partitions the image into divisions from finer to coarser levels, and aggregates local features in them generates fixed- length output uses multi-level spatial bins(robust to object deformations ) can run at variable scales also allows varying sizes or scales during training: train the network with different input size at different epoch increases scale-invariance reduces over-fitting in object detection run the convolutional layers only once on the entire image then extract features by SPP-net on the feature maps speedup accuracy ÊñπÊ≥ïÔºö Convolutional Layers and Feature Maps the outputs of the convolutional layers are known as feature maps feature maps involve not only the strength of the responses(the strength of activation), but also their spatial positions(the reception field) The Spatial Pyramid Pooling Layer it can maintain spatial information by pooling in local spatial bins the spatial bins have sizes proportional to the image size(k-level: 1*1, 2*2, ‚Ä¶, k*k) we can resize the input image to any scale, which is important for the accuracy the coarsest pyramid level has a single bin that covers the entire image, which is in fact a ‚Äúglobal pooling‚Äù operation for a feature map of $a√óa$, with a pyramid level of $n√ón$ bins: the\ window\ size:\ win = ceiling(a/n)\\ the\ stride:\ str = floor(a/n) Training the Network Single-size training: fixed-size input (224√ó224) cropped from images, cropping for data augmentation Multi-size training: rather than cropping, we resize the aforementioned 224√ó224 region to 180√ó180, then we train two fixed-size networks that share parameters by altenate epoch ÂàÜÊûê 50 bins vs. 30 bins: the gain of multi-level pooling is not simply due to more parameters, it is because the multi-level pooling is robust to the variance in object deformations and spatial layout multi-size vs. single-size: multi results are more or less better than the single-size version full vs. crop: shows the importance of maintaining the complete content SPP-NET FOR OBJECT DETECTION We extract the feature maps from the entire image only once we apply the spatial pyramid pooling on each candidate window of the feature maps These representations are provided to the fully-connected layers of the network SVM samples: We use the ground-truth windows to generate the positive samples, use the samples with IOU&lt;30% as the negative samples multi-scale feature extraction: We resize the image at {480, 576, 688, 864, 1200}, and compute the feature maps of conv5 for each scale. we choose a single scale s ‚àà S such that the scaled candidate window has a number of pixels closest to 224√ó224. And we use the corresponding feature map to compute the feature for this window this is roughly equivalent to resizing the window to 224√ó224 fine-tuning: Since our features are pooled from the conv5 feature maps from windows of any sizes for simplicity we only fine-tune the fully-connected layers Mapping a Window to Feature Maps** we project the corner point of a window onto a pixel in the feature maps, such that this corner point in the image domain is closest to the center of the receptive field of that feature map pixel. ‚Äã Á°ÆÂÆöÂéüÂõæ‰∏äÁöÑ‰∏§‰∏™ËßíÁÇπÔºàÂ∑¶‰∏äËßíÂíåÂè≥‰∏ãËßíÔºâÔºåÊò†Â∞ÑÂà∞ feature map‰∏äÁöÑ‰∏§‰∏™ÂØπÂ∫îÁÇπÔºå‰ΩøÂæóÊò†Â∞ÑÁÇπ$(x^{‚Äò}, y^{‚Äò})$Âú®ÂéüÂßãÂõæ‰∏äÊÑüÂèóÈáéÔºà‰∏äÂõæÁªøËâ≤Ê°ÜÔºâÁöÑ‰∏≠ÂøÉÁÇπ‰∏é$(x,y)$Â∞ΩÂèØËÉΩÊé•Ëøë„ÄÇ Fast R-CNN: Fast Region-based Convolutional Network Âä®Êú∫ improve training and testing speed increase detection accuracy ËÆ∫ÁÇπ current approaches train models in multi-stage pipelines that are slow and inelegant R-CNN &amp; SPPnet: CNN+SVM+bounding-box regression disk storage: features are written to disk SPPnet: can only fine-tuning the fc layers, limits the accuracy of very deep networks task complexity: numerous candidate proposals rough localization proposals must be refined We propose: a single-stage training algorithm multi-task: jointly learns to classify object proposals and refine their spatial locations Ë¶ÅÁ¥† input: an entire image and a set of object proposals convs a region of interest (RoI) pooling layer: extracts a fixed-length feature vector from the feature map fcs that finally branch into two sibling output layers multi-outputs: one produces softmax probability over K+1 classes one outputs four bounding-box regression offsets per class ÊñπÊ≥ï RoI pooling an RoI is a rectangular window inside a conv feature map, which can be defined by (r, c, h, w) the RoI pooling layer converts the features inside any valid RoI into a small feature map with a fixed size H √ó W it is a special case of SPPnet when there is only one pyramid level (pooling window size = h/H * w/W) Initializing from pre-trained networks the last max pooling layer is replaced by a RoI pooling layer the last fully connected layer and softmax is replaced by the wo sibling layers + respective head (softmax &amp; regressor) modified to take two inputs Fine-tuning for detection why SPPnet is unable to update weights below the spatial pyramid pooling layer: ÂéüÊñáÊèêÂà∞feature vectorÊù•Ê∫ê‰∫é‰∏çÂêåÂ∞∫ÂØ∏ÁöÑÂõæÂÉè‚Äî‚Äî‰∏çÊòØ‰∏ªË¶ÅÂéüÂõ† feature vectorÂú®ÂéüÂõæ‰∏äÁöÑÊÑüÂèóÈáéÈÄöÂ∏∏ÂæàÂ§ßÔºàÊé•ËøëÂÖ®ÂõæÔºâ‚Äî‚Äîforward passÁöÑËÆ°ÁÆóÈáèÂ∞±ÂæàÂ§ß ‰∏çÂêåÁöÑÂõæÁâáforward passÁöÑËÆ°ÁÆóÁªìÊûú‰∏çËÉΩÂ§çÁî®Ôºàwhen each training sample (i.e. RoI) comes from a different image, which is exactly how R-CNN and SPPnet networks are trainedÔºâ We propose: takes advantage of feature sharing mini-batches are sampled hierarchically: N images and R/N RoIs from each image RoIs from the same image share computation and memory in the forward and backward passes jointly optimize the two tasks each RoI is labeled with a ground-truth class $u$ and a ground-truth bounding-box regression target $v$ the network outputs are K+1 probability $p=(p_0,‚Ä¶p_k)$ and K b-box regression offsets $t^k=(t_x^k, t_y^k, t_w^k,t_h^k)$ L(p, u, t^u, v) = L_{cls}(p,u) + \lambda[u>0]L_{loc}(t^u,v)\\ $L_{cls}$: L_{cls}(p,u) = -log p_u\\ $L_{loc}$: L_{loc}(t^u, v) = \sum_{i \in \{x,y,w,h\}}smooth_{L_1}(t^u_i - v_i)\\ smooth_{L_1}(x) = \begin{cases} 0.5x^2\ \ \ \ \ \ \ \ \ \ \ if |x|]]></content>
      <tags>
        <tag>ÁõÆÊ†áÊ£ÄÊµãÔºåtwo-stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN VisualizationÁ≥ªÂàó]]></title>
    <url>%2F2020%2F01%2F03%2FCNN-Visualization%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[1. Visualizing and Understanding Convolutional Networks Âä®Êú∫ give insight into the internal operation and behavior of the complex models then one can design better models reveal which parts of the scene in image are important for classification explore the generalization ability of the model to other datasets ËÆ∫ÁÇπ most visualizing methods limited to the 1st layer where projections to pixel space are possible Our approach propose a method that could projects high level feature maps to the pixel space * some methods give some insight into invariances basing on a simple quadratic approximation * Our approach, by contrast, provides a non-parametric view of invariance * some methods associate patches that responsible for strong activations at higher layers * In our approach they are not just crops of input images, but rather top-down projections that reveal structures ÊñπÊ≥ï 3.1 Deconvnet: use deconvnet to project the feature activations back to the input pixel space To examine a given convnet activation, we set all other activations in the layer to zero and pass the feature maps as input to the attached deconvnet layer Then successively (i) unpool, (ii) rectify and (iii) filter to reconstruct the activity of the layer beneath until the input pixel space is reached „ÄêUnpooling„Äëusing switches „ÄêRectification„Äëthe convnet uses relu to ensure always positive, same for back projection „ÄêFiltering„Äëtransposed conv Due to unpooling, the reconstruction obtained from a single activation resembles a small piece of the original input image 3.2 CNN model 3.3 visualization among layers for each layer, we take the top9 strongest activation across the validation data calculate the back projection separately alongside we provide the corresponding image patches 3.4 visualization during training randomly choose several strongest activation of a given feature map lower layers converge fast, higher layers conversely 3.5 visualizing the Feature Invariance 5 sample images being translated, rotated and scaled by varying degrees Small transformations have a dramatic effect in the first layer of the model(c2 &amp; c3ÂØπÊØî) the network is stable to translations and scalings, but not invariant to rotation 3.6 architecture selection old architecture(stride4, filterSize11)ÔºöThe first layer filters are a mix of extremely high and low frequency information, with little coverage of the mid frequencies. The 2nd layer visualization shows aliasing artifacts caused by the large stride 4 used in the 1st layer convolutions. (ËøôÁÇπÂèØ‰ª•ÂèÇËÄÉ‰πãÂâçvnet‰∏≠ÊèêÂà∞ÁöÑÔºådeconvÂØºËá¥ÁöÑÊ£ãÁõòÊ†º‰º™ÂΩ±ÔºåÂ§ßstride‰ºöÊõ¥ÊòéÊòæ) smaller stride &amp; smaller filter(stride2, filterSize7)Ôºömore coverage of mid frequencies, no aliasing, no dead feature 3.7 ÂØπ‰∫éÁâ©‰ΩìÁöÑÂÖ≥ÈîÆÈÉ®ÂàÜÈÅÆÊå°‰πãÂêé‰ºöÊûÅÂ§ßÁöÑÂΩ±ÂìçÂàÜÁ±ªÁªìÊûú Á¨¨‰∫å‰∏™ÂíåÁ¨¨‰∏â‰∏™‰æãÂ≠ê‰∏≠ÂàÜÂà´ÊòØÊñáÂ≠óÂíå‰∫∫ËÑ∏ÁöÑÂìçÂ∫îÊõ¥È´òÔºå‰ΩÜÊòØÂç¥‰∏çÊòØÂÖ≥ÈîÆÈÉ®ÂàÜ„ÄÇ ÁêÜËß£ 4.1 ÊÄªÁöÑÊù•ËØ¥ÔºåÁΩëÁªúÂ≠¶‰π†Âà∞ÁöÑÁâπÂæÅÔºåÊòØÂÖ∑ÊúâËæ®Âà´ÊÄßÁöÑÁâπÂæÅÔºåÈÄöËøáÂèØËßÜÂåñÂ∞±ÂèØ‰ª•ÁúãÂà∞Êàë‰ª¨ÊèêÂèñÂà∞ÁöÑÁâπÂæÅÂøΩËßÜ‰∫ÜËÉåÊôØÔºåËÄåÊòØÊääÂÖ≥ÈîÆÁöÑ‰ø°ÊÅØÁªôÊèêÂèñÂá∫Êù•‰∫Ü„ÄÇ‰ªélayer 1„ÄÅlayer 2Â≠¶‰π†Âà∞ÁöÑÁâπÂæÅÂü∫Êú¨‰∏äÊòØÈ¢úËâ≤„ÄÅËæπÁºòÁ≠â‰ΩéÂ±ÇÁâπÂæÅÔºõlayer 3ÂàôÂºÄÂßãÁ®çÂæÆÂèòÂæóÂ§çÊùÇÔºåÂ≠¶‰π†Âà∞ÁöÑÊòØÁ∫πÁêÜÁâπÂæÅÔºåÊØîÂ¶Ç‰∏äÈù¢ÁöÑ‰∏Ä‰∫õÁΩëÊ†ºÁ∫πÁêÜÔºõlayer 4Â≠¶‰π†Âà∞ÁöÑÂàôÊòØËæÉÂ§öÁöÑÁ±ªÂà´‰ø°ÊÅØÔºåÊØîÂ¶ÇÁãóÂ§¥Ôºõlayer 5ÂØπÂ∫îÁùÄÊõ¥Âº∫ÁöÑ‰∏çÂèòÊÄßÔºåÂèØ‰ª•ÂåÖÂê´Áâ©‰ΩìÁöÑÊï¥‰Ωì‰ø°ÊÅØ„ÄÇ„ÄÇ 4.2 Âú®ÁΩëÁªúËø≠‰ª£ÁöÑËøáÁ®ã‰∏≠ÔºåÁâπÂæÅÂõæÂá∫Áé∞‰∫Üsudden jumps„ÄÇ‰ΩéÂ±ÇÂú®ËÆ≠ÁªÉÁöÑËøáÁ®ã‰∏≠Âü∫Êú¨Ê≤°Âï•ÂèòÂåñÔºåÊØîËæÉÂÆπÊòìÊî∂ÊïõÔºåÈ´òÂ±ÇÁöÑÁâπÂæÅÂ≠¶‰π†ÂàôÂèòÂåñÂæàÂ§ß„ÄÇËøôËß£Èáä‰∫Ü‰ΩéÂ±ÇÁΩëÁªúÁöÑ‰ªéËÆ≠ÁªÉÂºÄÂßãÔºåÂü∫Êú¨‰∏äÊ≤°ÊúâÂ§™Â§ßÁöÑÂèòÂåñÔºåÂõ†‰∏∫Ê¢ØÂ∫¶Âº•Êï£„ÄÇÈ´òÂ±ÇÁΩëÁªúÂàöÂºÄÂßãÂá†Ê¨°ÁöÑËø≠‰ª£ÔºåÂèòÂåñ‰∏çÊòØÂæàÂ§ßÔºå‰ΩÜÊòØÂà∞‰∫Ü40~50ÁöÑËø≠‰ª£ÁöÑÊó∂ÂÄôÔºåÂèòÂåñÂæàÂ§ßÔºåÂõ†Ê≠§Êàë‰ª¨‰ª•ÂêéÂú®ËÆ≠ÁªÉÁΩëÁªúÁöÑÊó∂ÂÄôÔºå‰∏çË¶ÅÁùÄÊÄ•ÁúãÁªìÊûúÔºåÁúãÁªìÊûúÈúÄË¶Å‰øùËØÅÁΩëÁªúÊî∂Êïõ„ÄÇ 4.3 ÂõæÂÉèÁöÑÂπ≥Áßª„ÄÅÁº©Êîæ„ÄÅÊóãËΩ¨ÔºåÂèØ‰ª•ÁúãÂá∫Á¨¨‰∏ÄÂ±Ç‰∏≠ÂØπ‰∫éÂõæÂÉèÂèòÂåñÈùûÂ∏∏ÊïèÊÑüÔºåÁ¨¨7Â±ÇÂ∞±Êé•Ëøë‰∫éÁ∫øÊÄßÂèòÂåñ„ÄÇ 2. Striving for Simplicity: The All Convolutional Net Âä®Êú∫ traditional pipeline: alternating convolution and max-pooling layers followed by a small number of fully connected layers questioning the necessity of different components in the pipeline, max-pooling layer to be specified to analyze the network we introduce a new variant of the ‚Äúdeconvolution approach‚Äù for visualizing features ËÆ∫ÁÇπ two major improving directions based on traditional pipeline using more complex activation functions building multiple conv modules we study the most simple architecture we could conceive a homogeneous network solely consisting of convolutional layers without the need for complicated activation functions, any response normalization or max-pooling reaches state of the art performance ÊñπÊ≥ï replace the pooling layers with standard convolutional layers with stride two the spatial dimensionality reduction performed by pooling makes covering larger parts of the input in higher layers possible which is crucial for achieving good performance with CNNs make use of small convolutional layers greatly reduce the number of parameters in a network and thus serve as a form of regularization if the topmost convolutional layer covers a portion of the image large enough to recognize its content then fully connected layers can also be replaced by simple 1-by-1 convolutions the overall architecture consists only of convolutional layers with rectified linear non-linearities and an averaging + softmax layer to produce predictions Strided-CNN-C: pooling is removed and the preceded conv stride is increase ConvPool-CNN-C: a dense conv is placed, to show the effect of increasing parameters All-CNN-C: max-pooling is replaced by conv when pooling is replaced by an additional convolution layer with stride 2, performance stabilizes and even improves small 3 √ó 3 convolutions stacked after each other seem to be enough to achieve the best performance guided backpropagation the paper above proposed ‚Äòdeconvnet‚Äô, which we observe that it does not always work well without max-pooling layers For higher layers of our network the method of Zeiler and Fergus fails to produce sharp, recognizable image structure Our architecture does not include max-pooling, thus we can ‚Äôdeconvolve‚Äô without switches, i.e. not conditioning on an input image In order to obtain a reconstruction conditioned on an input image from our network without pooling layers we to combine the simple backward pass and the deconvnet Interestingly, the very first layer of the network does not learn the usual Gabor filters, but higher layers do 3. Cam: Learning Deep Features for Discriminative Localization Âä®Êú∫ we found that CNNs actually behave as object detectors despite no supervision on the location this ability is lost when fully-connected layers are used for classification we found that the advantages of global average pooling layers are beyond simply acting as a regularizer it makes it easily to localize the discriminative image regions despite not being trained for them ËÆ∫ÁÇπ 2.1 Weakly-supervised object localization previous methods are not trained end-to-end and require multiple forward passes Our approach is trained end-to-end and can localize objects in a single forward pass 2.2 Visualizing CNNs previous methods only analyze the convolutional layers, ignoring the fully connected thereby painting an incomplete picture of the full story we are able to understand our network from the beginning to the end ÊñπÊ≥ï 3.1 Class Activation Mapping A class activation map for a particular category indicates the discriminative image regions used by the network to identify that category the network architecture: convs‚Äî-gap‚Äî-fc+softmax we can identify the importance of the image regions by projecting back the weights of the output layer on to the convolutional feature maps by simply upsampling the class activation map to the size of the input image we can identify the image regions most relevant to the particular category 3.2 Weakly-supervised Object Localization our technique does not adversely impact the classification performance when learning to localize we found that the localization ability of the networks improved when the last convolutional layer before GAP had a higher spatial resolution, thus we removed several convolutional layers from the origin networks overall we find that the classification performance is largely preserved for our GAP networks compared with the origin fc structure our CAM approach significantly outperforms the backpropagation approach on generating bounding box low mapping resolution prevents the network from obtaining accurate localizations 3.3 Visualizing Class-Specific Units the convolutional units of various layers of CNNs act as visual concept detec- tors, identifying low-level concepts like textures or mate- rials, to high-level concepts like objects or scenes Deeper into the network, the units become increasingly discriminative given the fully-connected layers in many networks, it can be difficult to identify the importance of different units for identifying different categories 4. Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization5. Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks6. ÁªºËø∞ GAP È¶ñÂÖàÂõûÈ°æ‰∏Ä‰∏ãGAPÔºåNiN‰∏≠ÊèêÂá∫‰∫ÜGAPÔºå‰∏ªË¶Å‰∏∫‰∫ÜËß£ÂÜ≥ÂÖ®ËøûÊé•Â±ÇÂèÇÊï∞ËøáÂ§öÔºå‰∏çÊòìËÆ≠ÁªÉ‰∏îÂÆπÊòìËøáÊãüÂêàÁ≠âÈóÆÈ¢ò„ÄÇ ÂØπÂ§ßÂ§öÊï∞ÂàÜÁ±ª‰ªªÂä°Êù•ËØ¥‰∏ç‰ºöÂõ†‰∏∫ÂÅö‰∫ÜgapËÆ©ÁâπÂæÅÂèòÂ∞ëËÄåËÆ©Ê®°ÂûãÊÄßËÉΩ‰∏ãÈôç„ÄÇÂõ†‰∏∫GAPÂ±ÇÊòØ‰∏Ä‰∏™ÈùûÁ∫øÊÄßÊìç‰ΩúÂ±ÇÔºåËøôC‰∏™ÁâπÂæÅÁõ∏ÂΩì‰∫éÊòØ‰ªékxkxCÁªèËøáÈùûÁ∫øÊÄßÂèòÂåñÈÄâÊã©Âá∫Êù•ÁöÑÂº∫ÁâπÂæÅ„ÄÇ heatmap step1. ÂõæÂÉèÁªèËøáÂç∑ÁßØÁΩëÁªúÂêéÊúÄÂêéÂæóÂà∞ÁöÑÁâπÂæÅÂõæÔºåÂú®ÂÖ®ËøûÊé•Â±ÇÂàÜÁ±ªÁöÑÊùÉÈáçÔºà$w_{k,n}$ÔºâËÇØÂÆö‰∏çÂêåÔºå step2. Âà©Áî®ÂèçÂêë‰º†Êí≠Ê±ÇÂá∫ÊØèÂº†ÁâπÂæÅÂõæÁöÑÊùÉÈáçÔºå step3. Áî®ÊØèÂº†ÁâπÂæÅÂõæ‰πò‰ª•ÊùÉÈáçÂæóÂà∞Â∏¶ÊùÉÈáçÁöÑÁâπÂæÅÂõæÔºåÂú®Á¨¨‰∏âÁª¥Ê±ÇÂùáÂÄºÔºåreluÊøÄÊ¥ªÔºåÂΩí‰∏ÄÂåñÂ§ÑÁêÜ reluÂè™‰øùÁïôwxÂ§ß‰∫é0ÁöÑÂÄº‚Äî‚ÄîÊàë‰ª¨Ê≠£ÂìçÂ∫îÊòØÂØπÂΩìÂâçÁ±ªÂà´ÊúâÁî®ÁöÑÁâπÂæÅÔºåË¥üÂìçÂ∫î‰ºöÊãâ‰Ωé$\sum wx$ÔºåÂç≥‰ºöÈôç‰ΩéÂΩìÂâçÁ±ªÂà´ÁöÑÁΩÆ‰ø°Â∫¶ Â¶ÇÊûúÊ≤°ÊúâreluÔºåÂÆö‰ΩçÂõæË∞±ÊòæÁ§∫ÁöÑ‰∏ç‰ªÖ‰ªÖÊòØÊüê‰∏ÄÁ±ªÁöÑÁâπÂæÅ„ÄÇËÄåÊòØÊâÄÊúâÁ±ªÂà´ÁöÑÁâπÂæÅ„ÄÇ step4. Â∞ÜÁâπÂæÅÂõæresizeÂà∞ÂéüÂõæÂ∞∫ÂØ∏Ôºå‰æø‰∫éÂè†Âä†ÊòæÁ§∫ CAM CAMË¶ÅÊ±ÇÂøÖÈ°ª‰ΩøÁî®GAPÂ±ÇÔºå CAMÈÄâÊã©softmaxÂ±ÇÂÄºÊúÄÂ§ßÁöÑËäÇÁÇπÂèçÂêë‰º†Êí≠ÔºåÊ±ÇGAPÂ±ÇÁöÑÊ¢ØÂ∫¶‰Ωú‰∏∫ÁâπÂæÅÂõæÁöÑÊùÉÈáçÔºåÊØè‰∏™GAPÁöÑËäÇÁÇπÂØπÂ∫î‰∏ÄÂº†ÁâπÂæÅÂõæ„ÄÇ Grad-CAM Grad-CAM‰∏çÈúÄË¶ÅÈôêÂà∂Ê®°ÂûãÁªìÊûÑÔºå Grad-CAMÈÄâÊã©softmaxÂ±ÇÂÄºÊúÄÂ§ßÁöÑËäÇÁÇπÂèçÂêë‰º†Êí≠ÔºåÂØπÊúÄÂêé‰∏ÄÂ±ÇÂç∑ÁßØÂ±ÇÊ±ÇÊ¢ØÂ∫¶ÔºåÁî®ÊØèÂº†ÁâπÂæÅÂõæÁöÑÊ¢ØÂ∫¶ÁöÑÂùáÂÄº‰Ωú‰∏∫ËØ•ÁâπÂæÅÂõæÁöÑÊùÉÈáç„ÄÇ]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NiN: network in network]]></title>
    <url>%2F2019%2F12%2F25%2FNiN-network-in-network%2F</url>
    <content type="text"><![CDATA[Network In Network Âä®Êú∫ enhance model discriminability(Ëé∑ÂæóÊõ¥Â•ΩÁöÑÁâπÂæÅÊèèËø∞)Ôºöpropose mlpconv less prone to overfittingÔºöpropose global average pooling ËÆ∫ÁÇπ comparison 1: conventional CNN uses linear filter, which implicitly makes the assumption that the latent concepts are linearly separable. traditional CNN is stacking [linear filters+nonlinear activation/linear+maxpooling+nonlinear]ÔºöËøôÈáåÂºïÂá∫‰∫Ü‰∏Ä‰∏™ÊøÄÊ¥ªÂáΩÊï∞ÂíåÊ±†ÂåñÂ±ÇÂÖàÂêéÈ°∫Â∫èÁöÑÈóÆÈ¢òÔºåÂØπ‰∫éavg_poollingÔºå‰∏§ÁßçÊìç‰ΩúÂæóÂà∞ÁöÑÁªìÊûúÊòØ‰∏ç‰∏ÄÊ†∑ÁöÑÔºåÂÖàÊé•ÊøÄÊ¥ªÂáΩÊï∞‰ºö‰∏¢Â§±ÈÉ®ÂàÜ‰ø°ÊÅØÔºåÊâÄ‰ª•Â∫îËØ•ÂÖàÊ±†ÂåñÂÜçÊøÄÊ¥ªÔºåÂØπ‰∫éMAX_poolingÔºå‰∏§ÁßçÊìç‰ΩúÁªìÊûú‰∏ÄÊ†∑Ôºå‰ΩÜÊòØÂÖàÊ±†Âåñ‰∏ãÈááÊ†∑ÔºåÂèØ‰ª•ÂáèÂ∞ëÊøÄÊ¥ªÂáΩÊï∞ÁöÑËÆ°ÁÆóÈáèÔºåÊÄªÁªìÂ∞±ÊòØÂÖàÊ±†ÂåñÂÜçÊøÄÊ¥ª„ÄÇ‰ΩÜÊòØÂ•ΩÂ§öÁΩëÁªúÂÆûÈôÖÂÆûÁé∞‰∏äÈÉΩÊòØreluÁ¥ßË∑üÁùÄconvÔºåÂêéÈù¢Êé•poolingÔºåËøôÊ†∑ÊØîËæÉinterpretable‚Äî‚Äîcross feature map pooling mlpconv layer can be regarded as a highly nonlinear function(filter-fc-activation-fc-activation-fc-activation‚Ä¶) comparison 2: maxout network imposes the prior that instances of a latent concept lie within a convex set in the input space„ÄêQUESTION HERE„Äë mlpconv layer is a universal function approximator instead of a convex function approximator comparison 3: fully connected layers are prone to overfitting and heavily depend on dropout regularization global average pooling is more meaningful and interpretable, moreover it itself is a structural regularizer„ÄêQUESTION HERE„Äë ÊñπÊ≥ï use mlpconv layer to replace conventional GLM(linear filters) use global average pooling to replace traditional fully connected layers the overall structure is a stack of mlpconv layers, on top of which lie the global average pooling and the objective cost layer Sub-sampling layers can be added in between the mlpconv as in CNN dropout is applied on the outputs of all but the last mlpconv layers for regularization another regularizer applied is weight decay ÁªÜËäÇ preprocessingÔºöglobal contrast normalization and ZCA whitening augmentationÔºötranslation and horizontal flipping GAP for conventional CNNÔºöCNN+FC+DROPOUT &lt; CNN+GAP &lt; CNN+FC gap is effective as a regularizer slightly worse than the dropout regularizer result for some reason confidence maps explicitly enforce feature maps in the last mlpconv layer of NIN to be confidence maps of the categories by means of global average poolingÔºöNiNÂ∞ÜGAPÁöÑËæìÂá∫Áõ¥Êé•‰Ωú‰∏∫output layerÔºåÂõ†Ê≠§ÊØè‰∏Ä‰∏™Á±ªÂà´ÂØπÂ∫îÁöÑfeature mapÂèØ‰ª•Ëøë‰ººËÆ§‰∏∫ÊòØ confidence map„ÄÇ the strongest activations appear roughly at the same region of the object in the original imageÔºöÁâπÂæÅÂõæ‰∏äÈ´òÂìçÂ∫îÂå∫ÂüüÂü∫Êú¨‰∏éÂéüÂõæ‰∏äÁõÆÊ†áÂå∫ÂüüÂØπÂ∫î„ÄÇ this motivates the possibility of performing object detection via NIN architectureÔºöÂÆûÈôÖ‰∏≠Â§öÂ±ÇÊÑüÁü•Âô®‰ΩøÁî®1x1convÊù•ÂÆûÁé∞ÔºåÂ¢ûÂä†ÁöÑÂ§öÂ±ÇÊÑüÁü•Âô®Áõ∏ÂΩì‰∫éÊòØ‰∏Ä‰∏™Âê´ÂèÇÁöÑÊ±†ÂåñÂ±ÇÔºåÈÄöËøáÂØπÂ§ö‰∏™ÁâπÂæÅÂõæËøõË°åÂê´ÂèÇÊ±†ÂåñÔºåÂÜç‰º†ÈÄíÂà∞‰∏ã‰∏ÄÂ±ÇÁªßÁª≠Âê´ÂèÇÊ±†ÂåñÔºåËøôÁßçÁ∫ßËÅîÁöÑË∑®ÈÄöÈÅìÁöÑÂê´ÂèÇÊ±†ÂåñËÆ©ÁΩëÁªúÊúâ‰∫ÜÊõ¥Â§çÊùÇÁöÑË°®ÂæÅËÉΩÂäõ„ÄÇ ÊÄªÁªì mlpconvÔºöstronger local reception unit gapÔºöregularizer &amp; bring confidence maps]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[unet & vnet]]></title>
    <url>%2F2019%2F12%2F05%2Funet-vnet%2F</url>
    <content type="text"><![CDATA[U-NET: Convolutional Networks for Biomedical Image Segmentation Âä®Êú∫Ôºö train from very few images outperforms more precisely on segmentation tasks fast Ë¶ÅÁ¥†Ôºö ÁºñÁ†ÅÔºöa contracting path to capture context Ëß£Á†ÅÔºöa symmetric expanding path that enables precise localization ÂÆûÁé∞Ôºöpooling operators &amp; upsampling operators ËÆ∫ÁÇπÔºö when we talk about deep convolutional networksÔºö larger and deeper millions of parameters millions of training samples representative methodÔºörun a sliding-window and predict a pixel label based on its‚Äò patch drawbacksÔºö calculating redundancy of overlapping patches big patchÔºömore max-pooling layers that reduce the localization accuracy small patchÔºöless involvement of context metioned but not further explainedÔºöcascade structure ÊñπÊ≥ïÔºö In order to localize, high resolution features from the contracting path are combined with the upsampled output. A successive convolution layer can then learn to assemble a more precise output based on this information. ÁêÜËß£ÔºöÊ∑±Â±ÇÁâπÂæÅÂ±ÇÊÑüÂèóÈáéËæÉÂ§ßÔºåÂ∏¶ÊúâÂÖ®Â±Ä‰ø°ÊÅØÔºåÂ∞ÜÂÖ∂‰∏äÈááÊ†∑Áî®‰∫éÊèê‰æõlocalization informationÔºåËÄåÊ®™ÂêëaddËøáÊù•ÁâπÂæÅÂ±ÇÂ∏¶ÊúâÂ±ÄÈÉ®ÁâπÂæÅ‰ø°ÊÅØ„ÄÇ‰∏§‰∏™3*3ÁöÑconv blockÁî®‰∫éÂ∞Ü‰∏§Á±ª‰ø°ÊÅØÊï¥ÂêàÔºåËæìÂá∫Êõ¥Á≤æÁ°ÆÁöÑË°®Ëææ„ÄÇ In the upsampling part we have also a large number of feature channels, which allow the network to propagate context information to higher resolution layers. ÁêÜËß£ÔºöÂ∫îËØ•ÊòØÂ≠óÈù¢ÊÑèÊÄùÂêßÔºå‰∏∫‰∏äÈááÊ†∑ÁöÑÂç∑ÁßØÂ±Ç‰øùÁïôÊõ¥Â§öÁöÑÁâπÂæÅÈÄöÈÅìÔºåÂ∞±Áõ∏ÂΩì‰∫é‰øùÁïô‰∫ÜÊõ¥Â§öÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØ„ÄÇ we use excessive data augmentation. ÁªÜËäÇÔºö contracting pathÔºö typical CNNÔºöblocks of [2 3*3 unpadded convs+ReLU+2*2 stride2 maxpooling] At each downsampling step we double the number of feature channels expansive pathÔºö upsamplingÔºö 2*2 up-conv that half the channels concatenation the corresponding cropped feature map from the contracting path 2 [3x3 conv+ReLU] final layerÔºöuse a 1*1 conv to map the feature vectors to class vectors trainÔºö prefer larger input size to larger batch size sgd with 0.99 momentum so that the previously seen samples dominate the optimization lossÔºösoftmax &amp; cross entropy unbalanced weightÔºö pre-compute the weight map base on the frequency of pixels for a certain class add the weight for a certain element to force the learning emphasisÔºöe.g. the small separation borders initializationÔºöGaussian distribution data augmentationÔºö deformations ‚ÄúDrop-out layers at the end of the contracting path perform further implicit data augmentation‚Äù metricsÔºö‚Äúwarping error‚Äù, the ‚ÄúRand error‚Äù and the ‚Äúpixel error‚Äù for EM segmentation challenge and average IOU for ISBI cell tracking challenge predictionÔºö ÊåâÁÖßËÆ∫ÊñáÁöÑÊ®°ÂûãÁªìÊûÑÔºåËæìÂÖ•ÂíåËæìÂá∫ÁöÑÁª¥Â∫¶ÊòØ‰∏ç‰∏ÄÊ†∑ÁöÑ‚Äî‚ÄîÂú®valid paddingÁöÑËøáÁ®ã‰∏≠ÊúâËæπÁºò‰ø°ÊÅØÊçüÂ§±„ÄÇ ÈÇ£‰πàÂ¶ÇÊûúÊàë‰ª¨ÊÉ≥Ë¶ÅÈ¢ÑÊµãÈªÑÊ°ÜÂÜÖÁöÑÂàÜÂâ≤ÁªìÊûúÔºåÈúÄË¶ÅËæìÂÖ•‰∏ÄÂº†Êõ¥Â§ßÁöÑÂõæÔºàËìùÊ°ÜÔºâ‰Ωú‰∏∫ËæìÂÖ•ÔºåÂú®ÂõæÁâáËæπÁºòÁöÑÊó∂ÂÄôÔºåÊàë‰ª¨ÈÄöËøáÈïúÂÉèÁöÑÊñπÂºèË°•ÂÖ®„ÄÇ Âõ†ÊûúÂÖ≥Á≥ªÔºö È¶ñÂÖàÂõ†‰∏∫ÂÜÖÂ≠òÈôêÂà∂ÔºåËæìÂÖ•ÁöÑ‰∏çÊòØÊï¥Âº†ÂõæÔºåÊòØÂõæÁâápatchÔºå ‰∏∫‰∫Ü‰øùÁïô‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºå‰ΩøÂæóÈ¢ÑÊµãÊõ¥ÂáÜÁ°ÆÔºåÊàë‰ª¨ÁªôÂõæÁâápatchÊ∑ªÂä†‰∏ÄÂúàborderÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºàÂÆûÈôÖÊÑüÂÖ¥Ë∂£ÁöÑÊòØÈªÑÊ°ÜÂå∫ÂüüÔºâ Âú®ËÆ≠ÁªÉÊó∂Ôºå‰∏∫‰∫ÜÈÅøÂÖçÈáçÂè†ÂºïÂÖ•ÁöÑËÆ°ÁÆóÔºåÂç∑ÁßØÂ±Ç‰ΩøÁî®‰∫Üvalid padding Âõ†Ê≠§Âú®ÁΩëÁªúÁöÑËæìÂá∫Â±ÇÔºåËæìÂá∫Â∞∫ÂØ∏ÊâçÊòØÊàë‰ª¨ÁúüÊ≠£ÂÖ≥Ê≥®ÁöÑÈÉ®ÂàÜ Â¶ÇÊûúËÆ≠ÁªÉÊ†∑Êú¨Â∞∫ÂØ∏‰∏çÈÇ£‰πàhugeÔºåÂÆåÂÖ®ÂèØ‰ª•ÂÖ®ÂõæËæìÂÖ•ÔºåÁÑ∂Âêé‰ΩøÁî®same paddingÔºåÁõ¥Êé•È¢ÑÊµãÂÖ®Âõæmask ÊÄªÁªìÔºö train from very few images ‚Äî-&gt; data augmentation fast ‚Äî-&gt; full convolution layers precise ‚Äî-&gt; global? V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation Âä®Êú∫ entire 3D volume imbalance between the number of foreground and background voxelsÔºödice coefficient limited dataÔºöapply random non-linear transformations and histogram matching fast and accurate ËÆ∫ÁÇπÔºö early approaches based on patches local context challenging modailities efficiency issues fully convolutional networks 2D so far imbalance issueÔºöthe anatomy of interest occupies only a very small region of the scan thus predictions are strongly biased towards the background. re-weighting dice coefficient claims to be better that above Ë¶ÅÁ¥†Ôºö a compression path a decompression path ÊñπÊ≥ïÔºö compressionÔºö add residualËÉΩÂ§üÂä†ÈÄüÊî∂Êïõ resolution is reduced by [2*2*2 conv with stride 2]Áõ∏ÊØî‰∫émaxpoolingËäÇÁúÅ‰∫ÜbpÊâÄÈúÄswitch mapÁöÑmemoryÊ∂àËÄó double the number of feature maps as we reduce their resolution PReLU decompressionÔºö horizontal connectionsÔºö1) gather fine grained detail that would be otherwise lost in the compression path 2) improve the convergence time residual convÔºöblocks of [5*5*5 conv with stride 1] ÊèêÂèñÁâπÂæÅÁªßÁª≠Â¢ûÂ§ßÊÑüÂèóÈáé up-convÔºöexpands the spatial support of the lower resolution feature maps last layerÔºörun [1*1*1conv with 2 channel+softmax] to obtain the voxelwise probabilistic segmentations of the foreground and background dice coefficientÔºö [0,1] which we aim to maximiseÔºåassume $p_i$„ÄÅ$g_i$ belong to two binary volumes D = \frac{2\sum_i^N p_i g_i}{\sum_i^N p_i^2 + \sum_i^N g_i^2} trainÔºö input fix size 128 √ó 128 √ó 64 voxels and a spatial resolution of 1 √ó 1 √ó 1.5 millimeters each mini-batch contains 2 volumes online augmentationÔºö randomly deformation vary the intensity distributionÔºöÈöèÊú∫ÈÄâÂèñÊ†∑Êú¨ÁöÑÁÅ∞Â∫¶ÂàÜÂ∏É‰Ωú‰∏∫ÂΩìÂâçËÆ≠ÁªÉÊ†∑Êú¨ÁöÑÁÅ∞Â∫¶ÂàÜÂ∏É used a momentum of 0.99 and a initial learning rate of 0.0001 which decreases by one order of magnitude every 25K iterations metricsÔºö Dice coefficient Hausdorff distance of the predicted delineation to the ground truth annotation the score obtained on the challenge dice loss &amp; focal loss CE &amp; BCE CEÔºöcategorical_crossentropyÔºåÈíàÂØπÊâÄÊúâÁ±ªÂà´ËÆ°ÁÆóÔºåÁ±ªÂà´Èó¥‰∫íÊñ• CE(x) = -\sum_{i=1}^{n\_class}y_i log f_i(x) $x$ÊòØËæìÂÖ•Ê†∑Êú¨Ôºå$y_i$ÊòØÁ¨¨$i$‰∏™Á±ªÂà´ÂØπÂ∫îÁöÑÁúüÂÆûÊ†áÁ≠æÔºå$f_i(x)$ÊòØÂØπÂ∫îÁöÑÊ®°ÂûãËæìÂá∫ÂÄº„ÄÇ ÂØπÂàÜÁ±ªÈóÆÈ¢òÔºå$y_i$ÊòØone-hotÔºå$f_i(x)$ÊòØ‰∏™‰∏ÄÁª¥ÂêëÈáè„ÄÇÊúÄÁªàÂæóÂà∞‰∏Ä‰∏™Êï∞ÂÄº„ÄÇ BCEÔºöbinary_crossentropyÔºåÈíàÂØπÊØè‰∏™Á±ªÂà´ËÆ°ÁÆó BCE(x)_i = - [y_i log f_i(x) + (1-y_i)log(1-f_i(x))] $i$ÊòØÁ±ªÂà´ÁºñÂè∑ÔºåÊúÄÁªàÂæóÂà∞‰∏Ä‰∏™Áª¥Â∫¶‰∏∫$n_class$ÁöÑÂêëÈáè„ÄÇ ÂÜçÊ±ÇÁ±ªÂùáÂÄºÂæóÂà∞‰∏Ä‰∏™Êï∞ÂÄº‰Ωú‰∏∫Âçï‰∏™Ê†∑Êú¨ÁöÑloss„ÄÇ BCE(x) = \frac{\sum_{i=1}^{n\_class}BCE_i(x)}{n\_class} batch lossÔºöÂØπbatch‰∏≠ÊâÄÊúâÊ†∑Êú¨ÁöÑlossÊ±ÇÂùáÂÄº„ÄÇ ‰ªéÂÖ¨Âºè‰∏äÁúãÔºåCEÁöÑËæìÂá∫ÈÄöÂ∏∏ÊòØÁªèËøá‰∫ÜsoftmaxÔºåsoftmaxÁöÑÊüê‰∏Ä‰∏™ËæìÂá∫Â¢ûÂ§ßÔºåÂøÖÁÑ∂ÂØºËá¥ÂÖ∂ÂÆÉÁ±ªÂà´ÁöÑËæìÂá∫ÂáèÂ∞èÔºåÂõ†Ê≠§Âú®ËÆ°ÁÆólossÁöÑÊó∂ÂÄôÂÖ≥Ê≥®Ê≠£Á°ÆÁ±ªÂà´ÁöÑÈ¢ÑÊµãÂÄºÊòØÂê¶Ë¢´ÊãâÈ´òÂç≥ÂèØ„ÄÇ‰ΩøÁî®BCEÁöÑÂú∫ÊôØÈÄöÂ∏∏ÊòØ‰ΩøÁî®sigmoidÔºåÁ±ªÂà´Èó¥‰∏ç‰ºö‰∫íÁõ∏ÂéãÂà∂ÔºåÂõ†Ê≠§Êó¢Ë¶ÅËÄÉËôëÊâÄÂ±ûÁ±ªÂà´ÁöÑÈ¢ÑÊµãÊ¶ÇÁéáÂ§üÈ´òÔºå‰πüË¶ÅËÄÉËôë‰∏çÊâÄÂ±ûÁ±ªÂà´ÁöÑÈ¢ÑÊµãÊ¶ÇÁéáË∂≥Â§ü‰ΩéÔºàËøô‰∏ÄÈ°πÂú®softmax‰∏≠Ë¢´ÂÆûÁé∞‰∫ÜÊïÖCE‰∏çÈúÄË¶ÅËøô‰∏ÄÈ°πÔºâ„ÄÇ Âú∫ÊôØÔºö ‰∫åÂàÜÁ±ªÔºöÂè™Êúâ‰∏Ä‰∏™ËæìÂá∫ËäÇÁÇπÔºå$f(x) \in (0,1)$ÔºåÂ∫îËØ•‰ΩøÁî®sigmoid+BCE‰Ωú‰∏∫ÊúÄÂêéÁöÑËæìÂá∫Â±ÇÈÖçÁΩÆ„ÄÇ ÂçïÊ†áÁ≠æÂ§öÂàÜÁ±ªÔºöÂ∫îËØ•‰ΩøÁî®softmax+CEÁöÑÊñπÊ°àÔºåBCE‰πüÂêåÊ†∑ÈÄÇÁî®„ÄÇ Â§öÊ†áÁ≠æÂ§öÂàÜÁ±ªÔºömulti-labelÊØè‰∏™Ê†áÁ≠æÁöÑËæìÂá∫ÊòØÁõ∏‰∫íÁã¨Á´ãÁöÑÔºåÂõ†Ê≠§Â∏∏Áî®ÈÖçÁΩÆÊòØsigmoid+BCE„ÄÇ ÂØπÂàÜÂâ≤Âú∫ÊôØÊù•ËØ¥ÔºåËæìÂá∫ÁöÑÊØè‰∏Ä‰∏™channelÂØπÂ∫î‰∏Ä‰∏™Á±ªÂà´ÁöÑÈ¢ÑÊµãmapÔºåÂèØ‰ª•ÁúãÊàêÊòØÂ§ö‰∏™channelÈó¥ÁöÑÂçïÊ†áÁ≠æÂ§öÂàÜÁ±ªÔºàsoftmax+CEÔºâÔºå‰πüÂèØ‰ª•ÁúãÊàêÊòØÊØè‰∏™Áã¨Á´ãÈÄöÈÅìÁ±ªÂà´mapÁöÑ‰∫åÂàÜÁ±ªÔºàsigmoid+BCEÔºâ„ÄÇunetËÆ∫ÊñáÁî®‰∫ÜweightedÁöÑsoftmax+CE„ÄÇvnetËÆ∫ÊñáÁî®‰∫Üdice_loss„ÄÇ re-weighting(WCE) Âü∫‰∫éCE&amp;BCEÔºåÁªô‰∫ÜÊ†∑Êú¨‰∏çÂêåÁöÑÊùÉÈáç„ÄÇ unetËÆ∫Êñá‰∏≠ÊèêÂà∞‰∫ÜÂü∫‰∫épixel frequency‰∏∫‰∏çÂêåÁöÑÁ±ªÂà´ÂàõÂª∫‰∫Üweight map„ÄÇ ‰∏ÄÁßçÂÆûÁé∞ÔºöÂü∫‰∫éÊØè‰∏™Á±ªÂà´ÁöÑweight mapÔºåÂú®ÂÆûÁé∞CEÁöÑÊó∂ÂÄôÊîπÊàêÂä†ÊùÉÂπ≥ÂùáÂç≥ÂèØ„ÄÇ Âè¶‰∏ÄÁßçÂÆûÁé∞ÔºöÂü∫‰∫éÊØè‰∏™Ê†∑Êú¨ÁöÑweight mapÔºå‰Ωú‰∏∫ÁΩëÁªúÁöÑÈôÑÂä†ËæìÂÖ•ÔºåÂú®ÂÆûÁé∞CEÁöÑÊó∂ÂÄô‰πòÂú®loss map‰∏ä„ÄÇ focal loss ÊèêÂá∫ÊòØÂú®ÁõÆÊ†áÊ£ÄÊµãÈ¢ÜÂüüÔºåÁî®‰∫éËß£ÂÜ≥Ê≠£Ë¥üÊ†∑Êú¨ÊØî‰æã‰∏•ÈáçÂ§±Ë∞ÉÁöÑÈóÆÈ¢ò„ÄÇ ‰πüÊòØ‰∏ÄÁßçÂä†ÊùÉÔºå‰ΩÜÊòØÁõ∏ÊØîËæÉ‰∫ére-weightingÔºåÂõ∞ÈöæÊ†∑Êú¨ÁöÑÊùÉÈáçÁî±ÁΩëÁªúËá™Ë°åÊé®Êñ≠Âá∫ÔºåÈÄöËøáÊ∑ªÂä†$(\alpha)$Âíå$(-)^\lambda$Ëøô‰∏ÄÂä†ÊùÉÈ°πÔºö focal\_loss(x)_i = -[\alpha y_i (1-p_i)^\lambda log (p_i)+(1-\alpha)(1-y_i)p_i^\lambda log(1-p_i)] ÂØπ‰∫éÁ±ªÂà´Èó¥‰∏çÂùáË°°ÁöÑÊÉÖÂÜµÔºàÈÄöÂ∏∏Ë¥üÊ†∑Êú¨ËøúËøúÂ§ö‰∫éÊ≠£Ê†∑Êú¨ÔºâÔºå$(\alpha)$È°πÁî®‰∫éÂπ≥Ë°°Ê≠£Ë¥üÊ†∑Êú¨ÊùÉÈáç„ÄÇ ÂØπ‰∫éÁ±ªÂÜÖÂõ∞ÈöæÊ†∑Êú¨ÁöÑÊåñÊéòÔºå$(-)^\lambda$È°πÁî®‰∫éË∞ÉÊï¥ÁÆÄÂçïÊ†∑Êú¨ÂíåÂõ∞ÈöæÊ†∑Êú¨ÁöÑÊùÉÈáçÔºåÈ¢ÑÊµãÊ¶ÇÁéáÊõ¥Êé•ËøëÁúüÂÆûlabelÁöÑÊ†∑Êú¨ÔºàÁÆÄÂçïÊ†∑Êú¨ÔºâÁöÑÊùÉÈáç‰ºöË°∞ÂáèÊõ¥Âø´ÔºåÈ¢ÑÊµãÊ¶ÇÁéáÊØîËæÉ‰∏çÂáÜÁ°ÆÁöÑÊ†∑Êú¨ÔºàËã¶ÈöæÊ†∑Êú¨ÔºâÁöÑÊùÉÈáçÂàôÊõ¥È´ò‰∫õ„ÄÇ Áî±‰∫éÂàÜÂâ≤ÁΩëÁªúÁöÑËæìÂá∫ÁöÑÂçïÈÄöÈÅìÔºèÂ§öÈÄöÈÅìÁöÑÂõæÁâáÔºåÁõ¥Êé•‰ΩøÁî®focal loss‰ºöÂØºËá¥lossÂÄºÂæàÂ§ß„ÄÇ ‚Äã 1. ÈÄöÂ∏∏‰∏éÂÖ∂‰ªñlossÂä†ÊùÉÁªÑÂêà‰ΩøÁî® ‚Äã 2. sumÂèØ‰ª•ÊîπÊàêmean ‚Äã 3.‰∏çÂª∫ËÆÆÂú®ËÆ≠ÁªÉÂàùÊúüÂ∞±Âä†ÂÖ•ÔºåÂèØÂú®ËÆ≠ÁªÉÂêéÊúüÁî®‰∫é‰ºòÂåñÊ®°Âûã ‚Äã 4. ÂÖ¨Âºè‰∏≠Âê´logËÆ°ÁÆóÔºåÂèØËÉΩÂØºËá¥nanÔºåË¶ÅÂØπlog‰∏≠ÁöÑÂÖÉÁ¥†clip 12345678910111213def focal_loss(y_true, y_pred): gamma = 2. alpha = 0.25 # score = alpha * y_true * K.pow(1 - y_pred, gamma) * K.log(y_pred) + # this works when y_true==1 # (1 - alpha) * (1 - y_true) * K.pow(y_pred, gamma) * K.log(1 - y_pred) # this works when y_true==0 pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred)) pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred)) # avoid nan pt_1 = K.clip(pt_1, 1e-3, .999) pt_0 = K.clip(pt_0, 1e-3, .999) score = -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - \ K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0)) return score dice loss diceÂÆö‰πâ‰∏§‰∏™maskÁöÑÁõ∏‰ººÁ®ãÂ∫¶Ôºö dice = \frac{2 * A \bigcap B}{|A|+|B|} = \frac{2 * TP}{2*TP + FN + FP} ÂàÜÂ≠êÊòØTP‚Äî‚ÄîÂè™ÂÖ≥Ê≥®ÂâçÊôØ ÂàÜÊØçÂèØ‰ª•ÊòØ$|A|$ÔºàÈÄê‰∏™ÂÖÉÁ¥†Áõ∏Âä†ÔºâÔºå‰πüÂèØ‰ª•ÊòØÂπ≥ÊñπÂΩ¢Âºè$|A|^2$ Ê¢ØÂ∫¶Ôºö‚Äú‰ΩøÁî®dice lossÊúâÊó∂‰ºö‰∏çÂèØ‰ø°ÔºåÂéüÂõ†ÊòØÂØπ‰∫ésoftmaxÊàñlog lossÂÖ∂Ê¢ØÂ∫¶ÁÆÄË®Ä‰πãÊòØp-t Ôºåt‰∏∫ÁõÆÊ†áÂÄºÔºåp‰∏∫È¢ÑÊµãÂÄº„ÄÇËÄådice loss ‰∏∫ 2t2 / (p+t)2 Â¶ÇÊûúpÔºåtËøáÂ∞è‰ºöÂØºËá¥Ê¢ØÂ∫¶ÂèòÂåñÂâßÁÉàÔºåÂØºËá¥ËÆ≠ÁªÉÂõ∞Èöæ„ÄÇ‚Äù „ÄêËØ¶ÁªÜËß£Èáä‰∏ã„Äë‰∫§ÂèâÁÜµlossÔºö$L=-(1-|t-p|)log(1-|t-p|)$ÔºåÊ±ÇÂØºÂæóÂà∞$\frac{\partial L}{\partial p}=-log(1-|t-p|)$ÔºåÂÖ∂ÂÆûÂ∞±ÂèØ‰ª•ÁÆÄÂåñÁúã‰Ωú$t-p$ÔºåÂæàÊòæÁÑ∂Ëøô‰∏™Ê¢ØÂ∫¶ÊòØÊúâÁïåÁöÑÔºåÂõ†Ê≠§‰ΩøÁî®‰∫§ÂèâÁÜµlossÁöÑ‰ºòÂåñËøáÁ®ãÊØîËæÉÁ®≥ÂÆö„ÄÇËÄådice lossÁöÑ‰∏§ÁßçÂΩ¢ÂºèÔºà‰∏çÂπ≥Êñπ&amp;Âπ≥ÊñπÔºâÔºö$L=\frac{2pt}{p+t}\ or\ L=\frac{2pt}{p^2+t^2}$ÔºåÊ±ÇÂØº‰ª•ÂêéÂàÜÂà´ÊòØ$\frac{\partial L}{\partial p} = \frac{t^2+2pt}{(p+t)^2} \ or\ \frac{3tp^2+t^3}{(p^2+t^2)^2}$ËÆ°ÁÆóÁªìÊûúÊØîËæÉÂ§çÊùÇÔºåptÈÉΩÂæàÂ∞èÁöÑÊÉÖÂÜµ‰∏ãÔºåÊ¢ØÂ∫¶ÂÄºÂèØËÉΩÂæàÂ§ßÔºåÂèØËÉΩÂØºËá¥ËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÔºålossÊõ≤Á∫øÊ∑∑‰π±„ÄÇ vnetËÆ∫Êñá‰∏≠ÁöÑÂÆö‰πâÂú®ÂàÜÊØç‰∏äÁ®çÊúâ‰∏çÂêåÔºàsee belowÔºâ„ÄÇsmoothingÁöÑÂ•ΩÂ§ÑÔºö ÈÅøÂÖçÂàÜÂ≠êÈô§0 ÂáèÂ∞ëËøáÊãüÂêà 12345678def dice_coef(y_true, y_pred): smooth = 1. intersection = K.sum(y_true * y_pred, axis=[1,2,3]) union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3]) return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0) def dice_coef_loss(y_true, y_pred): 1 - dice_coef(y_true, y_pred, smooth=1) iou loss dice lossË°çÁîüÔºåintersection over unionÔºö iou = \frac{A \bigcap B}{A \bigcup B} ÂàÜÊØç‰∏äÊØîdiceÂ∞ë‰∫Ü‰∏Ä‰∏™intersection„ÄÇ ‚ÄúIOU lossÁöÑÁº∫ÁÇπÂêåDICE lossÔºåËÆ≠ÁªÉÊõ≤Á∫øÂèØËÉΩÂπ∂‰∏çÂèØ‰ø°ÔºåËÆ≠ÁªÉÁöÑËøáÁ®ã‰πüÂèØËÉΩÂπ∂‰∏çÁ®≥ÂÆöÔºåÊúâÊó∂‰∏çÂ¶Ç‰ΩøÁî®softmax lossÁ≠âÁöÑÊõ≤Á∫øÊúâÁõ¥ËßÇÊÄßÔºåÈÄöÂ∏∏ËÄåË®Äsoftmax lossÂæóÂà∞ÁöÑloss‰∏ãÈôçÊõ≤Á∫øËæÉ‰∏∫Âπ≥Êªë„ÄÇ‚Äù boundary loss dice lossÂíåiou lossÊòØÂü∫‰∫éÂå∫ÂüüÈù¢ÁßØÂåπÈÖçÂ∫¶ÂéªÂ≠¶‰π†ÔºåÊàë‰ª¨‰πüÂèØ‰ª•‰ΩøÁî®ËæπÁïåÂåπÈÖçÂ∫¶ÂéªÁõëÁù£ÁΩëÁªúÁöÑÂ≠¶‰π†„ÄÇ Âè™ÂØπËæπÁïå‰∏äÁöÑÂÉèÁ¥†ËøõË°åËØÑ‰º∞ÔºåÂíåGTÁöÑËæπÁïåÂêªÂêàÂàô‰∏∫0Ôºå‰∏çÂêªÂêàÁöÑÁÇπÔºåÊ†πÊçÆÂÖ∂Ë∑ùÁ¶ªËæπÁïåÁöÑË∑ùÁ¶ªËØÑ‰º∞ÂÆÉÁöÑLoss„ÄÇ Hausdorff distance Áî®‰∫éÂ∫¶Èáè‰∏§‰∏™ÁÇπÈõÜ‰πãÈó¥ÁöÑÁõ∏‰ººÁ®ãÂ∫¶Ôºådenote ÁÇπÈõÜ$A\{a_1, a_2, ‚Ä¶, a_p\}$ÔºåÁÇπÈõÜ$B\{b_1, b_2, ‚Ä¶, b_p\}$Ôºö HD(A, B) = max\{hd(A,B), hd(B,A)\}\\ hd(A,B) = max_{a \in A} min_{b in B} ||a-b||\\ hd(B,A) = max_{b \in B} min_{a in A} ||b-a|| ÂÖ∂‰∏≠HD(A,B)ÊòØHausdorff distanceÁöÑÂü∫Êú¨ÂΩ¢ÂºèÔºåÁß∞‰∏∫ÂèåÂêëË∑ùÁ¶ª hd(A,B)ÊèèËø∞ÁöÑÊòØÂçïÂêëË∑ùÁ¶ªÔºåÈ¶ñÂÖàÊâæÂà∞ÁÇπÈõÜA‰∏≠ÊØè‰∏™ÁÇπÂú®ÁÇπÈõÜB‰∏≠Ë∑ùÁ¶ªÊúÄËøëÁöÑÁÇπ‰Ωú‰∏∫ÂåπÈÖçÁÇπÔºåÁÑ∂ÂêéËÆ°ÁÆóËøô‰∫õa-b-pairÁöÑË∑ùÁ¶ªÁöÑÊúÄÂ§ßÂÄº„ÄÇ HD(A,B)ÂèñÂçïÂêëË∑ùÁ¶ª‰∏≠ÁöÑÊúÄÂ§ßÂÄºÔºåÊèèËø∞‰∫Ü‰∏§‰∏™ÁÇπÈõÜÂêàÁöÑÊúÄÂ§ß‰∏çÂåπÈÖçÁ®ãÂ∫¶„ÄÇ mix loss BCE + dice lossÔºöÂú®Êï∞ÊçÆËæÉ‰∏∫Âπ≥Ë°°ÁöÑÊÉÖÂÜµ‰∏ãÊúâÊîπÂñÑ‰ΩúÁî®Ôºå‰ΩÜÊòØÂú®Êï∞ÊçÆÊûÅÂ∫¶‰∏çÂùáË°°ÁöÑÊÉÖÂÜµ‰∏ãÔºå‰∫§ÂèâÁÜµÊçüÂ§±‰ºöÂú®Âá†‰∏™ËÆ≠ÁªÉ‰πãÂêéËøúÂ∞è‰∫éDice ÊçüÂ§±ÔºåÊïàÊûú‰ºöÊçüÂ§±„ÄÇ focal loss + dice lossÔºöÊï∞ÈáèÁ∫ßÈóÆÈ¢ò MSE ÂÖ≥ÈîÆÁÇπÊ£ÄÊµãÊúâÊó∂ÂÄô‰πü‰ºöÈááÁî®ÂàÜÂâ≤Ê°ÜÊû∂ÔºåËøôÊó∂ÂÄôground truthÊòØÈ´òÊñØmapÔºådiceÊòØÈíàÂØπ‰∫åÂÄºÂåñmaskÁöÑÔºåËøôÊó∂ÂÄôËøòÂèØ‰ª•Áî®MSE„ÄÇ ohnm online hard negative mining Âõ∞ÈöæÊ†∑Êú¨ÊåñÊéò Tversky loss ‰∏ÄÁßçÂä†ÊùÉÁöÑdice lossÔºådice loss‰ºöÂπ≥Á≠âÁöÑÊùÉË°°FPÔºàÁ≤æÂ∫¶ÔºåÂÅáÈò≥ÔºâÂíåFNÔºàÂè¨ÂõûÔºåÂÅáÈò¥ÔºâÔºå‰ΩÜÊòØÂåªÂ≠¶ÂõæÂÉè‰∏≠ÁóÖÁÅ∂Êï∞ÁõÆËøúÂ∞ë‰∫éËÉåÊôØÊï∞ÈáèÔºåÂæàÂèØËÉΩÂØºËá¥ËÆ≠ÁªÉÁªìÊûúÂÅèÂêëÈ´òÁ≤æÂ∫¶‰ΩÜÊòØ‰ΩéÂè¨ÂõûÁéáÔºåTversky lossÊéßÂà∂lossÊõ¥ÂÅèÂêëFNÔºö loss = 1-\frac{|PG|}{|PG|+\alpha|P\backslash G|+\beta|G\backslash P|}1234567891011def tversky_loss(y_true, y_pred): y_true_pos = K.flatten(y_true) y_pred_pos = K.flatten(y_pred) # TP true_pos = K.sum(y_true_pos * y_pred_pos) # FN false_neg = K.sum(y_true_pos * (1-y_pred_pos)) # FP false_pos = K.sum((1-y_true_pos) * y_pred_pos) alpha = 0.7 return 1 - (true_pos + K.epsilon())/(true_pos + alpha * false_neg + (1-alpha) * false_pos + K.epsilon()) Lovasz hinge &amp; Lovasz-Softmax loss IOU lossË°çÁîüÔºåjaccard lossÂè™ÈÄÇÁî®‰∫éÁ¶ªÊï£ÊÉÖÂÜµÔºåËÄåÁΩëÁªúÈ¢ÑÊµãÊòØËøûÁª≠ÂÄºÔºåÂ¶ÇÊûú‰∏ç‰ΩøÁî®Êüê‰∏™Ë∂ÖÂèÇÂ∞ÜÁ•ûÁªèÂÖÉËæìÂá∫‰∫åÂÄºÂåñÔºåÂ∞±‰∏çÂèØÂØº„ÄÇblabla ‰∏çÊòØÂæàÊáÇÁõ¥Êé•Áî®ÂêßÔºöhttps://github.com/bermanmaxim/LovaszSoftmax ‰∏Ä‰∫õË°•ÂÖÖ ÊîπËøõÔºö dropout„ÄÅbatch normalizationÔºö‰ªéËÆ∫Êñá‰∏äÁúãÔºåunetÂè™Âú®ÊúÄÊ∑±Â±ÇÂç∑ÁßØÂ±ÇÂêéÈù¢Ê∑ªÂä†‰∫Üdropout layerÔºåBNÊú™Ë°®ÔºåËÄåcommon senseÁî®ÊØè‰∏Ä‰∏™convÂ±ÇÂêéÈù¢Êé•BNÂ±ÇËÉΩÂ§üÊõøÊç¢ÊéâdropoutÂπ∂ËÉΩËé∑ÂæóÊÄßËÉΩÊèêÂçáÁöÑ„ÄÇ UpSampling2D„ÄÅConv2DTransposeÔºöunet‰ΩøÁî®‰∫Ü‰∏äÈááÊ†∑Ôºåvnet‰ΩøÁî®‰∫ÜdeconvÔºå‰ΩÜÊòØ‚ÄúDeConv will produce image with checkerboard effect, which can be revised by upsample and conv‚Äù(Reference)„ÄÇ valid padding„ÄÅsame paddingÔºöunetËÆ∫Êñá‰ΩøÁî®ÂõæÂÉèpatch‰Ωú‰∏∫ËæìÂÖ•ÔºåÁâπÂæÅÊèêÂèñÊó∂‰ΩøÁî®valid paddingÔºåÊçüÂ§±ËæπÁºò‰ø°ÊÅØ„ÄÇ network blocksÔºöunetÁî®ÁöÑconv blockÊòØ‰∏§‰∏™‰∏ÄÁªÑÁöÑ3*3convÔºåvnetÁ®çÂæÆ‰∏çÂêå‰∏ÄÁÇπÔºåÂèØ‰ª•Â∞ùËØïÁöÑblockÊúâResNetÔºèResNext„ÄÅDenseNet„ÄÅDeepLabÁ≠â„ÄÇ pretrained encoderÔºöfeature extraction path‰ΩøÁî®‰∏Ä‰∫õÁé∞ÊúâÁöÑbackboneÔºåÂèØ‰ª•Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÊùÉÈáç(Reference)ÔºåÂä†ÈÄüËÆ≠ÁªÉÔºåÈò≤Ê≠¢ËøáÊãüÂêà„ÄÇ Âä†ÂÖ•SEÊ®°Âùó(Reference)ÔºöÂØπÊØè‰∏™ÈÄöÈÅìÁöÑÁâπÂæÅÂä†ÊùÉ attention mechanismsÔºö ÂºïÁî®nn-Unet‰∏ªË¶ÅÁªìÊûÑÊîπËøõÂêàÈõÜÔºö‚ÄúJust to provide some prominent examples: variations of encoder-decoder style architectures with skip connections, first introduced by the U-Net [12], include the introduction of residual connections [9], dense connections [6], at- tention mechanisms [10], additional loss layers [5], feature recalibration [13], and others [11]. Ë°çÁîüÔºö TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation nnU-Net: Breaking the Spell on Successful Medical Image Segmentation TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation Âä®Êú∫Ôºö neural network initialized with pre-trained weights usually shows better performance than those trained from scratch on a small dataset. ‰øùÁïôencoder-decoderÁöÑÁªìÊûÑÔºåÂêåÊó∂ÂÖÖÂàÜÂà©Áî®ËøÅÁßªÂ≠¶‰π†ÁöÑ‰ºòÂäø ËÆ∫ÁÇπÔºö load pretrained weights Áî®huge datasetÂÅöÈ¢ÑËÆ≠ÁªÉ ÊñπÊ≥ïÔºö Áî®vgg11ÊõøÊç¢ÂéüÂßãÁöÑencoderÔºåÂπ∂load pre-trained weights on ImageNetÔºö ÊúÄÊ∑±Â±ÇËæìÂÖ•(maxpooling5)Ôºöuse a single conv of 512 channels that serves as a bottleneck central part of the network upsamplingÊç¢Êàê‰∫ÜconvTranspose loss functionÔºöIOU + BCEÔºö L = BCE - log(IOU) inferenceÔºöchoose a threshold 0.3, all pixel values below which are set to be zero ÁªìËÆ∫Ôºö converge faster better IOU nnU-Net: Breaking the Spell on Successful Medical Image Segmentation Âä®Êú∫ many proposed methods fail to generalize: ÂØπ‰∫éÂàÜÂâ≤‰ªªÂä°Ôºå‰ªéunetÂá∫Êù•‰πãÂêéÁöÑÂá†Âπ¥ÈáåÔºåÂú®ÁΩëÁªúÁªìÊûÑ‰∏äÂ∑≤ÁªèÊ≤°ÊúâÂ§öÂ∞ëÁöÑÁ™ÅÁ†¥‰∫ÜÔºåÁªìÊûÑ‰øÆÊîπË∂äÂ§öÔºåÂèçËÄåË∂äÂÆπÊòìËøáÊãüÂêà relies on just a simple U-Net architecture embedded in a robust training scheme automate necessary adaptations such as preprocessing, the exact patch size, batch size, and inference settings based on the properties of a given dataset: Êõ¥Â§öÁöÑÊèêÂçáÂÖ∂ÂÆûÂú®‰∫éÁêÜËß£Êï∞ÊçÆÔºåÈíàÂØπÊï∞ÊçÆÈááÁî®ÈÄÇÂΩìÁöÑÈ¢ÑÂ§ÑÁêÜÂíåËÆ≠ÁªÉÊñπÊ≥ïÂíåÊäÄÂ∑ß ËÆ∫ÁÇπ the diversity and individual peculiarities of imaging datasets make it difficult to generalize prominent modifications focus on architectural modifications, merely brushing over all the other hyperparameters we propose: ‰ΩøÁî®Âü∫Á°ÄÁâàunetÔºönnUNetÔºàno-new-NetÔºâ a formalism for automatic adaptation to new datasets automatically designs and executes a network training pipeline without any manual fine-tuning Ë¶ÅÁ¥† a segmentation task: $f_{\theta}(X) = \hat Y$, in this paper we seek for a $g(X,Y)=\theta$. First we distinguish two type of hyperparameters: static paramsÔºöin this case the network architecture and a robust training scheme dynamic paramsÔºöthose that need to be changed in dependence of $X$ and $Y$ Second we define g‚Äî‚Äîa set of heuristics rules covering the entire process of the task: È¢ÑÂ§ÑÁêÜÔºöresamplingÂíånormalization ËÆ≠ÁªÉÔºölossÔºåoptimizerËÆæÁΩÆ„ÄÅÊï∞ÊçÆÂ¢ûÂπø Êé®ÁêÜÔºöpatch-basedÁ≠ñÁï•„ÄÅtest-time-augmentationsÈõÜÊàêÂíåÊ®°ÂûãÈõÜÊàêÁ≠â ÂêéÂ§ÑÁêÜÔºöÂ¢ûÂº∫ÂçïËøûÈÄöÂüüÁ≠â ÊñπÊ≥ï Preprocessing Image NormalizationÔºö CTÔºö$normed_intensity = (intensity - fg_mean) / fg_standard_deviation$, $fg$ for $[0.05,0.95]$ foreground intensity not CTÔºö$normed_intensity = (intensity - mean) / standard_deviation $ Voxel SpacingÔºö for each axis chooses the median as the target spacing image resampled with third order spline interpolation z-axis using nearest neighbor interpolation if ‚Äòanisotropic spacing‚Äô occurs mask resampled with third order spline interpolation Training Procedure Network ArchitectureÔºö 3 independent modelÔºöa 2D U-Net, a 3D U-Net and a cascade of two 3D U-Net padded convolutionsÔºöto achieve identical output and input shapes instance normalizationÔºö‚ÄúBNÈÄÇÁî®‰∫éÂà§Âà´Ê®°ÂûãÔºåÊØîÂ¶ÇÂõæÁâáÂàÜÁ±ªÊ®°Âûã„ÄÇÂõ†‰∏∫BNÊ≥®ÈáçÂØπÊØè‰∏™batchËøõË°åÂΩí‰∏ÄÂåñÔºå‰ªéËÄå‰øùËØÅÊï∞ÊçÆÂàÜÂ∏ÉÁöÑ‰∏ÄËá¥ÊÄßÔºåËÄåÂà§Âà´Ê®°ÂûãÁöÑÁªìÊûúÊ≠£ÊòØÂèñÂÜ≥‰∫éÊï∞ÊçÆÊï¥‰ΩìÂàÜÂ∏É„ÄÇ‰ΩÜÊòØBNÂØπbatchsizeÁöÑÂ§ßÂ∞èÊØîËæÉÊïèÊÑüÔºåÁî±‰∫éÊØèÊ¨°ËÆ°ÁÆóÂùáÂÄºÂíåÊñπÂ∑ÆÊòØÂú®‰∏Ä‰∏™batch‰∏äÔºåÊâÄ‰ª•Â¶ÇÊûúbatchsizeÂ§™Â∞èÔºåÂàôËÆ°ÁÆóÁöÑÂùáÂÄº„ÄÅÊñπÂ∑Æ‰∏çË∂≥‰ª•‰ª£Ë°®Êï¥‰∏™Êï∞ÊçÆÂàÜÂ∏ÉÔºõINÈÄÇÁî®‰∫éÁîüÊàêÊ®°ÂûãÔºåÊØîÂ¶ÇÂõæÁâáÈ£éÊ†ºËøÅÁßª„ÄÇÂõ†‰∏∫ÂõæÁâáÁîüÊàêÁöÑÁªìÊûú‰∏ªË¶Å‰æùËµñ‰∫éÊüê‰∏™ÂõæÂÉèÂÆû‰æãÔºåÊâÄ‰ª•ÂØπÊï¥‰∏™batchÂΩí‰∏ÄÂåñ‰∏çÈÄÇÂêàÂõæÂÉèÈ£éÊ†ºÂåñÔºåÂú®È£éÊ†ºËøÅÁßª‰∏≠‰ΩøÁî®Instance Normalization‰∏ç‰ªÖÂèØ‰ª•Âä†ÈÄüÊ®°ÂûãÊî∂ÊïõÔºåÂπ∂‰∏îÂèØ‰ª•‰øùÊåÅÊØè‰∏™ÂõæÂÉèÂÆû‰æã‰πãÈó¥ÁöÑÁã¨Á´ã„ÄÇ‚Äù Leaky ReLUs Network HyperparametersÔºö sets the batch size, patch size and number of pooling operations for each axis based on the memory consumption large patch sizes are favored over large batch sizes pooling along each axis is done until the voxel size=4 start num of filters=30, double after each pooling If the selected patch size covers less than 25% of the voxels, train the 3D U-Net cascade on a downsampled version of the training data to keep sufficient context Network Training: five-fold cross-validation One epoch is defined as processing 250 batches loss = dice loss + cross-entropy loss Adam(lr=3e-4, decay=3e-5) lrReduce: EMA(train_loss), 30 epoch, factor=0.2 earlyStop: earning rate drops below 10 6 or 1000 epochs are exceeded data augmentation: elastic deformations, random scaling and random rotations as well as gamma augmentation($g(x,y)=f(x,y)^{gamma}$) keep transformations in 2D-plane if ‚Äòanisotropic spacing‚Äô occurs Inference sliding window with half the patch size: this increases the weight of the predictions close to the center relative to the borders ensemble: U-Net configurations (2D, 3D and cascade) furthermore uses the five models (five-fold cross-validation) Ablation studies 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation Âä®Êú∫ learns from sparsely/full annotated volumetric images (user annotates some slices) provides a dense 3D segmentation Ë¶ÅÁ¥† 3D operations avoid bottlenecks and use batch normalization for faster convergence on-the-fly elastic deformation train from scratch ËÆ∫ÁÇπ neighboring slices show almost the same information many biomedical applications generalizes reasonably well because medical images comprises repetitive structures thus we suggest dense-volume-segmentation-network that only requires some annotated 2D slices for training scenarios manual annotated ‰∏ÄÈÉ®ÂàÜsliceÔºåÁÑ∂ÂêéËÆ≠ÁªÉÁΩëÁªúÂÆûÁé∞dense seg Áî®‰∏ÄÈÉ®ÂàÜ sparsely annotatedÁöÑdataset‰Ωú‰∏∫training setÔºåÁÑ∂ÂêéËÆ≠ÁªÉÁöÑÁΩëÁªúÂÆûÁé∞Âú®Êñ∞ÁöÑÊï∞ÊçÆÈõÜ‰∏ädense seg ÊñπÊ≥ï Network Architecture compressionÔºö2*3x3x3 convs(+BN)+relu+2x2x2 maxpooling decompressionÔºö2x2x2 upconv+2*3x3x3 convs+relu headÔºö1x1x1 conv concat shortcut connections „ÄêQUESTION„Äëavoid bottlenecks by doubling the number of channels already before max pooling ‰∏™‰∫∫ÁêÜËß£Ëøô‰∏™double channelÊòØÂú®Ë∑üÂéüÂßãÁöÑunetÁªìÊûÑÂØπÊØîÔºåÂéüÂßãunetÊØè‰∏™stageÁöÑ‰∏§‰∏™convÁöÑfilter numÊòØ‰∏ÄÊ†∑ÁöÑÔºåÁÑ∂ÂêéËøõË°åmax pooling‰ºöÊçüÂ§±ÈÉ®ÂàÜ‰ø°ÊÅØÔºå‰ΩÜÊòØÂàÜÂâ≤‰ªªÂä°Êú¨Ë∫´ÊòØ‰∏™dense predictionÔºåÊâÄ‰ª•Â¢ûÂ§ßchannelÊù•ÂáèÂ∞ë‰ø°ÊÅØÊçüÂ§± ‰ΩÜÊòØ‰∏çÁêÜËß£‰ªÄ‰πàÂè´‚Äúavoid bottlenecks‚Äù ÂéüÊñáËØ¥ÊòØÂèÇËÄÉ‰∫Ü„ÄäRethinking the inception architecture for computer vision„ÄãÂ§ßÂêçÈºéÈºéÁöÑinception V3 ÂèØËÉΩÂØπÂ∫îÁöÑÊòØ‚Äú1. Avoid representational bottlenecks, especially early in the network.‚ÄùÔºå‰ªéËæìÂÖ•Âà∞ËæìÂá∫ÔºåË¶ÅÈÄêÊ∏êÂáèÂ∞ëfeature mapÁöÑÂ∞∫ÂØ∏ÔºåÂêåÊó∂Ë¶ÅÈÄêÊ∏êÂ¢ûÂä†feature mapÁöÑÊï∞Èáè„ÄÇ inputÔºö132x132x116 voxel tile outputÔºö44x44x28 BNÔºöbefore each ReLU weighted softmax loss functionÔºösetting the weights of unlabeled pixels to zero makes it possible to learn from only the labelled ones and, hence, to generalize to the whole volumeÔºàÊòØ‰∏çÊòØrandom set the loss zeros of some samplesÊÄªËÉΩËÆ©ÁΩëÁªúÊõ¥Â•ΩÁöÑgeneralizeÔºüÔºâ Data manually annotated some orthogonal xy, xz, and yz slices annotation slices were sampled uniformly ran on down-sampled versions of the original resolution by factor of two labelsÔºö0: ‚Äúinside the tubule‚Äù; 1: ‚Äútubule‚Äù; 2: ‚Äúbackground‚Äù, and 3: ‚Äúunlabeled‚Äù. Training rotation, scaling and gray value augmentation a smooth dense deformationÔºörandom vector, normal distribution, B-spline interpolation weighted cross-entropy lossÔºöincrease weights ‚Äúinside the tubule‚Äù, reduce weights ‚Äúbackground‚Äù, set zero ‚Äúunlabeled‚Äù 2.5D-UNet: Automatic Segmentation of Vestibular Schwannoma from T2-Weighted MRI by Deep Spatial Attention with Hardness-Weighted Loss ‰∏ì‰∏öÊúØËØ≠ Vestibular Schwannoma(VS) tumorsÔºöÂâçÂ∫≠Á•ûÁªèÈûòÁò§ through-plane resolutionÔºöÂ±ÇÂéö isotropic resolutionÔºöÂêÑÂêëÂêåÊÄß anisotropic resolutionsÔºöÂêÑÂêëÂºÇÊÄß Âä®Êú∫ tumorÁöÑÁ≤æÁ°ÆËá™Âä®ÂàÜÂâ≤ challenge low contrastÔºöhardness-weighted Dice loss functio small target regionÔºöattention module low through-plane resolutionÔºö2.5D ËÆ∫ÁÇπ segment small structures from large image contexts coarse-to-fine attention map Dice loss our method end-to-end supervision on the learning of attention map voxel-level hardness- weighted Dice loss function CNN 2D CNNs ignore inter-slice correlation 3D CNNs most applied to images with isotropic resolution requiring upsampling to balance the physical receptive field (in terms of mm rather than voxels)Ôºömemory rise our method high in-plane resolution &amp; low through-plane resolution 2.5D CNN combining 2D and 3D convolutions use inter-slice features more efficient than 3D CNNs Êï∞ÊçÆ T2-weighted MR images of 245 patients with VS tumor high in-plane resolution around 0.4 mm√ó0.4 mmÔºå512x512 slice thickness and inter-slice spacing 1.5 mmÔºåslice number 19 to 118 cropped cube sizeÔºö100 mm√ó50 mm√ó50 mm ÊñπÊ≥ï architecture five levelsÔºöL1„ÄÅL2 use 2DÔºåL3„ÄÅL4„ÄÅL5 use 3D After the first two max-pooling layers that downsample the feature maps only in 2D, the feature maps in L3 and the followings have a near- isotropic 3D resolution. start channelsÔºö16 conv blockÔºöconv-BN-pReLU add a spatial attention module to each level of the decoder spatial attention module A spatial attention map can be seen as a single-channel image of attention coefficient inputÔºöfeature map with channel $N_l$ conv1+ReLUÔºö channel $N_l/2$ conv2+SigmoidÔºöchannel 1Ôºåoutputs the attention map multiplied the feature map with the attention map a residual connection explicit supervision multi-scale attention loss $L_{attention} = \frac{1}{L} \sum_{L} l(A_l, G_l^f)$ $A_l$ÊòØÊØè‰∏ÄÂ±ÇÁöÑattention mapÔºå$G_l^f$ÊòØÊØè‰∏ÄÂ±ÇÊòØÂâçÊôØground truth average-poolÂà∞ÂΩìÂâçresolutionÁöÑmask Voxel-Level Hardness-Weighted Dice Loss automatic hard voxel weightingÔºö$w_i = \lambda * abs(p_i - g_i) + (1-\lambda)$ $\lambda \in [0,1]$Ôºåcontrols the degree of hard voxel weighting hardness-weighted Dice loss (HDL) Ôºö l(P,G) = 1.0 - \frac{1}{C}\sum_{C} \frac{2\sum_i w_i p_i g_i + \epsilon}{\sum_i w_i (p_i + g_i) + \epsilon} total lossÔºö L = \frac{1}{L} \sum_{L} l(A_l, G_l^f) + l(P,G) Combining analysis of multi-parametric MR images into a convolutional neural network: Precise target delineation for vestibular schwannoma treatment planningÂè™ÊúâÊëòË¶ÅÂíå‰∏ÄÂπÖÂõæ multi-parametric MR imagesÔºöT1W„ÄÅT2W„ÄÅT1C two-pathway U-Net model kernel 3 √ó 3 √ó 1 and 1 √ó 1 √ó 3 respectively to extract the in-plane and through-plane features of the anisotropic MR images ÁªìËÆ∫ The proposed two-pathway U-Net model outperformed the single-pathway U-Net model when segmenting VS using anisotropic MR images. multi-inputsÔºàT1„ÄÅT2Ôºâoutperforms single-inputs]]></content>
      <tags>
        <tag>paper, ËØ≠‰πâÂàÜÂâ≤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yoloÁ≥ªÂàó]]></title>
    <url>%2F2019%2F11%2F28%2Fyolo%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[ÁªºËø∞ [yolov1] Yolov1: You Only Look Once: Unified, Real-Time Object Detection [yolov2] Yolov2: YOLO9000: Better, Faster, Stronger [yolov3] Yolov3: An Incremental Improvement [yolov4] YOLOv4: Optimal Speed and Accuracy of Object Detection [poly-yolo] POLY-YOLO: HIGHER SPEED, MORE PRECISE DETECTION AND INSTANCE SEGMENTATION FOR YOLOV3 [scaled-yolov4] Scaled-YOLOv4: Scaling Cross Stage Partial Network 0. review review0121ÔºöÂÖ≥‰∫éyolo loss ‰πãÂâçÁúãkerasÁâàÁöÑyolo lossÔºåÂåÖÂê´ÂàÜÁ±ªÁöÑbceÔºåÂõûÂΩíÁöÑl2/mseÔºå‰ª•ÂèäconfidenceÁöÑÂõûÂΩílossÔºåÂÖ∂‰∏≠conf lossË¢´Âª∫Ê®°ÊàêÂçïÁ∫ØÁöÑ0-1ÂàÜÁ±ªÈóÆÈ¢òÔºåÁî®bceÊù•ÂÆûÁé∞„ÄÇ ‰∫ãÂÆû‰∏äÂéüÁâàÁöÑyolo loss‰∏≠ÔºåobjectnessÊòØiouÔºàpredÂíågtÁöÑiouÔºâÔºå‰ªéÊÑè‰πâ‰∏äÔºå‰∏ç‰ªÖÊåáÁ§∫ÂΩìÂâçÊ†ºÂ≠êÊúâÊó†ÁõÆÊ†áÔºåËøòÂØπÂΩìÂâçÁöÑbox predictionÂÅö‰∫ÜËØÑ‰º∞ Âõû‰º†Ê¢ØÂ∫¶ ‰∏çÂõû‰º†Ê¢ØÂ∫¶ iouÊòØÈÄöËøáxywhËÆ°ÁÆóÁöÑÔºåscaled_yolov4‰∏≠ÊääËøô‰∏™Ê¢ØÂ∫¶Êà™Êñ≠ÔºåÂè™‰Ωú‰∏∫‰∏Ä‰∏™ÂÄºÔºåÂØπconfidenceËøõË°åÊ¢ØÂ∫¶Âõû‰º†Ôºå Ê¢ØÂ∫¶‰∏çÊà™Êñ≠‰πüÊ≤°ÊúâÈóÆÈ¢òÔºåÁõ∏ÂΩì‰∫éÂØπxywhÂÜçÂõû‰º†‰∏Ä‰∏™iouÁöÑloss 1. Yolov1: You Only Look Once: Unified, Real-Time Object Detection Âä®Êú∫: end-to-end: 2 stages ‚Äî-&gt; 1 stage real-time ËÆ∫ÁÇπÔºö past methods: complex pipelines, hard to optimize(trained separately) DPM use a sliding window and a classifier to evaluate an object at various locations R-CNN use region proposal and run classifier on the proposed boxes, then post-processing in this paper: you only look once at an image rebuild the framework as a single regression problem: single stands for you don‚Äôt have to run classifiers on each patch straight from image pixels to bounding box coordinates and class probabilities: straight stands for you obtain the bounding box and the classification results side by side, comparing to the previous serial pipeline advantagesÔºö fast &amp; twice the mean average precision of other real-time systems CNN sees the entire image thus encodes contextual information generalize better disadvantage: accuracy: ‚Äú it struggles to precisely localize some objects, especially small ones‚Äù ÁªÜËäÇÔºö gridÔºö Our system divides the input image into an S √ó S grid. If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object. predictionÔºö Each grid cell predicts B bounding boxes, confidence scores for these boxes , and C conditional class probabilities for each grid that is an S*S*(B*5+C) tensor We normalize the bounding box width and height by the image width and height so that they fall between 0 and 1. We parametrize the bounding box x and y coordinates to be offsets of a particular grid cell so they are also bounded between 0 and 1. at test timeÔºö We obtain the class-specific confidence for individual box by multiply the class probability and box confidenceÔºö Pr(Class_i | Object) * Pr(Object)* IOU^{truth}_{pred} = Pr(Class_i)* IOU^{truth}_{pred} networkÔºö the convolutional layers extract features from the image while the fully connected layers predict the probabilities and coordinates trainingÔºö activationÔºöuse a linear activation function for the final layer and leaky rectified linear activation all the other layers optimizationÔºöuse sum-squared error, however it does not perfectly align with the goal of maximizing average precision ‚Äã * weights equally the localization error and classification errorÔºö$\lambda_{coord}$ ‚Äã * weights equally the grid cells containing and not-containing objectsÔºö$\lambda_{noobj}$ ‚Äã * weights equally the large boxes and small boxesÔºösquare roots the h&amp;w insteand of the straight h&amp;w lossÔºöpick the box predictor has the highest current IOU with the ground truth per grid cell avoid overfittingÔºödropout &amp; data augmentation ‚Äã * use dropout after the first connected layer, ‚Äã * introduce random scaling and translations of up to 20% of the original image size for data augmentation ‚Äã * randomly adjust the exposure and saturation of the image by up to a factor of 1.5 in the HSV color space for data augmentation inferenceÔºö multiple detectionsÔºösome objects locates near the border of multiple cells and can be well localized by multiple cells. Non-maximal suppression is proved critical, adding 2- 3% in mAP. LimitationsÔºö strong spatial constraintsÔºödecided by the settings of bounding boxes softmax classificationÔºöcan only have one class for each grid ‚ÄúThis spatial constraint lim- its the number of nearby objects that our model can pre- dict. Our model struggles with small objects that appear in groups, such as flocks of birds. ‚Äú ‚Äú It struggles to generalize to objects in new or unusual aspect ratios or configurations. ‚Äú coarse bounding box predictionÔºöthe architecture has multiple downsampling layers the loss function treats errors the same in small bounding boxes versus large bounding boxesÔºö The same error has much greater effect on a small box‚Äôs IOU than a big box. ‚ÄúOur main source of error is incorrect localizations. ‚Äú ComparisonÔºö mAP among real-time detectors and Less Than Real-Time detectorsÔºöless mAP than fast-rcnn but much faster error analysis between yolo and fast-rcnnÔºögreater localization error and less background false-positive combination analysisÔºö[fast-rcnn+yolo] defeats [fast-rcnn+fast-rcnn] since YOLO makes different kinds of mistakes with fast-rcnn generalizabilityÔºöRCNN degrades more because the Selective Search is tuned for natural images, change of dataset makes the proposals get worse. YOLO degrades less because it models the size and shape of objects, change of dataset varies less at object level but more at pixel level. 2. Yolov2: YOLO9000: Better, Faster, Stronger Âä®Êú∫Ôºö run at varying sizesÔºöoffering an easy tradeoff between speed and accuracy recognize a wide variety of objects Ôºöjointly train on object detection and classification, so that the model can predict objects that aren‚Äôt labelled in detection data better performance but still fast ËÆ∫ÁÇπÔºö Current object detection datasets are limited compared to classification datasets leverage the classification data to expand the scope of current detection system joint training algorithm making the object detectors working on both detection and classification data Better performance often hinges on larger networks or ensembling multiple models. However we want a more accurate detector that is still fast YOLOv1‚Äôs shortcomings more localization errors low recall Ë¶ÅÁ¥†Ôºö better faster backbone stronger uses labeled detection images to learn to precisely localize objects uses classification images to increase its vocabulary and robustness ÊñπÊ≥ïÔºö betterÔºö batch normalizationÔºöconvergence &amp; regularization add batch normalization on all of the convolutional layers remove dropout from the model high resolution classifierÔºöpretrain a hi-res classifier first fine tune the classification network at the full 448 √ó 448 resolution for 10 epochs on ImageNet then fine tune the resulting network on detection convolutional with anchor boxesÔºö YOLOv1ÈÄöËøáÁΩëÁªúÊúÄÂêéÁöÑÂÖ®ËøûÊé•Â±ÇÔºåÁõ¥Êé•È¢ÑÊµãÊØè‰∏™grid‰∏äbounding boxÁöÑÂùêÊ†á ËÄåRPNÂü∫‰∫éÂÖàÈ™åÊ°ÜÔºå‰ΩøÁî®ÊúÄÂêé‰∏ÄÂ±ÇÂç∑ÁßØÂ±ÇÔºåÂú®ÁâπÂæÅÂõæÁöÑÂêÑ‰ΩçÁΩÆÈ¢ÑÊµãbounding boxÁöÑoffsetÂíåconfidence ‚ÄúPredicting offsets instead of coordinates simplifies the problem and makes it easier for the network to learn‚Äù YOLOv2ÂéªÊéâ‰∫ÜÂÖ®ËøûÊé•Â±ÇÔºå‰πü‰ΩøÁî®anchor boxÊù•ÂõûÂΩíbounding box eliminate one pooling layer to make the network output have higher resolution shrink the network input to 416416 to obtain an odd number so that there is a *single center cell in the feature map predict class and objectness for every anchor box(offset prediction) instead of nothing(direct location&amp;scale prediction) dimension clusteringÔºö what we want are priors that lead to good IOU scores, thus comes the distance metricÔºö d(box, centroid) = 1 - IOU(box, centroid) direct location predictionÔºö YOLOv1 encounter model instability issue for predicting the (x, y) locations for the box RPN also takes a long time to stabilize by predicting a (tx, ty) and obtain the (x, y) center coordinates indirectly because this formulation is unconstrained so any anchor box can end up at any point in the imageÔºö x = t_x * w_a - x_a\\ y = t_y * h_a - y_a Â≠¶‰π†RPNÔºöÂõûÂΩí‰∏Ä‰∏™Áõ∏ÂØπÈáèÔºåÊØîÁõ≤ÁåúÂõûÂΩí‰∏Ä‰∏™ÁªùÂØπlocationÔºàYOLOv1ÔºâÊõ¥Â•ΩÂ≠¶‰π† Â≠¶‰π†YOLOv1ÔºöÂü∫‰∫écellÁöÑÈ¢ÑÊµãÔºåÂ∞Übounding boxÈôêÂÆöÂú®ÊúâÈôêÂå∫ÂüüÔºå‰∏çÊòØÂÖ®ÂõæÈ£ûÔºàRPNÔºâ YOLOv2ÂØπÊØè‰∏™cellÔºåÂü∫‰∫é5‰∏™prior anchor sizeÔºåÈ¢ÑÊµã5‰∏™bounding boxÔºåÊØè‰∏™bounding boxÂÖ∑Êúâ5Áª¥Ôºö b_x = \sigma(t_x) + c_x\\ b_y = \sigma(t_y) + c_y\\ b_w = p_w e^{t_w}\\ b_h = p_h e^{t_h}\\ Pr(object)*IOU(b,object) = \sigma(t_o) $t_x\ \&amp;\ t_y$Áî®‰∫éÂõûÂΩíbounding boxÁöÑ‰ΩçÁΩÆÔºåÈÄöËøásigmoidÊøÄÊ¥ªÂáΩÊï∞Ë¢´ÈôêÂÆöÂú®0-1ÔºåÈÄöËøá‰∏äÂºèËÉΩÂ§üÈó¥Êé•ÂæóÂà∞bounding boxÁöÑÂΩí‰∏ÄÂåñ‰ΩçÁΩÆÔºàÁõ∏ÂØπÂéüÂõæÔºâ $t_w\ \&amp;\ t_h$Áî®‰∫éÂõûÂΩíbounding boxÁöÑÂ∞∫Â∫¶ÔºåËæìÂá∫Â∫îËØ•‰∏çÊòØ0-1ÈôêÂÆöÔºå$p_w\ \&amp;\ p_h$ÊòØÂÖàÈ™åÊ°ÜÁöÑÂΩí‰∏ÄÂåñÂ∞∫Â∫¶ÔºåÈÄöËøá‰∏äÂºèËÉΩÂ§üÈó¥Êé•ÂæóÂà∞bounding boxÁöÑÂΩí‰∏ÄÂåñÂ∞∫Â∫¶ÔºàÁõ∏ÂØπÂéüÂõæÔºâ $t_o$Áî®‰∫éÂõûÂΩíobjectnessÔºåÈÄöËøásigmoidÈôêÂÆöÂú®0-1‰πãÈó¥ÔºåÂõ†‰∏∫$Pr(object)\ \&amp;\ IOU(b,object)$ÈÉΩÊòØ0-1‰πãÈó¥ÁöÑÂÄºÔºåIOUÈÄöËøáÂâçÈù¢Âõõ‰∏™ÂÄºËÉΩÂ§üÊ±ÇËß£ÔºåËøõËÄåÂèØ‰ª•Ëß£ËÄ¶objectness fine-grained featuresÔºö motiveÔºöÂ∞èÁâ©‰ΩìÁöÑÊ£ÄÊµã‰æùËµñÊõ¥Âä†ÁªÜÁ≤íÂ∫¶ÁöÑÁâπÂæÅ cascadeÔºöFaster R-CNN and SSD both run their proposal networks at various feature maps in the network to get a range of resolutions „ÄêQUESTION„ÄëYOLOv2 simply adds a passthrough layer from an earlier layer at 26 √ó 26 resolutionÔºö latter featuremap ‚Äî-&gt; upsampling concatenate with early featuremap the detector runs on top of this expanded feature map predicts a $NN(3*(4+1+80))$ tensor for each scale multi-scale trainingÔºö Ê®°ÂûãÊú¨Ë∫´‰∏çÈôêÂÆöËæìÂÖ•Â∞∫ÂØ∏Ôºömodel only uses convolutional and pooling layers thus it can be resized on the fly forces the network to learn to predict well across a variety of input dimensions the same network can predict detections at different resolutions lossÔºöcited from the latter yolov3 paper use sum of squared error loss for box coordinate(x,y,w,h)Ôºöthen the gradient is $y_{true} - y_{pred}$ use logistic regression for objectness scoreÔºöwhich should be 1 if the bounding box prior overlaps a ground truth object by more than any other bounding box prior if a bounding box prior is not assigned to a ground truth object it incurs no loss(coordinate&amp;objectness) use binary cross-entropy loss for multilabel classification fasterÔºö darknet-19Ôºö YOLOv1‰∏≠ËÆ®ËÆ∫ËøáÊç¢VGG-16ÂíåYOLOv1‰ΩøÁî®ÁöÑbackboneÂØπÊØîÔºåÂâçËÄÖÊúâmapÊèêÂçáÔºå‰ΩÜÊòØËÄóÊó∂„ÄÇ YOLOv2ÁöÑÊñ∞backboneÔºåÂèÇÊï∞Êõ¥Â∞ëÔºåËÄå‰∏îÁõ∏ÂØπ‰∫éVGG16Âú®ImageNet‰∏äÁ≤æÂ∫¶Êõ¥È´ò„ÄÇ training for classificationÔºö * first train on ImageNet using 224*224 * then fine-tuning on 448*448 training for detectionÔºö remove the last convolutional layer add on three 3 √ó 3 convolutional layers with 1024 filters each followed by a final 1√ó1 convolutional layer with the number of outputs we need for detection add a passthrough from the final 3 √ó 3 √ó 512 layer to the second to last convolutional layer so that our model can use fine grain features. strongerÔºö jointly trainingÔºö‰ª•ÂêéÂÜçÂ°´Âùë ÊûÑÈÄ†Ê†áÁ≠æÊ†ë classification sampleÁî®cls lossÔºådetection sampleÁî®detect loss È¢ÑÊµãÊ≠£Á°ÆÁöÑclassification sampleÁªô‰∏Ä‰∏™.3 IOUÁöÑÂÅáËÆæÂÄºÁî®‰∫éËÆ°ÁÆóobjectness loss 3. Yolov3: An Incremental Improvement Âä®Êú∫Ôºö nothing like super interesting, just a bunch of small changes that make it better ÊñπÊ≥ïÔºö bounding box predictionÔºö use anchor boxes and predicts offsets for each bounding box use sum of squared error loss for training predicts the objectness score for each bounding box using logistic regression one ground truth coresponds to one best box and one loss class predictionÔºö use binary cross-entropy loss for multilabel classification „ÄêNEW„Äëprediction across scalesÔºö the detectorÔºöa few more convolutional layers following the feature map, the last of which predicts a 3-d(for 3 priors) tensor encoding bounding box, objectness, and class predictions expanded feature mapÔºöupsampling the deeper feature map by 2X and concatenating with the former features ‚ÄúWith the new multi-scale predictions, YOLOv3 has better perfomance on small objects and comparatively worse performance on medium and larger size objects ‚Äú „ÄêNEW„Äëfeature extractorÔºö darknet-53 ! trainingÔºöcommon skills 4. ‰∏Ä‰∫õË°•ÂÖÖ metricsÔºömAP ÊúÄÊó©Áî±PASCAL VOCÊèêÂá∫ÔºåËæìÂá∫ÁªìÊûúÊòØ‰∏Ä‰∏™ranked listÔºåÊØè‰∏ÄÈ°πÂåÖÂê´Ê°Ü„ÄÅconfidence„ÄÅclassÔºå yolov3ÊèêÂà∞‰∫Ü‰∏Ä‰∏™‚ÄúCOCOs weird average mean AP metric ‚Äù IoUÔºöÈ¢ÑÊµãÊ°Ü‰∏éground truthÁöÑ‰∫§Âπ∂ÊØîÔºå‰πüË¢´Áß∞‰∏∫JaccardÊåáÊï∞ÔºåÊàë‰ª¨ÈÄöÂ∏∏Áî®ÂÖ∂Êù•Âà§Êñ≠ÊØè‰∏™Ê£ÄÊµãÁöÑÊ≠£Á°ÆÊÄß„ÄÇPASCAL VOCÊï∞ÊçÆÈõÜÁî®0.5‰∏∫ÈòàÂÄºÊù•Âà§ÂÆöÈ¢ÑÊµãÊ°ÜÊòØTrue PositiveËøòÊòØFalse PositiveÔºåCOCOÊï∞ÊçÆÈõÜÂàôÂª∫ËÆÆÂØπ‰∏çÂêåÁöÑIoUÈòàÂÄºËøõË°åËÆ°ÁÆó„ÄÇ ÁΩÆ‰ø°Â∫¶ÔºöÈÄöËøáÊîπÂèòÁΩÆ‰ø°Â∫¶ÈòàÂÄºÔºåÊàë‰ª¨ÂèØ‰ª•ÊîπÂèò‰∏Ä‰∏™È¢ÑÊµãÊ°ÜÊòØPositiveËøòÊòØ Negative„ÄÇ precision &amp; recallÔºöprecision = TP Ôºè(TP + FP)Ôºårecall = TPÔºè(TP + FN)„ÄÇÂõæÁâá‰∏≠Êàë‰ª¨Ê≤°ÊúâÈ¢ÑÊµãÂà∞ÁöÑÊØè‰∏™ÈÉ®ÂàÜÈÉΩÊòØNegativeÔºåÂõ†Ê≠§ËÆ°ÁÆóTrue NegativesÊØîËæÉÈöæÂäû„ÄÇ‰ΩÜÊòØÊàë‰ª¨Âè™ÈúÄË¶ÅËÆ°ÁÆóFalse NegativesÔºåÂç≥Êàë‰ª¨Ê®°ÂûãÊâÄÊºèÊ£ÄÁöÑÁâ©‰Ωì„ÄÇ APÔºö‰∏çÂêåÁöÑÁΩÆ‰ø°Â∫¶‰∏ã‰ºöÂæóÂà∞‰∏çÂêåÁöÑprecision-recall„ÄÇ‰∏∫‰∫ÜÂæóÂà∞precision-recallÊõ≤Á∫øÔºåÈ¶ñÂÖàÂØπÊ®°ÂûãÈ¢ÑÊµãÁªìÊûúËøõË°åÊéíÂ∫èÔºåÊåâÁÖßÂêÑ‰∏™È¢ÑÊµãÂÄºÁΩÆ‰ø°Â∫¶ÈôçÂ∫èÊéíÂàó„ÄÇÁªôÂÆö‰∏çÂêåÁöÑÁΩÆ‰ø°Â∫¶ÈòàÂÄºÔºåÂ∞±Êúâ‰∏çÂêåÁöÑranked outputÔºåRecallÂíåPrecision‰ªÖÂú®È´ò‰∫éËØ•rankÂÄºÁöÑÈ¢ÑÊµãÁªìÊûú‰∏≠ËÆ°ÁÆó„ÄÇËøôÈáåÂÖ±ÈÄâÊã©11‰∏™‰∏çÂêåÁöÑrecallÔºà[0, 0.1, ‚Ä¶, 0.9, 1.0]ÔºâÔºåÈÇ£‰πàAPÂ∞±ÂÆö‰πâ‰∏∫Âú®Ëøô11‰∏™recall‰∏ãprecisionÁöÑÂπ≥ÂùáÂÄºÔºåÂÖ∂ÂèØ‰ª•Ë°®ÂæÅÊï¥‰∏™precision-recallÊõ≤Á∫øÔºàÊõ≤Á∫ø‰∏ãÈù¢ÁßØÔºâ„ÄÇÁªôÂÆörecall‰∏ãÁöÑprecisionËÆ°ÁÆóÔºåÊòØÈÄöËøá‰∏ÄÁßçÊèíÂÄºÁöÑÊñπÂºèÔºö AP = \frac{1}{11}\sum_{r\in\{0,0.1,...,1.0\}}p_{interp}(r) \\ p_{interp}(r) = max_{\tilde r: \tilde r > r} p(\tilde r) mAPÔºöÊ≠§Â∫¶ÈáèÊåáÊ†áÂú®‰ø°ÊÅØÊ£ÄÁ¥¢ÂíåÁõÆÊ†áÊ£ÄÊµãÈ¢ÜÂüüÊúâ‰∏çÂêåÁöÑËÆ°ÁÆóÊñπÂºè„ÄÇÂØπ‰∫éÁõÆÊ†áÊ£ÄÊµãÔºåÂØπ‰∫éÂêÑ‰∏™Á±ªÂà´ÔºåÂàÜÂà´ÊåâÁÖß‰∏äËø∞ÊñπÂºèËÆ°ÁÆóAPÔºåÂèñÊâÄÊúâÁ±ªÂà´ÁöÑAPÂπ≥ÂùáÂÄºÂ∞±ÊòØmAP„ÄÇ evalÔºö yolo_headËæìÂá∫Ôºöbox_xyÊòØboxÁöÑ‰∏≠ÂøÉÂùêÊ†áÔºå(0~1)Áõ∏ÂØπÂÄºÔºõbox_whÊòØboxÁöÑÂÆΩÈ´òÔºå(0~1)Áõ∏ÂØπÂÄºÔºõbox_confidenceÊòØÊ°Ü‰∏≠Áâ©‰ΩìÁΩÆ‰ø°Â∫¶Ôºõbox_class_probsÊòØÁ±ªÂà´ÁΩÆ‰ø°Â∫¶Ôºõ yolo_correct_boxesÂáΩÊï∞ÔºöËÉΩÂ§üÂ∞Übox‰∏≠ÂøÉÁöÑÁõ∏ÂØπ‰ø°ÊÅØËΩ¨Êç¢Êàê[y_min,x_min,y_max,x_max]ÁöÑÁªùÂØπÂÄº yolo_boxes_and_scoresÂáΩÊï∞ÔºöËæìÂá∫ÁΩëÁªúÈ¢ÑÊµãÁöÑÊâÄÊúâbox yolo_evalÂáΩÊï∞ÔºöÂü∫‰∫éscore_threshold„ÄÅmax_boxes‰∏§È°πËøáÊª§ÔºåÁ±ªÂÜÖNMSÔºåÂæóÂà∞ÊúÄÁªàËæìÂá∫ 4. YOLOv4: Optimal Speed and Accuracy of Object Detection Âä®Êú∫ Practical testing the tricks of improving CNN some features work for certain problems/dataset exclusively applicable to the majority of models, tasks, and datasets only increase the training cost [bag-of-freebies] only increase the inference cost by a small amount but can significantly improve the accuracy [bag-of-specials] Optimal Speed and Accuracy ËÆ∫ÁÇπ headÔºö predict classes and bounding boxes one-stage head YOLO, SSD, RetinaNet anchor-freeÔºöCenterNet, CornerNet, FCOS two-stage head R-CNN series anchor-freeÔºöRepPoints neckÔºö collect feature maps from different stages FPN, PAN, BiFPN, NAS-FPN backboneÔºö pre-trained on ImageNet VGG, ResNet, ResNeXt, DenseNet Bag of freebies data augmentation pixel-wise adjustments photometric distortionsÔºöbrightness, contrast, hue, saturation, and noise geometric distortionsÔºörandom scaling, cropping, flipping, and rotating object-wise cutÔºö to imageÔºöCutOut to featuremapsÔºöDropOut, DropConnect, DropBlock addÔºöMixUp, CutMix, GAN data imbalance for classification two-stageÔºöhard example mining one-stageÔºöfocal loss, soft label bounding box regression MSE-regressionÔºötreat [x,y,w,h] as independent variables IoU lossÔºöconsider the integrity &amp; scale invariant Bag of specials enlarging receptive fieldÔºöimproved SPP, ASPP, RFB introducing attention mechanism channel-wise attentionÔºöSE, increase the inference time by about 10% point-wise attentionÔºöSpatial Attention Module (SAM), does not affect the speed of inference strengthening feature integration channel-wise levelÔºöSFAM point-wise levelÔºöASFF scale-wise levelÔºöBiFPN activation functionÔºöA good activation function can make the gradient more efficiently propagated post-processingÔºöÂêÑÁßçNMS ÊñπÊ≥ï choose a backbone ‚Äî- CSPDarknet53 Higher input network size (resolution) ‚Äì for detecting multiple small-sized objects More conv layers ‚Äì for a higher receptive field to cover the increased size of input network More parameters ‚Äì for greater capacity of a model to detect multiple objects of different sizes in a single image add the SPP block over the CSPDarknet53 significantly increases the receptive field separates out the most significant context features causes almost no re- duction of the network operation speed use PANet as the method of parameter aggregation Modified PAN replace shortcut connection of PAN to concatenation use YOLOv3 (anchor based) head Mosaic data augmentation mixes 4 training images allows detection of objects outside their normal context reduces the need for a large mini-batch size Self-Adversarial Training (SAT) data augmentation 1st stage alters images 2nd stage train on the modified images CmBNÔºöa CBN modified version modified SAMÔºöfrom spatial-wise attention to point- wise attention ËøôÈáåÁöÑSAM for „ÄäAn Empirical Study of Spatial Attention Mechanisms in Deep Networks„Äã ÔºåÁ©∫Èó¥Ê≥®ÊÑèÂäõÊú∫Âà∂ ËøòÊúâ‰∏ÄÁØáSAMÊòØ„ÄäSharpness-Aware Minimization for Efficiently Improving Generalization„ÄãÔºågoogleÁöÑÈîêÂ∫¶ÊÑüÁü•ÊúÄÂ∞èÂåñÔºåÁî®Êù•ÊèêÂçáÊ®°ÂûãÊ≥õÂåñÊÄßËÉΩÔºåÊ≥®ÊÑèÂå∫ÂàÜ ÂÆûÈ™å Influence of different features on Classifier training BluringÂíåSwishÊ≤°ÊúâÊèêÂçá Influence of different features on Detector training IoU threshold, CmBN, Cosine annealing sheduler, CIOUÊúâÊèêÂçá POLY-YOLO: HIGHER SPEED, MORE PRECISE DETECTION AND INSTANCE SEGMENTATION FOR YOLOV3 Âä®Êú∫ yoloV3‚Äôs weakness rewritten labels inefficient distribution of anchors light backboneÔºö stairstep upsampling single scale output to extend instance segmentation detect size-independent polygons defined on a polar grid real-time processing ËÆ∫ÁÇπ yolov3 real-time low precision cmp with RetinaNet, EfficientDet low precision of the detection of big boxes rewriting of labels by each-other due to the coarse resolution this paper solutionÔºö Ëß£ÂÜ≥yoloÁ≤æÂ∫¶ÈóÆÈ¢òÔºöpropose a brand-new feature decoder with a single ouput tensor that goes to head with higher resolution Â§öÂ∞∫Â∫¶ÁâπÂæÅËûçÂêàÔºöutilize stairstep upscaling ÂÆû‰æãÂàÜÂâ≤Ôºöbounding polygon within a poly grid instance segmentation two-stageÔºömask-rcnn one-stageÔºö top-downÔºösegmenting this object within a bounding box bottom-upÔºöstart with clustering pixels direct methodsÔºöÊó¢‰∏çÈúÄË¶Åbounding box‰πü‰∏çÈúÄË¶Åclustered pixelsÔºåPolarMask cmp with PolarMask size-independentÔºöÂ∞∫Â∫¶ÔºåÂ§ßÂ∞èÁõÆÊ†áÈÉΩËÉΩÊ£ÄÊµã dynamic number of verticesÔºöÂ§öËæπÂΩ¢ÂÆöÁÇπÂèØÂèò yolov3 issues rewriting of labelsÔºö ‰∏§‰∏™ÁõÆÊ†áÂ¶ÇÊûúËêΩÂú®Âêå‰∏Ä‰∏™Ê†ºÂ≠êÈáåÔºåÂú®‰∏Ä‰∏™Â∞∫Â∫¶‰∏äground truth labelÂè™‰ºö‰øùÁïô‰∏Ä‰∏™box ÂØπË∂äÂ∞èÁöÑÁâπÂæÅÂõæÔºågridË∂äÂ§ßÔºåËøô‰∏™ÈóÆÈ¢òË∂ä‰∏•Èáç imbalanced distribution of anchors across output scales anchorÂ¶ÇÊûúÈÄâÁöÑ‰∏çÂêàÁêÜÔºå‰ºöÂØºËá¥ÁâπÂæÅÂõæÂ∞∫Â∫¶ÂíåanchorÂ∞∫Â∫¶‰∏çÂåπÈÖç most of the boxes will be captured by the middle output layer and the two other layers will be underused Â¶Ç‰∏äÈù¢ËΩ¶ÁöÑcaseÔºåÂ§ßÂ§öÊï∞ËΩ¶ÁöÑÊ°ÜÂæàÂ∞èÔºåËÅöÁ±ªÂá∫ÁöÑÁªôlevel0Âíålevel1ÁöÑanchor shapeËøòÊòØÂæàÂ∞èÔºå‰ΩÜÊòØlevel0ÊòØÁ®ÄÁñègrid ‰∏ÄÊñπÈù¢Ôºågrid shapeÂíåanchor shape‰∏çÂåπÈÖç ‰∏ÄÊñπÈù¢Ôºålabel rewritenÈóÆÈ¢ò‰ºöÂçáÁ∫ß ÂèçËøáÊù•ÔºåÂ¶ÇÊûúdense grid‰∏äÈ¢ÑÊµãÂ§ßÁõÆÊ†áÔºå‰ºöÂèóÂà∞ÊÑüÂèóÈáéÁöÑÂà∂Á∫¶ ‰∏ÄÁßçËß£ÂÜ≥ÊñπÊ°àÊòØÂü∫‰∫éÊÑüÂèóÈáéÈ¶ñÂÖàÂØπgt boxÂàÜÊàê‰∏âÁªÑÔºåÁÑ∂ÂêéÂàÜÂà´ËÅöÁ±ªÔºåÁÑ∂Âêé9ÈÄâ1 yolov3ÂéüÊñáÔºöYOLOv3 has relatively high $AP_{small}$ performance. However, it has comparatively worse performance on medium and larger size objects. More investigation is needed to get to the bottom of this. Â∞èÁõÆÊ†áperformanceÊõ¥Â•ΩÔºåÂ§ßÁõÆÊ†áworseÔºå‰∏ªË¶ÅÊòØÂ∞±ÊòØÂõ†‰∏∫coarse grid‰∏äÂ≠òÂú®label rewritenÈóÆÈ¢òÔºåÂ≠òÂú®ÈÉ®ÂàÜgt boxË¢´ÊäëÂà∂Êéâ‰∫Ü„ÄÇ ÊñπÊ≥ï architecture single output higher resolutionÔºöstride4 handle all the anchors at once cross-scale fusion hypercolumn techniqueÔºöadd operation stairstep interpolationÔºöx2 x2 ‚Ä¶ SE-blocks reduced the number of convolutional filters to 75% in the feature extraction phase bounding polygons extend the box tupleÔºö$b_i=\{b_i^{x^1},b_i^{y^1},b_i^{x^2},b_i^{y^2},V_i\}$ The center of a bounding box is used as the origin polygon tupleÔºö$v_{i,j}=\{\alpha_{i,j},\beta_{i,j},\gamma_{i,j}\}$ polar coordinateÔºödistance &amp; oriented angleÔºåÁõ∏ÂØπË∑ùÁ¶ªÔºàÁõ∏ÂØπanchor boxÁöÑÂØπËßíÁ∫øÔºâÔºåÁõ∏ÂØπËßíÂ∫¶ÔºànormÂà∞[0,1]Ôºâ polar cellÔºö‰∏ÄÂÆöËßíÂ∫¶ÁöÑÊâáÂΩ¢Âå∫Âüü ÂÜÖÔºåÂ¶ÇÊûúsectorÂÜÖÊ≤°ÊúâÂÆöÁÇπÔºåconf=0 general shapeÔºö ‰∏çÂêåÂ∞∫Â∫¶ÔºåÂΩ¢Áä∂Áõ∏ÂêåÁöÑobjectÔºåÂú®polar coord‰∏ãË°®Á§∫ÊòØ‰∏ÄÊ†∑ÁöÑ distance*anchor boxÁöÑÂØπËßíÁ∫øÔºåËΩ¨Êç¢ÊàêÁªùÂØπÂ∞∫Â∫¶ bounding boxÁöÑ‰∏§‰∏™ÂØπËßíÈ¢ÑÊµãÔºåË¥üË¥£Â∞∫Â∫¶‰º∞ËÆ°ÔºåpolygonÂè™Ë¥üË¥£È¢ÑÊµãÂΩ¢Áä∂ sharing values should make the learning easier mix loss outputÔºöa*(4+1+3*n_vmax) box center lossÔºöbce box wh lossÔºöl2 loss conf lossÔºöbce with ignore mask cls lossÔºöbce polygon lossÔºö$\gamma(log(\frac{\alpha}{anchor^d})-\hat a)^2 + \gammabce(\beta,\hat{beta})+bce(\gamma, \hat \gamma)$ auxiliary task learningÔºö ‰ªªÂä°Èó¥Áõ∏‰∫íboost converge faster Scaled-YOLOv4: Scaling Cross Stage Partial Network Âä®Êú∫ model scaling method redesign yolov4 and propose yolov4-CSP develop scaled yolov4 yolov4-tiny yolov4-large Ê≤°‰ªÄ‰πàÊäÄÊúØÁªÜËäÇÔºåÂ∞±ÊòØÁΩëÁªúÁªìÊûÑÂ§ßÊõ¥Êñ∞ ËÆ∫ÁÇπ common technique changes depth &amp; width of the backbone recently there are NAS model scaling input size„ÄÅwidth„ÄÅdepthÂØπÁΩëÁªúËÆ°ÁÆóÈáèÂëàÁé∞square, linear, and square increase ÊîπÊàêCSPÁâàÊú¨‰ª•ÂêéÔºåËÉΩÂ§üÂáèÂ∞ëÂèÇÊï∞Èáè„ÄÅËÆ°ÁÆóÈáèÔºåÊèêÈ´òaccÔºåÁº©Áü≠inference time Ê£ÄÊµãÁöÑÂáÜÁ°ÆÊÄßÈ´òÂ∫¶‰æùËµñreception fieldÔºåRFÈöèÁùÄdepthÁ∫øÊÄßÂ¢ûÈïøÔºåÈöèÁùÄstrideÂÄçÊï∞Â¢ûÈïøÔºåÊâÄ‰ª•‰∏ÄËà¨ÂÖàÁªÑÂêàË∞ÉËäÇinput sizeÂíåstageÔºåÁÑ∂ÂêéÂÜçÊ†πÊçÆÁÆóÂäõË∞ÉÊï¥depthÂíåwidth ÊñπÊ≥ï backboneÔºöCSPDarknet53 neckÔºöCSP-PANÔºåÂáèÂ∞ë40%ËÆ°ÁÆóÈáèÔºåSPP yoloV4-tiny yoloV4-largeÔºöP456]]></content>
      <tags>
        <tag>ÁõÆÊ†áÊ£ÄÊµãÔºåone-stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[triplet-center-lossËÆ∫Êñá]]></title>
    <url>%2F2019%2F11%2F13%2Ftriplet-center-loss%E8%AE%BA%E6%96%87%2F</url>
    <content type="text"><![CDATA[0. before readingÁªìÂêàÔºö triplet lossÔºöËÄÉËôëÁ±ªÈó¥ÂÖ≥Á≥ªÔºå‰ΩÜËÆ°ÁÆóÂ§çÊùÇÂ∫¶È´òÔºåÂõ∞ÈöæÊ†∑Êú¨ÈöæÊåñÊéò center lossÔºöËÄÉËôëÁ±ªÂÜÖÂÖ≥Á≥ª TCLÔºöÂêåÊó∂Â¢ûÂä†Á±ªÂÜÖÊï∞ÊçÆÁöÑÁ¥ßÂÆûÂ∫¶ÔºàcompactnessÔºâÂíåÁ±ªÈó¥ÁöÑÂàÜÁ¶ªÂ∫¶ÔºàseparabilityÔºâ ‰∏âÂÖÉÁªÑÂè™ËÄÉËôëÊ†∑Êú¨„ÄÅÊâÄÂ±ûÁ±ª‰∏≠ÂøÉ„ÄÅÊúÄËøëÈÇªÁ±ªÁöÑ‰∏≠ÂøÉ„ÄÇÈÅøÂÖç‰∫ÜÂª∫Á´ãtripletsÁöÑÂ§çÊùÇÂ∫¶Âíåmining hard samplesÁöÑÈöæÂ∫¶„ÄÇ titleÔºöTriplet-Center Loss for Multi-View 3D Object Retrieval Âä®Êú∫Ôºödeep metric learning the learned features using softmax loss are not discriminative enough in nature although samples of the two classes are separated by the decision boundary elaborately, there exists significant intra-class variations QUESTION1Ôºöso what? how does this affect the current task? Âä®Êú∫ÊèèËø∞‰∏çÂÖÖÂàÜ„ÄÇ QUESTION2ÔºöÂú®‰∫åÁª¥Âπ≥Èù¢‰∏äoverlap‰∏ç‰ª£Ë°®Âú®È´òÁª¥Á©∫Èó¥‰∏≠overlapÔºåËøôÁßçillustrationÁ©∂Á´üÊòØÂê¶ÊúâÊÑè‰πâ„ÄÇ ANSWER for aboveÔºöÈ´òÁª¥Á©∫Èó¥ÂèØÂàÜÔºåÊäïÂΩ±Âà∞‰∫åÁª¥Âπ≥Èù¢‰∏ç‰∏ÄÂÆöÂèØÂàÜÔºå‰ΩÜÊòØÂèçËøáÊù•Ôºå‰∫åÁª¥Âπ≥Èù¢‰∏äÈ´òÂ∫¶ÂèØÂàÜÔºåÊò†Â∞Ñ‰ºöÈ´òÁª¥Á©∫Èó¥Êï∞ÊçÆ‰ªçÊóßÊòØÈ´òÂ∫¶ÂèØÂàÜÁöÑ„ÄÇÂè™ËÉΩËØ¥ÔºåÂêéËÄÖËÉΩÂ§üÁ°Æ‰øù‰∏çÂêåÁ±ªÂà´Êï∞ÊçÆÁ¶ªÊï£ÊÄßÊõ¥Â•ΩÔºå‰∏çËÉΩËØ¥ÊòéÂâçËÄÖÊï∞ÊçÆÁ¶ªÊï£ÊÄß‰∏çÂ•ΩÔºàÂ¶ÇÊûúÂÆö‰πâ‰∫ÜÈ´òÁª¥Ë∑ùÁ¶ªÔºå‰πüÂèØ‰ª•ËØ¥ÊòéÔºâ„ÄÇ Â∫îÁî®Âú∫ÊôØÔºö3D object retrieval Ë¶ÅÁ¥†Ôºö learns a center for each class requires that the distances between samples and centers from the same class are smaller than those from different classes, in this way the samples are pulled closer to the corresponding center and meanwhile pushed away from the different centers both the inter-class separability and the intra-class variations are considered ËÆ∫ÁÇπÔºö Compared with triplet loss, TCL avoids the complex construction of triplets and hard sample mining mechanism. Compared with center loss, TCL not only considers to reduce the intra-class variations. QUESTIONÔºöwhat about the comparison with [softmax loss + center loss]? ANSWER for aboveÔºöcenter-loss is actually representing for the joint loss [softmax loss + center loss]. ‚Äò‚ÄôSince the class centers are updated at each iteration based on a mini-batch instead of the whole dataset, which can be very unstable, it has to be under the joint supervision of softmax loss during training. ‚Äò‚Äô Êú¨ÊñáÂÅöÊ≥ïÔºö the proposed TCL is used as the supervision loss the softmax loss could be also combined in as an addition ÁªÜËäÇÔºö TCLÔºö L_{tc} = \sum_{i=1}^Mmax(D(f_i, c_{y^i}) + m - min_{j\neq y^i}D(f_i, c_j), 0) ÂâçÂçäÈÉ®ÂàÜÊòØcenter-lossÔºåÁ±ªÂÜÖÊ¨ßÂá†ÈáåÂæóË∑ùÁ¶ªÔºåÂêéÂçäÈÉ®ÂàÜÊòØÊØè‰∏™Ê†∑Êú¨Âíå‰∏éÂÖ∂ÊúÄËøëÁöÑnegative center‰πãÈó¥ÁöÑË∑ùÁ¶ª„ÄÇ ‚ÄòUnlike center loss, TCL can be used independently from softmax loss. However‚Ä¶ ‚Äò ‰ΩúËÄÖËß£ÈáäËØ¥ÔºåÂõ†‰∏∫center layerÊòØÈöèÊú∫ÂàùÂßãÂåñÂá∫Êù•ÁöÑÔºåËÄå‰∏îÊòØbatch updatingÔºåÂõ†Ê≠§ÂºÄÂßãÈò∂ÊÆµ‰ºöÊØîËæÉtrickyÔºå‚Äôwhile softmax loss could serve as a good guider for seeking better class centers ‚Äò Ë∞ÉÂèÇ‰∏≠ÊèêÂà∞‚Äôm is fixed to 5‚ÄôÔºåËØ¥ÊòéÊú¨ÊñáÂØπfeature vectorÊ≤°ÊúâÂÅönormalizationÔºàÁõ∏ÊØî‰πã‰∏ãfacenetÂÅö‰∫ÜÂΩí‰∏ÄÂåñÔºåÈôêÂÆöÊâÄÊúâembeddingÂàÜÂ∏ÉÂú®È´òÁª¥ÁêÉÈù¢‰∏äÔºâ„ÄÇ Ë°°ÈáèÊåáÊ†áÔºöAUCÂíåMAPÔºåËøôÊòØ‰∏Ä‰∏™retrieval‰ªªÂä°ÔºåÊúÄÁªàÈúÄË¶ÅÁöÑÊòØembeddingÔºåÁªôÂÆöQueryÔºåÂè¨Âõûtop matches„ÄÇ reviewsÔºö ‰∏™‰∫∫ÁêÜËß£Ôºö softmaxÂàÜÁ±ªÂô®Êó®Âú®Êï∞ÊçÆÂèØÂàÜÔºåÂØπ‰∫éÂàÜÁ±ªËæπÁïå„ÄÅfeature vectorÁöÑÁ©∫Èó¥ÊÑè‰πâ‰∏çÂ≠òÂú®‰∏Ä‰∏™ÂÖ∑Ë±°ÁöÑÊèèËø∞„ÄÇdeep metric learningËÉΩÂ§üÂºïÂÖ•ËøôÁßçÂÖ∑Ë±°ÁöÑ„ÄÅÂõæÂÉèÂ≠¶ÁöÑÊÑè‰πâÔºåÂú®Ê≠§Âü∫Á°Ä‰∏äÔºåÊé¢ËÆ®distance„ÄÅcenterÊâçÊúâÊÑè‰πâ„ÄÇ Â∞±Â∞ÅÈó≠Á±ªÊï∞ÊçÆÔºàÁ±ªÂà´ÊúâÈôê‰∏îÂ∑≤Áü•ÔºâÂàÜÁ±ªÊù•ËÆ≤ÔºåÂàÜÁ±ªËæπÁïåÊúâÊó†ÂõæÂÉèÂ≠¶ÊèèËø∞ÂÖ∂ÂÆûÊÑè‰πâ‰∏çÂ§ß„ÄÇÂ∑≤Áü•ÁöÑÊï∞ÊçÆÂàÜÂ∏ÉÂ∞ΩÂèØËÉΩdiscriminativeÁöÑ‰∏ªË¶ÅÊÑè‰πâÊòØÈíàÂØπÊú™Áü•Á±ªÂà´ÔºåÊàë‰ª¨Â∏åÊúõÁªôÂà∞Ê®°Âûã‰∏Ä‰∏™Êú™Áü•Êï∞ÊçÆÊó∂ÔºåÂÆÉËÉΩÂ§üÊ£ÄÊµãÂá∫Êù•ÔºåËÄå‰∏çÊòØÂàíÂÖ•Êüê‰∏™Â∑≤Áü•Á±ªÔºàsoftmaxÔºâ„ÄÇ TCLÁöÑÊúÄÂ§ßË¥°ÁåÆÂ∫îËØ•ÊòØÊÉ≥Âà∞Áî®centerÊõø‰ª£Ê†∑Êú¨Êù•ËøõË°åmetric judgementÔºåÊîπÂñÑtriplet-lossÂ§çÊùÇËÆ°ÁÆóÈáèËøô‰∏ÄÈóÆÈ¢òÔºåÂêéËÄÖÂÆûÈôÖËÆ≠Ëµ∑Êù•Â§™Èöæ‰∫ÜÔºåÊ≤°ÊúâÊÑüÊÉÖÁöÑGPUÂêûÂô¨Êú∫Âô®„ÄÇ XXXÔºö ËÉΩÂ§üÂºïÂÖ•ËøôÁßçÂÖ∑Ë±°ÁöÑ„ÄÅÂõæÂÉèÂ≠¶ÁöÑÊÑè‰πâÔºåÂú®Ê≠§Âü∫Á°Ä‰∏äÔºåÊàë‰ª¨Êé¢ËÆ®distance„ÄÅcenterÊâçÊúâÊÑè‰πâ„ÄÇ ‚Äã]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dicomReader]]></title>
    <url>%2F2019%2F11%2F11%2FdicomReader%2F</url>
    <content type="text"><![CDATA[read a dcm file 1234import SimpleITK as sitkimage = sitk.ReadImage(dcm_file)image_arr = sitk.GetArrayFromImage(image) read a dcm series 12345678910series_IDs = sitk.ImageSeriesReader.GetGDCMSeriesIDs(series_path)nb_series = len(series_IDs)print(nb_series)# ÈªòËÆ§Ëé∑ÂèñÁ¨¨‰∏Ä‰∏™Â∫èÂàóÁöÑÊâÄÊúâÂàáÁâáË∑ØÂæÑdicom_names = sitk.ImageSeriesReader.GetGDCMSeriesFileNames(file_path)series_reader = sitk.ImageSeriesReader()series_reader.SetFileNames(dicom_names)image3D = series_reader.Execute() read a dcm case 123456series_IDs = sitk.ImageSeriesReader.GetGDCMSeriesIDs(case_path)for series_id in series_IDs: dicom_names = sitk.ImageSeriesReader.GetGDCMSeriesFileNames(case_path, series_id) series_reader = sitk.ImageSeriesReader() series_reader.SetFileNames(dicom_names) image3D = series_reader.Execute() read tag 12# È¶ñÂÖàÂæóÂà∞imageÂØπË±°Image_type = image.GetMetaData("0008|0008") if image.HasMetaData("0008|0008") else 'Nan' ÂèëÁé∞‰∏ÄÁßçÂ∫èÂàóÔºåÊØèÂº†ÂõæÁöÑÂ∞∫ÂØ∏‰∏çÂêåÔºåËøôÊ†∑ÊâßË°åseries_readerÁöÑÊó∂ÂÄô‰ºöÊä•ÈîôÔºåÂõ†‰∏∫series_reader‰ºö‰æùÁÖßÁ¨¨‰∏ÄÂ±ÇÁöÑÂõæÂÉèÂ∞∫ÂØ∏Áî≥ËØ∑Á©∫Èó¥ÔºåÊâÄ‰ª•Ë¶Å‰πàÂºÇÂ∏∏Ë¶Å‰πàÈÄêÂº†ËØª„ÄÇ reference: http://itk-users.7.n7.nabble.com/ITK-users-Reader-InvalidRequestedRegionError-td38608.html]]></content>
      <tags>
        <tag>SimpleITK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++ tricks in engineering]]></title>
    <url>%2F2019%2F11%2F06%2Fc-tricks-in-engineering%2F</url>
    <content type="text"><![CDATA[Êï∞ÁªÑ‰º†ÂèÇ Â∑•Á®ãÂåñË¢´Âùë‰∫ÜÂ•ΩÂ§öÂõûÔºÅ C/C++ ‰º†ÈÄíÊï∞ÁªÑÔºåËôΩÁÑ∂‰º†ÈÄíÁöÑÊòØÈ¶ñÂú∞ÂùÄÂú∞ÂùÄÔºå‰ΩÜÊòØÂèÇÊï∞Âà∞‰∫ÜÂáΩÊï∞ÂÜÖÂ∞±Êàê‰∫ÜÊôÆÈÄöÊåáÈíà„ÄÇ ÊâÄ‰ª•ËØïÂõæÂú®Ë∞ÉÁî®ÂáΩÊï∞‰∏≠Ê±ÇÂèñÊâÄ‰º†ÈÄíÊï∞ÁªÑÁöÑÈïøÂ∫¶ÊòØË°å‰∏çÈÄöÁöÑ„ÄÇ vector‰º†ÂèÇ ‰º†ÂÄº‚Äî&gt;Êã∑Ë¥ùÊûÑÈÄ†Ôºå‰º†ÂºïÁî®ÔºèÊåáÈíà‚Äî&gt;‰∏çÂèëÁîüÊã∑Ë¥ùÊûÑÈÄ†„ÄÇ ÂÆûÈôÖÂ∑•Á®ãÂåñ‰∏≠ÈÅáÂà∞ÁöÑÈóÆÈ¢òÊòØÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™vector\ imgsÂØπË±°Ôºå‰º†ÂÖ•ÂáΩÊï∞‰ª•ÂêéÔºåÂú®ÂáΩÊï∞ÂÜÖÈÉ®ÂàõÂª∫Á©∫Èó¥cv::Mat imgÔºåÁÑ∂ÂêéÂ∞Üimg pushËøõvector„ÄÇÂú®ÂáΩÊï∞Â§ñËØªÂèñËØ•vectorÁöÑÊó∂ÂÄôÂèëÁé∞ÂÖ∂ÂÜÖÈÉ®Ê≤°ÂÄº„ÄÇ Ë¶ÅÁÇπÔºö1. Ë¶Å‰º†ÂºïÁî®Ôºå2. push cloneÔºöimgs.push_back(img) Âè¶Â§ñÔºåvectorÂèØ‰ª•‰Ωú‰∏∫ÂáΩÊï∞ËøîÂõûÂÄº„ÄÇ]]></content>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÂõæÂÉèÁÆóÊ≥ïÁªºËø∞]]></title>
    <url>%2F2019%2F10%2F31%2F%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[Á±ªÂà´ ÊåâÁÖß‰ªªÂä°Á±ªÂûãÔºöÂ∫¶ÈáèÂ≠¶‰π†Ôºàmetric learningÔºâÂíåÊèèËø∞Â≠êÂ≠¶‰π†Ôºàimage descriptor learningÔºâ ÊåâÁÖßÁΩëÁªúÁªìÊûÑÔºöpairwiseÁöÑsiameseÁªìÊûÑ„ÄÅtripletÁöÑthree branchÁªìÊûÑ„ÄÅ‰ª•ÂèäÂºïÂÖ•Â∞∫Â∫¶‰ø°ÊÅØÁöÑcentral-surroundÁªìÊûÑ ÊåâÁÖßÁΩëÁªúËæìÂá∫ÔºöÁâπÂæÅÂêëÈáèÔºàfeature embeddingÔºâÂíåÂçï‰∏™Ê¶ÇÁéáÂÄºÔºàpairwise similarityÔºâ ÊåâÁÖßÊçüÂ§±ÂáΩÊï∞ÔºöÂØπÊØîÊçüÂ§±ÂáΩÊï∞„ÄÅ‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞„ÄÅtriplet loss„ÄÅhinge lossÁ≠âÔºåÊ≠§Â§ñÊçüÂ§±ÂáΩÊï∞ÂèØ‰ª•Â∏¶ÊúâÈöêÂºèÁöÑÂõ∞ÈöæÊ†∑Êú¨ÊåñÊéòÔºå‰æãÂ¶Çpn-net‰∏≠ÁöÑsoftpnÁ≠âÔºå‰πüÂèØ‰ª•ÊòØÊòæÁ§∫ÁöÑÂõ∞ÈöæÊåñÊéò„ÄÇ PlainÁΩëÁªú ‰∏ªË¶ÅÊòØËØ¥AlexNetÔºèVGG-NetÔºåÂêéËÄÖÊõ¥Â∏∏Áî®‰∏Ä‰∫õ„ÄÇ PlainÁΩëÁªúÁöÑËÆæËÆ°‰∏ªË¶ÅÈÅµÂæ™‰ª•‰∏ãÂá†‰∏™ÂáÜÂàôÔºö Ôºà1ÔºâËæìÂá∫ÁâπÂæÅÂõæÂ∞∫ÂØ∏Áõ∏ÂêåÁöÑÂ±Ç‰ΩøÁî®Áõ∏ÂêåÊï∞ÈáèÁöÑÊª§Ê≥¢Âô®„ÄÇ Ôºà2ÔºâÂ¶ÇÊûúÁâπÂæÅÂõæÂ∞∫ÂØ∏ÂáèÂçäÔºåÈÇ£‰πàÊª§Ê≥¢Âô®Êï∞ÈáèÂ∞±Âä†ÂÄçÔºå‰ªéËÄå‰øùËØÅÊØèÂ±ÇÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶Áõ∏ÂêåÔºàËøôÊòØ‰∏∫Âï•ÔºüÔºüÔºâ„ÄÇ ÂêçËØç ÊÑüÂèóÈáéÔºöÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÊØè‰∏ÄÂ±ÇËæìÂá∫ÁöÑÁâπÂæÅÂõæ‰∏äÁöÑÂÉèÁ¥†ÁÇπÂú®ÂéüÂßãÂõæÂÉè‰∏äÊò†Â∞ÑÂå∫ÂüüÁöÑÂ§ßÂ∞è„ÄÇÈÄö‰øóÁöÑËØ¥ÔºåÂ∞±ÊòØËæìÂÖ•ÂõæÂÉèÂØπËøô‰∏ÄÂ±ÇËæìÂá∫ÁöÑÁ•ûÁªèÂÖÉÁöÑÂΩ±ÂìçÊúâÂ§öÂ§ß„ÄÇ ÊÑüÂèóÈáéËÆ°ÁÆóÔºöÁî±ÂΩìÂâçÂ±ÇÂêëÂâçÊé®ÔºåÈúÄË¶ÅÁöÑÂèÇÊï∞ÊòØkernel sizeÂíåstride„ÄÇ N\_RF = kernel\_size + (cur\_RF-1)*stride ÂÖ∂‰∏≠$cur_RF$ÊòØÂΩìÂâçÂ±ÇÔºàstart from 1ÔºâÔºå$N_RF$„ÄÅ$kernel_size$„ÄÅ$stride$ÊòØ‰∏ä‰∏ÄÂ±ÇÂèÇÊï∞„ÄÇ ÊúâÊïàÊÑüÂèóÈáéÔºöÂπ∂‰∏çÊòØÊÑüÂèóÈáéÂÜÖÊâÄÊúâÂÉèÁ¥†ÂØπËæìÂá∫ÂêëÈáèÁöÑË¥°ÁåÆÁõ∏ÂêåÔºåÂú®ÂæàÂ§öÊÉÖÂÜµ‰∏ãÊÑüÂèóÈáéÂå∫ÂüüÂÜÖÂÉèÁ¥†ÁöÑÂΩ±ÂìçÂàÜÂ∏ÉÊòØÈ´òÊñØÔºåÊúâÊïàÊÑüÂèóÈáé‰ªÖÂç†ÁêÜËÆ∫ÊÑüÂèóÈáéÁöÑ‰∏ÄÈÉ®ÂàÜÔºå‰∏îÈ´òÊñØÂàÜÂ∏É‰ªé‰∏≠ÂøÉÂà∞ËæπÁºòÂø´ÈÄüË°∞Âáè„ÄÇ ÊÑüÂèóÈáéÂ§ßÂ∞èÔºö Â∞èÊÑüÂèóÈáéÔºölocalÔºå‰ΩçÁΩÆ‰ø°ÊÅØÊõ¥ÂáÜÁ°Æ Â§ßÊÑüÂèóÈáéÔºöglobalÔºåËØ≠‰πâ‰ø°ÊÅØÊõ¥‰∏∞ÂØå inception moduleÔºö‰∏ãÂõæ‰∏∫ÂÖ∂‰∏≠‰∏ÄÁßç„ÄÇ ÊÑè‰πâÔºöÂ¢ûÂä†ÁΩëÁªúÊ∑±Â∫¶ÂíåÂÆΩÂ∫¶ÁöÑÂêåÊó∂ÔºåÂáèÂ∞ëÂèÇÊï∞„ÄÇÁªìÊûÑ‰∏≠ÂµåÂÖ•‰∫ÜÂ§öÂ∞∫Â∫¶‰ø°ÊÅØÔºåÈõÜÊàê‰∫ÜÂ§öÁßç‰∏çÂêåÊÑüÂèóÈáé‰∏äÁöÑÁâπÂæÅ„ÄÇ building blockÔºöÂ∑¶ËæπËøôÁßçÔºåÁ∫¢Ëâ≤Ê°ÜÊ°ÜÈáåÈù¢ÊòØ‰∏Ä‰∏™block„ÄÇ Âá†‰∏™Áõ∏ÂêåÁöÑbuilding blockÂ†ÜÂè†‰∏∫‰∏ÄÂ±Çconv„ÄÇÂú®Á¨¨‰∏Ä‰∏™building BlockÂùó‰∏≠ÔºåËæìÂá∫ÁâπÂæÅÂõæÁöÑÂ∞∫ÂØ∏‰∏ãÈôç‰∏ÄÂçäÔºàÁ¨¨‰∏Ä‰∏™Âç∑ÁßØstride=2ÔºâÔºåÂâ©‰ΩôÁöÑbuilding BlockÂùóËæìÂÖ•ËæìÂá∫Â∞∫ÂØ∏ÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇ bottleneckÔºöÂè≥ËæπËøôÁßçÔºåËìùËâ≤Ê°ÜÊ°Üblock„ÄÇÂ≠óÈù¢ÊÑèÊÄùÔºåÁì∂È¢àÔºåÂΩ¢ÂÆπËæìÂÖ•ËæìÂá∫Áª¥Â∫¶Â∑ÆË∑ùËæÉÂ§ß„ÄÇ Á¨¨‰∏Ä‰∏™1*1Ë¥üË¥£Èôç‰ΩéÁª¥Â∫¶ÔºåÁ¨¨‰∫å‰∏™1*1Ë¥üË¥£ÊÅ¢Â§çÁª¥Â∫¶Ôºå3*3Â±ÇÂ∞±Â§ÑÂú®‰∏Ä‰∏™ËæìÂÖ•ÔºèËæìÂá∫Áª¥Â∫¶ËæÉÂ∞èÁöÑÁì∂È¢à„ÄÇ Â∑¶Âè≥‰∏§ÁßçÁªìÊûÑÊó∂Èó¥Â§çÊùÇÂ∫¶Áõ∏‰ºº„ÄÇ &lt;img src=&quot;ÂõæÂÉèÁÆóÊ≥ïÁªºËø∞/block.png&quot; width=&quot;30%;&quot; /&gt; &lt;img src=&quot;ÂõæÂÉèÁÆóÊ≥ïÁªºËø∞/ImageNet.png&quot; width=&quot;110%;&quot; /&gt; top-1Âíåtop-5Ôºötop-1Â∞±ÊòØÈ¢ÑÊµãÊ¶ÇÁéáÊúÄÂ§ßÁöÑÁ±ªÂà´Ôºåtop-5ÂàôÂèñÊúÄÂêéÈ¢ÑÊµãÊ¶ÇÁéáÁöÑÂâç‰∫î‰∏™ÔºåÂè™Ë¶ÅÂÖ∂‰∏≠ÂåÖÂê´Ê≠£Á°ÆÁ±ªÂà´ÂàôËÆ§‰∏∫È¢ÑÊµãÊ≠£Á°Æ„ÄÇ ‰ΩøÁî®top-5‰∏ªË¶ÅÊòØÂõ†‰∏∫ImageNet‰∏≠ÂæàÂ§öÂõæÁâá‰∏≠ÂÖ∂ÂÆûÊòØÂåÖÂê´Â§ö‰∏™Áâ©‰ΩìÁöÑ„ÄÇ accuracy„ÄÅerror rate„ÄÅF1-score„ÄÅsensitivity„ÄÅspecificity„ÄÅprecision„ÄÅrecall accuracyÔºöÊÄª‰ΩìÂáÜÁ°ÆÁéá precisionÔºö‰ªéÁªìÊûúËßíÂ∫¶ÔºåÂçï‰∏ÄÁ±ªÂà´ÂáÜÁ°ÆÁéá recallÔºö‰ªéËæìÂÖ•ËßíÂ∫¶ÔºåÈ¢ÑÊµãÁ±ªÂà´ÁúüÂÆû‰∏∫1ÁöÑÂáÜÁ°ÆÁéá P-RÊõ≤Á∫øÔºöÈÄâÁî®‰∏çÂêåÈòàÂÄºÔºåprecision-recallÂõ¥ÊàêÁöÑÊõ≤Á∫ø APÔºöÂπ≥ÂùáÁ≤æÂ∫¶ÔºåP-RÊõ≤Á∫øÂõ¥‰ΩèÁöÑÈù¢ÁßØ F1-scoreÔºöÂØπ‰∫éÊüê‰∏™ÂàÜÁ±ªÔºåÁªºÂêà‰∫ÜPrecisionÂíåRecallÁöÑ‰∏Ä‰∏™Âà§Êñ≠ÊåáÊ†áÔºåÂõ†‰∏∫ÈÄâÁî®‰∏çÂêåÈòàÂÄºÔºåprecision-recall‰ºöÈöè‰πãÂèòÂåñÔºåF1-scoreÁî®‰∫éÈÄâÂá∫ÊúÄ‰Ω≥ÈòàÂÄº„ÄÇ sensitivityÔºö=recall specificityÔºöÈ¢ÑÊµãÁ±ªÂà´ÁúüÂÆû‰∏∫0ÁöÑÂáÜÁ°ÆÁéá referenceÔºöhttps://zhuanlan.zhihu.com/p/33273532 trade-offÔºö FLOPSÔºöÊØèÁßíÊµÆÁÇπËøêÁÆóÊ¨°Êï∞ÊòØÊØèÁßíÊâÄÊâßË°åÁöÑÊµÆÁÇπËøêÁÆóÊ¨°Êï∞ÁöÑÁÆÄÁß∞ÔºåË¢´Áî®Êù•‰º∞ÁÆóÁîµËÑëÊïàËÉΩ„ÄÇ ROC„ÄÅAUC„ÄÅMAPÔºö ROCÔºöTPRÂíåFPRÂõ¥ÊàêÁöÑÊõ≤Á∫ø AUCÔºöROCÂõ¥‰ΩèÁöÑÈù¢ÁßØ mAPÔºöÊâÄÊúâÁ±ªÂà´APÁöÑÂπ≥ÂùáÂÄº Ê¢ØÂ∫¶Âº•Êï£Ôºö ‚ÄúÂ∫ïÂ±ÇÂÖàÊî∂Êïõ„ÄÅÈ´òÂ±ÇÂÜçÊî∂Êïõ‚ÄùÔºö ÁâπÂæÅÂõæÔºöÂç∑ÁßØÂ±ÇÈÄöËøáÁ∫øÊÄßÊª§Ê≥¢Âô®ËøõË°åÁ∫øÊÄßÂç∑ÁßØËøêÁÆóÔºåÁÑ∂ÂêéÂÜçÊé•‰∏™ÈùûÁ∫øÊÄßÊøÄÊ¥ªÂáΩÊï∞ÔºåÊúÄÁªàÁîüÊàêÁâπÂæÅÂõæ„ÄÇ TTA test time augmentationÔºöÊµãËØïÊó∂Â¢ûÂº∫Ôºå‰∏∫ÂéüÂßãÂõæÂÉèÈÄ†Âá∫Â§ö‰∏™‰∏çÂêåÁâàÊú¨ÔºåÂåÖÊã¨‰∏çÂêåÂå∫ÂüüË£ÅÂâ™ÂíåÊõ¥ÊîπÁº©ÊîæÁ®ãÂ∫¶Á≠âÔºåÂπ∂Â∞ÜÂÆÉ‰ª¨ËæìÂÖ•Âà∞Ê®°Âûã‰∏≠ÔºõÁÑ∂ÂêéÂØπÂ§ö‰∏™ÁâàÊú¨ËøõË°åËÆ°ÁÆóÂæóÂà∞Âπ≥ÂùáËæìÂá∫Ôºå‰Ωú‰∏∫ÂõæÂÉèÁöÑÊúÄÁªàËæìÂá∫ÂàÜÊï∞„ÄÇ pooling mode: full modeÔºö‰ªéfilterÂíåimageÂàöÂºÄÂßãÁõ∏‰∫§ÂºÄÂßãÂç∑ÁßØ same modeÔºöÂΩìfilterÁöÑ‰∏≠ÂøÉÂíåimageÁöÑËßíÈáçÂêàÊó∂ÂºÄÂßãÂç∑ÁßØÔºåÂ¶ÇÊûústride=1ÔºåÈÇ£‰πàËæìÂÖ•ËæìÂá∫Â∞∫ÂØ∏Áõ∏Âêå valid modeÔºöÂΩìfilterÂÆåÂÖ®Âú®imageÈáåÈù¢Êó∂ÂºÄÂßãÂç∑ÁßØ Á©∫Èó¥‰∏çÂèòÊÄßÔºö Âπ≥Áßª‰∏çÂèòÊÄßÔºö‰∏çÁÆ°ËæìÂÖ•Â¶Ç‰ΩïÂπ≥ÁßªÔºåÁ≥ªÁªü‰∫ßÁîüÂÆåÂÖ®Áõ∏ÂêåÁöÑÂìçÂ∫îÔºåÊØîÂ¶ÇÂõæÂÉèÂàÜÁ±ª‰ªªÂä°ÔºåÂõæÂÉè‰∏≠ÁöÑÁõÆÊ†á‰∏çÁÆ°Ë¢´ÁßªÂä®Âà∞ÂõæÁâáÁöÑÂì™‰∏™‰ΩçÁΩÆÔºåÂæóÂà∞ÁöÑÁªìÊûúÔºàÊ†áÁ≠æÔºâÂ∫îËØ•ÊòØÁõ∏ÂêåÁöÑ Âπ≥ÁßªÂêåÂèòÊÄßÔºàtranslation equivarianceÔºâÔºöÁ≥ªÁªüÂú®‰∏çÂêå‰ΩçÁΩÆÁöÑÂ∑•‰ΩúÂéüÁêÜÁõ∏ÂêåÔºå‰ΩÜÂÆÉÁöÑÂìçÂ∫îÈöèÁùÄÁõÆÊ†á‰ΩçÁΩÆÁöÑÂèòÂåñËÄåÂèòÂåñÔºåÊØîÂ¶ÇÂÆû‰æãÂàÜÂâ≤‰ªªÂä°ÔºåÁõÆÊ†áÂ¶ÇÊûúË¢´Âπ≥Áßª‰∫ÜÔºåÈÇ£‰πàËæìÂá∫ÁöÑÂÆû‰æãÊé©Á†Å‰πüÁõ∏Â∫îÂèòÂåñ Â±ÄÈÉ®ËøûÊé•ÔºöÊØè‰∏™Á•ûÁªèÂÖÉÊ≤°ÊúâÂøÖË¶ÅÂØπÂÖ®Â±ÄÂõæÂÉèËøõË°åÊÑüÁü•ÔºåÂè™ÈúÄË¶ÅÂØπÂ±ÄÈÉ®ËøõË°åÊÑüÁü•ÔºåÁÑ∂ÂêéÂú®Êõ¥È´òÂ±ÇÂ∞ÜÂ±ÄÈÉ®ÁöÑ‰ø°ÊÅØÁªºÂêàËµ∑Êù•Â∞±ÂæóÂà∞‰∫ÜÂÖ®Â±ÄÁöÑ‰ø°ÊÅØ ÊùÉÂÄºÂÖ±‰∫´ÔºöÂØπ‰∫éËøô‰∏™ÂõæÂÉè‰∏äÁöÑÊâÄÊúâ‰ΩçÁΩÆÔºåÊàë‰ª¨ÈÉΩËÉΩ‰ΩøÁî®ÂêåÊ†∑ÁöÑÂ≠¶‰π†ÁâπÂæÅ Ê±†ÂåñÔºöÈÄöËøáÊ∂àÈô§ÈùûÊûÅÂ§ßÂÄºÔºåÈôç‰Ωé‰∫Ü‰∏äÂ±ÇÁöÑËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇÊúÄÂ§ßÊ±†ÂåñËøîÂõûÊÑüÂèóÈáé‰∏≠ÁöÑÊúÄÂ§ßÂÄºÔºåÂ¶ÇÊûúÊúÄÂ§ßÂÄºË¢´ÁßªÂä®‰∫ÜÔºå‰ΩÜÊòØ‰ªçÁÑ∂Âú®Ëøô‰∏™ÊÑüÂèóÈáé‰∏≠ÔºåÈÇ£‰πàÊ±†ÂåñÂ±Ç‰πü‰ªçÁÑ∂‰ºöËæìÂá∫Áõ∏ÂêåÁöÑÊúÄÂ§ßÂÄº„ÄÇ Âç∑ÁßØÂíåÊ±†ÂåñËøô‰∏§ÁßçÊìç‰ΩúÂÖ±ÂêåÊèê‰æõ‰∫Ü‰∏Ä‰∫õÂπ≥Áßª‰∏çÂèòÊÄßÔºåÂç≥‰ΩøÂõæÂÉèË¢´Âπ≥ÁßªÔºåÂç∑ÁßØ‰øùËØÅ‰ªçÁÑ∂ËÉΩÊ£ÄÊµãÂà∞ÂÆÉÁöÑÁâπÂæÅÔºåÊ±†ÂåñÂàôÂ∞ΩÂèØËÉΩÂú∞‰øùÊåÅ‰∏ÄËá¥ÁöÑË°®Ëææ„ÄÇ ÂêåÁêÜÔºåÊâÄË∞ìÁöÑCNNÁöÑÂ∞∫Â∫¶„ÄÅÊóãËΩ¨‰∏çÂèòÊÄßÔºå‰πüÊòØÁî±‰∫époolingÊìç‰ΩúÔºåÂºïÂÖ•ÁöÑÂæÆÂ∞èÂΩ¢ÂèòÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ Ê®°ÂûãÂ§ßÂ∞è‰∏éÂèÇÊï∞ÈáèÔºöfloat32ÊòØ4‰∏™Â≠óËäÇÔºåÂõ†Ê≠§Ê®°ÂûãÂ§ßÂ∞èÂ≠óËäÇÊï∞=ÂèÇÊï∞Èáè√ó4 ËÆ≠ÁªÉÊäÄÂ∑ß ËøÅÁßªÂ≠¶‰π†ÔºöÂΩìÊï∞ÊçÆÈõÜÂ§™Â∞èÔºåÊó†Ê≥ïÁî®Êù•ËÆ≠ÁªÉ‰∏Ä‰∏™Ë∂≥Â§üÂ•ΩÁöÑÁ•ûÁªèÁΩëÁªúÔºåÂèØ‰ª•ÈÄâÊã©fine-tune‰∏Ä‰∫õÈ¢ÑËÆ≠ÁªÉÁΩëÁªú„ÄÇ‰ΩøÁî®Êó∂‰øÆÊîπÊúÄÂêéÂá†Â±ÇÔºåÈôç‰ΩéÂ≠¶‰π†Áéá„ÄÇ keras‰∏≠‰∏Ä‰∫õÈ¢ÑËÆ≠ÁªÉÊùÉÈáç‰∏ãËΩΩÂú∞ÂùÄÔºöhttps://github.com/fchollet/deep-learning-models/releases/ K-fold‰∫§ÂèâÈ™åËØÅÔºö Êàë‰ª¨‰∏çËÉΩÂ∞ÜÂÖ®ÈÉ®Êï∞ÊçÆÈõÜÁî®‰∫éËÆ≠ÁªÉ‚Äî‚ÄîËøôÊ†∑Â∞±Ê≤°ÊúâÊï∞ÊçÆÊù•ÊµãËØïÊ®°ÂûãÊÄßËÉΩ‰∫Ü Â∞ÜÊï∞ÊçÆÈõÜÂàÜÂâ≤‰∏∫training set Âíå test setÔºåË°°ÈáèÁªìÊûúÂèñÂÜ≥‰∫éÊï∞ÊçÆÈõÜÂàíÂàÜÔºåtraining setÂíåÂÖ®ÈõÜ‰πãÈó¥Â≠òÂú®biasÔºå‰∏çÂêåtest‰∏ãÁªìÊûúvarietyÂæàÂ§ß ‰∫§ÂèâÈ™åËØÅCross-ValidationÔºö ÊûÅÁ´ØÊÉÖÂÜµLOOCVÔºöÂÖ®ÈõÜNÔºåÊØèÊ¨°Âèñ‰∏Ä‰∏™ÂÅötestÔºåÂÖ∂‰ªñÂÅötrainÔºåÈáçÂ§çNÊ¨°ÔºåÂæóÂà∞N‰∏™Ê®°ÂûãÔºåÂπ∂ËÆ°ÁÆóN‰∏™testÂÅöÂπ≥Âùá K-foldÔºöÂÖ®ÈõÜÂàáÂàÜÊàêk‰ªΩÔºåÊØèÊ¨°Âèñ‰∏Ä‰∏™ÂÅötestÔºåÂÖ∂‰ªñÂÅötrainÔºåÈáçÂ§çkÊ¨°ÔΩû ÂÆûÈ™åÊòæÁ§∫LOOCVÂíå10-foldCVÁöÑÁªìÊûúÂæàÁõ∏ËøëÔºåÂêéËÄÖËÆ°ÁÆóÊàêÊú¨ÊòéÊòæÂáèÂ∞è Bias-Variance Trade-OffÔºöKË∂äÂ§ßÔºåtrain setË∂äÊé•ËøëÂÖ®ÈõÜÔºåbiasË∂äÂ∞èÔºå‰ΩÜÊòØÊØè‰∏™train set‰πãÈó¥Áõ∏ÂÖ≥ÊÄßË∂äÂ§ßÔºåËÄåËøôÁßçÂ§ßÁõ∏ÂÖ≥ÊÄß‰ºöÂØºËá¥ÊúÄÁªàÁöÑtest errorÂÖ∑ÊúâÊõ¥Â§ßÁöÑVariance ÂàÜÂâ≤ ÂÆû‰æãÂàÜÂâ≤&amp;ËØ≠‰πâÂàÜÂâ≤ instance segmentationÔºöÊ†áËÆ∞ÂÆû‰æãÂíåËØ≠‰πâ, ‰∏ç‰ªÖË¶ÅÂàÜÂâ≤Âá∫‰∫∫Ëøô‰∏™Á±ª, ËÄå‰∏îË¶ÅÂàÜÂâ≤Âá∫Ëøô‰∏™‰∫∫ÊòØË∞Å, ‰πüÂ∞±ÊòØÂÖ∑‰ΩìÁöÑÂÆû‰æã semantic segmentationÔºöÂè™Ê†áËÆ∞ËØ≠‰πâ, ‰πüÂ∞±ÊòØËØ¥Âè™ÂàÜÂâ≤Âá∫‰∫∫Ëøô‰∏™Á±ªÊù•]]></content>
      <tags>
        <tag>CV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Segmentation]]></title>
    <url>%2F2019%2F08%2F22%2FSegmentation%2F</url>
    <content type="text"><![CDATA[idea: CTÂõæ‰∏ÄËà¨ÊòØÂçïÈÄöÈÅìÁÅ∞Â∫¶ÂõæÂÉèÔºåÂÅáÂ¶ÇÊàëÂ∞Ü128Âº†CTÂõæÂ†ÜÂè†Âú®‰∏ÄËµ∑ÔºàÂç≥128ÈÄöÈÅìÁöÑÂõæÂÉèÔºâÔºåÁÑ∂ÂêéÁî®2DÂç∑ÁßØÔºà‰ºöËÄÉËôëÈÄöÈÅìÊï∞128ÔºâÔºåËøôÊ†∑ÂíåÁõ¥Êé•Áî®3DÂç∑ÁßØ‰ºöÊúâÁªìÊûú‰∏äÁöÑÂ∑ÆÂà´ÂêóÔºü 3dÁΩëÁªúÂèØ‰ª•ÁªìÂêàÂõæÂÉèÂ±ÇÈó¥‰ø°ÊÅØÔºåËÉΩÂ§ü‰øùËØÅÈöîÂ±ÇÂõæÂÉèMask‰πãÈó¥ÁöÑ‰∏Ä‰∏™ÂèòÂåñËøûÁª≠ÊÄßÔºåÊïàÊûú‰ºöÊØî2dÂ•Ω„ÄÇÂ±ÇÈó¥Ë∑ùÂ§ßÁöÑÂõæÂÉèÔºåÂú®È¢ÑÂ§ÑÁêÜ‰∏≠‰ºöÊúâÊèíÂÄº„ÄÇ 3dÁΩëÁªúÂõ†‰∏∫ÊòæÂ≠òÁöÑÈôêÂà∂Ôºå‰∏ÄÁßçÂ§ÑÁêÜÊñπÂºèÊòØË£ÅÊàê3d patch‰Ωú‰∏∫ËæìÂÖ•ÔºåÂØºËá¥ÂÖ∂ÊÑüÂèóÈáéÊúâÈôêÔºåÈÄöÂ∏∏Âè™ËÉΩ‰∏ìÊ≥®‰∫éÁªÜËäÇÂíåÂ±ÄÈÉ®ÁâπÂæÅÔºåÈÄÇÂêà‰Ωú‰∏∫Á¨¨‰∫åÁ∫ßÁΩëÁªúÁî®‰∫éÂØπÁªÜËäÇÂÅöÁ≤æ‰ºòÂåñ„ÄÇ‰∏ÄÁßçÂ§ÑÁêÜÊñπÂºèÊòØÈôçÈááÊ†∑ÔºåÂàÜÂâ≤Á≤æÂ∫¶‰∏ãÈôç„ÄÇ 2.5DÁΩëÁªú„ÄÇ]]></content>
  </entry>
  <entry>
    <title><![CDATA[keras note]]></title>
    <url>%2F2019%2F08%2F14%2Fkeras-note%2F</url>
    <content type="text"><![CDATA[1. keras LambdaËá™ÂÆö‰πâÂ±ÇÂÆòÊñπÊñáÊ°£ÔºöÂ∞Ü‰ªªÊÑèË°®ËææÂºè(function)Â∞ÅË£Ö‰∏∫ Layer ÂØπË±°„ÄÇ1keras.layers.Lambda(function, output_shape=None, mask=None, arguments=None) function: ÈúÄË¶ÅÂ∞ÅË£ÖÁöÑÂáΩÊï∞„ÄÇ Â∞ÜËæìÂÖ•Âº†Èáè‰Ωú‰∏∫Á¨¨‰∏Ä‰∏™ÂèÇÊï∞„ÄÇ output_shape: È¢ÑÊúüÁöÑÂáΩÊï∞ËæìÂá∫Â∞∫ÂØ∏„ÄÇ(‰ΩøÁî® TensorFlow Êó∂ÔºåÂèØËá™Âä®Êé®ÁêÜÂæóÂà∞) arguments: ÂèØÈÄâÁöÑÈúÄË¶Å‰º†ÈÄíÁªôÂáΩÊï∞ÁöÑÂÖ≥ÈîÆÂ≠óÂèÇÊï∞„ÄÇ‰ª•Â≠óÂÖ∏ÂΩ¢Âºè‰º†ÂÖ•„ÄÇ Âá†‰∏™Ê†óÂ≠êÔºö 1.1 ÊúÄÁÆÄÔºö‰ΩøÁî®ÂåøÂêçÂáΩÊï∞123model.add(Lambda(lambda x: x ** 2))x = Lambda(lambda image: K.image.resize_images(image, (target_size, target_size)))(inpt) ÂÖ∂‰∏≠ÔºålambdaÊòØpythonÁöÑÂåøÂêçÂáΩÊï∞ÔºåÂêéÈù¢ÁöÑ[xx: xxxx]Áî®Êù•ÊèèËø∞ÂáΩÊï∞ÁöÑË°®ËææÂΩ¢ÂºèÔºålambda xx: xxxxÊï¥‰Ωì‰Ωú‰∏∫LambdaÂáΩÊï∞ÁöÑfunctionÂèÇÊï∞„ÄÇ 1.2 ‰∏≠Á∫ßÔºöÈÄöËøáÂ≠óÂÖ∏‰º†ÂèÇÔºåÂ∞ÅË£ÖËá™ÂÆö‰πâÂáΩÊï∞ÔºåÂÆûÁé∞Êï∞ÊçÆÂàáÁâá123456789101112131415from keras.layers import Input, Lambda, Dense, Activation, Reshape, concatenatefrom keras.utils import plot_modelfrom keras.models import Modeldef slice(x, index): return x[:, :, index]a = Input(shape=(4,2))x1 = Lambda(slice,output_shape=(4,1),arguments=&#123;'index':0&#125;)(a)x2 = Lambda(slice,output_shape=(4,1),arguments=&#123;'index':1&#125;)(a)x1 = Reshape((4,1,1))(x1)x2 = Reshape((4,1,1))(x2)output = concatenate([x1,x2])model = Model(a, output)plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True) Ê®°ÂûãÁªìÊûÑÂõæÂ¶Ç‰∏ãÔºö 1.3 È´òÁ∫ßÔºöËá™ÂÆö‰πâÊçüÂ§±ÂáΩÊï∞ step 1. Êääy_trueÂÆö‰πâ‰∏∫‰∏Ä‰∏™ËæìÂÖ• step 2. ÊäälossÂÜôÊàê‰∏Ä‰∏™Â±ÇÔºå‰Ωú‰∏∫ÁΩëÁªúÁöÑÊúÄÁªàËæìÂá∫ step 3. Âú®compileÁöÑÊó∂ÂÄôÔºåÂ∞ÜlossËÆæÁΩÆ‰∏∫y_pred 123456789101112# yolov3 train.py create_model:model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss', arguments=&#123;'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5&#125;)( [*model_body.output, *y_true])model = Model([model_body.input, *y_true], model_loss)model.compile(optimizer=Adam(lr=1e-3), loss=&#123;'yolo_loss': lambda y_true, y_pred: y_pred&#125;)# yolov3 model.py yolo_lossdef yolo_loss(args, anchors, num_classes, ignore_thresh=.5, print_loss=False): ... return loss 2. keras Ëá™ÂÆö‰πâlossË°•ÂÖÖ1.3: ‰πüÂèØ‰ª•‰∏çÂÆö‰πâ‰∏∫ÁΩëÁªúÂ±ÇÔºåÁõ¥Êé•Ë∞ÉÁî®Ëá™ÂÆö‰πâlossÂáΩÊï∞ÂèÇÊï∞Ôºö y_true: ÁúüÂÆûÊ†áÁ≠æÔºåTheano/Tensorflow Âº†Èáè„ÄÇ y_pred: È¢ÑÊµãÂÄº„ÄÇÂíå y_true Áõ∏ÂêåÂ∞∫ÂØ∏ÁöÑ Theano/TensorFlow Âº†Èáè„ÄÇ1234def mycrossentropy(y_true, y_pred, e=0.1): return (1-e)*K.categorical_crossentropy(y_pred,y_true) + e*K.categorical_crossentropy(y_pred, K.ones_like(y_pred)/num_classes)model.compile(optimizer='rmsprop',loss=mycrossentropy, metrics=['accuracy']) Â∏¶ÂèÇÊï∞ÁöÑËá™ÂÆö‰πâlossÔºö ÊúâÊó∂ÂÄôÊàë‰ª¨ËÆ°ÁÆólossÁöÑÊó∂ÂÄô‰∏çÂè™Ë¶ÅÁî®Âà∞y_trueÂíåy_predÔºåËøòÊÉ≥ÂºïÂÖ•‰∏Ä‰∫õÂèÇÊï∞Ôºå‰ΩÜÊòØkerasÈôêÂÆöÊûÑÈÄ†lossÂáΩÊï∞Êó∂Âè™ËÉΩÊé•Êî∂(y_true, y_pred)‰∏§‰∏™ÂèÇÊï∞ÔºåÂ¶Ç‰Ωï‰ºòÈõÖÁöÑ‰º†ÂÖ•ÂèÇÊï∞Ôºü ‰ºòÈõÖÁöÑËß£ÂÜ≥ÊñπÊ°àÂ¶Ç‰∏ãÔºö 1234567891011# build modelmodel = my_model()# define loss funcmodel_loss = dice_loss(smooth=1e-5, thresh=0.5)model.compile(loss=model_loss)# ÂÆûÁé∞def dice_loss(smooth, thresh): def dice(y_true, y_pred): return 1-dice_coef(y_true, y_pred, smooth, thresh) return dice 3. keras Ëá™ÂÆö‰πâmetricsmodel.compileÈáåÈù¢Èô§‰∫ÜlossËøòÊúâ‰∏Ä‰∏™metricsÔºåÁî®‰∫éÊ®°ÂûãÊÄßËÉΩËØÑ‰º∞ÂèÇÊï∞Ôºö y_true: ÁúüÂÆûÊ†áÁ≠æÔºåTheano/Tensorflow Âº†Èáè„ÄÇ y_pred: È¢ÑÊµãÂÄº„ÄÇÂíå y_true Áõ∏ÂêåÂ∞∫ÂØ∏ÁöÑ Theano/TensorFlow Âº†Èáè„ÄÇ12345678910111213141516def precision(y_true, y_pred): # Calculates the precision true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1))) precision = true_positives / (predicted_positives + K.epsilon()) return precisiondef recall(y_true, y_pred): # Calculates the recall true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) possible_positives = K.sum(K.round(K.clip(y_true, 0, 1))) recall = true_positives / (possible_positives + K.epsilon()) return recallmodel.compile(optimizer='rmsprop',loss=mycrossentropy, metrics=['accuracy', recall, precision]) 4. keras Ëá™ÂÆö‰πâLayerÊ∫ê‰ª£Á†ÅÔºöhttps://github.com/keras-team/keras/blob/master/keras/engine/base_layer.py Ëá™ÂÆö‰πâlayerÁªßÊâøkerasÁöÑLayerÁ±ªÔºåÈúÄË¶ÅÂÆûÁé∞‰∏â‰∏™ÊñπÊ≥ïÔºö build(input_shape)ÔºöÂÆö‰πâÊùÉÈáçÔºåË∞ÉÁî®add_weightÊù•ÂàõÂª∫Â±ÇÁöÑÊùÉÈáçÁü©ÈòµÔºåÂÖ∂‰∏≠ÊúâÂèÇÊï∞trainableÂ£∞ÊòéËØ•ÂèÇÊï∞ÁöÑÊùÉÈáçÊòØÂê¶‰∏∫ÂèØËÆ≠ÁªÉÊùÉÈáçÔºåËã•trainable==TrueÔºå‰ºöÊâßË°åself._trainable_weights.append(weight)Â∞ÜËØ•ÊùÉÈáçÂä†ÂÖ•Âà∞ÂèØËÆ≠ÁªÉÊùÉÈáçÁöÑlst‰∏≠„ÄÇ call(x)ÔºöÁºñÂÜôÂ±ÇÈÄªËæë compute_output_shape(input_shape)ÔºöÂÆö‰πâÂº†ÈáèÂΩ¢Áä∂ÁöÑÂèòÂåñÈÄªËæë get_configÔºöËøîÂõû‰∏Ä‰∏™Â≠óÂÖ∏ÔºåËé∑ÂèñÂΩìÂâçÂ±ÇÁöÑÂèÇÊï∞‰ø°ÊÅØ Áúã‰∫Ükeras‰∏Ä‰∫õÂ±ÇÁöÑÂÆûÁé∞Ôºåkeras‰∏≠Â±ÇÔºàÂ¶Çconv„ÄÅdepthwise convÔºâÁöÑcallÂáΩÊï∞Âü∫Êú¨ÈÉΩÊòØÈÄöËøáË∞ÉÁî®tf.backend‰∏≠ÁöÑÊñπÊ≥ïÊù•ÂÆûÁé∞ 4.1 Ê†óÂ≠êÔºöCenterLossLayer 123456789101112131415161718192021222324252627282930class CenterLossLayer(Layer): def __init__(self, alpha=0.5, **kwargs): # alphaÔºöcenter updateÁöÑÂ≠¶‰π†Áéá super().__init__(**kwargs) self.alpha = alpha def build(self, input_shape): # add_weightÔºö‰∏∫ËØ•Â±ÇÂàõÂª∫‰∏Ä‰∏™ÂèØËÆ≠ÁªÉÔºè‰∏çÂèØËÆ≠ÁªÉÁöÑÊùÉÈáç self.centers = self.add_weight(name='centers', shape=(10, 2), initializer='uniform', trainable=False) # ‰∏ÄÂÆöË¶ÅÂú®ÊúÄÂêéË∞ÉÁî®ÂÆÉ super().build(input_shape) def call(self, x, mask=None): # x[0] is Nx2, x[1] is Nx10 onehot, self.centers is 10x2 delta_centers = K.dot(K.transpose(x[1]), (K.dot(x[1], self.centers) - x[0])) # 10x2 center_counts = K.sum(K.transpose(x[1]), axis=1, keepdims=True) + 1 # 10x1 delta_centers /= center_counts new_centers = self.centers - self.alpha * delta_centers # add_updateÔºöÊõ¥Êñ∞Â±ÇÂÜÖÂèÇÊï∞(build‰∏≠ÂÆö‰πâÁöÑÂèÇÊï∞)ÁöÑÊñπÊ≥ï self.add_update((self.centers, new_centers), x) self.result = x[0] - K.dot(x[1], self.centers) self.result = K.sum(self.result ** 2, axis=1, keepdims=True) #/ K.dot(x[1], center_counts) return self.result # Nx1 def compute_output_shape(self, input_shape): return K.int_shape(self.result) Êúâ‰∏Ä‰∫õËá™ÂÆö‰πâÂ±ÇÔºåÊúâÊó∂ÂÄô‰ºö‰∏çÂÆûÁé∞compute_output_shapeÂíåget_config Âú®callÊñπÊ≥ï‰∏≠ÔºåËæìÂá∫tensorÂ¶ÇÊûúÂèëÁîü‰∫ÜshapeÁöÑÂèòÂåñÔºåkeras layerÊòØ‰∏ç‰ºöËá™Âä®Êé®ÂØºÂá∫ËæìÂá∫shapeÁöÑÔºåÊâÄ‰ª•Ë¶ÅÊòæÁ§∫ÁöÑËá™ÂÆö‰πâcompute_output_shape ‰∏çÁÆ°ÂÆö‰∏çÂÆö‰πâget_configÊñπÊ≥ïÔºåÈÉΩÂèØ‰ª•‰ΩøÁî®load_weightsÊñπÊ≥ïÂä†ËΩΩ‰øùÂ≠òÁöÑÊùÉÈáç ‰ΩÜÊòØÂ¶ÇÊûúË¶Å‰ΩøÁî®load_modelÊñπÊ≥ïËΩΩÂÖ•ÂåÖÂê´Ëá™ÂÆö‰πâÂ±ÇÁöÑmodelÔºåÂøÖÈ°ªË¶ÅÊòæÁ§∫Ëá™ÂÆö‰πâget_configÊñπÊ≥ïÔºåÂê¶Âàôkeras Êó†Ê≥ïËé∑Áü• Linear ÁöÑÈÖçÁΩÆÂèÇÊï∞ÔºÅ Âú® __init__ ÁöÑÊúÄÂêéÂä†‰∏ä **kwargs ÂèÇÊï∞ÔºåÂπ∂Áî® **kwargs ÂèÇÊï∞ÂàùÂßãÂåñÁà∂Á±ª„ÄÇ ÂÆûÁé∞‰∏äËø∞ÁöÑ get_config ÊñπÊ≥ïÔºåËøîÂõûËá™ÂÆö‰πâÁöÑÂèÇÊï∞ÈÖçÁΩÆÂíåÈªòËÆ§ÁöÑÂèÇÊï∞ÈÖçÁΩÆ 4.2 Ë°•ÂÖÖ1.3 &amp; 2: Ëá™ÂÆö‰πâÊçüÂ§±ÂáΩÊï∞Èô§‰∫ÜÂèØ‰ª•Áî®LambdaÂ±ÇÔºå‰πüÂèØ‰ª•ÂÆö‰πâLayerÂ±ÇÔºåËøôÊòØ‰∏™Ê≤°ÊúâÊùÉÈáçÁöÑËá™ÂÆö‰πâLayer„ÄÇ 123456789101112131415161718# ÂÆòÊñπÁ§∫‰æãÔºöCustom loss layerclass CustomVariationalLayer(Layer): def __init__(self, **kwargs): self.is_placeholder = True super(CustomVariationalLayer, self).__init__(**kwargs) def vae_loss(self, x, x_decoded_mean): xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)#Square Loss kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)# KL-Divergence Loss return K.mean(xent_loss + kl_loss) def call(self, inputs): x = inputs[0] x_decoded_mean = inputs[1] loss = self.vae_loss(x, x_decoded_mean) self.add_loss(loss, inputs=inputs) # We won't actually use the output. return x 4.3 Ë°•ÂÖÖ callÊñπÊ≥ïÁöÑÂÆåÊï¥ÂèÇÊï∞Ôºöcall(self, inputs, args, *kwargs) ÂÖ∂‰∏≠inputsÂ∞±ÊòØÂ±ÇËæìÂÖ•Ôºåtensor/tensors Èô§Ê≠§‰πãÂ§ñËøòÊúâ‰∏§‰∏™reserved keyword argumentsÔºötraining&amp;maskÔºå‰∏Ä‰∏™Áî®‰∫ébn/dropoutËøôÁßçtrain/testËÆ°ÁÆóÊúâÂå∫Âà´ÁöÑflagÔºå‰∏Ä‰∏™Áî®‰∫éRNNlayersÁ∫¶ÊùüÊó∂Â∫èÁõ∏ÂÖ≥ÂÖ≥Á≥ª argsÂíå*kwargsÊòØÈ¢ÑÁïô‰∏∫‰∫Ü‰ª•ÂêéÊâ©Â±ïÊõ¥Â§öËæìÂÖ•ÂèÇÊï∞ÁöÑ 5. keras GeneratorÊú¨Ë¥®‰∏äÂ∞±ÊòØpythonÁöÑÁîüÊàêÂô®ÔºåÊØèÊ¨°ËøîÂõû‰∏Ä‰∏™batchÁöÑÊ†∑Êú¨ÂèäÊ†áÁ≠æËá™ÂÆö‰πâgeneratorÁöÑÊó∂ÂÄôË¶ÅÂÜôÊàêÊ≠ªÂæ™ÁéØÔºàwhile trueÔºâÔºåÂõ†‰∏∫model.fit_generator()Âú®‰ΩøÁî®Âú®‰∏™ÂáΩÊï∞ÁöÑÊó∂ÂÄôÔºåÂπ∂‰∏ç‰ºöÂú®ÊØè‰∏Ä‰∏™epoch‰πãÂêéÈáçÊñ∞Ë∞ÉÁî®ÔºåÈÇ£‰πàÂ¶ÇÊûúËøôÊó∂ÂÄôgeneratorËá™Â∑±ÁªìÊùü‰∫ÜÂ∞±‰ºöÊúâÈóÆÈ¢ò„ÄÇÊ†óÂ≠êÊòØÊàë‰∏∫mixupÂÜôÁöÑgeneratorÔºöÊ≤°ÊúâÊòæÁ§∫ÁöÑwhile TrueÊòØÂõ†‰∏∫ÂàõÂª∫kerasËá™Â∏¶ÁöÑgeneratorÁöÑÊó∂ÂÄôÂ∑≤ÁªèÊòØÊ≠ªÂæ™ÁéØ‰∫ÜÔºàforÊ∞∏‰∏çË∑≥Âá∫Ôºâ123456789101112131415161718192021222324252627def Datagen_mixup(data_path, img_size, batch_size, is_train=True, mix_prop=0.8, alpha=1.0): if is_train: datagen = ImageDataGenerator() else: datagen = ImageDataGenerator() # using kerasÂ∫ìÂáΩÊï∞ generator = datagen.flow_from_directory(data_path, target_size=(img_size, img_size), batch_size=batch_size, color_mode="grayscale", shuffle=True) for x,y in generator: # a batch of &lt;img, label&gt; if alpha &gt; 0: lam = np.random.beta(alpha, alpha) else: lam = 1 idx = [i for i in range(x.shape[0])] random.shuffle(idx) mixed_x = lam*x + (1-lam)*x[idx] mixed_y = lam*y + (1-lam)*y[idx] n_origin = int(batch_size * mix_prop) gen_x = np.vstack(x[:n_origin], mixed_x[:(batch_size-n_origin)]) gen_y = np.vstack(y[:n_origin], mixed_y[:(batch_size-n_origin)]) yield gen_x, gen_y „ÄêÂ§öËøõÁ®ã„Äëfit_generator‰∏≠Êúâ‰∏Ä‰∏™ÂèÇÊï∞use_multiprocessingÔºåÈªòËÆ§ËÆæÁΩÆ‰∏∫falseÔºåÂõ†‰∏∫‚Äòusing a generator with use_multiprocessing=True and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence‚Äô class‚Äô Â¶ÇÊûúËÆæÁΩÆÂ§öËøõÁ®ãuse_multiprocessingÔºå‰ª£Á†Å‰ºöÊää‰Ω†ÁöÑÊï∞ÊçÆÂ§çÂà∂Âá†‰ªΩÔºåÂàÜÁªô‰∏çÂêåÁöÑworkersËøõË°åËæìÂÖ•ÔºåËøôÊòæÁÑ∂‰∏çÊòØÊàë‰ª¨Â∏åÊúõÁöÑÔºåÊàë‰ª¨Â∏åÊúõ‰∏Ä‰ªΩÊï∞ÊçÆÁõ¥Êé•Âπ≥ÂùáÂàÜÁªôÂ§ö‰∏™workersÂ∏ÆÂøôËæìÂÖ•ÔºåËøôÊ†∑ÊâçÊòØÊúÄÂø´ÁöÑ„ÄÇËÄåSequenceÊï∞ÊçÆÁ±ªËÉΩÂÆåÁæéËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò„ÄÇ keras.utils.Sequence()Ôºö ÊØè‰∏Ä‰∏™ Sequence ÂøÖÈ°ªÂÆûÁé∞ __getitem__ Âíå __len__ ÊñπÊ≥ï __getitem__ ÊñπÊ≥ïÂ∫îËØ•ËåÉÂõ¥‰∏Ä‰∏™ÂÆåÊï¥ÁöÑÊâπÊ¨° Â¶ÇÊûú‰Ω†ÊÉ≥Âú®Ëø≠‰ª£‰πãÈó¥‰øÆÊîπ‰Ω†ÁöÑÊï∞ÊçÆÈõÜÔºå‰Ω†ÂèØ‰ª•ÂÆûÁé∞ on_epoch_endÔºà‰ºöÂú®ÊØè‰∏™Ëø≠‰ª£‰πãÈó¥Ë¢´ÈöêÂºèË∞ÉÁî®Ôºâ\ github‰∏äÊúâissueÂèçÊò†on_epoch_end‰∏ç‰ºöÊ≤°Ë∞ÉÁî®ÔºåËß£ÂÜ≥ÊñπÊ°àÔºöÂú®__len__ÊñπÊ≥ï‰∏≠ÊòæÁ§∫Ëá™Ë°åË∞ÉÁî® Áõ¥Êé•ÁúãÊ†óÂ≠êÔºö 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import kerasimport mathimport osimport cv2import numpy as npfrom keras.applications import ResNet50from keras.optimizers import SGDclass DataGenerator(keras.utils.Sequence): def __init__(self, data, batch_size=1, shuffle=True): self.batch_size = batch_size self.data = data self.indexes = np.arange(len(self.data)) self.shuffle = shuffle def __len__(self): # ËÆ°ÁÆóÊØè‰∏Ä‰∏™epochÁöÑËø≠‰ª£Ê¨°Êï∞ return math.ceil(len(self.data) / float(self.batch_size)) def __getitem__(self, index): # ÁîüÊàêÊØè‰∏™batchÊï∞ÊçÆ batch_indices = self.indexes[index*self.batch_size:(index+1)*self.batch_size] batch_data = [self.data[k] for k in batch_indices] x_batch, y_batch = self.data_generation(batch_data) return x_batch, y_batch def on_epoch_end(self): if self.shuffle == True: np.random.shuffle(self.indexes) def data_generation(self, batch_data): images = [] labels = [] # ÁîüÊàêÊï∞ÊçÆ for i, data in enumerate(batch_data): image = cv2.imread(data, 0) image = cv2.resize(image, dsize=(64,64), interpolation=cv2.INTER_LINEAR) if np.max(image)&gt;1: image = image / 255. image = np.expand_dims(image, axis=-1) images.append(image) if 'd0' in data: labels.append([1,0]) else: labels.append([0,1]) return np.array(images), np.array(labels)if __name__ == '__main__': # data data_dir = "/Users/amber/dataset/mnist" data_lst = [] for file in os.listdir(data_dir+"/d0")[:]: data_lst.append(os.path.join(data_dir, "d0", file)) for file in os.listdir(data_dir+"/d1")[:]: data_lst.append(os.path.join(data_dir, "d1", file)) training_generator = DataGenerator(data_lst, batch_size=128) # model model = ResNet50(input_shape=(64,64,1),weights=None, classes=2) model.compile(optimizer=SGD(1e-3), loss='categorical_crossentropy', metrics=['accuracy']) model.fit_generator(training_generator, epochs=50,max_queue_size=200,workers=2) ÁªèÈ™åÂÄºÔºö workersÔºö2/3 max_queue_sizeÔºöÈªòËÆ§10ÔºåÂÖ∑‰ΩìÂü∫‰∫éGPUÂ§Ñ‰∫éÁ©∫Èó≤Áä∂ÊÄÅÈÄÇÈáèË∞ÉËäÇ „ÄêÈôÑÂä†„ÄëÂÆûÈ™å‰∏≠ËøòÂèëÁé∞‰∏Ä‰∏™ÈóÆÈ¢òÔºåÊúÄÂºÄÂßãÂÆö‰πâ‰∫Ü‰∏Ä‰∏™sequential modelÔºåÁÑ∂ÂêéÂú®Ë∞ÉÁî®fit_generator‰∏ÄÁõ¥Êä•ÈîôÔºömodel not compileÔºå‰ΩÜÊòØÊòæÁÑ∂modelÊòØcompileËøá‰∫ÜÁöÑÔºåÁΩë‰∏äÊü•Âà∞ÁöÑËß£ÈáäÔºö‚ÄòSequential model works with model.fit but not with model.fit_generator‚Äô 6. Â§öGPUÂ§öGPUËøêË°åÂàÜ‰∏∫‰∏§ÁßçÊÉÖÂÜµÔºö * Êï∞ÊçÆÂπ∂Ë°å * ËÆæÂ§áÂπ∂Ë°å 6.1 Êï∞ÊçÆÂπ∂Ë°å Êï∞ÊçÆÂπ∂Ë°åÂ∞ÜÁõÆÊ†áÊ®°ÂûãÂú®Â§ö‰∏™GPU‰∏äÂêÑÂ§çÂà∂‰∏Ä‰ªΩÔºå‰ΩøÁî®ÊØè‰∏™Â§çÂà∂ÂìÅÂ§ÑÁêÜÊï∞ÊçÆÈõÜÁöÑ‰∏çÂêåÈÉ®ÂàÜ„ÄÇ ‰∏Ä‰∏™Ê†óÂ≠êÔºöÂÜôtripleNetÊ®°ÂûãÊó∂ÔºåÂèñ‰∫Übatch=4ÔºåÊÄªÂÖ±15Á±ªÔºåÈÇ£‰πà‰∏âÂÖÉÁªÑÊÄªÂÖ±Êúâ$(4/2)^2*15=60$‰∏™ÔºåËÆ≠ÁªÉÁî®‰∫Ü224ÁöÑÂõæÂÉèÔºåÂçïÂº†GPUÂÜÖÂ≠ò‰ºöÊ∫¢Âá∫ÔºåÂõ†Ê≠§ÈúÄË¶ÅÂçïÊú∫Â§öÂç°Êï∞ÊçÆÂπ∂Ë°å„ÄÇ ‚Äã step1. Âú®Ê®°ÂûãÂÆö‰πâ‰∏≠ÔºåÁî®multi_gpu_modelÂ∞Å‰∏ÄÂ±ÇÔºåÈúÄË¶ÅÂú®model.compile‰πãÂâç„ÄÇ 12345678910111213141516171819202122232425262728from keras.util import multi_gpu_modeldef triple_model(input_shape=(512,512,1), n_classes=10, multi_gpu=False): anchor_input = Input(shape=input_shape) positive_input = Input(shape=input_shape) negative_input = Input(shape=input_shape) sharedCNN = base_model(input_shape) encoded_anchor = sharedCNN(anchor_input) encoded_positive = sharedCNN(positive_input) encoded_negative = sharedCNN(negative_input) # class branch x = Dense(n_classses, activation='softmax')(encoded_anchor) # distance branch encoded_anchor = Activation('sigmoid')(encoded_anchor) encoded_positive = Activation('sigmoid')(encoded_positive) encoded_negative = Activation('sigmoid')(encoded_negative) merged = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=-1, name='tripleLossLayer') model = Model(inputs=[anchor_input,positive_input,negative_input], outputs=[x, merged]) if multi_gpu: model = multi_gpu_model(model, GPU_COUNT) model.compile(optimizer=SGD, loss=[cls_loss, triplet_loss], metrics=[cls_acc]) return model ‚Äã step2. Âú®ÂÆö‰πâcheckpointÊó∂ÔºåË¶ÅÁî®ParallelModelCheckpointÂ∞Å‰∏ÄÂ±ÇÔºåÂàùÂßãÂåñÂèÇÊï∞ÁöÑmodelË¶Å‰º†ÂéüÂßãÊ®°Âûã„ÄÇ 1234567891011121314151617181920212223from keras.callbacks import ModelCheckpointclass ParallelModelCheckpoint(ModelCheckpoint): def __init__(self,single_model,multi_model, filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1): self.single_model = single_model self.multi_model = multi_model super(ParallelModelCheckpoint,self).__init__(filepath, monitor, verbose,save_best_only, save_weights_only,mode, period) def set_model(self, model): self.single_model.optimizer = self.multi_model.optimizer super(ParallelModelCheckpoint,self).set_model(self.single_model) def on_epoch_end(self, epoch, logs=None): # save optimizer weights self.single_model.optimizer = self.multi_model.optimizer super(ParallelModelCheckpoint, self).on_epoch_end(epoch, logs)model = triple_model(multi_gpu=True)single_model = triple_model(multi_gpu=False)filepath = "./tripleNet_&#123;epoch:02d&#125;_val_loss_&#123;val_loss:.3f&#125;.h5"check_point = ParallelModelCheckpoint(single_model, filepath) ‚Äã step3. Âú®‰øùÂ≠òÊùÉÈáçÊó∂ÔºåÈÄöËøácpuÊ®°ÂûãÊù•‰øùÂ≠ò„ÄÇ 1234567# ÂÆû‰æãÂåñÂü∫Á°ÄÊ®°ÂûãÔºåËøôÊ†∑ÂÆö‰πâÊ®°ÂûãÊùÉÈáç‰ºöÂ≠òÂÇ®Âú®CPUÂÜÖÂ≠ò‰∏≠with tf.device('/cpu:0'): model = Resnet50(input_shape=(512,512,3), classes=4, weights=None) parallel_model = multi_gpu_model(model, GPU_COUNT)parallel_model.fit(x,y, epochs=20, batch_size=32)model.save('model.h5') ‚Äã „Äêattention„ÄëÂêåÁêÜÔºåÂú®loadÊùÉÈáçÊó∂Ôºå‰πüÊòØloadÂçïÊ®°ÂûãÁöÑÊùÉÈáçÔºåÂÜçË∞ÉÁî®multi_gpu_modelÂ∞ÜÊ®°ÂûãÂ§çÂà∂Âà∞Â§ö‰∏™GPU‰∏äÔºö 12345model = Model(inputs=[anchor_input,positive_input,negative_input], outputs=[x, merged])if multi_gpu: if os.path.exists(weight_pt): model.load_weights(weight_pt) model = multi_gpu_model(model, GPU_COUNT) „ÄêATTENTION„ÄëÂÆûÈ™å‰∏≠ÂèëÁé∞‰∏Ä‰∏™ÈóÆÈ¢òÔºöÂú®Êúâ‰∫õcase‰∏≠ÔºåÊàë‰ª¨‰ΩøÁî®‰∫ÜËá™ÂÆö‰πâloss‰Ωú‰∏∫ÁΩëÁªúÁöÑËæìÂá∫ÔºåÊ≠§Êó∂ÁΩëÁªúÁöÑËæìÂá∫ÊòØ‰∏™Ê†áÈáèÔºå‰ΩÜÊòØÂú®Ë∞ÉÁî®multi_gpu_modelËøô‰∏™ÊñπÊ≥ïÊó∂ÔºåÂÖ∑‰ΩìÂÆûÁé∞Âú®multi_gpu_utils.py‰∏≠ÔºåÊúÄÂêé‰∏Ä‰∏™Ê≠•È™§Ë¶ÅmergeÂá†‰∏™deviceÁöÑËæìÂá∫ÔºåÈÄöËøáaxis=0ÁöÑconcatÂÆûÁé∞ÔºåÁΩëÁªúËæìÂá∫ÊòØÊ†áÈáèÁöÑËØùÂ∞±‰ºöÊä•Èîô‚Äî‚Äîlist assignment index out of range„ÄÇ Â∞ùËØïÁöÑËß£ÂÜ≥ÊñπÊ°àÊòØÊîπÊàêÁõ∏Âä†Ôºö 1234567# Merge outputs under expected scope.with tf.device('/cpu:0' if cpu_merge else '/gpu:%d' % target_gpu_ids[0]): merged = [] for name, outputs in zip(output_names, all_outputs): merged.append(Lambda(lambda x: K.sum(x))(outputs)) # merged.append(concatenate(outputs, axis=0, name=name)) return Model(model.inputs, merged) „ÄêATTENTION++„ÄëÁΩëÁªúÁöÑËæìÂá∫‰∏çËÉΩÊòØÊ†áÈáèÔºÅÔºÅÊ∞∏Ëøú‰ºöÈöêËóè‰øùÁïô‰∏Ä‰∏™batch dimÔºå‰πãÂâçÊòØÂÜôÈîô‰∫ÜÔºÅÔºÅ model lossÊòØ‰∏Ä‰∏™Ê†áÈáè ‰Ωú‰∏∫ËæìÂá∫Â±ÇÁöÑlossÊòØ‰øùÁïôbatch dimÁöÑÔºÅÔºÅ 6.2 ËÆæÂ§áÂπ∂Ë°å ËÆæÂ§áÂπ∂Ë°åÈÄÇÁî®‰∫éÂ§öÂàÜÊîØÁªìÊûÑÔºå‰∏Ä‰∏™ÂàÜÊîØÁî®‰∏Ä‰∏™GPU„ÄÇÈÄöËøá‰ΩøÁî®TensorFlow device scopesÂÆûÁé∞„ÄÇ Ê†óÂ≠êÔºö 12345678910111213141516# Model where a shared LSTM is used to encode two different sequences in parallelinput_a = keras.Input(shape=(140, 256))input_b = keras.Input(shape=(140, 256))shared_lstm = keras.layers.LSTM(64)# Process the first sequence on one GPUwith tf.device_scope('/gpu:0'): encoded_a = shared_lstm(tweet_a)# Process the next sequence on another GPUwith tf.device_scope('/gpu:1'): encoded_b = shared_lstm(tweet_b)# Concatenate results on CPUwith tf.device_scope('/cpu:0'): merged_vector = keras.layers.concatenate([encoded_a, encoded_b],axis=-1) 7. Â∫ìÂáΩÊï∞ËÆ≤Ëß£7.1 BatchNormalization(axis=-1) Áî®‰∫éÂú®ÊØè‰∏™batch‰∏äÂ∞ÜÂâç‰∏ÄÂ±ÇÁöÑÊøÄÊ¥ªÂÄºÈáçÊñ∞ËßÑËåÉÂåñÔºåÂç≥‰ΩøÂæóÂÖ∂ËæìÂá∫Êï∞ÊçÆÁöÑÂùáÂÄºÊé•Ëøë0ÔºåÂÖ∂Ê†áÂáÜÂ∑ÆÊé•Ëøë1„ÄÇ Â∏∏Áî®ÂèÇÊï∞axisÔºöÊåáÂÆöË¶ÅËßÑËåÉÂåñÁöÑËΩ¥ÔºåÈÄöÂ∏∏‰∏∫ÁâπÂæÅËΩ¥ÔºåÂ¶ÇÂú®‚Äúchannels_first‚ÄùÁöÑdata format‰∏ãÔºåaxis=1ÔºåÂèç‰πãaxis=-1„ÄÇ 7.2 LSTM ÂèÇÊï∞Ôºö unitsÔºöËæìÂá∫Áª¥Â∫¶ÔºàÊúÄÂêé‰∏ÄÁª¥ÔºâÔºåÊ†áÂáÜËæìÂÖ•NxTxDÔºåN for batchÔºåT for time-stepÔºåD for vector-dimension„ÄÇ activationÔºöÊøÄÊ¥ªÂáΩÊï∞ recurrent_activationÔºöÁî®‰∫éÂæ™ÁéØÊó∂Èó¥Ê≠•ÁöÑÊøÄÊ¥ªÂáΩÊï∞ dropoutÔºöÂú® 0 Âíå 1 ‰πãÈó¥ÁöÑÊµÆÁÇπÊï∞„ÄÇ ÂçïÂÖÉÁöÑ‰∏¢ÂºÉÊØî‰æãÔºåÁî®‰∫éËæìÂÖ•ÁöÑÁ∫øÊÄßËΩ¨Êç¢ recurrent_dropoutÔºöÂú® 0 Âíå 1 ‰πãÈó¥ÁöÑÊµÆÁÇπÊï∞„ÄÇ ÂçïÂÖÉÁöÑ‰∏¢ÂºÉÊØî‰æãÔºåÁî®‰∫éÂæ™ÁéØÂ±ÇÁä∂ÊÄÅÁöÑÁ∫øÊÄßËΩ¨Êç¢ return_sequences: Â∏ÉÂ∞îÂÄºÔºåÈªòËÆ§False„ÄÇÊòØËøîÂõûËæìÂá∫Â∫èÂàó‰∏≠ÁöÑÊúÄÂêé‰∏Ä‰∏™ËæìÂá∫ÔºåËøòÊòØÂÖ®ÈÉ®Â∫èÂàóÁöÑËæìÂá∫„ÄÇÂç≥many-to-oneËøòÊòØmany-to-manyÔºåÁÆÄÂçïÊù•ËÆ≤ÔºåÂΩìÊàë‰ª¨ÈúÄË¶ÅÊó∂Â∫èËæìÂá∫Ôºàmany-to-manyÔºâÁöÑÊó∂ÂÄôÔºåÂ∞±set True„ÄÇ return_state: Â∏ÉÂ∞îÂÄºÔºåÈªòËÆ§False„ÄÇÈô§‰∫ÜËæìÂá∫‰πãÂ§ñÊòØÂê¶ËøîÂõûÊúÄÂêé‰∏Ä‰∏™Áä∂ÊÄÅÔºàcellÂÄºÔºâ 1234567891011121314151617181920# return_sequencesinputs1 = Input(tensor=(1Ôºå3, 1))lstm1 = LSTM(1, return_sequences=True)(inputs1)'''ËæìÂá∫ÁªìÊûú‰∏∫[[[-0.02243521][-0.06210149][-0.11457888]]]Ë°®Á§∫ÊØè‰∏™time-stepÔºåLSTM cellÁöÑËæìÂá∫'''# return_statelstm1, state_h, state_c = LSTM(1, return_state=True)(inputs1)'''ËæìÂá∫ÁªìÊûú‰∏∫[array([[ 0.10951342]], dtype=float32), array([[ 0.10951342]], dtype=float32), array([[ 0.24143776]], dtype=float32)] list‰∏≠‰æùÊ¨°‰∏∫ÁΩëÁªúËæìÂá∫ÔºåÊúÄÂêé‰∏Ä‰∏™time-stepÁöÑLSTM cellÁöÑËæìÂá∫ÂÄºÂíåcellÂÄº''' 7.2.5 TimeDistributed È°∫‰æøÂÜçËØ¥‰∏ãTimeDistributedÔºåÂΩìÊàë‰ª¨‰ΩøÁî®many-to-manyÊ®°ÂûãÔºåÊúÄÂêé‰∏ÄÂ±ÇLSTMÁöÑËæìÂá∫Áª¥Â∫¶‰∏∫kÔºåËÄåÊàë‰ª¨ÊÉ≥Ë¶ÅÁöÑÊúÄÁªàËæìÂá∫Áª¥Â∫¶‰∏∫nÔºåÈÇ£‰πàÂ∞±ÈúÄË¶ÅÂºïÂÖ•DenseÂ±ÇÔºåÂØπ‰∫éÊó∂Â∫èÊ®°ÂûãÔºåÊàë‰ª¨Ë¶ÅÂØπÊØè‰∏Ä‰∏™time-stepÂºïÂÖ•denseÂ±ÇÔºåËøôÂÆûË¥®‰∏äÊòØÂ§ö‰∏™DenseÊìç‰ΩúÔºåÈÇ£‰πàÊàë‰ª¨Â∞±ÂèØ‰ª•Áî®TimeDistributedÊù•ÂåÖË£πDenseÂ±ÇÊù•ÂÆûÁé∞„ÄÇ 123model = Sequential()model.add(LSTM(3, input_shape=(length, 1), return_sequences=True))model.add(TimeDistributed(Dense(1))) ÂÆòÊñπÊñáÊ°£ÔºöËøô‰∏™Â∞ÅË£ÖÂô®Â∞Ü‰∏Ä‰∏™Â±ÇÂ∫îÁî®‰∫éËæìÂÖ•ÁöÑÊØè‰∏™Êó∂Èó¥Áâá„ÄÇ ÂΩìËØ•Â±Ç‰Ωú‰∏∫Á¨¨‰∏ÄÂ±ÇÊó∂ÔºåÂ∫îÊòæÂºèËØ¥Êòéinput_shape TimeDistributedÂèØ‰ª•Â∫îÁî®‰∫é‰ªªÊÑèÂ±ÇÔºåÂ¶ÇConv3DÔºö 1234567891011121314151617# ‰æãÂ¶ÇÊàëÁöÑcrnn modeldef crnn(input_shape, cnn, n_classes=24): inpt = Input(input_shape) x = TimeDistributed(cnn, input_shape=input_shape)(inpt) x = LSTM(128, return_sequences=True)(x) x = LSTM(256, return_sequences=True)(x) x = TimeDistributed(Dense(n_classes))(x) model = Model(inpt, x) model.summary() return model crnn_model = crnn((24,128,128,128,2), cnn_model) 7.3 Embedding Áî®‰∫éÂ∞ÜÁ®ÄÁñèÁºñÁ†ÅÊò†Â∞Ñ‰∏∫Âõ∫ÂÆöÂ∞∫ÂØ∏ÁöÑÂØÜÈõÜË°®Á§∫„ÄÇ ËæìÂÖ•ÂΩ¢Â¶ÇÔºàsamplesÔºåsequence_lengthÔºâÁöÑ2DÂº†ÈáèÔºåËæìÂá∫ÂΩ¢Â¶Ç(samples, sequence_length, output_dim)ÁöÑ3DÂº†Èáè„ÄÇ ÂèÇÊï∞Ôºö input_dimÔºöÂ≠óÂÖ∏ÈïøÂ∫¶ÔºåÂç≥ËæìÂÖ•Êï∞ÊçÆÊúÄÂ§ß‰∏ãÊ†á+1 output_dimÔºö input_lengthÔºö Ê†óÂ≠êÔºö 123456789101112# centerloss branchlambda_c = 1input_ = Input(shape=(1,))centers = Embedding(10,2)(input_) # (None, 1, 2)# ËøôÈáåÁöÑËæìÂÖ•ÊòØ0-9ÁöÑÊûö‰∏æÔºàdim=10ÔºâÔºåÁÑ∂ÂêéÊò†Â∞ÑÊàê‰∏Ä‰∏™Á∞áÂøÉintra_loss = Lambda(lambda x:K.sum(K.square(x[0]-x[1][:,0]),1,keepdims=True))([out1,centers])model_center_loss = Model([inputs,input_],[out2,intra_loss])model_center_loss.compile(optimizer="sgd", loss=["categorical_crossentropy",lambda y_true,y_pred:y_pred], loss_weights=[1,lambda_c/2.], metrics=["acc"])model_center_loss.summary() 7.4 plot_model 12from keras.utils import plot_modelplot_model(model, to_file=&apos;model.png&apos;, show_shapes=False, show_layer_names=True) 7.5 K.function Ëé∑ÂèñÊ®°ÂûãÊüêÂ±ÇÁöÑËæìÂá∫Ôºå‰∏ÄÁßçÊñπÊ≥ïÊòØÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑÊ®°ÂûãÔºå‰ΩøÂÆÉÁöÑËæìÂá∫ÊòØÁõÆÊ†áÂ±ÇÔºåÁÑ∂ÂêéË∞ÉÁî®predict„ÄÇ 123456model = ... # the original modelnew_model = Model(input=model.input, output=model.get_layer('my_layer').output)intermediate_output = new_model.predict(input_dataÔºâ ‰πüÂèØ‰ª•ÂàõÂª∫‰∏Ä‰∏™ÂáΩÊï∞Êù•ÂÆûÁé∞Ôºökeras.backend.function(inputs, outputs, updates=None) 123456# ËøôÊòØÂÜôcenter-lossÊó∂ÂÜôÁöÑÊ†óÂ≠êÔºöfunc = K.function(inputs=[model.input[0]], outputs=[model.get_layer('out1').output]) # model.input[0]: one input of the multi-input modeltest_features = func([x_test])[0] 7.6 K.gradients(y,x) Ê±ÇyÂÖ≥‰∫éxÁöÑÂØºÊï∞ÔºåyÂíåxÂèØ‰ª•ÊòØÂº†ÈáèÔºèÂº†ÈáèÂàóË°®„ÄÇËøîÂõûÂº†ÈáèÂàóË°®ÔºåÂàóË°®ÈïøÂ∫¶ÂêåxÂàóË°®ÔºåÂàóË°®‰∏≠ÂÖÉÁ¥†shapeÂêåxÂàóË°®‰∏≠ÂÖÉÁ¥†„ÄÇ ÂØπ‰∫é$y=[y_1, y_2], x=[x_1, x_2, x_3]$ÔºåÊúâËøîÂõûÂÄº$[grad_1, grad_2, grad_3]$ÔºåÁúüÂÆûÁöÑËÆ°ÁÆóËøáÁ®ã‰∏∫Ôºö grad_1 = \frac{\partial y_1}{\partial x_1} + \frac{\partial y_2}{\partial x_1} \\ grad_2 = \frac{\partial y_1}{\partial x_2} + \frac{\partial y_2}{\partial x_2} \\ grad_3 = \frac{\partial y_1}{\partial x_3} + \frac{\partial y_2}{\partial x_3}7.7 ModelCheckpoint„ÄÅReduceLROnPlateau„ÄÅEarlyStopping„ÄÅLearningRateScheduler„ÄÅTensorboard Ê®°ÂûãÊ£ÄÊü•ÁÇπModelCheckpoint 12ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1) filepathÂèØ‰ª•Áî± epoch ÁöÑÂÄºÂíå logs ÁöÑÈîÆÊù•Â°´ÂÖÖÔºåÂ¶Çweights.{epoch:02d}-{val_loss:.2f}.hdf5„ÄÇ moniterÔºöË¢´ÁõëÊµãÁöÑÊï∞ÊçÆ modeÔºöÂú® auto Ê®°Âºè‰∏≠ÔºåÊñπÂêë‰ºöËá™Âä®‰ªéË¢´ÁõëÊµãÁöÑÊï∞ÊçÆÁöÑÂêçÂ≠ó(‰∏çÈù†Ë∞±ü§∑‚Äç‚ôÄÔ∏è)‰∏≠Âà§Êñ≠Âá∫Êù•„ÄÇ Â≠¶‰π†ÁéáË°∞ÂáèReduceLROnPlateau Â≠¶‰π†ÁéáÁöÑÊñπÊ°àÁõ∏ÂØπÁÆÄÂçïÔºåË¶Å‰πàÂú®È™åËØÅÈõÜÁöÑÊçüÂ§±ÊàñÂáÜÁ°ÆÁéáÂºÄÂßãÁ®≥ÂÆöÊó∂Ë∞É‰ΩéÂ≠¶‰π†ÁéáÔºåË¶Å‰πàÂú®Âõ∫ÂÆöÈó¥Èöî‰∏äË∞É‰ΩéÂ≠¶‰π†Áéá„ÄÇ 1ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0) ÂΩìÂ≠¶‰π†ÂÅúÊ≠¢Êó∂ÔºåÊ®°ÂûãÊÄªÊòØ‰ºöÂèóÁõä‰∫éÈôç‰Ωé 2-10 ÂÄçÁöÑÂ≠¶‰π†ÈÄüÁéá„ÄÇ moniterÔºöË¢´ÁõëÊµãÁöÑÊï∞ÊçÆ factorÔºöÊñ∞ÁöÑÂ≠¶‰π†ÈÄüÁéá = Â≠¶‰π†ÈÄüÁéá * factor patienceÔºöË¢´ÁõëÊµãÊï∞ÊçÆÊ≤°ÊúâËøõÊ≠•ÁöÑËÆ≠ÁªÉËΩÆÊï∞ÔºåÂú®Ëøô‰πãÂêéËÆ≠ÁªÉÈÄüÁéá‰ºöË¢´Èôç‰Ωé„ÄÇ Êõ¥Â§çÊùÇÁöÑÂ≠¶‰π†ÁéáÂèòÂåñÊ®°ÂºèÂÆö‰πâLearningRateScheduler ÂâçÊèêÊòØÂè™ÈúÄË¶ÅÁî®Âà∞ÈªòËÆ§ÂèÇÊï∞ÊòØepoch 12345678910111213141516# È¶ñÂÖàÂÆö‰πâ‰∏Ä‰∏™ÂèòÂåñÊ®°Âºèdef warmup_scheduler(epoch, mode='power_decay'): lr_base = 1e-5 lr_stable = 1e-4 lr = lr_base * math.pow(10, epoch) if lr&gt;lr_stable: return lr_stable else: return lr # ÁÑ∂ÂêéË∞ÉÁî®LearningRateSchedulerÊñπÊ≥ïwrapperËøô‰∏™schedulerscheduler = LearningRateScheduler(warmup_scheduler)# Âú®‰ΩøÁî®ÁöÑÊó∂ÂÄôÊîæÂú®callbacksÁöÑlistÈáåÈù¢ÔºåÂú®ÊØè‰∏™epochÁªìÊùüËß¶Âèëcallbacks = [checkpoint, reduce_lr, scheduler, early_stopping] Êõ¥Êõ¥Â§çÊùÇÁöÑÂ≠¶‰π†ÁéáÂèòÂåñÊ®°ÂºèÂÆö‰πâÂèØ‰ª•Áõ¥Êé•ÁªßÊâøCallbackÔºöhttps://kexue.fm/archives/5765 ÂΩìÊàë‰ª¨ÈúÄË¶Å‰º†ÂÖ•Êõ¥‰∏∞ÂØåÁöÑËá™ÂÆö‰πâÂèÇÊï∞/ÈúÄË¶ÅËøõË°åby stepÁöÑÂèÇÊï∞Êõ¥Êñ∞Á≠âÔºåÂèØ‰ª•Áõ¥Êé•ÁªßÊâøCallbackÔºåËøõË°åÊõ¥Ëá™Áî±ÁöÑËá™ÂÆö‰πâ 123456789101112131415161718192021222324252627282930313233# ‰ª•‰ΩôÂº¶ÈÄÄÁÅ´ÁÆóÊ≥ï‰∏∫‰æãÔºöclass CosineAnnealingScheduler(Callback): """Cosine annealing scheduler. """ def __init__(self, epochs, scale=1.6, shift=0, verbose=0): super(CosineAnnealingScheduler, self).__init__() self.epochs = epochs self.scale = scale self.shift = shift self.verbose = verbose def on_epoch_begin(self, epoch, logs=None): if epoch&lt;=6: # linearly increase from 0 to 1.6 in first 5 epochs lr = 1.6 / 5 * (epoch+1) else: # cosine annealing lr = self.shift + self.scale * (1 + math.cos(math.pi * (epoch+1-5) / self.epochs)) / 2 K.set_value(self.model.optimizer.lr, lr) if self.verbose &gt; 0: print('\nEpoch %05d: CosineAnnealingScheduler setting learning rate to %s.' % (epoch+1, lr)) def on_epoch_end(self, epoch, logs=None): logs = logs or &#123;&#125; logs['lr'] = K.get_value(self.model.optimizer.lr) # Ë∞ÉÁî®lrscheduler = CosineAnnealingScheduler(epochs=2, verbose=1)callbacks = [checkpoint, lrscheduler]model.fit(..., callbacks=callbacks) Êúâ‰∏Ä‰∫õËÆ°ÁÆóÊåáÊ†áÔºå‰∏çÂ•ΩÂÜôÊàêÂº†ÈáèÂΩ¢ÂºèÔºå‰πüÂèØ‰ª•ÊîæÂú®CallbackÂô®ÈáåÈù¢ÔºåÊÉ≥ÂíãÂÜôÂ∞±ÂíãÂÜô ËØ¥ÁôΩ‰∫ÜÂ∞±ÊòØon_epoch_endÈáåÈù¢ÁöÑÊï∞ÊçÆÊòØarrayÔºåËÄå‰∏çÊòØtensorÔºåÊØîËæÉÂ•ΩÂÜô 12345678910111213141516171819202122232425262728from keras.callbacks import Callback# ÂÆö‰πâCallbackÂô®ÔºåËÆ°ÁÆóÈ™åËØÅÈõÜÁöÑaccÔºåÂπ∂‰øùÂ≠òÊúÄ‰ºòÊ®°Âûãclass Evaluate(Callback): def __init__(self): self.accs = [] self.highest = 0. def on_epoch_end(self, epoch, logs=None): ###### Ëá™Áî±ÂèëÊå•Âå∫Âüü pred = model.predict(x_test) acc = np.mean(pred.argmax(axis=1) == y_test) ######## self.accs.append(acc) if acc &gt;= self.highest: # ‰øùÂ≠òÊúÄ‰ºòÊ®°ÂûãÊùÉÈáç self.highest = acc model.save_weights('best_model.weights') print('acc: %s, highest: %s' % (acc, self.highest))evaluator = Evaluate()model.fit(x_train, y_train, epochs=10, callbacks=[evaluator]) CallbackÁ±ªÂÖ±ÊîØÊåÅÂÖ≠ÁßçÂú®‰∏çÂêåÈò∂ÊÆµÁöÑÊâßË°åÂáΩÊï∞Ôºö on_epoch_beginÔºöwarmup on_epoch_endÔºömetrics on_batch_begin on_batch_end on_train_begin on_train_end ÊèêÂâçÂÅúÊ≠¢ËÆ≠ÁªÉEarlyStopping 1EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False) moniterÔºöË¢´ÁõëÊµãÁöÑÊï∞ÊçÆ patienceÔºöË¢´ÁõëÊµãÊï∞ÊçÆÊ≤°ÊúâËøõÊ≠•ÁöÑËÆ≠ÁªÉËΩÆÊï∞ÔºåÂú®Ëøô‰πãÂêéËÆ≠ÁªÉÈÄüÁéá‰ºöË¢´Èôç‰Ωé„ÄÇ min_deltaÔºöÂú®Ë¢´ÁõëÊµãÁöÑÊï∞ÊçÆ‰∏≠Ë¢´ËÆ§‰∏∫ÊòØÊèêÂçáÁöÑÊúÄÂ∞èÂèòÂåñÔºåÂ∞è‰∫é min_delta ÁöÑÁªùÂØπÂèòÂåñ‰ºöË¢´ËÆ§‰∏∫Ê≤°ÊúâÊèêÂçá„ÄÇ baseline: Ë¶ÅÁõëÊéßÁöÑÊï∞ÈáèÁöÑÂü∫ÂáÜÂÄº„ÄÇ ‰ª•‰∏äËøôÂõõ‰∏™ÈÉΩÊòØÁªßÊâøËá™keras.callbacks() ÂèØËßÜÂåñÂ∑•ÂÖ∑TensorBoard Ëøô‰∏™ÂõûË∞ÉÂáΩÊï∞‰∏∫ Tensorboard ÁºñÂÜô‰∏Ä‰∏™Êó•ÂøóÔºå ËøôÊ†∑‰Ω†ÂèØ‰ª•ÂèØËßÜÂåñÊµãËØïÂíåËÆ≠ÁªÉÁöÑÊ†áÂáÜËØÑ‰º∞ÁöÑÂä®ÊÄÅÂõæÂÉèÔºå ‰πüÂèØ‰ª•ÂèØËßÜÂåñÊ®°Âûã‰∏≠‰∏çÂêåÂ±ÇÁöÑÊøÄÊ¥ªÂÄºÁõ¥ÊñπÂõæ„ÄÇ 1TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch') ÂÆûÈôÖ‰ΩøÁî®Êó∂ÂÖ≥Ê≥®Á¨¨‰∏Ä‰∏™ÂèÇÊï∞log_dirÂ∞±Â•ΩÔºåÊü•ÁúãÊó∂ÈÄöËøáÂëΩ‰ª§Ë°åÂêØÂä®Ôºö 1tensorboard --logdir=/full_path_to_your_logs ËøôÂá†‰∏™ÂõûË∞ÉÂáΩÊï∞ÔºåÈÄöÈÄöÂú®ËÆ≠ÁªÉÊó∂Ôºàmodel.fit / fit_generatorÔºâÊîæÂú®callbacksÂÖ≥ÈîÆÂ≠óÈáåÈù¢„ÄÇ 7.8 ÂèçÂç∑ÁßØ Conv2DTranspose ‰∏â‰∏™Ê†∏ÂøÉÁöÑÂèÇÊï∞filtes„ÄÅkernel_size„ÄÅstrides„ÄÅpadding=‚Äôvalid‚Äô filtesÔºöËæìÂá∫ÈÄöÈÅìÊï∞ stridesÔºöÊ≠•Èïø kernel_sizeÔºö‰∏ÄËà¨ÈúÄË¶ÅÈÄöËøá‰∏äÈù¢‰∏§È°πËÆ°ÁÆóÂæóÂà∞ ÂèçÂç∑ÁßØËøêÁÆóÂíåÊ≠£ÂêëÂç∑ÁßØËøêÁÆó‰øùÊåÅ‰∏ÄËá¥ÔºåÂç≥Ôºö (output\_shape - kernel\_size) / stride + 1 = input\_shape1234# fcn example: current feature map x (,32,32,32), input_shape (512,512,2), output_shape (,512,512,1)strides = 2kernel_size = input_shape[0] - (x.get_shape().as_list()[1] - 1)*stridesy = Conv2DTranspose(1, kernel_size, padding='valid', strides=strides) 7.9 K.shape &amp; K.int_shape &amp; tensor._keras_shape tensor._keras_shapeÁ≠â‰ª∑‰∫éK.int_shapeÔºöÂº†ÈáèÁöÑshapeÔºåËøîÂõûÂÄºÊòØ‰∏™tuple K.shapeÔºöËøîÂõûÂÄºÊòØ‰∏™tensorÔºåtensorÊòØ‰∏™‰∏ÄÁª¥ÂêëÈáèÔºåÂÖ∂‰∏≠ÊØè‰∏Ä‰∏™ÂÖÉÁ¥†ÂèØ‰ª•Áî®[i]Êù•ËÆøÈóÆÔºåÊòØ‰∏™Ê†áÈáètensor ‰∏§‰∏™ÊñπÊ≥ïÁöÑ‰∏ªË¶ÅÂå∫Âà´ÊòØÔºöÂâçËÄÖËøîÂõûÂÄºÊòØ‰∏™Â∏∏ÈáèÔºåÂè™ËÉΩË°®ÂæÅËØ≠Âè•ÊâßË°åÊó∂ÂàªÔºàÂ¶ÇÊûÑÂª∫ÂõæÔºâtensorÁöÑÁä∂ÊÄÅÔºåÂêéËÄÖËøîÂõûÂÄºÊòØ‰∏™ÂèòÈáèÔºåwrapperÁöÑÊñπÊ≥ïÂèØ‰ª•ÁúãÊàê‰∏Ä‰∏™ËäÇÁÇπÔºåÂú®graphÁöÑ‰ΩúÁî®ÂüüÂÜÖÂßãÁªàÊúâÊïàÔºåÂú®ÊûÑÂª∫ÂõæÁöÑÊó∂ÂÄôÂèØ‰ª•ÊòØNoneÔºåÂú®ÂÆûÈôÖÊµÅÂÖ•Êï∞ÊçÆÊµÅÁöÑÊó∂ÂÄô‰º†ÂÄºÂ∞±Ë°åÔºåÂ¶Çbatch_sizeÔºÅÔºÅÔºÅ 1234567891011import keras.backend as Kfrom keras.layers import Inputx = Input((22,22,1))print(K.shape(x))# Tensor("Shape:0", shape=(4,), dtype=int32)print(K.shape(x)[0])# Tensor("strided_slice:0", shape=(), dtype=int32)print(K.int_shape(x))# (None, 22, 22, 1) 7.10 binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0) from_logitsÔºölogitsË°®Á§∫ÁΩëÁªúÁöÑÁõ¥Êé•ËæìÂá∫‚Äî‚ÄîÊ≤°ÁªèËøásigmoidÊàñËÄÖsoftmaxÁöÑÊ¶ÇÁéáÂåñÔºåÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºåÊàë‰ª¨ËÆ§‰∏∫y_predÊòØÂ∑≤ÁªèÂ§ÑÁêÜËøáÁöÑÊ¶ÇÁéáÂàÜÂ∏É 8. Ë°çÁîüÔºö‰∏Ä‰∫õtfÂáΩÊï∞8.1 tf.where(condition, x=None, y=None,name=None) ‰∏§ÁßçÁî®Ê≥ïÔºö Â¶ÇÊûúxÔºåy‰∏∫Á©∫ÔºåËøîÂõûÂÄºÊòØÊª°Ë∂≥conditionÂÖÉÁ¥†ÁöÑÁ¥¢ÂºïÔºåÊØè‰∏™Á¥¢ÂºïÂç†‰∏ÄË°å„ÄÇ Â¶ÇÊûúxÔºåy‰∏ç‰∏∫Á©∫ÔºåÈÇ£‰πàcondition„ÄÅx„ÄÅy ÂíåËøîÂõûÂÄºÁõ∏ÂêåÁª¥Â∫¶Ôºåcondition‰∏∫TrueÁöÑ‰ΩçÁΩÆÊõøÊç¢x‰∏≠ÂØπÂ∫îÂÖÉÁ¥†Ôºåcondition‰∏∫FalseÁöÑ‰ΩçÁΩÆÊõøÊç¢y‰∏≠ÂØπÂ∫îÂÖÉÁ¥†„ÄÇ ÂÖ≥‰∫éÁ¥¢ÂºïindicesÔºö conditionÁöÑshapeÁöÑdimÔºåÂ∞±ÊòØÊØè‰∏ÄË°åÁ¥¢ÂºïvectorÁöÑshapeÔºå‰æãÔºö 1234567891011import tensorflow as tfimport numpy as npcondition1 = np.array([[True,False,False],[False,True,True]])print(condition1.shape) # (2,3)with tf.compat.v1.Session() as sess: print(sess.run(tf.where(condition1)))# [[0 0]# [1 1]# [1 2]]# conditionÊòØ2x3ÁöÑarrÔºå‰πüÂ∞±ÊòØdim=2ÔºåÈÇ£‰πàÁ¥¢ÂºïvectorÁöÑshapeÂ∞±ÊòØ2ÔºåÁ∫µËΩ¥ÁöÑshapeÊòØÊª°Ë∂≥condÁöÑÊï∞Èáè Á¥¢ÂºïÈÄöÂ∏∏‰∏étf.gatherÂíåtf.gather_ndÊê≠ÈÖç‰ΩøÁî®Ôºö tf.gather(params,indices,axis=0.name=None)Ôºötf.gatherÂè™ËÉΩÊé•Âèó1-DÁöÑÁ¥¢ÂºïÔºåaxisÁî®Êù•ÊåáÂÆöËΩ¥Ôºå‰∏Ä‰∏™Á¥¢ÂºïÂèñÂõûÂØπÂ∫îÁª¥Â∫¶ÁöÑ‰∏Ä‰∏™ÂêëÈáè tf.gather_nd(params,indices)Ôºötf.gather_ndÂèØ‰ª•Êé•ÂèóÂ§öÁª¥ÁöÑÁ¥¢ÂºïÔºåÂ¶ÇÊûúÁ¥¢ÂºïÁöÑdimÂ∞è‰∫éparamsÁöÑdimÔºåÂàô‰ªéaxis=0ÂºÄÂßãÁ¥¢ÂºïÔºåÂêéÈù¢ÁöÑÂèñÂÖ®ÈÉ®„ÄÇ 8.2 tf.Print() Áõ∏ÂΩì‰∫é‰∏Ä‰∏™ËäÇÁÇπÔºåÂÆö‰πâ‰∫ÜÊï∞ÊçÆÁöÑÊµÅÂÖ•ÂíåÊµÅÂá∫„ÄÇ ‰∏Ä‰∏™errorÔºöÂú®Ê®°ÂûãÂÆö‰πâ‰∏≠ÔºåÁõ¥Êé•Ë∞ÉÁî®Ôºö 1234intra_distance = tf.Print(intra_distance, [intra_distance], message='Debug info: ', summarize=10) ‰ºöÊä•ÈîôÔºöAttributeError: ‚ÄòTensor‚Äô object has no attribute ‚Äò_keras_history‚Äô ÂèÇËÄÉÔºöhttps://stackoverflow.com/questions/56096399/creating-model-throws-attributeerror-tensor-object-has-no-attribute-keras You cannot use backend functions directly in Keras tensors, every operation in these tensors must be a layer. You need to wrap each custom operation in a Lambda layer and provide the appropriate inputs to the layer. ‰πãÂâç‰∏ÄÁõ¥Ê≤°Ê≥®ÊÑèÂà∞Ëøô‰∏™ÈóÆÈ¢òÔºåÂá°ÊòØË∞ÉÁî®‰∫Ütf.XXXÁöÑoperationÔºåÈÉΩË¶ÅwrapperÂú®LambdaÂ±ÇÈáå„ÄÇ ÊîπÂÜôÔºö 123456789101112131415# wrapper functiondef debug(args): intra_distance, min_inter_distance = args intra_distance = tf.Print(intra_distance, [intra_distance], message='Debug info: ', summarize=10) min_inter_distance = tf.Print(min_inter_distance, [min_inter_distance], message='Debug info: ', summarize=10) return [intra_distance, min_inter_distance]# Ê®°ÂûãÂÜÖintra_distance, min_inter_distance = Lambda(debug)([intra_distance, min_inter_distance]) „ÄêÂ§πÂ∏¶ÁßÅË¥ß„Äëtf.PrintÂêåÊó∂‰πüÂèØ‰ª•ÊâìÂç∞wrapper functionÂÜÖÁöÑ‰∏≠Èó¥ÂèòÈáèÔºåÈÉΩÊîæÂú®ÂàóË°®ÈáåÈù¢Â∞±ÂèØ‰ª•‰∫Ü„ÄÇ 8.3 tf.while_loop(cond, body, init_value) tensorflow‰∏≠ÂÆûÁé∞Âæ™ÁéØÁöÑËØ≠Âè• ÁªàÊ≠¢Êù°‰ª∂condÔºöÊòØ‰∏Ä‰∏™ÂáΩÊï∞ Âæ™ÁéØ‰ΩìbodyÔºöÊòØ‰∏Ä‰∏™ÂáΩÊï∞ init_valueÔºöÊòØ‰∏Ä‰∏™listÔºå‰øùÂ≠òÂæ™ÁéØÁõ∏ÂÖ≥ÂèÇÊï∞ cond„ÄÅbodyÁöÑÂèÇÊï∞ÊòØË¶Å‰∏éinit_valueÂàóË°®‰∏≠ÂèòÈáè‰∏Ä‰∏ÄÂØπÂ∫îÁöÑ bodyËøîÂõûÂÄºÁöÑÊ†ºÂºèË¶Å‰∏éinit_valueÂèòÈáè‰∏ÄËá¥ÔºàtensorÂΩ¢Áä∂‰øùÊåÅ‰∏çÂèòÔºâ Ëã•ÈùûË¶ÅÂèòÊÄé‰πàÂäûÔºàÊúâÊó∂ÂÄôÊàë‰ª¨Â∏åÊúõÂú®while_loopÁöÑËøáÁ®ã‰∏≠ÔºåÁª¥Êä§‰∏Ä‰∏™listÔºâÔºüÂä®ÊÄÅÊï∞ÁªÑTensorArrayÔºèÈ´òÁ∫ßÂèÇÊï∞shape_invariants 8.3.1 Âä®ÊÄÅÊï∞ÁªÑ 12345# ÂÆö‰πâb_boxes = tf.TensorArray(K.dtype(boxes), size=1, dynamic_size=True, clear_after_read=False)# ÂÜôÂÖ•ÊåáÂÆö‰ΩçÁΩÆb_boxes = b_boxes.write(b, boxes_) ‚Äã tensor arrayÂèòÈáè‰∏≠‰∏Ä‰∏™‰ΩçÁΩÆÂè™ËÉΩÂÜôÂÖ•‰∏ÄÊ¨° 8.3.2 shape_invariants ‚Äã reference stackoverflow 12345678910i = tf.constant(0)l = tf.Variable([])def body(i, l): temp = tf.gather(array,i) l = tf.concat([l, [temp]], 0) return i+1, lindex, list_vals = tf.while_loop(cond, body, [i, l], shape_invariants=[i.get_shape(), tf.TensorShape([None])]) ‚Äã Âú®while_loop‰∏≠ÊòæÁ§∫Âú∞ÊåáÂÆöÂèÇÊï∞ÁöÑshapeÔºå‰∏äÈù¢ÁöÑ‰æãÂ≠êÁî®‰∫Ütf.TensorShape([None])‰ª§ÂÖ∂Ëá™Âä®Êé®Êñ≠ÔºåËÄå‰∏çÊòØÂõ∫ÂÆöÊ£ÄÊü•ÔºåÂõ†Ê≠§ÂèØ‰ª•Ëß£ÂÜ≥ÂèòÂåñÈïøÂ∫¶ÂàóË°®„ÄÇ ‰∏Ä‰∏™ÂÆåÊï¥ÁöÑÊ†óÂ≠êÔºöÁ¨¨‰∏ÄÊ¨°ËßÅwhile_loopÔºåÂú®yolo_lossÈáåÈù¢ Âü∫‰∫ébatchÁª¥Â∫¶ÂÅöÈÅçÂéÜ loopÁªìÊùüÂêéÂ∞ÜÂä®ÊÄÅÊï∞ÊçÆstackËµ∑Êù•ÔºåÈáçËé∑batch dim 12345678910111213# Find ignore mask, iterate over each of batch.# extract the elements on the mask which has iou &lt; ignore_threshignore_mask = tf.TensorArray(K.dtype(y_true[0]), size=1, dynamic_size=True) # Âä®ÊÄÅsizeÊï∞ÁªÑobject_mask_bool = K.cast(object_mask, 'bool')def loop_body(b, ignore_mask): true_box = tf.boolean_mask(y_true[l][b,...,0:4], object_mask_bool[b,...,0]) # (H,W,3,5) iou = box_iou(pred_box[b], true_box) # (H,W,3,1) best_iou = K.max(iou, axis=-1) ignore_mask = ignore_mask.write(b, K.cast(best_iou&lt;ignore_thresh, K.dtype(true_box))) return b+1, ignore_mask_, ignore_mask = K.control_flow_ops.while_loop(lambda b,*args: b&lt;m, loop_body, [0, ignore_mask])ignore_mask = ignore_mask.stack()ignore_mask = K.expand_dims(ignore_mask, -1) # ÔºàN,H,W,3,1Ôºâ 8.4 tf.image.non_max_suppression() ÈùûÊúÄÂ§ßÂÄºÊäëÂà∂ÔºöË¥™Â©™ÁÆóÊ≥ïÔºåÊåâscoresÁî±Â§ßÂà∞Â∞èÊéíÂ∫èÔºåÈÄâÂÆöÁ¨¨‰∏Ä‰∏™Ôºå‰æùÊ¨°ÂØπ‰πãÂêéÁöÑÊ°ÜÊ±ÇiouÔºåÂà†Èô§ÈÇ£‰∫õÂíåÈÄâÂÆöÊ°ÜiouÂ§ß‰∫éÈòàÂÄºÁöÑbox„ÄÇ 12345# ËøîÂõûÊòØË¢´ÈÄâ‰∏≠ËæπÊ°ÜÂú®ÂèÇÊï∞boxes‰∏≠ÁöÑ‰∏ãÊ†á‰ΩçÁΩÆselected_indices=tf.image.non_max_suppression(boxes, scores, max_output_size, iou_threshold=0.5, name=None)# Ê†πÊçÆindicesËé∑ÂèñËæπÊ°Üselected_boxes=tf.gather(boxes,selected_indices) boxesÔºö2-DÁöÑfloatÁ±ªÂûãÁöÑÔºåÂ§ßÂ∞è‰∏∫[num_boxes,4]ÁöÑÂº†Èáè scoresÔºö1-DÁöÑfloatÁ±ªÂûãÁöÑÔºåÂ§ßÂ∞è‰∏∫[num_boxes]ÔºåÂØπÂ∫îÁöÑÊØè‰∏Ä‰∏™boxÁöÑ‰∏Ä‰∏™score max_output_sizeÔºöÊ†áÈáèÊï¥Êï∞TensorÔºåËæìÂá∫Ê°ÜÁöÑÊúÄÂ§ßÊï∞Èáè iou_thresholdÔºöÊµÆÁÇπÊï∞ÔºåIOUÈòàÂÄº selected_indicesÔºö1-DÁöÑÊï¥Êï∞Âº†ÈáèÔºåÂ§ßÂ∞è‰∏∫[M]ÔºåÁïô‰∏ãÊù•ÁöÑËæπÊ°Ü‰∏ãÊ†áÔºåMÂ∞è‰∫éÁ≠â‰∫émax_output_size „ÄêÊãìÂ±ï„ÄëËøòÊúâMulti-class version of NMS‚Äî‚Äîtf.multiclass_non_max_suppression() 8.5 ÈôêÂà∂GPUÁî®Èáè linux‰∏ãÊü•ÁúãGPU‰ΩøÁî®ÊÉÖÂÜµÔºå1ÁßíÂà∑Êñ∞‰∏ÄÊ¨°Ôºö 1watch -n 1 nvidia-smi ÊåáÂÆöÊòæÂç°Âè∑ 12import osos.environ["CUDA_VISIBLE_DEVICES"] = "2" ÈôêÂà∂GPUÁî®Èáè 1234567891011121314import tensorflow as tfimport keras.backend as K# ËÆæÁΩÆÁôæÂàÜÊØîconfig = tf.ConfigProto()config.gpu_options.per_process_gpu_memory_fraction = 0.3session = tf.Session(config=config)K.set_session(session)# ËÆæÁΩÆÂä®ÊÄÅÁî≥ËØ∑config = tf.ConfigProto() config.gpu_options.allow_growth=True #‰∏çÂÖ®ÈÉ®Âç†Êª°ÊòæÂ≠ò, ÊåâÈúÄÂàÜÈÖçsession = tf.Session(config=config)K.set_session(session) 8.6 tf.boolean_mask() tf.boolean_mask(tensor,mask,name=‚Äôboolean_mask‚Äô,axis=None) ÂÖ∂‰∏≠ÔºåtensorÊòØNÁª¥Â∫¶ÁöÑÔºåmaskÊòØKÁª¥Â∫¶ÁöÑÔºå$K \leq N$ axisË°®Á§∫maskÁöÑËµ∑ÂßãÁª¥Â∫¶ÔºåË¢´maskÁöÑÁª¥Â∫¶Âè™‰øùÁïômask‰∏∫TrueÁöÑÊï∞ÊçÆÔºåÂêåÊó∂ËøôÈÉ®ÂàÜÊï∞ÊçÆflattenÊàê‰∏ÄÁª¥ÔºåÊúÄÁªàtensorÁöÑÁª¥Â∫¶ÊòØN-K+1 Ê†óÂ≠êÔºöyolov3ÈáåÈù¢ÔºåÊääÁâπÂæÅÂõæ‰∏äÊúâobjectÁöÑgridÊèêÂèñÂá∫Êù•Ôºö 123# y_trues: [b,h,w,a,4]# conf_gt: [b,h,w,a,1]true_box = tf.boolean_mask(y_trues[i][b,...,0:4], conf_gt[b,...,0]) 9. kerasËá™ÂÆö‰πâ‰ºòÂåñÂô®optimizer9.1 ÂÖ≥‰∫éÊ¢ØÂ∫¶ÁöÑ‰ºòÂåñÂô®ÂÖ¨ÂÖ±ÂèÇÊï∞ÔºåÁî®‰∫éÊ¢ØÂ∫¶Ë£ÅÂâ™ clipnormÔºöÂØπÊâÄÊúâÊ¢ØÂ∫¶ËøõË°ådownscaleÔºå‰ΩøÂæóÊ¢ØÂ∫¶vector‰∏≠l2ËåÉÊï∞ÊúÄÂ§ß‰∏∫1Ôºàg * 1 / max(1, l2_norm)Ôºâ clipvalueÔºöÂØπÁªùÂØπÂÄºËøõË°å‰∏ä‰∏ãÈôêÊà™Êñ≠ 9.2 kerasÁöÑOptimizierÂØπË±° kerasÁöÑÂÆòÊñπ‰ª£Á†ÅÊúâoptimizier_v1Âíåoptimizier_v2‰∏§ÁâàÔºåÂàÜÂà´Èù¢Âêëtf1Âíåtf2Ôºåv1ÁöÑÁúãËµ∑Êù•ÁÆÄÊ¥Å‰∏Ä‰∫õ self.updates &amp; self.weights self.updatesÔºöstores the variables that will be updated with every batch that is processed by the model in training Áî®Êù•‰øùÂ≠ò‰∏éÊ®°ÂûãËÆ≠ÁªÉÁõ∏ÂÖ≥ÁöÑÂèÇÊï∞Ôºàiterations„ÄÅparams„ÄÅmoments„ÄÅaccumulatorsÔºåetcÔºâ symbolic graph variableÔºåÈÄöËøáK.update_addÊñπÊ≥ïËØ¥ÊòéÂõæÁöÑoperation self.weightsÔºöthe functions that save and load optimizers will save and load this property Áî®Êù•‰øùÂ≠ò‰∏é‰ºòÂåñÂô®Áõ∏ÂÖ≥ÁöÑÂèÇÊï∞ model.save()ÊñπÊ≥ï‰∏≠Ê∂âÂèäinclude_optimizer=FalseÔºåÂÜ≥ÂÆö‰ºòÂåñÂô®ÁöÑ‰øùÂ≠òÂíåÈáçËΩΩ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class Optimizer(object): # - ÊäΩË±°Á±ªÔºåÊâÄÊúâÁúüÂÆûÁöÑ‰ºòÂåñÂô®ÁªßÊâøËá™OptimizerÂØπË±° # - Êèê‰æõ‰∏§‰∏™Áî®‰∫éÊ¢ØÂ∫¶Êà™Êñ≠ÁöÑÂÖ¨ÂÖ±ÂèÇÊï∞ def __init__(self, **kwargs): allowed_kwargs = &#123;'clipnorm', 'clipvalue'&#125; for k in kwargs: if k not in allowed_kwargs: raise TypeError('Unexpected keyword argument passed to optimizer: ' + str(k)) # checks that clipnorm &gt;= 0 and clipvalue &gt;= 0 if kwargs[k] &lt; 0: raise ValueError('Expected &#123;&#125; &gt;= 0, received: &#123;&#125;'.format(k, kwargs[k])) self.__dict__.update(kwargs) self.updates = [] # ËÆ°ÁÆóÊõ¥Êñ∞ÁöÑÂèÇÊï∞ self.weights = [] # ‰ºòÂåñÂô®Â∏¶Êù•ÁöÑÊùÉÈáçÔºåÂú®get_updates‰ª•ÂêéÊâçÊúâÂÖÉÁ¥†ÔºåÂú®‰øùÂ≠òÊ®°ÂûãÊó∂‰ºöË¢´‰øùÂ≠ò # Set this to False, indicating `apply_gradients` does not take the # `experimental_aggregate_gradients` argument. _HAS_AGGREGATE_GRAD = False def _create_all_weights(self, params): # Â£∞ÊòéÈô§‰∫Ügrads‰ª•Â§ñÁî®‰∫éÊ¢ØÂ∫¶Êõ¥Êñ∞ÁöÑÂèÇÊï∞ÔºåÂàõÂª∫ÂÜÖÂ≠òÁ©∫Èó¥ÔºåÂú®get_updatesÊñπÊ≥ï‰∏≠‰ΩøÁî® raise NotImplementedError def get_updates(self, loss, params): # ÂÆö‰πâÊ¢ØÂ∫¶Êõ¥Êñ∞ÁöÑËÆ°ÁÆóÊñπÊ≥ï, Êõ¥Êñ∞self.updates raise NotImplementedError def get_config(self): # configÈáåÈù¢ÊòØ‰ºòÂåñÂô®Áõ∏ÂÖ≥ÁöÑÂèÇÊï∞ÔºåÈªòËÆ§Âè™Êúâ‰∏§‰∏™Ê¢ØÂ∫¶Êà™Êñ≠ÁöÑÂèÇÊï∞ÔºåÈúÄË¶ÅÊ†πÊçÆÂÆûÈôÖ‰ºòÂåñÂô®Ê∑ªÂä†Ôºàlr„ÄÅdecay ...Ôºâ config = &#123;&#125; if hasattr(self, 'clipnorm'): config['clipnorm'] = self.clipnorm if hasattr(self, 'clipvalue'): config['clipvalue'] = self.clipvalue return config def get_gradients(self, loss, params): # ËÆ°ÁÆóÊ¢ØÂ∫¶ÂÄºÔºåÂπ∂Âú®ÊúâÂøÖË¶ÅÊó∂ËøõË°åÊ¢ØÂ∫¶Êà™Êñ≠ grads = K.gradients(loss, params) if any(g is None for g in grads): raise ValueError('An operation has `None` for gradient. ' 'Please make sure that all of your ops have a ' 'gradient defined (i.e. are differentiable). ' 'Common ops without gradient: ' 'K.argmax, K.round, K.eval.') if hasattr(self, 'clipnorm'): grads = [tf.clip_by_norm(g, self.clipnorm) for g in grads] if hasattr(self, 'clipvalue'): grads = [ tf.clip_by_value(g, -self.clipvalue, self.clipvalue) for g in grads ] return grads def set_weights(self, weights): # ÁªôoptimizerÁöÑweightsÁî®‰∏ÄÁ≥ªÂàónp arrayËµãÂÄº # Ê≤°ÁúãÂà∞ÊúâË∞ÉÁî®ÔºåÁúÅÁï•codeÔºö K.batch_set_value() def get_weights(self): # Ëé∑ÂèñweightsÁöÑnp arrayÂÄº # Ê≤°ÁúãÂà∞ÊúâË∞ÉÁî®ÔºåÁúÅÁï•codeÔºö K.batch_get_value() @classmethod def from_config(cls, config): return cls(**config) 9.3 ÂÆû‰æãÂåñ‰∏Ä‰∏™‰ºòÂåñÂô® based on keras.OptimizerÂØπË±° ‰∏ªË¶ÅÈúÄË¶ÅÈáçÂÜôget_updatesÂíåget_configÊñπÊ≥ï get_updatesÁî®Êù•ÂÆö‰πâÊ¢ØÂ∫¶Êõ¥Êñ∞ÁöÑËÆ°ÁÆóÊñπÊ≥ï get_configÁî®Êù•ÂÆö‰πâÂÆû‰æãÁî®Âà∞ÁöÑÂèÇÊï∞ ‰ª•SoftSGD‰∏∫‰æãÔºö ÊØèÈöî‰∏ÄÂÆöÁöÑbatchÊâçÊõ¥Êñ∞‰∏ÄÊ¨°ÂèÇÊï∞Ôºå‰∏çÊõ¥Êñ∞Ê¢ØÂ∫¶ÁöÑstepÊ¢ØÂ∫¶‰∏çÊ∏ÖÁ©∫ÔºåÊâßË°åÁ¥ØÂä†Ôºå‰ªéËÄåÂÆûÁé∞batchsizeÁöÑÂèòÁõ∏Êâ©Â§ß Âª∫ËÆÆÊê≠ÈÖçÈó¥ÈöîÊõ¥Êñ∞ÂèÇÊï∞ÁöÑBNÂ±ÇÊù•‰ΩøÁî®ÔºåÂê¶ÂàôBNËøòÊòØÂü∫‰∫éÂ∞èbatchsizeÊù•Êõ¥Êñ∞ÂùáÂÄºÂíåÊñπÂ∑Æ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class SoftSGD(Optimizer): # [new arg] steps_per_update: how many batch to update gradient def __init__(self, lr=0.01, momentum=0., decay=0., nesterov=False, steps_per_update=2, **kwargs): super(SoftSGD, self).__init__(**kwargs) with K.name_scope(self.__class__.__name__): self.iterations = K.variable(0, dtype='int64', name='iterations') self.lr = K.variable(lr, name='lr') self.steps_per_update = steps_per_update # Â§öÂ∞ëbatchÊâçÊõ¥Êñ∞‰∏ÄÊ¨° self.momentum = K.variable(momentum, name='momentum') self.decay = K.variable(decay, name='decay') self.initial_decay = decay self.nesterov = nesterov def get_updates(self, loss, params): # learning rate decay lr = self.lr if self.initial_decay &gt; 0: lr = lr * (1. / (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay)))) shapes = [K.int_shape(p) for p in params] sum_grads = [K.zeros(shape) for shape in shapes] # Âπ≥ÂùáÊ¢ØÂ∫¶ÔºåÁî®Êù•Ê¢ØÂ∫¶‰∏ãÈôç grads = self.get_gradients(loss, params) # ÂΩìÂâçbatchÊ¢ØÂ∫¶ self.updates = [K.update_add(self.iterations, 1)] self.weights = [self.iterations] + sum_grads for p, g, sg in zip(params, grads, sum_grads): # momentum Ê¢ØÂ∫¶‰∏ãÈôç v = self.momentum * sg / float(self.steps_per_update) - lr * g # velocity if self.nesterov: new_p = p + self.momentum * v - lr * sg / float(self.steps_per_update) else: new_p = p + v # Â¶ÇÊûúÊúâÁ∫¶ÊùüÔºåÂØπÂèÇÊï∞Âä†‰∏äÁ∫¶Êùü if getattr(p, 'constraint', None) is not None: new_p = p.constraint(new_p) # Êª°Ë∂≥Êù°‰ª∂ÊâçÊõ¥Êñ∞ÂèÇÊï∞ cond = K.equal(self.iterations % self.steps_per_update, 0) self.updates.append(K.switch(cond, K.update(p, new_p), p)) self.updates.append(K.switch(cond, K.update(sg, g), K.update(sg, sg + g))) return self.updates def get_config(self): config = &#123;'lr': float(K.get_value(self.lr)), 'steps_per_update': self.steps_per_update, 'momentum': float(K.get_value(self.momentum)), 'decay': float(K.get_value(self.decay)), 'nesterov': self.nesterov &#125; base_config = super(SoftSGD, self).get_config() return dict(list(base_config.items()) + list(config.items())) 10. kerasËá™ÂÆö‰πâÊøÄÊ¥ªÂáΩÊï∞activation10.1 ÂÆö‰πâÊøÄÊ¥ªÂáΩÊï∞ 123def gelu(x): cdf = 0.5 * (1.0 + tf.erf(x / tf.sqrt(2.0))) return x*cdf 10.2 ‰ΩøÁî®Ëá™ÂÆö‰πâÊøÄÊ¥ªÂáΩÊï∞ ‰ΩøÁî®ActivationÊñπÊ≥ï 1x = Activation(gelu)(x) ‰∏çËÉΩÊï¥ÂêàËøõÂ∏¶ÊúâactivationÂèÇÊï∞ÁöÑÂ±ÇÔºàÂ¶ÇConv2DÔºâÔºåÂõ†‰∏∫ConvÂü∫Á±ªÁöÑget_config()ÊñπÊ≥ï‰ªékeras.activationsÈáåÈù¢ËØªÂèñÁõ∏Â∫îÁöÑÊøÄÊ¥ªÂáΩÊï∞ÔºåÂÖ∂‰∏≠Â∏¶ÂèÇÊï∞ÁöÑÊøÄÊ¥ªÂáΩÊï∞Â¶ÇPReLUÔºàAdvanced activationsÔºâ„ÄÅ‰ª•ÂèäËá™ÂÆö‰πâÁöÑÊøÄÊ¥ªÂáΩÊï∞ÈÉΩ‰∏çÂú®Ëøô‰∏™Â≠óÂÖ∏‰∏≠ÔºåÂê¶Âàô‰ºöÊä•ÈîôÔºö AttributeError: ‚ÄòActivation‚Äô object has no attribute ‚Äòname‚Äò 10.3 checkpoint issue ÁΩë‰∏äËøòÊúâÂè¶‰∏ÄÁßçÂÜôÊ≥ïÔºö 1234567891011from keras.layers import Activationfrom keras.utils.generic_utils import get_custom_objectsdef gelu(x): cdf = 0.5 * (1.0 + tf.erf(x / tf.sqrt(2.0))) return x*cdf get_custom_objects().update(&#123;'gelu': Activation(gelu)&#125;)# ÂêéÈù¢ÂèØ‰ª•ÈÄöËøáÂêçÂ≠óË∞ÉÁî®ÊøÄÊ¥ªÂáΩÊï∞x = Activation('gelu')(x) ËøôÁßçÂÜôÊ≥ïÂú®‰ΩøÁî®ModelCheckpointsÊñπÊ≥ï‰øùÂ≠òÊùÉÈáçÊó∂‰ºöÊä•ÈîôÔºö AttributeError: ‚ÄòActivation‚Äô object has no attribute ‚Äòname‚Äò ÁúãlogÂèëÁé∞ÂΩì‰ΩøÁî®ÂêçÂ≠ó‰ª£Ë°®ÊøÄÊ¥ªÂ±ÇÁöÑÊó∂ÂÄôÔºåÂú®‰øùÂ≠òÊ®°ÂûãÁöÑÊó∂ÂÄôÔºåÂèà‰ºöÊúâ‰∏Ä‰∏™get_config()ÂáΩÊï∞‰ªékeras.activations‰∏≠Êü•Ë°® 11. kerasËá™ÂÆö‰πâÊ≠£ÂàôÂåñÂô®regularizers11.1 ‰ΩøÁî®Â∞ÅË£ÖÂ•ΩÁöÑregularizers kerasÁöÑÊ≠£ÂàôÂåñÂô®Ê≤°ÊúâglobalÁöÑ‰∏ÄÈîÆÊ∑ªÂä†ÊñπÊ≥ïÔºåË¶Ålayer-wise‰∏∫ÊØè‰∏ÄÂ±ÇÊ∑ªÂä† kerasÁöÑÂ±Çshare 3 commonÂèÇÊï∞Êé•Âè£Ôºö kernel_regularizer bias_regularizer activity_regularizer ÂèØÈÄâÁî®ÁöÑÊ≠£ÂàôÂåñÂô® keras.regularizers.l1(0.01) keras.regularizers.l2(0.01) keras.regularizers.l1_l2(l1=0.01, l2=0.01) ‰ΩøÁî® 12345678layer = tf.keras.layers.Dense(5, kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l1(0.01), activity_regularizer=tf.keras.regularizers.l2(0.01))tensor = tf.ones(shape=(5, 5)) * 2.0out = layer(tensor)# The kernel regularization term is 0.25# The activity regularization term (after dividing by the batch size) is 5print(tf.math.reduce_sum(layer.losses)) # 5.25 (= 5 + 0.25) 11.2 custom regularizer ‰∏ÄËà¨‰∏ç‰ºöËá™ÂÆö‰πâËøô‰∏™‰∏úË•øÔºåÁ°¨Ë¶ÅcustomÁöÑËØùÔºå‰∏§ÁßçÊñπÂºè ÁÆÄÂçïÁâàÔºåÊé•Âè£ÂèÇÊï∞ÊòØweight_matrixÔºåÊó†È¢ùÂ§ñÂèÇÊï∞ÔºåÂ±ÇÁõ¥Êé•Ë∞ÉÁî® 1234def my_regularizer(x): return 1e-3 * tf.reduce_sum(tf.square(x))layer = tf.keras.layers.Dense(5, kernel_initializer='ones', kernel_regularizer=my_regularizer) Â≠êÁ±ªÁªßÊâøÁâàÔºåÂèØ‰ª•Âä†È¢ùÂ§ñÂèÇÊï∞ÔºåÈúÄË¶ÅË°•ÂÖÖget_configÊñπÊ≥ïÔºåÊîØÊåÅËØªÂÜôÊùÉÈáçÊó∂ÁöÑ‰∏≤Ë°åÂåñ 123456789101112class MyRegularizer(regularizers.Regularizer): def __init__(self, strength): self.strength = strength def __call__(self, x): return self.strength * tf.reduce_sum(tf.square(x)) def get_config(self): return &#123;'strength': self.strength&#125;layer = tf.keras.layers.Dense(5, kernel_initializer='ones', kernel_regularizer=MyRegularizer(0.01)) 11.3 Âº∫Ë°åglobal ÊØèÂ±ÇÂä†Ëµ∑Êù•Â§™ÁÉ¶‰∫ÜÔºåÊâπÈáèÂä†ÁöÑÂÆûË¥®‰πüÊòØÈÄêÂ±ÇÂä†ÔºåÂè™‰∏çËøáÂÜôÊàêÂæ™ÁéØ Ê†∏ÂøÉÊòØlayerÁöÑadd_lossÊñπÊ≥ï 12345678model = keras.applications.ResNet50(include_top=True, weights='imagenet')alpha = 0.00002 # weight decay coefficientfor layer in model.layers: if isinstance(layer, keras.layers.Conv2D) or isinstance(layer, keras.layers.Dense): layer.add_loss(lambda: keras.regularizers.l2(alpha)(layer.kernel)) if hasattr(layer, 'bias_regularizer') and layer.use_bias: layer.add_loss(lambda: keras.regularizers.l2(alpha)(layer.bias)) 12. kerasÊü•ÁúãÊ¢ØÂ∫¶&amp;ÊùÉÈáç12.1 easiest way Êü•ÁúãÊ¢ØÂ∫¶ÊúÄÁÆÄÂçïÁöÑÊñπÊ≥ïÔºöÈÄöËøáK.gradientsÊñπÊ≥ïÂÆö‰πâ‰∏Ä‰∏™Ê±ÇÊ¢ØÂ∫¶ÁöÑfuncÔºåÁÑ∂ÂêéÁªôÂÆöËæìÂÖ•ÔºåÂæóÂà∞Ê¢ØÂ∫¶ÔºàCAMÂ∞±ÊòØËøô‰πàÂπ≤ÁöÑÔºâ Êü•ÁúãÊùÉÈáçÊúÄÁÆÄÂçïÁöÑÊñπÊ≥ïÔºöÂ≠òÂú®h5Êñá‰ª∂ÔºåÁÑ∂ÂêéËä±Âºèh5pyËß£Êûê 12.2 dig deeper ‰∏Ä‰∏™ÊÄùË∑ØÔºöÂ∞ÜÊ¢ØÂ∫¶‰øùÂ≠òÂú®optimizerÁöÑself.weights‰∏≠ÔºåÂπ∂Âú®model.saveÂæóÂà∞ÁöÑÊ®°Âûã‰∏≠Ëß£Êûê 13. kerasÂÆûÁé∞ÊùÉÈáçÊªëÂä®Âπ≥Âùá13.1 why EMA on weights [reference1][https://www.jiqizhixin.com/articles/2019-05-07-18]ÔºöÊùÉÈáçÊªëÂä®Âπ≥ÂùáÊòØÊèê‰æõËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÁöÑÊúâÊïàÊñπÊ≥ïÔºåË¶Å‰πàÂú®‰ºòÂåñÂô®ÈáåÈù¢ÂÆûÁé∞ÔºåË¶Å‰πàÂ§ñÂµåÂú®ËÆ≠ÁªÉ‰ª£Á†ÅÈáå [reference2][https://cloud.tencent.com/developer/article/1636781]ÔºöËøôÈáåÈù¢‰∏æÁöÑ‰æãÂ≠êÂæàÊ∏ÖÊô∞‰∫ÜÔºåÂ∞±ÊòØ‰∏∫‰∫ÜÊùÉÈáçÊØè‰∏™stepÂâçÂêéÂèòÂåñ‰∏çÂ§ß ÊùÉÈáçEMAÁöÑËÆ°ÁÆóÊñπÂºèÊúâÁÇπÁ±ª‰ºº‰∫éBNÁöÑrunning mean&amp;varÔºö Âú®ËÆ≠ÁªÉÈò∂ÊÆµÔºöÂÆÉ‰∏çÊîπÂèòÊØè‰∏™training stepÁöÑ‰ºòÂåñÊñπÂêëÔºåËÄåÊòØ‰ªéinitial weightsÂºÄÂßãÔºåÂè¶Â§ñÁª¥Êä§‰∏ÄÁªÑshadow weightsÔºåÁî®ÊØèÊ¨°ÁöÑupdating weightsÊù•ËøõË°åÊªëÂä®Êõ¥Êñ∞ Âú®inferenceÈò∂ÊÆµÔºåÊàë‰ª¨Ë¶ÅÁî®shadow weightsÊù•ÊõøÊç¢ÂΩìÂâçÊùÉÈáçÊñá‰ª∂‰øùÂ≠òÁöÑweightsÔºàcurrent step‰∏ãËÆ°ÁÆóÁöÑÊñ∞ÊùÉÈáçÔºâ Â¶ÇÊûúË¶ÅÁªßÁª≠ËÆ≠ÁªÉÔºåË¶ÅÂ∞ÜÊõøÊç¢ÁöÑÊùÉÈáçÂú®Êç¢ÂõûÊù•ÔºåÂõ†‰∏∫„ÄêEMA‰∏çÂΩ±ÂìçÊ®°ÂûãÁöÑ‰ºòÂåñËΩ®Ëøπ„Äë 13.2 who uses EMA ÂæàÂ§öGANÁöÑËÆ∫ÊñáÈÉΩÁî®‰∫ÜEMAÔºå ËøòÊúâNLPÈòÖËØªÁêÜËß£Ê®°ÂûãQANetÔºå ËøòÊúâGoogleÁöÑefficientNet„ÄÅresnet_rs 13.3 how to implement outside 12345678910111213141516171819202122232425262728293031class ExponentialMovingAverage: """ÂØπÊ®°ÂûãÊùÉÈáçËøõË°åÊåáÊï∞ÊªëÂä®Âπ≥Âùá„ÄÇ Áî®Ê≥ïÔºöÂú®model.compile‰πãÂêé„ÄÅÁ¨¨‰∏ÄÊ¨°ËÆ≠ÁªÉ‰πãÂâç‰ΩøÁî®Ôºõ ÂÖàÂàùÂßãÂåñÂØπË±°ÔºåÁÑ∂ÂêéÊâßË°åinjectÊñπÊ≥ï„ÄÇ """ def __init__(self, model, momentum=0.9999): self.momentum = momentum self.model = model self.ema_weights = [K.zeros(K.shape(w)) for w in model.weights] def inject(self): """Ê∑ªÂä†Êõ¥Êñ∞ÁÆóÂ≠êÂà∞model.metrics_updates„ÄÇ """ self.initialize() for w1, w2 in zip(self.ema_weights, self.model.weights): op = K.moving_average_update(w1, w2, self.momentum) self.model.metrics_updates.append(op) def initialize(self): """ema_weightsÂàùÂßãÂåñË∑üÂéüÊ®°ÂûãÂàùÂßãÂåñ‰∏ÄËá¥„ÄÇ """ self.old_weights = K.batch_get_value(self.model.weights) K.batch_set_value(zip(self.ema_weights, self.old_weights)) def apply_ema_weights(self): """Â§á‰ªΩÂéüÊ®°ÂûãÊùÉÈáçÔºåÁÑ∂ÂêéÂ∞ÜÂπ≥ÂùáÊùÉÈáçÂ∫îÁî®Âà∞Ê®°Âûã‰∏äÂéª„ÄÇ """ self.old_weights = K.batch_get_value(self.model.weights) ema_weights = K.batch_get_value(self.ema_weights) K.batch_set_value(zip(self.model.weights, ema_weights)) def reset_old_weights(self): """ÊÅ¢Â§çÊ®°ÂûãÂà∞ÊóßÊùÉÈáç„ÄÇ """ K.batch_set_value(zip(self.model.weights, self.old_weights)) then train 1234EMAer = ExponentialMovingAverage(model) # Âú®Ê®°Âûãcompile‰πãÂêéÊâßË°åEMAer.inject() # Âú®Ê®°Âûãcompile‰πãÂêéÊâßË°åmodel.fit(x_train, y_train) # ËÆ≠ÁªÉÊ®°Âûã then inference 12345MAer.apply_ema_weights() # Â∞ÜEMAÁöÑÊùÉÈáçÂ∫îÁî®Âà∞Ê®°Âûã‰∏≠model.predict(x_test) # ËøõË°åÈ¢ÑÊµã„ÄÅÈ™åËØÅ„ÄÅ‰øùÂ≠òÁ≠âÊìç‰ΩúEMAer.reset_old_weights() # ÁªßÁª≠ËÆ≠ÁªÉ‰πãÂâçÔºåË¶ÅÊÅ¢Â§çÊ®°ÂûãÊóßÊùÉÈáç„ÄÇËøòÊòØÈÇ£Âè•ËØùÔºåEMA‰∏çÂΩ±ÂìçÊ®°ÂûãÁöÑ‰ºòÂåñËΩ®Ëøπ„ÄÇmodel.fit(x_train, y_train) # ÁªßÁª≠ËÆ≠ÁªÉ 14. kerasÁöÑModelÁ±ªÁªßÊâø14.1 ÂÆö‰πâÊ®°ÂûãÁöÑÊñπÂºè SequentialÔºöÊúÄÁÆÄÂçïÔºå‰ΩÜÊòØ‰∏çËÉΩË°®Á§∫Â§çÊùÇÊãìÊâëÁªìÊûÑ ÂáΩÊï∞Âºè APIÔºöÂíåSequentialÁî®Ê≥ïÂü∫Êú¨‰∏ÄËá¥ÔºåËæìÂÖ•Âº†ÈáèÂíåËæìÂá∫Âº†ÈáèÁî®‰∫éÂÆö‰πâ tf.keras.ModelÂÆû‰æã Ê®°ÂûãÂ≠êÁ±ªÂåñÔºöÂºïÂÖ•‰∫é Keras 2.2.0 kerasÊ∫ê‰ª£Á†ÅÂÆö‰πâÂú®Ôºöhttps://github.com/keras-team/keras/blob/master/keras/engine/training.py 14.2 Ê®°ÂûãÂ≠êÁ±ªÂåñoverview Êó¢ÂèØ‰ª•Áî®Êù•ÂÆö‰πâ‰∏Ä‰∏™modelÔºå‰πüÂèØ‰ª•Áî®Êù•ÂÆö‰πâ‰∏Ä‰∏™Â§çÊùÇÁöÑÁΩëÁªúÂ±ÇÔºå‰∏∫ÂÆûÁé∞Â§çÊùÇÊ®°ÂûãÊèê‰æõÊõ¥Â§ßÁöÑÁÅµÊ¥ªÊÄß ÊúâÁÇπÁ±ª‰ºº‰∫étorchÁöÑËØ≠Ê≥ï ÁΩëÁªúÂ±ÇÂÆö‰πâÂú® __init__(self, ...) ‰∏≠ÔºöË∑ütorchËØ≠Ê≥ïÁöÑ‰∏ªË¶ÅÂå∫Âà´Âú®‰∫éÂ±Ç‰∏çËÉΩÂ§çÁî®ÔºåtorchÂêå‰∏Ä‰∏™Â±ÇÂú®forward‰∏≠ÊØèË∞ÉÁî®‰∏ÄÊ¨°ËÉΩÂ§üÂàõÂª∫‰∏Ä‰∏™ÂÆû‰æãÔºåkerasÊØè‰∏™Â±ÇÂ∫îËØ•ÊòØÂú®init‰∏≠Â£∞ÊòéÂπ∂ÂàõÂª∫ÔºåÊâÄ‰ª•‰∏çËÉΩÂ§çÁî® ÂâçÂêë‰º†Êí≠Âú® call(self, inputs) ‰∏≠ÔºåËøôÈáåÈù¢‰πüÂèØ‰ª•Ê∑ªÂä†loss compute_output_shapeËÆ°ÁÆóÊ®°ÂûãËæìÂá∫ÁöÑÂΩ¢Áä∂ ÂíåkerasËá™ÂÆö‰πâÂ±ÇÁöÑËØ≠Ê≥ï‰πüÂæàÁõ∏‰ºº build(input_shape)Ôºö‰∏ªË¶ÅÂå∫Âà´Â∞±Âú®‰∫ébuildÔºåÂõ†‰∏∫Ëá™ÂÆö‰πâÂ±ÇÊúâbuildÔºåÊòæÂºèÂ£∞Êòé‰∫ÜÊï∞ÊçÆÊµÅÁöÑshapeÔºåËÉΩÂ§üÊûÑÈÄ†Âá∫ÈùôÊÄÅÂõæ call(x)Ôºö compute_output_shape(input_shape)Ôºö „Äê‰ª•‰∏ãÊñπÊ≥ïÂíåÂ±ûÊÄß‰∏çÈÄÇÁî®‰∫éÁ±ªÁªßÊâøÊ®°Âûã„ÄëÔºåÊâÄ‰ª•ËøòÊòØÊé®Ëçê‰ºòÂÖà‰ΩøÁî®ÂáΩÊï∞Âºè API model.inputs &amp; model.outputs model.to_yaml() &amp; model.to_json() model.get_config() &amp; model.save()ÔºöÔºÅÔºÅÔºÅÂè™ËÉΩsave_weightsÔºÅÔºÅÔºÅ 14.3 Ê†óÂ≠ê 12345678910111213141516171819202122232425262728293031import kerasimport numpy as npclass SimpleMLP(keras.Model): def __init__(self, num_classes=10): super(SimpleMLP, self).__init__(name='mlp') self.num_classes = num_classes self.dense1 = keras.layers.Dense(32, activation='relu') self.dense2 = keras.layers.Dense(num_classes, activation='softmax') self.dp = keras.layers.Dropout(0.5) self.bn = keras.layers.BatchNormalization(axis=-1) def call(self, inputs, training=None, mask=None): x = self.dense1(inputs) x = self.dp(x) x = self.bn(x, training=training) return self.dense2(x) def compute_output_shape(self, input_shape): batch, dim = input_shape return (batch, self.num_classes)model = SimpleMLP()model.compile('adam', loss='categorical_crossentropy')x = np.random.uniform(0,1,(32,100))y = np.random.randint(0, 2, (32,10))model.fit(x, y)model.summary() ÂèØ‰ª•ÁúãÂà∞ÔºåÁ±ªÁªßÊâøÊ®°ÂûãÊòØÊ≤°ÊúâÊåáÊòéinput_shapeÁöÑÔºåÊâÄ‰ª•‰πüÂ∞±‰∏çÂ≠òÂú®ÈùôÊÄÅÂõæÔºåË¶ÅÂú®ÊúâÁúüÊ≠£Êï∞ÊçÆÊµÅ‰ª•ÂêéÔºåmodelÊâçË¢´buildÔºåÊâçËÉΩÂ§üË∞ÉÁî®summayÊñπÊ≥ïÔºåÊü•ÁúãÂõæÁªìÊûÑ Á¨¨‰∫å‰∏™ÊòØÔºåcallÊñπÊ≥ïÁöÑÈªòËÆ§ÂèÇÊï∞Ôºödef call(self, inputs, training=None, mask=None)Ôºå Â≠êÁ±ªÁªßÊâøÊ®°Âûã‰∏çÊîØÊåÅÊòæÂºèÁöÑÂ§öËæìÂÖ•ÂÆö‰πâÔºåÊâÄÊúâÁöÑËæìÂÖ•ÊûÑÊàêinputs ÈúÄË¶ÅÊâãÂ∑•ÁÆ°ÁêÜtrainingÂèÇÊï∞Ôºåbn/dropoutÁ≠âÂú®train/inference mode‰∏ãËÆ°ÁÆó‰∏ç‰∏ÄÊ†∑ÁöÑÊÉÖÂÜµÔºåË¶ÅÊòæÂºè‰º†ÂÖ•trainingÂèÇÊï∞ maskÂú®ÊûÑÂª∫AttentionÊú∫Âà∂ÊàñËÄÖÂ∫èÂàóÊ®°ÂûãÊó∂‰ºö‰ΩøÁî®Âà∞ÔºåÂ¶ÇÊûúprevious layerÁîüÊàê‰∫ÜÊé©Á†ÅÔºàembeddingÁöÑmask_zeroÂèÇÊï∞‰∏∫TrueÔºâÔºåÂâç‰∏§ÁßçÊûÑÂª∫Ê®°ÂûãÁöÑÊñπÊ≥ï‰∏≠Ôºåmask‰ºöËá™Âä®‰º†ÂÖ•ÂΩìÂâçÂ±ÇÁöÑcallÊñπÊ≥ï‰∏≠ 15. low-level training &amp; evaluation loops15.1 kerasÁöÑModelÁ±ªÊèê‰æõ‰∫Übuild-inÁöÑtrain/evalÊñπÊ≥ï * fit() * evaluate() * predict() * reference: https://keras.io/api/models/model_training_apis/ * reference: https://keras.io/guides/training_with_built_in_methods/ 15.2 Â¶ÇÊûú‰Ω†ÊÉ≥‰øÆÊîπÊ®°ÂûãÁöÑËÆ≠ÁªÉËøáÁ®ãÔºå‰ΩÜ‰ªçÊóßÈÄöËøáfit()ÊñπÊ≥ïËøõË°åËÆ≠ÁªÉÔºåModelÁ±ª‰∏≠Êèê‰æõ‰∫Ütrain_step()ÂèØ‰ª•ÁªßÊâøÂíåÈáçËΩΩ reference: https://keras.io/guides/customizing_what_happens_in_fit/ ModelÁ±ª‰∏≠Êúâ‰∏Ä‰∏™train_step()ÊñπÊ≥ïÔºåfitÊØè‰∏™batchÁöÑÊó∂ÂÄôÈÉΩ‰ºöË∞ÉÁî®‰∏ÄÊ¨° Âú®ÈáçÂÜôËøô‰∏™train_step()ÊñπÊ≥ïÊó∂ ‰º†ÂÖ•ÂèÇÊï∞dataÔºöÂèñÂÜ≥‰∫éfit()ÊñπÊ≥ï‰º†ÂÖ•ÁöÑÂèÇÊï∞ÂΩ¢ÂºèÔºåtuple(x,y) / tf.data.Dataset forward passÔºöself(model) ËÆ°ÁÆólossÔºöself.compiled_loss ËÆ°ÁÆóÊ¢ØÂ∫¶Ôºötf.GradientTape() Êõ¥Êñ∞ÊùÉÈáçÔºöself.optimizer Êõ¥Êñ∞metricsÔºöself.compiled_metrics ËøîÂõûÂÄºa dictionary mapping metric names Ê†óÂ≠êüå∞ 12345678910111213141516171819202122232425262728293031323334class CustomModel(keras.Model): def train_step(self, data): # Unpack the data x, y = data with tf.GradientTape() as tape: y_pred = self(x, training=True) # Forward pass # Compute the loss value # (the loss function is configured in `compile()`) loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses) # Compute gradients trainable_vars = self.trainable_variables gradients = tape.gradient(loss, trainable_vars) # Update weights self.optimizer.apply_gradients(zip(gradients, trainable_vars)) # Update metrics (includes the metric that tracks the loss) self.compiled_metrics.update_state(y, y_pred) # Return a dict mapping metric names to current value return &#123;m.name: m.result() for m in self.metrics&#125;import numpy as np# Construct and compile an instance of CustomModelinputs = keras.Input(shape=(32,))outputs = keras.layers.Dense(1)(inputs)model = CustomModel(inputs, outputs)model.compile(optimizer="adam", loss="mse", metrics=["mae"])# Just use `fit` as usualx = np.random.random((1000, 32))y = np.random.random((1000, 1))model.fit(x, y, epochs=3) * Ê≥®ÊÑèÂà∞ËøôÈáåÈù¢Êàë‰ª¨Ë∞ÉÁî®‰∫Üself.compiled_lossÂíåself.compiled_metricsÔºåËøôÂ∞±ÊòØÂú®Ë∞ÉÁî®compile()ÊñπÊ≥ïÁöÑÊó∂ÂÄô‰º†ÂÖ•ÁöÑlossÂíåmetricsÂèÇÊï∞ get lower lossÂíåmetrics‰πüÂèØ‰ª•‰∏ç‰º†ÔºåÁõ¥Êé•Âú®CustomModelÈáåÈù¢Â£∞ÊòéÂíåÂÆö‰πâ Â£∞ÊòéÔºöÈáçËΩΩmetrics()ÊñπÊ≥ïÔºåÂàõÂª∫metric instancesÔºåÁî®‰∫éËÆ°ÁÆólossÂíåmetricsÔºåÊää‰ªñ‰ª¨ÊîæÂú®ËøôÈáåÊ®°Âûã‰ºöÂú®fit()/evaluate()ÊñπÊ≥ïÁöÑÊØè‰∏™epochËµ∑ÂßãÈò∂ÊÆµË∞ÉÁî®reset_states()ÊñπÊ≥ïÔºåÁ°Æ‰øùlossÂíåmetricsÁöÑstatesÈÉΩÊòØper epochÁöÑÔºåËÄå‰∏çÊòØavg from the beginning Êõ¥Êñ∞ÔºöË∞ÉÁî®update_state()ÊñπÊ≥ïÊõ¥Êñ∞‰ªñ‰ª¨ÁöÑÁä∂ÊÄÅÂèÇÊï∞ÔºåË∞ÉÁî®result()ÊñπÊ≥ïÊãøÂà∞‰ªñ‰ª¨ÁöÑcurrent value 12345678910111213141516171819202122232425262728293031323334353637383940414243loss_tracker = keras.metrics.Mean(name="loss")mae_metric = keras.metrics.MeanAbsoluteError(name="mae")class CustomModel(keras.Model): def train_step(self, data): x, y = data with tf.GradientTape() as tape: y_pred = self(x, training=True) # Forward pass # Compute our own loss loss = keras.losses.mean_squared_error(y, y_pred) # Compute gradients trainable_vars = self.trainable_variables gradients = tape.gradient(loss, trainable_vars) # Update weights self.optimizer.apply_gradients(zip(gradients, trainable_vars)) # Compute our own metrics loss_tracker.update_state(loss) mae_metric.update_state(y, y_pred) return &#123;"loss": loss_tracker.result(), "mae": mae_metric.result()&#125; @property def metrics(self): # We list our `Metric` objects here so that `reset_states()` can be called automatically per epoch return [loss_tracker, mae_metric]# Construct an instance of CustomModelinputs = keras.Input(shape=(32,))outputs = keras.layers.Dense(1)(inputs)model = CustomModel(inputs, outputs)# We don't passs a loss or metrics here.model.compile(optimizer="adam")# Just use `fit` as usual -- you can use callbacks, etc.x = np.random.random((1000, 32))y = np.random.random((1000, 1))model.fit(x, y, epochs=5) Áõ∏ÂØπÂ∫îÂú∞Ôºå‰πüÂèØ‰ª•ÂÆöÂà∂model.evaluate()ÁöÑËÆ°ÁÆóËøáÁ®ã‚Äî‚Äîoverride test_step()ÊñπÊ≥ï ‰º†ÂÖ•ÂèÇÊï∞dataÔºöÂèñÂÜ≥‰∫éfit()ÊñπÊ≥ï‰º†ÂÖ•ÁöÑÂèÇÊï∞ÂΩ¢ÂºèÔºåtuple(x,y) / tf.data.Dataset forward passÔºöself(model) ËÆ°ÁÆólossÔºöself.compiled_loss ËÆ°ÁÆómetricsÔºöself.compiled_metrics ËøîÂõûÂÄºa dictionary mapping metric names 12345678910111213141516171819202122232425class CustomModel(keras.Model): def test_step(self, data): # Unpack the data x, y = data # Compute predictions y_pred = self(x, training=False) # Updates the metrics tracking the loss self.compiled_loss(y, y_pred, regularization_losses=self.losses) # Update the metrics. self.compiled_metrics.update_state(y, y_pred) # Return a dict mapping metric names to current value. # Note that it will include the loss (tracked in self.metrics). return &#123;m.name: m.result() for m in self.metrics&#125;# Construct an instance of CustomModelinputs = keras.Input(shape=(32,))outputs = keras.layers.Dense(1)(inputs)model = CustomModel(inputs, outputs)model.compile(loss="mse", metrics=["mae"])# Evaluate with our custom test_stepx = np.random.random((1000, 32))y = np.random.random((1000, 1))model.evaluate(x, y) 15.3 ÂÆûÁé∞ÂÆåÊï¥ÁöÑtrain loops reference: https://keras.io/guides/writing_a_training_loop_from_scratch/ a train loop a for loopÔºöiter for each epoch a for loopÔºöiter over the dataset * open a `GradientTape()` scopeÔºötensorflowÁöÑÊ¢ØÂ∫¶APIÔºåÁî®‰∫éÁªôÂÆölossËÆ°ÁÆóÊ¢ØÂ∫¶ * Inside this scopeÔºöforward passÔºåcompute loss * Outside the scopeÔºöretrieve the gradients * use optimizer to update the gradientsÔºö`optimizer.apply_gradients`Ôºå‰ΩøÁî®ËÆ°ÁÆóÂæóÂà∞ÁöÑÊ¢ØÂ∫¶Êù•Êõ¥Êñ∞ÂØπÂ∫îÁöÑvariable Ê†óÂ≠êüå∞ 12345678910111213141516171819202122232425262728293031323334353637383940# modelmodel = keras.Model(inputs=inputs, outputs=outputs)optimizer = keras.optimizers.SGD(learning_rate=1e-3)loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)# datatrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)# iter epochsepochs = 2for epoch in range(epochs): print("\nStart of epoch %d" % (epoch,)) # iter batches for step, (x_batch_train, y_batch_train) in enumerate(train_dataset): # Open a GradientTape to record the operations run # during the forward pass, which enables auto-differentiation. with tf.GradientTape() as tape: # Run the forward pass of the layer. logits = model(x_batch_train, training=True) # Logits for this minibatch # Compute the loss value for this minibatch. loss_value = loss_fn(y_batch_train, logits) # automatically retrieve the gradients of the trainable variables with respect to the loss grads = tape.gradient(loss_value, model.trainable_weights) # Run one step of gradient descent by updating the value of the variables to minimize the loss. optimizer.apply_gradients(zip(grads, model.trainable_weights)) # Log every 200 batches. if step % 200 == 0: print( "Training loss (for one batch) at step %d: %.4f" % (step, float(loss_value)) ) print("Seen so far: %s samples" % ((step + 1) * 64)) 16. kerasËá™ÂÆö‰πâÂàùÂßãÂåñinitializers16.1 Âü∫Á±ªÔºökeras.initializers.Initializer()ÔºåÁî®‰∫éÁªôÂ±ÇÂèÇÊï∞kernel_initializerÂíåbias_initializer‰º†ÂÖ•ÂàùÂßãÂåñÊñπÊ≥ï ÂÜÖÁΩÆÂàùÂßãÂåñÂô®Ôºö keras.initializers.Zeros()ÔºöÂÖ®0 keras.initializers.Ones()ÔºöÂÖ®1 keras.initializers.Constant(value=0)ÔºöÂÖ®ËµãÂÄº‰∏∫ÊåáÂÆöÂ∏∏Èáè keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)ÔºöÊ≠£ÊÄÅÂàÜÂ∏É keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)ÔºöÂùáÂåÄÂàÜÂ∏É keras.initializers.VarianceScaling(scale=1.0, mode=‚Äôfan_in‚Äô, distribution=‚Äônormal‚Äô, seed=None)ÔºöÊ†πÊçÆÂ±ÇÂ±ûÊÄßÂÜ≥ÂÆöÂàÜÂ∏ÉÁöÑÂèÇÊï∞ÔºåÂ¶ÇÊûúdistribution=‚Äônormal‚ÄôÔºåÈÇ£‰πàÊ≠£ÊÄÅÂàÜÂ∏ÉÁöÑstddev=sqrt(scale/n)ÔºåÂ¶ÇÊûúdistribution=‚Äùuniform‚ÄùÔºåÈÇ£‰πàÂùáÂåÄÂàÜÂ∏ÉÁöÑlimit=sqrt(3 * scale/n)ÔºånÂÜ≥ÂÆö‰∫émodeÔºåÂèØ‰ª•ÊòØÂ±ÇËæìÂÖ•ÂçïÂÖÉÊï∞/ËæìÂá∫ÂçïÂÖÉÊï∞/‰∏§ËÄÖÂπ≥Âùá keras.initializers.Orthogonal(gain=1.0, seed=None)ÔºöÈöèÊú∫Ê≠£‰∫§Áü©Èòµ keras.initializers.Identity(gain=1.0)ÔºöÂçï‰ΩçÁü©Èòµ keras.initializers.glorot_normal(seed=None)Ôºö Xavier Ê≠£ÊÄÅÂàÜÂ∏ÉÔºå0‰∏∫‰∏≠ÂøÉÔºåstddev=sqrt(2 / (fan_in + fan_out)) keras.initializers.glorot_uniform(seed=None)Ôºö Xavier ÂùáÂåÄÂàÜÂ∏ÉÔºålimit=sqrt(6 / (fan_in + fan_out)) keras.initializers.he_normal(seed=None)ÔºöKaimingÊ≠£ÊÄÅÂàÜÂ∏ÉÔºå0‰∏∫‰∏≠ÂøÉÔºåstddev=sqrt(2 / fan_in) keras.initializers.he_uniform(seed=None)ÔºöKaimingÂùáÂåÄÂàÜÂ∏ÉÔºålimit=sqrt(6/fan_in) ‰º†ÂÖ•Ôºö 123456789101112131415161718# Áî®ÂêçÂ≠óÂ≠óÁ¨¶‰∏≤model.add(Dense(64,kernel_initializer='random_uniform', bias_initializer='zeros'))# Áî®ÂàùÂßãÂåñÂô®ÁöÑÂÆû‰æãmodel.add(Dense(64,kernel_initializer=keras.initializers.Constant(value=3.0), bias_initializer=keras.initializers.RandomNormal(mean=0.,stddev=.05)))# use config_dictÔºåËøô‰∏™ÊòØÂú®efficientDetÊ∫êÁ†ÅÈáåÈù¢ÁúãÂà∞ÁöÑDENSE_KERNEL_INITIALIZER = &#123; 'class_name': 'VarianceScaling', 'config': &#123; 'scale': 1. / 3., 'mode': 'fan_out', 'distribution': 'uniform' &#125;&#125;x = Dense(n_classes, activation='softmax', kernel_initializer=DENSE_KERNEL_INITIALIZER)(x) 16.2 Ëá™ÂÆö‰πâÂàùÂßãÂåñÂô®Ôºö ÂøÖÈ°ª‰ΩøÁî®ÂèÇÊï∞shapeÂíådtype 123456from keras import backend as Kdef my_init(shape, dtype=None): return K.random_normal(shape, dtype=dtype)model.add(Dense(64, kernel_initializer=my_init))]]></content>
      <tags>
        <tag>keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencvÂ∫ìÂáΩÊï∞]]></title>
    <url>%2F2019%2F08%2F11%2Fopencv%E5%BA%93%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[1. imshowÊúâÊó∂ÂÄôimshowÁöÑÂõæÁâá‰ºöÊòæÁ§∫ÁöÑÂíåÂéüÂõæ‰∏ç‰∏ÄÊ†∑ÔºåË¶ÅÊü•ÁúãreadËøõÊù•ÁöÑÊï∞ÊçÆÊ†ºÂºèÔºåimshow‰ºöÊ†πÊçÆËØªÂÖ•ÁöÑÊï∞ÊçÆÊ†ºÂºèËá™Âä®ËøõË°åÂΩí‰∏ÄÂåñÔºåÊò†Â∞ÑÂà∞0-255„ÄÇ Â¶ÇÊûúimageÊòØÈªòËÆ§ÁöÑ8-bit unsignedÔºà0-255ÔºâÔºå‰∏çÂÅöÂ§ÑÁêÜ„ÄÇ Â¶ÇÊûúimageÊòØ16-bit unsignedÔºà0-65535ÔºâÊàñËÄÖ32-bit integerÔºàÔºüÔºüË¥ºÂ§ßÔºâÔºåÂÉèÁ¥†ÂÄºÈô§‰ª•256Ôºå[0,255*256]ÂΩí‰∏ÄÂåñÂà∞[0Ôºå255]„ÄÇ Â¶ÇÊûúimageÊòØ32-bit floatÔºåÂÉèÁ¥†ÂÄº‰πò‰ª•255Ôºå[0,1]ÂΩí‰∏ÄÂåñÂà∞[0Ôºå255]„ÄÇ 2. imwriteÈÄöÂ∏∏imwriteÊääÊâÄÊúâÊï∞ÊçÆÈÉΩÂº∫Âà∂ËΩ¨Êç¢ÊàêucharÔºà0-255Ôºâ„ÄÇ]]></content>
  </entry>
  <entry>
    <title><![CDATA[beautifulsoup saving file]]></title>
    <url>%2F2019%2F06%2F11%2Fbeautifulsoup-saving-file%2F</url>
    <content type="text"><![CDATA[ÊúÄËøëÁî®bs4Â§ÑÁêÜxmlÊñá‰ª∂ÔºåÈÅáÂà∞‰∫Ü‰∏Ä‰∏™Âú®Áà¨Ëô´Êó∂ÂÄô‰ªéÊú™ÊÄùËÄÉËøáÁöÑÈóÆÈ¢ò‚Äî‚Äî ‰øÆÊ≠£‰ªéxmlÊñá‰ª∂‰∏≠Ëß£ÊûêÂá∫ÁöÑÊñá‰ª∂Ê†ëÔºåÂπ∂Â∞Üchanges‰øùÂ≠òÂà∞ÂéüÊù•ÁöÑxmlÊñá‰ª∂‰∏≠„ÄÇ Êàë‰∏ÄÁõ¥Âú®beautifulsoupÁöÑÊâãÂÜå‰∏≠ÂéªÂØªÊâæÂ∫ìÂáΩÊï∞ÔºåÂÆûÈôÖÂè™ÈúÄË¶ÅÁÆÄÂçïÁöÑÊñá‰ª∂ËØªÂÜôÊìç‰ΩúÔºö 123456789from bs4 import BeautifulSoupsoup = BeautifulSoup(open('test.xml'), 'xml')add = BeautifulSoup("&lt;a&gt;Foo&lt;/a&gt;", 'xml')soup.orderlist.append(add)print(soup.prettify())f = open('test.xml', 'w')f.write(str(soup))f.close() ÈôÑ‰∏Ä‰∏™ÁÆÄÂçïxmlÊñá‰ª∂Áî®Êù•ÂÆûÈ™åÔºö 1234567891011121314&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;orderlist&gt;&lt;order&gt;&lt;customer&gt;ÂßìÂêç1&lt;/customer&gt;&lt;phone&gt;ÁîµËØù1&lt;/phone&gt;&lt;address&gt;Âú∞ÂùÄ1&lt;/address&gt;&lt;count&gt;ÁÇπÈ§êÊ¨°Êï∞1&lt;/count&gt;&lt;/order&gt;&lt;order&gt;&lt;customer&gt;ÂßìÂêç2&lt;/customer&gt;&lt;phone&gt;ÁîµËØù2&lt;/phone&gt;&lt;address&gt;Âú∞ÂùÄ2&lt;/address&gt;&lt;count&gt;ÁÇπÈ§êÊ¨°Êï∞2&lt;/count&gt;&lt;/order&gt;]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh visualization]]></title>
    <url>%2F2018%2F12%2F29%2Fssh-visualization%2F</url>
    <content type="text"><![CDATA[ssh boocax@192.168.1.100 ÂØÜÁ†ÅÔºörobot123 echo $ROS_MASTER_URI Êü•ÁúãÁ´ØÂè£Âè∑11311 Â∞èËΩ¶Á´ØÔºö export ROS_MASTER_URI=http://192.168.1.100:11311 export ROS_IP=192.168.1.100 ËôöÊãüÊú∫Á´ØÔºö export ROS_MASTER_URI=http://192.168.1.100:11311 export ROS_IP=172.16.128.142 # navËøúÁ®ãÂºÄÂêØ‰∏â‰∏™ÁªàÁ´ØÔºà‰ª£Á†ÅÈáçÊûÑ‰ª•ÂâçÔºâÔºö move_base: roslaunch teleop_twist_joy real_nav.launch mapserver: rosrun map_server map_server catkin_ws2/src/patrol/map/p1.yaml amcl: roslaunch patrol real_loc.launch Êú¨Âú∞ÂèØËßÜÂåñÔºörvizÔºèrqt_graph / rosservice call /rostopic pub ÂÖ®Â±ÄÂÆö‰ΩçÔºö 1rosservice call /global_localization "&#123;&#125;" ËÆæÁΩÆÂØºËà™ÁõÆÊ†áÁÇπÔºö 12345// Áõ∏ÂØπbase_linkÂùêÊ†áÁ≥ªrostopic pub -1 /navigation_simple/goal geometry_msgs/PoseStamped '&#123; header: &#123; frame_id: "base_link" &#125;, pose: &#123; position: &#123; x: 0.5, y: 0.0, z: 0 &#125;, orientation: &#123; x: 0, y: 0, z: 0, w: 1 &#125; &#125; &#125;'// Áõ∏ÂØπmapÂùêÊ†áÁ≥ªrostopic pub -1 /navigation_simple/goal geometry_msgs/PoseStamped '&#123; header: &#123; frame_id: "map" &#125;, pose: &#123; position: &#123; x: 5, y: 0.0, z: 0 &#125;, orientation: &#123; x: 0, y: 0, z: 0, w: 1 &#125; &#125; &#125;' Ê≥®ÊÑè-1ÔºåÂê¶ÂàôÂæ™ÁéØÂèëÂ∏É„ÄÇ # ÂæÄÂõûÂ§á‰ªΩÔºö 1scp -r boocax@192.168.1.100:/home/boocax/catkin_ws2019 bkp/]]></content>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Occupancy Grid Map]]></title>
    <url>%2F2018%2F12%2F04%2FOccupancy-Grid-Map%2F</url>
    <content type="text"><![CDATA[to be completed‚Ä¶ Inverse Sensor Model Incremental Updating]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[object tracking]]></title>
    <url>%2F2018%2F12%2F03%2Fobject-tracking%2F</url>
    <content type="text"><![CDATA[ËåÉÂõ¥ÈôêÂÆöÔºöËøáÊª§ÊéâËæÉËøúËåÉÂõ¥ÁöÑÁÇπ‰∫ëÊï∞ÊçÆ ËÅöÁ±ªÔºöK-MeansÔºèÊ¨ßÂºèËÅöÁ±ªÔºåÂõ†‰∏∫ÂâçËÄÖÈúÄË¶ÅËÆæÂÆöKÔºåÊïÖ‰ΩøÁî®ÂêéËÄÖ„ÄÇ D(p_i, p_{i+1}) = \sqrt{r_i^2 + r_{i+1}^2 - 2r_ir_{i+1}cos(\varphi_{i+1} - \varphi_i)}Â¶ÇÊûúËøûÁª≠Êâ´ÊèèÁÇπ‰πãÈó¥ÁöÑË∑ùÁ¶ªÂ∞è‰∫é‰∏Ä‰∏™ÈòàÂÄº$D_t$ÔºåÈÇ£‰πàËøô‰∏§‰∏™ÁÇπË¢´ËÆ§‰∏∫Â±û‰∫éÂêå‰∏Ä‰∏™ÂØπË±°„ÄÇËøô‰∏™ÈòàÂÄºÊòØÊ†πÊçÆÂΩìÂâçÂèÇËÄÉÁÇπÁöÑË∑ùÁ¶ªÂä®ÊÄÅË∞ÉÊï¥ÁöÑ„ÄÇ D_t = D_0 + a*r_i*sin(\Delta \varphi) ËøêÂä®ÁõÆÊ†áÁâπÂæÅÊèêÂèñÔºöÔºà‰∏≠ÂøÉÂùêÊ†áÔºåÈïøÔºèÂÆΩÔºèÂçäÂæÑÔºåÂèçÂ∞ÑÂº∫Â∫¶Ôºâ Áî±‰∏ä‰∏ÄÊó∂ÂàªÁöÑ‰ΩçÁΩÆÈÄüÂ∫¶ËÆæÁΩÆROIÔºö Âü∫‰∫éÂ±ÄÈÉ®ÂåπÈÖçÔºöÈÄöËøáÁõ∏‰ººÂ∫¶ËÆ°ÁÆóÈÄâÂèñÂìçÂ∫îÂÄºÊúÄÈ´òÁöÑÁõÆÊ†á Âü∫‰∫éÂàÜÁ±ªÂô®ÔºöÂä®ÊÄÅÁõÆÊ†áÂ∑≤Áü•Ôºà‰∫∫ËÖøÔºâÔºåÈááÈõÜÊ≠£Ë¥üÊ†∑Êú¨ÔºåÊûÑÈÄ†ÂàÜÁ±ªÂô®Ôºå Âç°Â∞îÊõºÊª§Ê≥¢Ôºö]]></content>
      <tags>
        <tag>extensions for slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlibÁöÑcolormap]]></title>
    <url>%2F2018%2F11%2F29%2Fmatplotlib%E7%9A%84colormap%2F</url>
    <content type="text"><![CDATA[Áî®pltÁöÑimshowÁîªÂõæÔºåÊÄªÊòØÊâæ‰∏çÂà∞ÂøÉ‰ª™ÁöÑcolorbarÔºåÂèØ‰ª•Ëá™ÂÆö‰πâÔºö Âú®ÂéüÊúâcmapÂü∫Á°Ä‰∏äËá™ÂÆö‰πâÔºö 12345colorbar = plt.get_cmap('Greys')(range(180))cm = LinearSegmentedColormap.from_list(name="grey_cm", colors=colorbar)plt.register_cmap(cmap=cm)plt.imshow(map2d.data, cmap='grey_cm') define‰∏Ä‰∏™Êñ∞ÁöÑcmapÔºö 123456def colormap(): colors = ['#FFFFFF', '#9ff113', '#5fbb44', '#f5f329', '#e50b32'] return colors.ListedColormap(colors, 'my_cmap')my_cmap = colormap()plt.imshow(map2d.data, cmap=my_cmap)]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mpv-video-cutter]]></title>
    <url>%2F2018%2F11%2F26%2Fmpv-video-cutter%2F</url>
    <content type="text"><![CDATA[mpvÁöÑÂ∞èÊèí‰ª∂ÔºåËÉΩÂ§ü‰∏ÄÈîÆÔºà‰∏âÈîÆÔºâÂâ™Ëæë„ÄÇ Â∑•Á®ãÂú∞ÂùÄÔºöhttps://github.com/rushmj/mpv-video-cutter step1ÔºöÊääc_concat.shÂíåcutter.lua‰∏§‰∏™Êñá‰ª∂Â§çÂà∂Âà∞~/.config/mpv/scripts/ÁõÆÂΩï‰∏ã„ÄÇ step2ÔºöÁªôc_concat.shËÑöÊú¨Ê∑ªÂä†ÊâßË°åÊùÉÈôê„ÄÇ step3ÔºöÁî®ÂëΩ‰ª§Ë°åÊâìÂºÄÊñá‰ª∂Ôºåc-c-oÂú®ÂéüÁõÆÂΩï‰∏ãÁîüÊàêÂâ™ËæëÊñá‰ª∂„ÄÇ]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[problems with ROS]]></title>
    <url>%2F2018%2F11%2F22%2Fproblems-with-ROS%2F</url>
    <content type="text"><![CDATA[[WARN] Detected jump back in time of 5.51266s. Clearing TF buffer. ÊâãÂä®Âª∫ÂõæÁöÑÊó∂ÂÄôÔºåÊó∂‰∏çÊó∂ÁöÑÂ∞±Ë∑≥Âá∫Êù•Ëøô‰∏™ÔºåÁÑ∂ÂêéÂ∞èËΩ¶Ë∑≥ÂèòÂà∞ÂàùÂßã‰ΩçÁΩÆÔºåËÄå‰∏îËøòÊòØÊ†πÊçÆTF bufferÂõûÊ∫ØÂõûÂéªÁöÑÔºåÁúüÈ´òÁ∫ß„ÄÇ„ÄÇ„ÄÇ ÊéíÊü•ÂéüÂõ†ÂèëÁé∞Á´üÁÑ∂ÊòØÂøòËÆ∞ËøêË°åroscore‰∫ÜÔºåmmp„ÄÇ [rosrun] Couldn&#39;t find executable named patrol.py below /home/carrol/catkin_ws/src/patrol ÂéüÂõ†Â¶ÇÊèêÁ§∫ÔºåpythonÊòØËÑöÊú¨ÊâßË°åÔºåË¶ÅÊ∑ªÂä†ÂèØÊâßË°åÊùÉÈôê„ÄÇ error: ‚Äòarray‚Äô is not a member of ‚Äòstd‚Äô ÁºñËØëÂØºËà™ÂåÖÊó∂ÂèçÂ§çÂá∫Áé∞Ëøô‰∏™ÈîôËØØÔºåÂõ†‰∏∫cmakeÁâàÊú¨ÊØîËæÉ‰ΩéÔºà2.8ÔºâÔºå‰∏ç‰ºöËá™Âä®Êâæc++11ÔºåËß£ÂÜ≥ÂäûÊ≥ïÂú®ÂØπÂ∫îpackageÁöÑcmakeÊñá‰ª∂‰∏≠Ê∑ªÂä†c++Â£∞ÊòéÔºöadd_definitions(-std=c++11) ÂêåÊ†∑ÁöÑÈîôËØØcatkin_makeÊó∂ÈáçÂ§çÂá∫Áé∞ÔºåÊàëËøò‰ª•‰∏∫ÈóÆÈ¢òÊ≤°Ëß£ÂÜ≥Ôºö Âà†Èô§buildÊñá‰ª∂Â§π‰∏≠ÂØπÂ∫îÂåÖÔºåÂÜçËøõË°åcatkin_make„ÄÇÂ¶ÇÊûúÂà†Èô§‰∫ÜÊüê‰∏™ÂåÖÔºåËøòË¶ÅÂà†Èô§develÊñá‰ª∂Â§πÂÜçÁºñËØë„ÄÇ cmake warning conflicts with AnacondaÔºö ÁºñËØëÂà∞ÊúÄÂêé‰ºöÂç°Ê≠ªÔºåÈîôËØØÂÖ∑‰ΩìÂï•ÊÑèÊÄùÊàë‰πüÊ≤°ÂºÑÊòéÁôΩÔºåÁ≤óÊö¥Ëß£ÂÜ≥‰∫ÜÔºåÂ∞ÜÁ≥ªÁªüÁéØÂ¢ÉÂèòÈáèÈáåÈù¢ÁöÑanaconda pathÊöÇÊó∂Â±èËîΩÔºåÈ¶ñÂÖàÊü•ÁúãÁéØÂ¢ÉÂèòÈáèÔºöecho $PATHÔºåÁÑ∂ÂêéËøîÂõûÁªìÊûúÔºö /home/[username]/anaconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games ÁÑ∂ÂêéÂú®ÂΩìÂâçÂëΩ‰ª§Ë°åÊâßË°åÔºöexport PATH=&quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games&quot; c++: internal compiler error: Killed (program cc1plus) ËôöÊãüÊú∫ÂÜÖÂ≠ò‰∏çË∂≥„ÄÇ undefined error with CONSOLE_BRIDGE_logError/CONSOLE_BRIDGE_logWarn ÂÆâË£ÖÂπ∂ÁºñËØëconsole_bridgeÂåÖÔºåÊ≥®ÊÑèbuild instructionsÔºö 12345git clone git://github.com/ros/console_bridge.gitcd console_bridgecmake .makesudo make install there are no arguments to ‚ÄòlogDebug‚Äô that depend on a template parameter, so a declaration of ‚ÄòlogDebug‚Äô must be available [-fpermissive] ÂèÇËÄÉÔºàReferenceÔºâÔºåËøòÊòØ‰∏äÈù¢ÁöÑÈóÆÈ¢òÔºå console_bridgeÁöÑAPIÂèò‰∫ÜÔºåÂ∞ÜlogDebugÊîπÊàêCONSOLE_BRIDGE_logDebugÂ∞±Ë°å‰∫Ü„ÄÇ running environmentÁõ∏ÂÖ≥ÂåÖÁöÑÁº∫Â§±ÂíåÂÆâË£ÖÔºö Âú®ÂÆòÁΩëÊü•ÊâæÁõ∏ÂÖ≥ÂåÖÂíå‰æùËµñÔºåÁÑ∂ÂêéÊâßË°åÔºö 12345# installsudo dpkg -i ËΩØ‰ª∂ÂåÖÂêç.deb# uninstallsudo apt-get remove ËΩØ‰ª∂ÂåÖÂêçÁß∞]]></content>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[amcl]]></title>
    <url>%2F2018%2F11%2F16%2Famcl%2F</url>
    <content type="text"><![CDATA[Êª§Ê≥¢ÔºöÊú∫Âô®‰∫∫‰ªéÂ∑≤Áü•ÁÇπ$x_0$ÂºÄÂßãËøêÂä®ÔºåÈáåÁ®ãËÆ°ËØØÂ∑ÆÈÄêÊ∏êÁ¥ØÁßØÔºå‰ΩçÁΩÆ‰∏çÁ°ÆÂÆöÊÄßÂ∞ÜË∂äÊù•Ë∂äÂ§ßÔºà$x_1, x_2$Ôºâ„ÄÇÂõ†Ê≠§ÈúÄË¶ÅÂÄüÂä©Â§ñÈÉ®ÁéØÂ¢É‰ø°ÊÅØÂØπËá™Â∑±ËøõË°åÂÆö‰ΩçÔºå‰∫éÊòØÂºïÂÖ•ÊµãÈáèÂÄº$d$ÔºåËÆ°ÁÆóÂá∫ÂΩìÂâç‰ΩçÁΩÆ$x_2^{‚Äò}$ÔºåÂÜçÁªìÂêàÈ¢ÑÊµãÂÄº$x_2$ÔºåÂæóÂà∞‰∏Ä‰∏™Áü´Ê≠£‰ΩçÁΩÆ$x_2^{‚Äò‚Äô}$Ôºå‰ΩøÂÖ∂‰∏çÁ°ÆÂÆöÊÄßÈôçÂà∞ÊúÄÂ∞è„ÄÇ Ë¥ùÂè∂ÊñØÊª§Ê≥¢Ôºö$p(x|z) = \frac{p(z|x)p(x)}{p(z)}$ÂÖàÈ™åÔºö$p(x_t|u_t, x_{t-1})$ÔºåÈÄöËøáÈ¢ÑÊµãÊñπÁ®ãÂæóÂà∞ ‰ººÁÑ∂Ôºö$p(z_t| x_t)$ÔºåÈÄöËøáÊµãÈáèÊñπÁ®ãÂæóÂà∞ ÂêéÈ™åÔºö$p(x_t|z_t)$ÔºåÈÄöËøáË¥ùÂè∂ÊñØÊñπÁ®ãÂæóÂà∞ ÂØπ‰∫é‰∏ÄËà¨ÁöÑÈùûÁ∫øÊÄß„ÄÅÈùûÈ´òÊñØÁ≥ªÁªüÔºåÂæàÈöæÈÄöËøá‰∏äËø∞ÊñπÊ≥ïÂæóÂà∞ÂêéÈ™åÊ¶ÇÁéáÁöÑËß£ÊûêËß£„ÄÇ ËíôÁâπÂç°Ê¥õÈááÊ†∑ÔºöÂÅáËÆæËÉΩ‰ªé‰∏Ä‰∏™ÁõÆÊ†áÂàÜÂ∏É$p(x)$Ëé∑Âæó‰∏ÄÁ≥ªÂàóÊ†∑Êú¨$x_1, x2, ‚Ä¶, x_N$ÔºåÈÇ£‰πàÂ∞±ËÉΩÂà©Áî®Ëøô‰∫õÊ†∑Êú¨Âéª‰º∞ËÆ°Ëøô‰∏™ÂàÜÂ∏ÉÁöÑÊüê‰∫õÂáΩÊï∞ÁöÑÊúüÊúõÂÄº„ÄÇ E(f(x)) = \int_a^{b}f(x)p(x)dx \approx\frac{f(x_1) + f(x_2) + ... + f(x_N)}{N}ËíôÁâπÂç°Ê¥õÈááÊ†∑ÁöÑÊ†∏ÂøÉÊÄùÊÉ≥Â∞±ÊòØÁî®ÂùáÂÄºÊù•‰ª£ÊõøÁßØÂàÜ„ÄÇ ÂÅáËÆæÂèØ‰ª•‰ªéÂêéÈ™åÊ¶ÇÁéá‰∏≠ÈááÊ†∑Âà∞N‰∏™Ê†∑Êú¨ÔºåÈÇ£‰πàÂêéÈ™åÊ¶ÇÁéáÂèØ‰ª•Ë°®Á§∫‰∏∫Ôºö \hat p(x_t|z_{1:t}) = \frac{1}{N} \sum_{i=1}^{N}\delta(x_n - x_n^{i}) \approx p(x_t|z_{1:t})Á≤íÂ≠êÊª§Ê≥¢Ôºö \begin{align} E(f(x)) & \approx \int f(x_n) \hat p(x_t|z_{1:t}) dx \nonumber\\ & = \frac{1}{N}\sum_N f(x_n) \delta(x_n - x_n^{i})\nonumber\\ & = \frac{1}{N}\sum_N f(x_n^{i}) \nonumber \end{align}Áî®ÈááÊ†∑Á≤íÂ≠êÔºàÊúç‰ªéÂêéÈ™åÊ¶ÇÁéáÔºâÁöÑÁä∂ÊÄÅÂÄºÁõ¥Êé•Âπ≥Âùá‰Ωú‰∏∫ÊúüÊúõÂÄºÔºåËøôÂ∞±ÊòØÁ≤íÂ≠êÊª§Ê≥¢„ÄÇ MCLÔºöËíôÁâπÂç°Ê¥õÂÆö‰ΩçÔºèÁ≤íÂ≠êÊª§Ê≥¢ÂÆö‰Ωç Randomly generate a bunch of particles Predict next state of the particles Update the weighting of the particles based on the measurement. ResampleÔºöDiscard highly improbable particle and replace them with copies of the more probable particles. This leads to a new particle set with uniform importance weights, but with an increased number of particles near the three likely places. Compute the weighted mean and covariance of the set of particles to get a state estimate. ÊùÉÂÄºÈÄÄÂåñÔºöÂ¶ÇÊûú‰ªªÁî±Á≤íÂ≠êÊùÉÂÄºÂ¢ûÈïøÔºåÂè™ÊúâÂ∞ëÊï∞Á≤íÂ≠êÁöÑÊùÉÂÄºËæÉÂ§ßÔºåÂÖ∂‰ΩôÁ≤íÂ≠êÁöÑÊùÉÂÄºÂèØ‰ª•ÂøΩÁï•‰∏çËÆ°ÔºåÂèòÊàêÊó†ÊïàÁ≤íÂ≠êÔºåÂõ†Ê≠§ÈúÄË¶ÅÂºïÂÖ•ÈáçÈááÊ†∑„ÄÇÈááÁî®$N_{eff}$Ë°°ÈáèÁ≤íÂ≠êÊùÉÂÄºÁöÑÈÄÄÂåñÁ®ãÂ∫¶„ÄÇ N_{eff} \approx \hat{N_{eff}} = \frac{1}{\sum_N (w_k^{i})^2}Á≤íÂ≠êÂ§öÊ†∑ÊÄßÔºöÈÄöÂ∏∏Êàë‰ª¨‰ºöËàçÂºÉÊùÉÂÄºËæÉÂ∞èÁöÑÁ≤íÂ≠êÔºå‰ª£‰πã‰ª•ÊùÉÂÄºËæÉÂ§ßÁöÑÁ≤íÂ≠ê„ÄÇËøôÊ†∑‰ºöÂØºËá¥ÊùÉÂÄºÂ∞èÁöÑÁ≤íÂ≠êÈÄêÊ∏êÁªùÁßçÔºåÁ≤íÂ≠êÁæ§Â§öÊ†∑ÊÄßÂáèÂº±Ôºå‰ªéËÄå‰∏çË∂≥‰ª•Ëøë‰ººË°®ÂæÅÂêéÈ™åÂØÜÂ∫¶„ÄÇ ÈáçË¶ÅÊÄßÈááÊ†∑ÂÆûÈôÖ‰∏äÂêéÈ™åÊ¶ÇÁéáÂπ∂‰∏çÁü•ÈÅìÔºåË∞à‰ΩïÈááÊ†∑Ôºà$x_n^i$Ôºâ„ÄÇÊàë‰ª¨ÂèØ‰ª•‰ªé‰∏Ä‰∏™Â∑≤Áü•ÁöÑÂàÜÂ∏É$q(x|z)$ÈáåÊù•ÈááÊ†∑ÔºåÈó¥Êé•ÂæóÂà∞Êª§Ê≥¢ÂÄº„ÄÇ \begin{align} E(f(x_k))& = \int f(x) \frac{p(x|z)}{q(x|z)} q(x|z) dx \nonumber\\ & = \frac{E_{q(x|z)}W_k(x_k)f(x_k)}{E_{q(x|z)}W_k(x_k)}\nonumber\\ & \approx \frac{\frac{1}{N} \sum_{i=1}^N W_k({x_k^{i})f(x_k^{i})}}{\frac{1}{N} \sum_{i=1}^N W_k({x_k^{i}})}\nonumber\\ & = \sum_N \hat W_k(x_k^i)f(x_k^i) \nonumber \end{align}Áõ∏ÊØîËæÉ‰∫éÂéüÂßãÁöÑÂùáÂÄºË°®Á§∫ÔºåÂèòÊàê‰∫ÜÂä†ÊùÉÂπ≥ÂùáÂÄº„ÄÇ‰∏çÂêåÁ≤íÂ≠êÊã•Êúâ‰∫Ü‰∏çÂêåÁöÑÊùÉÈáç„ÄÇ \hat W_k(x_k^i) = \frac{W_k(x_k^i)}{\sum_N W_k(x_k^i)}\\ W_k (x_k) \propto \frac{p(x_k|z_{1:k})}{q(x_k|z_{1:k})}Â∑≤Áü•ÁöÑ$q$ÂàÜÂ∏ÉÂè´ÂÅöÈáçË¶ÅÊÄßÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞„ÄÇ ÈÄíÊé®ÁÆóÊ≥ïÔºöÂ∫èË¥ØÈáçË¶ÅÊÄßÈááÊ†∑ \{x_k^i, w_k^i\} = SIS(\{x_{k-1}, w_{k-1}\})_{i=1}^N, y_k)È¶ñÂÖàÂÅáËÆæÈáçË¶ÅÊÄßÂàÜÂ∏É$q(x|z)$Êª°Ë∂≥Ôºö q(x_k | x_{0:k-1}, y_{1:k}) = q(x_k|x_{k-1}, y_k)Âç≥Âè™ÂíåÂâç‰∏ÄÊó∂ÂàªÁöÑÁä∂ÊÄÅ$x_{k-1}$ÂíåÊµãÈáè$y_k$ÊúâÂÖ≥„ÄÇ‰∫éÊòØÊúâÔºö w_k^i \approx w_{k-1}^i \frac{p(y_k|x_k^i)p(x_k^i|x_{k-1}^i)}{q(x_k^i|x_{k-1}^i, y_k)}‰º™‰ª£Á†ÅÔºö 1234567For i=1:N (1)ÈááÊ†∑ÔºöÂºè1 (2)ÊùÉÂÄºÊõ¥Êñ∞ÔºöÂºè2End ForÊùÉÂÄºÂΩí‰∏ÄÂåñÂä†ÊùÉÂπ≥ÂùáÂæóÂà∞Á≤íÂ≠êÊª§Ê≥¢ÂÄºÔºå‰πüÂ∞±ÊòØÂΩìÂâçÁä∂ÊÄÅÁöÑ‰º∞ËÆ°ÂÄºÈáçÈááÊ†∑ ÈáçÈááÊ†∑Êó¢ÁÑ∂ÊùÉÈáçÂ∞èÁöÑÈÇ£‰∫õÁ≤íÂ≠ê‰∏çËµ∑‰ΩúÁî®‰∫ÜÔºåÈÇ£Â∞±‰∏çË¶Å‰∫Ü„ÄÇ‰∏∫‰∫Ü‰øùÊåÅÁ≤íÂ≠êÊï∞ÁõÆ‰∏çÂèòÔºåÂ∞±Ë¶ÅË°•ÂÖÖÊñ∞Á≤íÂ≠êÔºåÊúÄÁÆÄÂçïÁöÑÂäûÊ≥ïÂ∞±ÊòØÂ§çÂà∂ÊùÉÈáçÂ§ßÁöÑÁ≤íÂ≠ê„ÄÇÁî®$x_k^i$Ë°®Á§∫kÊó∂ÂàªÁöÑÁ≤íÂ≠êÔºå$x_k^j$Ë°®Á§∫ÈáçÈááÊ†∑‰ª•ÂêéÁöÑÁ≤íÂ≠êÔºåÈÇ£‰πàÔºö \tilde p (x_k|y_{1:k}) = \sum_N \frac{1}{N}\delta(x_k - x_k^j) = \sum_N \frac{n_i}{N}\delta(x_k - x_i^j)ÊÄªÁöÑÊù•ËØ¥ÔºåÊñ∞Á≤íÂ≠êÊåâÁÖßÊùÉÈáçÊØî‰æãÊù•Ë°•ÂÖÖÔºåÁÆóÊ≥ïÊµÅÁ®ã‰∏∫Ôºö 12345678ËÆ°ÁÆóÊ¶ÇÁéáÁ¥ØÁßØÂíåwcum(N)Áî®[0,1]‰πãÈó¥ÁöÑÂùáÂåÄÂàÜÂ∏ÉÈöèÊú∫ÈááÊ†∑N‰∏™ÂÄºu(N)for i in 1:N: k = 1 while u(i)&lt;wcum(k): k += 1 end while resample(i) = k SIRÊª§Ê≥¢Âô®ÔºàSampling Importance Resampling Filter ÔºâÈÄâÂèñÁâπÂÆöÁöÑÈáçË¶ÅÊÄßÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞Ôºö q(x_k^i|x_{k-1}^i, y_k) = p(x_k^i|x_{k-1}^i)‰∫éÊòØÊùÉÈáçÊõ¥Êñ∞ÂÖ¨ÂºèÂèØ‰ª•ÁÆÄÂåñÔºö w_k^i \propto w_{k-1}^i \frac{p(z|x)p(x|x_{k-1})}{q(x|x_{k-1})}Áî±‰∫éÈáçÈááÊ†∑‰ª•ÂêéÔºåÁ≤íÂ≠êÂàÜÂ∏ÉÊõ¥Êñ∞ÔºåÊùÉÂÄºÁªü‰∏Ä‰∏∫$\frac{1}{N}$Ôºå‰∫éÊòØÊùÉÈáçÊõ¥Êñ∞ÂÖ¨ÂºèËøõ‰∏ÄÊ≠•ÁÆÄÂåñÔºö w_k^i \propto p(z_k|x_k^i)Ê†πÊçÆÊµãÈáèÊñπÁ®ãÂèØÁü•Ôºå‰∏äÈù¢Ëøô‰∏™Ê¶ÇÁéáÂ∞±ÊòØ‰ª•ÁúüÂÆûÊµãÈáèÂÄº‰∏∫ÂùáÂÄºÔºå‰ª•Âô™Â£∞ÊñπÂ∑Æ‰∏∫ÊñπÂ∑ÆÁöÑÈ´òÊñØÂàÜÂ∏É„ÄÇ Ê≠§ÁÆóÊ≥ï‰∏≠ÁöÑÈááÊ†∑ÔºåÂπ∂Ê≤°ÊúâÂä†ÂÖ•ÊµãÈáè$z_k$ÔºåÂè™Âá≠ÂÖàÈ™åÁü•ËØÜ$p(x_k|x_{k-1})$ÔºåËôΩÁÑ∂ÁÆÄÂçïÊòìÁî®Ôºå‰ΩÜÊòØÂ≠òÂú®ÊïàÁéá‰∏çÈ´òÂíåÂØπÂ•áÂºÇÁÇπ(outliers)ÊïèÊÑüÁöÑÈóÆÈ¢ò„ÄÇ AMCLMCLÁÆóÊ≥ïËÉΩÂ§üÁî®‰∫éÂÖ®Â±ÄÂÆö‰ΩçÔºå‰ΩÜÊòØÊó†Ê≥ï‰ªéÊú∫Âô®‰∫∫ÁªëÊû∂ÊàñÂÖ®Â±ÄÂÆö‰ΩçÂ§±Ë¥•‰∏≠ÊÅ¢Â§çËøáÊù•ÔºåÂõ†‰∏∫ÈöèÁùÄ‰ΩçÁΩÆË¢´Ëé∑ÂèñÔºåÂÖ∂‰ªñÂú∞ÊñπÁöÑ‰∏çÊ≠£Á°ÆÁ≤íÂ≠ê‰ºöÈÄêÊ∏êÊ∂àÂ§±„ÄÇÁ®≥ÂÆöÁä∂ÊÄÅ‰∏ãÔºåÁ≤íÂ≠êÂè™‚ÄúÁîüÂ≠ò‚ÄùÂú®‰∏Ä‰∏™Âçï‰∏ÄÁöÑÂßøÊÄÅÈôÑËøëÔºåÂ¶ÇÊûúËøô‰∏™ÂßøÊÄÅÊÅ∞Â•Ω‰∏çÊ≠£Á°ÆÔºàÂú®ÈáçÈááÊ†∑Ê≠•È™§‰∏≠ÂèØËÉΩÊÑèÂ§ñÁöÑ‰∏¢ÂºÉÊâÄÊúâÊ≠£Á°Æ‰ΩçÂßøÈôÑËøëÁöÑÁ≤íÂ≠êÔºâÔºåÁÆóÊ≥ïÂ∞±Êó†Ê≥ïÊÅ¢Â§ç„ÄÇ AMCLÂ∞±ÊòØ‰∏∫‰∫ÜËß£ÂÜ≥‰∏äËø∞ÈóÆÈ¢òÔºöÁªìÂêà‰∫ÜËá™ÈÄÇÂ∫îÔºàAugmented_MCLÔºâÂíåÂ∫ìÂ∞îË¥ùÂÖã-Ëé±‰∏çÂãíÊï£Â∫¶ÈááÊ†∑ÔºàKLD_Sampling_MCLÔºâ Augmented_MCLÔºöÂú®Êú∫Âô®‰∫∫ÈÅ≠Âà∞ÁªëÊû∂ÁöÑÊó∂ÂÄôÔºåÂÆÉ‰ºöÂú®ÂèëÁé∞Á≤íÂ≠ê‰ª¨ÁöÑÂπ≥ÂùáÂàÜÊï∞Á™ÅÁÑ∂Èôç‰Ωé‰∫ÜÔºåËøôÊÑèÂë≥ÁùÄÊ≠£Á°ÆÁöÑÁ≤íÂ≠êÂú®ÊüêÊ¨°Ëø≠‰ª£‰∏≠Ë¢´ÊäõÂºÉ‰∫ÜÔºåÊ≠§Êó∂‰ºöÈöèÊú∫ÁöÑÂÖ®Â±ÄÊ≥®ÂÖ•Á≤íÂ≠êÔºàinjection of random particlesÔºâ„ÄÇ KLD_Sampling_MCLÔºöÂä®ÊÄÅË∞ÉÊï¥Á≤íÂ≠êÊï∞ÔºåÂΩìÊú∫Âô®‰∫∫ÂÆö‰ΩçÂ∑Æ‰∏çÂ§öÂæóÂà∞‰∫ÜÁöÑÊó∂ÂÄôÔºåÁ≤íÂ≠êÈÉΩÈõÜ‰∏≠Âú®‰∏ÄÂùó‰∫ÜÔºåÂ∞±Ê≤°ÂøÖË¶ÅÁª¥ÊåÅËøô‰πàÂ§öÁöÑÁ≤íÂ≠ê‰∫Ü‚Äî‚ÄîÂú®Ê†ÖÊ†ºÂú∞Âõæ‰∏≠ÔºåÁúãÁ≤íÂ≠êÂç†‰∫ÜÂ§öÂ∞ëÊ†ÖÊ†º„ÄÇÂç†ÂæóÂ§öÔºåËØ¥ÊòéÁ≤íÂ≠êÂæàÂàÜÊï£ÔºåÂú®ÊØèÊ¨°Ëø≠‰ª£ÈáçÈááÊ†∑ÁöÑÊó∂ÂÄôÔºåÂÖÅËÆ∏Á≤íÂ≠êÊï∞ÈáèÁöÑ‰∏äÈôêÈ´ò‰∏Ä‰∫õ„ÄÇÂç†ÂæóÂ∞ëÔºåËØ¥ÊòéÁ≤íÂ≠êÈÉΩÂ∑≤ÁªèÈõÜ‰∏≠‰∫ÜÔºåÈÇ£Â∞±Â∞Ü‰∏äÈôêËÆæ‰Ωé„ÄÇ ÁÆóÊ≥ïÊµÅÁ®ã‰∏äÁúãÔºåaugmented_MCLÁÆóÊ≥ïÊúÄÊòæËëóÁöÑÂå∫Âà´Â∞±ÊòØÂºïÂÖ•‰∫ÜÂõõ‰∏™ÂèÇÊï∞Áî®‰∫éÂ§±ÊïàÊÅ¢Â§çÔºö $w_{slow}$ÔºöÈïøÊúü‰ººÁÑ∂Âπ≥Âùá‰º∞ËÆ° $w_{fast}$ÔºöÁü≠Êúü‰ººÁÑ∂Âπ≥Âùá‰º∞ËÆ° $\alpha_{slow}$ÔºöÈïøÊúüÊåáÊï∞Êª§Ê≥¢Âô®Ë°∞ÂáèÁéá $\alpha_{fast}$ÔºöÁü≠ÊúüÊåáÊï∞Êª§Ê≥¢Âô®Ë°∞ÂáèÁéá Â§±ÊïàÊÅ¢Â§çÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÔºöÊµãÈáè‰ººÁÑ∂ÁöÑ‰∏Ä‰∏™Á™ÅÁÑ∂Ë°∞ÂáèÔºàÁü≠Êúü‰ººÁÑ∂Âä£‰∫éÈïøÊúü‰ººÁÑ∂ÔºâË±°ÂæÅÁùÄÁ≤íÂ≠êË¥®ÈáèÁöÑ‰∏ãÈôçÔºåËøôÂ∞ÜÂºïËµ∑ÈöèÊú∫ÈááÊ†∑Êï∞ÁõÆÁöÑÂ¢ûÂä†„ÄÇ $w_{avg}$ËÆ°ÁÆó‰∫ÜÁ≤íÂ≠êÁöÑÂπ≥ÂùáÊùÉÈáçÔºåÂΩìÁ≤íÂ≠êË¥®Èáè‰∏ãÈôçÊó∂ÔºåÂπ≥ÂùáÊùÉÈáçÈöè‰πã‰∏ãÈôçÔºå$w_{slow}„ÄÅw_{fast}$‰πü‰ºöÈöè‰πã‰∏ãÈôçÔºå‰ΩÜÊòØÊòæÁÑ∂$w_{fast}$‰∏ãÈôçÁöÑÈÄüÂ∫¶Ë¶ÅÂø´‰∫é$w_{slow}$‚Äî‚ÄîËøôÁî±Ë°∞ÂáèÁéáÂÜ≥ÂÆöÔºåÂõ†Ê≠§ÈöèÊú∫Ê¶ÇÁéá$p = 1 - \frac{w_{fast}}{w_{slow}}$‰ºöÂ¢ûÂ§ßÔºåÈöèÊú∫Á≤íÂ≠êÊï∞ÁõÆÂ¢ûÂä†„ÄÇËÄåÂΩìÁ≤íÂ≠êË¥®ÈáèÊèêÈ´òÊó∂ÔºåÁ≤íÂ≠êÁü≠ÊúüÊùÉÈáçË¶ÅÂ•Ω‰∫éÈïøÊúüÔºåÈöèÊú∫Ê¶ÇÁéáÂ∞è‰∫é0Ôºå‰∏çÁîüÊàêÈöèÊú∫Á≤íÂ≠ê„ÄÇ ROS amclÂèÇÊï∞Ëß£ÊûêAugmented_MCLÔºö &lt;param name=&quot;recovery_alpha_slow&quot; value=&quot;0.0&quot;/&gt;ÔºöÈªòËÆ§0ÔºàMCLÔºâÔºåÊàëÁöÑ0.001„ÄÇ &lt;param name=&quot;recovery_alpha_fast&quot; value=&quot;0.0&quot;/&gt;ÔºöÈªòËÆ§0ÔºàMCLÔºâÔºåÊàëÁöÑ0.8„ÄÇ Âú®rvizÈáåÈÄöËøá2D Pose EstimateÊåâÈíÆÁßªÂä®Êú∫Âô®‰∫∫Êù•Ëß¶ÂèëÔºåÊú∫Âô®‰∫∫‰ΩçÁΩÆÁ™ÅÂèòÂêéË¶ÅËøá‰∏Ä‰ºöÂÑøÊâçÊ≥®ÂÖ•ÈöèÊú∫Á≤íÂ≠êÔºåÂõ†‰∏∫Ê¶ÇÁéáÊòØÊ∏êÂèòÁöÑ„ÄÇ KLDÔºö &lt;param name=&quot;kld_z&quot; value=&quot;0.99&quot;/&gt;Ôºö KLDÈááÊ†∑‰ª•Ê¶ÇÁéá$1-\deltaÔºàkld_zÔºâ$Á°ÆÂÆöÊ†∑Êú¨Êï∞„ÄÇ &lt;param name=&quot;kld_err&quot; value=&quot;0.05&quot;/&gt;Ôºö ÁúüÂÆûÁöÑÂêéÈ™å‰∏éÂü∫‰∫éÈááÊ†∑ÁöÑËøë‰ºº‰πãÈó¥ÁöÑËØØÂ∑Æ„ÄÇ Âä®ÊÄÅÈöúÁ¢çÁâ©ÔºöÁéØÂ¢É‰∏≠ÁöÑÂä®ÊÄÅÁâ©‰ΩìÊÄªÊòØ‰ºöËé∑ÂæóÊØîÈùôÊÄÅÈöúÁ¢çÁâ©Êõ¥Áü≠ÁöÑËØªÊï∞ÔºåÂõ†Ê≠§ÂèØ‰ª•Ê†πÊçÆËøôÊ†∑ÁöÑ‰∏çÂØπÁß∞ÊÄßÂéªÈô§ÂºÇÂ∏∏ÂÄº„ÄÇ ÈùôÊÄÅÈöúÁ¢çÁâ©Â∫îËØ•Êúç‰ªéÁ®≥ÂÆöÁöÑÈ´òÊñØÂàÜÂ∏ÉÔºå‰ª•Ë∑ùÁ¶ª‰º†ÊÑüÂô®ÁöÑÁúüÂÆûË∑ùÁ¶ª‰∏∫ÂùáÂÄº„ÄÇ Êâ´ÊèèÂà∞Âä®ÊÄÅÁõÆÊ†áÁöÑbeamÂàôÊúç‰ªéË°∞ÂáèÂàÜÂ∏ÉÔºå$-\eta e ^{-\lambda z}$„ÄÇ laser_model_typeÔºö‰ΩøÁî®beam modelÊó∂‰ºöÁî®Âà∞Âõõ‰∏™Ê∑∑ÂêàÊùÉÈáçÂèÇÊï∞z_hitÔºåz_shortÔºåz_maxÂíåz_randÔºå‰ΩøÁî®likelihood_field modelÊó∂‰ΩøÁî®‰∏§‰∏™z_hitÂíåz_rand„ÄÇ laser_z_hitÔºödefault=0.95Ôºå‰ª•ÁúüÂÆûÂÄº‰∏∫ÂùáÂÄºÁöÑÂô™Â£∞È´òÊñØÂàÜÂ∏É laser_z_randÔºödefualt=0.05ÔºåÈöèÊú∫ÊµãÈáèÊùÉÈáçÔºåÂùáÂåÄÂàÜÂ∏É laser_z_shortÔºödefault=0.1ÔºåÊÑèÂ§ñÂØπË±°ÊùÉÈáçÔºåË°∞ÂáèÂàÜÂ∏É laser_z_maxÔºödefault=0.05ÔºåÊµãÈáèÂ§±Ë¥•ÊùÉÈáçÔºå0/1ÂàÜÂ∏É ÂàùÂßã‰ΩçÂßøÔºö ÂèØ‰ª•Âú®rvizÈáåÈÄöËøá2D Pose EstimateÊåâÈíÆËÆæÂÆöÔºàrviz‰ºöÂèëÂ∏ÉinitialPoseËØùÈ¢òÔºâ„ÄÇ ÊàñËÄÖÂÜôÂú®launchÊñá‰ª∂‰∏≠Ôºö 123456&lt;param name="initial_pose_x" value="0.0"/&gt;&lt;param name="initial_pose_y" value="0.0"/&gt;&lt;param name="initial_pose_a" value="0.0"/&gt; &lt;param name="initial_cov_xx" value="0.25"/&gt;&lt;param name="initial_cov_yy" value="0.25"/&gt;&lt;param name="initial_cov_aa" value="(pi/12)*(pi/12)"/&gt; Ë∞ÉÁî®ÂÖ®Â±ÄÂÆö‰ΩçÊúçÂä°Ôºö 1rosservice call /global_localization "&#123;&#125;" ‰ΩçÂßøÈöèÊú∫ÂàùÂßãÂåñÔºåÁ≤íÂ≠êÊ¥íÊª°Âú∞ÂõæÔºö transform_toleranceÔºö ÈªòËÆ§ÊòØ0.1secondsÔºåÂÆòÊñπÂÆö‰πâÊòØTime with which to post-date the transform that is published, to indicate that this transform is valid into the future. tfÂèòÊç¢ÂèëÂ∏ÉÊé®ËøüÁöÑÊó∂Èó¥ÔºåÊÑèÊÄùÊòØtfÂèòÊç¢Âú®Êú™Êù•ËøôÊÆµÊó∂Èó¥ÂÜÖÊòØÂèØÁî®ÁöÑ„ÄÇ „ÄêÂ≠òÁñë„ÄëÊàë‰∏™‰∫∫ÁêÜËß£tfÁöÑÊõ¥Êñ∞È¢ëÁéáÂ∫îËØ•Ë∂äÂø´Ë∂äÂáÜÁ°ÆÔºålaunchÊñá‰ª∂‰∏≠ÊúÄÂºÄÂßãËÆæÂÆö‰∏∫0.5Ôºå‰ΩÜÊòØÂÆûÈôÖ‰∏äÊú∫Âô®‰∫∫ÁßªÂä®ÈÄüÂ∫¶Ë∞ÉÂø´Êó∂Ôºå‰ºöÊä•ÈîôCostmap2DROS transform timeout...Could not get robot pose, cancelling reconfigurationÔºåÁÑ∂ÂêéÊàëË∞ÉÊï¥‰∏∫1.5Â∞±‰∏çÊä•Èîô‰∫Ü„ÄÇ ÁõÆÂâçËÆæÂÆö‰∏∫1.0Ôºå‰ªøÁúüÈáåËßÇÊµã‰∏çÂá∫Â∑ÆÂºÇ„ÄÇ]]></content>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[recovery behavior]]></title>
    <url>%2F2018%2F11%2F16%2Frecovery-behavior%2F</url>
    <content type="text"><![CDATA[navigation stackÁöÑmove_baseÂåÖ‰∏≠‰∏Ä‰∏™Êèí‰ª∂„ÄÇDWAÁöÑÈÄüÂ∫¶Á©∫Èó¥‰∏≠Â¶ÇÊûúÊ≤°ÊúâÂèØË°åÁöÑÈááÊ†∑ÁÇπÔºåÈÇ£‰πàÊú∫Âô®‰∫∫get stuckÔºåËß¶ÂèërecoveryË°å‰∏∫„ÄÇ recoveryË°å‰∏∫ÁöÑÂÆûË¥®ÊòØclear out space‚Äî‚ÄîËØïÂõæÊêûÊ∏ÖÊ•öËá™Â∑±ÁöÑÂ§ÑÂ¢ÉÔºö È¶ñÂÖàÊú∫Âô®‰∫∫‰ºöÊ∏ÖÊâ´Âú∞Âõæ‚Äî‚Äîconservative reset ÁÑ∂ÂêéÂéüÂú∞ÊóãËΩ¨360Â∫¶ÔºåÂà∑Êñ∞Â§ÑÂ¢É‚Äî‚Äîclearing rotation Â¶ÇÊûúËøòÊòØÂØºËà™Â§±Ë¥•ÔºåÊú∫Âô®‰∫∫‰ºöÊõ¥Âä†ÊøÄËøõÁöÑÊ∏ÖÊâ´Âú∞Âõæ‚Äî‚Äîaggressive reset ÁÑ∂ÂêéÂéüÂú∞ÊóãËΩ¨360Â∫¶ÔºåÂà∑Êñ∞Â§ÑÂ¢É‚Äî‚Äîclearing rotation Â¶ÇÊûú‰ªçÊóßÂ§±Ë¥•‚Äî‚Äîmission impossible Ê∫ê‰ª£Á†ÅÂú®move_base.cppÈáåÈù¢ÔºåÁªßÊâø‰∫Ünav_coreÁöÑÊé•Âè£ÔºåËÆæÁΩÆÂú®move_base_params.yamlÈÖçÁΩÆÊñá‰ª∂‰∏≠„ÄÇ nav_coreÁöÑrecovery_behavior.hÂ∞ÅË£Ö‰∫ÜRecoveryBehaviorÁ±ª„ÄÇ move_base‰∏≠ÂàõÂª∫‰∫ÜÂêç‰∏∫‚Äùclear_costmap_recovery/ClearCostmapRecovery‚Äù„ÄÅ‚Äùrotate_recovery/RotateRecovery‚Äù„ÄÅ‚Äùclear_costmap_recovery/ClearCostmapRecovery‚ÄùÁöÑÈªòËÆ§ÂØπË±°„ÄÇ move_baseÁöÑ‰∏ªÁ®ãÂ∫èÊòØ‰∏Ä‰∏™Áä∂ÊÄÅÊú∫Ôºåcase CLEARINGÂ∞±Ë∞ÉÁî®RecoveryBehaviorÁöÑrunBehavior()„ÄÇ]]></content>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dynamic window approach]]></title>
    <url>%2F2018%2F11%2F15%2Fdynamic-window-approach%2F</url>
    <content type="text"><![CDATA[Âä®ÊÄÅÁ™óÂè£Ôºö Á™óÂè£Ê°ÜÁöÑÊòØÈÄüÂ∫¶Á©∫Èó¥ÁöÑÈááÊ†∑ÁÇπ$(v_t, w_t)$Ôºå‰∏ÄÂØπ$(v_t, w_t)$Â∞±‰ª£Ë°®‰∏ÄÊÆµËΩ®ËøπÔºåËΩ®ËøπÈÄöËøáÊú∫Âô®‰∫∫Â∫ïÁõòÁöÑËøêÂä®Â≠¶Âª∫Ê®°ÂæóÂà∞„ÄÇ 12345678910// base_local_plannerÁöÑsimple_trajectory_generator.cpp// Áõ¥Á∫øÊ®°Âûãdouble dt = (current_time - last_time).toSec();double delta_x = (vx * cos(theta) - vy * sin(theta)) * dt;double delta_y = (vx * sin(theta) + vy * cos(theta)) * dt;double delta_th = vth * dt;x += delta_x;y += delta_y;theta += delta_th; Á™óÂè£ÁöÑÈÄâÊã©Ôºö ÈÄüÂ∫¶ÈôêÂà∂ (V, W) = \{v \in[v_{min}, v_{max}], w \in [w_{min}, w_{max}] \} Âä†ÈÄüÂ∫¶ÈôêÂà∂ (V, W) = \left\{ \begin{array} & v \in[v_c - \dot{v}*\Delta t, v_c + \dot{v}*\Delta t], \\ w \in [w_c - \dot{w}*\Delta t, w_c + \dot{w}*\Delta t] \end{array} \right\} ÈöúÁ¢çÁâ©Âà∂Âä®ÈôêÂà∂ (V, W) = \left\{ v \leq \sqrt{2*dist(v,w)*\dot{v}}, w \leq \sqrt{2*dist(v,w)*\dot{w}} \right\}$dist(v,w)$Ë°®Á§∫ÈááÊ†∑ÁÇπ$(v, w)$ÂØπÂ∫îËΩ®Ëøπ‰∏äÁ¶ªÈöúÁ¢çÁâ©ÊúÄËøëÁöÑË∑ùÁ¶ª„ÄÇ Á°ÆÂÆöÁ™óÂè£ÂêéËøõË°åÈááÊ†∑ÔºåÂèØ‰ª•ÂæóÂà∞‰∏ÄÁ≥ªÂàóËΩ®ËøπÔºö ËΩ®ËøπÁöÑÈÄâÊã©Ôºö ÂéüÂßãËÆ∫Êñá‰∏≠ÈááÁî®ËØÑ‰ª∑ÂáΩÊï∞Ôºö G(v,w) = \sigma [\alpha * heading(v, w) + \beta * dist(v,w) + \gamma * velocity(v,w)] Êñπ‰ΩçËßíËØÑ‰ª∑ÂáΩÊï∞ÔºöÈááÁî®ÂΩìÂâçÈááÊ†∑ÁÇπËÆæÂÆö‰∏ãÔºåËææÂà∞Ê®°ÊãüËΩ®ËøπÊú´Á´ØÊó∂Êú∫Âô®‰∫∫ÁöÑÊúùÂêëËßí‰∏éÁõÆÊ†áÁÇπÊúùÂêëËßíÁöÑÂ∑ÆË∑ù„ÄÇ Á©∫ÈöôËØÑ‰ª∑ÔºöÂΩìÂâçÊ®°ÊãüËΩ®Ëøπ‰∏ä‰∏éÊúÄËøëÈöúÁ¢çÁâ©‰πãÈó¥ÁöÑË∑ùÁ¶ª„ÄÇ ÈÄüÂ∫¶ËØÑ‰ª∑ÔºöÈááÊ†∑ÁÇπÈÄüÂ∫¶‰∏éÊúÄÂ§ßÈÄüÂ∫¶ÁöÑÂ∑ÆË∑ù„ÄÇ ‰∏äËø∞ËØÑ‰ª∑ÂáΩÊï∞Ë¶ÅËøõË°åÂΩí‰∏ÄÂåñ„ÄÇ ÁÆóÊ≥ïÊµÅÁ®ãÔºö 123456789101112131415161718BEGIN DWA(robotPose,robotGoal,robotModel) laserscan = readScanner() allowable_v = generateWindow(robotV, robotModel) allowable_w = generateWindow(robotW, robotModel) for each v in allowable_v for each w in allowable_w dist = find_dist(v,w,laserscan,robotModel) breakDist = calculateBreakingDistance(v) if (dist &gt; breakDist) //can stop in time heading = hDiff(robotPose,goalPose, v,w) clearance = (dist-breakDist)/(dmax - breakDist) cost = costFunction(heading,clearance, abs(desired_v - v)) if (cost &gt; optimal) best_v = v best_w = w optimal = cost set robot trajectory to best_v, best_wEND]]></content>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A-star algorithm]]></title>
    <url>%2F2018%2F11%2F14%2FA-star-algorithm%2F</url>
    <content type="text"><![CDATA[ÊúâÁõÆÁöÑÊÄßÂú∞ÂØªÊâæÊúÄ‰Ω≥Ë∑ØÂæÑÔºåÈ¶ñÂÖàÂÆö‰πâ‰∏Ä‰∏™ÊçüÂ§±ÂáΩÊï∞ÔºåË°®Á§∫ËäÇÁÇπÊ∂àËÄóÔºö f = g+h$g$Ë°®Á§∫Ëµ∑ÁÇπÂà∞ÂΩìÂâçËäÇÁÇπÁöÑÂ∑≤Áü•Ê∂àËÄó $h$Ë°®Á§∫ÂØπÂΩìÂâçËäÇÁÇπÂà∞ÁªàÁÇπÊ∂àËÄóÁöÑÁåúÊµãÔºå‰º∞‰ª∑ÂáΩÊï∞ÊúâÂ§öÁßçÂΩ¢Âºè‚Äî‚ÄîÂêØÂèëÂºèÊé¢Á¥¢ÁöÑÊ†∏ÂøÉ ÁÆóÊ≥ïÊµÅÁ®ãÔºö 123456789101112131415161718192021ÂàùÂßãÂåñopenListÂàùÂßãÂåñcloseListÂ∞ÜËµ∑ÁÇπÊîæÂÖ•openListwhile openListÈùûÁ©∫Ôºö ÊâæÂà∞ÂºÄÂêØÂàóË°®‰∏äfÊúÄÂ∞èÁöÑËäÇÁÇπÔºåËÆ∞‰∏∫q ÊâæÂà∞qÂë®Âõ¥ÁöÑÂ≠êËäÇÁÇπpÔºåËÆ∞ÂÖ∂Áà∂ËäÇÁÇπ‰∏∫q for ÊØè‰∏Ä‰∏™Â≠êËäÇÁÇπpÔºö if pÊòØÁªàÁÇπÔºö ÁÆóÊ≥ïÁªàÊ≠¢ p.g = q.g + qp p.h = h(p, terminate) p.f = p.g + p.h if pÂ∑≤ÁªèÂú®ÂºÄÂêØÂàóË°®‰∏≠‰∏î‰øùÂ≠òÁöÑfÂÄºÂ∞è‰∫éÂΩìÂâçËÆ°ÁÆóÂÄº||pÂú®ÂÖ≥Èó≠ÂàóË°®‰∏≠Ôºö Ë∑≥ËøáËØ•ËäÇÁÇπ elseÔºö if pÂ∑≤ÁªèÂú®ÂºÄÂêØÂàóË°®‰∏≠Ôºö ‰øÆÊîπËØ•ËäÇÁÇπÁöÑ‰ø°ÊÅØÔºàÁà∂ËäÇÁÇπ„ÄÅfghÔºâ elseÔºö Â∞ÜËØ•ËäÇÁÇπÂä†ÂÖ•openList Â∞ÜqÊîæÂÖ•closeList ÁÆóÊ≥ïÊÄßËÉΩÂú®ÁªÜËäÇ‰∏äÁöÑ‰ºòÂåñÔºöhttp://theory.stanford.edu/~amitp/GameProgramming/ Â∫èË®ÄÔºöË∑ØÂæÑÊêúÁ¥¢ÁÆóÊ≥ïÁöÑÂâç‰∏ñ‰ªäÁîü DijkstraÁÆóÊ≥ïÔºö‰ªéÂàùÂßãËäÇÁÇπÂºÄÂßãÂêëÂ§ñÊâ©Â±ïÔºåÁõ¥Âà∞Âà∞ËææÁõÆÊ†áËäÇÁÇπ„ÄÇÁÆóÊ≥ï‰øùËØÅËÉΩÊâæÂà∞‰ªéÂàùÂßãÁÇπÂà∞ÁõÆÊ†áÁÇπÁöÑÊúÄÁü≠Ë∑ØÂæÑ„ÄÇ ÊúÄ‰Ω≥‰ºòÂÖàÊêúÁ¥¢BFSÁÆóÊ≥ïÔºöÁÆóÊ≥ïËÉΩÂ§üËØÑ‰º∞‰ªªÊÑèËäÇÁÇπÂà∞ÁõÆÊ†áÁÇπÁöÑ‰ª£‰ª∑ÔºåÂπ∂‰ºòÂÖàÈÄâÊã©Á¶ªÁõÆÊ†áÊúÄËøëÁöÑÁªìÁÇπ„ÄÇÂêØÂèëÂºèÁÆóÊ≥ïÔºåÊØîDijkstraÁÆóÊ≥ïËøêË°åÂø´ÂæóÂ§öÔºå‰ΩÜÊòØ‰∏çËÉΩ‰øùËØÅË∑ØÂæÑÊúÄÁü≠„ÄÇ Â¶Ç‰∏ãÈù¢ËøôÁßçÊÉÖÂÜµÔºö Âõ†‰∏∫BFSÊòØÂü∫‰∫éË¥™ÂøÉÁ≠ñÁï•ÁöÑÔºåÂÆÉÂè™ÂÖ≥Ê≥®Âà∞Â∞ΩÂèØËÉΩÂêëÁùÄÁõÆÊ†áÁÇπÁßªÂä®ÔºåËÄå‰∏çËÄÉËôëÂ∑≤Ëä±Ë¥πÁöÑ‰ª£‰ª∑„ÄÇDijkstraÁÆóÊ≥ïÂàôÊ≠£Áõ∏ÂèçÔºåÂÆÉ‰ºöÁ°Æ‰øùÊØè‰∏ÄÊ≠•ÈÉΩÊòØÊúÄ‰ºòÁöÑÔºå‰ΩÜÊòØ‰∏∫Ê≠§Ë¶ÅÈÅçÂéÜÂë®Âõ¥ÂÖ®ÈÉ®ÁöÑËäÇÁÇπ„ÄÇ A*ÁÆóÊ≥ïÔºöÂ∞Ü‰∏§ÁßçË∑ØÂæÑÊêúÁ¥¢ÁÆóÊ≥ïÁöÑÊÄùÊÉ≥ÁªìÂêàËµ∑Êù•ÔºåËÄÉËôë‰∏§‰∏™ÊûÅÁ´ØÂèäÂÖ∂‰∏≠Èó¥ÁöÑÊÉÖÂÜµÔºö Â¶ÇÊûú$h(n)$ÊòØ0ÔºåÂè™Êúâ$g(n)$Ëµ∑‰ΩúÁî®ÔºåÈÇ£‰πàÁÆóÊ≥ïÊºîÂèò‰∏∫DijkstraÁÆóÊ≥ï„ÄÇ Â¶ÇÊûú$h(n)$ËÉΩÂ§üÂßãÁªàÊª°Ë∂≥‚ÄúÊØîÂΩìÂâçËäÇÁÇπÁßªÂä®Âà∞ÁõÆÊ†áËäÇÁÇπÁöÑÂÆûÈôÖ‰ª£‰ª∑Â∞è‚ÄùÔºåÈÇ£‰πàÁÆóÊ≥ï‰øùËØÅËÉΩÂ§üÊâæÂà∞ÊúÄÁü≠Ë∑ØÂæÑ„ÄÇÔºà$h(n)$Ë∂äÂ∞èÔºåÁÆóÊ≥ïÊâ©Â±ïÁöÑËäÇÁÇπÊï∞Â∞±Ë∂äÂ§öÔºâ Â¶ÇÊûú$h(n)$ËÉΩÂ§üÁ≤æÁ°ÆÁ≠â‰∫é‚ÄúÂΩìÂâçËäÇÁÇπÁßªÂä®Âà∞ÁõÆÊ†áËäÇÁÇπÁöÑÂÆûÈôÖ‰ª£‰ª∑‚ÄùÔºåÈÇ£‰πàÁÆóÊ≥ïÂ∞Ü‰ºö‰ªÖ‰ªÖÊâ©Â±ïÊúÄ‰ºòË∑ØÂæÑ„ÄÇËÄå‰∏çÊâ©Â±ïÂÖ∂‰ªñËäÇÁÇπÔºåÁÆóÊ≥ïËøêË°åÈùûÂ∏∏Âø´„ÄÇ Â¶ÇÊûú$h(n)$ÊúâÊó∂‰ºö‚ÄúÊØîÂΩìÂâçËäÇÁÇπÁßªÂä®Âà∞ÁõÆÊ†áËäÇÁÇπÁöÑÂÆûÈôÖ‰ª£‰ª∑Â§ß‚ÄùÔºåÈÇ£‰πàÊ≠§Êó∂ÁÆóÊ≥ï‰∏çËÉΩ‰øùËØÅÊúÄÁü≠Ë∑ØÂæÑ‰∫Ü„ÄÇ Â¶ÇÊûú$g(n)$ÊØî$h(n)$Â∞èÁöÑÂ§öÔºåÂè™Êúâ$h(n)$Ëµ∑‰ΩúÁî®ÔºåÈÇ£‰πàÁÆóÊ≥ïÊºîÂèò‰∏∫BFSÁÆóÊ≥ï„ÄÇ ‰º∞‰ª∑ÂáΩÊï∞Heuristic function $h(a, b)$ ‰º∞‰ª∑ÂáΩÊï∞ÁöÑÈÄâÊã©ÂèØ‰ª•follow‰ª•‰∏ãÁöÑinstructionsÔºö square grid that allows 4 directionsÔºöuse Manhattan distance (L1) dx = abs(a.x - b.x)\\ dy = abs(a.y - b.y)\\ dis = D*(dx+dy) square grid that allows 8 directionsÔºöuse Diagonal distance (L‚àû) dx = abs(a.x - b.x)\\ dy = abs(a.y - b.y)\\ dis = D*(dx+dy) + (D2 - 2*D)*min(dx, dy)ÂΩì$D = D2 =1$Êó∂Ôºå$dis = dx + dy -min(dx, dy) = max(dx, dy)$ÔºåËøô‰∏™Ë∑ùÁ¶ªÁß∞‰∏∫ÂàáÊØîÈõ™Â§´Ë∑ùÁ¶ª„ÄÇ ÂΩì$D = 1, D2 =\sqrt 2$Êó∂ÔºåËøô‰∏™Ë∑ùÁ¶ªÁß∞‰∏∫octile distance„ÄÇ square grid that allows any direcitonsÔºöuse Euclidean distance (L2) dx = abs(a.x - b.x)\\ dy = abs(a.y - b.y)\\ dis = D*\sqrt{dx*dx + dy*dy} If A* is finding paths on the grid but you are allowing movement not on the grid, you may want to consider other representations of the map ‚Äã Ê¨ßÂá†ÈáåÂæóË∑ùÁ¶ªÂπ∂‰∏çÈÄÇÁî®‰∫éÊ†ÖÊ†ºÂú∞ÂõæÔºåÂõ†‰∏∫Ëøô‰ºöÂØºËá¥‰ª£‰ª∑ÂáΩÊï∞gÂíå‰º∞‰ª∑ÂáΩÊï∞ÁöÑ‰∏çÂåπÈÖçÔºà‰ª£‰ª∑ÂáΩÊï∞Âπ∂‰∏çÊòØËøûÁª≠ÁöÑÔºâ„ÄÇ Áî±‰∫éÊ¨ßÂá†ÈáåÂæóË∑ùÁ¶ªÂºïÂÖ•‰∫ÜÂºÄÊ†πÂè∑ËÆ°ÁÆóÔºå‰∏Ä‰∫õÁÆóÊ≥ï‰ºöÁõ¥Êé•Áî®$dis = D(dxdx + dydy)$Êù•‰ª£ÊõøÔºå*‰∏çÂª∫ËÆÆÔºÅÔºå‰ºöÂºïÂÖ•Â∞∫Â∫¶ÈóÆÈ¢òÔºå$f = g + h$ÔºåÂÖ∂‰∏≠‰ª£‰ª∑ÂáΩÊï∞‰ºöÈÄêÊ∏êÂ¢ûÈïøÔºå‰º∞‰ª∑ÂáΩÊï∞ÂàôÈÄêÊ∏êÂáèÂ∞èÔºåÂπ≥Êñπ‰ºöÂØºËá¥‰∏§‰∏™ÂáΩÊï∞ÁöÑÂèòÂåñÈÄüÁéá‰∏çmatchÔºå‰ΩøÂæó‰º∞‰ª∑ÂáΩÊï∞ÁöÑÊùÉÈáçËøáÂ§ßÔºåÂØºËá¥BFS„ÄÇ codeÔºö 12345678910111213public function manhattanHeuristic(a:Object, b:Object):Number &#123; return graph.distance(a, b) + simpleCost(a, b) - 1;&#125;public function simpleCost(a:Object, b:Object):Number &#123; var c:Number = costs[graph.nodeToString(b)]; if (isNaN(c)) &#123; return 1; &#125; else &#123; return c; &#125;&#125;// simpleCostÈôêÂÆö‰∏∫Â∞è‰∫éÁ≠â‰∫é1ÁöÑÊï∞ Ê≠§Êó∂$h(a,b) = dis(a,b)+c-1 \leq h^{*}(a,b)$ÔºåÊ≠§Êó∂ËÉΩÂ§üÊâæÂà∞ÊúÄ‰ºòËß£„ÄÇ]]></content>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[branch and bound]]></title>
    <url>%2F2018%2F11%2F13%2Fbranch-and-bound%2F</url>
    <content type="text"><![CDATA[‰∏Ä‰∏™Ê†óÂ≠êÔºöÊï¥Êï∞ËßÑÂàíÈóÆÈ¢òÊ¨≤Ê±Ç$max\ z = 5x_1 + 8x_2$ \left\{ \begin{align} & x_1 + x_2 \leq 6 \nonumber\\ & 5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\ & x_1,x_2‰∏∫Êï¥Êï∞ \nonumber \end{align} \right.Ê†πÊçÆÊñπÁ®ãÁªÑÂèØ‰ª•ÁªòÂà∂‰∏ãÂõæÔºö ‰∫éÊòØÂèØ‰ª•ÂæóÂà∞ÂÆûÊï∞Á©∫Èó¥‰∏äÁöÑÊúÄ‰ºòËß£Ôºö$x_1 = 2.25, x_2 = 3.75, z_0 = 41.25$„ÄÇ‚Äî‚ÄîÊùæÂºõÈóÆÈ¢ò Áî±‰∫éÂ≠òÂú®Êï¥Êï∞ÈôêÂÆöÊù°‰ª∂Ôºö ÊúÄ‰ºòËß£$0 \leq z^{*} \leq 41$Ôºå‰∏îÂøÖ‰∏∫Êï¥Êï∞ x_2ÁöÑÊúÄ‰ºòËß£‰∏çÂú®3Âíå4‰πãÈó¥ÔºåÂõ†‰∏∫ÈôêÂÆö‰∏∫Êï¥Êï∞ ‰∏Ä„ÄÅÂàÜÊîØ ‰∫éÊòØÈóÆÈ¢òÂèØ‰ª•ÊãÜÂàÜ‰∏∫Ôºö$max\ z = 5x_1 + 8x_2$ p1\left\{ \begin{align} & x_1 + x_2 \leq 6 \nonumber\\ & 5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\ & x_2\leq 3 \nonumber \end{align} \right.\\ p2\left\{ \begin{align} & x_1 + x_2 \leq 6 \nonumber\\ & 5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\ & x_2\geq 4 \nonumber \end{align} \right.\\ÈóÆÈ¢òÊãÜÂàÜÁöÑÂÆûË¥®ÊòØÂ∞Ü$x_2$Âú®3Âíå4‰πãÈó¥ÁöÑÂ∞èÊï∞ÈÉ®ÂàÜÂàíÊéâ‰∫ÜÔºåÂ∞ÜÂèØË°åÂüüÊãÜÂàÜÊàê$x_2 \leq 3$ Âíå$x_3 \geq 4$Ôºå‰ΩÜÊòØÊ≤°ÊúâÊéíÈô§‰ªª‰ΩïÊï¥Êï∞ÂèØË°åËß£„ÄÇ‚Äî‚ÄîÂàÜÊîØ ‰∫å„ÄÅÂÆöÁïå Â≠êÈóÆÈ¢ò$p1$ÁöÑÊúÄ‰ºòËß£‰∏∫Ôºö$x_1 = 3, x_2=3, z^{*}=39$ Â≠êÈóÆÈ¢ò$p2$ÁöÑÊúÄ‰ºòËß£‰∏∫Ôºö$x_1 = 1.8, x_2=4, z^{*}=41$ ‰πüÂ∞±ÊòØËØ¥ÔºåÂ≠êÈóÆÈ¢ò$p1$ÁöÑÊï¥‰∏™ÂèÇÊï∞Á©∫Èó¥‰∏äÔºåËÉΩÂ§üÂèñÂæóÁöÑÊúÄ‰ºòËß£‰∏∫39ÔºåÂ≠êÈóÆÈ¢ò$p2$‰∏äÂàô‰∏∫41ÔºåÊòæÁÑ∂ÊúÄ‰ºòËß£Â∫îËØ•‰Ωç‰∫éÂ≠êÈóÆÈ¢ò$p2$ÊâÄÂú®ÁöÑÂèÇÊï∞Á©∫Èó¥‰∏≠Ôºå‰∏î$39\leq z^{} \leq41$„ÄÇ‚Äî‚Äî*ÂÆöÁïå ‰∏â„ÄÅËø≠‰ª£ ÂØπ$p2$ÂèÇÊï∞Á©∫Èó¥ÂÜçÂàÜÊîØÔºåÂèÇÊï∞$x_1$ÂèØ‰ª•ÊãÜÂàÜ‰∏∫$x_1 \leq 1$Âíå$x_1 \geq 2$Ôºö p3\left\{ \begin{align} & x_1 + x_2 \leq 6 \nonumber\\ & 5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\ & x_1\leq 1 \nonumber\\ & x_2\geq 4 \nonumber \end{align} \right.\\ p4\left\{ \begin{align} & x_1 + x_2 \leq 6 \nonumber\\ & 5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\ & x_1\geq 2 \nonumber \\ & x_2\geq 4 \nonumber \end{align} \right.\\Âõõ„ÄÅÊÄªÁªì ÂàÜÊîØÂÆöÁïåÁÆóÊ≥ïÁöÑÊÄª‰ΩìÊµÅÁ®ãÂ¶Ç‰∏ãÔºö ÂÖàÊ±ÇËß£Áõ∏Â∫îÁöÑÊùæÂºõÈóÆÈ¢òÔºåÂæóÂà∞ÊúÄ‰ºòËß£ÔºåÊ£ÄÊü•ÂÖ∂ÊòØÂê¶Á¨¶ÂêàÂéüÈóÆÈ¢òÁ∫¶ÊùüÔºåËã•Á¨¶ÂêàÂàô‰∏∫ÊúÄ‰ºòËß£ÔºåÂê¶ÂàôËøõË°å‰∏ã‰∏ÄÊ≠•„ÄÇ ÂÆöÁïåÔºåÂèñÂêÑÂàÜÊîØ‰∏≠ÁõÆÊ†áÂáΩÊï∞ÊúÄÂ§ßÁöÑ‰Ωú‰∏∫‰∏äÁïå$U_z$ÔºåÂèñÂÖ∂‰ΩôÂàÜÊîØ‰∏≠ÁõÆÊ†áÂáΩÊï∞‰∏≠ÊúÄÂ§ßÁöÑ‰Ωú‰∏∫‰∏ãÁïå$L_z$„ÄÇ$L_z \leq z^{*} \leq U_z$„ÄÇ ÂàÜÊîØÔºåÂê¶ÂàôÈÄâÊã©‰∏Ä‰∏™‰∏çÁ¨¶ÂêàÂéüÈóÆÈ¢òÊù°‰ª∂ÁöÑÂèòÈáèÔºåÊûÑÂª∫Â≠êÈóÆÈ¢ò„ÄÇ ÂØπÂêÑÂàÜÊîØÔºåÊúâÂ∫èÂú∞ÔºåËøõË°åÊ≠•È™§1„ÄÇ Âú®Ê±ÇËß£ÂØπÂ∫îÁöÑÊùæÂºõÈóÆÈ¢òÊó∂ÔºåÈÄöÂ∏∏‰ºöÊúâ‰ª•‰∏ãÂá†ÁßçÊÉÖÂÜµÔºö ÊùæÂºõÈóÆÈ¢òÊ≤°ÊúâÂèØË°åËß£ÔºåÈÇ£‰πàÂéüÈóÆÈ¢ò‰πüÊ≤°ÊúâÂèØË°åËß£„ÄÇ ÊùæÂºõÈóÆÈ¢òÁöÑÊúÄ‰ºòËß£‰πüÊª°Ë∂≥ÂéüÈóÆÈ¢òÁ∫¶ÊùüÔºåÈÇ£‰πàËØ•Ëß£‰πüÊòØÂéüÈóÆÈ¢òÁöÑÊúÄ‰ºòËß£ÔºåÁÆóÊ≥ïÁªàÊ≠¢„ÄÇ ÊùæÂºõÈóÆÈ¢òÁöÑÊúÄ‰ºòËß£Â∞è‰∫éÁé∞Êúâ‰∏ãÁïåÔºåÈÇ£‰πàÂ∫îËØ•ÂØπËØ•Â≠êÈóÆÈ¢òËøõË°åÂâ™Êûù„ÄÇ ‰∫î„ÄÅDFS ÂØπ‰∏ÄÈ¢óÊêúÁ¥¢Ê†ëÔºå‰∏çÁî®ËÆ°ÁÆóÊØè‰∏ÄÂ±ÇÂÖ®ÈÉ®ËäÇÁÇπÁöÑscoreÔºàBFSÔºâÔºåÊàë‰ª¨‰ºöÁª¥Êä§‰∏Ä‰∏™‰ºòÂÖàÈòüÂàóÔºåÂÖ∂‰∏≠ÊåâÁÖßscoreÁöÑÂ§ßÂ∞èÂ≠òÊîæËäÇÁÇπÔºåÁÑ∂ÂêéÈÄâÊã©scoreÊúÄÂ§ßÁöÑËäÇÁÇπÔºàthe most promising childÔºâËøõË°åÂàÜÊîØ„ÄÇ]]></content>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[trifles with arduino]]></title>
    <url>%2F2018%2F10%2F30%2Ftrifles-with-arduino%2F</url>
    <content type="text"><![CDATA[Á≥ªÁªüÊÄª‰ΩìÁöÑÈÄö‰ø°Êû∂ÊûÑÂ¶Ç‰∏ãÔºö Â∫ïÁõòÈ©±Âä®ÊùøArduinoË¥üË¥£Êé•Êî∂‰∏äÂ±ÇÁöÑËøêÂä®ÊéßÂà∂Êåá‰ª§ÔºåÂπ∂È©±Âä®ÁîµÊú∫Ôºå‰∏§ÂùóÊùøÂ≠êÈÄöËøá‰∏≤Âè£ËøõË°åÈÄö‰ø°„ÄÇ ROSÊèê‰æõ‰∫Ü‰∏Ä‰∏™ros_arduino_bridgeÂäüËÉΩÂåÖÈõÜÔºåÂÆÉÂåÖÊã¨‰∫ÜArduinoÂ∫ìÔºàROSArduinoBridgeÔºâ‰ª•Âèä‰∏ÄÁ≥ªÂàóÁî®Êù•ÊéßÂà∂Arduino-based robotÁöÑROSÂäüËÉΩÂåÖÔºåËøô‰∏™ÂäüËÉΩÂåÖÂèØ‰ª•ÂÆûÁé∞ËØªÂèñTwistÊéßÂà∂‰ø°ÊÅØÔºå‰ª•ÂèäÂèëÂ∏ÉÈáåÁ®ãËÆ°‰ø°ÊÅØÁ≠â‰ªªÂä°ÔºåÂ∞ÅË£Ö‰∫ÜRaspberry PiÂíåArduino‰πãÈó¥ÁöÑÂ∫ïÂ±ÇÈÄö‰ø°„ÄÇ ArduinoÂ∫ìÔºàROSArduinoBridgeÔºâ‰Ωç‰∫éros_arduino_firmware/src/libraries/Ë∑ØÂæÑ‰∏ãÔºåÈáåÈù¢ÊòØ‰∏Ä‰∫õarduinoËÑöÊú¨ÂíåÂ§¥Êñá‰ª∂ÔºåÂ∞ÜËøô‰∏™Êñá‰ª∂Â§πÂ§çÂà∂Âà∞Êàë‰ª¨Arduino IDEÁöÑSKETCHBOOK_PATH‰∏ãÔºåÁÑ∂ÂêéÂú®Arduino IDE‰∏≠Â∞±ÂèØ‰ª•Áõ¥Êé•ÊâìÂºÄËøô‰∏™sketchÈ°πÁõÆ„ÄÇ ROSArduinoBridgeÊñá‰ª∂‰∏ãÊòØ‰∏Ä‰∫õÈÖçÁΩÆÈÄâÈ°πÔºåÂè¶Â§ñcommands.hÊñá‰ª∂‰∏≠ÁªôÂá∫‰∫Ü‰∏Ä‰∫õÂèØÁî®ÁöÑ‰∏≤Âè£ÊéßÂà∂Êåá‰ª§ÔºåÂ¶ÇÁîµÊú∫ÊéßÂà∂Êåá‰ª§Ôºö 1m 20 20 // move the robot forward at 20 encoder ticks per second]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[skid-steer drive]]></title>
    <url>%2F2018%2F10%2F29%2Fskid-steer-drive%2F</url>
    <content type="text"><![CDATA[ÂÆûÈ™å‰∏≠‰ΩøÁî®‰∫Ü‰∏§ÁßçÁ±ªÂûãÁöÑÂ∫ïÁõòÔºåÂü∫‰∫éÂ∑ÆÈÄüÈ©±Âä®ÁöÑ2WDÂ∫ïÁõòÂíåÂü∫‰∫éÊªëÂä®ËΩ¨ÂêëÁöÑ4WDÂ∫ïÁõò„ÄÇ‰∏§ÁßçÈ©±Âä®ÊñπÂºèÂéüÁêÜÁõ∏‰ººÔºå‰πüÊúâÂÖ∂ÊòæËëóÁöÑÂå∫Âà´„ÄÇ Áõ∏ÂêåÁÇπÔºö ‰∏§ÁßçÂ∫ïÁõòÈÉΩÊ≤°ÊúâÊòæÁ§∫ÁöÑËΩ¨Âä®Êú∫Âà∂ÔºåÈááÁî®Â∑ÆÈÄüÈ©±Âä®ÁöÑÊñπÂºèÈÄöËøá‰ª•‰∏çÂêåÁöÑÊñπÂêëÊàñÈÄüÂ∫¶È©±Âä®‰∏§ËæπËΩÆÂ≠êÊù•ÂÆûÁé∞ÊñπÂêëÊéßÂà∂„ÄÇ Â∑ÆÈÄüÈ©±Âä®ÁöÑËøêÂä®ÂΩ¢ÂºèÈÄöÂ∏∏Êúâ‰∏Ä‰∏ãÂá†ÁßçÁ±ªÂûãÔºö Á¨¨‰∏ÄÁßçÊòØÂéüÂú∞ÊóãËΩ¨ÔºåÂ∑¶Âè≥ËΩÆÁöÑÈÄüÂ∫¶Â§ßÂ∞èÁõ∏Á≠âÔºåÊñπÂêëÁõ∏ÂèçÔºåËøôÊ†∑Áõ∏ÂΩì‰∫éÁªïÁùÄÂ∫ïÁõòÁöÑÂΩ¢ÂøÉÂéüÂú∞ÊâìËΩ¨„ÄÇ Á¨¨‰∫åÁßçÊòØÊ≤øÁùÄÊüê‰∏™ÊñπÂêëÁõ¥Á∫øË°åËµ∞ÔºåÊ≠§Êó∂Â∑¶Âè≥ËΩÆÈÄüÂ∫¶Áõ∏Âêå„ÄÇ Á¨¨‰∏âÁßçÊòØÊ≤øÁùÄÊüêÊù°Êõ≤Á∫øÂâçË°åÊàñÂêéÈÄÄÔºåÊ≠§Êó∂Â∑¶Âè≥ËΩÆÈÄüÂ∫¶ÊñπÂêëÁõ∏ÂêåÔºåÂ§ßÂ∞è‰∏çÂêå„ÄÇ Á¨¨ÂõõÁßçÊòØÊóãËΩ¨ËΩ¨ÂºØÔºåÊ≠§Êó∂Â∑¶Âè≥ËΩÆÈÄüÂ∫¶ÊñπÂêëÁõ∏Âèç„ÄÇ ‰∏§ÁßçÊú∫ÊûÑÂÖ±ÂêåÁöÑ‰ºòÂäøÊòØÔºöÊ≤°ÊúâÊòæÁ§∫ÁöÑËΩ¨ÂêëÊú∫ÊûÑÔºåÊûÅÂ§ßÂú∞ÁÆÄÂåñ‰∫ÜËøêÂä®Â≠¶Ê®°Âûã„ÄÇ ËÄå‰∏§ÁßçÊú∫ÊûÑÂÖ±ÂêåÁöÑÁº∫ÁÇπÊòØÔºöÁî±‰∫é‰∏§‰æßÁöÑËΩÆÂ≠êÊòØÁî±Áã¨Á´ãÁîµÊú∫ÂàÜÂà´È©±Âä®ÁöÑÔºåÁõ¥Á∫øËøêÂä®Ë¶ÅÊ±Ç‰∏§‰æßÁöÑËΩÆÂ≠ê‰ª•Áõ∏ÂêåÈÄüÂ∫¶ËΩ¨Âä®ÔºåËøôÂ∞ÜÂæàÈöæÂÆåÊàê„ÄÇ ‰∏çÂêåÁÇπÔºö Â∑ÆÈÄüÈ©±Âä®Â∫ïÁõòÈÄöÂ∏∏ÊòØÁî±‰∏Ä‰∏™‰∏§ËΩÆÁ≥ªÁªüÔºåÊØè‰∏™ËΩÆÂ≠êÈÉΩÂ∏¶ÊúâÁã¨Á´ãÁöÑÊâßË°åÊú∫ÊûÑÔºàÁõ¥ÊµÅÁîµÊú∫ÔºâÔºå‰ª•Âèä‰∏Ä‰∏™Êó†È©±Âä®ËΩÆÔºàÂèØ‰ª•ÊòØËÑöËΩÆÊàñËÄÖ‰∏áÂêëÊªöÁè†ÔºâÁªÑÊàêÔºåÊú∫Âô®‰∫∫ÁöÑËøêÂä®Áü¢ÈáèÊòØÊØè‰∏™Áã¨Á´ãËΩ¶ËΩÆËøêÂä®ÁöÑÊÄªÂíå„ÄÇ ÊªëÂä®ËΩ¨ÂêëÂ∫ïÁõòÈÄöÂ∏∏Ë¢´Áî®Âú®Â±•Â∏¶ËΩ¶‰∏äÔºåÊØîÂ¶ÇÂù¶ÂÖãÂíåÊé®ÂúüÊú∫Ôºå‰πüË¢´Áî®‰∫éÊüê‰∫õÂõõËΩÆÂÖ≠ËΩÆÊú∫ÊûÑ‰∏äÔºåÁõ∏ÊØîËæÉ‰∫é‰∏§ËΩÆÂ∑ÆÈÄüÂ∫ïÁõòÔºåÊªëÂä®ËΩ¨ÂêëÁöÑ‰∏ªË¶ÅÂå∫Âà´Âú®‰∫éÔºö ‰ºòÂäøÔºöÊªëÂä®ËΩ¨Âêë‰ΩøÁî®‰∫Ü‰∏§‰∏™È¢ùÂ§ñÁöÑÈ©±Âä®ËΩÆ‰ª£Êõø‰∫ÜÂ∑ÆÈÄüÈ©±Âä®ÁöÑËÑöËΩÆÔºåÂ¢ûÂ§ß‰∫ÜÁâµÂºïÂäõ„ÄÇ Âä£ÂäøÔºöÂºïÂÖ•‰∫ÜÊªëÂä®ÔºåÂú®ÂØπÈáåÁ®ãËÆ°Ë¶ÅÊ±ÇÈ´òÁöÑÂú∫ÊôØ‰∏≠ÔºåÊªëÂä®ÊòØ‰∏Ä‰∏™Ëá¥ÂëΩÁöÑÁº∫Èô∑ÔºåÂõ†‰∏∫Ëøô‰ºöÂØπÁºñÁ†ÅÂô®ÈÄ†ÊàêË¥üÈù¢ÂΩ±ÂìçÔºåÊªëÂä®ÁöÑËΩÆÂ≠ê‰∏ç‰ºöË∑üË∏™Êú∫Âô®‰∫∫ÁöÑÁ°ÆÂàáËøêÂä®„ÄÇ ËøêÂä®Â≠¶ÂàÜÊûêÔºö ÂØπ‰∫éÂ∑ÆÈÄüÈ©±Âä®Êú∫ÊûÑÔºåÁßªÂä®Êú∫Âô®‰∫∫Ëà™ÂêëËßíÂèòÂåñ‰∫ÜÂ§öÂ∞ëËßíÂ∫¶ÔºåÂÆÉÂ∞±ÁªïÂÖ∂ËøêÂä®ËΩ®ËøπÁöÑÂúÜÂøÉÊóãËΩ¨‰∫ÜÂ§öÂ∞ëËßíÂ∫¶„ÄÇËøôÂè•ËØùÂæàÂ•ΩÈ™åËØÅÔºåÊàë‰ª¨ËÆ©Êú∫Âô®‰∫∫ÂÅöÂúÜÂë®ËøêÂä®Ôºå‰ªéËµ∑ÁÇπÂá∫ÂèëÁªïÂúÜÂøÉ‰∏ÄÂúàÂõûÂà∞Ëµ∑ÁÇπÂ§ÑÔºåÂú®ËøôËøáÁ®ã‰∏≠Êú∫Âô®‰∫∫Á¥ØËÆ°ÁöÑËà™ÂêëËßí‰∏∫360Â∫¶ÔºåÂêåÊó∂ÂÆÉ‰πüÁ°ÆÂÆûÁªïËΩ®ËøπÂúÜÂøÉËøêÂä®‰∫Ü360Â∫¶„ÄÇ Êú∫Âô®‰∫∫ÁöÑÈÄüÂ∫¶ÊòØÊåá‰∏§‰∏™Áõ∏ÈÇªÁöÑÊéßÂà∂Êó∂Âàª‰πãÈó¥ÁöÑÈÄüÂ∫¶ÔºåÂõ†Ê≠§Â∞èËΩ¶ÁöÑË°åÈ©∂ËΩ®ËøπÂèØ‰ª•ÂàÜËß£‰∏∫ËøûÁª≠ÁöÑÂúÜÂºßÁâáÊÆµÔºåÂØπ‰∫éÊØè‰∏ÄÊÆµÂúÜÂºßÔºåÊ†πÊçÆÈòøÂÖãÊõºËΩ¨ÂêëÂá†‰ΩïÂéüÁêÜÔºåÂú®Â∞èËΩ¶ËΩ¨ÂêëÊó∂Ôºå‰∏∫‰øùËØÅË°åÈ©∂Á®≥ÂÆöÊÄßÔºå‰∏§‰æßËΩÆËÉéÈÉΩËøë‰ººÂõ¥Áªï‰∏Ä‰∏™‰∏≠ÂøÉÁÇπÊóãËΩ¨„ÄÇÂç≥Êï¥‰∏™Â∞èËΩ¶Â∫ïÁõòÈÉΩÂõ¥Áªï‰∏Ä‰∏™‰∏≠ÂøÉÁÇπÊóãËΩ¨ÔºåÂ∑≤Áü•Â∞èËΩ¶‰∏≠ÂøÉÁöÑÁ∫øÈÄüÂ∫¶Ôºà‰∏äÂ±ÇÁÆóÊ≥ïÁªôÂÆöÔºâÔºåÊ≠§Êó∂Â∞èËΩ¶Â∫ïÁõòÁöÑËøêÂä®Â≠¶Ê®°ÂûãÂ¶Ç‰∏ãÂõæÔºö ÂèÇÊï∞ËØ¥ÊòéÔºö $\alpha_1$ÊòØÂ∞èËΩ¶ÂâçÂ∑¶ËΩÆÂíåÂêéÂ∑¶ËΩÆÁöÑËΩ¨Ëßí„ÄÇ $\alpha_2$ÊòØÂ∞èËΩ¶ÂâçÂè≥ËΩÆÂíåÂêéÂè≥ËΩÆÁöÑËΩ¨Ëßí„ÄÇ $2L$ÊòØÂ∑¶Âè≥ËΩÆË∑ùÁ¶ª„ÄÇ $2K$ÊòØÂâçÂêéËΩÆË∑ùÁ¶ª„ÄÇ $w$ÊòØÂ∞èËΩ¶ËΩ¨ËΩ¥ÁöÑËßíÈÄüÂ∫¶„ÄÇ $v$ÊòØÂ∞èËΩ¶Âá†‰Ωï‰∏≠ÂøÉÁöÑÁ∫øÈÄüÂ∫¶„ÄÇ $v1, v2, v3, v4$ÊòØÂõõ‰∏™ËΩ¶ËΩÆÁöÑÈÄüÂ∫¶„ÄÇ $i$ÊòØÁîµÊú∫ÁöÑÂáèÈÄüÊØî„ÄÇ $r$ÊòØËΩ¶ËΩÆÂçäÂæÑ„ÄÇ È¶ñÂÖàÂèØ‰ª•ÂæóÂà∞ÂêÑËΩ¶ËΩÆÈÄüÂ∫¶ÂíåËßíÈÄüÂ∫¶ÁöÑÂÖ≥Á≥ªÔºö V_1 = w * R_1 = w * \frac{K}{sin\alpha_1}\\ V_2 = w * R_2 = w * \frac{K}{sin\alpha_2}\\ V_3 = V_1 = w * \frac{K}{sin\alpha_1}\\ V_4 = V_2 = w * \frac{K}{sin\alpha_2}\\ÂÖ∂‰∏≠ËΩ¶ËΩÆÊ≤øÁùÄËΩ¨Âä®ÊñπÂêëÔºà$y$ÊñπÂêëÔºâÁöÑÈÄüÂ∫¶Áî±ÁîµÊú∫Êèê‰æõÔºåÂàáÂêëÈÄüÂ∫¶Áî±Âú∞Èù¢Êë©Êì¶Êèê‰æõÔºåËΩ¶ËΩÆÊ≤øÁùÄ$y$ÊñπÂêëÁöÑÈÄüÂ∫¶‰∏∫Ôºö R =\frac{v}{w}\\ V_{1y} = V_1 * cos\alpha_1 = w * \frac{K}{tan \alpha_1} = w(R-L)\\ V_{2y} = V_2 * cos\alpha_2 = w * \frac{K}{tan \alpha_2} = w(R+L)\\ V_{3y} = V_{1y} = w(R-L)\\ V_{4y} = V_{2y} = w(R+L)\\ÈÇ£‰πàÁîµÊú∫ÁöÑËßíÈÄüÂ∫¶‰∏∫Ôºö w_n= \frac{V_{ny}*i}{r}, n = 1,2,3,4\\Áõ∏Â∫îÁîµÊú∫ÁöÑËΩ¨ÈÄüÔºàby rpmÔºâ‰∏∫Ôºö n = \frac{w_n*60}{2\pi}Êï¥ÁêÜÊàêÁü©ÈòµË°®ËææÂºè‰∏∫Ôºö \begin{bmatrix} w_1\\ w_2\\ w_3\\ w_4 \end{bmatrix}= \begin{bmatrix} 1 & - L\\ 1 & L\\ 1 & - L\\ 1 & L \end{bmatrix} \begin{bmatrix} v\\ w \end{bmatrix}ËØ•Ë°®ËææÂºèÂèçÊò†‰∫ÜÊú∫Âô®‰∫∫ÂÖ≥ÈîÆÁÇπÈÄüÂ∫¶‰∏é‰∏ªÂä®ËΩÆËΩ¨ÈÄü‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÁªôÂÆöÂ∞èËΩ¶Â∫ïÁõòÁîµÊú∫ËΩ¨ÈÄüÂ∞±ÂèØ‰ª•Ê±ÇÂá∫Êú∫Âô®‰∫∫ÂÖ≥ÈîÆÁÇπÁöÑÈÄüÂ∫¶ÔºåÂπ∂Áî±Ê≠§ÂæóÂà∞Êú∫Âô®‰∫∫‰∏ä‰ªªÊÑè‰∏ÄÁÇπÁöÑÈÄüÂ∫¶ÔºàÂ¶ÇÊøÄÂÖâÈõ∑ËææÁöÑÂÆâË£Ö‰ΩçÁΩÆÁöÑÈÄüÂ∫¶ÔºâÔºå‰∏äÂ±ÇÁÆóÊ≥ïÁªôÂá∫ÁöÑÂÖ≥ÈîÆÁÇπÈÄüÂ∫¶ÊéßÂà∂‰ø°Âè∑‰πüÂèØ‰ª•Áî±Ê≠§ËΩ¨ÂåñÊàêÁîµÊú∫ÁöÑÊéßÂà∂Èáè„ÄÇ ÈáåÁ®ãËÆ°Ê®°Âûã Ôºè Êú∫Âô®‰∫∫ÂÆö‰ΩçÊñπÊ≥ï ÂùêÊ†áÂèòÊç¢Ê®°ÂûãÔºö Âú®‰∏Ä‰∏™ËæÉÁü≠ÁöÑÊó∂Èó¥Èó¥Èöî$\Delta t$ÂÜÖÔºåÂÅáÂÆöÊú∫Âô®‰∫∫Â∑¶Âè≥ËΩÆÁöÑÁßªÂä®Ë∑ùÁ¶ªÂàÜÂà´ÊòØ$\Delta l$Âíå$\Delta r$ÔºåÈÇ£‰πàÂú®Êú∫Âô®‰∫∫ÂùêÊ†áÁ≥ª‰∏ãÔºöÊú∫Âô®‰∫∫‰∏≠ÂøÉÊ≤øÁùÄÊú∫Âô®‰∫∫ÂùêÊ†áÁ≥ªÁöÑ$x$ËΩ¥ÊñπÂêëÂâçËøõÁöÑË∑ùÁ¶ª‰∏∫$\Delta u = (\Delta l + \Delta r)/2$Ôºå$y$ËΩ¥ÊñπÂêëÂâçËøõÁöÑË∑ùÁ¶ª‰∏∫$\Delta v = 0$ÔºåËΩ¨ËøáÁöÑËßíÂ∫¶‰∏∫$\Delta \varphi = (\Delta l - \Delta r)/b$„ÄÇÊú∫Âô®‰∫∫ÂùêÊ†áÁ≥ªÂà∞‰∏ñÁïåÂùêÊ†áÁ≥ªÁöÑÊóãËΩ¨ÂèòÊç¢Áü©Èòµ‰∏∫$R(\phi)$„ÄÇ ÈÇ£‰πàËΩ¨Êç¢Âà∞‰∏ñÁïåÂùêÊ†áÁ≥ª‰∏ãÊú∫Âô®‰∫∫ÁöÑËøêÂä®Â¢ûÈáè‰∏∫Ôºö \begin{bmatrix} \Delta x\\ \Delta y\\ \Delta \phi \end{bmatrix} = \begin{bmatrix} R(\phi) & 0\\ 0 & 1 \end{bmatrix} \begin{bmatrix} \Delta u\\ \Delta v\\ \Delta \varphi \end{bmatrix} = \begin{bmatrix} cos\phi & sin\phi & 0\\ -sin\phi & cos\phi & 0\\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} (\Delta l + \Delta r)/2\\ 0\\ (\Delta l - \Delta r)/b \end{bmatrix}‰∏ñÁïåÂùêÊ†áÁ≥ª‰∏ãÊú∫Âô®‰∫∫‰ΩçÂßøÊõ¥Êñ∞‰∏∫Ôºö \begin{bmatrix} x_t\\ y_t\\ \phi_t \end{bmatrix} = \begin{bmatrix} cos\phi_{t-1}(\Delta l + \Delta r)/2\\ sin\phi_{t-1}(\Delta l + \Delta r)/2\\ (\Delta l - \Delta r)/b \end{bmatrix} + \begin{bmatrix} x_{t-1}\\ y_{t-1}\\ \phi_{t-1} \end{bmatrix}beside fromÊµãÈáèËØØÂ∑ÆÔºåÂà©Áî®ÂùêÊ†áÂèòÊç¢Ê®°ÂûãÂéªÊé®ÁÆóÈáåÁ®ãËÆ°‰ø°ÊÅØÊòØÂºïÂÖ•‰∫ÜÊ®°ÂûãËØØÂ∑ÆÁöÑ‚Äî‚ÄîÂú®Êó∂Èó¥Èó¥Èöî$\Delta t$ÂÜÖÔºå‰∏∫‰∫ÜÁÆÄÂåñËÆ°ÁÆóÔºåÊú∫Âô®‰∫∫ÂùêÊ†áÁ≥ªÁõ∏ÂØπ‰∏ñÁïåÂùêÊ†áÁ≥ªÁöÑÊóãËΩ¨ÂèòÊç¢Áü©ÈòµË¢´ÂÅáÂÆö‰∏∫Ëµ∑ÂßãÂÄº$R(\phi)$„ÄÇÂú®ËΩ¨ÂêëËøêÂä®ÊØîËæÉÂ§öÁöÑÊÉÖÂÜµ‰∏ãÔºåÈáåÁ®ãËÆ°‰ø°ÊÅØ‰ºöËøÖÈÄüÊÅ∂Âåñ„ÄÇ ÂúÜÂºßÊ®°ÂûãÔºö Â∞ÜÊûÅÂ∞èÊó∂Èó¥Èó¥ÈöîÂÜÖÂ∞èËΩ¶ËøêÂä®ÁöÑËΩ®ËøπÁúã‰ΩúÊòØ‰∏ÄÊÆµÂúÜÂºßÔºåÈÇ£‰πàÂ∞±ÂèØ‰ª•Á°ÆÂÆöËØ•Êó∂ÂàªÁöÑËΩ¨Âä®‰∏≠ÂøÉ$C_{t-1}$ÔºåÂèäÂÜÖ‰æßËΩÆÁöÑËΩ¨Âä®ÂçäÂæÑ‰∏∫$R_{t-1}$ÔºåÊ†πÊçÆÂá†‰ΩïÂÖ≥Á≥ªÔºö \left\{ \begin{align} &\Delta l = (b + R)\Delta \varphi\\ &\Delta r = R \Delta \varphi \end{align} \right.Ëß£ÂæóÔºö \left\{ \begin{align} &R = \frac{b\Delta r}{\Delta l - \Delta r}\\ &\Delta \varphi = \frac{\Delta l - \Delta r }{b} \end{align} \right.Áî±‰∏âËßíÁõ∏‰ººÂæóÔºö \frac{Rsin(\Delta \varphi/2)}{D/2} = \frac{R}{R + b/2}Ëß£ÂæóÂº¶$D$ÁöÑÈïøÂ∫¶‰∏∫Ôºö D = [b(\Delta l + \Delta r)/(\Delta l-\Delta r)]sin[(\Delta l - \Delta r)/2b]Âº¶$D$‰∏é‰∏ñÁïåÂùêÊ†áÁ≥ª$x$ËΩ¥Ê≠£ÂêëÁöÑÂ§πËßí‰∏∫$\theta = \phi - \Delta \varphi/2$ÔºåÈÇ£‰πàÊú∫Âô®‰∫∫Âú®‰∏ñÁïåÂùêÊ†áÁ≥ª‰∏ãÁöÑ‰ΩçÂßøÂ¢ûÈáè‰∏∫Ôºö \left\{ \begin{align} &\Delta x_{t-1} = D_{t-1} cos\theta_{t-1}\\ &\Delta y_{t-1} = D_{t-1} sin\theta_{t-1}\\ &\Delta \varphi = (\Delta l - \Delta r )/b \end{align} \right.‰ΩøÁî®ÂúÜÂºßÊ®°ÂûãÂØπÈáåÁ®ãËÆ°Â¢ûÈáèËøõË°åÊé®ÁÆóÔºåÂÆåÂÖ®‰æùÁÖßÂá†‰ΩïÂÖ≥Á≥ªÊù•ËÆ°ÁÆóÔºåËÆ°ÁÆóËøáÁ®ã‰∏≠Ê≤°ÊúâËøë‰ººÔºåËÉΩÂ§üÊúâÊïàÊéßÂà∂ËØØÂ∑ÆÁ¥ØÁßØ„ÄÇ Ê¶ÇÁéáÊ®°ÂûãÔºö ‰∏çÊòØÂçïÁ∫ØÁöÑÂü∫‰∫éÈáåÁ®ãËÆ°ÁöÑ‰º∞ËÆ°ÔºåËÄåÊòØÁªìÂêàÂÖ∂‰ªñ‰º†ÊÑüÂô®ÁöÑÊµãÈáèÂÄºÂØπÈáåÁ®ãËÆ°ËøõË°åÁü´Ê≠£ÔºåËØ¶ËßÅÊª§Ê≥¢ÁÆóÊ≥ï„ÄÇ scan matchÊ®°ÂûãÔºö ÂêåÊ†∑‰πü‰∏çÊòØÂçïÁ∫ØÁöÑÂü∫‰∫éÈáåÁ®ãËÆ°ÁöÑ‰º∞ËÆ°ÔºåËØ¶ËßÅkarto scanMatch„ÄÇ‰∏éÊª§Ê≥¢ÁöÑÂå∫Âà´Âú®‰∫éÔºåËøîÂõûÁöÑ‰∏çÊòØÊ¶ÇÁéáÂàÜÂ∏ÉÔºåËÄåÊòØ‰∏Ä‰∏™‰ª£Ë°®ÊúÄ‰Ω≥‰º∞ËÆ°ÁöÑÂÄº„ÄÇ]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scan matcher]]></title>
    <url>%2F2018%2F10%2F26%2Fscan-matcher%2F</url>
    <content type="text"><![CDATA[ÁõÆÂâçÂºÄÊ∫êÁÆóÊ≥ï‰∏≠ÈááÂèñÁöÑscanMatchingÊñπÊ≥ï‰∏ªË¶ÅÊòØ‰ª•‰∏ãÂõõÁßçÔºö GmappingÔºöICPÔºàsimple Gradient DescentÔºâ HectorÔºöGuass-NewtonÔºàmulti-resolution mapÔºâ kartoÔºöReal-time CSMÔºàmulti-resolution + ‰∏âÁª¥Á™óÂè£ÈÅçÂéÜÂØª‰ºòÔºâ cartographerÔºöFast CSMÔºàmulti-resolution + branch and boundÔºâ scanMatcher‰∏ªË¶ÅÊ∂âÂèä‰∏§‰∏™ËØÑ‰ª∑ÂáΩÊï∞Ôºå‰∏Ä‰∏™scoreÁî®‰∫é‰ºòÂåñË∞ÉÊï¥Á≤íÂ≠êpose‰Ωú‰∏∫ÂèÇËÄÉÔºå‰∏Ä‰∏™likelihoodAndScoreÁî®‰∫éÊõ¥Êñ∞Á≤íÂ≠êÊùÉÈáçÔºö s(x, z, m) = \sum_i s(x, z^i, m)\\ s(x, z^i, m) = e^{d^2/ \sigma}Á≤íÂ≠êÊùÉÈáçÊ†πÊçÆÂú∞ÂõæÁöÑÂåπÈÖçÂ∫¶Êõ¥Êñ∞Ôºö laser\ frame \to map\ frame: \hat{z}^i = x \oplus z^i\\ map\ cell: (x,\ y)^T\\ d^2 = (\hat{z}^i - (x,y)^T)^T (\hat{z}^i - (x,y)^T)]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[navigation stack]]></title>
    <url>%2F2018%2F10%2F23%2Fnavigation-stack%2F</url>
    <content type="text"><![CDATA[part0Áé∞ÂÆû‰∏≠ÊÉ≥Ë¶ÅÁßªÂä®Âπ≥Âè∞ÁßªÂä®Âà∞ÊåáÂÆöÂú∞ÁÇπÔºåÊú∫Âô®‰∫∫ËøêÂä®ÊéßÂà∂Á≥ªÁªüÊû∂ÊûÑÂåÖÊã¨‰∫ÜÂ¶Ç‰∏ãÂá†‰∏™Â±ÇÊ¨°Ôºö ÊúÄÂ∫ïÂ±ÇÊòØÊú∫Âô®‰∫∫ÁöÑÂ∫ïÁõòÊéßÂà∂ÈÉ®ÂàÜÔºåÈ©±Âä®Âô®Êé•Êî∂ÁöÑÊòØÊú∫Âô®‰∫∫ÁöÑÊúüÊúõÈÄüÂ∫¶ÔºàTwistÔºâÔºåÂ∞ÜÈÄüÂ∫¶Ëß£ÁÆó‰∏∫Â∑¶Âè≥ËΩÆÁöÑÊúüÊúõÈÄüÂ∫¶ÔºåÂπ∂Ê†πÊçÆÊúüÊúõÈÄüÂ∫¶ÂØπÂ∑¶Âè≥ËΩÆÂàÜÂà´ËøõË°åPIDÈ©±ÊéßÈÄüÔºåËæìÂá∫ÁîµÊú∫ÁöÑËΩ¨ÈÄü„ÄÇ ËøôÈÉ®ÂàÜROSÁ§æÂå∫Â∑≤ÁªèÊúâÈíàÂØπArduinoÂ∞ÅË£ÖÂ•ΩÁöÑPackage‚Äî‚Äîrosserial_arduino„ÄÇ ‰∏≠Èó¥Â±ÇÊòØÈÄö‰ø°Â±ÇÔºåÁîµËÑëÁ´ØÂèëÂ∏ÉÈÄüÂ∫¶Êåá‰ª§ÁªôÂπ≥Âè∞ÔºåÂêåÊó∂Êé•Êî∂Âπ≥Âè∞ÂèëÂ∏ÉÁöÑÂΩìÂâçÈÄüÂ∫¶ÔºåÁÑ∂ÂêéÂèëÂ∏É/odom topicÔºåËÆ©ÂÖ∂‰ªñËäÇÁÇπËÆ¢ÈòÖ„ÄÇ ÊúÄ‰∏äÂ±ÇÊòØÂÜ≥Á≠ñÂ±ÇÔºå‰πüÂ∞±ÊòØÂØºËà™ËßÑÂàíÂ±ÇÔºågoal„ÄÅlocalizationÔºàmatching&amp;ÈáåÁ®ãËÆ°Ôºâ„ÄÅpath planner‰ª•ÂèäÊúÄÁªàËæìÂá∫ÈÄüÂ∫¶Êåá‰ª§ÔºåËøô‰∏ÄÈÉ®ÂàÜÈÉΩÂú®navigation stackÈáåÈù¢„ÄÇ part1 packagesnavigation stackÊòØROSÊèê‰æõÁöÑÂØºËà™ÊñπÊ°àÔºåÂÜÖÈÉ®ÈõÜÊàê‰∫ÜÂæàÂ§ö‰∏™packageÔºåÊ®°Âùó‰πãÈó¥ÂÆåÂÖ®Ëß£ËÄ¶ÔºåÂèØ‰ª•‰∏™ÊÄßÂåñÈÄâÊã©Êèê‰æõÁöÑÊñπÊ≥ï„ÄÇ ÂÆòÁΩë‰∏äÁªôÂá∫‰∫ÜÂØºËà™Ê†àÂÆèËßÇÁöÑÁªìÊûÑÊèèËø∞Ôºö move_base‰∏≠‰∏ªË¶ÅÂåÖÂê´‰∏â‰∏™ÈÉ®ÂàÜÔºåglobal_plan„ÄÅlocal_plan‰ª•Âèärecovery behavior„ÄÇÂØπÂ∫îÁöÑÊèí‰ª∂ÊúâÔºö global_planÔºöglobal_plannerÔºàÂÆûÁé∞‰∫ÜdijkstraÂíåA*ÁÆóÊ≥ïÔºâÔºåcarrot_plannerÔºånavfn local_planÔºöbase_local_plannerÔºàÂÆûÁé∞‰∫ÜTrajectory RolloutÂíåDWAÁÆóÊ≥ïÔºâÔºådwa_local_planner recoveryÔºöclear_costmap_recoveryÔºåmove_slow_and_clearÔºårotate_recovery nav_coreÊòØ‰∏Ä‰∏™Êé•Âè£Êèí‰ª∂ÔºåÂåÖÂê´‰∫Ü‰ª•‰∏äÊèí‰ª∂Âü∫Á±ªÁöÑÂ§¥Êñá‰ª∂Ôºåmove_base‰∏≠ÁöÑÊñπÊ≥ïÈÉΩÊòØÂú®ÂÖ∂ËßÑÂàô‰∏äÊâ©Â±ïÁöÑ„ÄÇ ‰∏§‰∏™ÁÅ∞Ëâ≤ÁöÑÊèí‰ª∂map_serverÂíåamclË°®Á§∫ÂèØÈÄâÂèØ‰∏çÈÄâÔºö ÂèØ‰ª•‰ΩøÁî®meta packageÊèê‰æõÁöÑmap_serverËäÇÁÇπÊù•ËøõË°å‰ª£‰ª∑Âú∞ÂõæÁÆ°ÁêÜÔºå‰πüÂèØ‰ª•‰ΩøÁî®ÂÖ∂‰ªñËäÇÁÇπÔºà‰æãÂ¶ÇÁõ¥Êé•‰ΩøÁî®gmappingÁöÑËæìÂá∫Ôºâ„ÄÇ ÂèØ‰ª•‰ΩøÁî®meta packageÊèê‰æõÁöÑamclËäÇÁÇπÊù•ËøõË°åËá™ÂÆö‰ΩçÔºå‰πüÂèØ‰ª•‰ΩøÁî®ÂÖ∂‰ªñÁÆóÊ≥ïÂåÖÔºà‰æãÂ¶ÇROSÈáåÈù¢ËøòÊúâ‰∏Ä‰∏™robot_pose_ekfËäÇÁÇπÔºâ„ÄÇ costmap_2dÂ∞Ü‰∏çÂêå‰º†ÊÑüÂô®ÁöÑËæìÂÖ•Â§ÑÁêÜÊàêÁªü‰∏ÄÁöÑÊ†ÖÊ†ºÂú∞ÂõæÊ†ºÂºè„ÄÇ‰ª•Â±ÇÁöÑÊ¶ÇÂøµÊù•ÁªÑÁªáÂõæÂ±ÇÔºåÁî®Êà∑ÂèØ‰ª•Ê†πÊçÆÈúÄË¶ÅËá™Â∑±ÈÖçÁΩÆÔºàÈÄöËøáSocial Costmap Layer„ÄÅRange Sensor LayerÁ≠âÂºÄÊ∫êÊèí‰ª∂ÔºâÔºåÈªòËÆ§ÁöÑÂ±ÇÊúâÔºö static_layerÔºöÈùôÊÄÅÂú∞ÂõæÂ±ÇÔºåÔºàÈÄöËøáËÆ¢ÈòÖmap_serverÁöÑ/map‰∏ªÈ¢òÔºâÊù•ÁîüÊàê„ÄÇ obstacle_layerÔºöÈöúÁ¢çÂú∞ÂõæÂ±ÇÔºåÊ†πÊçÆÂä®ÊÄÅÁöÑ‰º†ÊÑüÂô®‰ø°ÊÅØÊù•ÁîüÊàê„ÄÇ inflation_layerÔºöËÜ®ËÉÄÂ±ÇÔºåÂ∞ÜÂâç‰∏§‰∏™ÂõæÂ±ÇÁöÑ‰ø°ÊÅØÁªºÂêàËøõË°åÁºìÂÜ≤Âå∫Êâ©Â±ï„ÄÇ voxel_gridÊòØ‰∏âÁª¥‰ª£‰ª∑Âú∞Âõæ„ÄÇ fake_localizationÁî®Êù•ÂÅöÂÆö‰Ωç‰ªøÁúüÔºåÂÜÖÂê´/base_pose_ground_truthËØùÈ¢ò„ÄÇ part2 params move_base_params.yamlÔºö planner_frequencyÔºöÂÖ®Â±ÄËßÑÂàíÁöÑÊâßË°åÈ¢ëÁéáÔºåÂ¶ÇÊûúËÆæÁΩÆ‰∏∫0.0ÂàôÂÖ®Â±ÄËßÑÂàíÂô®‰ªÖÂú®Êé•ÂèóÂà∞Êñ∞ÁõÆÊ†áÁÇπÊàñËÄÖÂ±ÄÈÉ®ËßÑÂàíÂô®Êä•ÂëäË∑ØÂæÑÂ†µÂ°ûÊó∂Êâç‰ºöÈáçÊñ∞ÊâßË°å„ÄÇ global_planner_params.yamlÔºö default_toleranceÔºöÂΩìËÆæÁΩÆÁöÑÁõÆÁöÑÂú∞Ë¢´Âç†ÊçÆÊó∂Ôºå‰ª•ËØ•ÂèÇÊï∞‰∏∫ÂçäÂæÑÁöÑËåÉÂõ¥ÂÜÖÈÄâÂèñÊúÄËøëÁöÑÁÇπ‰Ωú‰∏∫Êñ∞ÁõÆÊ†áÁÇπ„ÄÇ dwa_local_planner_params.yamlÔºö latch_xy_goal_toleranceÔºöÂ¶ÇÊûúËÆæÁΩÆ‰∏∫trueÔºåËææÂà∞xy_goal_tolerance‰ª•ÂÜÖÊú∫Âô®‰∫∫Â∞±‰ºöÂéüÂú∞ÊóãËΩ¨ÔºåÂç≥‰Ωø‰ºöËΩ¨Âá∫ÂÆπÈîôÂúàÂ§ñ„ÄÇ sim_granularityÔºöÈó¥ÈöîÂ∞∫ÂØ∏ÔºåËΩ®Ëøπ‰∏äÈááÊ†∑ÁÇπÊ≠•Èïø„ÄÇ scaling_speedÔºöÂêØÂä®Êú∫Âô®‰∫∫Â∫ïÁõòÁöÑÈÄüÂ∫¶„ÄÇ global_costmap_params.yamlÔºö raytrace_rangeÔºöÂÆûÊó∂Ê∏ÖÈô§‰ª£‰ª∑Âú∞Âõæ‰∏äÈöúÁ¢çÁâ©ÁöÑÊúÄÂ§ßËåÉÂõ¥ÔºåÊ∏ÖÈô§ÁöÑÊòØobstacle_layerÁöÑÊï∞ÊçÆ„ÄÇ part3 topics move_base &amp; move_base_simpleÔºö 12345678ros::NodeHandle action_nh("move_base");action_goal_pub_ = action_nh.advertise&lt;move_base_msgs::MoveBaseActionGoal&gt;("goal", 1);//we'll provide a mechanism for some people to send goals as PoseStamped messages over a topic//they won't get any useful information back about its status, but this is useful for tools//like nav_view and rvizros::NodeHandle simple_nh("move_base_simple");goal_sub_ = simple_nh.subscribe&lt;geometry_msgs::PoseStamped&gt;("goal", 1, boost::bind(&amp;MoveBase::goalCB, this, _1)); ‰πãÂâçÊü•ÁúãËäÇÁÇπÂõæÁöÑÊó∂ÂÄôÂèëÁé∞Ëøô‰∏§‰∏™ËäÇÁÇπÈÉΩÊèê‰æõgoalÔºå‰∏ÄÁõ¥Ê≤°ÊÉ≥ÈÄö‰∏§ËÄÖÁöÑÂÖ≥Á≥ªÔºåÂèëÁé∞‰ª£Á†ÅÊ≥®ÈáäÈáåÈù¢ÊúâÔºåmove_baseÁªßÊâø‰∫ÜactionlibÔºåÊúâÁä∂ÊÄÅÂèçÈ¶àÔºàËØ¶ËßÅwiki 1.1.2 ActionAPIÔºâÔºåmove_base_simpleÂ∞±ÊòØ‰∏Ä‰∏™publisherÔºàtopicÂèØ‰ª•Êù•Ëá™rvizÔºècmd lineÔºâ„ÄÇ /result ËÆ∞ÂΩï‰∫ÜGoal reached /feedback ËÆ∞ÂΩï‰∫ÜÊØè‰∏™Êó∂ÂàªÊú∫Âô®‰∫∫ÁöÑ‰ΩçÂßø /status ËÆ∞ÂΩï‰∫Ü‰ªªÂä°ËøõÁ®ãÔºàgoal accepted„ÄÅfailed„ÄÅabortingÔºâ /cancel Ê≤°echoÂá∫‰ø°ÊÅØÔºåÂ∫îËØ•‰∏é‰∏äÂ±ÇÂØπÊé• „ÄêÂÆöÁÇπÂ∑°Ëà™„ÄëÂè¶Â§ñÔºåÂÆöÁÇπÂ∑°Ëà™ÁöÑÊó∂ÂÄôÂ∞Üglobal_pathÁöÑbufferËÆæÁΩÆ‰∏∫nÂ∞±ÂèØ‰ª•ÊòæÁ§∫Â§öÊù°Ë∑ØÂæÑ‰∫Ü„ÄÇ DWAPlannerÁöÑglobal_plan &amp; local_planÔºö local_planÂ∞±ÊòØDWAÁÆóÊ≥ïÊØè‰∏™Êó∂ÂàªËÆ°ÁÆóÁöÑÊúÄ‰ºòÈ¢ÑÊúüË∑ØÂæÑ„ÄÇglobal_planÊòØÊï¥‰∏™Â±ÄÈÉ®‰ª£‰ª∑Âú∞Âõæ‰∏äÁöÑË∑ØÂæÑÔºåÂÆÉÊòØÂÖ®Â±ÄË∑ØÂæÑÁöÑcropÔºåÂõ†‰∏∫Â±ÄÈÉ®Âä®ÊÄÅÁéØÂ¢É‰∏ç‰ºöÂΩ±ÂìçÂÖ®Â±ÄË∑ØÂæÑÔºåÊàë‰ª¨Âè™Á†îÁ©∂ËêΩÂú®localmap‰ª•ÂÜÖËøô‰∏ÄÊÆµË∑ØÂæÑÊòØÂê¶ÈúÄË¶ÅÁü´Ê≠£„ÄÇ]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Autolabor]]></title>
    <url>%2F2018%2F10%2F15%2Fautolabor%2F</url>
    <content type="text"><![CDATA[‰ªäÂ§©ÂèëÁé∞‰∫Ü‰∏Ä‰∏™ÊîØÊåÅ‰∫åÊ¨°ÂºÄÂèëÁöÑÂºÄÊ∫êÊñπÊ°àÔºåËØ¥ÁôΩ‰∫ÜÂ∞±ÊòØÊääkarto„ÄÅacml„ÄÅnavigation stackÁ≠âÂá†‰∏™ROSÂºÄÊ∫êÂåÖÊï¥ÂêàÁöÑÊØîËæÉÊºÇ‰∫ÆÔºå‰ª£Á†ÅÁªìÊûÑÂÄºÂæóÂÄüÈâ¥„ÄÇ ÊâßË°åkeyboard_control‰πãÂâçË¶ÅÂÖàÊâßË°åËÑöÊú¨ÔºåÊ∑ªÂä†ÈîÆÁõò„ÄÇ ËÄÅASÂåÖÈáåÈù¢Ê≤°Êúâkeyboard controlÔºåÂèØ‰ª•ÊâßË°åteleop_twist_keyboardÂåÖ„ÄÇautolabor_fakeÊòØËôöÊãüÂ∞èËΩ¶ÁöÑdriverÔºåÂª∫Ê®°‰∫ÜÁîµÊú∫„ÄÅodomÁõ∏ÂÖ≥‰ø°ÊÅØÔºåËÆ¢ÈòÖcmd_vel‰ø°ÊÅØ„ÄÇÊéßÂà∂ÁúüÂÆûÂ∞èËΩ¶Êó∂Ëøô‰∏™ËäÇÁÇπË¶ÅÊõøÊç¢„ÄÇ baseÊòØÂØπÊú∫Âô®‰∫∫Â∫ïÁõòÁöÑ‰ªøÁúüÔºålaunchÊñá‰ª∂ÁöÑÈªòËÆ§Fixed FrameÊòØbase_linkÔºåÊÉ≥Ë¶ÅÊéßÂà∂Â∞èËΩ¶ËøêÂä®ÂèØ‰ª•Â∞ÜFrameÂàáÊàêreal_mapÊàñodom„ÄÇ todolist: ÂëΩ‰ª§Ë°åÈáåÈù¢ÊéßÂà∂ÈáèÁöÑÊòæÁ§∫‰∏çÂ§™Â•ΩÁúãÔºåÂèØ‰ª•Â∞ùËØïÂú®Ê∫êÊñá‰ª∂ÈáåÈù¢‰ºòÂåñÔºåÊ∑ªÂä†‰∫§‰∫íÊèêÁ§∫„ÄÇ stageÊòØÂØπÂú∫ÊôØÁöÑ‰ªøÁúüÔºåÂú∫ÊôØÁî±map_serverËØªÂèñÔºålaunch‰ª•ÂêéÂ∞±ËÉΩÊü•ÁúãÂΩìÂâçÂú∫ÊôØÂú∞Âõæ„ÄÇ rostopicÈáåÈù¢Êúâ‰∏Ä‰∏™initialpose‰ø°ÊÅØÔºåÁî±rvizÂèëÂ∏É„ÄÇrostopic listÈáåÈù¢Â•ΩÂ§ötopicÈÉΩÊòØrvizÂèëÂ∏ÉÁöÑÔºåÂú®displaysÊ†èÁõÆÈáåÈù¢ÂèñÊ∂àÂãæÈÄâÂ∞±‰∏ç‰ºöÂèëÂ∏É‰∫Ü„ÄÇ objectÊòØÂØπÈöúÁ¢çÁâ©ÁöÑ‰ªøÁúüÔºåË∞ÉÁî®stageÔºåÊ∑ªÂä†interactivemarkerÔºåÁÑ∂ÂêéÈÄâÊã©InteractÂ∑•ÂÖ∑ÔºåÁêÜËÆ∫‰∏äÂú∞Âõæ‰∏äÂ∫îËØ•Âá∫Áé∞ÈöúÁ¢çÁâ©Ôºå‰ΩÜÊòØÊàëÊ≤°ÊâæÂà∞„ÄÇ„ÄÇ„ÄÇÁä∂ÊÄÅÊòæÁ§∫waiting for tf info„ÄÇ ‰øÆÊ≠£ÔºöÊ∑ªÂä†ÁöÑmarkerË¶ÅÂú®topicÈáåÈù¢ÈÄâÊã©Ôºå‰∏çË¶ÅÂú®typeÊ†è‰∏ã„ÄÇÁÑ∂ÂêéÂú∞Âõæ‰∏äÊîæÂ•ΩÈöúÁ¢çÁâ©‰ª•ÂêéË¶ÅÂè≥ÈîÆapply„ÄÇ lidarÊòØÂØπÈõ∑ËææÁÇπ‰∫ëÁöÑ‰ªøÁúüÔºålaunch‰∏≠Áªô‰∫Ü‰∏Ä‰∏™lidarÂíåmapÁöÑÈùôÊÄÅtfÔºåÂÆûÈôÖ‰ΩøÁî®‰∏≠Â∫îËØ•ÁªôlidarÂíåbase_linkÁöÑ„ÄÇ todolist: mapÂíåreal_mapÁªôÁöÑÊúâÁÇπÊ∑∑‰π±ÔºåÊòéÂ§©‰ºöÁªü‰∏Ä‰∏Ä‰∏ã„ÄÇ ËÄÅASÂåÖcreate_map‰ªøÁúüËøáÁ®ã‰∏≠ÔºåÁî±‰∫éÂú∫ÊôØÊèê‰æõmapserverÂíåslamÁÆóÊ≥ïÂêåÊó∂publish‰∫Ü/mapËøô‰∏™topicÔºåË¶ÅËøõË°åÂå∫ÂàÜÔºåÂú®launchÊñá‰ª∂ÈáåÈù¢ÂØπÂÖ∂‰∏≠‰∏Ä‰∏™ËøõË°åremapÔºö 123&lt;node pkg="map_server" type="map_server" name="map_server" args="$(find simulation_launch)/map/MG_map.yaml"&gt; &lt;remap from="/map" to="real_map" /&gt;&lt;/node&gt; ËøôÊó∂rostopicÈáåÈù¢Â∞±‰ºöÂá∫Áé∞real_mapËøô‰∏™ËØùÈ¢òÔºå‰∏§‰∏™Âú∞ÂõæËÉΩÂ§üÂêåÊó∂ÊòæÁ§∫„ÄÇ ‰ª£Á†ÅËß£ÊûêÈ¶ñÂÖàÊòØsimulationÂåÖÔºö autolabor_descriptionÊ≤°Âï•Â•ΩËØ¥ÁöÑÔºåurdfÊñá‰ª∂ÈáåÈù¢ÂÆö‰πâ‰∫Ü‰∏Ä‰∏™robotÔºåÊï¥‰∏™Êú∫Âô®‰∫∫Ë¢´Ê∏≤ÊüìÊàê‰∫Ü‰∏Ä‰∏™base_linkÔºåÊ≤°ÊúâÂ≠êËäÇÁÇπÔºåÊáí„ÄÇ autolabor_fakeÂåÖÊòØÂ∫ïÁõòÈ©±Âä®ÔºåÊèê‰æõ‰∫Ü‰∏Ä‰∏™autolabor_fake_nodeËäÇÁÇπÔºåÂÖ∂ËÆ¢ÈòÖÁ±ªÂûã‰∏∫geometry_msgs/TwistÁöÑËØùÈ¢òcmd_velÔºå‰ø°ÊÅØÊù•Ê∫êÂèØ‰ª•ÊòØjoystickÔºèkeyboardÔºàtele_op_xxxÔºâÔºècmd lineÔºàrostopic pub xxxÔºâ„ÄÇÂÖ∂ÂèëÂ∏ÉÁ±ªÂûã‰∏∫nav_msgs/OdometryÁöÑËØùÈ¢òodom„ÄÇÂêåÊó∂ËØ•ËäÇÁÇπËøò‰ºöÂ∞Üodom frameÂà∞base_link frameÁöÑtransform‰ø°ÊÅØÊèê‰æõÁªôtf nodeÔºåÁî®Êù•tf_broadcast„ÄÇ lidar_simulationÂåÖËá™Ë∫´Êèê‰æõ‰∫Ü‰∏§‰∏™ËäÇÁÇπlidar_simulationÂíåobstacle_simulation„ÄÇ 3.1 LidarSimulation::getPoseËøô‰∏™ÂáΩÊï∞‰∏≠Êúâ‰∏ÄÊÆµ‰ª£Á†ÅÂºÄÂßãÊØîËæÉÂõ∞ÊÉëÔºö 1234567// ROS_INFO("roll and pitch and yaw and esp :%lf %lf %lf %lf", roll, pitch, yaw, esp); //esp=0.000001,r&amp;p=0.000000if (pow(roll,2) + pow(pitch,2) &gt; esp)&#123; start_angle = yaw + max_angle_; reverse = -1.0;&#125;else&#123; // default situation: start_angle = yaw + min_angle_; reverse = 1.0; ÈÄöÂ∏∏ÊÉÖÂÜµ‰∏ãÔºåÊàë‰ª¨ÈÉΩ‰ΩøÁî®Âè≥ÊâãÂùêÊ†áÁ≥ªÔºå‰∫åÁª¥Âπ≥Èù¢‰∏ãÔºåglobal_frame_Âà∞lidar_frame_ÁöÑÂùêÊ†áÂèòÊç¢transformÊ¨ßÊãâËßíÂΩ¢Âºè‰∏ãÁöÑrÂíåpËßíÂ∫îËØ•ÂßãÁªàÊòØ$0.0$Ôºåyaw‰ª£Ë°®‰∫ÜÊøÄÂÖâÈõ∑Ëææ$x$ËΩ¥ÁöÑÂèòÊç¢ÔºåÂä†‰∏ämin_angle_Â∞±ÂàáÊç¢Êàê‰∫ÜÊøÄÂÖâÂÖâÊùüÁöÑÂàùÂßãÂèëÂ∞ÑËßíÂ∫¶start_angle„ÄÇÂ¶ÇÊûúÂùêÊ†áÁ≥ªÂÆö‰πâÂèç‰∫ÜÔºår&amp;pÂ∞±Â∫îËØ•ÊúâÂÄºÔºåËøôÊó∂Âõ†‰∏∫ÂùêÊ†áËΩ¥ÂÆö‰πâÂèçËøáÊù•‰∫ÜÔºåÊøÄÂÖâÂÖâÊùüÁöÑÂàùÂßãÂèëÂ∞ÑËßíÂ∫¶Â∞±ÂèòÊàê‰∫Ü‰ªéÊ≠£ÊñπÂêë‰∏äÁöÑmax_angle_ÂºÄÂßãÁöÑ„ÄÇ 3.2 LidarSimulation::updateMapËøô‰∏™ÂáΩÊï∞ÂÄºÂæóÊ≥®ÊÑèÔºåËøôÊòØ‰∏Ä‰∏™service clientÔºåÁî®Êù•Ë∞ÉÁî®Âú∞ÂõæÊõ¥Êñ∞ÔºåÂú®ÂΩìÂâçÂäüËÉΩÂåÖÁöÑÈªòËÆ§launchÊñá‰ª∂‰∏≠ÔºåÂè™Âä†ËΩΩ‰∫Ü‰∏ÄÊ¨°Âú∞ÂõæÔºåÊ≤°Êúâ‰ΩìÁé∞Âá∫ÂÆÉÁöÑ‰ΩúÁî®„ÄÇÂΩìÊâßË°åÂª∫Âõæ‰ªªÂä°Êó∂ÔºåÂõ†‰∏∫map frameÂíåodom frame‰ºö‰∏çÊñ≠ËøõË°åÁü´Ê≠£ÔºåÂª∫ÂõæÂåÖÂ∞±‰ºöcallËøô‰∏™requestÊù•ÂÆûÊó∂Êõ¥Êñ∞Âú∞Âõæ„ÄÇ 3.3 ËØ•ÂäüËÉΩÂåÖ‰∏ãËøòËá™ÂÆö‰πâ‰∫Ü‰∏Ä‰∏™obstacle serviceÔºåÊèê‰æõobstacle_simulationËäÇÁÇπÊù•Êõ¥Êñ∞ÈöúÁ¢çÁâ©‰ø°ÊÅØ„ÄÇËøôÈáåÁöÑÈöúÁ¢çÁâ©ÊòØÊåáÊâãÂä®Ê∑ªÂä†ÁöÑÈöúÁ¢çÁâ©ÔºàinteractiveMarkerÔºâÔºålaunchÊñá‰ª∂‰∏≠ÂèØ‰ª•ÂÆö‰πâÂÖ∂ÂΩ¢Áä∂È°∂ÁÇπ„ÄÇ ‚Äã ÂÖ∂‰∏≠ÁöÑObstacleSimulation::pnpolyÂáΩÊï∞Áî®Êù•Âà§Êñ≠ÊüêÁÇπÊòØÂê¶ËêΩÂú®Â§öËæπÂΩ¢ÂÜÖÔºå‰πãÂâçÂà∑ÁÆóÊ≥ïÊó∂ÊúâËÄÉËôëËøáËøô‰∏™ÈóÆÈ¢òÔºåËøôÈáåÁªôÂá∫ÁöÑËß£Ê≥ï‰∏çÁü•ÈÅìÊòØ‰∏çÊòØÊúÄ‰ºòÁöÑÔºåjust for recordÔºö 1234567891011bool ObstacleSimulation::pnpoly(geometry_msgs::Polygon&amp; footprint, float&amp; x, float&amp; y)&#123; int i,j; bool c = false; for (i=0, j=footprint.points.size()-1; i&lt;footprint.points.size(); j = i++)&#123; if ( ( (footprint.points.at(i).y &gt; y) != (footprint.points.at(j).y &gt; y) ) &amp;&amp; (x &lt; (footprint.points.at(j).x-footprint.points.at(i).x) * (y-footprint.points.at(i).y) / (footprint.points.at(j).y - footprint.points.at(i).y) + footprint.points.at(i).x) )&#123; c = !c; &#125; &#125; return c;&#125; ËøôÈáåÈù¢ÁöÑforÔºåÂæ™ÁéØÊù°‰ª∂ÈÅçÂéÜÁöÑÊòØÂ§öËæπÂΩ¢ÁöÑÊØè‰∏ÄÊù°ËæπÔºåÂ¶Ç$(3,0), (0,1), (1,2), (2, 3)$ËøôÊ†∑„ÄÇÂà§ÂÆöÁöÑÊòØÁªôÂÆöÁÇπÊòØÂê¶ËêΩÂú®ÁªôÂÆöËæπÁöÑÂÜÖ‰æßÔºåËøôÈáåÊâÄË∞ìÁöÑÂÜÖ‰æßÊòØ‰ª•ÁªôÂÆöËæπÁöÑËµ∑ÂßãËäÇÁÇπ‰∏∫ÂéüÁÇπÔºåÁªôÂÆöÁ∫øÊÆµÁöÑÈ°∫Êó∂ÈíàÊñπÂêë„ÄÇ 3.4 obstacleÁöÑÂÖ∑‰ΩìÊìç‰ΩúÂÆö‰πâÂú®static_map‰∏≠ÔºåËøôÈáåÈù¢Âá∫Áé∞‰∫Ü‰∏ñÁïåÂùêÊ†áÁ≥ªWorldÔºåÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊ†ÖÊ†ºÂú∞ÂõæÁöÑÂéüÁÇπÂú®Âú∞ÂõæÁöÑ‰∏ÄËßíÔºåÊ†ÖÊ†ºÁöÑ‰ΩçÁΩÆÁî®Êï¥ÂûãÊù•Ë°®Á§∫ÔºåËÄå‰∏ñÁïåÂùêÊ†áÁ≥ª‰∏≠Ê†ÖÊ†ºÁöÑ‰ΩçÁΩÆÁî±ÂÖ∂‰∏≠ÂøÉÊù•Ë°®Á§∫Ôºå‰∏§ËÄÖÁõ∏Â∑Æ$0.5$‰∏™resolution„ÄÇlidar_simulationÈáåÈù¢ÂàõÂª∫‰∫Ü‰∏Ä‰∏™static_mapÂØπË±°map_Ôºå‰ª•ÂèäÂõûË∞ÉÂáΩÊï∞LidarSimulation::obstacleHandleServer„ÄÇ 3.5 lidar_simulationÂäüËÉΩÂåÖ‰∏≠ÁöÑËøô‰∏§‰∏™ËäÇÁÇπÔºölidar_simulationÊòØmapÁ∫ßÁöÑÔºåobstacle_simulationÊòØobstacleÁ∫ßÁöÑ„ÄÇ Êé•‰∏ãÊù•Áúãsimulation_launchÂåÖÔºö Ëøô‰∏™ÂåÖÈáåÈù¢Ê≤°ÊúâÊ∫ê‰ª£Á†ÅÔºåÂè™Êèê‰æõ‰∫ÜÂá†‰∏™launchÊñá‰ª∂ÔºåÁî®Êù•‰ªøÁúüÂá†Áßç‰∏çÂêåÁöÑÊÉÖÂÜµÔºö sim_move_simulation.launchÂ∞±ÊòØÁÆÄÂçïÁöÑÂ∫ïÁõòÊéßÂà∂ÔºåÊéßÂà∂Â∞èËΩ¶Âú®ÁªôÂÆöÂú∞Âõæ‰∏äËøêÂä®ÔºåÂêåÊó∂ÂèØËßÜÂåñÈõ∑ËææÁÇπ‰∫ë‰ø°ÊÅØ„ÄÇ create_map_simulation.launchÁî®Êù•Âª∫ÂõæÔºåÂú®Â∫ïÁõòÊéßÂà∂ÁöÑÂü∫Á°Ä‰∏äÔºåÂêØÂä®‰∫ÜÂª∫ÂõæÂäüËÉΩÂåÖ„ÄÇÂèëÂ∏ÉÈªòËÆ§ÂêçÂ≠ó‰∏∫/mapÁöÑtopicÔºåÂëΩ‰ª§Ë°åÊâßË°åmap_saver‰øùÂ≠ò„ÄÇ move_base_simulation.launchÁî®Êù•ÂØºËà™ÔºåÂú®Â∫ïÁõòÊéßÂà∂ÁöÑÂü∫Á°Ä‰∏äÔºåÂêØÂä®‰∫ÜÂØºËà™Â•ó‰ª∂acml&amp;move_baseÔºåËøôÊó∂Â∞èËΩ¶ÁöÑÂ∫ïÁõòÊéßÂà∂ËäÇÁÇπautolabor_fake_nodeËÆ¢ÈòÖÁöÑcmd_vel‰ø°ÊÅØ‰∏çÂÜçÊù•Ëá™teleop_keyboardÔºåËÄåÊòØÊù•Ëá™move_baseÁöÑËßÑÂàíÁªìÊûú„ÄÇ ÊúÄÂêéÊòØmove_base_simËøô‰∏™ÂäüËÉΩÂåÖÔºö ÊòØÁî®‰ΩúÁúüÂÆûÂ∫ïÁõòÊéßÂà∂ÁöÑÔºàÁõÆÊµãÂ∞±ÊòØÂØπROSÂºÄÊ∫êÁöÑmove_baseÂåÖÁöÑ‰∫åÊ¨°Â∞ÅË£ÖÔºåË≤å‰ººÂà†‰∫Ü‰∏Ä‰∫õ‰∏çÁî®ÁöÑÊèí‰ª∂ÔºâÔºåÂÖàskipÔºåÊé•‰∏ãÊù•Êàë‰ºöÁõ¥Êé•Ëß£ÊûêROSÂØºËà™Â•ó‰ª∂„ÄÇ]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sublimeÊó†Ê≥ïÂÆâË£ÖÊèí‰ª∂]]></title>
    <url>%2F2018%2F10%2F12%2Fsublime%E6%97%A0%E6%B3%95%E5%AE%89%E8%A3%85%E6%8F%92%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Package ControlÁöÑÈÖçÁΩÆÊñá‰ª∂‰∏≠Ê∑ªÂä†Ôºö 123456789101112131415161718"downloader_precedence":&#123; "linux": [ "curl", "urllib", "wget" ], "osx": [ "curl", "urllib" ], "windows": [ "wininet" ]&#125;, OSXÂíåubuntu‰∏ã‰∫≤ÊµãÂùáÊúâÊïà„ÄÇ]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VMwareÊâ©ÂÆπ]]></title>
    <url>%2F2018%2F10%2F10%2FVMware%E6%89%A9%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[‰πãÂâçÂàõÂª∫ËôöÊãüÊú∫Êó∂‰∏çÁü•ÈÅìÈúÄË¶ÅÂ§öÂ§ßÁ©∫Èó¥ÔºåÁªô‰∫Ü20GÔºåË£Ö‰∫ÜROSÁõ∏ÂÖ≥Â•ó‰ª∂‰πãÂêéÁ£ÅÁõò‰ΩøÁî®ÁéáÂà∞‰∫Ü90%„ÄÇ ÁÑ∂ÂêéÂú®Á°¨ÁõòËÆæÁΩÆÈáåÈù¢Ë∞ÉÊï¥Âà∞‰∫Ü60GÔºåÊàë‰ª•‰∏∫ËøôÊ†∑Â∞±ÂèØ‰ª•‰∫Ü„ÄÇ„ÄÇ„ÄÇ„ÄÇ too naive„ÄÇ„ÄÇ„ÄÇËøòÊòØË¶ÅÊâãÂä®ÈÖçÁΩÆ‰∏Ä‰∏ãÔºåÈúÄË¶ÅÂÆâË£ÖÂ∑•ÂÖ∑GpartedÔºö 1sudo apt-get install gparted ÁÑ∂ÂêéÂÖàÊäπÊéâextendedÂíåswap‰∏§‰∏™ÂàÜÂå∫ÔºåÁÑ∂ÂêéÂ∞±ÂèØ‰ª•resize‰∏ªÂàÜÂå∫‰∫ÜÔºåÁÑ∂ÂêéÂú®ÈáçÊñ∞ÂàõÂª∫ÈÇ£‰∏§‰∏™ÂàÜÂå∫Â∞±Â•Ω‰∫Ü„ÄÇ AttentionÔºöÁ£ÅÁõòÂè™ËÉΩÊâ©Â±ïÔºå‰∏çËÉΩÂèòÂ∞èÔºåÂõ†Ê≠§Âª∫ËÆÆÈÄêÊ∏êÊâ©Â±ï„ÄÇ]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSM, Correlative Scan Matching]]></title>
    <url>%2F2018%2F10%2F08%2FCSM%2F</url>
    <content type="text"><![CDATA[SLAMÂâçÁ´Ø‰∏ªË¶ÅËß£ÂÜ≥‰∏§‰∏™ÈóÆÈ¢òÔºå‰∏ÄÊòØ‰ªé‰∏çÂêåÊó∂ÂàªÁöÑ‰º†ÊÑüÂô®ËæìÂÖ•‰∏≠ËØÜÂà´Âá∫Âêå‰∏Ä‰∏™Âú∞ÂõæÁâπÂæÅÔºå‰∫åÊòØËÆ°ÁÆóÊØè‰∏™ÂΩìÂâçÊó∂ÂàªÊú∫Âô®‰∫∫Áõ∏ÂØπËØ•ÁâπÂæÅÁöÑ‰ΩçÂßø„ÄÇ vSLAMËÉΩÂ§üËΩªÊùæËß£ÂÜ≥ÂâçËÄÖÔºåLidarSLAMËß£ÂÜ≥ÂêéËÄÖÊó†ÂéãÂäõ„ÄÇÊú¨ÊñáËÆ®ËÆ∫ÊøÄÂÖâSLAMÂâçÁ´ØÈóÆÈ¢ò1‚Äî‚ÄîÁâπÂæÅÁÇπÂåπÈÖç„ÄÇÁõÆÂâç‰∏§Â§ßÂ∏∏Áî®ÊÄùË∑ØÔºöscan2scan‚Äî‚ÄîICPÔºåscan2map‚Äî‚ÄîCSM„ÄÇ basicÁöÑCSMÁÆóÊ≥ïÊÄùË∑ØÂ¶Ç‰∏ãÔºö‰ªéÂâç‰∏ÄÂ∏ßÊú∫Âô®‰∫∫‰ΩçÂßøÂºÄÂßãÔºåÂØªÊâæÊúÄ‰ºòÂàöÊÄßÂèòÊç¢Ôºå‰ΩøÈõ∑ËææÁÇπÂú®Ê†ÖÊ†ºÂú∞Âõæ‰∏≠ÁöÑ‰ΩçÁΩÆÂ∞ΩÈáèÂØπÂ∫î‰∫éÂç†ÊçÆÂ∫¶‰∏∫1ÁöÑÊ†ÖÊ†º„ÄÇ ‰∏∫‰∫Ü‰øùÊåÅÂÆö‰Ωç‰ø°ÊÅØÂéüÊúâÁöÑÊï∞‰ΩçÁ≤æÂ∫¶Ôºå‰ΩøÁî®ÂèåÁ∫øÊÄßÊèíÂÄºÊñπÊ≥ïÊù•Ëé∑ÂèñÈõ∑ËææÁÇπÁöÑÂú∞ÂõæÂÄº(Âç†ÊçÆÂ∫¶)ÔºåËÄå‰∏çÊòØ‰ΩøÁî®ÂÖ∂ÊâÄÂú®Ê†ÖÊ†ºÁöÑÂú∞ÂõæÂÄºÊù•Áõ¥Êé•ÂØπÂ∫îÔºö $Pm$ÊòØÈõ∑ËææÁÇπÔºå$P_{00}, P_{01}, P_{10}, P_{11}$ÊòØÈõ∑ËææÁÇπÈÇªËøëÁöÑÂõõ‰∏™Ê†ÖÊ†º‰∏≠ÂøÉÁÇπ„ÄÇ‰∫éÊòØÂæóÂà∞Âú∞ÂõæÂÄºÔºö \begin{align} M(P_m)& \approx \frac{y-y_0}{y_1-y_0}M(I_0) +\frac{y_1-y}{y_1-y_0}M(I_1)\\ & =\frac{y-y_0}{y_1-y_0} \left ( \frac{x-x_0}{x_1-x_0}M(P_{11}) + \frac{x_1-x}{x_1-x_0}M(P_{01})\right)\\ & +\frac{y_1-y}{y_1-y_0}\left ( \frac{x-x_0}{x_1-x_0}M(P_{10}) + \frac{x_1-x}{x_1-x_0}M(P_{00})\right) \end{align}Èõ∑ËææÁÇπÊâÄÂú®‰ΩçÁΩÆÁöÑÂú∞ÂõæÂÄºÂèòÂåñÊ¢ØÂ∫¶Ôºö \triangledown M(P_m) = \left [\frac{\delta M(P_m)}{\delta x}, \frac{\delta M(P_m)}{\delta y} \right]\\ \frac{\delta M(P_m)}{\delta x} =\frac{y-y_0}{(y_1-y_0)(x_1-x_0)}(M(P_{11})- M(P_{01})) +\frac{y_1-y}{(y_1-y_0)(x_1-x_0)}(M(P_{10})- M(P_{00}))\\ \frac{\delta M(P_m)}{\delta y} =\frac{x-x_0}{(y_1-y_0)(x_1-x_0)}(M(P_{11})- M(P_{10})) +\frac{x_1-x}{(y_1-y_0)(x_1-x_0)}(M(P_{01})- M(P_{00}))\\ËÆ∞ÂΩìÂâçÊó∂ÂàªÁöÑÂ∑≤ÊúâÂú∞Âõæ$M$ÔºåÂΩìÂâçÂ∏ßÂÖ±ËæìÂÖ•$n$‰∏™Èõ∑ËææÁÇπ$S_1, ‚Ä¶, S_n$ÔºåÂÖ∂ÂØπÂ∫î‰ΩçÁΩÆÁöÑÂç†ÊçÆÂ∫¶‰∏∫$M(S_k)$ÔºåÊúÄ‰ºòÂèòÊç¢ÂÆö‰πâ‰∏∫$\xi = (\Delta x, \Delta y, \psi)^T$ÔºåÂàôÊúÄ‰ºòÈóÆÈ¢òÁöÑÊúÄÂ∞è‰∫å‰πòÊèèËø∞‰∏∫Ôºö \xi = argmin_{\xi} \sum_{k=1}^n[1-M(S_k(\xi))]^2scan2mapÁöÑÈ≤ÅÊ£íÊÄßÊõ¥Âº∫Ôºå‰ΩÜÊòØÂÆûÊó∂ÊÄß‰∏äÊâì‰∫ÜÊäòÊâ£„ÄÇÂØπÊ≠§‰∏ªË¶ÅÊúâ‰∏§‰∏™ÊîπËøõÊé™ÊñΩÔºö‰∏ÄÊòØÂ±ÄÈÉ®ÊêúÁ¥¢Ôºå‰∫åÊòØÂàÜËæ®ÁéáÈáëÂ≠óÂ°î„ÄÇ ‰∏Ä„ÄÅÂ±ÄÈÉ®ÊêúÁ¥¢ ÂÆûÈôÖËÆ°ÁÆó‰∏≠‰ºöÈÄâÂÆö‰∏Ä‰∏™ÊêúÁ¥¢Âå∫Èó¥ÔºåÈÄöÂ∏∏Âè™Âú®‰∏ä‰∏ÄÊó∂ÂàªÂú∞Âõæ‰ΩçÁΩÆÁöÑÈôÑËøëÔºåÂØπÂÖ∂‰∏≠ÂåÖÂê´ÁöÑÂÖ®ÈÉ®ÂèØËÉΩ‰ΩçÂßøËøõË°åËØÑÂàÜ„ÄÇ ‰∏äÂºèÔºàÊúÄÂ∞è‰∫å‰πòË°®ËææÂºèÔºâÂè™ÂåÖÂê´‰∫ÜÈõ∑ËææÁ´ØÁÇπÔºåËÄÉËôëÂà∞‰º†ÊÑüÂô®ÁöÑÂô™ÁÇπÂΩ±ÂìçÔºåÂ±ÄÈÉ®ÊûÅÂÄºÂΩ±ÂìçÂ§ß„ÄÇ Âü∫‰∫éÊ®°ÁâàÁöÑÂåπÈÖçÔºöÈõ∑ËææÁ´ØÁÇπÂèäÂ∞ÑÁ∫øÊâÄÂú®Ê†ÖÊ†ºÊûÑÊàêÁöÑÂ§öËæπÂΩ¢Âå∫ÂüüÔºå‰ª•Ê≠§‰Ωú‰∏∫Â±ÄÈÉ®Âú∞ÂõæÔºåËøõË°åmap2mapÂåπÈÖç„ÄÇÂáèÂ∞ëÂ±ÄÈÉ®ÊûÅÂÄºÁöÑÂΩ±ÂìçÔºåÊèêÈ´òËÆ°ÁÆó‰ª£‰ª∑ÔºåÂêåÊó∂ËÄÉËôëÂà∞Âä®ÊÄÅÁõÆÊ†áÔºå‰ºöÂºïÂÖ•Êñ∞ÁöÑÂ±ÄÈÉ®ÊûÅÂÄº„ÄÇ LMÁÆóÊ≥ïÔºöËø≠‰ª£ÁöÑÊñπÂºèÊ±ÇËß£ÊúÄÂ∞è‰∫å‰πòÁöÑÊúÄ‰ºòËß£„ÄÇ ÂàÜÊîØÁïåÂÆöÁÆóÊ≥ïÔºöÂü∫‰∫éÂπøÂ∫¶‰ºòÂÖàÊêúÁ¥¢ÁöÑÁÆóÊ≥ïÔºåÈÄöËøáÂØπËß£Á©∫Èó¥ÊêúÁ¥¢Ê†ëÁöÑÂàÜÊîØËøõË°åÊâ©Â±ïÂíåÂâ™ÊûùÔºå‰∏çÊñ≠Ë∞ÉÊï¥ÊêúÁ¥¢ÊñπÂêëÔºåÂä†Âø´ÊâæÂà∞ÂÖ®Â±ÄÊúÄ‰ºòËß£ÁöÑÈÄüÂ∫¶„ÄÇÁïåÂÆöÊ†∏ÂøÉÔºöËã•ÂΩìÂâçÂàÜÊîØÁöÑ‰∏ãÁïå$C_{branch}$Â∞è‰∫éËß£Á©∫Èó¥‰∏äÁïå$C_{HB}$ÔºåÂàôËøõË°åÊãìÂ±ïÔºåÂê¶ÂàôËøõË°åË£ÅÂâ™ÔºåÁõ¥Ëá≥Âà∞ËææÂè∂Â≠êÁªìÁÇπÔºåÂç≥ÊâæÂà∞ÊúÄÂ∞è‰ª£‰ª∑Ëß£„ÄÇ ‰∫å„ÄÅÂ§öÂàÜËæ®ÁéáÈáëÂ≠óÂ°î ‰∏§Â∏ßÈõ∑ËææÁÇπ‰∫ëÁöÑÁõ∏‰ººÂå∫ÂüüÂπ∂‰∏ç‰ºöÂΩ±ÂìçÂåπÈÖçÁöÑÊúÄÁªàÁªìÊûúÔºå‰ΩÜ‰ºöÂèÇ‰∏éËÆ°ÁÆóÔºåÂØºËá¥ÊêúÁ¥¢ÊïàÁéáÈôç‰ΩéÔºåÈúÄË¶ÅÊõ¥Â§öÁöÑËø≠‰ª£Ê¨°Êï∞ËææÂà∞Êî∂Êïõ„ÄÇ ÂΩìÂú∞ÂõæÂàÜËæ®ÁéáËæÉ‰ΩéÊó∂ÔºåÈÉ®ÂàÜÂú∞Âõæ‰ø°ÊÅØ‰ºöË¢´ÂøΩÁï•ÔºåËøôÁßçÈ´ò„ÄÅ‰ΩéÂàÜËæ®Áéá‰∏ãÁöÑÂ∑ÆÂºÇÔºåÊúâÂä©‰∫éÂØπÂú∞Âõæ‰∏≠ÁöÑÁõ∏‰ººÂú∫ÊôØËøõË°åÂå∫ÂàÜ„ÄÇÂÆûÈôÖ‰ΩøÁî®‰∏≠ÔºåÈ¶ñÂÖàÂ∞ÜÂàùÂßã‰ΩçÂßøÂØπÂ∫îÁöÑÈõ∑ËææÁÇπ‰∫ë‰∏éÊúÄ‰∏äÂ±ÇÔºàÁ≤óÂàÜËæ®ÁéáÔºâÁöÑÂú∞ÂõæËøõË°åÂåπÈÖçÔºåËÆ°ÁÆóÂá∫ÂΩìÂâçÂàÜËæ®Áéá‰∏ãÁöÑ‰ΩçÂßøÔºåÂπ∂‰Ωú‰∏∫ÂàùÂßãÂÄºËøõÂÖ•Ê¨°‰∏ÄÁ∫ßÂú∞ÂõæËøõË°åÂåπÈÖçÔºå‰ª•Ê≠§Á±ªÊé®„ÄÇ]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode95-‰∫åÂèâÊêúÁ¥¢Ê†ë]]></title>
    <url>%2F2018%2F08%2F16%2Fleetcode95-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%2F</url>
    <content type="text"><![CDATA[96Âíå95ÈÉΩÊòØ‰∫åÂèâÊêúÁ¥¢Ê†ëÔºåÂÖàÂÅöÁöÑ96ÔºåÊ±ÇÊ†ëÁöÑÁªìÊûÑÊúâÂá†ÁßçÔºåÊ≤°Ê≥®ÊÑèÁªìÁÇπÂ§ßÂ∞èÂÖ≥Á≥ªÔºåÁî®Âä®ÊÄÅËßÑÂàíÊù•ÂÅöÔºå$dp[i] = dp[0]*dp[i-1] + ‚Ä¶ + dp[i-1]*dp[0]$„ÄÇÊ≥®ÊÑèÈÄíÂΩíË∞ÉÁî®ÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶ÔºåËá™Â∫ïÂêë‰∏äÊù•ÁÆóÔºö 1234567891011def numTrees(self, n): res = [1, 1] if n &lt; 2: return res[n] res += [0]*(n-1) for i in range(2, n+1): for j in range(i): res[i] += res[j]*res[i-1-j] return res[n] 95Ë¶ÅÂàóÂá∫ÁªìÊûÑ‰∫ÜÔºåÊâçÂèëÁé∞‰ªÄ‰πàÊòØ‰∫åÂèâÊêúÁ¥¢Ê†ëÊù•ÁùÄ‚Äî‚ÄîÂ∑¶Â≠êÊ†ëÁöÑÁªìÁÇπÂÄºÂùáÂ∞è‰∫éÊ†πËäÇÁÇπÔºåÂè≥Â≠êÊ†ëÁöÑÁªìÁÇπÂÄºÂùáÂ§ß‰∫éÊ†πËäÇÁÇπ„ÄÇÊåâÁÖßÊÉØ‰æãÔºåÊ±ÇÊï∞ÈáèÁî®DPÔºåÊ±ÇÊûö‰∏æÂàôÁî®DFSÔºö ÈÅçÂéÜÊØè‰∏Ä‰∏™Êï∞Â≠ó$i$‰Ωú‰∏∫Ê†πËäÇÁÇπÔºåÈÇ£‰πà$[1, 2, ‚Ä¶, i-1]$ÊûÑÊàêÂÖ∂Â∑¶Â≠êÊ†ëÔºå$[i+1, i+2, ‚Ä¶, n]$ÊûÑÊàêÂÖ∂Âè≥Â≠êÊ†ë„ÄÇ 123456789101112131415161718192021def generateTrees(self, n): if n == 0: return [] return self.dfs(1,n)def dfs(self, b, e): if b &gt; e: return [None] res = [] for i in range(b, e+1): # set as the root node leftTrees = self.dfs(b, i-1) rightTrees = self.dfs(i+1, e) for left in leftTrees: for right in rightTrees: root = TreeNode(i) root.left = left root.right = right res.append(root) return res]]></content>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Áä∂ÊÄÅ‰º∞ËÆ°]]></title>
    <url>%2F2018%2F06%2F30%2F%E7%8A%B6%E6%80%81%E4%BC%B0%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[1. Ê¶ÇËø∞ ÁªèÂÖ∏ÁöÑSLAMÊ®°ÂûãÁî±‰∏Ä‰∏™ËøêÂä®ÊñπÁ®ãÂíå‰∏Ä‰∏™ËßÇÊµãÊñπÁ®ãÊûÑÊàêÔºö \left\{ \begin{split} & x_k = f(x_{k-1}, u_k) + w_k\\ & z_{k,j} = h(y_i, x_k) + v_{kj} \end{split} \right.ÂÖ∂‰∏≠Ôºå$x_k$Ë°®Á§∫Áõ∏Êú∫ÁöÑ‰ΩçÂßøÔºåÂèØ‰ª•Áî®ÂèòÊç¢Áü©ÈòµÊàñÊùé‰ª£Êï∞Êù•Ë°®Á§∫Ôºå$y_i$Ë°®Á§∫Ë∑ØÊ†áÔºå‰πüÂ∞±ÊòØÂõæÂÉè‰∏≠ÁöÑÁâπÂæÅÁÇπÔºå$w_k$Âíå$v_{kj}$Ë°®Á§∫Âô™Â£∞È°πÔºåÂÅáËÆæÊª°Ë∂≥Èõ∂ÂùáÂÄºÁöÑÈ´òÊñØÂàÜÂ∏É„ÄÇ Êàë‰ª¨Â∏åÊúõÈÄöËøáÂ∏¶Âô™Â£∞ÁöÑËßÇÊµãÊï∞ÊçÆ$z$ÂíåËæìÂÖ•Êï∞ÊçÆ$u$Êé®Êñ≠‰ΩçÂßø$x$ÂíåÂú∞Âõæ$y$ÁöÑÊ¶ÇÁéáÂàÜÂ∏É„ÄÇ‰∏ªË¶ÅÈááÁî®‰∏§Â§ßÁ±ªÊñπÊ≥ïÔºå‰∏ÄÁ±ªÊòØÊª§Ê≥¢ÊñπÊ≥ïÔºöÂü∫‰∫éÂΩìÂâçÁä∂ÊÄÅÊù•‰º∞ËÆ°‰∏ã‰∏ÄÁä∂ÊÄÅÔºåÂøΩÁï•ÂéÜÂè≤Ôºõ‰∏ÄÁ±ªÊòØÈùûÁ∫øÊÄß‰ºòÂåñÊñπÊ≥ïÔºå‰ΩøÁî®ÊâÄÊúâÊó∂ÂàªÁöÑÊï∞ÊçÆ‰º∞ËÆ°Êñ∞Áä∂ÊÄÅÁöÑÊúÄ‰ºòÂàÜÂ∏É„ÄÇ Êª§Ê≥¢ÊñπÊ≥ï‰∏ªË¶ÅÂàÜ‰∏∫Êâ©Â±ïÂç°Â∞îÊõºÊª§Ê≥¢ÂíåÁ≤íÂ≠êÊª§Ê≥¢‰∏§Â§ßÁ±ª„ÄÇ ÈùûÁ∫øÊÄß‰ºòÂåñÊ†πÊçÆÂÆûÁé∞ÁªÜËäÇÁöÑ‰∏çÂêå‰∏ªË¶ÅÂàÜ‰∏∫ÊªëÂä®Á™óÂè£Ê≥ïÂíåPose GraphÊ≥ï„ÄÇ 2. ÈùûÁ∫øÊÄß‰ºòÂåñ ÈùûÁ∫øÊÄß‰ºòÂåñÂü∫‰∫éÂéÜÂè≤ÔºåÂêåÊó∂‰πü‰ΩúÁî®‰∫éÂéÜÂè≤ÔºåÂõ†Ê≠§ÊääÊâÄÊúâÂæÖ‰º∞ËÆ°ÁöÑÂèòÈáèÊîæÂú®‰∏Ä‰∏™Áä∂ÊÄÅÂèòÈáè‰∏≠Ôºö x = \{x_1, ..., x_N, y_1, ..., y_M\}Âú®Â∑≤Áü•ËßÇÊµãÊï∞ÊçÆ$z$ÂíåËæìÂÖ•Êï∞ÊçÆ$u$ÁöÑÊù°‰ª∂‰∏ãÔºåÂØπÊú∫Âô®‰∫∫ÁöÑÁä∂ÊÄÅ‰º∞ËÆ°Ôºö P(x | z,u)ÂÖàÂøΩÁï•ÊµãÈáèËøêÂä®ÁöÑ‰º†ÊÑüÂô®Ôºå‰ªÖËÄÉËôëÊµãÈáèÊñπÁ®ãÔºåÊ†πÊçÆË¥ùÂè∂ÊñØÊ≥ïÂàôÔºö P(x|z) = \frac{P(z|x)P(x)}{P(z)} \varpropto P(z|x)P(x) ÂÖàÈ™åÊ¶ÇÁéá$P(x)$ÔºöÂÖàÈ™åÁöÑÊ¶ÇÂøµÊúÄÂ•ΩÁêÜËß£ÔºåÂ∞±ÊòØ‰∏Ä‰∏™‰∫ã‰ª∂ÁöÑÊ¶ÇÁéáÂàÜÂ∏É„ÄÇ ‰ººÁÑ∂Ê¶ÇÁéá$P(z|x)$ÔºöÂ∑≤Áü•‰∫ã‰ª∂ÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÔºå‰∫ã‰ª∂‰∏≠ÊüêÁä∂ÊÄÅÁöÑÊ¶ÇÁéá„ÄÇ ÂêéÈ™åÊ¶ÇÁéá$P(x|z)$ÔºöÂú®ÁªôÂÆöÊï∞ÊçÆÊù°‰ª∂‰∏ãÔºå‰∏çÁ°ÆÂÆöÊÄßÁöÑÊù°‰ª∂ÂàÜÂ∏É„ÄÇ $Posterior \varpropto Likelihood * Prior$ Ê±ÇËß£ÂêéÈ™åÂàÜÂ∏ÉÊØîËæÉÂõ∞ÈöæÔºå‰ΩÜÊòØÊ±Ç‰∏Ä‰∏™Áä∂ÊÄÅÊúÄ‰ºò‰º∞ËÆ°Ôºà‰ΩøÂæóÂêéÈ™åÊ¶ÇÁéáÊúÄÂ§ßÂåñÔºâÊòØÂèØË°åÁöÑÔºö x^*_{MAP} = argmaxP(x|z) = argmaxP(z|x)P(x) = argmaxP(z|x)Âõ†‰∏∫ÂÖàÈ™åÊ¶ÇÁéá‰∏çÁü•ÈÅìÔºåÊâÄ‰ª•ÈóÆÈ¢òÁõ¥Êé•ËΩ¨Êàê‰∏∫Ê±ÇËß£ÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÔºåÈóÆÈ¢ò‰∏≠ÁöÑÊú™Áü•Êï∞ÊòØ$x$ÔºåÁõ¥ËßÇÊÑè‰πâÂ∞±ÊòØÔºöÂØªÊâæ‰∏Ä‰∏™ÊúÄ‰ºòÁöÑÁä∂ÊÄÅÂàÜÂ∏ÉÔºå‰ΩøÂÖ∂ÊúÄÂèØËÉΩ‰∫ßÁîüÂΩìÂâçËßÇÊµãÂà∞ÁöÑÊï∞ÊçÆ„ÄÇ ÂÅáËÆæ‰∫ÜÂô™Â£∞È°π$v_{kj} \thicksim N(0, Q_{k,j})$ÔºåÊâÄ‰ª•ÊûÅÂ§ß‰ººÁÑ∂Ê¶ÇÁéá‰πüÊúç‰ªé‰∏Ä‰∏™È´òÊñØÂàÜÂ∏ÉÔºö P(z_{kj}|x_k, y_j) = N(h (y_j, x_i), Q)Ê±ÇÈ´òÊñØÂàÜÂ∏ÉÁöÑÊúÄÂÄºÈÄöÂ∏∏ÂèñË¥üÂØπÊï∞Â§ÑÁêÜÔºåÊúÄÂ§ßÂåñÂèòÊàêÊ±ÇÊúÄÂ∞èÂåñÔºö P(x) = \frac{1}{\sqrt{(2\pi)^Ndet(\Sigma)}} exp\bigg(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\bigg)\\ -ln(P(x) ) = \frac{1}{2}ln\big((2\pi)^Ndet(\Sigma)\big) + \frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)ÂØπÊï∞Á¨¨‰∏ÄÈ°π‰∏é$x$Êó†ÂÖ≥ÔºåÁ¨¨‰∫åÈ°πÁ≠â‰ª∑‰∫éÂô™Â£∞ÁöÑÂπ≥ÊñπÈ°π„ÄÇÂõ†Ê≠§ÂèØ‰ª•ÂæóÂà∞‰ºòÂåñÈóÆÈ¢òÁöÑÁõÆÊ†áÂáΩÊï∞Ôºö e_{k} = x_k - f(x_{k-1}, u_k)\\ e_{kj} = z_{kj} - h(x_k, y_j)\\ J(x) = \Sigma e_k^TR^{-1}_ke_{k} + \Sigma e_{kj}^TQ^{-1}_ke_{kj}\\ x* = argmin_x J(x)‰ª•‰∏äÁöÑÊúÄÂ∞è‰∫å‰πòÈóÆÈ¢òÂèØ‰ª•ÈááÁî®ÂêÑÁßçÂêÑÊ†∑ÁöÑÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÊ±ÇËß£ÊúÄ‰ºòËß£ÔºàÂèÇËÄÉÂõæ‰ºòÂåñÔºâ„ÄÇ Bundle Ajustment ‰ºòÂåñÈóÆÈ¢òÊúÄÁªàÂèØ‰ª•Ë°®Á§∫Êàê$H\Delta x = g$ÁöÑÂΩ¢ÂºèÔºåÂÖ∂ÂØπËßíÁ∫ø‰∏äÁöÑ‰∏§‰∏™Áü©Èòµ‰∏∫Á®ÄÁñèÁü©ÈòµÔºå‰∏îÂè≥‰∏ãËßíÁöÑÁü©ÈòµÁª¥Â∫¶ÂæÄÂæÄËøúÂ§ß‰∫éÂ∑¶‰∏äËßíÔºàÂõ†‰∏∫ÁâπÂæÅÁÇπÁöÑÊï∞ÁõÆËøúÂ§ß‰∫é‰ΩçÂßøËäÇÁÇπÔºâÔºö H = \begin{bmatrix} B &E\\ E^T & C \end{bmatrix}‰∏Ä‰∏™ÊúâÊïàÁöÑÊ±ÇËß£ÊñπÂºèÁß∞‰∏∫SchurÊ∂àÂÖÉÔºå‰πüÂè´ËæπÁºòÂåñMarginalizationÔºå‰∏ªË¶ÅÊÄùË∑ØÂ¶Ç‰∏ãÔºöÈ¶ñÂÖàÊ±ÇËß£$C$Áü©ÈòµÁöÑÈÄÜÁü©ÈòµÔºåÁÑ∂ÂêéÂØπ$H$Áü©ÈòµËøõË°åÊ∂àÂÖÉÔºåÁõÆÊ†áÊòØÊ∂àÂéªÂÖ∂Âè≥‰∏äËßíÁöÑ$E$Áü©ÈòµÔºåËøôÊ†∑Â∞±ËÉΩÂ§üÂÖàÁã¨Á´ãÊ±ÇËß£Áõ∏Êú∫ÂèÇÊï∞$\Delta x_c$ÔºåÂÜçÂà©Áî®Ê±ÇÂæóÁöÑËß£Êù•Ê±ÇlandmarksÂèÇÊï∞$\Delta x_p$Ôºö \begin{bmatrix} I &-EC^{-1}\\ 0 & I \end{bmatrix} \begin{bmatrix} B &E\\ E^T & C \end{bmatrix} \begin{bmatrix} \Delta x_c \\ \Delta x_p \end{bmatrix} = \begin{bmatrix} I &-EC^{-1}\\ 0 & I \end{bmatrix} \begin{bmatrix} v \\ w \end{bmatrix} \\ \begin{bmatrix} B - EC^{-1}E^T & 0\\ E^T & C \end{bmatrix}\begin{bmatrix} \Delta x_c \\ \Delta x_p \end{bmatrix} = \begin{bmatrix} v - EC^{-1}w\\ w \end{bmatrix}Âõ†Ê≠§ÂèØ‰ª•Ëß£Âæó$\Delta x_c$Ôºö \begin{bmatrix} B - EC^{-1}E^T \end{bmatrix} \Delta x_c = v - EC^{-1}w Ëøô‰∏™Áü©ÈòµÁß∞‰∏∫Ê∂àÂÖÉ‰πãÂêéÁöÑ$S$Áü©ÈòµÔºåÂÆÉÁöÑÁª¥Â∫¶‰∏éÁõ∏Êú∫ÂèÇÊï∞ÁöÑÁª¥Â∫¶‰∏ÄËá¥ $S$Áü©ÈòµÁöÑÊÑè‰πâÊòØ‰∏§‰∏™Áõ∏Êú∫ÂèòÈáè‰πãÈó¥ÊòØÂê¶Â≠òÂú®ÁùÄÂÖ±ÂêåËßÇÊµãÁÇπ $S$Áü©ÈòµÁöÑÁ®ÄÁñèÊÄßÁî±ÂÆûÈôÖÊï∞ÊçÆÊÉÖÂÜµÂÜ≥ÂÆöÔºåÂõ†Ê≠§Âè™ËÉΩÈÄöËøáÊôÆÈÄöÁöÑÁü©ÈòµÂàÜËß£ÁöÑÊñπÂºèÊù•Ê±ÇËß£ Ê†∏ÂáΩÊï∞ ÂΩìËØØÂ∑ÆÂæàÂ§ßÊó∂Ôºå‰∫åËåÉÊï∞Â¢ûÈïøÁöÑÂæàÂø´Ôºå‰∏∫‰∫ÜÈò≤Ê≠¢ÂÖ∂ËøáÂ§ßÊé©ÁõñÊéâÂÖ∂‰ªñÁöÑËæπÔºåÂèØ‰ª•Â∞ÜÂÖ∂ÊõøÊç¢ÊàêÂ¢ûÈïøÊ≤°ÈÇ£‰πàÂø´ÁöÑÂáΩÊï∞Ôºå‰ΩøÂæóÊï¥‰∏™‰ºòÂåñÁªìÊûúÊõ¥‰∏∫Á®≥ÂÅ•ÔºåÂõ†Ê≠§ÂèàÂè´È≤ÅÊ£íÊ†∏ÂáΩÊï∞ÔºåÂ∏∏Áî®ÁöÑÊ†∏ÊúâHuberÊ†∏„ÄÅCauchyÊ†∏„ÄÅTukeyÊ†∏Á≠âÔºåHuberÊ†∏ÁöÑÂÆö‰πâÂ¶Ç‰∏ãÔºö H(e) = \left\{ \begin{split} & \frac{1}{2} e ^2, \ \ \ |e| \leq \delta\\ & \delta(|e| - \frac{1}{2}\delta), \ \ else \end{split} \right. 3. Âç°Â∞îÊõºÊª§Ê≥¢Êª§Ê≥¢ÊÄùË∑ØÂü∫‰∫é‰∏Ä‰∏™ÈáçË¶ÅÁöÑÂÅáËÆæÔºö‰∏ÄÈò∂È©¨Â∞îÂèØÂ§´ÊÄß‚Äî‚ÄîkÊó∂ÂàªÁä∂ÊÄÅÂè™‰∏ék-1Êó∂ÂàªÁä∂ÊÄÅÊúâÂÖ≥ÔºåÊï¥ÁêÜÊàê‰∏§‰∏™Ë¶ÅÁ¥†Â¶Ç‰∏ãÔºö $x_{k-1}$ contains the whole history $x_k = f(x_{k-1}, u_k, z_k)$ Âú®ËøôÈáåÊàë‰ª¨Âè™ÈúÄË¶ÅÁª¥Êä§‰∏Ä‰∏™Áä∂ÊÄÅÈáè$x_k$ÔºåÂπ∂ÂØπÂÆÉ‰∏çÊñ≠ËøõË°åËø≠‰ª£Êõ¥Êñ∞ÔºåmoreoverÔºåÂ¶ÇÊûúÁä∂ÊÄÅÈáèÊúç‰ªéÈ´òÊñØÂàÜÂ∏ÉÔºåÊàë‰ª¨Âè™ÈúÄË¶ÅÁª¥Êä§Áä∂ÊÄÅÈáèÁöÑÂùáÂÄºÂíåÊñπÂ∑ÆÂç≥ÂèØÔºàËøõ‰∏ÄÊ≠•ÁÆÄÂåñÔºâ„ÄÇ È¶ñÂÖàËÄÉËôë‰∏Ä‰∏™Á∫øÊÄßÁ≥ªÁªüÔºö \left\{ \begin{split} & x_k = A_k x_{k-1}+u_k + w_k\\ & z_k = C_k x_k + v_k \end{split} \right.\\ w_k \thicksim N(0, R), v_k \thicksim N(0, Q)Âç°Â∞îÊõºÊª§Ê≥¢Âô®ÁöÑÁ¨¨‰∏ÄÊ≠•È¢ÑÊµãÔºåÈÄöËøáËøêÂä®ÊñπÁ®ãÁ°ÆÂÆö$x_k$ÁöÑÂÖàÈ™åÂàÜÂ∏ÉÔºåÊ≥®ÊÑèÁî®‰∏çÂêåÁöÑ‰∏äÊ†áÂå∫ÂàÜ‰∏çÂêåÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÔºöÂ∞ñÂ∏ΩÂ≠ê$\hat x_k$Ë°®Á§∫ÂêéÈ™åÔºåÂπ≥Â∏ΩÂ≠ê$\bar x_k$Ë°®Á§∫ÂÖàÈ™åÔºö P(\bar x_k) = N(\bar x_k, \bar P_k)\\ \bar x_k = A_k \hat x_{k-1} + u_k\\ \bar P_k = A_k\hat P_{k-1}A_k^T+RÁ¨¨‰∫åÊ≠•‰∏∫ËßÇÊµãÔºåÈÄöËøáÂàÜÊûêÂÆûÈôÖËßÇÊµãÂÄºÔºåËÆ°ÁÆóÂú®ÊüêÁä∂ÊÄÅ‰∏ãÂ∫îËØ•‰∫ßÁîüÊÄéÊ†∑ÁöÑÂàÜÂ∏ÉÔºö P(z_k|x_k) = N(C_kx_k, Q)Á¨¨‰∏âÊ≠•‰∏∫Êõ¥Êñ∞ÔºåÊ†πÊçÆÁ¨¨‰∏ÄËäÇ‰∏≠ÁöÑË¥ùÂè∂ÊñØÊ≥ïÂàôÔºåÂæóÂà∞$x_k$ÁöÑÂêéÈ™åÂàÜÂ∏ÉÔºö (\hat x_k, \hat P_k) = N(C_kx_k, Q)N(\bar x_k, \bar P_k)\\ K = \bar P_k C_k^T(C_k\bar P_k C_k^T + Q_k)^{-1}\\ \hat x_k = \bar x_k + K(z_k-C_k\bar x_k)\\ \hat P_k = (I - KC_k)\bar P_kÊï¥‰ΩìÁöÑÊµÅÁ®ãÂõæÂ¶Ç‰∏ãÔºö ÂÖ∑‰ΩìËøáÁ®ãÊú¨ËäÇ‰∏≠‰∏çÂÅöÂ±ïÂºÄÔºåËØ¶ÊÉÖÂèØ‰ª•ÂèÇËÄÉÂç°Â∞îÊõºÊª§Ê≥¢„ÄÇÈ´òÊñØÂàÜÂ∏ÉÁªèËøáÁ∫øÊÄßÂèòÊç¢‰ªçÁÑ∂Êúç‰ªéÈ´òÊñØÂàÜÂ∏ÉÔºåÂõ†Ê≠§Êï¥‰∏™ËøáÁ®ãÊ≤°ÊúâÂèëÁîü‰ªª‰ΩïÁöÑËøë‰ººÔºåÂõ†Ê≠§ÂèØ‰ª•ËØ¥Âç°Â∞îÊõºÊª§Ê≥¢Âô®ÊûÑÊàê‰∫ÜÁ∫øÊÄßÁ≥ªÁªüÁöÑÊúÄ‰ºòÊó†ÂÅè‰º∞ËÆ°„ÄÇ ‰∏ãÈù¢ËÄÉËôëÈùûÁ∫øÊÄßÁ≥ªÁªüÔºö SLAM‰∏≠‰∏çÁÆ°ÊòØ‰∏âÁª¥ËøòÊòØÂπ≥Èù¢Âàö‰ΩìËøêÂä®ÔºåÂõ†‰∏∫ÈÉΩÂºïÂÖ•‰∫ÜÊóãËΩ¨ÔºåÂõ†Ê≠§ÂÖ∂ËøêÂä®ÊñπÁ®ãÂíåËßÇÊµãÊñπÁ®ãÈÉΩÊòØÈùûÁ∫øÊÄßÂáΩÊï∞„ÄÇ‰∏Ä‰∏™È´òÊñØÂàÜÂ∏ÉÔºåÁªèËøáÈùûÁ∫øÊÄßÂèòÊç¢ÔºåÈÄöÂ∏∏Â∞±‰∏çÂÜçÊúç‰ªéÈ´òÊñØÂàÜÂ∏ÉÔºåÂõ†Ê≠§ÂØπ‰∫éÈùûÁ∫øÊÄßÁ≥ªÁªüÔºåÂøÖÈ°ªÈááÂèñ‰∏ÄÂÆöÁöÑËøë‰ººÔºåÂ∞Ü‰∏Ä‰∏™ÈùûÈ´òÊñØÂàÜÂ∏ÉËøë‰ººÊàêÈ´òÊñØÂàÜÂ∏É„ÄÇ ÈÄöÂ∏∏ÁöÑÂÅöÊ≥ïÊòØÔºåÂ∞ÜkÊó∂ÂàªÁöÑËøêÂä®ÊñπÁ®ãÂíåËßÇÊµãÊñπÁ®ãÂú®$\hat x_{k-1}$Ôºå$\hat P_{k-1}$Â§ÑÂÅö‰∏ÄÈò∂Ê≥∞ÂãíÂ±ïÂºÄÔºåÂæóÂà∞‰∏§‰∏™ÈõÖÂèØÊØîÁü©ÈòµÔºö F = \frac{\partial f}{\partial x_{k-1}}\bigg|_{\hat x_{k-1}}\\ H = \frac{\partial h}{\partial x_k}\bigg|_{\hat x_k}‰∏≠Èó¥ÈáèÂç°Â∞îÊõºÂ¢ûÁõä$K_k$Ôºö \bar P_k = F\hat P_{k-1}F^T + R_k\\ K_k = \bar P_k H^T(H \bar P_k H^T + Q_k)^{-1}ÂêéÈ™åÊ¶ÇÁéáÔºö \hat x_k = \bar x_k + K_k(z_k - h(\bar x_k))\\ \hat P_k = (I - K_k H)\bar P_k ÂØπ‰∫éSLAMËøôÁßçÈùûÁ∫øÊÄßÁöÑÊÉÖÂÜµÔºåEKFÁªôÂá∫ÁöÑÊòØÂçïÊ¨°Á∫øÊÄßËøë‰ºº‰∏ãÁöÑÊúÄÂ§ßÂêéÈ™å‰º∞ËÆ°ÔºàMAPÔºâ„ÄÇ 4. EKF VS Graph-Optimization È©¨Â∞îÂèØÂ§´ÊÄßÊäõÂºÉ‰∫ÜÊõ¥‰πÖ‰πãÂâçÁöÑÁä∂ÊÄÅÔºå‰ºòÂåñÊñπÊ≥ïÂàôËøêÁî®‰∫ÜÊõ¥Â§öÁöÑ‰ø°ÊÅØ„ÄÇ ÈùûÁ∫øÊÄßËØØÂ∑ÆÔºö‰∏§ÁßçÊñπÊ≥ïÈÉΩ‰ΩøÁî®‰∫ÜÁ∫øÊÄßÂåñËøë‰ººÔºåEKFÂè™Âú®$x_{k-1}$Â§ÑÂÅö‰∫Ü‰∏ÄÊ¨°Á∫øÊÄßÂåñÔºåÂõæ‰ºòÂåñÊ≥ïÂàôÂú®ÊØè‰∏ÄÊ¨°Ëø≠‰ª£Êõ¥Êñ∞Êó∂ÈÉΩÂØπÊñ∞ÁöÑÁä∂ÊÄÅÁÇπÂÅöÊ≥∞ÂãíÂ±ïÂºÄÔºåÂÖ∂Á∫øÊÄßÂåñÁöÑÊ®°ÂûãÊõ¥Êé•ËøëÂéüÂßãÈùûÁ∫øÊÄßÊ®°Âûã„ÄÇ Â≠òÂÇ®ÔºöEKFÁª¥Êä§ÁöÑÊòØÁä∂ÊÄÅÁöÑÂùáÂÄºÂíåÊñπÂ∑ÆÔºåÂ≠òÂÇ®Èáè‰∏éÁä∂ÊÄÅÁª¥Â∫¶ÊàêÂπ≥ÊñπÂ¢ûÈïøÔºåÂõæ‰ºòÂåñÂ≠òÂÇ®ÁöÑÊòØÊØè‰∏™Áä∂ÊÄÅÁÇπÁöÑ‰ΩçÂßøÔºåÂ≠òÂÇ®Á∫øÊÄßÂ¢ûÈïø„ÄÇ]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[‰ºòÂåñÂ∫ìÔºöCeres & g2o]]></title>
    <url>%2F2018%2F06%2F28%2F%E4%BC%98%E5%8C%96%E5%BA%93%EF%BC%9Ag2o-Ceres%2F</url>
    <content type="text"><![CDATA[Ceres‰ΩøÁî®CeresÊ±ÇËß£ÈùûÁ∫øÊÄß‰ºòÂåñÈóÆÈ¢òÔºå‰∏ªË¶ÅÂàÜ‰∏∫‰∏â‰∏™ÈÉ®ÂàÜÔºö Á¨¨‰∏ÄÈÉ®ÂàÜÔºöÊûÑÂª∫‰ª£‰ª∑ÂáΩÊï∞Cost_Functor 12345678// ÂÆö‰πâ‰∏Ä‰∏™ÂÆû‰æãÂåñÊó∂ÊâçÁü•ÈÅìÁöÑÁ±ªÂûãTtemplate &lt;typename T&gt;// ËøêÁÆóÁ¨¶()ÁöÑÈáçËΩΩÔºåÁî®Êù•ÂæóÂà∞ÊÆãÂ∑Æfibool operator()(const T* const x, T* residual) const &#123; residual[0] = T(10.0) - x[0]; return true; &#125; Á¨¨‰∫åÈÉ®ÂàÜÔºöÊûÑÂª∫ÊúÄÂ∞è‰∫å‰πòÈóÆÈ¢òproblem 12345Problem problem;// ‰ΩøÁî®Ëá™Âä®Ê±ÇÂØºÔºåÁ¨¨‰∏Ä‰∏™1ÊòØËæìÂá∫Áª¥Â∫¶ÔºàÊÆãÂ∑ÆÈ°πÔºâÔºåÁ¨¨‰∫å‰∏™1ÊòØËæìÂÖ•Áª¥Â∫¶Ôºà‰ºòÂåñÈ°πÔºâCostFunction* cost_function = new AutoDiffCostFunction&lt;CostFunctor, 1, 1&gt;(new CostFunctor);// Ê∑ªÂä†ËØØÂ∑ÆÈ°πÔºåNULLË°®Á§∫‰∏ç‰ΩøÁî®Ê†∏ÂáΩÊï∞ÔºåxÊòØ‰ºòÂåñÈ°πproblem.AddResidualBlock(cost_function, NULL, &amp;x); Á¨¨‰∏âÈÉ®ÂàÜÔºöÊ±ÇËß£Âô®ÂèÇÊï∞ÈÖçÁΩÆSolver 1234567Solver::Options options;options.linear_solver_type = ceres::DENSE_QR; //ÈÖçÁΩÆÂ¢ûÈáèÊñπÁ®ãÁöÑËß£Ê≥ïÔºåÁ®†ÂØÜÁöÑQRÂàÜËß£options.minimizer_progress_to_stdout = true;//ËæìÂá∫Âà∞coutSolver::Summary summary;//‰ºòÂåñ‰ø°ÊÅØSolve(options, &amp;problem, &amp;summary);//Ê±ÇËß£cout &lt;&lt; summary.BriefReport() &lt;&lt; "\n";//ËæìÂá∫‰ºòÂåñÁöÑÁÆÄË¶Å‰ø°ÊÅØ ‰ΩøÁî®Ê†∏ÂáΩÊï∞ÔºöÊï∞ÊçÆ‰∏≠ÂæÄÂæÄÂ≠òÂú®Á¶ªÁæ§ÁÇπÔºåÁ¶ªÁæ§ÁÇπ‰ºöÂØπÂØª‰ºòÁªìÊûúÈÄ†ÊàêÂΩ±ÂìçÔºåËøôÊó∂ÂèØ‰ª•‰ΩøÁî®‰∏Ä‰∫õÊçüÂ§±Ê†∏ÂáΩÊï∞Êù•ÂØπÁ¶ªÁæ§ÁÇπÁöÑÂΩ±ÂìçÂä†‰ª•Ê∂àÈô§ÔºåCeresÂ∫ì‰∏≠Êèê‰æõÁöÑÊ†∏ÂáΩÊï∞‰∏ªË¶ÅÊúâÔºöTrivialLoss „ÄÅHuberLoss„ÄÅ SoftLOneLoss „ÄÅ CauchyLoss„ÄÇ 12// ‰ΩøÁî®Ê†∏ÂáΩÊï∞problem.AddResidualBlock(cost_function, new CauchyLoss(0.5, &amp;x); g2oÁî®g2o‰ºòÂåñÂ∫ìÊù•ËøõË°å‰ºòÂåñÁöÑÊ≠•È™§Â¶Ç‰∏ãÔºö ÂÆö‰πâËäÇÁÇπÂíåËæπÁöÑÁ±ªÂûãÔºåÈÄöÂ∏∏Âú®ÈªòËÆ§ÁöÑÂü∫Á°ÄÁ±ªÂûã‰∏äÂÅö‰øÆÊîπ ÂÆö‰πâÈ°∂ÁÇπÔºåÈ°∂ÁÇπÁöÑÂü∫Á±ª‰∏∫g2o::BaseVertex&lt;‰ºòÂåñÂèòÈáèÁª¥Â∫¶ÔºåÊï∞ÊçÆÁ±ªÂûã&gt; 1class CurveFittingVertex: public g2o::BaseVertex&lt;3, Eigen::Vector3d&gt; È°∂ÁÇπÁöÑÊõ¥Êñ∞ÂáΩÊï∞oplusImplÔºöÂÆö‰πâÂ¢ûÈáèÂä†Ê≥ïÔºåÂõ†‰∏∫‰ºòÂåñÂèòÈáèÂíåÂ¢ûÈáè‰πãÈó¥Âπ∂‰∏ç‰∏ÄÂÆöÊòØÁ∫øÊÄßÂè†Âä†ÁöÑÂÖ≥Á≥ªÔºåÂ¶Ç‰ΩçÂßøÂèòÊç¢„ÄÇ ÂÆö‰πâËæπÔºå Êú¨‰æã‰∏≠ÁöÑËæπ‰∏∫‰∏ÄÂÖÉËæπÔºåÂü∫Á±ª‰∏∫g2o::BaseUnaryEdge&lt;ËßÇÊµãÂÄºÁª¥Â∫¶ÔºåÊï∞ÊçÆÁ±ªÂûãÔºåËøûÊé•È°∂ÁÇπÁ±ªÂûã&gt; 1class CurveFittingEdge: public g2o::BaseUnaryEdge&lt;1, double , CurveFittingVertex&gt; ËØØÂ∑ÆÈ°πËÆ°ÁÆóÂáΩÊï∞computeErrorÔºöËÆ°ÁÆóÈ¢ÑÊµãÂÄºÂíåËßÇÊµãÂÄºÁöÑËØØÂ∑Æ„ÄÇ‰º∞ËÆ°ÂÄºÊòØÂü∫‰∫éÂΩìÂâçÂØπ‰ºòÂåñÂèòÈáèÁöÑestimateËÆ°ÁÆóÂá∫ÁöÑÔºåËßÇÊµãÂÄºÊòØÁõ¥Êé•Ëé∑ÂèñÁöÑÔºåÂ¶ÇÊú¨‰æã‰∏≠ÁöÑyÂÄº„ÄÇ ÊûÑÂª∫ÂõæÊ®°Âûã 123456789101112131415// vertexg2o::VertexSE3Expmap* pose = new g2o::VertexSE3Expmap(); // camera posepose-&gt;setId( index );pose-&gt;setEstimate( expression );optimizer.addVertex ( pose );// edgeg2o::EdgeProjectXYZ2UV* edge = new g2o::EdgeProjectXYZ2UV();edge-&gt;setId ( index );edge-&gt;setVertex ( 0, point );edge-&gt;setVertex ( 1, pose );edge-&gt;setMeasurement ( Eigen::Vector2d ( p.x, p.y ) ); // ÂØºÂÖ•ËßÇÊµãÂÄºedge-&gt;setParameterId ( 0,0 );edge-&gt;setInformation ( Eigen::Matrix2d::Identity() ); // ËÆæÁΩÆ‰ø°ÊÅØÁü©Èòµoptimizer.addEdge ( edge ); ‰ø°ÊÅØÁü©Èòµedge-&gt;setInformation(‰ø°ÊÅØÁü©Èòµ)ÔºöÂõ†‰∏∫ÊúÄÁªàÁöÑ‰ºòÂåñÂáΩÊï∞ÊòØ$\sum e_i^T \Sigma^{-1}e_i$ÔºåÊòØËØØÂ∑ÆÈ°πÂíå‰ø°ÊÅØÁü©Èòµ‰πòÁßØÁöÑÂΩ¢Âºè„ÄÇ ‰ºòÂåñÂô®ÈÖçÁΩÆ Áü©ÈòµÂùóBlock ‰ºòÂåñÁÆóÊ≥ïsolver ÂõæÊ®°Âûãoptimizer ÊâßË°å‰ºòÂåñ]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Áõ¥Êé•Ê≥ï]]></title>
    <url>%2F2018%2F06%2F16%2F%E7%9B%B4%E6%8E%A5%E6%B3%95%2F</url>
    <content type="text"><![CDATA[ÂâçÈù¢ËØ¥ÂÆå‰∫Ü PnPÔºåË∂ÅÁÉ≠ÊâìÈìÅÊõ¥Êñ∞Áõ¥Êé•Ê≥ïÔºåÂõ†‰∏∫‰∏§ËÄÖÁöÑÊÄùË∑ØÂü∫Êú¨‰∏ÄËá¥Ôºå‰∏ªË¶ÅÁöÑÂ∑ÆÂà´Âú®‰∫éPnP‰∏≠Âà©Áî®ÁöÑÊòØÁâπÂæÅÁÇπÁöÑÈáçÊäïÂΩ±ËØØÂ∑Æ‚Äî‚ÄîÂåπÈÖçÁÇπÂú®queryÂ∏ßÂÉèÁ¥†Âπ≥Èù¢‰∏äÁöÑÂÆûÈôÖ‰ΩçÁΩÆÂíå‰º∞ËÆ°‰ΩçÁΩÆÁöÑËØØÂ∑ÆÔºåÁõ¥Êé•Ê≥ï‰∏çÊèêÂèñÁâπÂæÅÁÇπÔºåËÄåÊòØÈááÁî®ÂÉèÁ¥†‰∫ÆÂ∫¶ËØØÂ∑Æ„ÄÇ 1. Áõ¥Êé•Ê≥ïÁöÑÊé®ÂØº‰ª•Á¨¨‰∏Ä‰∏™Áõ∏Êú∫‰∏∫ÂèÇËÄÉÁ≥ªÔºåÁ¨¨‰∫å‰∏™Áõ∏Êú∫ÁöÑËøêÂä®ÂèÇÊï∞‰∏∫$R, t, \xi$ÔºåÂØπÊüê‰∏™Á©∫Èó¥ÁÇπ$P$Ôºö p_1 = \begin{bmatrix} u_1\\ v_1\\ 1 \end{bmatrix} = \frac{1}{Z_1}KP\\ p_2 = \begin{bmatrix} u_2\\ v_2\\ 1 \end{bmatrix} = \frac{1}{Z_2}K(RP+t) = \frac{1}{Z_2}Kexp(\xi^{\wedge})P‰∏§‰∏™ÂÉèÁ¥†ÁÇπÁöÑ‰∫ÆÂ∫¶ËØØÂ∑ÆÔºö e = I_1(p_1) - I_2(p_2)ÁõÆÊ†áÂáΩÊï∞Ôºö min_{\xi} J(\xi) = \sum_{i=1}^N||e_i^Te_i||^2_2ËØØÂ∑ÆÂáΩÊï∞ÂÖ≥‰∫é‰ºòÂåñÂèòÈáèÁöÑÂØºÊï∞Ôºö \begin{split} e(\xi + \delta \xi)& = I_1\big(\frac{1}{Z_1}KP\big) - I_2\big(\frac{1}{Z_2}K(exp(\delta \xi^{\wedge})exp(\xi^{\wedge})P\big)\\ & \approx I_1\big(\frac{1}{Z_1}KP\big) - I_2\big(\frac{1}{Z_2}K(1+\delta \xi^{\wedge})exp(\xi^{\wedge})P\big)\\ & = I_1\big(\frac{1}{Z_1}KP\big) - I_2\big(\frac{1}{Z_2}Kexp(\xi^{\wedge})P\big) - I_2\big(\frac{1}{Z_2}K\delta \xi^{\wedge} exp(\xi^{\wedge})P \big)\\ & = e(\xi) - I_2\big(\frac{1}{Z_2}K\delta \xi^{\wedge} exp(\xi^{\wedge})P \big) \end{split}‰∏äÈù¢ÁöÑÊâ∞Âä®Áõ∏ÂÖ≥È°π‰∏≠ÔºåËÆ∞Ôºö q = \delta \xi^{\wedge} exp(\xi^{\wedge})P\\ u = \frac{1}{Z_2}Kq\\ËØØÂ∑ÆÂáΩÊï∞Á∫øÊÄßÂåñÔºö e(\xi + \delta \xi) = e(\xi) - I_2(u)\\ \therefore \frac{e(\xi + \delta \xi)}{\partial \delta \xi} = \frac{-I_2(u)}{\partial \delta \xi}\\ e(\xi + \delta \xi) =e(\xi) - (\frac{\partial I_2}{\partial u}\frac{\partial u}{\partial q}\frac{\partial q}{\partial \delta \xi})\delta \xi$q$Ë°®Á§∫Êâ∞Âä®ÂàÜÈáèÂú®Á¨¨‰∫åÁõ∏Êú∫ÂùêÊ†áÁ≥ª‰∏ãÁöÑÂùêÊ†áÔºàÂõûÈ°æÂÖ≥‰∫é$R$ÁöÑÂæÆÂàÜÊñπÁ®ãÔºö$\dot R(t) = \phi_0^{\wedge}R(t)$ÔºâÔºåÂõ†Ê≠§$u$ÁöÑÊÑè‰πâ‰∏∫ÂÉèÁ¥†ÂùêÊ†áÔºå$\frac{\partial I_2}{\partial u}$ÁöÑÁâ©ÁêÜÊÑè‰πâ‰∏∫ÂÉèÁ¥†Ê¢ØÂ∫¶Ôºå$\frac{\partial u}{\partial q}$ÁöÑÁâ©ÁêÜÊÑè‰πâ‰∏∫ÂÉèÁ¥†ÂùêÊ†áÂÖ≥‰∫é‰∏âÁª¥ÁÇπÁöÑÂØºÊï∞ÔºàÂèÇËÄÉÈíàÂ≠îÁõ∏Êú∫Ê®°ÂûãÔºâÔºå$\frac{\partial q}{\partial \delta \xi}$ÁöÑÁâ©ÁêÜÊÑè‰πâ‰∏∫‰∏âÁª¥ÁÇπÂÖ≥‰∫éÊâ∞Âä®ÁöÑÂØºÊï∞ÔºàÂèÇËÄÉÊùé‰ª£Êï∞Ôºâ„ÄÇ 2. Áõ¥Êé•Ê≥ïÂàÜÁ±ªÊ†πÊçÆPÁöÑÊù•Ê∫êÔºåÁõ¥Êé•Ê≥ïÂàÜ‰∏∫‰∏âÁ±ªÔºö PÊù•Ëá™‰∫éÁ®ÄÁñèÂÖ≥ÈîÆÁÇπ‚Äî‚ÄîÁ®ÄÁñèÁõ¥Êé•Ê≥ï PÊù•Ëá™ÈÉ®ÂàÜÂÉèÁ¥†ÔºåÂè™‰ΩøÁî®Â∏¶ÊúâÊ¢ØÂ∫¶ÁöÑÂÉèÁ¥†ÁÇπ‚Äî‚ÄîÂçäÁ®†ÂØÜÁõ¥Êé•Ê≥ï P‰∏∫ÊâÄÊúâÂÉèÁ¥†‚Äî‚ÄîÁ®†ÂØÜÁõ¥Êé•Ê≥ï 3. ‰ª£Á†ÅÂÆûÁé∞‰∏ªË¶ÅÂÖ≥Ê≥®EdgeÁ±ªÈáåÈù¢ÈáçÂÆö‰πâÁöÑÂ¢ûÈáèÊõ¥Êñ∞ÂáΩÊï∞linearizeOplus()ÈáåÈù¢JacobianÁü©ÈòµÁöÑÂÜôÊ≥ï„ÄÇ \begin{split} J & = -\frac{\partial I_2}{\partial u}\frac{\partial u}{\partial q}\frac{\partial q}{\partial \delta \xi}\\ & = -\frac{\partial I_2}{\partial u}\frac{\partial u}{\partial \delta \xi}\\ \end{split} Ââç‰∏ÄÈ°πÊòØ$u$Â§ÑÁöÑÂÉèÁ¥†Ê¢ØÂ∫¶Ôºå‰ΩøÁî®Êï∞ÂÄºÂØºÊï∞Ôºö 1234// jacobian from I to u (1*2)Eigen::Matrix&lt;double, 1, 2&gt; jacobian_pixel_uv;jacobian_pixel_uv ( 0,0 ) = ( getPixelValue ( u+1,v )-getPixelValue ( u-1,v ) ) /2;jacobian_pixel_uv ( 0,1 ) = ( getPixelValue ( u,v+1 )-getPixelValue ( u,v-1 ) ) /2; getPixelValueËøô‰∏™ÂáΩÊï∞Ê∂âÂèäÂà∞‰∏Ä‰∏™ÂèåÁ∫øÊÄßÊèíÂÄºÔºåÂõ†‰∏∫‰∏äÈù¢ÁöÑ‰∫åÁª¥ÂùêÊ†áuvÊòØÈÄöËøáÁõ∏Êú∫ÊäïÂΩ±ÂèòÊç¢ÂæóÂà∞ÁöÑÔºåÊòØÊµÆÁÇπÂΩ¢ÂºèÔºåËÄåÂÉèÁ¥†ÂÄºÊòØÁ¶ªÊï£ÁöÑÊï¥Êï∞ÂÄºÔºå‰∏∫‰∫ÜÊõ¥Á≤æÁªÜÂú∞Ë°®Á§∫ÂÉèÁ¥†‰∫ÆÂ∫¶ÔºåË¶ÅÂØπÂõæÂÉèËøõË°åËøõË°åÊèíÂÄº„ÄÇ Á∫øÊÄßÊèíÂÄºÔºöÂ∑≤Áü•Êï∞ÊçÆ$(x_0, y_0)$Âíå$(x_1, y_1)$ÔºåË¶ÅËÆ°ÁÆó$[x_0, x_1]$Âå∫Èó¥ÂÜÖ‰ªª‰∏Ä$x$ÂØπÂ∫îÁöÑ$y$ÂÄºÔºö \frac{y - y_0}{x-x_0} = \frac{y_1-y_0}{x_1-x_0}\\ \therefore y = \frac{x_1 -x}{x_1 - x_0}y_0 + \frac{x-x_0}{x_1-x_0}y_1 ÂèåÁ∫øÊÄßÊèíÂÄºÔºöÊú¨Ë¥®‰∏äÂ∞±ÊòØÂú®‰∏§‰∏™ÊñπÂêë‰∏äÂÅöÁ∫øÊÄßÊèíÂÄºÔºö È¶ñÂÖàÊòØxÊñπÂêëÔºö f(R_1) = \frac{x_2 - x}{x_2-x_1}f(Q_{11}) + \frac{x - x_1}{x_2-x_1}f(Q_{21}), \ where\ R_1 = (x, y_1)\\ f(R_2) = \frac{x_2 - x}{x_2-x_1}f(Q_{12}) + \frac{x - x_1}{x_2-x_1}f(Q_{22}), \ where\ R_2 = (x, y_2)ÁÑ∂ÂêéyÊñπÂêëÔºö f(P) = \frac{y_2 - y}{y_2 - y_1}f(R_1) + \frac{y-y_1}{y_2-y_1}f(R_2)ÁªºÂêàËµ∑Êù•Â∞±ÊòØÔºö f(x,y) = \frac{f(Q_{11})}{(x_2-x_1)(y_2-y_1)}(x_2-x)(y_2-y) + \frac{f(Q_{21})}{(x_2-x_1)(y_2-y_1)}(x-x_1)(y_2-y) \\ + \frac{f(Q_{12})}{(x_2-x_1)(y_2-y_1)}(x_2-x)(y-y_1) + \frac{f(Q_{22})}{(x_2-x_1)(y_2-y_1)}(x-x_1)(y-y_1) 1234567891011inline float getPixelValue ( float x, float y )&#123; uchar* data = &amp; image_-&gt;data[ int ( y ) * image_-&gt;step + int ( x ) ]; float xx = x - floor ( x ); float yy = y - floor ( y ); return float ( ( 1-xx ) * ( 1-yy ) * data[0] + xx* ( 1-yy ) * data[1] + ( 1-xx ) *yy*data[ image_-&gt;step ] + xx*yy*data[image_-&gt;step+1] ); Âêé‰∏§È°πÈÉΩÊòØ‰∏éÁõ∏Êú∫ÂèÇÊï∞Âíå‰∏âÁª¥ÁÇπÂùêÊ†áÊúâÂÖ≥ÔºåÂèØ‰ª•ÂêàÂπ∂ÔºåÂêåÊó∂Ê≥®ÊÑèg2o‰∏≠ÂØπSE3ÁöÑÂÆö‰πâÂπ≥ÁßªÂíåÊóãËΩ¨ÂíåÊú¨ÊñáËÆæÂÆöÊòØÂèçËøáÊù•ÁöÑ„ÄÇ \xi = \begin{bmatrix} \rho\\ \phi \end{bmatrix}\\ \frac{\partial u}{\partial \delta \xi}= \begin{bmatrix} \frac{f_x}{Z} & 0 & -\frac{f_xX}{Z^2} & |& -\frac{f_xXY}{Z^2} & f_x + \frac{f_xX^2}{Z^2} & -\frac{f_xY}{Z}\\ 0 & \frac{f_y}{Z} & -\frac{f_yY}{Z^2}& | & - f_y - \frac{f_xY^2}{Z^2} & \frac{f_yXY}{Z^2} & \frac{f_yX}{Z}\\ \end{bmatrix}12345678910111213141516// jacobian from u to xi (2*6)Eigen::Matrix&lt;double, 2, 6&gt; jacobian_uv_ksai;// xi = [\phi, \pho]jacobian_uv_ksai ( 0,0 ) = - x*y*invz_2 *fx_;jacobian_uv_ksai ( 0,1 ) = ( 1+ ( x*x*invz_2 ) ) *fx_;jacobian_uv_ksai ( 0,2 ) = - y*invz *fx_;jacobian_uv_ksai ( 0,3 ) = invz *fx_;jacobian_uv_ksai ( 0,4 ) = 0;jacobian_uv_ksai ( 0,5 ) = -x*invz_2 *fx_;jacobian_uv_ksai ( 1,0 ) = - ( 1+y*y*invz_2 ) *fy_;jacobian_uv_ksai ( 1,1 ) = x*y*invz_2 *fy_;jacobian_uv_ksai ( 1,2 ) = x*invz *fy_;jacobian_uv_ksai ( 1,3 ) = 0;jacobian_uv_ksai ( 1,4 ) = invz *fy_;jacobian_uv_ksai ( 1,5 ) = -y*invz_2 *fy_;]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenCVÂÆöÂà∂Ê∫êÁ†ÅÁºñËØë]]></title>
    <url>%2F2018%2F06%2F12%2FOpenCV%E5%AE%9A%E5%88%B6%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[1. cmakeÈÄâÈ°π ÊµãËØïÂçïÂÖÉÂèØ‰ª•ÂÖ≥ÊéâÔºöBUILD_DOCSÔºåBUILD_EXAMPLESÔºåBUILD_XXX_TESTSÔºåBUILD_opencv_ts(‰∏Ä‰∫õÂçïÂÖÉÊµãËØï‰ª£Á†Å)ÔºåBUILD_PACKAGE (CPACK_BINARY_XXXÔºåCPACK_SOURCE_XXX)ÔºåINSTALL_XXX ÂáèÂ∞ëÂºïÂÖ•‰ΩìÁßØÔºöÊâìÂºÄ‰∏ñÁïåÊ®°ÂùóÂºÄÂÖ≥BUILD_opencv_world(ÊöÇÊó∂Ê≤°ÂºÄÔºåÂõ†‰∏∫ÁºñËØë‰πãÂêéÂèëÁé∞Êâæ‰∏çÂà∞Ë¶ÅÂºïÁî®ÁöÑÂ§¥Êñá‰ª∂‰∫Ü)ÔºåÊâìÂºÄBUILD_SHARED_LIBS ÂÖ≥ÊéâÈü≥ËßÜÈ¢ëÂ§ÑÁêÜÁõ∏ÂÖ≥Ê®°ÂùóÔºöBUILD_opencv_videoÔºåBUILD_opencv_videoioÔºåBUILD_opencv_videostabÔºåWITH_1394ÔºåWITH_GSTREAMER_XXX ÂÖ≥Èó≠GPUÁõ∏ÂÖ≥Ê®°ÂùóÔºöWITH_OPENCLÔºåWITH_CUDA ÊâìÂºÄTBBÊ®°ÂùóÔºöÈöêÂºèÁöÑÂπ∂Ë°åËÆ°ÁÆóÁ®ãÂ∫èÔºåÂ∫ïÂ±Ç‰æùËµñ‰∫éÊìç‰ΩúÁ≥ªÁªüÁöÑÂ§öÁ∫øÁ®ãÂ∫ìÔºåBUILD_TBB ÊâìÂºÄvizÊ®°ÂùóÔºöWITH_VTKÔºåBUILD_opencv_viz ÊöÇÊó∂Ê≤°ÂºÄÂêØJavaÁõ∏ÂÖ≥Ê®°ÂùóÔºöantÔºåÂ∞±Ê≤°brewËøáËøô‰∏™ÂåÖ ‰ª•‰∏äreference from ÂçöÂÆ¢1ÔºåÂçöÂÆ¢2 2. extra modules with opencv3.0, SURF/SIFT and some other things have been moved to a seperate opencv_contrib repo. ‰∏ÄÈÉ®ÂàÜÊ®°ÂùóË¢´Áã¨Á´ãÂà∞‰∫Üopencv_contribËøô‰∏™ÂåÖÔºåÈ¶ñÂÖàcloneÂà∞Êú¨Âú∞ÔºåÁÑ∂ÂêéÂú®cmakeÈÄâÈ°πÈáåÈù¢ÊâæÂà∞OPENCV_EXTRA_MODULES_PATHÔºåÂ°´Â•Ω„ÄÇ 2. ÂÖ∂‰ªñËØ¥ÊòéÂè¶Â§ñ‰πãÂâçbrew installÁöÑopencvÂåÖ‰∏ÄÂÆöË¶ÅÂç∏ËΩΩÊéâÔºå‰∏çË¶Å‰π±linkÔºåÂê¶ÂàôINCLUDEÂíåLIBSÁöÑË∑ØÂæÑÈÉΩ‰ºöÂá∫ÈóÆÈ¢òÔºåÊâãÂä®‰øÆÊîπcmakeÊñá‰ª∂‰∏çË¶ÅÂ§™ÈÖ∏ÁàΩ„ÄÇ]]></content>
      <tags>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode 81 ÊêúÁ¥¢ÊóãËΩ¨Êï∞ÁªÑ2]]></title>
    <url>%2F2018%2F06%2F10%2Fleetcode-81-%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%842%2F</url>
    <content type="text"><![CDATA[ÂÆûÂêçdissËøôÈÅìÈ¢òÔºå‰∏ÄÁúãÊòØÊóãËΩ¨ÊéíÂ∫èÊï∞ÁªÑÁõ¥Êé•ÂÅö‰∫ÜÔºåÁÑ∂ÂêéÊâçÂèëÁé∞ÊµãËØïÊ†∑‰æãÈáåÈù¢Âá∫Áé∞‰∫ÜÂ∑¶Âè≥ËæπÁïåÈáçÂêàÁöÑÊÉÖÂÜµÔºåÁÑ∂Âêé‰ªîÁªÜÂÜçÂÆ°È¢òÊâçÂèëÁé∞ËøôË°åÂ∞èÂ≠óÔºö ‰∏çÂåÖÂê´ÈáçÂ§çÂÖÉÁ¥†ÁöÑÊÉÖÂÜµ‰∏ã‰ª£Á†ÅÂÆûÁé∞Â¶Ç‰∏ãÔºö 12345678910111213141516171819202122232425class Solution: def search(self, nums, target): size = len(nums) start = 0 end = size - 1 while start &lt;= end: mid = (start + end) // 2 if nums[mid] == target: return True if nums[mid] &lt;= nums[end]: if target &lt; nums[mid] or target &gt; nums[end]: end = mid - 1 else: start = mid + 1 else: if target &gt; nums[mid] or target &lt; nums[start]: start = mid + 1 else: end = mid - 1 return False Âõ†‰∏∫Êï∞ÁªÑ‰∏≠Áé∞Âú®Â≠òÂú®ÈáçÂ§çÁöÑÂÖÉÁ¥†ÔºåÂõ†Ê≠§Êúâ‰∏Ä‰∏™ÁâπÊÆäÁöÑÊÉÖÂÜµÔºöÂ∑¶Âè≥ËæπÁïåÂÄºÁõ∏ÂêåÔºåÂπ∂‰∏înums[mid]ÂÄº‰∏éËæπÁïåÂÄº‰πüÁõ∏ÂêåÔºåËøôÊó∂nums[mid]ÂèØËÉΩ‰Ωç‰∫é‰∏§ÊÆµÊï∞ÁªÑÁöÑ‰ªªÊÑè‰∏ÄËæπ„ÄÇÂõ†Ê≠§Ë¶ÅÁã¨Á´ãËÆ®ËÆ∫‰∏Ä‰∏ãËøô‰∏™ÊÉÖÂÜµÔºö 1234567891011121314151617181920212223242526272829class Solution: def search(self, nums, target): size = len(nums) start = 0 end = size - 1 while start &lt;= end: mid = (start + end) // 2 if nums[mid] == target: return True if nums[mid] &lt; nums[end]: if target &lt; nums[mid] or target &gt; nums[end]: end = mid - 1 else: start = mid + 1 elif nums[mid] &gt; nums[end]: if target &gt; nums[mid] or target &lt; nums[start]: start = mid + 1 else: end = mid - 1 # nums[mid] = nums[end]ÁöÑÊÉÖÂÜµ else: end -= 1 return False ÊµãËØïÁî®Êó∂50msÔºåÂõ†‰∏∫ËæπÁïåÈáçÂ§çÁöÑÂæ™ÁéØÊ≤°ÊúâÊúâÊïàÂú∞‰∫åÂàÜÊï∞ÁªÑÔºå‰ΩÜÊòØÊÄùË∑ØË¥ºÁÆÄÂçïÂïä„ÄÇ]]></content>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PnP, Perspective-n-Point]]></title>
    <url>%2F2018%2F06%2F07%2FPnP-Perspective-n-Point%2F</url>
    <content type="text"><![CDATA[1. Ê¶ÇËø∞ PnPÊòØÊ±ÇËß£3DÂà∞2DÁÇπÂØπËøêÂä®ÁöÑÊñπÊ≥ïÔºåÂÆÉÊèèËø∞‰∫ÜÂΩìÁü•ÈÅìn‰∏™3DÁ©∫Èó¥ÁÇπÂèäÂÖ∂ÊäïÂΩ±‰ΩçÁΩÆÊó∂ÔºåÂ¶Ç‰Ωï‰º∞ËÆ°Áõ∏Êú∫ÁöÑ‰ΩçÂßøÂèòÊç¢„ÄÇÊúÄÂ∞ëÂè™ÈúÄË¶Å3‰∏™ÁÇπÂØπÂ∞±ÂèØ‰ª•‰º∞ËÆ°Áõ∏Êú∫ÁöÑËøêÂä®„ÄÇ ËØ•ÊñπÊ≥ï‰ΩøÁî®ÁöÑÊù°‰ª∂ÊòØÔºåÂèÇËÄÉÁÇπ‰∏∫‰∏ñÁïåÂùêÊ†áÁ≥ª‰∏ãÁöÑÁâπÂæÅÁÇπÔºåÂÖ∂Á©∫Èó¥‰ΩçÁΩÆÂ∑≤Áü•ÔºåÂπ∂‰∏îÁü•ÈÅìqueryÂ∏ß‰∏≠ÂØπÂ∫îÂèÇËÄÉÁÇπÁöÑÂÉèÁ¥†ÂùêÊ†á„ÄÇ PnPÈóÆÈ¢òÁöÑÊ±ÇËß£ÊñπÊ≥ïÊúâÂæàÂ§öÁßçÔºö Áõ¥Êé•Á∫øÊÄßÂèòÊç¢ P3P ÈùûÁ∫øÊÄß‰ºòÂåñÊñπÊ≥ï‚Äî‚ÄîBundle Ajustment Êú¨ËäÇÁùÄÁúº‰∫éBAÊ±ÇËß£ÊñπÊ≥ïÔºåÂÖ∂‰ªñÊñπÊ≥ïÊöÇÊó∂‰∏çÂÅöÂ±ïÂºÄ„ÄÇ 2. Bundle AjustmentÂà©Áî®‰ºòÂåñÊ±ÇËß£ÁöÑÊÄùË∑ØÊòØÔºöÊúÄÂ∞èÂåñÈáçÊäïÂΩ±ËØØÂ∑Æ‚Äî‚ÄîÊúüÊúõËÆ°ÁÆóqueryÂ∏ßÁöÑÁõ∏Êú∫‰ΩçÂßø$R, t$ÔºåÂÆÉÁöÑÊùé‰ª£Êï∞‰∏∫$\xi$ÔºåÁ©∫Èó¥ÁâπÂæÅÁÇπÁöÑÂùêÊ†á‰∏∫$P_i = [X_i, Y_i, Z_i]^T$ÔºåÂÖ∂Âú®queryÂ∏ß‰∏äÁöÑÂÉèÁ¥†ÂùêÊ†á‰∏∫$u_i = [u_i, v_i]^T$ÔºåÈÇ£‰πàÁêÜËÆ∫‰∏äÔºö s_i u_i = K exp(\xi^{\wedge})P_iÊûÑÂª∫ÊàêÊúÄÂ∞è‰∫å‰πòÈóÆÈ¢òÂ∞±ÊòØÔºöÂØªÊâæÊúÄ‰ºòÁöÑÁõ∏Êú∫‰ΩçÂßø$\xi$Ôºå‰ΩøÂæóËØØÂ∑ÆÊúÄÂ∞èÂåñÔºö \xi^* = argmin_{\xi} \frac{1}{2}\sum_{i=1}^{n}||u_i -\frac{1}{s_i}Kexp(\xi^{\wedge})P_i||^2_2Âú®‰ΩøÁî®‰ºòÂåñÂ∫ìÊù•Ê±ÇËß£‰πãÂâçÔºåËøòÊúâ‰∏Ä‰∏™ÈóÆÈ¢ò‚Äî‚ÄîÊØè‰∏™ËØØÂ∑ÆÈ°π$e_i = u_i -\frac{1}{s_i}Kexp(\xi^{\wedge})P_i$ÁöÑÂØºÊï∞$J_i$„ÄÇ ÂõûÂøÜÂõæ‰ºòÂåñ‰∏≠ËÆ≤ËøáÁöÑÔºå‰ºòÂåñÈóÆÈ¢òÊúÄÁªàËΩ¨ÂåñÊàê‰∏∫Áü©ÈòµÁöÑÁ∫øÊÄßÊ±ÇËß£$H\Delta x = g$ÔºåÂÖ∂‰∏≠Áü©Èòµ$H$ÊòØÁî±Âçï‰∏™ËØØÂ∑ÆÈ°π‰∏ÄÈò∂Â±ïÂºÄ$e(x+\Delta x) = e(x) + J\Delta x$‰∏≠ÁöÑÈõÖÂèØÊØîÁü©Èòµ$J_i$ ÊûÑÊàêÁöÑÁ®ÄÁñèÂØπÁß∞Èòµ„ÄÇ ËØØÂ∑ÆÈ°πÊòØ‰∏Ä‰∏™‰∫åÁª¥ÂêëÈáèÔºàÂÉèÁ¥†ÂùêÊ†áÂ∑ÆÔºâÔºå‰ºòÂåñÂèòÈáèÊòØ‰∏Ä‰∏™ÂÖ≠Áª¥ÂêëÈáèÔºàÁ©∫Èó¥‰ΩçÂßøÊùé‰ª£Êï∞ÔºâÔºåÂõ†Ê≠§$J$ÊòØ‰∏Ä‰∏™2*6ÁöÑÁü©Èòµ„ÄÇ \frac{\partial e}{\partial \delta \xi} = \lim_{\delta \xi \rightarrow0} \frac{\partial e}{\partial P^{'}}\frac{\partial P^{'}}{\partial \delta \xi}ÂÖ∂‰∏≠$P^{‚Äò}$ÊòØÁâπÂæÅÁÇπËΩ¨Êç¢Âà∞Áõ∏Êú∫ÂùêÊ†áÁ≥ª‰∏ãÁöÑÁ©∫Èó¥ÂùêÊ†áÔºö su = KP^{'}\\ u = f_x \frac{X^{'}}{Z^{'}} + c_x\\ v = f_y \frac{X^{'}}{Z^{'}} + c_yÂõ†Ê≠§ËØØÂ∑ÆÈ°πÂØºÊï∞ÁöÑÁ¨¨‰∏ÄÈ°π‰∏∫Ôºö \frac{\partial e}{\partial P^{'}} = - \begin{bmatrix} \frac{\partial u}{\partial X^{'}} & \frac{\partial u}{\partial Y^{'}} & \frac{\partial u}{\partial Z^{'}}\\ \frac{\partial v}{\partial X^{'}} & \frac{\partial v}{\partial Y^{'}} & \frac{\partial v}{\partial Z^{'}} \end{bmatrix} =- \begin{bmatrix} \frac{f_x}{Z^{'}} & 0 & -\frac{f_xX^{'}}{Z^{'2}}\\ 0 & \frac{f_y}{Z^{'}} & -\frac{f_yY^{'}}{Z^{'2}}\\ \end{bmatrix}ËØØÂ∑ÆÈ°πÁöÑÁ¨¨‰∫åÈ°π‰∏∫ÂèòÊç¢ÂêéÁöÑÁÇπÂÖ≥‰∫éÊùé‰ª£Êï∞ÁöÑÂØºÊï∞ÔºåÂèÇËÄÉÊùé‰ª£Êï∞ËäÇÔºö \frac{\partial TP}{\partial \delta \xi} = \begin{bmatrix} I & -P^{'\wedge}\\ 0^T & 0^T \end{bmatrix}ÂÖ∂‰∏≠$P^{‚Äò\wedge}$ÊòØ$P^{‚Äò}$ÁöÑÂèçÂØπÁß∞ÈòµÔºö P^{'\wedge} = \begin{bmatrix} 0 & -Z^{'} & Y^{'}\\ Z^{'} & 0 & -X^{'}\\ -Y^{'} & X^{'} & 0 \end{bmatrix}Âõ†Ê≠§ÂæóÂà∞ÂÆåÊï¥ÁöÑÈõÖÂèØÊØîÁü©ÈòµÔºö \frac{\partial e}{\partial \delta \xi} =\frac{\partial e}{\partial P^{'}} \begin{bmatrix} I & P^{'\wedge} \end{bmatrix}=- \begin{bmatrix} \frac{f_x}{Z^{'}} & 0 & -\frac{f_xX^{'}}{Z^{'2}} & |& -\frac{f_xX^{'}Y^{'}}{Z^{'2}} & f_x + \frac{f_xX^{'2}}{Z^{'2}} & -\frac{f_xY^{'}}{Z^{'}}\\ 0 & \frac{f_y}{Z^{'}} & -\frac{f_yY^{'}}{Z^{'2}}& | & - f_y - \frac{f_xY^{'2}}{Z^{'2}} & \frac{f_yX^{'}Y^{'}}{Z^{'2}} & \frac{f_yX^{'}}{Z^{'}}\\ \end{bmatrix}Èô§‰∫Ü‰ºòÂåñÁõ∏Êú∫‰ΩçÂßø‰ª•Â§ñÔºåËøòÂèØ‰ª•ÂêåÊó∂‰ºòÂåñÁâπÂæÅÁÇπÁöÑÁ©∫Èó¥‰ΩçÁΩÆ$P$Ôºö \frac{\partial e}{\partial P} = \frac{\partial e}{\partial P^{'}} \frac{\partial P^{'}}{\partial P}ÂÖ∂‰∏≠ÁöÑÁ¨¨‰∫åÈ°π‰∏∫Ôºö P^{'} = exp(\xi^{\wedge})P = RP +t\\ \therefore \frac{\partial P^{'}}{\partial P} = R^T3. ‰ºòÂåñÂ∫ì‰ΩøÁî®ÊûÑÂª∫ÂõæÊ®°ÂûãÔºö ‰ºòÂåñÂèòÈáè1ÔºöËäÇÁÇπ1ÔºöqueryÁõ∏Êú∫‰ΩçÂßøÔºàÂÖ≠Áª¥ÂêëÈáèÊùé‰ª£Êï∞Ôºâ ‰ºòÂåñÂèòÈáè2ÔºöËäÇÁÇπ2ÔºöÁâπÂæÅÁÇπÁ©∫Èó¥‰ΩçÁΩÆÔºà‰∏âÁª¥ÂêëÈáèÂùêÊ†áÊèèËø∞Ôºâ È¢ÑÊµãÂÄºÔºöËæπnÔºöÊ†πÊçÆÂΩìÂâçestimateÁöÑ‰ºòÂåñÈáèÔºåÊäïÂΩ±Âà∞ÊäïÂΩ±Âπ≥Èù¢ÁöÑÂÉèÁ¥†ÂùêÊ†á$z_i = h(\xi, P_i)$ ËßÇÊµãÂÄºÔºöËÉΩÂ§üÁõ¥Êé•ËØªÂá∫ÁöÑÔºåqueryÂ∏ß‰∏äÂØπÂ∫îÁâπÂæÅÁÇπÁöÑÂÆûÈôÖÊäïÂΩ±ÂùêÊ†á$u_i$ g2o‰∏≠Â∑≤ÁªèÊèê‰æõ‰∫ÜÁõ∏ËøëÁöÑÂü∫Á±ªÔºàÂú®g2o/types/sba/types_six_dof_expmap.h‰∏≠ÔºâÔºöÊùé‰ª£Êï∞‰ΩçÂßøËäÇÁÇπVertexSE3Expmap„ÄÅÁ©∫Èó¥ÁÇπ‰ΩçÁΩÆËäÇÁÇπVertexSBAPointXYZ„ÄÅÊäïÂΩ±ÊñπÁ®ãËæπEdgeProjectXYZ2UV„ÄÇ ËæπÁ±ªÈáåÈù¢Ë¶ÅÂÖ≥Ê≥®linearizeOplusÂáΩÊï∞ÔºåËøô‰∏™ÂáΩÊï∞ÊèèËø∞ÁöÑÊòØÈùûÁ∫øÊÄßÂáΩÊï∞ËøõË°åÁ∫øÊÄßÂåñÁöÑËøáÁ®ã‰∏≠ÔºåÊ±ÇÂØºÁöÑËß£ÊûêËß£ÔºàÂΩìÁÑ∂‰πüÂèØ‰ª•‰ΩøÁî®Êï∞ÂÄºËß£ÔºâÔºåÊúÄÂêéÂáΩÊï∞ÁªôÂá∫ÁöÑÊòØÊØè‰∏™ËäÇÁÇπÁöÑÂØºÊï∞Áü©ÈòµÔºà$\frac{\partial e}{\partial \delta \xi} $Âíå$\frac{\partial e}{\partial P_i}$Ôºâ „ÄÇËøôÁÇπÊòØCeresÂ∫ìÂíåg2oÂ∫ìÁöÑ‰∏ÄÁÇπ‰∏ªË¶ÅÂ∑ÆÂà´ÔºöCeresÈÉΩÊòØ‰ΩøÁî®Ëá™Âä®ÁöÑÊï∞ÂÄºÂØºÊï∞Ôºåg2oË¶ÅËá™Â∑±Ê±ÇÂØº„ÄÇ 1234567891011void EdgeProjectXYZ2UV::linearizeOplus()&#123; VertexSE3Expmap * vj = static_cast&lt;VertexSE3Expmap*&gt;(_vertices[1]); VertexSBAPointXYZ* vi = static_cast&lt;VertexSBAPointXYZ*&gt;(_vertices[0]); ... ... _jacobianOplusXi = -1./z * tmp * T.rotation().toRotationMatrix(); _jacobianOplusXj(0,0) = x*y/z^2 * cam-&gt;focal_length; ...&#125; 4. OpenCVÂÜÖÁΩÆÂáΩÊï∞basicÔºö 12345void cv::solvePnP ( pts3d, pts2d, K, Mat(), r, t, false );Mat r, t, R;cv::solvePnP ( pts3d, pts2d, K, Mat(), r, t, false );cv::Rodrigues ( r, R ); // ÊóãËΩ¨ÂêëÈáèÂíåÊóãËΩ¨Áü©ÈòµÁöÑËΩ¨Êç¢ advancedÔºö 1234567891011121314bool cv::solvePnPRansac( InputArray objectPoints, // 3DÁ©∫Èó¥ÂùêÊ†á vector&lt;cv::Point3f&gt; pts3d InputArray imagePoints, // 2DÂÉèÁ¥†ÂùêÊ†á vector&lt;cv::Point2f&gt; pts2d InputArray cameraMatrix, // Áõ∏Êú∫ÂÜÖÈÉ®ÂèÇÊï∞Áü©Èòµ K InputArray distCoeffs, // Áï∏ÂèòÁ≥ªÊï∞ÂêëÈáè cv::Mat() OutputArray rvec, // ÊóãËΩ¨ÂêëÈáè OutputArray tvec, // Âπ≥ÁßªÂêëÈáè bool useExtrinsicGuess = false, // If true, the function uses the provided rvec and tvec values as initial int iterationsCount = 100, // Number of iterations float reprojectionError = 8.0,// ÈáçÊäïÂΩ±ËØØÂ∑ÆÊúÄÂ§ßÂÄº double confidence = 0.99, OutputArray inliers = noArray(), // Output vector that contains indices of inliers in objectPoints and imagePoints . int flags = SOLVEPNP_ITERATIVE // method for solving PnP) RansacÔºöËÄÉËôëÂà∞Êàë‰ª¨Êèê‰æõÁöÑÂåπÈÖçÈáåÈù¢Â≠òÂú®ËØØÂåπÈÖçÁöÑÊÉÖÂÜµÔºåOpenCVÈááÁî®‚ÄúÈöèÊú∫ÈááÊ†∑‰∏ÄËá¥ÊÄßÁÆóÊ≥ï‚ÄùÔºàRandom Sample ConsensusÔºâÔºå‰ªéÁé∞ÊúâÂåπÈÖç‰∏≠ÈöèÊú∫Âèñ‰∏ÄÈÉ®ÂàÜÁî®Êù•‰º∞ËÆ°ËøêÂä®ÔºàPnPÁöÑËß£ÊûêËß£Ê≥ïÊúÄÂ∞ëÂè™ÈúÄË¶Å‰∏â‰∏™ÁÇπÂ∞±ËÉΩËÆ°ÁÆóÁõ∏ÂØπ‰ΩçÂßøÔºâÔºåÊ≠£Á°ÆÁöÑÂåπÈÖçÁªìÊûúÈÉΩÊòØËøë‰ººÁöÑÔºå‰ªéËÄåÂâîÈô§ËØØÂåπÈÖç„ÄÇ inlierÔºöÂÜÖÁÇπÔºåÂáΩÊï∞ÊúÄÁªàÁªôÂá∫ÁöÑÂåπÈÖçÂèØ‰ø°ÁöÑÁÇπ„ÄÇ RANSACÂè™ÈááÁî®Â∞ëÊï∞Âá†‰∏™ÈöèÊú∫ÁÇπÊù•ËÆ°ÁÆóPnPÔºåÂÆπÊòìÂèóÂà∞Âô™Â£∞ÂΩ±Âìç„ÄÇÂ∑•Á®ã‰∏äÈÄöÂ∏∏‰ΩøÁî®RANSACÁöÑËß£‰Ωú‰∏∫ÂàùÂÄºÔºåÂÜç‰ΩøÁî®ÈùûÁ∫øÊÄß‰ºòÂåñÊñπÊ≥ïÊ±ÇËß£ÊúÄ‰ºòÂÄº„ÄÇ 12345678910111213// RansacÁ≤óÂåπÈÖçcv::solvePnPRansac( pts3d, pts2d, K, Mat(), rvec, tvec, false, 100, 4.0, 0.99, inliers );cv::Rodrigues ( rvec, R ); Eigen::Matrix3d rotation_matrix = R.at&lt;double&gt;;T_c_r_estimated_ = SE3d( SO3d(rotation_matrix), Vector3d( tvec.at&lt;double&gt;(0,0), tvec.at&lt;double&gt;(1,0), tvec.at&lt;double&gt;(2,0)) ); // BAÂ±ÄÈÉ®‰ºòÂåñg2o::VertexSE3Expmap* pose new g2o::VertexSE3Expmap();...pose-&gt;setEstimate(g2o::SE3Quat( T_c_r_estimated_.rotationMatrix(), T_c_r_estimated_.translation()));]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[karto key concepts]]></title>
    <url>%2F2018%2F06%2F05%2Fkarto-key-concepts%2F</url>
    <content type="text"><![CDATA[Âü∫‰∫éÊûÅÂÖ∂Ê†áÂáÜÁöÑÂõæ‰ºòÂåñSLAMÊ°ÜÊû∂Êù•ÂÆûÁé∞Ôºö ÊèêÂá∫‰∫ÜÈááÁî®Á®ÄÁñèÁÇπË∞ÉÊï¥ÔºàSPAÔºâÁöÑÊñπÊ≥ïÊù•È´òÊïàÊ±ÇËß£‰ΩçÂßøÂõæ‰ºòÂåñÈóÆÈ¢òÔºåÈíàÂØπÊú¨ÁÆóÊ≥ïÁöÑÊñáÁåÆ‰∏∫„ÄäEfficient Sparse Pose Adjustment for 2D Mapping„Äã„ÄÇ scan matchingÈÉ®ÂàÜÁöÑÂèÇËÄÉÊñáÁåÆ‰∏∫„ÄäReal-time correlative scan matching„ÄãÔºåM3RSMÁöÑÂâçË∫´ÔºÅ key conceptsÔºö keyScanÔºöÊú∫Âô®‰∫∫ËøêÂä®‰∏ÄÂÆöÁöÑË∑ùÁ¶ªÊàñËßíÂ∫¶ÔºàÂÖ≥ÈîÆÂ∏ßÔºâÔºåÂÇ®Â≠òÂú®sensorManagerÔºåÊó†Âú∞ÂõæÁºìÂ≠ò„ÄÇ look-up tableÊü•ÊâæË°®ÔºöÊü•ÊâæË°®ÁöÑÊÑè‰πâÂ∞±ÊòØÁõ∏ÊØî‰∫éÊö¥ÂäõÂåπÈÖçÔºå‰∏çÈúÄË¶ÅÊØèÊ¨°ÈÉΩÈáçÊñ∞ËÆ°ÁÆóÊØè‰∏™ÊøÄÂÖâÊï∞ÊçÆ‰ø°ÊÅØÔºåÁõ∏ÂêåËßíÂ∫¶‰∏çÂêå‰ΩçÁΩÆÁöÑÊøÄÂÖâÊï∞ÊçÆ‰ø°ÊÅØÂè™ÈúÄË¶ÅË¢´Á¥¢Âºï‰∏ÄÊ¨°„ÄÇ responseÂìçÂ∫îÂÄºÔºöÂ∞ÜÊü•ÊâæË°®‰ª•‰∏ÄÂÆöÁöÑ‰ΩçÁßªÊäïÂà∞Â≠êÂõæ‰∏äÔºåÊÄªÂÖ±Êúân‰∏™ÁÇπË¢´Êü•ÊâæË°®Âáª‰∏≠ÔºàhitÔºâÔºåÂáª‰∏≠ÁöÑÊØè‰∏™ÁÇπÂæóÂàÜ‰∏çÂêåÔºàscoreÔºâÔºåÁ¥ØÂä†ÂæóÂàÜÂπ∂Èô§‰ª•ÂèØ‰ª•ËææÂà∞ÁöÑÊúÄÈ´òÂàÜ„ÄÇ response = \frac{\sum_{i=0}^n goal_i}{goalmax} ÂçèÊñπÂ∑ÆÔºöÊñáÁåÆ‰∏ìÈó®Áî®‰∫Ü‰∏ÄËäÇËÆ°ÁÆóÂçèÊñπÂ∑ÆÔºå‰ΩÜÊòØÊ≤°ÁúãÂà∞Áî®Âú®Âì™ÔºåÊòØ‰∏∫‰∫ÜÂêéÈù¢Ê±ÇËØØÂ∑ÆÂÅöÂáÜÂ§áÂêóÔºüÔºüÔºü addScansÊ∑ªÂä†È°∂ÁÇπÂíåËæπÔºöËæπÊòØËØØÂ∑ÆÂÄºÔºåÊ∑ªÂä†ÁöÑËæπÁ∫¶ÊùüÊù•Ëá™‰∏§ÈÉ®ÂàÜÔºå Ôºà1Ôºâlink to running scansÔºåË∑ùÂΩìÂâçÂ∏ß‰∏ÄÂÆöËåÉÂõ¥ÂÜÖÁöÑÊøÄÂÖâÊï∞ÊçÆÈìæÔºàRunningScan chainÔºâ„ÄÇ Ôºà2Ôºâlink to other near chainsÔºå‰ªéÂΩìÂâçËäÇÁÇπÂºÄÂßãÂπøÂ∫¶‰ºòÂÖàÈÅçÂéÜ‰∏ÄÂÆöË∑ùÁ¶ªËåÉÂõ¥ÂÜÖÊâÄÊúâËäÇÁÇπÔºå‰æùÊçÆÂΩìÂâçid‰ªésensorManager‰∏≠ÂàÜÂà´ÈÄíÂ¢ûÂíåÈÄíÂáèÂØªÊâæ‰∏ÄÂÆöËåÉÂõ¥ÂÜÖÁöÑchainÔºà‰∏ç‰∏ÄÂÆöÁõ¥Êé•Áõ∏ËøûÔºâ„ÄÇ ÂõûÁéØÊ£ÄÊµãÔºöÊìç‰Ωú‰∏éÊ∑ªÂä†ËæπÁ∫¶ÊùüÁ±ª‰ººÔºå‰ΩçÂßøÂõæ‰∏äË¶ÅÂéªÈô§ÈÇ£‰∫õÂíåÂΩìÂâçËäÇÁÇπÁöÑÊó∂Èó¥Áõ∏ÈÇªÁöÑËäÇÁÇπ„ÄÇ Ôºà1ÔºâÊâæÂà∞‰∏ÄÂÆöË∑ùÁ¶ªËåÉÂõ¥ÂÜÖÔºànearÔºâÂíåÁõ∏ËøûÔºàadjacentÔºâÁöÑËäÇÁÇπÊ∑ªÂä†ËøõnearLinkedScans„ÄÇ Ôºà2ÔºâMapperGraph::FindPossibleLoopClosureÔºö‰ªésensorManager‰∏≠‰ªéÂâçÂà∞ÂêéÔºå‰æùÊçÆÂ∫èÂè∑ÊåëÈÄâ‰∏éÂΩìÂâçËäÇÁÇπÂú®‰∏ÄÂÆöË∑ùÁ¶ªËåÉÂõ¥ÂÜÖÔºå‰∏î‰∏çÂú®nearLinkedScans‰∏≠ÁöÑcandidate„ÄÇËøîÂõûÊΩúÂú®chain„ÄÇÂÖ∂‰∏≠Ê∂âÂèä‰∏§‰∏™ÂèÇÊï∞Ôºö LoopSearchMaximumDistanceÔºöcandidateScan‰∏éÂΩìÂâçscanÁöÑË∑ùÁ¶ªÂøÖÈ°ªÂú®ÂèØÂÆπËÆ∏ÁöÑË∑ùÁ¶ªÂÜÖ„ÄÇ LoopMatchMinimumChainSizeÔºöchain‰∏≠ÁöÑËäÇÁÇπÊï∞ÂøÖÈ°ª‰∏çÂ∞è‰∫éÈôêÂÆöÂÄº„ÄÇ Ôºà3ÔºâMapperGraph::TryCloseLoopÔºöscan2mapÂåπÈÖçÔºåÂΩìresponseÂíåcovarianceËææÂà∞‰∏ÄÂÆöË¶ÅÊ±ÇËÆ§‰∏∫Èó≠ÁéØÊ£ÄÊµãÂà∞ÔºåÂæóÂà∞correct poseÔºà‰πüÂ∞±ÊòØËÆ§‰∏∫candidateScanÁöÑposeÊâçÊòØÂΩìÂâçÂ∏ßÁöÑÂÆûÈôÖposeÔºâ„ÄÇ Ôºà4Ôºâadd link to loopÔºåÊûÑÊàêÂÖ®Â±ÄÈó≠ÁéØ„ÄÇ Ôºà5ÔºâËß¶ÂèëcorrectPoseÔºåËøõË°åspa‰ºòÂåñ„ÄÇ ‰ª£Á†ÅÈöèÊâãËÆ∞Ôºö ROS‰∏äÈù¢Êèê‰æõ‰∏â‰∏™ÂºÄÊ∫êÂåÖÔºönav2d_karto, open_karto, slam_karto„ÄÇ ROS Wiki‰∏äËøô‰πàÊèèËø∞nav2d_kartoËøô‰∏™packageÔºöGraph-based Simultaneous Localization and Mapping module. Includes OpenKarto GraphSLAM library by ‚ÄúSRI International‚Äù. open_kartoÔºöÂºÄÊ∫êÁöÑkartoÂåÖÔºåÂÆûÁé∞Â∫ïÂ±ÇÁöÑkartoslam slam_kartoÔºörosÂ±ÇÔºåÂ∫îÁî®Â±ÇÁöÑkartoslamÊé•Âè£ The LaserRangeFinder contains parameters for physical laser sensor used by the mapper for scan matching Also contains information about the maximum range of the sensor and provides a threshold for limiting the range of readings. The optimal value for the range threshold depends on the angular resolution of the scan and the desired map resolution. resolutionÔºö0.25 &amp; 0.5 &amp; 1 degree number of range readings (beams)ÔºöÔºàmaximumAngle - minimumAngleÔºâÔºèangularResolution + 1 GridStatesÔºö0 for UnknownÔºå100 for OccupiedÔºå 255 for Free„ÄÇ flipYÔºöÊúÄÂºÄÂßãÊú∫Âô®‰∫∫Â∫îËØ•Â§ÑÂú®‰∏ñÁïåÂùêÊ†áÁ≥ªÁöÑÂéüÁÇπÔºå‰º†ÊÑüÂô®ÂùêÊ†áÁ≥ª‰∏éÊú∫Âô®‰∫∫baselinkÂ≠òÂú®‰∏Ä‰∏™ÂùêÊ†áÂèòÊç¢ÔºåÂéüÂßãÁöÑ‰º†ÊÑüÂô®ÂùêÊ†áÁ≥ª‰ΩçÁΩÆÂ∫îËØ•‰∏éÂú∞ÂõæÂùêÊ†áÁ≥ªÈáçÂêàÔºåËøôÂ∞±ÊòØworldÂíågrid‰πãÈó¥ÁöÑoffset„ÄÇflipÊòØÂï•Âë¢ÔºüÔºü LookupArray[index]ÔºöCreate lookup tables for point readings at varying angles in grid. This is to speed up finding best angle/position for a localized range scan MapperGraphÔºöËä±ÂºèÊûÑÈÄ†‰ΩçÂßøÂõæ CorrelationGridÔºöImplementation of a correlation grid used for scan matching Region of Interest ROIÔºö smearÔºöThe point readings are smeared by this value in X and Y to create a smoother response. ‰∏™‰∫∫ÁêÜËß£ËøôÂè•ËØùÊòØËØ¥ÁÇπÂÆπÊòìÁîüÊàêÁ™ÅÂèòÔºåÁî®‰ª•ÁÇπ‰∏∫‰∏≠ÂøÉÁöÑ‰∏ÄÂ∞èÁâáÂå∫ÂüüÂπ≥Êªë‰∏ÄÁÇπ„ÄÇ ScanMatchÔºöËøîÂõûÂìçÂ∫îÂÄºresponse ÂâçÁ´ØÂåπÈÖçË∞ÉÁî®m_pSequentialScanMatcher-&gt;MatchScan Èó≠ÁéØÊ£ÄÊµãË∞ÉÁî®m_pLoopScanMatcher-&gt;MatchScan ‰∏§‰∏™ÂáΩÊï∞ÁªßÊâø‰∫éScanMatcher::MatchScanÔºö 12345678kt_double ScanMatcher::MatchScan( LocalizedRangeScan* pScan, const LocalizedRangeScanVector&amp; rBaseScans, Pose2&amp; rMean, Matrix3&amp; rCovariance, kt_bool doPenalize, kt_bool doRefineMatch) // default is True, ÂÜ≥ÂÆöÊòØÂê¶ÂÅöÁ≤æÂåπÈÖç// @return: strength of response (best response) ÂÖ∂‰∏≠‰ºöË∞ÉÁî®ScanMatcher::CorrelateScanÊñπÊ≥ï„ÄÇScanMatcher::CorrelateScanÊñπÊ≥ï‰∏≠Ë∞ÉÁî®ScanMatcher::GetResponseÊñπÊ≥ïËÆ°ÁÆóÂìçÂ∫îÂÄº„ÄÇ 123kt_double ScanMatcher::GetResponse( kt_int32u angleIndex, kt_int32s gridPositionIndex) const GetResponseÁöÑÊ†∏ÂøÉÂú®kt_int8u* pByteÂíåconst LookupArray* pOffsets‰∏§‰∏™Êï∞ÊçÆÁªìÊûÑÔºö ÂâçËÄÖÊòØÂú®correlationGridËåÉÂõ¥ÂÜÖÁöÑreal sensedÂç†ÊçÆÊÉÖÂÜµ„ÄÇ ÂêéËÄÖÊòØlookup-table‰∏≠ÔºàÂ∑≤Áü•Âú∞ÂõæÔºâËØªÂèñÁöÑÊ†ÖÊ†ºÂç†ÊçÆÊÉÖÂÜµÔºåÂè™ÂåÖÂê´Âç†ÊçÆÁöÑÊ†ÖÊ†ºÔºåkeyÊòØangular„ÄÇ ËÆ°ÁÆóresponseÂè™Ë¶ÅÁúãÂú∞Âõæ‰∏äÁöÑÂç†ÊçÆÁÇπÊòØÂê¶Âú®ËßÇÊµã‰∏≠ÊòØÂê¶‰πüÊòØÂç†ÊçÆÁöÑÔºö 123456789101112for (kt_int32u i = 0; i &lt; pOffsets-&gt;GetSize(); i++)&#123; // ignore points that fall off the grid kt_int32s pointGridIndex = gridPositionIndex + pAngleIndexPointer[i]; if (!math::IsUpTo(pointGridIndex, m_pCorrelationGrid-&gt;GetDataSize()) || pAngleIndexPointer[i] == INVALID_SCAN) &#123; continue; &#125; // uses index offsets to efficiently find location of point in the grid response += pByte[pAngleIndexPointer[i]];&#125; ÊúÄÁªàÁöÑresponseË¶ÅnormalizeÔºö 123// normalize responseresponse /= (nPoints * GridStates_Occupied); // GridStates_Occupied = 100,assert(fabs(response) &lt;= 1.0); kartoÂè™Âú®Èó≠ÁéØÁöÑÊó∂ÂÄôËß¶ÂèëÂêéÁ´Ø‰ºòÂåñCorrectPoses()ÔºåScanSolverÁöÑÂÆûÁé∞Âú®SamplesË∑ØÂæÑ‰∏ãÁöÑSpaSolverÔºåË∞ÉÁî®‰∫ÜÁé∞ÊúâÁöÑBAÊ±ÇËß£Âô®sba(A Generic Sparse Bundle Adjustment C/C++ Package Based on the Levenberg-Marquardt Algorithm)„ÄÇ ÂèÇÊï∞&amp;‰ºòÂåñÊñπÂêë Èó≠ÁéØ‰∏≠candidateÊï∞ÈáèÁöÑË∞ÉÊï¥Ôºö ÂáèÂ∞èLoopSearchMaximumDistanceÔºåËøõÂÖ•candidateËåÉÂõ¥ÁöÑËäÇÁÇπÊï∞ÊçÆÂáèÂ∞ë ÂáèÂ∞èLoopMatchMinimumChainSizeÔºåÁî®Êù•ËÆ°ÁÆó‰ºòÂåñÁöÑcandidateÊï∞ÈáèÂáèÂ∞ë Â¢ûÂ§ßminimum_travel_distanceÂíåminimum_travel_headingÔºåËøôÊ†∑ÊÄª‰ΩìÁöÑËäÇÁÇπÊï∞ÂáèÂ∞ë Map_update_intervalÔºöÂèëÂ∏ÉÂú∞ÂõæÁöÑÈó¥ÈöîÔºåÂÖ∂‰∏ªË¶ÅËøáÁ®ãÊòØÈÅçÂéÜÂΩìÂâçÊâÄÊúâËäÇÁÇπÊï∞ÊçÆÔºåÂØπÊØè‰∏™Ê†ÖÊ†ºÁöÑÂç†ÊúâÁä∂ÊÄÅËøõË°åÂà§ÂÆöÔºåÁîüÊàêÊ†ÖÊ†ºÂú∞Âõæ„ÄÇ ScanBufferSizeÂíåScanBufferMaximumScanDistanceÔºöÊéßÂà∂buffer‰πüÂ∞±ÊòØchainÁöÑÂ§ßÂ∞è„ÄÇchain‰∏çËÉΩÂ§™Â§ß‰πü‰∏çËÉΩÂ§™Â∞èÔºåÂ§™Â∞è‰ºöÈÄ†ÊàêÂâçÁ´ØËØØÂ∑ÆÁ¥ØÁßØÔºåÂ§™Â§ß‰ºöÂØºËá¥ÊûÑÂª∫Èó≠ÁéØÁöÑËäÇÁÇπÊï∞‰∏çË∂≥„ÄÇÊé®ËçêÂÄºÊòØScanBufferMaximumScanDistanceÔºèminimum_travel_distance„ÄÇ ‰ΩçÂßøÁ∫†Ê≠£‰∏≠Ôºö CorrelationSearchSpaceDimensionÔºöThe size of the search grid CorrelationSearchSpaceResolutionÔºöThe size of the correlation grid ÂõûÁéØÊ£ÄÊµã‰∏≠Ôºö LoopSearchMaximumDistanceÔºöÈó≠ÁéØÊ£ÄÊµãÁöÑÊêúÁ¥¢Ë∑ùÁ¶ªÔºåÊï∞ÂÄºË∂äÂ§ßËÉΩË∂äÊó©ÂèëÁé∞Èó≠ÁéØÔºå‰πüËÉΩÂÆπÂøçÊõ¥Â§ßÁöÑÂÅèÁ¶ªËØØÂ∑Æ„ÄÇ LoopMatchMinimumResponseCoarseÂíåLoopMatchMinimumResponseFineÔºöÁ≤óÂåπÈÖçÂíåÁ≤æÂåπÈÖçÁöÑÂìçÂ∫îÈòàÂÄºÔºå‰∏éÈó≠ÁéØ‰∏≠candidateÊï∞ÈáèÁõ∏ÂÖ≥„ÄÇÈòàÂÄºËøá‰Ωé‰ºöÂØºËá¥candidateËøÖÈÄüË¢´Â°´Êª°ÔºåÁúüÊ≠£Â•ΩÁöÑÁÇπËøòÊ≤°ÊâæÂà∞„ÄÇÈòàÂÄºËøáÈ´ò‰ºöÂØºËá¥ÂõûÁéØÂ§±Ë¥•Ôºà‰∏ÄÁõ¥Êâæ‰∏çÂà∞ÂõûÁéØÁÇπÔºâÔºåÂú∞Âõæ‰∏äÂá∫Áé∞ÈáçÂΩ±„ÄÇ CPU UsageÁÆóÊ≥ïËµÑÊ∫êÂç†Áî®ÁöÑ‰∏ªË¶ÅÂéãÂäõÊù•Ê∫êÔºö Âú∞ÂõæÊõ¥Êñ∞ ÂõûÁéØÊ£ÄÊµã SPA‰ºòÂåñ]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode79 ÂçïËØçÊêúÁ¥¢]]></title>
    <url>%2F2018%2F06%2F04%2Fleetcode79-%E5%8D%95%E8%AF%8D%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[ÊåÇ‰∏ÄÈÅìÂæàÁå•ÁêêÁöÑÈ¢òÔºå‰∫åÁª¥ÁΩëÊ†º‰∏≠ÊêúÁ¥¢ÂçïËØçÔºåÂêå‰∏ÄÂçïÂÖÉÊ†º‰∏çËÉΩÈáçÂ§ç‰ΩøÁî®Ôºö 12345678910board =[ [&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;E&apos;], [&apos;S&apos;,&apos;F&apos;,&apos;C&apos;,&apos;S&apos;], [&apos;A&apos;,&apos;D&apos;,&apos;E&apos;,&apos;E&apos;]]ÁªôÂÆö word = &quot;ABCCED&quot;, ËøîÂõû true.ÁªôÂÆö word = &quot;SEE&quot;, ËøîÂõû true.ÁªôÂÆö word = &quot;ABCB&quot;, ËøîÂõû false. Ê≤°Âï•Â•ΩÁÆóÊ≥ïÔºåÂ∞±ÊòØDFSÔºå‰ΩÜÊòØÂùëÂú®‰∫évisitedÁöÑÂ≠òÂÇ®ÔºåpythonÊï∞ÁªÑÈªòËÆ§ÊµÖÊã∑Ë¥ùÔºåÈÄíÂΩí‰º†ËøõÂéªÂÜçÂõûÂà∞‰∏ä‰∏ÄÂ±ÇÁΩëÊ†ºÁä∂ÊÄÅÂ∞±Âèò‰∫ÜÔºå‰πãÂâç‰∏ÄË¥ØÁöÑÂÅöÊ≥ïÂ∞±ÊòØÊñ∞ÂºÄ‰∏ÄÂùóÂÜÖÂ≠òÁ©∫Èó¥Ôºå‰º†Êñ∞ÁöÑÊï∞ÁªÑËøõÂéªÔºåÁÑ∂ËÄåËøôÊ¨°Ë∂ÖÊó∂‰∫ÜÔºåÂõ†‰∏∫ÊµãËØïÁî®‰æãÁöÑ‰∫åÁª¥Êï∞ÁªÑÂ∞∫ÂØ∏Ë¥ºÂ§ßÔºåÁªà‰∫éÊúâÊú∫‰ºöÊ≠£ËßÜËøô‰∏™ÈóÆÈ¢òÔºåÂπ∂Ëé∑ÂèñÊ≠£Á°ÆÁöÑÊâìÂºÄÊñπÂºèÔºö Â∞∫ÂØ∏Ë¥ºÂ§ßÁöÑ‰∫åÁª¥Êï∞ÁªÑÔºåÊØèÊ¨°Âè™ÈúÄË¶Å‰øÆÊîπ‰∏Ä‰∏™ÂÄºÔºåÈáçÊñ∞ÂàíÁ©∫Èó¥Êã∑Ë¥ùÂÜç‰øÆÊîπÊó∂Èó¥Â§çÊùÇÂ∫¶Áû¨Èó¥Â¢ûÂ§ßO(m*n)ÂÄçÔºåÂæàÊòéÊòæ‰º†ÂéüÊù•ÁöÑÊï∞ÁªÑËøõÂéªÊØîËæÉÂêàÈÄÇ„ÄÇ ‰ΩÜÊòØÊ∑±Â±ÇÈÄíÂΩí‰ºö‰øÆÊîπ‰º†ËøõÂéªÁöÑÂèÇÊï∞ÔºåÂõ†Ê≠§Âú®ÊØèÊ¨°ÈÄíÂΩí‰πãÂâçÂÖàÂàõÂª∫‰∏Ä‰∏™tmpÔºåËÆ∞ÂΩï‰øÆÊîπË°å‰∏∫ÔºåÈÄíÂΩíÂáΩÊï∞ËøõË°åÂÆå‰ª•ÂêéÔºåÂÜçÊ†πÊçÆËÆ∞ÂΩïÊÅ¢Â§çÂéüÊù•ÁöÑÂèÇÊï∞Ôºå‰øùËØÅÊú¨Â±ÇÂèÇÊï∞‰∏çÂèò„ÄÇ 123456789101112131415161718def search_tail(board, word, h, w): size = len(word) char = word[0] height, width = len(board), len(board[0]) exist = False if h - 1 &gt;= 0 and board[h-1][w] == char: if size == 1: return True else: tmp = board[h-1][w] board[h-1][w] = 'INF' exist = search_tail(board, word[1:], h-1, w) if exist: return True board[h-1][w] = tmp if h + 1 &lt; height and board[h+1][w] == char: ... ... ÁÑ∂ÂêéÈíàÂØπÊú¨È¢òËøòÊúâ‰∏Ä‰∏™È™öÊìç‰ΩúÔºåÂæàÂ§ö‰∫∫‰∏ìÈó®ÂàõÂª∫‰∏Ä‰∏™visitedË°®Êù•ËÆ∞ÂΩïÊêúÁ¥¢Ë∑ØÂæÑÔºå‰ΩÜÊòØÂõ†‰∏∫Êú¨È¢òÁöÑ‰∫åÁª¥Êï∞ÁªÑÈôêÂÆöÂ≠òÂÇ®Â≠óÊØçÔºåÊâÄ‰ª•‰ªªÊÑè‰∏Ä‰∏™ÈùûÂ≠óÊØçÈÉΩÂèØ‰ª•‰Ωú‰∏∫Ê†áÂøó‰ΩçÔºåÁæéÊªãÊªãÂèàÁúÅ‰∏ã‰∏Ä‰∏™O(m*n)„ÄÇ]]></content>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kd-tree]]></title>
    <url>%2F2018%2F06%2F01%2Fkd-tree%2F</url>
    <content type="text"><![CDATA[ReferenceÔºöBentley J L. Multidimensional Binary Search Trees Used for Associative Searching[J]. Communications of the Acm, 1975, 18(9):509-517. ÂâçÈù¢Êõ¥Êñ∞basic ICPÁöÑÊó∂ÂÄôÁïô‰∫Ü‰∏Ä‰∏™Âùë‚Äî‚ÄîÊúÄËøëÈÇªÁöÑÊ±ÇÊ≥ï„ÄÇÁ∫øÊÄßÊâ´ÊèèÔºüÊâãÂä®Êå•Êâã„ÄÇ 1. ÂÖàËØ¥ËØ¥Ë∑ùÁ¶ªÂêß1.1 Ê¨ßÂºèË∑ùÁ¶ª d(x,y) = \sqrt{(x_1 - y_1)^2 + (x_2-y_2)^2 + ... + (x_n-y_n)^2} = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}1.2 Ê†áÂáÜÂåñÊ¨ßÂºèË∑ùÁ¶ª È¶ñÂÖàÂ∞ÜÊï∞ÊçÆÂêÑÁª¥Â∫¶ÂàÜÈáèÈÉΩÊ†áÂáÜÂåñÂà∞ÂùáÂÄº‰∏∫0ÔºåÊñπÂ∑Æ‰∏∫1ÔºåÂÜçÊ±ÇÊ¨ßÂºèË∑ùÁ¶ªÔºö X_{stand} = \frac{X - \mu}{\sigma}ÁÆÄÂçïÊé®ÂØºÂêéÂèØ‰ª•ÂèëÁé∞ÔºåÊ†áÂáÜÂåñÊ¨ßÂºèË∑ùÁ¶ªÂÆûÈôÖ‰∏äÂ∞±ÊòØ‰∏ÄÁßçÂä†ÊùÉÊ¨ßÂºèË∑ùÁ¶ªÔºö d(x,y) = \sqrt{\sum_{i=1}^n (\frac{x_i-y_i}{s_i})^2}1.3 È©¨Ê∞èË∑ùÁ¶ªÔºàMahalanobis DistanceÔºâ D(X_i,X_j) = \sqrt{(X_i - X_j)S^{-1}(X_i-X_j)}ÂÖ∂‰∏≠$S$‰∏∫ÂçèÊñπÂ∑ÆÁü©Èòµ$Cov$Ôºö Cov(X,Y) = E\{[X-E(X)][Y-E(Y)]\}Ëã•ÂçèÊñπÂ∑ÆÁü©ÈòµÊòØÂçï‰ΩçÈòµÔºàÊ†∑Êú¨ÂêÑÁª¥Â∫¶‰πãÈó¥Áã¨Á´ãÂêåÂàÜÂ∏ÉÔºâÔºåÈÇ£ÂÖ¨ÂºèÂ∞±ÂèòÊàêÊ¨ßÂºèË∑ùÁ¶ª‰∫ÜÔºåËã•ÂçèÊñπÂ∑ÆÁü©ÈòµÊòØÂØπËßíÈòµÔºåÈÇ£ÂÖ¨ÂºèÂ∞±ÂèòÊàêÊ†áÂáÜÂåñÊ¨ßÂºèË∑ùÁ¶ª‰∫Ü„ÄÇ 1.4 Áõ∏‰ººÂ∫¶ Áõ∏‰ººÂ∫¶‰πüÊòØË∑ùÁ¶ªÁöÑ‰∏ÄÁßçË°®ÂæÅÊñπÂºèÔºåË∑ùÁ¶ªË∂äÁõ∏ËøëÔºåÁõ∏‰ººÂ∫¶Ë∂äÈ´ò„ÄÇ Ê¨ßÂºèË∑ùÁ¶ªÁõ∏‰ººÂ∫¶ÔºöÂ∞ÜÊ¨ßÂºèË∑ùÁ¶ªÈôêÂÆöÂú®0 1‰πãÈó¥ÂèòÂåñ Áõ∏‰ººÂ∫¶ = \frac{1}{1+Ê¨ßÂºèË∑ùÁ¶ª} ‰ΩôÂº¶Áõ∏‰ººÂ∫¶Ôºö-1Âà∞1‰πãÈó¥ÂèòÂåñ cos\theta = \frac{A\cdot B}{||A||\ ||B||} ÁöÆÂ∞îÈÄäÁõ∏ÂÖ≥Á≥ªÊï∞Ôºö-1Âà∞1‰πãÈó¥ÂèòÂåñ 1return numpy.corrcoef(A, B) 2. KDÊ†ë k-NNÁÆóÊ≥ï Êé®ËçêÁ≥ªÁªü SIFTÁâπÂæÅÂåπÈÖç ICPËø≠‰ª£ÊúÄËøëÁÇπ ÊúÄËøëÂú®ÂÅöÁöÑM3RCM‰∏≠ÁöÑÂ†ÜÁªìÊûÑÔºàËøô‰∏™ÊúâÁÇπÁâµÂº∫ÔºåÂõ†‰∏∫‰∏çÊòØ‰∫åÂèâÊ†ëÔºâ ÊÄª‰πã‰ª•‰∏äËøô‰∫õÂü∫‰∫éÂåπÈÖçÔºèÊØîËæÉÁöÑÁõÆÁöÑËÄåËøõË°åÁöÑÊï∞ÊçÆÂ∫ìÊü•ÊâæÔºèÂõæÂÉèÊ£ÄÁ¥¢ÔºåÊú¨Ë¥®‰∏äÈÉΩÂèØ‰ª•ÂΩíÁªì‰∏∫ÈÄöËøáË∑ùÁ¶ªÂáΩÊï∞Âú®È´òÁª¥Áü¢Èáè‰πãÈó¥ÁöÑÁõ∏‰ººÊÄßÊ£ÄÁ¥¢ÈóÆÈ¢ò„ÄÇ ‰∏ÄÁª¥ÂêëÈáèÊúâ‰∫åÂàÜÊ≥ïÊü•ÊâæÔºåÂØπÂ∫îÂú∞È´òÁª¥Á©∫Èó¥ÊúâÊ†ëÂΩ¢ÁªìÊûÑ‰æø‰∫éÂø´ÈÄüÊ£ÄÁ¥¢„ÄÇÂà©Áî®Ê†ëÂΩ¢ÁªìÊûÑÂèØ‰ª•ÁúÅÂéªÂØπÂ§ßÈÉ®ÂàÜÊï∞ÊçÆÁÇπÁöÑÊêúÁ¥¢Ôºå‰ªéËÄåÂáèÂ∞ëÊ£ÄÁ¥¢ÁöÑËÆ°ÁÆóÈáè„ÄÇ KDÊ†ëÊòØ‰∏ÄÁßç‰∫åÂèâÊ†ëÔºåÈÄöËøá‰∏çÊñ≠Âú∞Áî®ÂûÇÁõ¥‰∫éÊüêÂùêÊ†áËΩ¥ÁöÑË∂ÖÂπ≥Èù¢Â∞ÜkÁª¥Á©∫Èó¥ÂàáÂàÜÊûÑÈÄ†ËÄåÊàê„ÄÇ 2.1 ÊûÑÈÄ†Ê†ë ÈÄíÂΩíÂàõÂª∫ËäÇÁÇπÔºöËäÇÁÇπ‰ø°ÊÅØÂåÖÂê´ÂàáÂàÜÂùêÊ†áËΩ¥ÂíåÂàáÂàÜÁÇπÔºå‰ªéËÄåÁ°ÆÂÆöË∂ÖÂπ≥Èù¢ÔºåÂ∞ÜÂΩìÂâçÁ©∫Èó¥ÂàáÂàÜ‰∏∫Â∑¶Âè≥‰∏§‰∏™Â≠êÁ©∫Èó¥ÔºåÈÄíÂΩíÁõ¥Âà∞ÂΩìÂâçÂ≠êÁ©∫Èó¥ÂÜÖÊ≤°ÊúâÂÆû‰æã‰∏∫Ê≠¢„ÄÇ 123456class Node: def __init__(self, point, axis): self.value = point self.axis = axis self.left = None self.right = None ‰∏∫‰∫Ü‰ΩøÂæóÊûÑÈÄ†Âá∫ÁöÑKDÊ†ëÂ∞ΩÂèØËÉΩÂπ≥Ë°°ÔºàÈ´òÊïàÂàÜÂâ≤Á©∫Èó¥ÔºâÔºö ÈÄâÊã©ÂùêÊ†áËΩ¥ÔºöÁÆÄÂçïÁÇπÁöÑÊñπÂºèÊòØÂæ™ÁéØ‰∫§ÊõøÈÄâÊã©ÂùêÊ†áËΩ¥ÔºåÂ§çÊùÇÁÇπÁöÑÂÅöÊ≥ïÊòØÈÄâÊã©ÂΩìÂâçÊñπÂ∑ÆÊúÄÂ§ßÁöÑËΩ¥‰Ωú‰∏∫ÂàáÂàÜËΩ¥„ÄÇ ÈÄâÊã©ÂàáÂàÜÁÇπÔºöÂèñÈÄâÂÆöÂùêÊ†áËΩ¥‰∏äÊï∞ÊçÆÁöÑ‰∏≠ÂÄº‰Ωú‰∏∫ÂàáÂàÜÁÇπ„ÄÇ Ê≥®ÊÑèÔºöKDÊ†ëÁöÑÊûÑÈÄ†Êó®Âú®È´òÊïàÂàÜÂâ≤Á©∫Èó¥ÔºåÂÖ∂Âè∂Â≠êËäÇÁÇπÂπ∂ÈùûÊòØÊúÄËøëÈÇªÊêúÁ¥¢Á≠âÂ∫îÁî®Âú∫ÊôØÁöÑÊúÄ‰ºòËß£„ÄÇ 12345678910def kdTree(points, depth): if len(points) == 0: return None axis = depth % len(points[0]) points.sort(key=lambda x: x[axis]) cut_idx = centreValue(points) node = Node(points[cut_idx], axis) node.left = kdTree(points[:cut_idx], depth+1) node.right = kdTree(points[cut_idx+1:], depth+1) return node ÂØπ‰∫éÂåÖÂê´n‰∏™ÂÆû‰æãÁöÑkÁª¥Êï∞ÊçÆÊù•ËØ¥ÔºåÊûÑÈÄ†KDÊ†ëÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶‰∏∫O(k*n*log n)„ÄÇ 2.2 Êñ∞Â¢ûËäÇÁÇπ ÈÄíÂΩíÂÆûÁé∞Ôºö‰ªéÊ†πËäÇÁÇπÂºÄÂßãÂÅöÊØîËæÉÔºåÂ§ß‰∫éÂàôÊèíÂÖ•Â∑¶Â≠êÊ†ëÔºåÂ∞è‰∫éÂàôÊèíÂÖ•Âè≥Â≠êÊ†ë„ÄÇÁõ¥Âà∞ËææÂà∞Âè∂Â≠êËäÇÁÇπÔºåÂπ∂ÂàõÂª∫Êñ∞ÁöÑÂè∂Â≠êËäÇÁÇπ„ÄÇ 2.3 Âà†Èô§ËäÇÁÇπ Â∞ÜÂæÖÂà†Èô§ÁöÑËäÇÁÇπÁöÑÊâÄÊúâÂ≠êËäÇÁÇπÁªÑÊàê‰∏Ä‰∏™ÈõÜÂêàÔºåÈáçÊñ∞ÊûÑÂª∫KDÂ≠êÊ†ëÔºåÊõøÊç¢ÂæÖÂà†Èô§ËäÇÁÇπ„ÄÇ 2.4 ÊúÄËøëÈÇªÊêúÁ¥¢ ÊêúÁ¥¢ÊúÄËøëÈÇªÁÆóÊ≥ï‰∏ªË¶ÅÂàÜ‰∏∫‰∏§ÈÉ®ÂàÜÔºöÈ¶ñÂÖàÊòØÊ∑±Â∫¶‰ºòÂÖàÈÅçÂéÜÔºåÁõ¥Âà∞ÈÅáÂà∞Âè∂Â≠êËäÇÁÇπÔºåÁîüÊàêÊêúÁ¥¢Ë∑ØÂæÑ„ÄÇ 12345678910111213141516171819202122232425def searchNearest(node, target): # input: node: root node of the tree # target: list # output: nearest: list # dist: distance between target and nearest if node == None: return None # ÁîüÊàêÊêúÁ¥¢Ë∑ØÂæÑ search_path = deque() nearest = node print("search path: ") while node: print(node.value) search_path.append(node) # if Dist(nearest.value, target) &gt; Dist(node.value, target): # nearest.value = node.value # minDist = Dist(node.value, target) axis = node.axis if target[axis] &gt; node.value[axis]: node = node.right else: node = node.left ... ... ÁÑ∂ÂêéÊòØÂõûÊ∫ØÊü•ÊâæÔºåÂ¶ÇÊûúÁõÆÊ†áÁÇπÂíåÂΩìÂâçÊúÄËøëÁÇπÊûÑÊàêÁöÑÁêÉÂΩ¢Âå∫Âüü‰∏éÂÖ∂‰∏äÊ∫ØËäÇÁÇπÁõ∏‰∫§ÔºåÈÇ£‰πàÂ∞±Êúâ‰∏ÄÁßçÊΩúÂú®ÁöÑÂèØËÉΩ‚Äî‚Äî‰∏äÊ∫ØËäÇÁÇπÁöÑÂè¶‰∏Ä‰∏™Â≠êÁ©∫Èó¥ÁöÑÂÆû‰æãÂèØËÉΩ‰Ωç‰∫éÂΩìÂâçËøô‰∏™ÁêÉÂΩ¢Âå∫ÂüüÂÜÖÔºåÂõ†Ê≠§Ë¶ÅËøõË°å‰∏ÄÊ¨°Âà§Êñ≠„ÄÇ 1234567891011121314151617181920212223242526def searchNearest(node, target): # input: node: root node of the tree # target: list # output: nearest: list # dist: distance between target and nearest ... ... # ÂõûÊ∫Ø print("\nsearch backwards: ") nearest = search_path.pop() minDist = Dist(nearest.value, target) while search_path: node = search_path.pop() print(node.value) if node.axis: axis = node.axis if minDist &gt; Dist1(node.value[axis], target[axis]): if target[axis] &gt; node.value[axis]: search_path.append(node.left) else: search_path.append(node.right) if Dist(target, nearest.value) &gt; Dist(node.value, target): nearest = node minDist = Dist(node.value, target) return nearest.value, minDist ‰∏§‰∏™ÂèÇËÄÉÁÇπÔºö 123samples = [(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)]target = (2.1, 3.1)target = (2, 4.5) KDÊ†ëÊêúÁ¥¢ÁöÑÊ†∏ÂøÉÂ∞±ÊòØÔºöÂΩìÊü•ËØ¢ÁÇπÁöÑÈÇªÂüü‰∏éÂàÜÂâ≤Ë∂ÖÂπ≥Èù¢‰∏§‰æßÁ©∫Èó¥‰∫§Ââ≤Êó∂ÔºåÈúÄË¶ÅÊü•ÊâæÂè¶‰∏Ä‰æßÂ≠êÁ©∫Èó¥ÔºÅÔºÅÔºÅÁÆóÊ≥ïÂπ≥ÂùáÂ§çÊùÇÂ∫¶O(N logN)„ÄÇÂÆûÈôÖÊó∂Èó¥Â§çÊùÇÂ∫¶‰∏éÂÆû‰æãÂàÜÂ∏ÉÊÉÖÂÜµÊúâÂÖ≥Ôºå$t_{worst} = O(kN^{1-\frac{1}{k}})$ÔºåÈÄöÂ∏∏Ë¶ÅÊ±ÇÊï∞ÊçÆËßÑÊ®°ËææÂà∞$N \geq 2^D$ÊâçËÉΩËææÂà∞È´òÊïàÁöÑÊêúÁ¥¢„ÄÇ 3. ÊîπËøõÁÆóÊ≥ïÔºöBBFÁÆóÊ≥ïÂõûÊ∫ØÊòØÁî±Êü•ËØ¢Ë∑ØÂæÑÂÜ≥ÂÆöÁöÑÔºåÂõ†Ê≠§‰∏ÄÁßçÁÆóÊ≥ïÊîπËøõÊÄùË∑ØÂ∞±ÊòØÂ∞ÜÊü•ËØ¢Ë∑ØÂæÑ‰∏äÁöÑÁªìÁÇπÊéíÂ∫èÔºåÂõûÊ∫ØÊ£ÄÊü•ÊÄªÊòØ‰ªé‰ºòÂÖàÁ∫ßÊúÄÈ´òÁöÑÊ†ëËäÇÁÇπÂºÄÂßã‚Äî‚ÄîBest-Bin-First BBFÁÆóÊ≥ï„ÄÇËØ•ÁÆóÊ≥ïËÉΩÁ°Æ‰øù‰ºòÂÖàÊ£ÄÁ¥¢ÂåÖÂê´ÊúÄÈÇªËøëÁÇπÂèØËÉΩÊÄßËæÉÈ´òÁöÑÁ©∫Èó¥„ÄÇ ‰ºòÂÖàÈòüÂàóÔºö‰ºòÂÖàÁ∫ßÂèñÂÜ≥‰∫éÂÆÉ‰ª¨Á¶ªÊü•ËØ¢ÁÇπÁöÑË∑ùÁ¶ªÔºåË∑ùÁ¶ªË∂äËøëÔºå‰ºòÂÖàÁ∫ßË∂äÈ´òÔºåÂõûÊ∫ØÁöÑÊó∂ÂÄô‰ºòÂÖàÈÅçÂéÜ„ÄÇ ÂØπÂõûÊ∫ØÂèØËÉΩÈúÄË¶ÅË∑ØËøáÁöÑÁªìÁÇπÂä†ÂÖ•ÈòüÂàóÔºöÂàáÂàÜÁöÑÊó∂ÂÄôÔºåÊääÊú™ÈÄâ‰∏≠ÁöÑÈÇ£‰∏™ÂÖÑÂºüÁªìÁÇπÂä†ÂÖ•Âà∞ÈòüÂàó‰∏≠„ÄÇ]]></content>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Real-Time loop Loop Closure in 2D Lidar SLAM ËÆ∫ÊñáÁ¨îËÆ∞]]></title>
    <url>%2F2018%2F05%2F27%2FReal-Time-loop-Loop-Closure-in-2D-Lidar-SLAM%2F</url>
    <content type="text"><![CDATA[ÊñáÁ´†ÁöÑÊ†∏ÂøÉÊÄùÊÉ≥Âú®‰∫éËß£ÂÜ≥loop closureÈóÆÈ¢ò„ÄÇ ÂÖ®Â±ÄÂú∞ÂõæÁî±‰∏ÄÁ≥ªÂàóÁöÑsubmapÊûÑÊàêÔºåÊØè‰∏™submapÂàôÁî±‰∏ÄÁ≥ªÂàóÁöÑ‰ΩçÂßøËäÇÁÇπÂèäÂØπÂ∫îÁöÑscanÊï∞ÊçÆÊûÑÊàê„ÄÇ ÊñáÁ´†ÁöÑÈáçÁÇπÂú®Á¨¨ÂõõÈÉ®ÂàÜÂíåÁ¨¨‰∫îÈÉ®ÂàÜÔºö Á¨¨ÂõõÈÉ®ÂàÜÔºölocal 2d slamÔºåÂ∞Üscan‰∏éÂΩìÂâçsubmapÁöÑÂåπÈÖçÈóÆÈ¢òËΩ¨ÂåñÊàê‰∏Ä‰∏™ÊúÄÂ∞è‰∫å‰πò‰ºòÂåñÈóÆÈ¢òÔºåÁî±ceresÊù•Ê±ÇËß£„ÄÇÂèÇËÄÉÊñáÁåÆ„ÄäMany-to-Many Multi-Resolution Scan Matching „Äã Á¨¨‰∫îÈÉ®ÂàÜÔºöclosing loopÔºåÈááÁî®SPAËøõË°åÂêéÁ´Øloop closureÔºåÊèêÂá∫‰∏ÄÁßçÂπ∂Ë°åÁöÑscan‰∏éfinished submapsÂåπÈÖçÁöÑÊñπÊ≥ïBBSÔºåÂ§ßÂπÖÊèêÈ´òÁ≤æÂ∫¶ÂíåÈÄüÂ∫¶„ÄÇÂèÇËÄÉÊñáÁåÆ„ÄäEfficient Sparse Pose Adjustment for 2D Mapping(SPA)„Äã„ÄÅ„ÄäReal-Time Correlative Scan Matching(BBS)„Äã]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[‰∏âÁª¥Âàö‰ΩìËøêÂä® & Êùé‰ª£Êï∞]]></title>
    <url>%2F2018%2F05%2F12%2F%E4%B8%89%E7%BB%B4%E5%88%9A%E4%BD%93%E8%BF%90%E5%8A%A8-%E6%9D%8E%E4%BB%A3%E6%95%B0%2F</url>
    <content type="text"><![CDATA[0. ÂêëÈáè ÂùêÊ†áÔºöÈ¶ñÂÖàÁ°ÆÂÆö‰∏Ä‰∏™ÂùêÊ†áÁ≥ªÔºå‰πüÂ∞±Á°ÆÂÆö‰∫Ü‰∏ÄÁªÑÂü∫$(e_1, e_2, e_3)$ÔºåÈÇ£‰πàÂêëÈáè$a$ÁöÑÂùêÊ†á‰∏∫Ôºö a = [e_1, e_2, e_3] \begin{bmatrix} a_1\\ a_2\\ a_3 \end{bmatrix} = a_1e_1 + a_2e_2 + a_3e_3 ÂÜÖÁßØÔºöÂØπÂêëÈáè$a, b \in R^3$ÔºåÂÖ∂ÂÜÖÁßØ‰∏∫Ôºö a \cdot b = a^Tb = \Sigma a_ib_i = |a||b|cosÂÜÖÁßØÂèØ‰ª•ÊèèËø∞ÂêëÈáèÈó¥ÁöÑÊäïÂΩ±ÂÖ≥Á≥ª„ÄÇ Â§ñÁßØÔºö a \times b = \begin{vmatrix} i & j & k\\ a_1 & a_2 & a_3\\ b_1 & b_2 & b_3 \end{vmatrix}= \begin{bmatrix} 0 & -a_3 & a_2\\ a_3 & 0 & -a_1\\ -a_2 & a_1 & 0 \end{bmatrix}b=a^{\wedge}bÂ§ñÁßØÁöÑÊñπÂêëÂûÇÁõ¥‰∏éËøô‰∏§‰∏™ÂêëÈáèÔºåÂ§ßÂ∞è‰∏∫$|a||b|sin$„ÄÇ Â§ñÁßØÂèØ‰ª•Ë°®Á§∫ÂêëÈáèÁöÑÊóãËΩ¨ÔºåÂêëÈáè$a$Âà∞$b$ÁöÑÊóãËΩ¨ÂêëÈáèÔºåÂ§ñÁßØÁöÑÊñπÂêëÊòØÊóãËΩ¨ÂêëÈáèÁöÑÊñπÂêëÔºåÂ§ßÂ∞èÁî±Â§πËßíÂÜ≥ÂÆö„ÄÇ 1. ÊóãËΩ¨Áü©ÈòµR‰∏éÂèòÊç¢Áü©ÈòµT ÈÄöÂ∏∏ËÆæÁΩÆÂõ∫ÂÆöÁöÑ‰∏ñÁïåÂùêÊ†áÁ≥ª$O_w$ÂíåËøêÂä®ÁöÑÁõ∏Êú∫ÂùêÊ†áÁ≥ª$O_c$ÔºåÁõ∏Êú∫ËøêÂä®ÊòØÂàö‰ΩìËøêÂä®Ôºå‰∏§‰∏™ÂùêÊ†áÁ≥ª‰πãÈó¥ÁöÑÂèòÊç¢Áß∞‰∏∫Ê¨ßÂºèÂèòÊç¢„ÄÇ ÊóãËΩ¨Áü©Èòµ$R$ÔºöÂèØ‰ª•ÊèèËø∞Áõ∏Êú∫ÁöÑÊóãËΩ¨ ÂùêÊ†áÁ≥ªÊóãËΩ¨ÂâçÂêéÂêå‰∏Ä‰∏™ÂêëÈáèÁöÑÂùêÊ†áÂèòÊç¢ÂÖ≥Á≥ªÔºö a =\begin{bmatrix} a_1\\ a_2\\ a_3 \end{bmatrix}= \begin{bmatrix} e_1^T\\ e_2^T\\ e_3^T \end{bmatrix} \begin{bmatrix} e_1^{'} & e_2^{'}& e_3^{'} \end{bmatrix} \begin{bmatrix} a_1^{'}\\ a_2^{'}\\ a_3^{'} \end{bmatrix}= Ra^{'}‰∏çÈöæÈ™åËØÅÊóãËΩ¨Áü©ÈòµÊòØË°åÂàóÂºè‰∏∫1ÁöÑÊ≠£‰∫§Áü©ÈòµÔºåÂõ†Ê≠§ÂèØ‰ª•ÊääÊóãËΩ¨Áü©ÈòµÁöÑÈõÜÂêàÁâπÊÆäÊ≠£‰∫§Áæ§ÂÆö‰πâÂ¶Ç‰∏ãÔºö SO(n) = \{R \in R^{n*n} | RR^T=I, det(R)=1\}Áõ∏ÂèçÁöÑÊóãËΩ¨Ôºö a = Ra^{'}\\ a^{'} = R^{-1}a = R^Ta Ê¨ßÂºèÂèòÊç¢ÔºöÂåÖÊã¨ÊóãËΩ¨ÂíåÂπ≥Áßª a^{'} = Ra + t ÈΩêÊ¨°ÂùêÊ†áÔºöÂ∞ÑÂΩ±Âá†‰ΩïÁöÑÊ¶ÇÂøµÔºåÊØè‰∏™ÂàÜÈáèÂêå‰πò‰∏Ä‰∏™ÈùûÈõ∂Â∏∏Êï∞‰ªçÁÑ∂Ë°®Á§∫Âêå‰∏Ä‰∏™ÁÇπÔºö \tilde{x} = [x,y,z,w]^T=[x/w, y/w, z/w, 1]^T ÈΩêÊ¨°ÂèòÊç¢Áü©Èòµ$T$Ôºö‰ΩøÂæóÊ¨ßÂºèÂèòÊç¢‰ªçÊóß‰øùÊåÅÁ∫øÊÄßÂÖ≥Á≥ªÔºö \begin{bmatrix} a^{'}\\ 1 \end{bmatrix}= \begin{bmatrix} R & t\\ 0 &1 \end{bmatrix} \begin{bmatrix} a\\ 1 \end{bmatrix} =T \begin{bmatrix} a\\ 1 \end{bmatrix}ÂèòÊç¢Áü©ÈòµÁöÑÈõÜÂêàÁâπÊÆäÊ¨ßÂºèÁæ§Ôºö SE(3) = \left\{T= \begin{bmatrix} R & t\\ 0 & 1 \end{bmatrix} \in R^{4*4} | R\in SO(3), t \in R^3 \right\} 2. ÊóãËΩ¨ÂêëÈáè Axis-Angle‰∏Ä‰∏™ÊóãËΩ¨Âè™Êúâ3‰∏™Ëá™Áî±Â∫¶ÔºåÊóãËΩ¨Áü©ÈòµRË¶ÅÁî®9ÁöÑÂèÇÊï∞Êù•ÊèèËø∞ÔºåÊòæÁÑ∂ÊòØÂÜó‰ΩôÁöÑ„ÄÇ‰∏ÄÁßçÁ¥ßÂáëÁöÑÊñπÂºè‚Äî‚Äî‰ªª‰ΩïÊóãËΩ¨ÈÉΩÂèØ‰ª•Áî®‰∏Ä‰∏™ÊóãËΩ¨ËΩ¥$n$Âíå‰∏Ä‰∏™ÊóãËΩ¨Ëßí$\theta$Êù•ÂàªÁîªÔºö R = cos\theta I + (1-cos\theta)nn^T + sin\theta n^{\wedge}\\ \theta = arccos(\frac{tr(R)-1}{2})\\ÊóãËΩ¨ËΩ¥‰∏äÁöÑÂêëÈáèÂú®ÊóãËΩ¨Âêé‰∏çÂèëÁîüÊîπÂèòÔºåÂõ†Ê≠§ÊúâÔºö Rn = nËΩ¨ËΩ¥$n$ÊòØÊóãËΩ¨Áü©Èòµ$R$ÁöÑÁâπÂæÅÂÄº1ÂØπÂ∫îÁöÑÁâπÂæÅÂêëÈáèÔºåÂèØ‰ª•Áî±Ê≠§Êù•ËÆ°ÁÆóËΩ¨ËΩ¥$n$„ÄÇ 3. Ê¨ßÊãâËßí rpyÊääÊóãËΩ¨ÂàÜËß£Âà∞3‰∏™ËΩ¥‰∏äÔºårpyËßíÁöÑÊóãËΩ¨È°∫Â∫èÊòØZYXÔºö È¶ñÂÖàÁªïÁâ©‰ΩìÁöÑZËΩ¥ÊóãËΩ¨ÔºåÂæóÂà∞ÂÅèËà™Ëßíyaw ÁÑ∂ÂêéÁªïÊóãËΩ¨‰πãÂêéÁöÑYËΩ¥ÊóãËΩ¨ÔºåÂæóÂà∞‰øØ‰ª∞Ëßípitch ÁªïÊóãËΩ¨‰πãÂêéÁöÑXËΩ¥ÊóãËΩ¨ÔºåÂæóÂà∞ÊªöËΩ¨Ëßíroll ‰∏áÂêëÈîÅÈóÆÈ¢òÔºöÂú®‰øØ‰ª∞Ëßí‰∏∫$\pm 90^{\circ}$Êó∂ÔºåÁ¨¨‰∏ÄÊ¨°ÂíåÁ¨¨‰∏âÊ¨°ÊóãËΩ¨‰ΩøÁî®Âêå‰∏ÄÊ†πËΩ¥Ôºå‰∏¢‰∫ÜËá™Áî±Â∫¶‚Äî‚ÄîÂ•áÂºÇÊÄßÈóÆÈ¢ò„ÄÇ 4. ÂõõÂÖÉÊï∞ qÂõõÂÖÉÊï∞ÊòØ‰∏ÄÁßçÊâ©Â±ïÁöÑË¥üÊï∞ÔºåÁî±‰∏Ä‰∏™ÂÆûÈÉ®Âíå‰∏â‰∏™ËôöÈÉ®ÁªÑÊàêÔºåÂèØ‰ª•Êää‰∏â‰∏™ËôöÈÉ®ËÑëË°•ÊàêÁ©∫Èó¥‰∏≠ÁöÑ‰∏âÊ†πËΩ¥Ôºö q = q_0 + q_1i + q_2j + q_3k\\ \left\{ \begin{split} & i^2 = j^2=k^2=-1\\ & ij = k, ji = -k\\ & jk = i, kj = -i\\ & ki=j, ik=-j \end{split} \right. ‰πò‰ª•$i$ÂØπÂ∫îÁùÄÁªï$i$ËΩ¥ÊóãËΩ¨$180^{\circ}$ ‰ªªÊÑèÁöÑÊóãËΩ¨ÂèØ‰ª•Áî±‰∏§‰∏™‰∫í‰∏∫Áõ∏ÂèçÊï∞ÁöÑÂõõÂÖÉÊï∞Ë°®Á§∫ ‰∏éÊóãËΩ¨ÂêëÈáè$n = [n_x, n_y, n_z]^T, \theta$ËΩ¨Êç¢ÂÖ≥Á≥ªÔºö q = [cos\frac{\theta}{2}, n_xsin\frac{\theta}{2}, n_ysin\frac{\theta}{2}, n_zsin\frac{\theta}{2}]^T\\ \left\{ \begin{split} & \theta = 2arccos q_0\\ & [n_x, n_y, n_z]^T = [q_1, q_2, q_3]^T/sin\frac{\theta}{2} \end{split} \right. ‰∏éÊóãËΩ¨Áü©Èòµ$R$ÁöÑÂÖ≥Á≥ªÔºö R = \begin{bmatrix} 1-2q_2^2 - 2q_3^2 & 2q_1q_2-2q_0q_3 & 2q_1q_3+2q_0q_2\\ 2q_1q_2+2q_0q_3 & 1-2q_1^2 - 2q_3^2 & 2q_2q_3-2q_0q_1\\ 2q_1q_3-2q_0q_2 & 2q_2q_3+2q_0q_1 & 1-2q_1^2 - 2q_2^2 \end{bmatrix}\\ q_0 = \frac{\sqrt{tr(R)+1}}{2}, q_1 = \frac{R_{23}-R_{32}}{4q_0}, q_2 = \frac{R_{31}-R_{13}}{4q_0}, q_3 = \frac{R_{12}-R_{21}}{4q_0} Ë°®Á§∫ÊóãËΩ¨Ôºö Á©∫Èó¥‰∏≠ÁÇπ$p = [x, y,z]^T\in R^3$ÔºåÂ∑≤Áü•ÊóãËΩ¨ËΩ¥Ëßí$n,\theta$ÔºåÊóãËΩ¨‰πãÂêéÁÇπÂùêÊ†áÂèò‰∏∫$p^{‚Äò}$ÔºåÂ¶ÇÊûúÁî®ÊóãËΩ¨Áü©ÈòµÊèèËø∞Ôºö p^{'} = RpÂõõÂÖÉÊï∞$q = [cos\frac{\theta}{2}, nsin\frac{\theta}{2}]$ÔºåÈÇ£‰πàÊóãËΩ¨ÂêéÁöÑÁÇπ$p^{‚Äò}$ÂèØ‰ª•Ë°®Á§∫‰∏∫Ôºö p^{'} = qpq^{-1} 5. ÊùéÁæ§‰∏äÈù¢ÊèêÂà∞‰∫ÜÊóãËΩ¨Áü©ÈòµÊûÑÊàêÁöÑÁâπÊÆäÊ≠£‰∫§Áæ§$SO(3)$ÂíåÁî±ÂèòÊç¢Áü©ÈòµÊûÑÊàêÁöÑÁâπÊÆäÊ¨ßÂºèÁæ§$SE(3)$Ôºö SO(n) = \left\{R \in R^{n*n} | RR^T=I, det(R)=1\right\} \\ SE(3) = \left\{T= \begin{bmatrix} R & t\\ 0 & 1 \end{bmatrix} \in R^{4*4} | R\in SO(3), t \in R^3 \right\} $SO(n)$Âíå$SE(n)$ÂØπÂä†Ê≥ï‰∏çÂ∞ÅÈó≠ÔºåÂØπ‰πòÊ≥ïÊòØÂ∞ÅÈó≠ÁöÑ„ÄÇ Áæ§ÊòØ‰∏ÄÁßçÈõÜÂêà$A$Âä†‰∏ä‰∏ÄÁßçËøêÁÆó$\ \cdot \ $ÁöÑ‰ª£Êï∞ÁªìÊûÑÔºåËÆ∞‰Ωú$G = (A, \ \cdot \ )$ÔºåÁæ§ÂÜÖÂÖÉÁ¥†Êª°Ë∂≥Â∞ÅÈó≠ÊÄß„ÄÅÁªìÂêàÂæã„ÄÅÂπ∫ÂÖÉ„ÄÅÂèØÈÄÜÂõõ‰∏™Êù°‰ª∂„ÄÇ ÊùéÁæ§ÊòØÊåáÂÖ∑ÊúâËøûÁª≠ÊÄßË¥®ÁöÑÁæ§„ÄÇÂàö‰ΩìÂú®Á©∫Èó¥‰∏≠ËÉΩÂ§üËøûÁª≠Âú∞ËøêÂä®ÔºåÂõ†Ê≠§$SO(n)$Âíå$SE(n)$ÊòØÊùéÁæ§„ÄÇ 6. Êùé‰ª£Êï∞6.1 ÂºïÂÖ• ÂØπ‰ªªÊÑèÊóãËΩ¨Áü©Èòµ$R$ÔºåÈÉΩÊª°Ë∂≥$RR^T=I$„ÄÇÊääÂÆÉÂÜôÊàêÂÖ≥‰∫éÊó∂Èó¥ÁöÑÂáΩÊï∞$R(t)$ÊúâÔºö R(t)R(t)^T = IÂØπÁ≠âÂºè‰∏§ËæπÊ±ÇÂØºÔºö \dot R(t)R(t)^T + R(t)\dot R(t)^T=0\\ \dot R(t)R(t)^T =-\big(\dot R(t)R(t)^T \big)^TÂèØ‰ª•ÁúãÂá∫$\dot R(t)R(t)^T $ÊòØ‰∏Ä‰∏™ÂèçÂØπÁß∞ÈòµÔºåÂØπ‰ªªÊÑè‰∏Ä‰∏™ÂèçÂØπÁß∞ÈòµÔºåÈÉΩÂèØ‰ª•ÊâæÂà∞‰∏Ä‰∏™‰∏é‰πãÂØπÂ∫îÁöÑÂêëÈáèÔºö a^{\wedge} = A, A^{\vee}=a‰∫éÊòØÂèØ‰ª•ÊâæÂà∞‰∏Ä‰∏™‰∏âÁª¥ÂêëÈáè$\phi(t) \in R^3$‰∏é‰πãÂØπÂ∫îÔºö \dot R(t)R(t)^T = \phi(t)^{\wedge}\\ \dot R(t) = \phi(t)^{\wedge}R(t)ÂèØ‰ª•ÁúãÂà∞ÔºåÊØèÂØπÊóãËΩ¨Áü©ÈòµÊ±Ç‰∏ÄÊ¨°ÂØºÊï∞ÔºåÂè™ÈúÄÂ∑¶‰πò‰∏Ä‰∏™ÂèçÂØπÁß∞Èòµ$\phi(t)^{\wedge}$Âç≥ÂèØ„ÄÇ Ê±ÇËß£‰∏äÈù¢ÁöÑÂæÆÂàÜÊñπÁ®ãÔºåÂèØ‰ª•ÂæóÂà∞$R(t) = exp(\phi^{\wedge}t)$„ÄÇ‰πüÂ∞±ÊòØËØ¥$\phi$ÊèèËø∞‰∫Ü$R$Âú®Â±ÄÈÉ®ÁöÑÂØºÊï∞ÂÖ≥Á≥ª„ÄÇ 6.2 Êùé‰ª£Êï∞ ÊØè‰∏™ÊùéÁæ§ÈÉΩÊúâ‰∏é‰πãÂØπÂ∫îÁöÑÊùé‰ª£Êï∞„ÄÇÊùé‰ª£Êï∞ÊèèËø∞‰∫ÜÊùéÁæ§ÁöÑÂ±ÄÈÉ®ÊÄßË¥®„ÄÇ Êùé‰ª£Êï∞Áî±‰∏Ä‰∏™ÈõÜÂêà$V$Ôºå‰∏Ä‰∏™Êï∞Âüü$F$ÔºåÂíå‰∏Ä‰∏™‰∫åÂÖÉËøêÁÆóÊùéÊã¨Âè∑$[,]$ÁªÑÊàêÔºåËÆ∞‰Ωú$( V, F, [,])$„ÄÇÊùé‰ª£Êï∞ÁöÑÂÖÉÁ¥†Êª°Ë∂≥Â∞ÅÈó≠ÊÄß„ÄÅÂèåÁ∫øÊÄß„ÄÅËá™ÂèçÊÄß„ÄÅÈõÖÂèØÊØîÁ≠â‰ª∑ÂõõÊù°ÊÄßË¥®„ÄÇ ‰∏ä‰∏ÄËäÇÁöÑ$\phi$Â∞±ÊòØ$SO(3)$ÂØπÂ∫îÁöÑÊùé‰ª£Êï∞$so(3)$Ôºå‰∏§ËÄÖÁöÑÂÖ≥Á≥ªÁî±ÊåáÊï∞Êò†Â∞ÑÁªôÂÆöÔºö R = exp(\phi^{\wedge})\\ so(3) = \left\{ \phi \in R^3, \Phi = \phi^{\wedge} \in R^{3*3}\right\} $SE(3)$ÂØπÂ∫îÁöÑÊùé‰ª£Êï∞$se(3)$‰Ωç‰∫é$R^6$Á©∫Èó¥‰∏≠Ôºö se(3) = \left\{ \xi = \begin{bmatrix} \rho\\ \phi \end{bmatrix} \in R^6, \rho \in so(3), \xi^{\wedge} = \begin{bmatrix} \phi^{\wedge} & \rho\\ 0^T & 0 \end{bmatrix} \in R^{4*4} \right\} ÊåáÊï∞Êò†Â∞Ñ Áî±‰∫é$\phi$ÊòØ‰∏Ä‰∏™‰∏âÁª¥ÂêëÈáèÔºåÂõ†Ê≠§ÂèØ‰ª•ÂÜô‰Ωú$\theta a$ÁöÑÂΩ¢ÂºèÔºå$a$ÊòØ‰∏Ä‰∏™Âçï‰ΩçÂêëÈáèÔºåÂõ†Ê≠§ÂÖ∑Êúâ‰ª•‰∏ãÊÄßË¥®Ôºö a^{\wedge}a^{\wedge} = aa^T-I\\ a^{\wedge}a^{\wedge}a^{\wedge} = -a^{\wedge}ÂØπ$so(3)$Êùé‰ª£Êï∞ÁöÑÊåáÊï∞Êò†Â∞ÑÂÅöÊ≥∞ÂãíÂ±ïÂºÄÔºåÂèØ‰ª•ÂæóÂà∞Ôºö \begin{split} &R= exp(\phi^{\wedge}) = exp(\theta a^{\wedge})=\Sigma_{n=0}^{\infty}\frac{1}{n!} (\theta a^{\wedge})^n\\ & =cos\theta I + (1-cos\theta)aa^T+sin\theta a^{\wedge} \end{split}ÂèØ‰ª•ÁúãÂà∞$so(3)$ÂÆûÈôÖ‰∏äÂ∞±ÊòØÊóãËΩ¨ÂêëÈáèÁªÑÊàêÁöÑÁ©∫Èó¥ÔºåÊåáÊï∞Êò†Â∞ÑÂç≥ÊòØÁΩóÂæ∑ÈáåÊ†ºÊñØÂÖ¨Âºè„ÄÇ ÊåáÊï∞Êò†Â∞ÑÊòØ‰∏Ä‰∏™Êª°Â∞ÑÔºåÊØè‰∏™$SO(3)$‰∏≠ÁöÑÂÖÉÁ¥†ÔºåÈÉΩÂèØ‰ª•ÊâæÂà∞Ëá≥Â∞ë‰∏Ä‰∏™$so(3)$ÂÖÉÁ¥†‰∏é‰πãÂØπÂ∫îÔºà$\theta + 2k\pi$Ôºâ„ÄÇ $se(3)$‰∏äÁöÑÊåáÊï∞Êò†Â∞Ñ‰∏∫Ôºö T = exp(\xi^{\wedge}) = \begin{bmatrix} R & J\rho\\ 0 &1 \end{bmatrix}\\ J = \frac{sin\theta}{\theta}I + (1-\frac{sin\theta}{\theta})aa^T + \frac{1-cos\theta}{\theta}a^{\wedge} 6.3 Êùé‰ª£Êï∞Ê±ÇÂØº ‰∏§‰∏™Êùé‰ª£Êï∞ÊåáÊï∞Êò†Â∞Ñ‰πòÁßØÁöÑÂÆåÊï¥ÂΩ¢ÂºèÁî±BCHÂÖ¨ÂºèÁªôÂá∫Ôºö ln(exp(A)exp(B)) = A+B + \frac{1}{2}[A, B] + \frac{1}{12}[A,[A,B]] + ... ÂØπ$ln(exp(\phi_1^{\wedge})exp(\phi_2^{\wedge}))^{\vee}$ÔºåÂΩì$\phi_1$Êàñ$\phi_2$‰∏∫Â∞èÈáèÊó∂ÔºåBCHÂÖ¨ÂºèÁªôÂá∫‰∫ÜÁ∫øÊÄßËøë‰ººË°®ËææÔºö ln(exp(\phi_1^{\wedge})exp(\phi_2^{\wedge}))^{\vee} = \left\{ \begin{split} J_l(\phi_2)^{-1}\phi_1 + \phi_2\ \ \ \ \ \ \phi_1‰∏∫Â∞èÈáè\\ J_r(\phi_1)^{-1}\phi_2 + \phi_1\ \ \ \ \ \ \phi_2‰∏∫Â∞èÈáè\\ \end{split} \right.BCHËøë‰ººÈõÖÂèØÊØî$J_l$Â∞±ÊòØ‰∏ä‰∏ÄËäÇÁöÑ$J$Ôºö J_l = J = \frac{sin\theta}{\theta}I + (1-\frac{sin\theta}{\theta})aa^T + \frac{1-cos\theta}{\theta}a^{\wedge}\\ J_l^{-1} = \frac{\theta}{2}cot\frac{\theta}{2}I + (1-\frac{\theta}{2}cot\frac{\theta}{2})aa^T - \frac{\theta}{2}a^{\wedge}\\ J_r(\phi) = J_l(-\phi)Áî±‰ª•‰∏äÂÖ¨ÂºèËØ¥Êòé‰∫ÜÊùéÁæ§‰πòÊ≥ïÂíåÊùé‰ª£Êï∞Âä†Ê≥ïÁöÑËøë‰ººËΩ¨Êç¢ÂÖ≥Á≥ª„ÄÇ Âú®$SO(3)„ÄÅSE(3)$‰∏äÊ≤°ÊúâËâØÂ•ΩÂÆö‰πâÁöÑÂä†Ê≥ïÔºåËÄåÊùé‰ª£Êï∞Áî±ÂêëÈáèÁªÑÊàêÔºåÊúâËâØÂ•ΩÁöÑÂä†Ê≥ïËøêÁÆó„ÄÇÂõ†Ê≠§Âú®ËÆ°ÁÆó‰ΩçÂßøÁöÑÂØºÊï∞Êó∂ÔºåÈÄöÂ∏∏‰ΩøÁî®Êùé‰ª£Êï∞Ëß£ÂÜ≥ÔºåÊùé‰ª£Êï∞Ê±ÇÂØºÁöÑ‰∏§ÁßçÊÄùË∑ØÔºö Êùé‰ª£Êï∞Ê±ÇÂØº$\delta \phi$ÔºöÁî®Êùé‰ª£Êï∞Ë°®Á§∫ÂßøÊÄÅÔºåÁÑ∂ÂêéËΩ¨ÂåñÊàêÂØπÊùé‰ª£Êï∞Ê±ÇÂØº$\phi + \delta \phi$ \begin{split} &\frac{\partial (Rp)}{\partial R} = \frac{\partial(exp(\phi^{\wedge})p)}{\partial \phi}\\ &= lim \frac{exp((\phi+\delta\phi)^{\wedge})p-exp(\phi^{\wedge})p}{\partial \phi}\\ &=-(Rp)^{\wedge}J_l \end{split} Êâ∞Âä®Ê®°Âûã$\Delta R$ÔºöÂØπ$R$ËøõË°åÊâ∞Âä®ÔºåÁÑ∂ÂêéÂØπÊâ∞Âä®Ê±ÇÂØº$\Delta R R$ \begin{split} &\frac{\partial (Rp)}{\partial R} = lim \frac{exp(\varphi^{\wedge})exp(\phi^{\wedge})p-exp(\phi^{\wedge})p}{\partial \varphi}= -(Rp)^{\wedge}\\ & \frac{\partial Tp}{\partial \delta \xi} = \begin{bmatrix} I & -(Rp+t)^{\wedge}\\ 0 & 0 \end{bmatrix} = (Tp)^{\odot} \end{split}]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++ for record]]></title>
    <url>%2F2018%2F05%2F10%2Fc-for-record%2F</url>
    <content type="text"><![CDATA[ÊúÄËøëÂºÄÂßãÁùÄÊâãÂÜôslam‰ª£Á†ÅÔºåÁúã‰∏Ä‰∫õÂ∏∏Áî®Â∫ìÊ∫êÁ†ÅÁöÑÊó∂ÂÄôÂèëÁé∞ÂêÑÁßçÂäõ‰∏ç‰ªéÂøÉÔºå‰∏Ä‰∫õc++11ÁöÑÈ™öÊìç‰ΩúÁ´üÁÑ∂Ê≤°ËßÅËøáÔºåÊòØÊó∂ÂÄôÂÆåÊï¥Êí∏‰∏ÄÂèëc++ primerÁ•≠Â§©‰∫Ü„ÄÇ iostream Ê†áÂáÜËæìÂÖ•Ôºöcin Ê†áÂáÜËæìÂá∫Ôºöcout„ÄÅcerr„ÄÅclog 123456#include &lt;iostream&gt;using namespace std;int v1=0, v2=0;cin &gt;&gt; v1 &gt;&gt; v2;cout &lt;&lt; v1+v2 &lt;&lt; endl;cerr &lt;&lt; "This is nonsense." &lt;&lt; endl; &lt;&lt; Âíå &gt;&gt; ÁöÑÊñπÂêëË°®Á§∫‰∫ÜÊï∞ÊçÆÊµÅÁöÑËµ∞ÂêëÔºå‰πüÂ∞±ÊòØËµãÂÄºÁöÑÊñπÂêë„ÄÇcerrÁî®Êù•ËæìÂá∫ÈîôËØØ‰ø°ÊÅØ„ÄÇ ÊéßÂà∂ÊµÅ whileÔºöÊØèÊ¨°ÊâßË°åÂæ™ÁéØ‰πãÂâçÂÖàÊ£ÄÊü•Âæ™ÁéØÊù°‰ª∂ do whileÔºöÂÖàÊâßË°åÂæ™ÁéØ‰ΩìÂêéÊ£ÄÊü•Êù°‰ª∂ 123456while (condition) statement do statementwhile (condition); forÔºöÊØèÊ¨°ÊâßË°åÂæ™ÁéØ‰πãÂâçÂÖàÊ£ÄÊü•Âæ™ÁéØÊù°‰ª∂ÔºåÊâßË°åÂæ™ÁéØ‰πãÂêéÊâßË°åË°®ËææÂºè 1234567for (init-statement; condition; expression)&#123; statemnt&#125;// ËåÉÂõ¥forËØ≠Âè•for (declaration : expression) statement switchÔºö case labelÔºöcaseÊ†áÁ≠æÂøÖÈ°ªÊòØÊï¥ÂΩ¢Â∏∏ÈáèË°®ËææÂºè Â¶ÇÊûúÊüê‰∏™caseÊ†áÁ≠æÂåπÈÖçÊàêÂäüÔºå‰ºöÂæÄÂêéÈ°∫Â∫èÊâßË°åÊâÄÊúâcaseÂàÜÊîØÔºåÁõ¥Âà∞ÁªìÂ∞æÊàñËÄÖÈÅáÂà∞break defaultÊ†áÁ≠æ 123456switch(ch)&#123; case 'a': case 'b': case 'c': ++cnt; break;&#125; breakÔºöË¥üË¥£ÁªàÊ≠¢Á¶ª‰ªñÊúÄËøëÁöÑwhile„ÄÅdo while„ÄÅforÊàñswitchËØ≠Âè•„ÄÇ continueÔºöË¥üË¥£ÁªàÊ≠¢Á¶ª‰ªñÊúÄËøëÁöÑwhile„ÄÅdo while„ÄÅforÂæ™ÁéØÁöÑÂΩìÂâçËø≠‰ª£„ÄÇ gotoÔºöÊó†Êù°‰ª∂Ë∑≥ËΩ¨Âà∞Âêå‰∏ÄÂáΩÊï∞ÂÜÖÁöÑÊüê‰∏™Â∏¶Ê†áÁ≠æËØ≠Âè•„ÄÇ labeled statement: label: statement ÂºÇÂ∏∏ throwÔºöÂºïÂèëÂºÇÂ∏∏ÔºåÂêéÈù¢Á¥ßÈöè‰∏Ä‰∏™ÂºÇÂ∏∏Á±ªÂûãÔºåÁªàÊ≠¢ÂΩìÂâçÂáΩÊï∞ÔºåÂ∞ÜÊéßÂà∂ÊùÉËΩ¨ÁßªÁªôËÉΩÂ§üÂ§ÑÁêÜËØ•ÂºÇÂ∏∏ÁöÑ‰ª£Á†Å„ÄÇ 123#include &lt;stdexcept&gt;// runtime_error Ê†áÂáÜÂ∫ìÂºÇÂ∏∏Á±ªÂûãthrow runtime_error("Data must refer to same name"); tryÔºöÂ§ÑÁêÜÂºÇÂ∏∏ÔºåÂêéÈù¢Á¥ßÈöè‰∏ÄÂ•ócatchÂ≠êÂè•Áî®Êù•Â§ÑÁêÜÂºÇÂ∏∏„ÄÇ 1234567try&#123; program statements&#125; catch (exception-declaration) &#123; handler-statements&#125; catch (exception-declaration) &#123; handler-statements&#125; ... tryËØ≠Âè•ÂùóÂÜÖÂ£∞ÊòéÁöÑÂèòÈáèÂú®ÂùóÂ§ñÊó†Ê≥ïËÆøÈóÆÔºåÂç≥‰ΩøÊòØcatchËØ≠Âè•„ÄÇ catch‰∏ÄÊó¶ÂÆåÊàêÔºåÁ®ãÂ∫èË∑≥ËΩ¨Âà∞ÊúÄÂêé‰∏Ä‰∏™catchÂ≠êÂè•‰πãÂêéÁöÑËØ≠Âè•„ÄÇ Á±ª Á±ªÂûã &amp; ÂØπË±°ÔºàÂÆû‰æãÔºâÔºåÂèòÈáè &amp; Ë°å‰∏∫ÔºàÊñπÊ≥ïÔºâ„ÄÇ Â≠òÂú®Á±ªÂÜÖÈªòËÆ§ÂàùÂßãÂåñ Á±ªÈÄöÂ∏∏Ë¢´ÂÆö‰πâÂú®Â§¥Êñá‰ª∂‰∏≠ÔºåÂ§¥Êñá‰ª∂ÂêçÂ≠óÂ∫î‰∏éÁ±ªÁöÑÂêçÂ≠ó‰øùÊåÅ‰∏ÄËá¥ Â§¥Êñá‰ª∂ÈÄöÂ∏∏ÂåÖÂê´Âè™ËÉΩË¢´ÂÆö‰πâ‰∏ÄÊ¨°ÁöÑÂÆû‰ΩìÔºåÂ¶ÇÁ±ª„ÄÅconstÁ≠â„ÄÇ Â§¥Êñá‰ª∂‰øùÊä§Á¨¶#ifndefÁ≥ªÂàóÔºåÂàõÂª∫È¢ÑÂ§ÑÁêÜÂèòÈáèÔºåÈò≤Ê≠¢Â§öÊ¨°ÂåÖÂê´„ÄÇ ÊûÑÈÄ†ÂáΩÊï∞ÂàùÂßãÂÄºÂàóË°®ÔºöÂÜíÂè∑‰ª•ÂèäÂÜíÂè∑ÂíåËä±Êã¨Âè∑‰πãÈó¥ÁöÑ‰ª£Á†Å ÂàóË°®Âè™ËØ¥ÊòéÁî®‰∫éÂàùÂßãÂåñÊàêÂëòÁöÑÂÄºÔºåËÄå‰∏çÈôêÂÆöÂàùÂßãÂåñÁöÑÂÖ∑‰ΩìÈ°∫Â∫è„ÄÇ ÊàêÂëòÁöÑÂàùÂßãÂåñÈ°∫Â∫è‰∏éÂÆÉ‰ª¨Âú®Á±ªÂÆö‰πâ‰∏≠ÁöÑÂá∫Áé∞È°∫Â∫è‰∏ÄËá¥„ÄÇ 12345678910// ‰∏∫Á±ªÊàêÂëòÂàùÂßãÂåñSales_data(const string &amp;s, unsigned n, double p) : bookNo(s), units_sold(n), revenue(p*n) &#123;&#125; // Âå∫Âà´‰∫éËµãÂÄºSales_data(const string &amp;s, unsigned n, double p)&#123; bookNo = s; ...&#125; Êé•Âè£‰∏éÂ∞ÅË£ÖÔºö ÂÆö‰πâÂú®privateËØ¥ÊòéÁ¨¶‰πãÂêéÁöÑÊàêÂëòÂè™ËÉΩË¢´Á±ªÂÜÖÊàêÂëòÂáΩÊï∞ËÆøÈóÆÔºåÂ∞ÅË£Ö‰∫ÜÁ±ªÁöÑÂÆûÁé∞ÁªÜËäÇ„ÄÇ ÂÆö‰πâÂú®publicËØ¥ÊòéÁ¨¶‰πãÂêéÁöÑÊàêÂëòÂèØ‰ª•Âú®Êï¥‰∏™Á®ãÂ∫èÂÜÖË¢´ËÆøÈóÆÔºåÂÆö‰πâÁ±ªÁöÑÊé•Âè£„ÄÇ classÂíåstructÁöÑÂå∫Âà´ÔºöÊàêÂëòËÆøÈóÆÊùÉÈôê structÔºöÂÆö‰πâÂú®Á¨¨‰∏Ä‰∏™ËØ¥ÊòéÁ¨¶‰πãÂâçÁöÑÊàêÂëòÊòØpublic classÔºöÂÆö‰πâÂú®Á¨¨‰∏Ä‰∏™ËØ¥ÊòéÁ¨¶‰πãÂâçÁöÑÊàêÂëòÊòØprivate ÂèãÂÖÉÔºöÂÖÅËÆ∏ÂÖ∂‰ªñÁ±ªÊàñÂáΩÊï∞ËÆøÈóÆÂÆÉÁöÑÈùûÂÖ¨ÊúâÊàêÂëòÔºåÂú®Á±ªÂÜÖÊ∑ªÂä†‰ª•friendÂÖ≥ÈîÆÂ≠óÂºÄÂßãÁöÑÂèãÂÖÉÂ£∞Êòé„ÄÇ ÂèãÂÖÉÁöÑÂ£∞Êòé‰ªÖ‰ªÖÊåáÂÆö‰∫ÜËÆøÈóÆÊùÉÈôêÔºåËÄåÈùû‰∏Ä‰∏™ÈÄöÂ∏∏ÊÑè‰πâ‰∏äÁöÑÂáΩÊï∞Â£∞Êòé„ÄÇ 12345678class Sales_data &#123;// ÂèãÂÖÉÂ£∞Êòéfriend Sales_data add(const Sales_data&amp;, const Sales_data&amp;);// ÈùûÂÖ¨ÊúâÊàêÂëòprivate: string bookNo; double revenue = 0.0;&#125;; ÈùôÊÄÅÊàêÂëòstaticÔºö‰∏éÁ±ªÊú¨Ë∫´Áõ∏ÂÖ≥ËÅîÔºå‰∏çÂ±û‰∫é‰ªª‰Ωï‰∏Ä‰∏™ÂØπË±°ÔºåÂõ†Ê≠§‰∏çÊòØÂú®ÂàõÂª∫Á±ªÂØπË±°ÁöÑÊó∂ÂÄôË¢´ÂÆö‰πâÁöÑÔºåÂõ†Ê≠§ÈÄöÂ∏∏Âú®Á±ªÁöÑÂ§ñÈÉ®ÂÆö‰πâÂíåÂàùÂßãÂåñÔºåÂú®Á±ªÂÜÖÈÉ®Ê∑ªÂä†‰ª•staticÂÖ≥ÈîÆÂ≠óÂºÄÂßãÁöÑÈùôÊÄÅÊàêÂëòÂ£∞Êòé„ÄÇ ÂÜÖÁΩÆÁ±ªÂûã ÂÜÖÂ≠ò‰∏≠ÁöÑ‰∏Ä‰∏™Âú∞ÂùÄÂØπÂ∫î‰∏Ä‰∏™Â≠óËäÇ unsignedÁ±ªÂûãË°®Á§∫Â§ß‰∫éÁ≠â‰∫é0ÁöÑÊï∞Ôºà$[0, 2^{n}-1]$ÔºâÔºåË¢´ËµãÁªô‰∏Ä‰∏™Ë∂ÖÂá∫Ë°®Á§∫ËåÉÂõ¥ÁöÑÊï∞Êó∂ÔºåËá™Âä®Âèñ‰ΩôÔºå‰Ωú‰∏∫Âæ™ÁéØÊù°‰ª∂Êó∂ÂΩìÂøÉËøõÂÖ•Êó†ÈôêÂæ™ÁéØ signedÁ±ªÂûãÊ≠£Ë¥üÂÄºËåÉÂõ¥Âπ≥Ë°°Ôºà$[-2^{n-1}, 2^{n-1}-1]$ÔºâÔºåË¢´ËµãÁªô‰∏Ä‰∏™Ë∂ÖÂá∫Ë°®Á§∫ËåÉÂõ¥ÁöÑÊï∞Êó∂ÔºåÁªìÊûúÊú™ÂÆö‰πâ Â≠óÁ¨¶ÂûãcharÔºåÂçïÂºïÂè∑Ôºå‰∏Ä‰∏™Â≠óËäÇ Â≠óÁ¨¶‰∏≤ÂûãÔºåÂèåÂºïÂè∑ÔºåÂ∏∏ÈáèÂ≠óÁ¨¶Êï∞ÁªÑÔºåÁªìÂ∞æÈöêÂê´Á©∫Â≠óÁ¨¶ ‚Äò\0‚Äô nullptr = 0Ôºà‰º†ÁªüNULLÂåÖÂê´Âú®cstdlibÂ§¥Êñá‰ª∂ÂÜÖÔºâ ÂèòÈáè ÂàóË°®ÂàùÂßãÂåñÔºåËä±Êã¨Âè∑ 123456// Êã∑Ë¥ùÂàùÂßãÂåñint x=0;int x=&#123;0&#125;;// Áõ¥Êé•ÂàùÂßãÂåñint x&#123;0&#125;;int x(0); ÂèòÈáèÂ£∞ÊòéexternÔºåÊ∫ê‰∫éÂàÜÁ¶ªÂºèÁºñËØëÊú∫Âà∂Ôºå‰∏Ä‰∏™ÂèòÈáèÂè™ËÉΩË¢´ÂÆö‰πâ‰∏ÄÊ¨°ÔºåÂèØ‰ª•Â£∞ÊòéÂ§öÊ¨° ‰ΩúÁî®ÂüüÔºåÂµåÂ•ó‰ΩúÁî®Âüü &amp; ÂÜÖÈÉ®ÈáçÂÆö‰πâ Â§çÂêàÁ±ªÂûã ÂºïÁî®Ôºåtypename &amp;declarationÔºåÊµÖÊã∑Ë¥ùÔºåÁªëÂÆö‰∏Ä‰∏™ÂØπË±°ÔºåÂºïÁî®‰∏çÊòØÂØπË±° ÊåáÈíàÔºåtypename *declarationÔºåÂ≠òÊîæÂØπË±°Âú∞ÂùÄ 1234int a;int *p, *q=a;p = &amp;a;p = q; ÂèñÂú∞ÂùÄÁ¨¶&amp; 123int *p = a;int *p = &amp;a;// a---&gt;ÂØπË±° &amp;a---&gt;Âú∞ÂùÄ Ëß£ÂºïÁî®Á¨¶* 123456int a;int *p;*p ---&gt; undefinedp = &amp;a;*p = 10;// p---&gt;ÊåáÈíà *p---&gt;ÂØπË±° void* ÊåáÈíàÔºåÂèØ‰ª•ÊåáÂêë‰ªªÊÑèÁ±ªÂûãÁöÑÂØπË±°Ôºå‰ΩÜÊòØ‰∏çËÉΩËøõË°åÂØπË±°Êìç‰Ωú constÈôêÂÆöÁ¨¶ ÂèÇ‰∏éÁºñËØëÈ¢ÑÂ§ÑÁêÜ Ë¶ÅÂÆûÁé∞Â§ö‰∏™Êñá‰ª∂ÂÖ±‰∫´ÔºåÂøÖÈ°ªÂú®constÂèòÈáèÂÆö‰πâ‰πãÂâçÂä†‰∏äexternÂÖ≥ÈîÆÂ≠ó 1234// defineextern const int bufferSize = fcn();// declareextern const int bufferSize; ÂÖÅËÆ∏‰ªªÊÑèË°®ËææÂºè‰Ωú‰∏∫ÂàùÂßãÂÄºÔºàÂÖÅËÆ∏ÈöêÂºèÁ±ªÂûãËΩ¨Êç¢Ôºâ Â∏∏ÈáèÂºïÁî®ÔºåÂÖÅËÆ∏ÈùûÂ∏∏ÈáèËµãÂÄºÔºåÂÆûÈôÖÂºïÁî®‰∏Ä‰∏™ÂÜÖÂ≠ò‰∏≠ÁöÑ‚Äú‰∏¥Êó∂ÂÄº‚Äù ÊåáÂêëÂ∏∏ÈáèÁöÑÊåáÈíàÔºåÂÖÅËÆ∏ÈùûÂ∏∏ÈáèËµãÂÄºÔºå‰ΩÜÊòØ‰∏çËÉΩÈÄöËøáËØ•ÊåáÈíà‰øÆÊîπÂØπË±° Â∏∏ÈáèÊåáÈíàÔºåÊåáÈíàÂßãÁªàÊåáÂêëÂêå‰∏Ä‰∏™ÂØπË±° Â∏∏ÈáèË°®ËææÂºèconstexprÔºåË°®ËææÂºèÂú®ÁºñËØëËøáÁ®ã‰∏≠Â∞±ËÉΩÂæóÂà∞ËÆ°ÁÆóÁªìÊûú Â§ÑÁêÜÁ±ªÂûã Á±ªÂûãÂà´Âêçtypedef &amp; using 1234567// ‰º†Áªütypedef double base;typedef base *p; // pÊòØdoubleÊåáÈíàbase a;p p1=&amp;a;// c++11using base = double; autoÁ±ªÂûãËØ¥ÊòéÁ¨¶ÔºåËÆ©ÁºñËØëÂô®ÂàÜÊûêË°®ËææÂºèÊâÄÂ±ûÁ±ªÂûãÂπ∂‰∏∫ÂèòÈáèËµãÂÄº 12// ‰∏ÄÊù°Á±ªÂûãÂ£∞ÊòéËØ≠Âè•‰∏≠ÊâÄÊúâÂèòÈáèÁöÑÁ±ªÂûãÂøÖÈ°ª‰øùÊåÅ‰∏ÄËá¥auto i=0, *p=&amp;i; decltypeÁ±ªÂûãÊåáÁ§∫Á¨¶Ôºå‰ªÖÂàÜÊûêË°®ËææÂºèËøîÂõûÁ±ªÂûãÔºå‰∏çÂÅöËµãÂÄºÔºàÂõ†Ê≠§‰∏çÂÅöÂÆûÈôÖËÆ°ÁÆóÔºâ 1decltype(f()) a=x; string ËØªÂèñÔºå&gt;&gt;‰∏çËØªÂèñÁ©∫ÁôΩÔºåÈÅáÂà∞Á©∫ÁôΩÁ¨¶ÂÅúÊ≠¢Ôºågetline‰øùÁïôÁ©∫ÁôΩÁ¨¶ÔºåÈÅáÂà∞Êç¢Ë°åÁ¨¶ÂÅúÊ≠¢„ÄÇ Â≠óÁ¨¶‰∏≤Â≠óÈù¢ÂÄº‰∏çÊòØstringÂØπË±°ÔºåËÄåÊòØCÈ£éÊ†ºÂ≠óÁ¨¶‰∏≤Ôºåc_str()ÊàêÂëòÂáΩÊï∞ËÉΩÂ§üÂ∞ÜstringÂØπË±°ËΩ¨ÂåñÊàêCÈ£éÊ†ºÂ≠óÁ¨¶‰∏≤ ÈÅçÂéÜÔºåËåÉÂõ¥forËØ≠Âè•ÔºåÊØèÊ¨°Ëø≠‰ª£declareÁöÑÂèòÈáè‰ºöË¢´ÂàùÂßãÂåñ‰∏∫expressionÁöÑ‰∏ã‰∏Ä‰∏™ÂÖÉÁ¥† 123456789for (declaration : expression) statementstring str("some string");// ËµãÂÄºfor (auto c: str) cout &lt;&lt; c &lt;&lt; endl;// ÂºïÁî®for (auto &amp;c: str) c = toupper(c); size()ËøîÂõûÁöÑÁ±ªÂûãÊòØstring::size_typeÔºåÈÄöÂ∏∏Áî®auto vector Á±ªÊ®°ÁâàÔºåÁõ∏ÂêåÁ±ªÂûãÂØπË±°ÁöÑÈõÜÂêàÔºåÂ£∞ÊòéÊó∂ÂøÖÈ°ªÊèê‰æõÂÖÉÁ¥†Á±ªÂûãvector&lt;int&gt; Ê∑ªÂä†ÂÖÉÁ¥†push_back() Ëø≠‰ª£Âô® ÊâÄÊúâÊ†áÂáÜÂ∫ìÂÆπÂô®ÈÉΩÊîØÊåÅËø≠‰ª£Âô®ÔºåÂè™ÊúâÂ∞ëÊï∞ÊîØÊåÅ‰∏ãÊ†áËÆøÈóÆ begin()ËøîÂõûÊåáÂêëÁ¨¨‰∏Ä‰∏™ÂÖÉÁ¥†ÁöÑËø≠‰ª£Âô®Ôºåend()ËøîÂõûÂ∞æÂêéÂÖÉÁ¥†ÁöÑËø≠‰ª£Âô® cbegin()Âíåcend()Êìç‰ΩúÁ±ª‰ººÔºåËøîÂõûÂÄºÊòØconst_iteratorÔºå‰∏çËÉΩ‰øÆÊîπÂØπË±° Ëø≠‰ª£Âô®ÁöÑÁ±ªÂûãÊòØcontainer::iteratorÂíåcontainer::const_iteratorÔºåÈÄöÂ∏∏Áî®auto Ëß£ÂºïÁî®Ëø≠‰ª£Âô®ÂæóÂà∞ÂØπË±° ÁÆ≠Â§¥ËøêÁÆóÁ¨¶-&gt;ÔºåÁªìÂêàËß£ÂºïÁî®+ÊàêÂëòËÆøÈóÆ‰∏§‰∏™Êìç‰Ωú Ëø≠‰ª£Âô®Â§±ÊïàÔºöÂÆπÂô®ÊîπÂèòÂÆπÈáè Êï∞ÁªÑ Â§ßÂ∞èÂõ∫ÂÆöÔºåÁºñËØëÁöÑÊó∂ÂÄôÁª¥Â∫¶Â∫îËØ•Â∑≤Áü•ÔºåÂõ†Ê≠§ÂøÖÈ°ªÊòØÂ∏∏ÈáèË°®ËææÂºè ‰∏çËÉΩÁî®ÂÅöÊã∑Ë¥ùÂíåËµãÂÄº Ë°®ËææÂºè Â∑¶ÂÄºÂíåÂè≥ÂÄº ‚Äã CËØ≠Ë®Ä‰∏≠ÔºåÂ∑¶ÂÄºÊåáÁöÑÊòØÊó¢ËÉΩÂá∫Áé∞Âú®Á≠âÂè∑Â∑¶Ëæπ‰πüËÉΩÂá∫Áé∞Âú®Á≠âÂè∑Âè≥ËæπÁöÑÂèòÈáèÊàñË°®ËææÂºèÔºåÈÄöÂ∏∏Êù•ËØ¥Â∞±ÊòØÊúâÂêçÂ≠óÁöÑÂèòÈáèÔºåËÄåÂè≥ÂÄºÂè™ËÉΩÂá∫Áé∞Âú®Á≠âÂè∑Âè≥‰æßÔºåÈÄöÂ∏∏Â∞±ÊòØ‰∏Ä‰∫õÊ≤°ÊúâÂêçÂ≠ó‰πüÂèñ‰∏çÂà∞Âú∞ÂùÄÁöÑ‰∏≠Èó¥ÁªìÊûú„ÄÇ ÁªßÊâøÂà∞C++‰∏≠ÂΩíÁ∫≥Êù•ËÆ≤Â∞±ÊòØÔºöÂΩì‰∏Ä‰∏™ÂØπË±°Ë¢´Áî®‰ΩúÂè≥ÂÄºÁöÑÊó∂ÂÄôÔºåÁî®ÁöÑÊòØÂØπË±°ÁöÑÂÄºÔºàÂÜÖÂÆπÔºâÔºåÂΩìË¢´Áî®‰ΩúÂ∑¶ÂÄºÁöÑÊó∂ÂÄôÔºåÁî®ÁöÑÊòØÂØπË±°ÁöÑË∫´‰ªΩÔºàÂú®ÂÜÖÂ≠ò‰∏≠ÁöÑ‰ΩçÁΩÆÔºâ„ÄÇ Ê±ÇÂÄºÈ°∫Â∫è ÊúâÂõõÁßçËøêÁÆóÁ¨¶ÊòéÁ°ÆËßÑÂÆö‰∫ÜÊ±ÇÂÄºÈ°∫Â∫èÔºåÈÄªËæë‰∏éÔºà&amp;&amp;Ôºâ„ÄÅÈÄªËæëÊàñÔºà||Ôºâ„ÄÅÊù°‰ª∂Ôºà?:Ôºâ„ÄÅÈÄóÂè∑Ôºà,ÔºâËøêÁÆóÁ¨¶„ÄÇ 12int i = 0;cout &lt;&lt; i &lt;&lt; ++i &lt;&lt; endl; ÂâçÁΩÆÁâàÊú¨ÂíåÂêéÁΩÆÁâàÊú¨ÁöÑÈÄíÂ¢ûÈÄíÂáè Áî®‰∫éÂ§çÂêàËøêÁÆó‰∏≠Êó∂Ôºå ÂâçÁΩÆÁâàÊú¨È¶ñÂÖà‰øÆÊîπÂØπË±°ÔºåÁÑ∂ÂêéÂ∞ÜÂØπË±°Êú¨Ë∫´‰Ωú‰∏∫Â∑¶ÂÄºËøîÂõû„ÄÇ ÂêéÁΩÆÁâàÊú¨Â∞ÜÂØπË±°ÂéüÂßãÂÄºÁöÑÂâØÊú¨‰Ωú‰∏∫Âè≥ÂÄºËøîÂõû„ÄÇ ‰ΩçËøêÁÆó Êï¥ÂΩ¢ÊèêÂçáÔºåchar8-&gt;int32 Ê∑ª0ÔºåË∂äÁïå‰∏¢ÂºÉ ÈÄóÂè∑ËøêÁÆóÁ¨¶ÔºöÂê´Êúâ‰∏§‰∏™ËøêÁÆóÂØπË±°ÔºåÈ¶ñÂÖàÂØπÂ∑¶Ë°®ËææÂºèÊ±ÇÂÄºÔºåÁÑ∂ÂêéÂ∞ÜÊ±ÇÂÄºÁªìÊûú‰∏¢ÂºÉÊéâÔºåÊúÄÂè≥ËæπÁöÑË°®ËææÂºèÁöÑÂÄºÂ∞Ü‰Ωú‰∏∫Êï¥‰∏™ÈÄóÂè∑Ë°®ËææÂºèÁöÑÂÄº„ÄÇÊú¨Ë¥®‰∏äÔºåÈÄóÂè∑ÁöÑ‰ΩúÁî®ÊòØÂØºËá¥‰∏Ä‰∫õÂàóËøêÁÆóË¢´È°∫Â∫èÊâßË°å„ÄÇ 12// ÂàÜÂà´ÂØπÈÄóÂè∑Ë°®ËææÂºèÂÜÖÂØπË±°ËµãÂÄºÔºåÁÑ∂ÂêéËøîÂõûÊúÄÂè≥cntÁöÑÂÄºvar = (count=19, incr=10, cnt++) ÂáΩÊï∞ Â±ÄÈÉ®ÈùôÊÄÅÂØπË±°staticÔºöÈ¶ñÊ¨°Ë∞ÉÁî®Êó∂Ë¢´ÂàùÂßãÂåñÔºåÁõ¥Âà∞Á®ãÂ∫èÁªàÊ≠¢ÊâçË¢´ÈîÄÊØÅ„ÄÇ 123456int f()&#123; // Âè™ÂàùÂßãÂåñ‰∏ÄÊ¨°ÔºåÂáΩÊï∞Ë∞ÉÁî®ÁªìÊùü‰ª•ÂêéËøô‰∏™ÂÄº‰ªçÊúâÊïà static cnt = 0; return ++cnt;&#125; ÂèÇÊï∞‰º†ÈÄíÔºöÂ¶ÇÊûúÂΩ¢ÂèÇË¢´Â£∞Êòé‰∏∫ÂºïÁî®Á±ªÂûãÔºåÂÆÉÂ∞ÜÁªëÂÆöÂà∞ÂØπÂ∫îÁöÑÂÆûÂèÇ‰∏äÔºà‰º†ÂºïÁî®Ë∞ÉÁî®ÔºâÔºåÂê¶ÂàôÂ∞ÜÂÆûÂèÇÁöÑÂÄºÊã∑Ë¥ùÂêéËµãÁªôÂΩ¢ÂèÇÔºà‰º†ÂÄºË∞ÉÁî®Ôºâ„ÄÇ Âê´ÊúâÂèØÂèòÂΩ¢ÂèÇÁöÑÂáΩÊï∞ ÊâÄÊúâÂÆûÂèÇÁ±ªÂûãÁõ∏ÂêåÔºåÂèØ‰ª•‰ΩøÁî®initializer_listÊ®°ÁâàÁ±ªÂûãÁöÑÂΩ¢ÂèÇÔºåÂàóË°®‰∏≠ÂÖÉÁ¥†ÊòØconst„ÄÇ 12initializer_list&lt;T&gt; lst;initializer_list&lt;T&gt; lst&#123;a, b, c, ...&#125;; ÁºñÂÜôÂèØÂèòÂèÇÊï∞Ê®°Áâà ÁúÅÁï•Á¨¶ÂΩ¢ÂèÇÔºöÂØπÂ∫îÁöÑÂÆûÂèÇÊó†ÈúÄÁ±ªÂûãÊ£ÄÊü• 1234// Â∏¶ÈÉ®ÂàÜÂΩ¢ÂèÇÁ±ªÂûãvoid foo(parm_list, ...);void foo(...); ÂÜÖËÅîÂáΩÊï∞inclineÔºöÈÅøÂÖçÂáΩÊï∞Ë∞ÉÁî®ÂºÄÈîÄ Ë∞ÉËØïÂ∏ÆÂä© NDEBUGÈ¢ÑÂ§ÑÁêÜÂèòÈáèÔºöÁî®‰∫éÂÖ≥Èó≠Ë∞ÉËØïÁä∂ÊÄÅÔºåassertÂ∞ÜË∑≥Ëøá‰∏çÊâßË°å„ÄÇ assert (expr) È¢ÑÂ§ÑÁêÜÂÆèÔºöÂ¶ÇÊûúË°®ËææÂºè‰∏∫ÂÅáÔºåassertËæìÂá∫‰ø°ÊÅØÂπ∂ÁªàÊ≠¢Á®ãÂ∫è„ÄÇ È¢ÑÂ§ÑÁêÜÂêçÂ≠óÁî±È¢ÑÂ§ÑÁêÜËÄåÈùûÁºñËØëÂô®ÁÆ°ÁêÜÔºåÂõ†Ê≠§ÂèØ‰ª•Áõ¥Êé•‰ΩøÁî®ÂêçÂ≠óËÄåÊó†È°ªÊèê‰æõusingÂ£∞Êòé„ÄÇ static_castÂíådynamic_castÂº∫Âà∂Á±ªÂûãËΩ¨Êç¢ static_cast \ (expression)ÔºöÊö¥ÂäõÁ±ªÂûãËΩ¨Êç¢Ôºå‰∏çËøêË°åÁ±ªÂûãÊ£ÄÊü•„ÄÇ dynamic_cast\ (expression)ÔºöËøêË°åÁ±ªÂûãÊ£ÄÊü•Ôºå‰∏ãË°åËΩ¨Êç¢ÂÆâÂÖ®„ÄÇ new &amp; deleteÔºönew [] Ë¶ÅÂíå delete []ÂØπÂ∫î‰∏ä„ÄÇ c++ÁöÑoopÁâπÊÄßÔºàprivate public‚Ä¶ÔºâÂè™Âú®ÁºñËØëÊó∂ÂàªÊúâÊÑè‰πâ„ÄÇÂêå‰∏ÄÁ±ªÁöÑÂØπË±°ÂèØ‰ª•‰∫íÁõ∏ËÆøÈóÆÁßÅÊúâÊàêÂëò„ÄÇ firendÔºöÊ≥®ÊÑèÊñπÂêëÊòØgive acess toÔºåÊéàÊùÉfriendËÆøÈóÆËá™Â∑±ÁöÑprivate„ÄÇÁºñËØëÊó∂ÂàªÊ£ÄÊü•„ÄÇ compositionÔºöÁªÑÂêàÔºåÁî®‰∏ÄÁ≥ªÂàóÂØπË±°ÊûÑÈÄ†ÂØπË±°„ÄÇ inheritanceÔºöÁªßÊâøÔºåÁî®‰∏Ä‰∫õÁ±ªÊù•ÊûÑÈÄ†Êñ∞ÁöÑÁ±ª„ÄÇ 1234class A;class B : public A&#123; ....&#125;; ÊûÑÈÄ†ÔºöÂ≠êÁ±ªÊûÑÈÄ†ÁöÑÊó∂ÂÄôË¶ÅÂÖàÊûÑÈÄ†Áà∂Á±ªÔºåÊûêÊûÑÁöÑÊó∂ÂÄôÂèçËøáÊù•ÔºåÂÖàÊûêÊûÑÂ≠êÁ±ª„ÄÇ ÈáçÂêçÔºöname hidingÔºåspecial for c++„ÄÇ ‚Äã protectedÔºödesigned for sub class„ÄÇÂ≠êÁ±ªÂèØ‰ª•Áõ¥Êé•ËÆøÈóÆ„ÄÇÂÖ∂‰ªñÁ±ªÁúã‰∏çÂà∞„ÄÇ overloadÔºöÂèÇÊï∞Ë°®ÂøÖÈ°ª‰∏çÂêåÔºåÂê¶ÂàôÁºñËØëÂô®Êó†Ê≥ïËØÜÂà´„ÄÇ default argumentÔºödefaults must be added from right to left„ÄÇmust be declared in .h files„ÄÇÂèëÁîüÂú®ÁºñËØëÊó∂Âàª„ÄÇ inlineÔºö‰∏çÁî®ÁúüÊ≠£Ë∞ÉÁî®ÂáΩÊï∞ÔºåËÄåÊòØÁõ¥Êé•ÊèíÂÖ•Ê±áÁºñ‰ª£Á†ÅÊÆµ„ÄÇtradeoff between space and time consuming„ÄÇÂå∫Âà´‰∫éÂÆèÔºåÂÆèÊòØÊ≤°ÊúâÁ±ªÂûãÊ£ÄÊü•ÁöÑ„ÄÇ const declare a variableÔºöÊòØÂèòÈáèÔºåËÄå‰∏çÊòØÂ∏∏Êï∞]]></content>
      <tags>
        <tag>basic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cmake for record]]></title>
    <url>%2F2018%2F05%2F08%2Fcmake%2F</url>
    <content type="text"><![CDATA[0. ÂèòÈáèÂèòÈáè‰ΩøÁî® ${ } ÁöÑÊñπÂºèÂèñÂÄºÔºå‰ΩÜÊòØÂú®ifÊéßÂà∂ËØ≠Âè•‰∏≠Áõ¥Êé•‰ΩøÁî®ÂèòÈáèÂêç„ÄÇ 1. projectproject ( project_name [CXX] [C] [Java] ) Áî®Êù•ÊåáÂÆöÂ∑•Á®ãÂêçÁß∞ÂíåÂ∑•Á®ãËØ≠Ë®ÄÔºàÂèØÁúÅÁï•ÔºâÔºåÊåá‰ª§ÈöêÂºèÂÆö‰πâ‰∫Üprojectname_BINARY_DIRÂíåprojectname_SOURCE_DIR‰∏§‰∏™ÂèòÈáèÔºàÂÜôÂú®cmake_cacheÈáåÈù¢ÔºâÔºåÊåáÁöÑÊòØÁºñËØëÂèëÁîüÁöÑÂΩìÂâçÁõÆÂΩï„ÄÇ 2. setset ( VAR [VALUE] ) Áî®Êù•ÊòæÂºèÂÆö‰πâÂèòÈáèÔºåÂ¶Çset (SRC_LIST main.c t1.c t2.c) „ÄÇÔºàÁ´üÁÑ∂‰∏çÁî®Â•óÊã¨Âè∑ÔºüÔºâ 3. messagemessage ( [SEND_ERROR | STATUS | FATAL_ERROR] ‚Äúmessage to display‚Äù VAR ) Áî®Êù•ÂêëÁªàÁ´ØËæìÂá∫Áî®Êà∑ÂÆö‰πâÁöÑ‰ø°ÊÅØ„ÄÇ 4. add_executableadd_executable ( executable_filename [source_filename] ) ÁîüÊàêÂêçÂ≠ó‰∏∫executable_filenameÁöÑÂèØÊâßË°åÊñá‰ª∂ÔºåÁõ∏ÂÖ≥ÁöÑÊ∫êÊñá‰ª∂ [source_filename] ÂèØ‰ª•ÊòØ‰∏Ä‰∏™Ê∫êÊñá‰ª∂ÂàóË°®„ÄÇ 5. Ê∏ÖÁêÜÊûÑÂª∫ÁªìÊûúmake clean ÂØπÊûÑÂª∫Âá∫ÁöÑÂèØÊâßË°åÊñá‰ª∂ËøõË°åÊ∏ÖÁêÜ„ÄÇ 6. Â§ñÈÉ®ÊûÑÂª∫1234mkdir buildcd buildcmake ..make ÊâÄÊúâÁºñËØëÂä®‰ΩúÂèëÁîüÂú®ÁºñËØëÁõÆÂΩïÔºåÂØπÂéüÊúâÂ∑•Á®ãÊ≤°Êúâ‰ªª‰ΩïÂΩ±Âìç„ÄÇ 7. add_subdirectoryadd_subdirectory ( source_dir [binary_dir] [EXCLUDE_FROM_ALL] ) ÂêëÂΩìÂâçÂ∑•Á®ãÁõÆÂΩïÊ∑ªÂä†Â≠òÊîæÊ∫êÊñá‰ª∂ÁöÑÂ≠êÁõÆÂΩïsource_dirÔºåÂπ∂ÊåáÂÆöÂ≠òÊîæ‰∏≠Èó¥‰∫åËøõÂà∂Êñá‰ª∂ÂíåÁõÆÊ†á‰∫åËøõÂà∂Êñá‰ª∂ÁöÑ‰ΩçÁΩÆbinary_dir„ÄÇÊåá‰ª§ÈöêÂºè‰øÆÊîπ EXECUTABLE_OUTPUT_PATH Âíå LIBRARY_OUTPUT_PATH ‰∏§‰∏™ÂèòÈáè„ÄÇ 8. Êõ¥Âä†ÂÉè‰∏Ä‰∏™Â∑•Á®ã ÂàõÂª∫Â∑•Á®ãÊ†πÁõÆÂΩïÔºåÂàõÂª∫CMakeLists.txt„ÄÇ 1234567# ÊåáÂÆöÊúÄ‰ΩéÁºñËØëÁâàÊú¨cmake_minimum_required(VERSION 3.7)# ÊåáÂÆöÂ∑•Á®ãÂêçÂ≠óPROJECT(HELLO)# ÊµãËØïÁ±ªÊâìÂç∞‰ø°ÊÅØMESSAGE(STATUS "This is BINARY dir " $&#123;HELLO_BINARY_DIR&#125;)MESSAGE(STATUS "This is SOURCE dir " $&#123;HELLO_SOURCE_DIR&#125;) Ê∑ªÂä†Â≠êÁõÆÂΩïsrcÔºåÁî®Êù•Â≠òÊîæÊ∫êÊñá‰ª∂Ôºå‰∏∫Â≠êÁõÆÂΩïÂàõÂª∫CMakeLists.txt„ÄÇ 123# Âú®Ê†πÁõÆÂΩïCMakeLists.txt‰∏≠Ê∑ªÂä†Â≠êÁõÆÂΩïÂ£∞Êòéadd_subdirectory(src bin)# ÁºñËØë‰∫ßÁîüÁöÑ‰∏≠Èó¥Êñá‰ª∂‰ª•ÂèäÁõÆÊ†áÊñá‰ª∂Â∞Ü‰øùÂ≠òÂú®ÁºñËØëÊñá‰ª∂Â§πÁöÑbinÂ≠êÁõÆÂΩï‰∏ã 12345# ÁºñÂÜôÂΩìÂâçÂ≠êÁõÆÂΩïÁöÑCMakeLists.txtadd_executable(hello main.c)# ‰øÆÊîπÊúÄÁªàÁîüÊàêÁöÑÂèØÊâßË°åÊñá‰ª∂‰ª•ÂèäÂ∫ìÁöÑË∑ØÂæÑÔºåËøô‰∏§‰∏™Êåá‰ª§Ë¶ÅËøΩÈöèÂØπÂ∫îÁöÑadd_executable()Âíåadd_library()Êåá‰ª§set(EXECUTABLE_OUTPUT_PATH $&#123;PROJECT_BINARY_PATH&#125;/bin)set(LIBRARY_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;/lib) Ê∑ªÂä†Â≠êÁõÆÂΩïbuildÔºå‰Ωú‰∏∫Â§ñÈÉ®ÁºñËØëÊñá‰ª∂Â§πÔºà ${PROJECT_BINARY_DIR} ÔºâÔºåÂ≠òÊîæÁºñËØëÁöÑËøáÁ®ãÂíåÁõÆÊ†áÊñá‰ª∂„ÄÇ 123cd buildcmake ..make Ê∑ªÂä†Â≠êÁõÆÂΩïdocÔºåÁî®Êù•Â≠òÊîæÂ∑•Á®ãÊñáÊ°£hello.txt„ÄÇ Ê∑ªÂä†ÊñáÊú¨Êñá‰ª∂READMEÔºåCOPYRIGHT„ÄÇ Ê∑ªÂä†runhello.shËÑöÊú¨ÔºåÁî®Êù•Ë∞ÉÁî®ÂèØÊâßË°åÊñá‰ª∂hello„ÄÇ 9. ÊâìÂåÖÂÆâË£Ö Âú®Ê†πÁõÆÂΩïÁöÑCMakeList.txt‰∏≠Ê∑ªÂä†ÂÆâË£Ö‰ø°ÊÅØ 123456# ÂÆâË£ÖCOPYRIGHT/READMEÂà∞&lt;prefix&gt;/share/doc/cmake/t2INSTALL(FILES COPYRIGHT README DESTINATION share/doc/cmake/t2)# ÂÆâË£Örunhello.shÂà∞&lt;prefix&gt;/binINSTALL(PROGRAMS runhello.sh DESTINATION bin)# ÂÆâË£ÖÂ∑•Á®ãÊñáÊ°£Âà∞&lt;prefix&gt;/share/doc/cmake/t2INSTALL(DIRECTORY doc/ DESTINATION share/doc/cmake/t2) Âú®Â≠êÁõÆÂΩïÁöÑCMakeList.txt‰∏≠Ê∑ªÂä†ÂÆâË£Ö‰ø°ÊÅØ 1234# ÂÆâË£ÖËÑöÊú¨Ë¶ÅË∞ÉÁî®ÁöÑÂèØÊâßË°åÊñá‰ª∂helloÂà∞&lt;prefix&gt;/binÔºå# Ê≥®ÊÑèinstall(targets)Êåá‰ª§‰πüË¶ÅËøΩÈöèÂØπÂ∫îadd_executable()Âíåadd_library()Êåá‰ª§ÁöÑË∑ØÂæÑINSTALL(TARGETS hello RUNTIME DESTINATION bin) ÂÆâË£ÖÁ®ãÂ∫èÂåÖ 12345678910cd build# Âú®cmakeÂëΩ‰ª§‰∏≠ÊåáÊòéÂÆâË£ÖÁõÆÂΩïÁöÑÂâçÁºÄ&lt;prefix&gt;# CMAKE_INSTALL_PREFIX ÈªòËÆ§ÊòØ/usr/localcmake -DCMAKE_INSTALL_PREFIX=/Users/carrol/tmp ..makemake install# Êü•ÁúãÁõÆÊ†áÊñá‰ª∂Â§πj tmptree -a 10. add_libraryadd_library ( name [SHARED | STATIC | MODULE] [source_filename] ) ÁîüÊàêÂêçÂ≠ó‰∏∫libname.XÁöÑÂ∫ìÊñá‰ª∂„ÄÇ SHAREDÔºåÂä®ÊÄÅÂ∫ìÔºålibname.dylib STATICÔºåÈùôÊÄÅÂ∫ìÔºålibname.a ËÆæÁΩÆÁõÆÊ†áÂä®ÊÄÅÂ∫ìÂíåÈùôÊÄÅÂ∫ìÂêåÂêç set_target_properties 12345# ËÆæÁΩÆÁõÆÊ†áÂä®ÈùôÊÄÅÂ∫ìÂêåÂêçadd_library(hello SHARED hello.c)add_library(hello_static hello.c)set_target_properties(hello_static PROPERTIES OUTPUT_NAME hello) Èò≤Ê≠¢ÊûÑÂª∫‰∏≠Ê∏ÖÁêÜÂêåÂêçÊñá‰ª∂ set_target_properties cmakeÂú®ÊûÑÂª∫‰∏Ä‰∏™targetÊó∂Ôºå‰ºöÂ∞ùËØïÊ∏ÖÁêÜÊéâÂÖ∂‰ªñ‰ΩøÁî®Ëøô‰∏™ÂêçÂ≠óÁöÑÂ∫ì‚Äî‚ÄîÂú®ÊûÑÂª∫libhello.aÊó∂‰ºöÊ∏ÖÁêÜÊéâlibhello.dylib„ÄÇ ÊàëÂÆûÈôÖÊìç‰ΩúÊó∂ÂÄô‰ºö‰øùÁïô‰∏§‰∏™Â∫ìÊñá‰ª∂Ôºå‰ΩÜÊòØÂú®‰Ωú‰∏∫Á¨¨‰∏âÊñπË¢´ÂºïÁî®ÁöÑÊó∂ÂÄô‰ºöÊä•ÈîôÔºö dyld: Library not loaded: libhello.dylib Reason: image not found 1234SET_TARGET_PROPERTIES(hello PROPERTIES CLEAN_DIRECT_OUTPUT 1)SET_TARGET_PROPERTIES(hello_static PROPERTIES CLEAN_DIRECT_OUTPUT 1) ËÆæÁΩÆÂä®ÊÄÅÁâàÊú¨Âè∑ set_target_properties 1234# ËÆæÁΩÆÂä®ÊÄÅÂ∫ìÁâàÊú¨Âè∑set_target_properties(hello PROPERTIES VERSION 1.2 SOVERSION 1) ÁºñËØëÊñá‰ª∂Â§π‰∏ãÁîüÊàê‰∫Ülibhello.1.2.dylib„ÄÅlibhello.1.dylib„ÄÅlibhello.dylib‰∏â‰∏™Âä®ÊÄÅÂ∫ìÊñá‰ª∂ÔºåÂè™Êúâ‰∏Ä‰∏™ÊòØÁúüÁöÑÔºåÂè¶Â§ñ‰∏§‰∏™ÊòØÊõøË∫´„ÄÇ ÂÆâË£ÖÂÖ±‰∫´Â∫ìÂíåÂ§¥Êñá‰ª∂ ‰øÆÊîπÂ∫ìÁöÑÊ∫êÊñá‰ª∂Â§π‰∏ãÁöÑCMakeLIsts.txt 123456# Â∫ìÊñá‰ª∂install(TARGETS hello hello_static ARCHIVE DESTINATION lib //ÈùôÊÄÅÂ∫ì LIBRARY DESTINATION lib) //Âä®ÊÄÅÂ∫ì# Â§¥Êñá‰ª∂install(FILES hello.h DESTINATION include/hello) 11. include_directoriesinclude_directories( dir1 dir2 ‚Ä¶ ) Áî®Êù•ÂêëÂ∑•Á®ãÊ∑ªÂä†Â§ö‰∏™ÁâπÂÆöÁöÑÂ§¥Êñá‰ª∂ÊêúÁ¥¢Ë∑ØÂæÑ 12. link_directories &amp; target_link_librarieslink_directories( dir1 dir2 ‚Ä¶ ) Ê∑ªÂä†ÈùûÊ†áÂáÜÁöÑÂÖ±‰∫´Â∫ìÊêúÁ¥¢Ë∑ØÂæÑ target_link_libraries( target lib1 lib2 ‚Ä¶ ) Áî®Êù•‰∏∫ÁõÆÊ†átargetÊ∑ªÂä†ÈúÄË¶ÅÈìæÊé•ÁöÑÂÖ±‰∫´Â∫ìÔºåtargetÂèØ‰ª•ÊòØ‰∏Ä‰∏™ÂèØÊâßË°åÊñá‰ª∂Ôºå‰πüÂèØ‰ª•ÊòØ‰∏Ä‰∏™Â∫ìÊñá‰ª∂„ÄÇ Êü•ÁúãÁîüÊàêÁõÆÊ†áÁöÑÂ∫ì‰æùËµñÊÉÖÂÜµ 12345# ÁîüÊàêÁöÑÁõÆÊ†áÂèØÊâßË°åÊñá‰ª∂‰∏∫main# for OSXotool -L main# for linuxldd main Âè™ËÉΩÂàóÂá∫Âä®ÊÄÅÂ∫ì„ÄÇ 13. Â∏∏Áî®ÂèòÈáèPROJECT_BINARY_DIRÔºöÁºñËØëÂèëÁîüÁöÑÁõÆÂΩï PROJECT_SOURCE_DIRÔºöÂ∑•Á®ãÈ°∂Â±ÇÁõÆÂΩï CMAKE_CURRENT_SOURCE_DIRÔºöÂΩìÂâçCMakeLists.txtÊâÄÂú®ÁõÆÂΩï CMAKE_MODULE_PATHÔºöËá™ÂÆö‰πâÁöÑcmakeÊ®°ÂùóÊâÄÂú®Ë∑ØÂæÑ LIBRARY_OUTPUT_PATHÔºöÈáçÂÆö‰πâÁõÆÊ†áÂ∫ìÊñá‰ª∂Â≠òÊîæÁõÆÂΩï EXECUTABLE_OUTPUT_PATHÔºöÈáçÂÆö‰πâÁõÆÊ†áÂèØÊâßË°åÊñá‰ª∂Â≠òÊîæÁõÆÂΩï 14. findNAME.cmakeÊ®°Âùó Âú®Â∑•Á®ãÁõÆÂΩï‰∏≠ÂàõÂª∫cmakeÊñá‰ª∂Â§πÔºåÂπ∂ÂàõÂª∫FindHELLO.cmakeÊ®°ÂùóÔºö 123456789101112131415# Á§∫‰æãFIND_PATH(HELLO_INCLUDE_DIR hello.h /usr/local/include/hello)FIND_LIBRARY(HELLO_LIBRARY hello /usr/local/lib)IF (HELLO_INCLUDE_DIR AND HELLO_LIBRARY) SET(HELLO_FOUND TRUE)ENDIF (HELLO_INCLUDE_DIR AND HELLO_LIBRARY)IF (HELLO_FOUND) IF (NOT HELLO_FIND_QUIETLY) MESSAGE(STATUS "Found Hello: $&#123;HELLO_LIBRARY&#125;") ENDIF (NOT HELLO_FIND_QUIETLY)ELSE (HELLO_FOUND) IF (HELLO_FIND_REQUIRED) MESSAGE(FATAL_ERROR "Could not find hello library") ENDIF (HELLO_FIND_REQUIRED)ENDIF (HELLO_FOUND) Âú®‰∏ªÁõÆÂΩïCMakeLists.txt‰∏≠Ê∑ªÂä†cmakeÊ®°ÂùóÊâÄÂú®Ë∑ØÂæÑÔºö 12# ‰∏∫find_package()Êåá‰ª§ÊàêÂäüÊâßË°åset(CMAKE_MODULE_PATH $&#123;PROJECT_SOURCE_DIR&#125;/cmake) ÁÑ∂ÂêéÂ∞±ÂèØ‰ª•Âú®Ê∫êÊñá‰ª∂CMakeLists.txt‰∏≠Ë∞ÉÁî® find_packageÔºö find_package ( name [QUIET] [REQUIRED] ) Áî®Êù•Ë∞ÉÁî®È¢ÑÂÆö‰πâÂú®CMAKE_MODULE_PATH‰∏ãÁöÑFind\.cmakeÊ®°Âùó„ÄÇ ÊØè‰∏Ä‰∏™Ê®°ÂùóÈÉΩ‰ºöÂÆö‰πâ‰ª•‰∏ãÂá†‰∏™ÂèòÈáèÔºö NAME_FOUND NAME_INCLUDE_DIR or NAME_INCLUDES NAME_LIBRARY or NAME_LIBRARIES Ê†πÊçÆÊåá‰ª§ÂêéÈù¢ÁöÑÂèÇÊï∞Ëøò‰ºöÊúâ‰ª•‰∏ãÂèòÈáèÔºö NAME_FIND_QUIETLYÔºåÂ¶ÇÊûúÊåáÂÆö‰∫ÜQUIETÂèÇÊï∞ÔºåÂ∞±‰∏ç‰ºöÊâßË°åÂ¶Ç‰∏ãËØ≠Âè•Ôºö 1MESSAGE(STATUS "Found Hello: $&#123;NAME_LIBRARY&#125;") NAME_FIND_REQUIREDÔºåÂ¶ÇÊûúÊåáÂÆö‰∫ÜREQUIREDÂèÇÊï∞ÔºåÂ∞±ÊòØÊåáËøô‰∏™ÂÖ±‰∫´Â∫ìÊòØÂ∑•Á®ãÂøÖÈ°ªÁöÑÔºåÂ¶ÇÊûúÊâæ‰∏çÂà∞ÔºåÂ∑•Á®ãÂ∞±‰∏çËÉΩÁºñËØëÔºåÂØπÂ∫îÂú∞‰ºöÊâßË°åÂ¶Ç‰∏ãËØ≠Âè•Ôºö 1MESSAGE(FATAL_ERROR "Could not find NAME library") ÂèØ‰ª•ÈÄöËøá\_FOUNDÂà§Êñ≠Ê®°ÂùóÊòØÂê¶Ë¢´ÊâæÂà∞ÔºåÂπ∂ÊâßË°å‰∏çÂêåÁöÑÊìç‰ΩúÔºàÂ¶ÇÊ∑ªÂä†ÈùûÊ†áÂáÜË∑ØÂæÑ„ÄÅËæìÂá∫ÈîôËØØ‰ø°ÊÅØÁ≠âÔºâ„ÄÇ 15. find_Êåá‰ª§ find_path find_path ( VAR name1 path1 path2 ‚Ä¶ ) VARÂèòÈáè‰ª£Ë°®ÂåÖÂê´name1Êñá‰ª∂ÁöÑË∑ØÂæÑ‚Äî‚ÄîË∑ØÂæÑ„ÄÇ find_library find_library ( VAR name1 path1 path2 ‚Ä¶) VARÂèòÈáèÂåÖÂê´ÊâæÂà∞ÁöÑÂ∫ìÁöÑÂÖ®Ë∑ØÂæÑÔºåÂåÖÊã¨Â∫ìÊñá‰ª∂Âêç‚Äî‚ÄîË∑ØÂæÑ‰∏ãÁöÑÊâÄÊúâÊñá‰ª∂„ÄÇ]]></content>
      <tags>
        <tag>basic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ICP, Iterative Closest Points]]></title>
    <url>%2F2018%2F05%2F05%2FICP-Iterative-Closest-Points%2F</url>
    <content type="text"><![CDATA[1 Âü∫Êú¨ÂÆûÁé∞Êï∞ÊçÆÁÇπ‰∫ëÈÖçÂáÜÔºåÊúÄÁªèÂÖ∏ÁöÑÊñπÊ≥ïÂ∞±ÊòØICPËø≠‰ª£ÊúÄËøëÁÇπÊ≥ï„ÄÇ ÊúÄËøëÁÇπÔºöÊ¨ßÂá†ÈáåÂæóÊÑè‰πâ‰∏äË∑ùÁ¶ªÊúÄËøëÁöÑÁÇπ„ÄÇ Ëø≠‰ª£ÔºöËø≠‰ª£ÁõÆÊ†áÊòØÈÄöËøá‰∏çÊñ≠Êõ¥Êñ∞ËøêÂä®ÂèÇÊï∞Ôºå‰ΩøÂæó‰∏§‰∏™ÁÇπ‰∫ëÁöÑÈáçÂè†ÈÉ®ÂàÜÂÖÖÂàÜÂêªÂêà„ÄÇ ICPÁöÑÊ±ÇËß£ÂàÜ‰∏∫‰∏§ÁßçÊñπÂºèÔºö Âà©Áî®Á∫øÊÄß‰ª£Êï∞Ê±ÇËß£ÔºàSVDÔºâÔºåÂú®ÁªôÂÆö‰∫ÜÂåπÈÖçÁöÑÊÉÖÂÜµ‰∏ãÔºåÊúÄÂ∞è‰∫å‰πòÈóÆÈ¢òÂÆûÈôÖ‰∏äÂÖ∑ÊúâËß£ÊûêËß£„ÄÇ Âà©Áî®ÈùûÁ∫øÊÄß‰ºòÂåñÊñπÂºèÊ±ÇËß£ÔºåÁ±ª‰ºº‰∫éBAÊñπÊ≥ïÔºåÈÄÇÁî®‰∫éÂåπÈÖçÊú™Áü•ÁöÑÊÉÖÂÜµ„ÄÇ 2 SVDÊñπÊ≥ïÊ±ÇËß£ÁÆóÊ≥ïÊé®ÂØºÂ¶Ç‰∏ãÔºö È¶ñÂÖàÂ∞ÜÁÇπ‰∫ëÊñá‰ª∂ËøõË°åÁ≤óÂåπÈÖçÔºåÂ¶ÇORBÁâπÂæÅÁÇπÂåπÈÖç„ÄÇ ‰ªéÁÇπÈõÜ$P={\overrightarrow{p_1}, \overrightarrow{p_2}, ‚Ä¶, \overrightarrow{p_n}}$‰∏≠ÈöèÊú∫ÈÄâÂèñÊåáÂÆöÊï∞ÈáèÁöÑÁÇπ$\{\overrightarrow{p_t}\}$‰Ωú‰∏∫ÂèÇËÄÉÁÇπÔºåÂèÇËÄÉÁÇπÁöÑÊï∞ÈáèÂÜ≥ÂÆö‰∫ÜICPÁÆóÊ≥ïÁöÑËÆ°ÁÆóÊïàÁéáÂíåÈÖçÂáÜÁ≤æÂ∫¶„ÄÇ Âú®Âè¶‰∏Ä‰∏™ÁÇπÈõÜ$Q={\overrightarrow{q_1}, \overrightarrow{q_2}, ‚Ä¶, \overrightarrow{q_m}}$ÊòØÂæÖÂåπÈÖçÁöÑÁÇπquery pointsÔºåÈÇ£‰πàÊÉ≥Ë¶ÅÊâæÂà∞‰∏Ä‰∏™Ê¨ßÂºèÂèòÊç¢$R, t$Ôºå‰ΩøÂæó$\forall i, p_i = Rq_i + t$„ÄÇ Ê±ÇËß£Ê¨ßÂºèÂèòÊç¢$T^k$Ôºå‰ΩøÂæó$E^k=\Sigma| \overrightarrow{p_t} - T^k \overrightarrow{q_t}|^2$ÊúÄÂ∞èÂåñ„ÄÇ Â∞ÜÁ©∫Èó¥ÂèòÊç¢ÂàÜËß£‰∏∫ÊóãËΩ¨ÂíåÂπ≥Áßª‰∏§ÈÉ®ÂàÜÔºåÈ¶ñÂÖàÂÆö‰πâ‰∏§‰∏™ÁÇπ‰∫ëÁöÑË¥®ÂøÉÔºö \overrightarrow{p} = \frac{1}{n} \Sigma \overrightarrow{p_t}, \ \ \overrightarrow{q} = \frac{1}{n} \Sigma \overrightarrow{q_t}ÔºåË¥®ÂøÉ,\ Áî®‰∫éÊèèËø∞Âπ≥Áßª\\ \overrightarrow p_i = \overrightarrow{p_t} - \overrightarrow{p}, \ \ \overrightarrow q_i = \overrightarrow{q_t} - \overrightarrow pÔºå‰∏≠ÂøÉÂåñÁÇπ‰∫ë,\ Áî®‰∫éÊèèËø∞ÊóãËΩ¨\\‰∫éÊòØÊúâÁõÆÊ†áÂáΩÊï∞Ôºö \begin{split} E^k & = \Sigma|\overrightarrow{p_t} - T^k \overrightarrow{q_t}|^2 = \Sigma|(p+p_i) -T (q+q_i)|^2\\ & = \Sigma|(p+p_i) -R (q+q_i) -t|^2\\ & = \Sigma |(p_i - Rq_i) + (p - Rq -t)|^2\\ & = \Sigma( |p_i - Rq_i|^2 + |p - Rq -t|^2)\\ & = \Sigma( |p_i - Rq_i|^2\\ J &= \frac{1}{2} \sum e_i = \frac{1}{2} E^k \end{split}ÂØπÁõÆÊ†áÂáΩÊï∞Â±ïÂºÄÔºåËÄå‰∏îÂ∑≤Áü•ÊóãËΩ¨Áü©ÈòµÊòØÊ≠£‰∫§ÈòµÔºå$R^TR=I‚Äã$ÔºåÊâÄ‰ª•ÁõÆÊ†áÂáΩÊï∞ÁöÑÂâç‰∏§È°πÈÉΩ‰∏é$R‚Äã$Êó†ÂÖ≥Ôºö R^* = argmin_R J = \frac{1}{2}\sum p_i^Tp_i + q_i^TR^TRq_i - 2p_i^TRq_iÂè™ÊúâÊúÄÂêé‰∏ÄÈ°π‰∏é$R$ÊúâÂÖ≥Ôºå‰∫éÊòØÂæóÂà∞ÂÖ≥‰∫é$R$ÁöÑÁõÆÊ†áÂáΩÊï∞Ôºö J(R) = \sum_{unrelated} -\ p_i^TRq_i = \sum - \ tr(Rq_ip_i^T) = -tr(R\sum_{i=1}^nq_ip_i^T)ÁÑ∂ÂêéÈÄöËøáSVDÂ•áÂºÇÂÄºÂàÜËß£Ê±ÇËß£‰∏äËø∞ÈóÆÈ¢òÁöÑÊúÄ‰ºò$R$ÔºåÈ¶ñÂÖàÂÆö‰πâ$W = \sum_1^n pq^T$ÔºåÂΩì$W$Êª°Áß©Êó∂Ôºö W = \sum_{i=1}^{n} \overrightarrow{p_i}*\overrightarrow{q_i^T} = U \begin{bmatrix} \sigma1 & 0 & 0 \\ 0 & \sigma2 & 0 \\ 0 & 0 & \sigma3 \end{bmatrix} V^T\\ R = UV^TÁÑ∂ÂêéÈó¥Êé•ÂæóÂà∞Âπ≥Áßª$t$Ôºö t = {p} - R{q} ‰ª£Á†ÅÂÆûÁé∞Â¶Ç‰∏ãÔºö 1234567891011121314151617181920212223242526272829303132333435363738void pose_estimation_3d3d( const vector&lt;Point3f&gt;&amp; pts1, // point cloud 1 const vector&lt;Point3f&gt;&amp; pts2, // point cloud 2 Mat&amp; R, Mat&amp; t, Eigen::Matrix3d&amp; R_, Eigen::Vector3d&amp; t_ )&#123; Point3f p1, p2; // center of Mass int N = pts1.size(); for(int i=0; i&lt;N; i++) &#123; p1 += pts1[i]; p2 += pts2[i]; &#125; p1 /= N; p2 /= N; vector&lt;Point3f&gt; q1(N), q2(N); // remove the COM for(int i=0; i&lt;N; i++) &#123; q1[i] = pts1[i] - p1; q2[i] = pts2[i] - p2; &#125; Eigen::Matrix3d W = Eigen::Matrix3d::Zero(); // calculate W matrix for(int i=0; i&lt;N; i++) &#123; W += Eigen::Vector3d(q1[i].x, q1[i].y, q1[i].z) * Eigen::Vector3d(q2[i].x, q2[i].y, q2[i].z).transpose(); &#125; // SVD decomposition Eigen::JacobiSVD&lt;Eigen::Matrix3d&gt; svd(W, Eigen::ComputeFullU|Eigen::ComputeFullV); // SVD Eigen::Matrix3d U = svd.matrixU(); Eigen::Matrix3d V = svd.matrixV(); // calculate R,t R_ = U * V.transpose(); t_ = Eigen::Vector3d(p1.x, p1.y, p1.z) - R_ * Eigen::Vector3d(p2.x, p2.y, p2.z); 3 ÈùûÁ∫øÊÄß‰ºòÂåñÊñπÊ≥ïÂè¶‰∏ÄÁßçÊñπÂºèÊòØÈÄöËøáËø≠‰ª£ÁöÑÊñπÂºèÊù•ÂØªÊâæÊúÄ‰ºòÂÄºÔºåËØØÂ∑ÆÈ°πÁöÑË°®Á§∫‰∏é‰∏ä‰∏ÄËäÇÁõ∏ÂêåÔºåÁî®Êùé‰ª£Êï∞Êù•Ë°®Ëææ‰ΩçÂßøÔºåÊóãËΩ¨ÂíåÂπ≥Áßª‰∏çÁî®ÂÜçËß£ËÄ¶Ë°®Á§∫ÔºåÁõÆÊ†áÂáΩÊï∞‰∏∫Ôºö \xi^* = argmin \frac{1}{2}\sum_{i=1}^n ||p_i - exp(\xi ^{\wedge})q_i||_2^2Âçï‰∏™ËØØÂ∑ÆÈ°πÂÖ≥‰∫é‰ΩçÂßøÁöÑÂØºÊï∞ÂèØ‰ª•‰ΩøÁî®Êùé‰ª£Êï∞Êâ∞Âä®Ê®°ÂûãÊù•ÊèèËø∞Ôºö \frac{\partial e}{\partial \delta \xi} = (e)^{\odot} = (p_i - exp(\xi^{\wedge})q_i)^{\odot}ÂÖ∂‰∏≠$p_i$‰Ωú‰∏∫ÂèÇËÄÉÁÇπÔºåÂØπÊâ∞Âä®ÁöÑÂØºÊï∞‰∏∫0ÔºåÂõ†Ê≠§Ôºö \frac{\partial e}{\partial \delta \xi} = - (exp(\xi^{\wedge})q_i)^{\odot}Â∞ÜÊúÄÂ∞è‰∫å‰πòÈóÆÈ¢òËøõË°åÂõæÊèèËø∞Ôºö‰ºòÂåñÂèòÈáè‰∏∫Êùé‰ª£Êï∞Ë°®ËææÁöÑ‰ΩçÂßø$\xi$ÔºåÂõ†Ê≠§Âõæ‰∏≠Âè™Êúâ‰∏Ä‰∏™ËäÇÁÇπÔºåËØØÂ∑ÆÈ°π‰∏∫‰∏ÄÂÖÉËæπÔºà‰ªéÂΩìÂâçËäÇÁÇπÊåáÂêëÂΩìÂâçËäÇÁÇπÔºâÔºåÂØπËØØÂ∑ÆÈ°πÂÅöÁ∫øÊÄßÂ±ïÂºÄÔºö e_i(\xi + \delta \xi) = e(\xi) + J(\xi)\delta \xiÂÖ∂‰∏≠ÁöÑÈõÖÂèØÊØîÁü©Èòµ‰πüÂ∞±ÊòØ‰∏äÈù¢ËØ¥ÁöÑÔºåÂçï‰∏™ËØØÂ∑ÆÈ°πÂÖ≥‰∫é‰ΩçÂßøÁöÑ‰∏ÄÈò∂ÂØºÊï∞„ÄÇ 4ÁÆóÊ≥ï‰ºòÂåñ Âà†Èô§ÁÇπ‰∫ëÊï∞ÊçÆÈááÈõÜ‰∏≠‰∫ßÁîüÁöÑÂô™Â£∞ÂèäÂºÇÂ∏∏ÂÄº„ÄÇ Êü•ÊâæÊúÄËøëÁÇπÁöÑËøáÁ®ãÈááÁî®KD-TreeÊï∞ÊçÆÁªìÊûÑÔºåÂáèÂ∞ëÊó∂Èó¥Â§çÊùÇÂ∫¶„ÄÇ]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CLion for record]]></title>
    <url>%2F2018%2F05%2F03%2Fclion%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1 cmakeËØ¶ËßÅcmake for record„ÄÇ 2 ÁÆÄÂçïÈÖçÁΩÆ‰∏ªË¶ÅÂ∞±ÊòØkeymapÂæà‰∏çÈÄÇÂ∫îÔºåÂü∫Êú¨‰∏äÂà†Èô§‰∫ÜÂ§ßÈÉ®ÂàÜeditingÁöÑÈÖçÁΩÆÔºåÂõ†‰∏∫ÂèØ‰ª•Áî®vim„ÄÇÂâ©‰∏ãÁöÑ‰øÆÊîπ‰∏ªË¶ÅÂª∂Áª≠sublimeÂíåOSXÁöÑ‰π†ÊÉØ„ÄÇ 2.1 ÊêúÁ¥¢ÂÖ®Â±ÄÊêúÁ¥¢Ôºöcmd + F Ââ©‰∏ãÁöÑ‰∫§Áªôvim„ÄÇ 2.2 ÂØºËà™search for fileÔºöcmd + O search for classÔºöopt + cmd + O search for symbolÔºöshift + cmd + O go to lineÔºöcmd + G backÔºöctrl + cmd + left forwardÔºöctrl + cmd + right Ââ©‰∏ãÁöÑ‰∫§Áªôvim„ÄÇ 2.3 Ê≥®Èáä‰ª£Á†ÅÂùóÊ≥®ÈáäÔºöshift + cmd + Ôºè 2.4 Êô∫ËÉΩÊèêÁ§∫ÁúãËßÅÂ∞èÁÅØÊ≥°Â∞±Ôºöopt + enter 2.5 run &amp; buildrunÔºöcmd + R buildÔºöcmd + B 2.6 ‰ª£Á†ÅÁîüÊàêinsertÔºöcmd + J ÊúÄËøëÂú®ÁÜüÊÇâEigenÂ∫ìÔºåÁªèÂ∏∏Ë¶ÅÊâìÂç∞‰∏úË•øÔºåÂä†‰∫Ü‰∏Ä‰∏™splitÊ®°ÁâàÂø´ÈÄüÂàÜÂâ≤‰ª£Á†ÅÁâáÊÆµ„ÄÇ generateÔºöcmd + N ËøòÊúâ‰∏Ä‰∫õvim‰∏éideÂÜ≤Á™ÅÁöÑÈîÆÔºåÂèØ‰ª•ÊâãÂä®ÈÄâÊã©ÊòØÊúç‰ªéideËøòÊòØvim„ÄÇ]]></content>
      <tags>
        <tag>ide</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Graph-based Optimization]]></title>
    <url>%2F2018%2F05%2F02%2Fgraph-based-optimization%2F</url>
    <content type="text"><![CDATA[1 ÁªºËø∞Âü∫‰∫éÂõæ‰ºòÂåñÁöÑslam‰∏ªË¶ÅÂàÜ‰∏∫‰ª•‰∏ã‰∏â‰∏™ÈÉ®ÂàÜÔºö ÂâçÁ´ØÔºöÂü∫‰∫é‰º†ÊÑüÂô®Êï∞ÊçÆÂª∫ÂõæÔºåÂåπÈÖçÁõ∏ÈÇªÂ∏ßÔºåÊ∑ªÂä†ËäÇÁÇπÂíåËæπÔºàraw graphÔºâÔºåËäÇÁÇπË°®Á§∫Êú∫Âô®‰∫∫ÁöÑ‰ΩçÂßøÔºåËæπË°®Á§∫ËäÇÁÇπ‰πãÈó¥ÁöÑ‰ΩçÂßøËÅîÁ≥ª„ÄÇ‰ΩçÂßø‰ø°ÊÅØÂèØ‰ª•Êù•Ëá™ÈáåÁ®ãËÆ°ËÆ°ÁÆóÔºåÂèØ‰ª•Êù•Ëá™ICPÊøÄÂÖâÁÇπ‰∫ëÂåπÈÖçÔºå‰πüÂèØ‰ª•Êù•Ëá™Èó≠ÁéØÊ£ÄÊµãÂèçÈ¶à„ÄÇ ÂêéÁ´ØÔºö‰ºòÂåñÂõæÔºåÂü∫‰∫éÂéÜÂè≤‰ø°ÊÅØÁöÑÁ∫¶ÊùüÔºåË∞ÉÊï¥Êñ∞Âä†ÂÖ•ÁöÑÊú∫Âô®‰∫∫‰ΩçÂßøÈ°∂ÁÇπ‰ΩøÂÖ∂Â∞ΩÈáèÊª°Ë∂≥ËæπÁöÑÁ∫¶ÊùüÔºàoptimized graphÔºâ„ÄÇ ÂÆèËßÇÁöÑÈó≠ÁéØÊ£ÄÊµãÔºöÊ†πÊçÆÈó≠ÁéØ‰ø°ÊÅØ‰ºòÂåñÁü´Ê≠£Êï¥‰∏™ÊãìÊâëÂõæ„ÄÇ ËøôÈáåÈù¢Ê∂âÂèäÂà∞‰∫Ü‰∏§‰∏™‰ºòÂåñÔºå‰∏Ä‰∏™ÊòØÂêéÁ´ØÂ±ÄÈÉ®‰ºòÂåñÔºå‰∏Ä‰∏™ÊòØÂÖ®Â±ÄÈó≠ÁéØ‰ºòÂåñÔºå‰∏§ËÄÖËÆ°ÁÆóÁöÑÊÄùË∑ØÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇ 2 ‰ºòÂåñ2.1 ÂÖ®Â±ÄÈó≠ÁéØ‰ºòÂåñÔºåÁî®‰∫éÁü´Ê≠£Êï¥‰∏™ÊãìÊâëÂõæÂâçÁ´ØÂêéÁ´ØÂÆåÊàêÁöÑ‰∫ãÊÉÖÊòØÊé¢Á¥¢Âπ∂ÂàõÂª∫Êñ∞ÁöÑËäÇÁÇπÔºåËé∑ÂæóÊñ∞ÁöÑÊµãÈáèÂÄºÔºåÊ∑ªÂä†Êñ∞ÁöÑ‰ΩçÂßøÂÖ≥Á≥ªÊñπÁ®ãÔºö \begin{eqnarray} \begin{split} & x_0 + z_{01} = x_1\\ & x_1 + z_{12} = x_2\\ & ...\\ & x_{k-1} + z_{k-1, k} = x_{k}\\ \end{split} \end{eqnarray}ËÄåÂÖ®Â±ÄÈó≠ÁéØÊ£ÄÊµãÊ∑ªÂä†Â∑≤Áü•ËäÇÁÇπ‰πãÈó¥ÁöÑ‰ΩçÂßøÁ∫¶ÊùüÂÖ≥Á≥ªÔºö \begin{equation} x_i + z_{i j} = x_j, \ \ \ \ i,j\in [0, k] \end{equation}ÂÜçÊ∑ªÂä†‰∏Ä‰∏™ÂàùÂßãÊù°‰ª∂Ôºà‰∏çÊòØÂøÖÈ°ªÁöÑÔºå‰ΩÜÊòØÂêéÈù¢ÂÆûÈ™åË°®ÊòéÂõ∫ÂÆö‰∏Ä‰∏™È°∂ÁÇπÊØî‰∏çÂõ∫ÂÆöÊïàÊûúË¶ÅÂ•Ω‚Äî‚ÄîÁõ∏ÂΩì‰∫éÊúâ‰∏Ä‰∏™ÊòéÁ°ÆÂèØ‰ø°ÁöÑÂü∫ÂáÜÔºâÔºö x_0 = 0 ‰ª•‰∏äÁ∫øÊÄßÊñπÁ®ãÁªÑ‰∏≠ÔºåÈó≠ÁéØÊ£ÄÊµãÈÉ®ÂàÜÁöÑÊñπÁ®ã‰∏≠ÁöÑ‰∏§‰∏™ÁªìÁÇπÈÉΩÂú®ÂâçÈù¢Âá∫Áé∞ËøáÔºåÂõ†Ê≠§‰∏çÂ¢ûÂä†Áü©ÈòµÁöÑÁß©ÔºåÂõ†Ê≠§ÊúÄÁªàË¶ÅÊ±ÇËß£ÂåÖÂê´$k$‰∏™ÊñπÁ®ã$k+1$‰∏™Êú™Áü•Êï∞ÁöÑÁ∫øÊÄßÊñπÁ®ãÁªÑ„ÄÇ Èó≠ÁéØÁöÑÂÖ≥ÈîÆÊÄßÔºöÂ¶ÇÊûúÊ≤°ÊúâÈó≠ÁéØÊù°‰ª∂ÔºåÊñπÁ®ãÁªÑ$Ax=b$Â∑¶Âè≥‰∏§ËæπÁß©ÊòØÁõ∏Á≠âÁöÑ‚Äî‚ÄîÊúâÂîØ‰∏ÄËß£ÔºåÊ∑ªÂä†‰∫ÜÈó≠ÁéØÊù°‰ª∂‰ª•ÂêéÔºåÁõ∏ÂΩì‰∫éÊñπÁ®ãÁªÑÂ∑¶‰æß$A$ÁöÑÁß©‰∏çÂèòÔºå‰ΩÜÊòØÂè≥‰æß$b$ÁöÑÁß©ÂàôÂ¢ûÂä†‰∫ÜÔºå$rank(A) &lt; rank(A, b)$‚Äî‚ÄîÊ≤°ÊúâËß£ÊûêËß£ÔºåÂè™ÊúâÊúÄ‰ºò„ÄÇ ÂÆûÈôÖ‰∏äÁä∂ÊÄÅ$\textbf x$ÊòØ‰∏Ä‰∏™ÂåÖÂê´Â§πËßí$\theta$ÁöÑÂêëÈáè$[x, y, \theta]$ÔºåÂÆûÈôÖÁõ∏ÂØπ‰ΩçÂßøÁöÑËÆ°ÁÆóÂπ∂ÈùûÁÆÄÂçïÁöÑÁ∫øÊÄßÂè†Âä†Ôºö \textbf x \oplus \Delta \textbf x = \begin{pmatrix} x + \Delta x cos\theta - \Delta y sin \theta \\ y + \Delta x sin\theta + \Delta y cos \theta \\ normAngle(\theta + \Delta \theta) \end{pmatrix}‰∏æ‰∏™Ê†óÂ≠êÔºö Êú∫Âô®‰∫∫‰ªéËµ∑Âßã‰ΩçÁΩÆ$x_0=0$Â§ÑÂá∫ÂèëÔºåÈáåÁ®ãËÆ°ÊµãÂæóÂÆÉÂêëÂâçÁßªÂä®‰∫Ü1mÔºåÂà∞Ëææ$x_1$ÔºåÊé•ÁùÄÊµãÂæóÂÆÉÂêëÂêéÁßªÂä®‰∫Ü0.8mÔºåÂà∞Ëææ$x_2$ÔºåËøôÊó∂ÈÄöËøáÈó≠ÁéØÊ£ÄÊµãÔºåÂèëÁé∞‰ªñÂõûÂà∞‰∫ÜËµ∑Âßã‰ΩçÁΩÆ„ÄÇ È¶ñÂÖàÊ†πÊçÆÁªôÂá∫‰ø°ÊÅØÊûÑÂª∫ÂõæÔºö x_0 + 1 = x_1\\ x_1 - 0.8 = x_2ÁÑ∂ÂêéÊ†πÊçÆÈó≠ÁéØÊù°‰ª∂Ê∑ªÂä†Á∫¶ÊùüÔºö x_2 = x_0Ë°•ÂÖÖÂàùÂßãÊù°‰ª∂Ôºö x_0 = 0‰ΩøÁî®ÊúÄÂ∞è‰∫å‰πòÊ≥ïÊ±Ç‰∏äËø∞ÊñπÁ®ãÁªÑÁöÑÊúÄ‰ºòËß£ÔºåÈ¶ñÂÖàÊûÑÂª∫ÊÆãÂ∑ÆÂπ≥ÊñπÂíåÂáΩÊï∞Ôºö \begin{eqnarray} \begin{split} & f_1 = x_0 = 0\\ & f_2 = x_1 - x_0 - 1 = 0\\ & f_3 = x_2 - x_1 + 0.8 = 0\\ & f_4 = x_2 - x_0 = 0 \end{split} \end{eqnarray} c = \Sigma_{i=1}^4 f_i^2 = x_0^2 + (x_1-x_0-1)^2 + (x_2-x_1+0.8)^2 + (x_2-x_0)^2ÁÑ∂ÂêéÂØπÊØè‰∏™ÂèÇÊï∞Ê±ÇÂÅèÂØºÔºö \frac{\partial c}{\partial x_1} = -x_0 + 2x_1-x_2 -1.8=0\\ \frac{\partial c}{\partial x_2} = -x_0 - x_1 +2x_2 + 0.8 = 0Ëß£Âæó$x_1 = 0.93, x_2 = 0.07$ÔºåÂèØ‰ª•ÁúãÂà∞Èó≠ÁéØÁü´Ê≠£‰∫ÜÊâÄÊúâËäÇÁÇπÁöÑ‰ΩçÂßøÔºå‰ºòÂåñ‰∫ÜÊï¥‰∏™ÊãìÊâëÂõæ„ÄÇ 2.2 ÂêéÁ´ØÂ±ÄÈÉ®‰ºòÂåñÔºåÁî®‰∫éÁü´Ê≠£Â±ÄÈÉ®Âú∞ÂõæÂÜç‰∏æ‰∏™Ê†óÂ≠êÔºö Êú∫Âô®‰∫∫‰ªéËµ∑Âßã‰ΩçÁΩÆ$x_0=0$Â§ÑÂá∫ÂèëÔºåÂπ∂ËßÇÊµãÂà∞ÂÖ∂Ê≠£ÂâçÊñπ2mÂ§ÑÊúâ‰∏Ä‰∏™Ë∑ØÊ†á$l_0$ÔºåÈáåÁ®ãËÆ°ÊµãÂæóÂÆÉÂêëÂâçÁßªÂä®‰∫Ü1mÔºåÂà∞Ëææ$x_1$ÔºåËøôÊó∂ËßÇÊµãÂà∞Ë∑ØÊ†áÂú®ÂÖ∂Ê≠£ÂâçÊñπ0.8mÂ§Ñ„ÄÇ È¶ñÂÖàÊ†πÊçÆÂâçÁ´Ø‰ø°ÊÅØÂª∫Âõæ raw graphÔºàËøôÊ†∑Âª∫ÂõæÊòéÊòæÊòØÂ≠òÂú®Á¥ØÁßØËØØÂ∑ÆÁöÑÔºâÔºö x_0 + 1 = x_1ÁÑ∂ÂêéÊ∑ªÂä†Èó≠ÁéØÁ∫¶ÊùüÔºö x_1 + 0.8 = l_0\\ x_0 + 2 = l_0ÂàùÂßãÊù°‰ª∂Ôºö x_0 = 0ÊûÑÂª∫ÊÆãÂ∑ÆÂπ≥ÊñπÂíåÔºö c = \Sigma_{i=1}^4 f_i^2 = x_0^2 + (x_1-x_0-1)^2 +(l_0-x_0-2)^2 + (l_0-x_1-0.8)^2Ê±ÇÂÅèÂØºÊ±ÇËß£Ôºö$x_1 = 1.07, l_0 = 1.93$ÔºåÂèØ‰ª•ÁúãÂà∞ÂêéÁ´ØÊòØÂØπÂâçÁ´ØÊñ∞Ê∑ªÂä†ËøõÊù•ÁöÑËäÇÁÇπ‰ΩçÂßøÂÅö‰∫ÜÁü´Ê≠£ÔºåÊ∂àÈô§ÈÉ®ÂàÜÊµãÈáèËØØÂ∑Æ„ÄÇ ËøôÈáåÈù¢Ê∂âÂèäÂà∞‰∏§Áßç‰º†ÊÑüÂô®‰ø°ÊÅØ‚Äî‚ÄîÈáåÁ®ãËÆ°ÂíåÊøÄÂÖâÈõ∑ËææÔºå‰∏§Áßç‰º†ÊÑüÂô®ÁöÑÁ≤æÂ∫¶ÊòØÊúâÂ∑ÆÂà´ÁöÑÔºåÊàë‰ª¨ÂØπÂÖ∂ÁöÑ‰ø°‰ªªÁ®ãÂ∫¶‰πüÂ∫îËØ•‰∏çÂêåÔºåÂèçÊò†Âà∞ÂÖ¨Âºè‰∏≠Â∞±ÊòØË¶Å‰∏∫‰∏çÂêå‰º†ÊÑüÂô®‰ø°ÊÅØËµã‰∫à‰∏çÂêåÁöÑÊùÉÈáç„ÄÇÂÅáËÆæÁºñÁ†ÅÂô®‰ø°ÊÅØÊõ¥ÂáÜÁ°ÆÔºåÈÇ£‰πàÔºö c = \Sigma_{i=1}^4 f_i^2 = x_0^2 + \textbf{10}(x_1-x_0-1)^2 +(l_0-x_0-2)^2 + (l_0-x_1-0.8)^2Ë∞ÉÊï¥ÊùÉÈáç‰πãÂêéËß£ÂæóÔºö$x_1 = 1.01, l_0 = 1.9$ÔºåÂèØ‰ª•ÁúãÂà∞ËÆ°ÁÆóÁªìÊûú‰ºöÂêëÁùÄÊõ¥‰ø°‰ªªÁöÑ‰º†ÊÑüÂô®ÁöÑÊµãÈáèÁªìÊûúÈù†Ëøë„ÄÇ 2.3 ‰∏•Ê†ºÊé®ÂØº2.3.1 ‰ø°ÊÅØÁü©ÈòµÔºàËØØÂ∑ÆÊùÉÈáçÁü©Èòµ) Âõæ‰ºòÂåñÈóÆÈ¢òËΩ¨Âåñ‰∏∫ÊúÄÂ∞è‰∫å‰πòÈóÆÈ¢òÔºåÈ¶ñÂÖàÊòØÂ∏¶ÊùÉÈáçÁöÑÊÆãÂ∑ÆÂπ≥ÊñπÂíåÂáΩÊï∞ÁöÑ‰∏ÄËà¨ÂΩ¢ÂºèÔºö F(x) = \Sigma_{i,j} e(x_i, x_j, z_{i,j})^T\Omega_{i,j}e(x_i, x_j, z_{i,j}) x^{*} = argminF(x)ÂÖ∂‰∏≠ÁöÑ$\Omega_{i,j}$È°πÂ∞±ÊòØ‰∏äÊñáÊèêÂà∞ÁöÑËØØÂ∑ÆÊùÉÈáçÁü©ÈòµÔºåÂÆÉÁöÑÊ≠£ÂºèÂêçÂ≠óÂè´‰ø°ÊÅØÁü©Èòµ„ÄÇ ‰º†ÊÑüÂô®ÁöÑÊµãÈáèÂÄºÔºåÂèØ‰ª•Áúã‰ΩúÊòØ‰ª•ÁúüÂÄº‰∏∫‰∏≠ÂøÉÁöÑÂ§öÂÖÉÈ´òÊñØÂàÜÂ∏ÉÔºö f_x(x_1, x_2, ..., x_k) = \frac{1}{\sqrt{}(2\pi)^k|\Sigma|}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))ÂçèÊñπÂ∑ÆÁü©Èòµ$\Sigma$ÂØπËßíÁ∫ø‰∏äÁöÑÂÄºË°®Á§∫ÊØè‰∏ÄÁª¥ÂØπÂ∫îÁöÑÊñπÂ∑ÆÔºåËØ•ÊñπÂ∑ÆÂÄºË∂äÂ§ßË°®Á§∫ËØ•Áª¥Â∫¶‰∏çÁ°ÆÂÆöÊÄßË∂äÂ§ßÔºåÂØπÂ∫îÁöÑ‰ø°ÊÅØÊùÉÈáçÂ∫îËØ•Ë∂äÂ∞è„ÄÇÂÆûÈôÖ‰∏äÊãìÊâëÂõæ‰∏äÊØèÊù°ËæπÂØπÂ∫îÁöÑ‰ø°ÊÅØÁü©ÈòµÂ∞±ÊòØÂØπÂ∫îÊµãÈáèÂçèÊñπÂ∑ÆÁü©ÈòµÁöÑÈÄÜ„ÄÇ 2.3.2 ÈùûÁ∫øÊÄß ‰∏äÊñáÂ∑≤ÁªèÊèêÂà∞Ôºå‰ΩçÂßøÂèòÂåñÈùûÁ∫øÊÄß‚Äî‚ÄîÈùûÁ∫øÊÄßÊúÄÂ∞è‰∫å‰πòÈóÆÈ¢òÔºåË¶ÅÈááÁî®Ëø≠‰ª£Ê≥ïÊ±ÇËß£„ÄÇËø≠‰ª£Ê≥ïÈúÄË¶ÅÊúâ‰∏Ä‰∏™Â•ΩÁöÑÂàùÂßãÂÅáËÆæÂÄºÔºåÁÑ∂ÂêéÂú®Ëøô‰∏™ÂÄºÈôÑËøëÂ¢ûÈáèÂºèËø≠‰ª£ÂØªÊâæÊúÄ‰ºòËß£„ÄÇ f(x) = \Sigma e^T\Omega e\\ ÊúÄÂ∞è‰∫å‰πòÈóÆÈ¢òÁõÆÊ†áÂáΩÊï∞Ôºömin \frac{1}{2}||f(x)||_2^2È¶ñÂÖàË¶ÅÂ∞ÜÈùûÁ∫øÊÄßÂáΩÊï∞ËΩ¨ÂåñÊàêÂÖ≥‰∫éÂ¢ûÈáè$\Delta x$ÁöÑÁ∫øÊÄßÂáΩÊï∞‚Äî‚ÄîÊ≥∞ÂãíÂ±ïÂºÄÔºåÊ†πÊçÆÂÖ∑‰ΩìÁöÑÂ±ïÂºÄÂΩ¢ÂºèÂèàÂàÜ‰∏∫Ôºö ‰∏ÄÈò∂„ÄÅ‰∫åÈò∂Ê¢ØÂ∫¶Ê≥ï Áõ¥Êé•ÂØπÁõÆÊ†áÂáΩÊï∞Âú®$x$ÈôÑËøëËøõË°åÊ≥∞ÂãíÂ±ïÂºÄÔºö ||f(x+\Delta x)||_2^2 \approx ||f(x)||_2^2 +J(x) \Delta x = \frac{1}{2} \Delta x^T H \Delta x‰∏ÄÈò∂Ê¢ØÂ∫¶Ê≥ïÔºàÊúÄÈÄü‰∏ãÈôçÊ≥ïÔºâÔºöÂè™‰øùÁïô‰∏ÄÈò∂Ê¢ØÂ∫¶ÔºåÂπ∂ÂºïÂÖ•Ê≠•Èïø$\lambda$Ôºö \Delta x^* = -\lambda J^T(x)‰∫åÈò∂Ê¢ØÂ∫¶Ê≥ïÔºàÁâõÈ°øÊ≥ïÔºâÔºö‰øùÁïô‰∏ÄÈò∂Âíå‰∫åÈò∂Ê¢ØÂ∫¶‰ø°ÊÅØ J^T(x)+H\Delta x^*=0ÊúÄÈÄü‰∏ãÈôçÊ≥ïËøá‰∫éË¥™ÂøÉÔºåÂÆπÊòìËµ∞Âá∫ÈîØÈΩøË∑ØÁ∫øÔºåÂ¢ûÂä†Ëø≠‰ª£Ê¨°Êï∞„ÄÇÁâõÈ°øÊ≥ïÈúÄË¶ÅËÆ°ÁÆóÁõÆÊ†áÂáΩÊï∞ÁöÑ‰∫åÈò∂ÂØºÊï∞ÔºàHessianÁü©ÈòµÔºâÔºåËÆ°ÁÆóÂõ∞Èöæ„ÄÇ È´òÊñØÁâõÈ°øÊ≥ï ÂØπ$f(x)$ËÄå‰∏çÊòØÁõÆÊ†áÂáΩÊï∞$f(x)^2$Âú®$x$ÈôÑËøëËøõË°å‰∏ÄÈò∂Ê≥∞ÂãíÂ±ïÂºÄÔºö f(x+\Delta x) \approx f(x) + J(x) \Delta xÂØπÂ∫îÊØè‰∏Ä‰∏™ËØØÂ∑ÆÂáΩÊï∞$e_{ij}$Ôºö \begin{split} & e_{ij}(x_i+\Delta x_i, x_j+\Delta x_j) = e_{i,j}(x+\Delta x) \\ &\approx e_{ij} +\frac{\partial e_{ij}}{\partial x}\Delta x = e_{ij} + J_{ij}\Delta x \end{split}‚Äã ÂÖ∂‰∏≠$J_{ij}$‰∏∫ÂàùÂßãÂÄºÈôÑËøëÁöÑÈõÖÂèØÊØîÁü©ÈòµÔºàÂÆö‰πâËßÅÂç°Â∞îÊõºÊª§Ê≥¢Ôºâ„ÄÇ Â∏¶ÂÖ•ÁõÆÊ†áÂáΩÊï∞ÂæóÂà∞Ëøë‰ºº‰∫åÈò∂Â±ïÂºÄÔºö \begin{split} & F_{ij}(x+\Delta x) = e_{ij}(x+\Delta x)^T \Omega_{ij}e_{ij}(x+\Delta x)\\ & \approx (e_{ij} + J_{ij}\Delta x)^T \Omega_{ij}(e_{ij} + J_{ij}\Delta x)\\ & = \underbrace{e_{ij}^T\Omega_{ij}e_{ij}}_{c_{ij}} + 2\underbrace{e_{ij}^T\Omega_{ij}J_{ij}}_{b_{ij}^T}\Delta x + \Delta x^T \underbrace{J_{ij}^T\Omega_{ij}J_{ij}}_{H_{ij}}\Delta x\\ & = c_{ij} + 2b_{ij}^T\Delta x + \Delta x^T H_{ij}\Delta x \end{split}Ê±ÇËß£Â¢ûÈáè$\Delta x$Ôºö 2b + 2H\Delta x^* = 0\\ È´òÊñØÁâõÈ°øÊñπÁ®ãÔºöH\Delta x^* = -b\\ÂØπÊØîÁâõÈ°øÊ≥ïÂèØËßÅÔºåÈ´òÊñØÁâõÈ°øÊ≥ïÁî®$J^TJ$‰Ωú‰∏∫‰∫åÈò∂HessianÁü©ÈòµÁöÑËøë‰ººÔºåÁÆÄÂåñ‰∫ÜËÆ°ÁÆó„ÄÇ ‰∏äËø∞ÁÆóÊ≥ïË¶ÅÊ±ÇËøë‰ºº$H$Áü©ÈòµÊòØÊ≠£ÂÆö‰∏îÂèØÈÄÜÁöÑÔºåÂÆûÈôÖÊï∞ÊçÆÂæàÈöæÊª°Ë∂≥ÔºåÂõ†ËÄåÂú®‰ΩøÁî®È´òÊñØÁâõÈ°øÁÆóÊ≥ïÊó∂ÂèØËÉΩÂá∫Áé∞$H$‰∏∫Â•áÂºÇÁü©ÈòµÊàñÁóÖÊÄÅÁöÑÊÉÖÂÜµÔºåÂ¢ûÈáèÁ®≥ÂÆöÊÄßËæÉÂ∑ÆÔºåÂØºËá¥ÁÆóÊ≥ï‰∏çÊî∂Êïõ„ÄÇ ÂõæÂΩ¢‰∏äÊù•ÊÄùËÄÉÔºåÂ∞±ÊòØËøë‰ººÂêéÁöÑÊ¢ØÂ∫¶ÊñπÂêë‰∏çÂÜçÊòØÊ¢ØÂ∫¶ÂèòÂåñÊúÄÂø´ÁöÑÊñπÂêëÔºåÂèØËÉΩÂºïËµ∑‰∏çÁ®≥ÂÆö„ÄÇ ÂàóÊñá‰ºØÊ†º‚ÄîÈ©¨Â§∏Â∞îÁâπÊ≥ï ‰∏∫$\Delta x$Ê∑ªÂä†‰∏Ä‰∏™‰ø°ËµñÂå∫ÂüüÔºå‰∏çËÆ©ÂÆÉÂõ†‰∏∫ËøáÂ§ßËÄå‰ΩøÂæóËøë‰ºº$f(x+\Delta x) = f(x) + J(x)\Delta x$‰∏çÂáÜÁ°Æ„ÄÇ \rho = \frac{f(x+\Delta x) - f(x)}{J(x) \Delta x}ÂèØ‰ª•ÁúãÂà∞Â¶ÇÊûú$\rho$Êé•Ëøë1ÔºåËØ¥ÊòéËøë‰ººÊØîËæÉÂ•Ω„ÄÇÂ¶ÇÊûú$\rho$ÊØîËæÉÂ§ßÔºåËØ¥ÊòéÂÆûÈôÖÂáèÂ∞èÁöÑÂÄºËøúÂ§ß‰∫é‰º∞ËÆ°ÂáèÂ∞èÁöÑÂÄºÔºåÈúÄË¶ÅÊîæÂ§ßËøë‰ººËåÉÂõ¥ÔºåÂèç‰πã‰Ω†ÊáÇÁöÑ„ÄÇ ||D\Delta x_k||_2^2 \leq \muÂ∞ÜÊØèÊ¨°Ëø≠‰ª£ÂæóÂà∞ÁöÑ$\Delta x$ÈôêÂÆöÂú®‰∏Ä‰∏™ÂçäÂæÑ‰∏∫‰ø°ËµñÂå∫ÂüüÁöÑÊ§≠ÁêÉ‰∏≠ÔºåÊ†πÊçÆ$\rho$ÁöÑÂ§ßÂ∞è‰øÆÊîπ‰ø°ËµñÂå∫Âüü„ÄÇ‰∫éÊòØÈóÆÈ¢òËΩ¨ÂåñÊàê‰∏∫‰∫ÜÂ∏¶‰∏çÁ≠âÂºèÁ∫¶ÊùüÁöÑ‰ºòÂåñÈóÆÈ¢òÔºö min_\Delta x \frac{1}{2}||f(x) + J(x)\Delta x||^2_2, \ \ s.t. ||D \Delta x ||^2 = \muÁî®ÊãâÊ†ºÊúóÊó•‰πòÂ≠êËΩ¨ÂåñÊàêÊó†Á∫¶ÊùüÈóÆÈ¢òÔºö min_\Delta x \frac{1}{2}||f(x) + J(x)\Delta x||^2_2 + \frac{\lambda}{2}||D \Delta x ||^2Â±ïÂºÄÂêéÂæóÂà∞Â¶Ç‰∏ãÂΩ¢ÂºèÔºö (H + \lambda D^TD)\Delta x^* = -bÈÄöÂ∏∏Êää$D$ÂèñÂÄº‰∏∫Âçï‰ΩçÈòµ$I$ÔºåÂæóÂà∞Êõ¥ÁÆÄÂåñÂΩ¢ÂºèÔºö (H + \lambda I)\Delta x^* = -bÂΩì$\lambda$ËæÉÂ∞èÊó∂Ôºå$H$Âç†‰∏ªË¶ÅÂú∞‰ΩçÔºåËØ¥Êòé‰∫åÊ¨°Ëøë‰ººÊ®°ÂûãËæÉÂ•ΩÔºåLMÁÆóÊ≥ïÊõ¥Êé•ËøëÈ´òÊñØÁâõÈ°øÊ≥ï„ÄÇÂΩì$\lambda$ËæÉÂ§ßÊó∂Ôºå$\lambda I$Âç†‰∏ªË¶ÅÂú∞‰ΩçÔºåLMÁÆóÊ≥ïÊõ¥Êé•Ëøë‰∏ÄÈò∂Ê¢ØÂ∫¶Ê≥ï„ÄÇ‰øÆÊ≠£‰∫ÜÁ∫øÊÄßÊñπÁ®ãÁªÑÁü©ÈòµÁöÑÁóÖÊÄÅÈóÆÈ¢òÔºåÊØîÈ´òÊñØÁâõÈ°øÊ≥ïÊõ¥Âä†ÂÅ•Â£ÆÔºå‰ΩÜÊòØÊî∂ÊïõÈÄüÂ∫¶‰πüÊõ¥ÊÖ¢„ÄÇ ÂõæÂΩ¢‰∏äÊÄùËÄÉÔºåLMÁÆóÊ≥ï‰øÆÊ≠£‰∫ÜÈ´òÊñØÁâõÈ°øÊ≥ïÂæóÂà∞ÁöÑÊ¢ØÂ∫¶Ôºå‰ª•Ê≠§Âõ∫ÂÆö‰∏Ä‰∏™ÊêúÁ¥¢Âå∫ÂüüÔºåÂú®Âå∫ÂüüÂÜÖÂØªÊâæÊúÄ‰ºò„ÄÇ 2.3.3 Á®ÄÁñèÁü©Èòµ ÂØπ‰∫éËØØÂ∑ÆÂáΩÊï∞$e_{ij}$ÔºåÂÆÉÂè™Âíå$e_i$Âíå$e_j$ÊúâÂÖ≥ÔºåÂõ†Ê≠§ÂÆÉÁöÑÈõÖÂèØÊØîÁü©ÈòµÊúâÂ¶Ç‰∏ãÁªìÊûÑÔºàË°åÊï∞ÊòØ$x$ÁöÑÁª¥Â∫¶ÔºåÂàóÊï∞ÊòØÊãìÊâëÂõæ‰∏≠ËäÇÁÇπÊò†Â∞ÑÂÖ≥Á≥ªÁöÑÊï∞ÁõÆÔºâÔºö J_{ij} = \begin{bmatrix} 0 & ... & 0 & \underbrace{\frac{\partial e_{i}}{\partial x_i}}_{A_{ij}} & 0 & ... & \underbrace{\frac{\partial e_{j}}{\partial x_j}}_{B_{ij}} & 0 & ... & 0 \end{bmatrix}Áõ∏Â∫îÂú∞$b_{ij}$ÊòØ‰∏Ä‰∏™ÂåÖÂê´ÂæàÂ§ö0ÁöÑÂàóÂêëÈáèÔºö \begin{split} b_{ij}^T& = e_{ij}^T \Omega_{ij} J_{ij}\\ &= e_{ij}^T\Omega_{ij} (0 ... A_{ij}...B_{ij}...0)\\ &=(0...e_{ij}^T\Omega_{ij}A_{ij}...e_{ij}^T\Omega_{ij}B_{ij}...0) \end{split}$b = \Sigma_{ij} b_{ij}$Ôºö $H_{ij}$ÊòØ‰∏Ä‰∏™ÂåÖÂê´ÂæàÂ§ö0ÁöÑÂØπÁß∞ÈòµÔºö \begin{split} H_{ij}& = J_{ij}^T \Omega_{ij}J_{ij} \\ & = \begin{pmatrix} ...\\ A_{ij}^T\\ ...\\ B_{ij}^T\\ ... \end{pmatrix} \Omega_{ij} \begin{pmatrix} ... & A_{ij} & ... & B_{ij} & ... \end{pmatrix}\\ & = \begin{pmatrix} & \\ & A_{ij}^T\Omega{ij}A_{ij} & A_{ij}^T\Omega_{ij}B_{ij} & \\ & \\ & B{ij}^T\Omega_{ij}A_{ij} & B_{ij}^T\Omega_{ij}B_{ij}& \\ & \end{pmatrix} \end{split}$H=\Sigma_{ij}H_{ij}$Ôºö Ê¢≥ÁêÜ‰∏Ä‰∏ãËÆ°ÁÆóÊµÅÁ®ãÔºö$e_{ij} \to J_{ij} \to A_{ij}, B_{ij} \to b_{ij}, H_{ij} \to b, H \to \Delta x^* \to x$ 2.3.4 ËØØÂ∑ÆÂáΩÊï∞ ÂâçÈù¢ÂÆö‰πâËøá‰ΩçÂßøÁöÑÈùûÁ∫øÊÄßÂè†Âä†ÔºåÊòæÁÑ∂‰ΩçÂßøËØØÂ∑Æ‰πü‰∏çÊòØÁÆÄÂçïÁöÑÁ∫øÊÄßÂä†ÂáèÂÖ≥Á≥ªÔºö e_{ij}(x_i, x_j) = t2v(Z_{ij}^{-1}(X_i^{-1}.X_j))ÂÖ∂‰∏≠ÁöÑ$Z_{ij}$„ÄÅ$X_i$„ÄÅ$X_j$ÈÉΩÊòØÁü©ÈòµÂΩ¢Âºè„ÄÇ$X_i^{-1}X_j$Ë°®Á§∫ËäÇÁÇπjÂà∞ËäÇÁÇπi‰πãÈó¥ÁöÑ‰ΩçÂßøÂ∑ÆÂºÇ$\hat Z_{ij}$ÔºåÂÅáËÆæËøô‰∏™ËΩ¨ÁßªÁü©ÈòµÂΩ¢ÂºèÂ¶Ç‰∏ãÔºö \hat Z_{ij} = \begin{bmatrix} R_{2*2} & t_{2*1} \\ 0 & 1 \end{bmatrix}ÂÅáËÆæÊµãÈáèÂÄº$Z_{ij}$ÂΩ¢ÂºèÂ¶Ç‰∏ãÔºö Z_{ij} = \begin{bmatrix} R^{'} & t^{'}\\ 0 & 1 \end{bmatrix}ÂàÜÂùóÁü©ÈòµÁöÑÊ±ÇÈÄÜËøáÁ®ãÂ¶Ç‰∏ãÔºö \begin{split} \begin{bmatrix} R & t\\ 0 & 1 \end{bmatrix}^{-1}& = \begin{bmatrix} \begin{bmatrix} I & t\\ 0 & 1 \end{bmatrix} \begin{bmatrix} R & 0\\ 0 & 1 \end{bmatrix} \end{bmatrix}^{-1}= \begin{bmatrix} R & 0\\ 0 & 1 \end{bmatrix}^{-1} \begin{bmatrix} I & t\\ 0 & 1 \end{bmatrix}^{-1}\\ & =\begin{bmatrix} R^T & 0\\ 0 & 1 \end{bmatrix} \begin{bmatrix} I & -t\\ 0 & 1 \end{bmatrix}= \begin{bmatrix} R^T & -R^Tt\\ 0 & 1 \end{bmatrix} \end{split}ÊâÄ‰ª•ËØØÂ∑Æ$e_{ij}$ËÆ°ÁÆóÂ¶Ç‰∏ãÔºö E_{ij} = Z_{ij}^{-1}\hat Z_{ij} = \begin{bmatrix} R^{'} & t^{'}\\ 0 & 1 \end{bmatrix}^{-1} \begin{bmatrix} R & t\\ 0 & 1 \end{bmatrix}= \begin{bmatrix} R^{'T} & -R^{'T}t^{'}\\ 0 & 1 \end{bmatrix} \begin{bmatrix} R & t\\ 0 & 1 \end{bmatrix}= \begin{bmatrix} R^{'T}R & R^{'T}(t-t^{'})\\ 0 & 1 \end{bmatrix} e_{ij} = t2v(E_{ij})= \begin{bmatrix} R_z(t_{\hat z} - t_z)_{2*1}\\ \theta_\hat z - \theta_z \end{bmatrix}_{3*1}= \begin{bmatrix} R_z(x_{\hat z} - x_z)\\ R_z(y_{\hat z} - y_z)\\ \theta_\hat z - \theta_z \end{bmatrix}= \begin{bmatrix} R_z[R_i(x_{j} - x_i) - x_z]\\ R_z[R_i(y_{j} - y_{i}) - y_z]\\ \theta_j - \theta_i - \theta_z \end{bmatrix}Ê±ÇËß£ÈõÖÂèØÊØîÁü©Èòµ$J_{ij}$Ôºö A_{ij} = \begin{bmatrix} \frac{\partial e_1}{\partial x_i} & \frac{\partial e_1}{\partial y_i} & \frac{\partial e_1}{\partial \theta_i}\\ \frac{\partial e_2}{\partial x_i} & \frac{\partial e_2}{\partial y_i} & \frac{\partial e_2}{\partial \theta_i}\\ \frac{\partial e_3}{\partial x_i} & \frac{\partial e_3}{\partial y_i} & \frac{\partial e_3}{\partial \theta_i}\\ \end{bmatrix}= \begin{bmatrix} -R_z^TR_i^T & 0 & R_z^T\frac{\partial R_I^T}{\partial \theta_i}(x_j-x_i)\\ 0 & -R_z^TR_i^T & R_z^T\frac{\partial R_I^T}{\partial \theta_i}(y_j-y_i)\\ 0 & 0 & -1 \end{bmatrix} B_{ij} = \begin{bmatrix} R_z^TR_i^T & 0\\ 0 & 1 \end{bmatrix}Á¥ØÂä†$b$Âíå$H$Áü©ÈòµÔºö b_{[i]} += A_{ij}\Omega_{ij}e_{ij}\\ b_{[j]} += B_{ij}^T\Omega_{ij}e_{ij}\\ H_{[ii]} += A_{ij}^T\Omega_{ij}A_{ij}\\ H_{[ij]} += A_{ij}^T\Omega_{ij}B_{ij}\\ H_{[ji]} += B_{ij}^T\Omega_{ij}A_{ij}\\ H_{[jj]} += B_{ij}^T\Omega_{ij}B_{ij}]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÂÖ≥ËÅîÂàÜÊûê]]></title>
    <url>%2F2018%2F04%2F29%2F%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[1 ÂºïË®ÄÈ¢ëÁπÅÈ°πÈõÜÔºöÈõÜÂêàÔºå${a, b, c}$ ÂÖ≥ËÅîËßÑÂàôÔºöÊò†Â∞ÑÔºå$a\to b$ ÊîØÊåÅÂ∫¶ÔºöÈíàÂØπÊüê‰∏™È¢ëÁπÅÈ°πÈõÜÔºå$support(È¢ëÁπÅÈ°πÈõÜa) = \frac{freq(È¢ëÁπÅÈ°πÈõÜa)}{freq(ÊâÄÊúâÈ°πÈõÜ)}$ ÂèØ‰ø°Â∫¶ÔºöË°°ÈáèÊüêÊù°ÂÖ≥ËÅîËßÑÂàôÔºå$confidence(a\to b) = \frac{support(a|b)}{support(a)}$ ÂØπ‰∫éÂåÖÂê´N‰∏™ÂÖÉÁ¥†ÁöÑÊï∞ÊçÆÈõÜÔºåÂèØËÉΩÁöÑÈõÜÂêàÊúâ$2^N - 1$ÁßçÔºåÊö¥ÂäõÈÅçÂéÜÊòæÁÑ∂ËçØ‰∏∏ÔºåÂõ†Ê≠§ÂºïÂÖ•AprioriÂéüÁêÜ„ÄÇ AprioriÂéüÁêÜÔºöÂáèÂ∞ëÂèØËÉΩÁöÑÈ°πÈõÜÔºåÈÅøÂÖçÊåáÊï∞Â¢ûÈïø„ÄÇ backwardsÔºöÂ¶ÇÊûúÊüê‰∏™È°πÈõÜÊòØÈ¢ëÁπÅÁöÑÔºåÈÇ£‰πàÂÆÉÁöÑÊâÄÊúâÂ≠êÈõÜ‰πüÊòØÈ¢ëÁπÅÁöÑ„ÄÇ forwardsÔºöÂ¶ÇÊûú‰∏Ä‰∏™È°πÈõÜÊòØÈùûÈ¢ëÁπÅÈ°πÈõÜÔºåÈÇ£‰πàÂÆÉÁöÑÊâÄÊúâË∂ÖÈõÜ‰πüÊòØÈùûÈ¢ëÁπÅÁöÑ„ÄÇ 2 AprioriÁÆóÊ≥ï1def apriori(dataSet, minSupport=0.5): ÁÆóÊ≥ïÊÄùË∑ØÔºö‰ªéÂçï‰∏™È°πÈõÜÂºÄÂßãÊ£ÄÊü•ÔºåÂéªÊéâÈÇ£‰∫õ‰∏çÊª°Ë∂≥ÊúÄÂ∞èÊîØÊåÅÂ∫¶ÁöÑÈ°πÈõÜÔºåÁÑ∂ÂêéÂØπÂâ©‰∏ãÁöÑÈõÜÂêàËøõË°åÁªÑÂêàÔºåÂæóÂà∞ÂåÖÂê´‰∏§‰∏™ÂÖÉÁ¥†ÁöÑÈ°πÈõÜÔºåÈáçÂ§çÊâ´ÊèèÔºåÁÑ∂ÂêéÂ∞ÜÂâ©‰ΩôÈ°πÈõÜÁªÑÂêàÊàêÂåÖÂê´‰∏â‰∏™ÂÖÉÁ¥†ÁöÑÈõÜÂêàÔºå‰æùÊ¨°Á±ªÊé®ÔºåÁõ¥Âà∞ÊâÄÊúâÈ°πÈõÜÈÉΩË¢´ÂéªÊéâ„ÄÇ ‰∏∫Âï•ÊúÄÂêé‰ºöÂæóÂà∞Á©∫ÈõÜÔºöÂõ†‰∏∫ÂåÖÂê´ÊâÄÊúâÂÖÉÁ¥†ÁöÑÈ°πÈõÜ‰∏ÄÂÆö‰∏çÊòØÈ¢ëÁπÅÈ°πÈõÜÔºåÂê¶ÂàôÊ†πÊçÆAprioriÂéüÁêÜÔºåÂÆÉÁöÑÂÖ®ÈÉ®Â≠êÈõÜÈÉΩÊòØÈ¢ëÁπÅÈ°πÈõÜ„ÄÇ Â¶Ç‰Ωï‰ªéÂåÖÂê´k‰∏™ÂÖÉÁ¥†ÁöÑÈ°πÈõÜÈõÜÂêàÁîüÊàêÂåÖÂê´k+1‰∏™ÂÖÉÁ¥†ÁöÑÈ°πÈõÜÈõÜÂêàÔºö‰ªék‰∏™ÂÖÉÁ¥†ÁöÑÈ°πÈõÜÂà∞k+1‰∏™ÂÖÉÁ¥†È°πÈõÜÁöÑÊâ©ÂÖÖÔºåÂè™ÂÖÅËÆ∏Êúâ‰∏Ä‰∏™ÂÖÉÁ¥†ÁöÑ‰∏çÂêåÔºåÁÆóÊ≥ï‰∏≠‰∏∫‰∫ÜÈÅøÂÖçÈáçÂ§çÁªìÊûúÔºåÂè™ÂØπÂâçk-1‰∏™ÂÖÉÁ¥†Áõ∏ÂêåÁöÑ‰∏§‰∏™È°πÈõÜÊ±ÇÂπ∂ÈõÜ„ÄÇ ‰ª£Á†ÅÂÆûÁé∞ËøáÁ®ã‰∏≠ÂèëÁé∞‰∫ÜÂá†‰∏™Áü•ËØÜËÆ∞ÂΩï‰∏Ä‰∏ãÔºö mapÂáΩÊï∞ÁöÑËøîÂõûÂÄºÔºöpython2‰∏ãÁõ¥Êé•ËøîÂõûÂàóË°®Ôºåpython3‰∏ãËøîÂõûÁöÑÊòØËø≠‰ª£Âô®Ôºö 12345map(frozenset, C1)# ËøîÂõû &lt;map object at 0x101e78940&gt; list(map(frozenset, C1))# ËøîÂõû list[frozenset1(), frozenset2(), ...] Â≠óÂÖ∏ÁöÑupdateÊñπÊ≥ïÔºö 12# Â∞Üdict2ÁöÑÈîÆÂÄºÊ∑ªÂä†Âà∞dict1‰∏≠ÔºåÂú®Ê∂âÂèäËø≠‰ª£Êìç‰ΩúÊó∂ÂèØ‰ª•ÁúÅÁï•‰º†ÈÄí‰∏≠Èó¥ÂÄºdict1.update(dict2) set &amp; frozensetÔºösetÊó†ÊéíÂ∫è‰∏î‰∏çÈáçÂ§çÔºåÂπ∂‰∏îÂèØÂèòÔºåÂõ†Ê≠§unhashable„ÄÇfrozenset‰∏çÂèØÂèòÔºåÂèØ‰ª•Áî®‰ΩúÂ≠óÂÖ∏ÁöÑkey„ÄÇ 3 ÂÖ≥ËÅîËßÑÂàôÂØπ‰∏Ä‰∏™ÂåÖÂê´k‰∏™ÂÖÉÁ¥†ÁöÑÈ¢ëÁπÅÈ°πÈõÜÔºåÂÖ∂‰∏≠ÂèØËÉΩÁöÑÂÖ≥ËÅîËßÑÂàôÊúâÔºö C_N^1 + C_N^2 + ... + C_N^{N-1}Êö¥ÂäõÈÅçÂéÜËÇØÂÆöÂèàËçØ‰∏∏ÔºåÂõ†Ê≠§Âª∂Áª≠AprioriÁöÑÊÄùË∑ØÔºåÂÖ≥ËÅîËßÑÂàô‰πüÊúâ‰∏ÄÊù°Á±ª‰ººÁöÑÂ±ûÊÄßÔºö Â¶ÇÊûúÊüêÊù°ËßÑÂàôÁöÑÂâç‰ª∂‰∏çÊª°Ë∂≥ÊúÄÂ∞èÂèØ‰ø°Â∫¶Ë¶ÅÊ±ÇÔºåÈÇ£‰πàÂÆÉÁöÑÊâÄÊúâÂ≠êÈõÜ‰πü‰∏çÊª°Ë∂≥ÊúÄÂ∞èÂèØ‰ø°Â∫¶Ë¶ÅÊ±Ç„ÄÇ ÂØπÂ∫îÁöÑÔºåÂ¶ÇÊûúÊüêÊù°ËßÑÂàôÁöÑÂêé‰ª∂‰∏çÊª°Ë∂≥ÊúÄÂ∞èÂèØ‰ø°Â∫¶Ë¶ÅÊ±ÇÔºåÈÇ£‰πàÂÆÉÁöÑÊâÄÊúâË∂ÖÈõÜ‰πü‰∏çÊª°Ë∂≥„ÄÇ ÁÆóÊ≥ïÊÄùË∑ØÔºöÂØπÊØè‰∏™Ëá≥Â∞ëÂåÖÂê´‰∏§‰∏™ÂÖÉÁ¥†ÁöÑÈ¢ëÁπÅÈ°πÈõÜÔºå‰ªéÂêéÈÉ®Âè™ÂåÖÂê´‰∏Ä‰∏™ÂÖÉÁ¥†ÁöÑËßÑÂàôÂºÄÂßãÔºåÂØπËøô‰∫õËßÑÂàôËøõË°åÊµãËØïÔºåÊé•‰∏ãÊù•ÂØπÊâÄÊúâÂâ©‰ΩôËßÑÂàôÁöÑÂêé‰ª∂ËøõË°åÁªÑÂêàÔºåÂæóÂà∞ÂåÖÂê´‰∏§‰∏™ÂÖÉÁ¥†ÁöÑÂêé‰ª∂ÔºàÂØπÂ∫îÁöÑË°•ÈõÜÂ∞±ÊòØÂâç‰ª∂ÔºâÔºå‰æùÊ¨°Á±ªÊé®ÔºåÁõ¥Âà∞ÊµãËØïÂÆåÊâÄÊúâÂèØËÉΩÁöÑÂêé‰ª∂„ÄÇ ‰∏∫Âï•Âè™Ê£ÄÊü•ÂâçÂêé‰ª∂‰∫íË°•ÁöÑËßÑÂàôÔºöÂõ†‰∏∫‰∏Ä‰∏™È¢ëÁπÅÈ°πÈõÜÁöÑÊâÄÊúâÂ≠êÈõÜ‰πüÈÉΩÊòØÈ¢ëÁπÅÈ°πÈõÜÔºåÊâÄ‰ª•‰∏Ä‰∏™È¢ëÁπÅÈ°πÈõÜ‰∏≠‰∏ç‰∫íË°•ÁöÑËßÑÂàôÂ∞ÜÊòØËØ•È¢ëÁπÅÈ°πÈõÜÁöÑÊüê‰∏™Â≠êÈõÜÁöÑ‰∫íË°•ËßÑÂàô„ÄÇ 4 FP-growthÁÆóÊ≥ïAprioriÁÆóÊ≥ïÈÅøÂÖç‰∫ÜÊö¥ÂäõÈÅçÂéÜÂ≠êÈ°πÈõÜÁöÑÊåáÊï∞ÂºèÂ¢ûÈïøÔºå‰ΩÜÊòØÂØπÊØè‰∏Ä‰∏™Êñ∞ÁîüÊàêÁöÑÈ¢ëÁπÅÈ°πÈõÜÔºåÈÉΩË¶ÅÊâ´ÊèèÊï¥‰∏™Êï∞ÊçÆÈõÜÔºåÂΩìÊï∞ÊçÆÈõÜÂæàÂ§ßÊó∂ÔºåËøôÁßçÊäõÁâ©Á∫øÂºèÂ¢ûÈïøÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶‰πü‰∏çÂ§™‰ª§‰∫∫Êª°ÊÑè„ÄÇ FP-growthÁÆóÊ≥ïÂÄüÂä©‰∏ÄÁßçÁß∞‰∏∫FPÊ†ëÁöÑÊï∞ÊçÆÁªìÊûÑÂ≠òÂÇ®Êï∞ÊçÆÔºåÊù•ÊäΩË±°ÂéüÂßãÊï∞ÊçÆÈõÜÔºö È°πÈõÜ‰ª•Ë∑ØÂæÑÁöÑÊñπÂºèÂ≠òÂÇ®Âú®Ê†ë‰∏≠ Áõ∏‰ººÈ°π‰πãÈó¥Áõ∏ËøûÊé•ÊàêÈìæË°® ‰∏Ä‰∏™ÂÖÉÁ¥†È°πÂèØ‰ª•Âú®FPÊ†ë‰∏≠Âá∫Áé∞Â§öÊ¨° FPÊ†ëÂ≠òÂÇ®ÁöÑÊòØÂÖÉÁ¥†ÁöÑÂá∫Áé∞È¢ëÁéá È°πÈõÜÂÆåÂÖ®‰∏çÂêåÊó∂ÔºåÊ†ëÊâç‰ºöÂàÜÂèâÔºåÂê¶Âàô‰ºöÊúâÂ§çÁî®Ë∑ØÂæÑ ‚ÄãÁÆóÊ≥ïÊÄùË∑ØÔºöÈ¶ñÂÖàÈÅçÂéÜ‰∏ÄÈÅçÂéüÂßãÊï∞ÊçÆÈõÜÔºåËÆ∞ÂΩïÂÖÉÁ¥†ÁöÑÂá∫Áé∞È¢ëÁéáÔºåÂéªÊéâ‰∏çÊª°Ë∂≥ÊúÄÂ∞èÊîØÊåÅÂ∫¶ÁöÑÂÖÉÁ¥†„ÄÇÁÑ∂ÂêéÂÜçÈÅçÂéÜ‰∏ÄÈÅçÂâ©‰∏ãÁöÑÈõÜÂêàÂÖÉÁ¥†ÔºåÊûÑÂª∫FPÊ†ë„ÄÇÁÑ∂ÂêéÂ∞±ÂèØ‰ª•ÈÄöËøáFPÊ†ëÊåñÊéòÈ¢ëÁπÅÈ°πÈõÜ„ÄÇ ÊûÑÂª∫FPÊ†ëÔºö‰æùÊ¨°ÈÅçÂéÜÊØè‰∏Ä‰∏™È°πÈõÜÔºåÈ¶ñÂÖàÂ∞ÜÂÖ∂‰∏≠ÁöÑÈùûÈ¢ëÁπÅÈ°πÁßªÈô§ÔºåÂπ∂ÊåâÁÖßÂÖÉÁ¥†Âá∫Áé∞È¢ëÁéáÂØπËøáÊª§ÂêéÁöÑÂÖÉÁ¥†ËøõË°åÈáçÊéíÂ∫è„ÄÇÂØπËøáÊª§„ÄÅÊéíÂ∫èÂêéÁöÑÈõÜÂêàÔºåÂ¶ÇÊûúÊ†ë‰∏≠Â∑≤Â≠òÂú®Áé∞ÊúâÂÖÉÁ¥†ÔºåÂàôÂ¢ûÂä†Áé∞ÊúâÂÖÉÁ¥†ÁöÑËÆ°Êï∞ÂÄºÔºåÂ¶ÇÊûú‰∏çÂ≠òÂú®ÔºåÂàôÂêëÊ†ë‰∏≠Ê∑ªÂä†‰∏Ä‰∏™ÂàÜÊîØÔºåÊñ∞Â¢ûËäÇÁÇπÁöÑÂêåÊó∂ËøòË¶ÅÊõ¥Êñ∞ÈìæË°®ÂÖÉÁ¥†„ÄÇ‰∏ªË¶ÅÂ∞±Ê∂âÂèä‰∏§‰∏™Êï∞ÊçÆÁªìÊûÑÔºö 12345678910111213# Ëá™ÂÆö‰πâËäÇÁÇπÊï∞ÊçÆÁªìÊûÑclass treeNode: def __init__(self, nameValue, numOccur, parentNode): self.name self.count self.nodeLink # ÈìæË°®‰ø°ÊÅØÔºåÊåáÂêë‰∏ã‰∏Ä‰∏™Áõ∏‰ººÈ°π self.parent self.children # Áî®‰∫éÂ≠òÂÇ®ÂÖÉÁ¥†frequency‰ª•ÂèäÈìæÊé•Áõ∏‰ººÈ°πÁöÑÂ≠óÂÖ∏Êï∞ÊçÆÁªìÊûÑfreq = &#123;&#125;freq[node_name] = [frequency, node1, node2, ...] Âõ†‰∏∫ÈõÜÂêà‰∏≠ÂÖÉÁ¥†ÁöÑÂá∫Áé∞È¢ëÁéáÂèØËÉΩÁõ∏Á≠âÔºåÂõ†Ê≠§ËøáÊª§ÊéíÂ∫èÁöÑÁªìÊûú‰∏çÂîØ‰∏ÄÔºåÁîüÊàêÁöÑÊ†ëÁªìÊûÑ‰πü‰ºöÊúâÂ∑ÆÂºÇ„ÄÇ Á¨¨‰∏ÄÊ¨°ÈÅçÂéÜÂà†Èô§ÈùûÈ¢ëÁπÅÂÖÉÁ¥†Êó∂ÔºåÂèëÁé∞Â≠óÂÖ∏Âú®Ëø≠‰ª£ËøáÁ®ã‰∏≠‰∏çËÉΩÂà†Èô§itemÔºåÊàëËΩ¨ÂåñÊàêlistÊö¥ÂäõËß£ÂÜ≥‰∫ÜÔºå‰∏çÁü•ÈÅìÊúâÊ≤°Êúâ‰ªÄ‰πà‰ºòÈõÖÁöÑÊñπÂºè„ÄÇ 123456del freq[item]# ËøîÂõû RuntimeError: dictionary changed size during iterationfor item in list(freq.keys()): if freq[item] &lt; minSupport: del(freq[item]) ÊåñÊéòÈ¢ëÁπÅÈ°πÈõÜÔºöÈ¶ñÂÖàÂàõÂª∫Êù°‰ª∂Ê®°ÂºèÂü∫ÔºåÁÑ∂ÂêéÂà©Áî®Êù°‰ª∂Ê®°ÂºèÂü∫ÔºåÊûÑÂª∫Êù°‰ª∂FPÊ†ë„ÄÇ 1 Êù°‰ª∂Ê®°ÂºèÂü∫Ôºö‰ª•ÊâÄÊü•ÊâæÂÖÉÁ¥†È°π‰∏∫ÁªìÂ∞æÁöÑÂâçÁºÄË∑ØÂæÑÈõÜÂêàÔºåÂπ∂‰∏îÊØèÊù°ÂâçÁºÄË∑ØÂæÑÈÉΩ‰∏éËµ∑ÂßãÂÖÉÁ¥†È°πÁöÑËÆ°Êï∞ÂÄºÁõ∏ÂÖ≥ËÅî„ÄÇÔºàËøôÈáåÈù¢Áî®Âà∞‰∫ÜÂâçÈù¢ÂÆö‰πâÁöÑparentÂíånodeLinkÂ±ûÊÄßÔºâ 2 ÊûÑÈÄ†Êù°‰ª∂FPÊ†ëÔºö‰∏éÊûÑÈÄ†Ê†ëÁöÑËøáÁ®ãÁõ∏ÂêåÔºå‰ΩøÁî®ÁöÑdataSetÊç¢Êàê‰∫ÜÊù°‰ª∂Ê®°ÂºèÂü∫ËÄåÂ∑≤ÔºåÂáΩÊï∞ÂèÇÊï∞countÂ∞±ÊòØÈ¢ÑÁïôÂΩ©Ëõã„ÄÇËøôÊ†∑ÂæóÂà∞ÁöÑÂ∞±ÊòØÊåáÂÆöÈ¢ëÁπÅÈ°πÁöÑÊù°‰ª∂FPÊ†ë„ÄÇ 12345def updateTree(cond_set, myTree, freq_header, count):# cond_set: ‰∏ÄÊù°path# myTree: Ê†πËäÇÁÇπ# freq_header: dict[node_name] = [frequency, head_node]# count: pathÂØπÂ∫îÁöÑcount ÊûÑÈÄ†ÁöÑÊù°‰ª∂FPÊ†ëËøáÊª§Êéâ‰∫ÜÊù°‰ª∂Ê®°ÂºèÂü∫‰∏≠ÁöÑ‰∏Ä‰∫õÂÖÉÁ¥†ÔºöËøô‰∫õÂÖÉÁ¥†Êú¨Ë∫´ÊòØÈ¢ëÁπÅÈ°πÔºå‰ΩÜÊòØ‰∏éÊåáÂÆöÂÖÉÁ¥†ÁªÑÂêàÁöÑÈõÜÂêà‰∏çÊòØÈ¢ëÁπÅÁöÑ„ÄÇ Áõ∏Â∫îÂú∞ÔºåÊù°‰ª∂Ê†ë‰∏≠Ââ©‰ΩôÂÖÉÁ¥†‰∏éÊåáÂÆöÈ¢ëÁπÅÈ°πÁªÑÂêàÁöÑÈõÜÂêàÊòØÈ¢ëÁπÅÁöÑ„ÄÇ 3 Ëø≠‰ª£Ôºö‰ªéÁîüÊàêÁöÑÊù°‰ª∂FPÊ†ë‰∏≠ÔºåÂèØ‰ª•ÂæóÂà∞Êõ¥Â§çÊùÇÁöÑÈ¢ëÁπÅÈ°π„ÄÇÊ±ÇËß£Â§çÊùÇÈ¢ëÁπÅÈ°πÁöÑÊù°‰ª∂Ê®°ÂºèÂü∫ÔºåËøõËÄåÁîüÊàêÂØπÂ∫îÁöÑÊù°‰ª∂FPÊ†ëÔºåÂ∞±ËÉΩÂæóÂà∞Êõ¥Â§çÊùÇÁöÑÈ¢ëÁπÅÈ°πÔºå‰æùÊ¨°Á±ªÊé®ËøõË°åËø≠‰ª£ÔºåÁõ¥Âà∞FPÊ†ë‰∏∫Á©∫„ÄÇ ‚Äã]]></content>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sublimeÊ≥®ÂÜåÁ†ÅË¢´Êó†ÈôêÊ¨°ÁßªÈô§]]></title>
    <url>%2F2018%2F04%2F25%2Fsublime%E6%B3%A8%E5%86%8C%E7%A0%81%E8%A2%AB%E6%97%A0%E9%99%90%E6%AC%A1%E7%A7%BB%E9%99%A4%2F</url>
    <content type="text"><![CDATA[ÊúÄËøë‰∏çÁü•ÈÅìsublime3ÊäΩ‰ªÄ‰πàÈ£éÔºåÁ™ÅÁÑ∂ÂºÄÂßãÈ™åËØÅÊ≥®ÂÜåÁ†Å‰∫ÜÔºåËæìÂÖ•‰∏Ä‰∏™valid codeÂàÜÂàÜÈíüÁªô‰Ω†ÁßªÈô§„ÄÇ Êî∂Ëóè‰∏Ä‰∏™Ëß£ÂÜ≥ÂäûÊ≥ïÔºåÊúâÊïàÊÄßÂæÖÈ™åËØÅÔºö 1234# add the following to your host file(/private/etc/hosts)127.0.0.1 license.sublimehq.com127.0.0.1 45.55.255.55127.0.0.1 45.55.41.223]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VTKÁºñËØëÊä•Èîôno override found for vtkpolydatamapper]]></title>
    <url>%2F2018%2F04%2F12%2FVTK%E7%BC%96%E8%AF%91%E6%8A%A5%E9%94%99no-override-found-for-vtkpolydatamapper%2F</url>
    <content type="text"><![CDATA[Êä•ÈîôÂéüÂõ†ÊòØÈÄöËøáIDEÁºñËØëËÄå‰∏çÊòØÁõ¥Êé•ÈÄöËøácmakeÔºåÂõ†Ê≠§Ë¶ÅÊ∑ªÂä†Â¶Ç‰∏ã‰ª£Á†ÅÊÆµÔºö 123#include "vtkAutoInit.h" VTK_MODULE_INIT(vtkRenderingOpenGL2); VTK_MODULE_INIT(vtkInteractionStyle); ÂÖàËÆ∞ÂΩïËß£ÂÜ≥ÂäûÊ≥ïÔºåmore details ÁïôÂà∞‰ª•Âêé„ÄÇ Âü∫Á°ÄÊµãËØïÔºö 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include "vtkAutoInit.h"VTK_MODULE_INIT(vtkRenderingOpenGL2); // VTK was built with vtkRenderingOpenGL2VTK_MODULE_INIT(vtkInteractionStyle);#include &lt;vtkSphereSource.h&gt;#include &lt;vtkPolyData.h&gt;#include &lt;vtkSmartPointer.h&gt;#include &lt;vtkPolyDataMapper.h&gt;#include &lt;vtkActor.h&gt;#include &lt;vtkRenderWindow.h&gt;#include &lt;vtkRenderer.h&gt;#include &lt;vtkRenderWindowInteractor.h&gt;int main(int, char *[])&#123; // Create a sphere vtkSmartPointer&lt;vtkSphereSource&gt; sphereSource = vtkSmartPointer&lt;vtkSphereSource&gt;::New(); sphereSource-&gt;SetCenter(0.0, 0.0, 0.0); sphereSource-&gt;SetRadius(5.0); //mapper vtkSmartPointer&lt;vtkPolyDataMapper&gt; mapper = vtkSmartPointer&lt;vtkPolyDataMapper&gt;::New(); mapper-&gt;SetInputConnection(sphereSource-&gt;GetOutputPort()); //actor vtkSmartPointer&lt;vtkActor&gt; actor = vtkSmartPointer&lt;vtkActor&gt;::New(); actor-&gt;SetMapper(mapper); //renderer ,renderWindow, renderWindowInteractor. vtkSmartPointer&lt;vtkRenderer&gt; renderer = vtkSmartPointer&lt;vtkRenderer&gt;::New(); vtkSmartPointer&lt;vtkRenderWindow&gt; renderWindow = vtkSmartPointer&lt;vtkRenderWindow&gt;::New(); renderWindow-&gt;AddRenderer(renderer); vtkSmartPointer&lt;vtkRenderWindowInteractor&gt; renderWindowInteractor = vtkSmartPointer&lt;vtkRenderWindowInteractor&gt;::New(); renderWindowInteractor-&gt;SetRenderWindow(renderWindow); renderer-&gt;AddActor(actor); renderer-&gt;SetBackground(.3, .6, .3); // Background color green renderWindow-&gt;Render(); renderWindowInteractor-&gt;Start(); return EXIT_SUCCESS;&#125;]]></content>
      <tags>
        <tag>basic</tag>
      </tags>
  </entry>
</search>
