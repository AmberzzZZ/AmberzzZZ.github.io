<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>KL Divergence</title>
      <link href="/2021/09/27/KL-Divergence/"/>
      <url>/2021/09/27/KL-Divergence/</url>
      <content type="html"><![CDATA[<ol><li>KL divergenceç”¨äºåº¦é‡ä¸¤ä¸ªåˆ†å¸ƒPå’ŒQçš„å·®å¼‚ï¼Œè¿™ç§åº¦é‡ã€ä¸å…·æœ‰ã€‘å¯¹ç§°æ€§<ul><li>Pæ˜¯å®é™…åˆ†å¸ƒï¼ˆpred probsï¼‰</li><li>Qæ˜¯å»ºæ¨¡åˆ†å¸ƒï¼ˆgtï¼‰</li><li>$D_{KL}(P||Q)=\sum_i P(i)ln\frac{P(i)}{Q(i)}$</li><li>æ•£åº¦å®šä¹‰ä¸ºåˆ†å¸ƒPå’Œåˆ†å¸ƒQä¹‹é—´çš„å¯¹æ•°å·®å¼‚çš„åŠ æƒå’Œï¼Œç”¨Pçš„æ¦‚ç‡å»åŠ æƒ</li><li>å½“Qæ˜¯one-hot labelçš„æ—¶å€™ï¼Œè¦å…ˆclipå†log</li></ul></li><li><p>æ–¹æ³•</p><ul><li>torch.nn.functional.kl_div(input, target, size_average=None, reduce=None, reduction=â€™meanâ€™)<ul><li>inputï¼šå¯¹æ•°æ¦‚ç‡</li><li>targetï¼šæ¦‚ç‡</li></ul></li><li>tf.distributions.kl_divergence(distribution_a, distribution_b, allow_nan_stats=True, name=None)<ul><li>distribution_a&amp;b æ¥è‡ªtf.distributions.Categorical(logits=None, prob=None, â€¦)</li><li>ä¼ å…¥logits/probsï¼Œå…ˆè½¬æ¢æˆdistributionï¼Œå†è®¡ç®—kl divergence</li></ul></li><li>torch.nn.KLDivLoss</li><li>tf.keras.losses.KLDivergence</li><li>tf.keras.losses.kullback_leibler_divergence</li></ul></li><li><p>code</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># torch version</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KL</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, args)</span>:</span></span><br><span class="line">      super(KL, self).__init__()</span><br><span class="line">        self.T = args.temperature</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, logits_p, logits_q)</span>:</span></span><br><span class="line">      log_p = F.log_softmax(logits_p/self.T, dim=<span class="number">1</span>)</span><br><span class="line">        q = F.softmax(logits_q/self.T, dim=<span class="number">1</span>)</span><br><span class="line">        loss = F.kl_div(log_p, p_t)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="comment"># keras version</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kl_div</span><span class="params">(logits_p, logits_q)</span>:</span></span><br><span class="line">  T = <span class="number">4.</span></span><br><span class="line">    log_p = tf.nn.log_softmax(logits_p/T)      <span class="comment"># (b,cls)</span></span><br><span class="line">    log_q = tf.nn.log_softmax(logits_q/T)</span><br><span class="line">    p = K.exp(log_p)</span><br><span class="line">    <span class="keyword">return</span> K.sum(p*(log_p-log_q), axis=<span class="number">-1</span>)   <span class="comment"># (b,)</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> æ•°å­¦ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Self-Knowledge Distillation</title>
      <link href="/2021/09/17/Self-Knowledge-Distillation/"/>
      <url>/2021/09/17/Self-Knowledge-Distillation/</url>
      <content type="html"><![CDATA[<h2 id="Refine-Myself-by-Teaching-Myself-Feature-Refinement-via-Self-Knowledge-Distillation"><a href="#Refine-Myself-by-Teaching-Myself-Feature-Refinement-via-Self-Knowledge-Distillation" class="headerlink" title="Refine Myself by Teaching Myself : Feature Refinement via Self-Knowledge Distillation"></a>Refine Myself by Teaching Myself : Feature Refinement via Self-Knowledge Distillation</h2><ol><li><p>åŠ¨æœº</p><ul><li>ä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦<ul><li>by stageï¼šå…ˆè®­ç»ƒåºå¤§çš„teacher</li></ul></li><li>self knowledge distillation<ul><li>without the pretrained network</li><li>åˆ†ä¸ºdata augmentation based approach å’Œ auxiliary network based approach</li><li>data augmentation approachå¦‚UDAï¼Œé€šè¿‡ç›‘ç£åŸå§‹å›¾åƒå’Œå¢å¼ºå›¾åƒçš„ä¸€è‡´æ€§ï¼Œä½†æ˜¯ä¼šloose local informationï¼Œå¯¹pixel-level tasksä¸å‹å¥½ï¼Œè€Œä¸”ç›‘ç£ä¿¡æ¯æ˜¯ä»logitså±‚ï¼Œæ²¡æœ‰ç›´æ¥å»refine feature maps</li></ul></li><li>our approach FRSKD<ul><li>auxiliary network based approach</li><li>utilize both soft label and featuremap distillation</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>various distillation methods</p><p>  <img src="/2021/09/17/Self-Knowledge-Distillation/methods.png" width="75%;"></p><ul><li>aæ˜¯ä¼ ç»ŸçŸ¥è¯†è’¸é¦ï¼Œæ·±ç»¿è‰²æ˜¯pretrained teacherï¼Œæµ…ç»¿è‰²æ˜¯studentï¼Œæ©™è‰²ç®­å¤´æ˜¯featureè’¸é¦ï¼Œç»¿è‰²ç®­å¤´æ˜¯soft labelè’¸é¦</li><li>bæ˜¯data augmentation based è‡ªè’¸é¦ï¼Œshared ç½‘ç»œï¼ŒåŸå›¾å’Œå¢å¼ºåçš„å›¾ï¼Œç”¨soft logitsæ¥è’¸é¦</li><li>cæ˜¯auxiliary classifier based è‡ªè’¸é¦ï¼Œcascadedåˆ†ç±»å¤´ï¼Œæ¯ä¸ªåˆ†ç±»å™¨éƒ½æ¥å‰ä¸€ä¸ªçš„</li><li>dæ˜¯æœ¬æ–‡è‡ªè’¸é¦ï¼Œå’Œcæœ€å¤§çš„ä¸åŒæ˜¯bifpnç»“æ„ä½¿å¾—ä¸¤ä¸ªåˆ†ç±»å™¨æ¯ä¸ªlevelçš„ç‰¹å¾å›¾ä¹‹é—´éƒ½æœ‰è¿ç»“ï¼Œç›‘ç£æ–¹å¼ä¸€æ ·çš„</li></ul></li><li><p>FPN</p><ul><li>PANetï¼šä¸Šè¡Œ+ä¸‹è¡Œ</li><li>biFPNï¼šä¸Šè¡Œ+ä¸‹è¡Œ+åŒå±‚çº§è”</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>overview</p><ul><li><p><img src="/2021/09/17/Self-Knowledge-Distillation/overview.png" width="80%;"></p></li><li><p>notations</p><ul><li>dataset $D=\{(x_1,y_1), (x_2,y_2),â€¦, (x_N,y_N)\}$</li><li>feature map $F_{i,j}$ï¼Œi-th sampleï¼Œj-th block</li><li>channel dimension $c_j$ï¼Œj-th block</li></ul></li></ul></li><li><p>self-teacher network</p><ul><li>self-teacher networkçš„ç›®çš„æ˜¯æä¾›refined feature mapå’Œsoft labelsä½œä¸ºç›‘ç£ä¿¡æ¯</li><li>inputsï¼šfeature maps $F_1, F_2, â€¦, F_n$ï¼Œä¹Ÿå°±æ˜¯è¯´teacheråœ¨è¿›è¡Œæ¢¯åº¦å›ä¼ çš„æ—¶å€™åˆ°Få°±åœæ­¢äº†ï¼Œä¸ä¼šæ›´æ–°student modelçš„å‚æ•°</li><li>modified biFPN<ul><li>ç¬¬ä¸€ä¸ªä¸åŒï¼šåˆ«çš„FPNéƒ½æ˜¯åœ¨fuseä¹‹å‰å…ˆç”¨ä¸€ä¸ªfixed-dim 1x1 convå°†æ‰€æœ‰levelçš„feature mapè½¬æ¢æˆç›¸åŒé€šé“æ•°ï¼ˆå¦‚256ï¼‰ï¼Œwe design $d_i$ according to $c_i$ï¼Œå¼•å…¥ä¸€ä¸ªå®½åº¦ç³»æ•°widthï¼Œ$d_i=width*c_i$ï¼Œ</li><li>ç¬¬äºŒä¸ªä¸åŒï¼šä½¿ç”¨depth-wise convolution</li><li>notations<ul><li>BiFPNï¼šæ¯å±‚dimå›ºå®šçš„ç‰ˆæœ¬</li><li>BiFPNcï¼šæ¯å±‚diméšè¾“å…¥å˜åŒ–çš„ç‰ˆæœ¬</li></ul></li></ul></li></ul></li><li><p>self-feature distillation</p><ul><li>feature distillation<ul><li>adapt attention transfer</li><li>å¯¹feature mapå…ˆè¿›è¡Œchannel-wiseçš„poolingï¼Œç„¶åL2 normï¼Œæå–spatial information</li><li><img src="/2021/09/17/Self-Knowledge-Distillation/feature.png" width="40%;"></li></ul></li><li>soft label distillation<ul><li>ä¸¤ä¸ªåˆ†ç±»å¤´çš„KL divergence</li><li><img src="/2021/09/17/Self-Knowledge-Distillation/label.png" width="40%;"></li></ul></li><li>CE with gt<ul><li>ä¸¤ä¸ªåˆ†ç±»å¤´åˆ†åˆ«è¿˜æœ‰æ­£å¸¸çš„CE loss</li></ul></li><li>overall<ul><li>æ€»çš„lossæ˜¯4ä¸ªlossç›¸åŠ ï¼š$L_{FRSKD}(x,y,\theta_c, \theta_t, K)=L_{CE}(x,y,\theta_c)+L_{CE}(x,y,\theta_t)+\alpha L_{KD}(x,\theta_c,\theta_t, K) + \beta L_{F}(T,F,\theta_c,\theta_T)$</li><li>$\alpha \in [1,2,3]$</li><li>$\beta \in [100,200]$</li><li>ã€QUESTIONã€‘FRSKD updates the parameters by the distillation lossï¼Œ$L_{KD}$ and $L_F$ï¼Œwhich is only applied to the student networkï¼Œè¿™ä¸ªå•¥æ„æ€æš‚æ—¶æ²¡ç†è§£</li></ul></li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li>experiment settings<ul><li>FRSKD\Fï¼šåªåšsoft labelçš„ç›‘ç£ï¼Œä¸åšfeature mapçš„ç›‘ç£</li><li>FRSKDï¼šæ ‡å‡†çš„æœ¬æ–‡æ–¹æ³•</li><li>FRSKD+SLAï¼šæœ¬æ–‡æ–¹æ³•çš„åŸºç¡€ä¸Šattach data augmentation based distillation</li></ul></li><li></li></ul></li></ol>]]></content>
      
      
    </entry>
    
    <entry>
      <title>L2 Regularization and Batch Norm</title>
      <link href="/2021/09/16/L2-Regularization-and-Batch-Norm/"/>
      <url>/2021/09/16/L2-Regularization-and-Batch-Norm/</url>
      <content type="html"><![CDATA[<p>referenceï¼š</p><p><a href="https://blog.janestreet.com/l2-regularization-and-batch-norm/" target="_blank" rel="noopener">https://blog.janestreet.com/l2-regularization-and-batch-norm/</a></p><p><a href="https://zhuanlan.zhihu.com/p/56142484" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/56142484</a></p><p><a href="https://vitalab.github.io/article/2020/01/24/L2-reg-vs-BN.html" target="_blank" rel="noopener">https://vitalab.github.io/article/2020/01/24/L2-reg-vs-BN.html</a></p><p>è§£é‡Šäº†ä¹‹å‰çš„ä¸€ä¸ªç–‘ç‚¹ï¼š</p><ul><li>åœ¨kerasè‡ªå®šä¹‰çš„BNå±‚ä¸­ï¼Œæ²¡æœ‰ç±»ä¼¼kernel_regularizerè¿™æ ·çš„å‚æ•°</li><li>åœ¨æˆ‘ä»¬å†™è‡ªå®šä¹‰optmizerçš„æ—¶å€™ï¼ŒBNå±‚ä¹Ÿä¸è¿›è¡Œweight decayçš„</li></ul><h2 id="L2-Regularization-versus-Batch-and-Weight-Normalization"><a href="#L2-Regularization-versus-Batch-and-Weight-Normalization" class="headerlink" title="L2 Regularization versus Batch and Weight Normalization"></a>L2 Regularization versus Batch and Weight Normalization</h2><ol><li><p>åŠ¨æœº</p><ul><li>ä¸¤ä¸ªcommon tricksï¼šNormalizationï¼ˆBNã€WNã€LNç­‰ï¼‰å’ŒL2 Regularization<ul><li>å‘ç°ä¸¤è€…ç»“åˆæ—¶L2 regularizationå¯¹normalizationå±‚æ²¡æœ‰æ­£åˆ™æ•ˆæœ</li><li>L2 regularizationåè€Œå¯¹norm layerçš„scaleæœ‰å½±å“ï¼Œé—´æ¥å½±å“äº†learning rate</li><li>ç°ä»£ä¼˜åŒ–å™¨å¦‚Adamåªèƒ½é—´æ¥æ¶ˆé™¤è¿™ç§å½±å“</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>BN</p><ul><li>popular in training deep networks</li><li>solve the problem of covariate shift</li><li>ä½¿å¾—æ¯ä¸ªç¥ç»å…ƒçš„è¾“å…¥ä¿æŒnormalåˆ†å¸ƒï¼ŒåŠ é€Ÿè®­ç»ƒ</li><li>mean &amp; varianceï¼štraining timeåŸºäºæ¯ä¸ªmini-batchè®¡ç®—ï¼Œtest timeä½¿ç”¨æ‰€æœ‰iterationçš„mean &amp; varianceçš„EMA</li></ul></li><li><p>usually trained with SGD with L2 regularization</p><ul><li><p>result in weight decayï¼šä»æ•°å­¦è¡¨ç¤ºä¸Šç­‰ä»·äºå¯¹æƒé‡åšè¡°å‡</p></li><li><p>æ¯ä¸€æ­¥æƒé‡scaled by a å°äº1çš„æ•°</p></li><li>ä½†æ˜¯normalization strategiesæ˜¯å¯¹scale of the weights invariantçš„ï¼Œå› ä¸ºåœ¨è¾“å…¥ç¥ç»å…ƒä¹‹å‰éƒ½ä¼šè¿›è¡Œnorm</li><li>therefore<ul><li>there is no regularizing effect</li><li>rather strongly influence the learning rate??ğŸ‘‚</li></ul></li></ul></li></ul></li><li><p>L2 Regularization </p><ul><li><p>formulationï¼š</p><ul><li>åœ¨lossçš„åŸºç¡€ä¸ŠåŠ ä¸€ä¸ªregularization termï¼Œ$L_{\lambda}(w)=L(w)+\lambda ||w||^2_2$</li><li>lossæ˜¯æ¯ä¸ªæ ·æœ¬ç»è¿‡ä¸€ç³»åˆ—æƒé‡è¿ç®—ï¼Œ$L(w)=\sum_N l_i (y(X_i;w,\gamma,\beta))$</li><li>å½“ä½¿ç”¨normalization layerçš„æ—¶å€™ï¼š$y(X_i;w,\alpha,\beta)=y(X_i;\alpha w,\gamma,\beta)$ï¼Œå³loss termä¸ä¼šå˜</li><li>$L_{\lambda}(\alpha w)=L(w)+\lambda \alpha^2||w||^2_2$</li><li>åœ¨æœ‰normalization layerçš„æ—¶å€™ï¼ŒL2 penaltyè¿˜æ˜¯èƒ½å¤Ÿé€šè¿‡reg term forceæƒé‡çš„scaleè¶Šæ¥è¶Šå°ï¼Œä½†æ˜¯ä¸ä¼šå½±å“ä¼˜åŒ–è¿›ç¨‹ï¼ˆä¸å½±å“main objective valueï¼‰ï¼Œå› ä¸ºloss termä¸å˜</li></ul></li><li><p>Effect of the Scale of Weights on Learning Rate</p><ul><li>BNå±‚çš„è¾“å‡ºæ˜¯scale invariantçš„ï¼Œä½†æ˜¯æ¢¯åº¦ä¸æ˜¯ï¼Œæ¢¯åº¦æ˜¯æˆåæ¯”è¢«æŠ‘åˆ¶çš„ï¼</li><li><p>æ‰€ä»¥weightsåœ¨å˜å°ï¼ŒåŒæ—¶æ¢¯åº¦åœ¨å˜å¤§ï¼</p><p><img src="/2021/09/16/L2-Regularization-and-Batch-Norm/lr.png" width="70%;"></p></li><li><p>è¿™ä¹Ÿå°±ç›¸å½“äºå­¦ä¹ ç‡åœ¨å˜å¤§ï¼Œè¿™ä¸æˆ‘ä»¬çš„åˆè¡·ç›¸æ‚–â€”â€”æˆ‘ä»¬å¸Œæœ›æ›´ç¨³å®šçš„æ¨¡å‹</p></li><li>æ‰€ä»¥åœ¨è®¾å®šhyperçš„æ—¶å€™ï¼Œå¦‚æœæˆ‘ä»¬è¦é€‚å½“åŠ å¤§weight decay $\lambda$ï¼Œå°±è¦åæ¯”scaleå­¦ä¹ ç‡</li><li></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> æ­£åˆ™åŒ– </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SAM</title>
      <link href="/2021/09/10/SAM/"/>
      <url>/2021/09/10/SAM/</url>
      <content type="html"><![CDATA[<p>google brainï¼Œå¼•ç”¨é‡51ï¼Œä½†æ˜¯ImageNetæ¦œå•/SOTAæ¨¡å‹çš„å¯¹æ¯”å®éªŒé‡Œé¢ç»å¸¸èƒ½å¤Ÿçœ‹åˆ°è¿™ä¸ªSAMï¼Œå‡ºåœˆå½¢å¼ä¸ºåˆ†ç±»æ¨¡å‹+SAM</p><p>SAMï¼šSharpness-Aware Minimizationï¼Œé”åº¦æ„ŸçŸ¥æœ€å°åŒ–</p><p>official repoï¼š<a href="https://github.com/google-research/sam" target="_blank" rel="noopener">https://github.com/google-research/sam</a></p><h2 id="Sharpness-Aware-Minimization-for-Efficiently-Improving-Generalization"><a href="#Sharpness-Aware-Minimization-for-Efficiently-Improving-Generalization" class="headerlink" title="Sharpness-Aware Minimization for Efficiently Improving Generalization"></a>Sharpness-Aware Minimization for Efficiently Improving Generalization</h2><ol><li><p>åŠ¨æœº</p><ul><li>heavily overparametered modelsï¼štraining lossèƒ½è®­åˆ°æå°ï¼Œä½†æ˜¯generalization issue</li><li>we propose<ul><li>Sharpness-Aware Minimization (SAM)</li><li>åŒæ—¶æœ€å°åŒ–losså’Œloss sharpness</li><li>improve model generalization</li><li>robustness to label noise</li></ul></li><li>verified on<ul><li>CIFAR 10&amp;100</li><li>ImageNet</li><li>finetuning tasks</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>typical loss &amp; optimizer</p><ul><li>population lossï¼šæˆ‘ä»¬å®é™…æƒ³å¾—åˆ°çš„æ˜¯åœ¨å½“å‰è®­ç»ƒé›†æ‰€ä»£è¡¨çš„åˆ†å¸ƒä¸‹çš„æœ€ä¼˜è§£</li><li><p>training set lossï¼šä½†äº‹å®ä¸Šæˆ‘ä»¬åªèƒ½ç”¨æ‰€æœ‰çš„è®­ç»ƒæ ·æœ¬æ¥ä»£è¡¨è¿™ä¸ªåˆ†å¸ƒ</p><p><img src="/2021/09/10/SAM/loss.png" width="80%;"></p></li><li><p>å› ä¸ºlosså‡½æ•°æ˜¯non-convexçš„ï¼Œæ‰€ä»¥å¯èƒ½å­˜åœ¨å¤šä¸ªlocal even global minimaå¯¹åº”çš„loss valueæ˜¯ä¸€æ ·çš„ï¼Œä½†æ˜¯generalization performanceç¡®æ˜¯ä¸åŒçš„</p></li></ul></li><li><p>æˆç†Ÿçš„å…¨å¥—é˜²æ­¢è¿‡æ‹Ÿåˆæ‰‹æ®µ</p><ul><li>loss</li><li>optimizer</li><li>dropout</li><li>batch normalization</li><li>mixed sample augmentations</li></ul></li><li>our approach<ul><li>directly leverage the geometry of the loss landscape</li><li>and its connection to generalization (generalization bound)</li><li>proved additive to existing techniques</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>motivation</p><ul><li>rather than å¯»æ‰¾ä¸€ä¸ªweight value that have low lossï¼Œæˆ‘ä»¬å¯»æ‰¾çš„æ˜¯é‚£ç§è¿å¸¦ä»–ä¸´è¿‘çš„valueéƒ½èƒ½æœ‰low lossçš„value</li><li>ä¹Ÿå°±æ˜¯æ—¢æœ‰low lossåˆæœ‰lowæ›²åº¦</li></ul></li><li><p>sharpness term</p><ul><li>$\max \limits_{||\epsilon||_p &lt; \rho} L_s(w+\epsilon) - L_s(w)$</li><li>è¡¡é‡æ¨¡å‹åœ¨wå¤„çš„sharpness</li></ul></li><li><p>Sharpness-Aware Minimization (SAM) formulation</p><ul><li>sharpness termå†åŠ ä¸Štrain losså†åŠ ä¸Šregularization term</li><li>$L_S^{SAM}(w)=\max\limits_{a} L_s(w+\epsilon)$</li><li>$\min \limits_{w} L_S^{SAM}(w) + \lambda ||w||^2_2$</li><li>prevent the model from converting to a sharp minimum</li></ul></li><li><p>effective approximation</p><ul><li><p>bound</p><ul><li>with $\frac{1}{p} + \frac{1}{q} = 1$</li></ul><p><img src="/2021/09/10/SAM/bound.png" width="80%;"></p></li><li><p>approximation</p><p><img src="/2021/09/10/SAM/approximation.png" width="80%;"></p></li></ul></li><li><p>pseudo code</p><p>  <img src="/2021/09/10/SAM/algorithm.png" width="80%;"></p><ul><li>given a min-batch</li><li>é¦–å…ˆè®¡ç®—å½“å‰batchçš„training lossï¼Œå’Œå½“å‰æ¢¯åº¦ï¼Œ$w_t$ to $w_{t+1}$</li><li>ç„¶åè®¡ç®—è¿‘ä¼¼ä¸ºæ¢¯åº¦normçš„æ­¥é•¿$\hat\epsilon(w)$ï¼Œequation2ï¼Œ$w_t$ to $w_{adv}$ï¼Œè¿™é‡Œé¢çš„advè”åŠ¨äº†å¦ä¸€ç¯‡è®ºæ–‡ã€ŠAdvProp: Adversarial Examples Improve Image Recognitionã€‹</li><li>ç„¶åè®¡ç®—è¿‘ä¼¼çš„sharpness termï¼Œå¯ä»¥ç†è§£ä¸ºtraining lossåœ¨wé‚»å±…å¤„çš„æ¢¯åº¦ï¼Œequation3ï¼Œåº”è¯¥æ˜¯è“è‰²ç®­å¤´çš„åæ–¹å‘ï¼Œå›¾ä¸Šæ²¡æ ‡è®°å‡ºæ¥</li><li>ç”¨wé‚»å±…çš„æ¢¯åº¦æ¥æ›´æ–°wçš„æƒé‡ï¼Œç”¨è´Ÿæ¢¯åº¦ï¼ˆè“è‰²ç®­å¤´ï¼‰</li><li>overllå°±æ˜¯ï¼šè¦å‘å‰èµ°ä¹‹å‰ï¼Œå…ˆå›é€€ï¼Œç¼ºç‚¹æ˜¯ä¸¤æ¬¡æ¢¯åº¦è®¡ç®—ï¼Œæ—¶é—´double</li></ul></li></ul></li><li><p>å®éªŒç»“è®º</p><ul><li><p>èƒ½ä¼˜åŒ–åˆ°æŸå¤±çš„æœ€å¹³å¦çš„æœ€å°å€¼çš„åœ°æ–¹ï¼Œå¢å¼ºæ³›åŒ–èƒ½åŠ›</p><p>  <img src="/2021/09/10/SAM/minimum.png" width="80%;"></p></li></ul></li></ol>]]></content>
      
      
    </entry>
    
    <entry>
      <title>MuSTè°·æ­Œå¤šä»»åŠ¡è‡ªè®­ç»ƒ</title>
      <link href="/2021/09/01/MuST/"/>
      <url>/2021/09/01/MuST/</url>
      <content type="html"><![CDATA[<ul><li><p>recollect</p><p>  [SimCLR]</p><p>  [MoCo]</p></li></ul><h2 id="Multi-Task-Self-Training-for-Learning-General-Representations"><a href="#Multi-Task-Self-Training-for-Learning-General-Representations" class="headerlink" title="Multi-Task Self-Training for Learning General Representations"></a>Multi-Task Self-Training for Learning General Representations</h2><ol><li><p>åŠ¨æœº</p><ul><li>learning general feature representations</li><li>expect a single general model<ul><li>ç›¸æ¯”è¾ƒäºtraining specialized models for various tasks</li><li>harness from independent specialized teacher models</li><li>with a multi-task pseudo dataset</li><li>trained with multi-task learning</li></ul></li><li>evalutate on 6 vision tasks<ul><li>image recognition (classification, detection, segmentation)</li><li>3D geometry estimation</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>pretraining &amp; transfer learning</p><ul><li>transformerä¸€èˆ¬éƒ½æ˜¯è¿™ä¸ªå¥—è·¯ï¼ŒBiT&amp;ViT</li><li>pretraining<ul><li>supervised / unsupervised</li><li>learn feature representations</li></ul></li><li>transfer learning<ul><li>on downstream tasks</li><li>the features may not necessarily be useful</li><li>æœ€å…¸å‹çš„å°±æ˜¯ImageNet pre-trainingå¹¶ä¸èƒ½improve COCO segmentationï¼Œä½†æ˜¯Objects365èƒ½å¤Ÿå¤§å¹…æå‡</li></ul></li><li>pretraining taskså¿…é¡»è¦å’Œdownstream task alignï¼Œlearn specialized featuresï¼Œä¸ç„¶ç™½è´¹</li></ul></li><li><p>learning general features</p><ul><li>a model simultaneously do well on multiple tasks</li><li>NLPçš„bertæ˜¯ä¸€ä¸ªå…¸å‹ç”¨å¤šä»»åŠ¡æå‡general abilityçš„</li><li>CVæ¯”è¾ƒéš¾è¿™æ ·åšæ˜¯å› ä¸ºæ ‡ç­¾varietyï¼Œæ²¡æœ‰è¿™æ ·çš„å¤§å‹multi-task dataset</li></ul></li><li><p>multi-task learning</p><ul><li>shared backbone (å¦‚ResNet-FPN)</li><li>small task-specific heads</li></ul></li><li><p>self-training</p><ul><li>use a supervised model to generate pseudo labels on unlabeled data</li><li>then a student model is trained on the pseudo labeled data</li><li>åœ¨å„ç±»ä»»åŠ¡ä¸Šéƒ½provedæ¶¨ç‚¹</li><li>ä½†æ˜¯è¿„ä»Šä¸ºæ­¢éƒ½æ˜¯focused on a single task</li></ul></li><li><p>in this work</p><ul><li><p>lack of large scale multi-task datasetçš„issueï¼Œé€šè¿‡self-training to fixï¼Œç”¨pseudo label</p></li><li><p>specialized/general issueï¼Œé€šè¿‡å¤šä»»åŠ¡ï¼Œè®­ç»ƒç›®æ ‡å°±æ˜¯å…­è¾¹å½¢æˆ˜å£«ï¼Œabsorb the knowledge of different tasks in the shared backbone</p></li><li><p>three steps</p><ul><li>trains specialized teachers independently on labeled datasets ï¼ˆåˆ†ç±»ã€åˆ†å‰²ã€æ£€æµ‹ã€æ·±åº¦ä¼°è®¡ï¼‰</li><li>the specialized teachers are then used to label a larger unlabeled datasetï¼ˆImageNetï¼‰ to create a multi- task pseudo labeled dataset</li><li><p>train a student model with multi-task learning</p><p><img src="/2021/09/01/MuST/MuST.png" width="50%;"></p></li></ul></li><li><p>MuSTçš„ç‰¹è´¨</p><ul><li>improve with more unlabeled dataï¼Œæ•°æ®è¶Šå¤šgeneral featureè¶Šå¥½</li><li>can improve upon already strong checkpointsï¼Œåœ¨æµ·é‡ç›‘ç£é«˜ç²¾åº¦æ¨¡å‹åŸºç¡€ä¸Šfine-tuneï¼Œä»æ—§èƒ½åœ¨downstream tasksæ¶¨ç‚¹</li></ul></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>Specialized Teacher Models</p><ul><li>4 teacher models<ul><li>classificationï¼štrain from scratchï¼ŒImageNet</li><li>detectionï¼štrain from scratchï¼ŒObject365</li><li>segmentationï¼štrain from scratchï¼ŒCOCO</li><li>depth estimationï¼šfine-tuning from pre-trained checkpoint</li></ul></li><li>pseudo labeling<ul><li>unlabeled / partially labeled datasets</li><li>for detectionï¼šhard score threshold of 0.5</li><li>for segmentationï¼šhard score threshold of 0.5</li><li>for classificationï¼šsoft labelsâ€”â€”probs distribution</li><li>for depthï¼šç›´æ¥ç”¨</li></ul></li></ul></li><li><p>Multi-Task Student Model</p><ul><li><p>æ¨¡å‹ç»“æ„</p><p>  <img src="/2021/09/01/MuST/student.png" width="50%;"></p><ul><li>shared back<ul><li>C5ï¼šfor classification</li><li>feature pyramids {P3,P4,P5,P6,P7}ï¼šfor detection</li><li>fused P2ï¼šfor pixel-wise predictionï¼ŒæŠŠfeature pyramids rescaleåˆ°level2ç„¶åsum</li></ul></li><li>heads<ul><li>classification headï¼šResNet designï¼ŒGAP C5 + çº¿æ€§å±‚</li><li>object detection taskï¼šMask R-CNN designï¼ŒRPNæ˜¯2 hidden convsï¼ŒFast R-CNNæ˜¯4 hidden convs + 1 fc</li><li>pixel-wise prediction headsï¼š3 hiddent convs + 1 linear conv headï¼Œåˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ä»»åŠ¡independentï¼Œä¸share heads</li></ul></li></ul></li><li><p>Teacher-student training</p><ul><li>using the same architecture </li><li>same data augmentation</li><li>teacherå’Œstudentçš„main differenceå°±æ˜¯datasetå’Œlabels</li></ul></li><li><p>Learning From Multiple Teachers</p><ul><li>every image has supervision for all tasks</li><li>labels may come from supervised or pseudo labels</li><li>å¦‚æœä½¿ç”¨ImageNetæ•°æ®é›†ï¼Œclassificationå°±æ˜¯çœŸæ ‡ç­¾ï¼Œdet/seg/depth supervisionåˆ™æ˜¯ä¼ªæ ‡ç­¾</li><li>balance the loss contribution<ul><li>åŠ æƒå’Œï¼Œtask-specific weights</li><li>for ImageNetï¼Œuse $w_i = \frac{b_slr_{it}}{b_{it}lr_{s}}$</li><li>follow the scaling ruleï¼šlrå’Œbatch sizeæˆæ­£æ¯”</li><li>except for depth loss</li></ul></li></ul></li><li><p>Cross Dataset Training</p><ul><li>training across ImageNet, object365 and COCO</li><li>æœ‰æ ‡ç­¾çš„å°±ç”¨åŸæ ‡ç­¾ï¼Œæ²¡æœ‰çš„ç”¨ä¼ªæ ‡ç­¾ï¼Œsupervised labels and pseudo labels are treated equallyï¼Œè€Œä¸æ˜¯åˆ†åˆ«é‡‡æ ·å’Œè®­ç»ƒ</li><li>balance the datasetsï¼šåˆåœ¨ä¸€èµ·ç„¶åå‡åŒ€é‡‡æ ·</li></ul></li><li><p>Transfer Learning</p><ul><li>å¾—åˆ°general student modelä»¥åï¼Œfine-tune on ä¸€ç³»åˆ—downstream tasks</li><li>è¿™äº›downstream datasetsä¸MuST modelçš„è®­ç»ƒæ•°æ®éƒ½æ˜¯not alignçš„</li><li>è¿™ä¸ªå®éªŒè¦è¯æ˜çš„æ˜¯supervised modelï¼ˆå¦‚teacher modelï¼‰å’Œself-supervised modelï¼ˆå¦‚ç”¨pseudo labelè®­ç»ƒå‡ºæ¥çš„student modelï¼‰ï¼Œåœ¨downstream tasksä¸Šè¿ç§»å­¦ä¹ èƒ½performanceæ˜¯å·®ä¸å¤šçš„ï¼Œã€æ³¨æ„âš ï¸ï¼šå¦‚æœè¿ç§»datasetså‰åalignå°±ä¸æ˜¯è¿™æ ·äº†ï¼Œpretrainæ˜¾ç„¶ä¼šæ›´å¥½ï¼ï¼ï¼ã€‘</li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> multi-taskï¼Œself-trainingï¼Œ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>GHM</title>
      <link href="/2021/08/31/GHM/"/>
      <url>/2021/08/31/GHM/</url>
      <content type="html"><![CDATA[<p>families:</p><ul><li>[class-imbalanced CE]</li><li>[focal loss]</li><li>[generalized focal loss] focal loss(CE)çš„è¿ç»­ç‰ˆæœ¬</li><li>[ohem]</li></ul><p>keras implementation:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weightedCE_loss</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">alpha = <span class="number">.8</span></span><br><span class="line">pt = K.abs(y_true-y_pred)</span><br><span class="line"><span class="comment"># clip</span></span><br><span class="line">    pt = K.clip(pt, K.epsilon(), <span class="number">1</span>-K.epsilon())</span><br><span class="line">    <span class="comment"># ce</span></span><br><span class="line">    ce = -K.log(<span class="number">1.</span>-pt)</span><br><span class="line">    <span class="comment"># pos/neg reweight</span></span><br><span class="line">    wce = tf.where(y_true&gt;<span class="number">0.5</span>, alpha* , (<span class="number">1</span>-alpha)* )</span><br><span class="line">    <span class="keyword">return</span> wce</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">focal_loss</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">alpha = <span class="number">.25</span></span><br><span class="line">gamma = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">pt = K.abs(y_true-y_pred)</span><br><span class="line"><span class="comment"># clip</span></span><br><span class="line">    pt = K.clip(pt, K.epsilon(), <span class="number">1</span>-K.epsilon())</span><br><span class="line">    <span class="comment"># easy/hard reweight</span></span><br><span class="line">fl = -K.pow(pt, gamma) * K.log(<span class="number">1.</span>-pt)</span><br><span class="line">    <span class="comment"># pos/neg reweight</span></span><br><span class="line">    fl = tf.where(y_true&gt;<span class="number">0.5</span>, alpha*fl, (<span class="number">1</span>-alpha)*fl)</span><br><span class="line">    <span class="keyword">return</span> fl</span><br><span class="line">  </span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">generalized_focal_loss</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line"><span class="comment"># CE = -ytlog(yp)-(1-yt)log(1-yp)</span></span><br><span class="line">  <span class="comment"># GFL = |yt-yp|^beta * CE</span></span><br><span class="line">    beta = <span class="number">2</span></span><br><span class="line">    <span class="comment"># clip y_pred</span></span><br><span class="line">    y_pred = K.clip(y_pred, K.epsilon(), <span class="number">1</span>-K.epsilon())</span><br><span class="line">    <span class="comment"># ce</span></span><br><span class="line">    ce = -y_true*K.log(y_pred) - (<span class="number">1</span>-y_true)*K.log(<span class="number">1</span>-y_pred)   <span class="comment"># [N,C]</span></span><br><span class="line">    <span class="comment"># easy/hard reweight</span></span><br><span class="line">    gfl = K.pow(K.abs(y_true-y_pred), beta) * ce</span><br><span class="line">    <span class="keyword">return</span> gfl</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ce_ohem</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">  pt = K.abs(y_true-y_pred)</span><br><span class="line"><span class="comment"># clip</span></span><br><span class="line">    pt = K.clip(pt, K.epsilon(), <span class="number">1</span>-K.epsilon())</span><br><span class="line">    <span class="comment"># ce</span></span><br><span class="line">    ce = -K.log(<span class="number">1.</span>-pt)</span><br><span class="line">    <span class="comment"># sort loss</span></span><br><span class="line">    k = <span class="number">50</span></span><br><span class="line">    ohem_loss, indices = tf.nn.top_k(ce, k=k)   <span class="comment"># topk loss: [k,], topk indices: [k,], idx among 0-b</span></span><br><span class="line">    mask = tf.where(ce&gt;=ohem_loss[k<span class="number">-1</span>], tf.ones_like(ce), tf.zeros_like(ce))</span><br><span class="line">    <span class="keyword">return</span> mask*ce</span><br></pre></td></tr></table></figure><h2 id="Gradient-Harmonized-Single-stage-Detector"><a href="#Gradient-Harmonized-Single-stage-Detector" class="headerlink" title="Gradient Harmonized Single-stage Detector"></a>Gradient Harmonized Single-stage Detector</h2><ol><li><p>åŠ¨æœº</p><ul><li>one-stage detector<ul><li>æ ¸å¿ƒchallengeå°±æ˜¯imbalance issue</li><li>imbalance between positives and negatives</li><li>imbalance between easy and hard examples</li><li>è¿™ä¸¤é¡¹éƒ½èƒ½å½’ç»“ä¸ºå¯¹æ¢¯åº¦çš„ä½œç”¨ï¼ša term of the gradient</li></ul></li><li>we propose a novel gradient harmonizing mechanism (GHM) <ul><li>balance the gradient flow</li><li>easy to embed in cls/reg losses like CE/smoothL1</li><li>GHM-C for anchor classification</li><li>GHM-R for bounding box refinement</li></ul></li><li>proved substantial improvement on COCO<ul><li>41.6 mAP</li><li>surpass FL by 0.8</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>imbalance issue</p><ul><li><p>easy and hardï¼š</p><ul><li>OHEM</li><li>directly abandon examples</li><li>å¯¼è‡´è®­ç»ƒä¸å……åˆ†</li></ul></li><li><p>positive and negative</p><ul><li>focal loss</li><li>æœ‰ä¸¤ä¸ªè¶…å‚ï¼Œè·Ÿdata distributionç»‘å®š</li><li>not adaptive</li></ul></li><li><p>é€šå¸¸æ­£æ ·æœ¬æ—¢æ˜¯å°‘é‡æ ·æœ¬åˆæ˜¯å›°éš¾æ ·æœ¬ï¼Œè€Œä¸”å¯ä»¥é€šé€šå½’ç»“ä¸ºæ¢¯åº¦åˆ†å¸ƒä¸å‡åŒ€çš„é—®é¢˜</p><p>  <img src="/2021/08/31/GHM/gradients.png" width="50%;"></p><ul><li>å¤§é‡æ ·æœ¬åªè´¡çŒ®å¾ˆå°çš„æ¢¯åº¦ï¼Œé€šå¸¸å¯¹åº”ç€å¤§é‡è´Ÿæ ·æœ¬ï¼Œæ€»é‡å¤šäº†ä¹Ÿå¯èƒ½ä¼šå¼•å¯¼æ¢¯åº¦ï¼ˆå·¦å›¾ï¼‰</li><li>hardæ ·æœ¬è¦æ¯”mediumæ ·æœ¬æ•°é‡å¤§ï¼Œæˆ‘ä»¬é€šå¸¸å°†å…¶çœ‹ä½œç¦»ç¾¤ç‚¹ï¼Œå› ä¸ºæ¨¡å‹ç¨³å®šä»¥åè¿™äº›hard examplesä»æ—§å­˜åœ¨ï¼Œä»–ä»¬ä¼šå½±å“æ¨¡å‹ç¨³å®šæ€§ï¼ˆå·¦å›¾ï¼‰</li><li>GHMçš„ç›®æ ‡å°±æ˜¯å¸Œæœ›ä¸åŒæ ·æœ¬çš„gradient contributionä¿æŒharmonyï¼Œç›¸æ¯”è¾ƒäºCEå’ŒFLï¼Œç®€å•æ ·æœ¬å’Œoutlierçš„total contributionéƒ½è¢«downweightï¼Œæ¯”è¾ƒharmonyï¼ˆå³å›¾ï¼‰</li></ul></li></ul></li><li><p>we propose gradient harmonizing mechanism (GHM)</p><ul><li>å¸Œæœ›ä¸åŒæ ·æœ¬çš„gradient contributionä¿æŒharmony</li><li>é¦–å…ˆç ”ç©¶gradient densityï¼ŒæŒ‰ç…§æ¢¯åº¦èšç±»æ ·æœ¬ï¼Œå¹¶ç›¸åº”reweight</li><li>é’ˆå¯¹åˆ†ç±»å’Œå›å½’è®¾è®¡GHM-C losså’ŒGHM-R loss</li><li>verified on COCO<ul><li>GHM-Cæ¯”CEå¥½å¾—å¤šï¼Œsligtly better than FL</li><li>GHM-Rä¹Ÿæ¯”smoothL1å¥½</li><li>attains SOTA</li></ul></li><li>dynamic lossï¼šadapt to each batch</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>Problem Description</p><ul><li><p>define gradient norm $g = |p - p^*|$</p><p>  <img src="/2021/08/31/GHM/norm.png" width="40%;"></p></li><li><p>the distribution g from a converged model</p><p>  <img src="/2021/08/31/GHM/distribution.png" width="40%;"></p><ul><li>easyæ ·æœ¬éå¸¸å¤šï¼Œä¸åœ¨ä¸€ä¸ªæ•°é‡çº§ï¼Œä¼šä¸»å¯¼global gradient</li><li>å³ä½¿æ”¶æ•›æ¨¡å‹ä¹Ÿæ— æ³•handleä¸€äº›æéš¾æ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬æ¢¯åº¦ä¸å…¶ä»–æ ·æœ¬å·®å¼‚è¾ƒå¤§ï¼Œæ•°é‡è¿˜ä¸å°‘ï¼Œä¹Ÿä¼šè¯¯å¯¼æ¨¡å‹</li></ul></li></ul></li><li><p>Gradient Density</p><ul><li>define gradient density $GD(g) = \frac{1}{l_{\epsilon}(g)} \sum_{k=1} \delta_{\epsilon}(g_k,g)$<ul><li><img src="/2021/08/31/GHM/hyper.png" width="40%;"></li><li>given a gradient value g</li><li>ç»Ÿè®¡è½åœ¨ä¸­å¿ƒvalueä¸º$g$ï¼Œå¸¦å®½ä¸º$\epsilon$çš„èŒƒå›´å†…çš„æ¢¯åº¦çš„æ ·æœ¬é‡</li><li>å†ç”¨å¸¦å®½å»norm</li></ul></li><li>define the gradient density harmony parameter $\beta_i = \frac{N}{GD(g_i)}$<ul><li>Næ˜¯æ€»æ ·æœ¬é‡</li><li>å…¶å®å°±æ˜¯ä¸densityæˆåæ¯”</li><li>large densityå¯¹åº”æ ·æœ¬ä¼šè¢«downweight</li></ul></li></ul></li><li><p>GHM-C Loss</p><ul><li><p>å°†harmony paramä½œä¸ºloss weightï¼ŒåŠ å…¥ç°æœ‰loss</p><p>  <img src="/2021/08/31/GHM/harmony.png" width="40%;"></p></li><li><p><img src="/2021/08/31/GHM/norm1.png" width="50%;"></p><ul><li>å¯ä»¥çœ‹åˆ°FLä¸»è¦å‹ç®€å•æ ·æœ¬ï¼ˆåŸºäºsample lossï¼‰ï¼ŒGHMä¸¤å¤´å‹ï¼ˆåŸºäºsample densityï¼‰</li><li>æœ€ç»ˆharmonize the total gradient contribution of different density group</li><li>dynamic wrt mini-batchï¼šä½¿å¾—è®­ç»ƒæ›´åŠ efficientå’Œrobust</li></ul></li><li><p>Unit Region Approximation</p><ul><li>å°†gradient norm [0,1]åˆ†è§£æˆMä¸ªunit region</li><li>æ¯ä¸ªregionçš„å®½åº¦$\epsilon = \frac{1}{M}$</li><li>è½åœ¨æ¯ä¸ªregionå†…çš„æ ·æœ¬æ•°è®¡ä½œ$R_{ind(g)}$ï¼Œ$ind(g)$æ˜¯gæ‰€åœ¨regionçš„start idx</li><li>the approximate gradient densityï¼š$\hat {GD}(g) = \frac{R_{ind(g)}}{\epsilon} =R_{ind(g)}M $</li><li><p>approximate harmony parameter &amp; lossï¼š</p><ul><li>we can attain good performance with quite small M</li><li>ä¸€ä¸ªå¯†åº¦åŒºé—´å†…çš„æ ·æœ¬å¯ä»¥å¹¶è¡Œè®¡ç®—ï¼Œè®¡ç®—å¤æ‚åº¦O(MN)</li></ul><p><img src="/2021/08/31/GHM/approx.png" width="45%;"></p></li></ul></li><li><p>EMA</p><ul><li>ä¸€ä¸ªmini-batchå¯èƒ½æ˜¯ä¸ç¨³å®šçš„</li><li>æ‰€ä»¥é€šè¿‡å†å²ç´¯ç§¯æ¥æ›´æ–°ç»´ç¨³ï¼šSGDMå’ŒBNéƒ½ç”¨äº†EMA</li><li>ç°åœ¨æ¯ä¸ªregioné‡Œé¢çš„æ ·æœ¬ä½¿ç”¨åŒä¸€ç»„æ¢¯åº¦ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªregionçš„æ ·æœ¬é‡åº”ç”¨äº†EMA<ul><li>t-th iteraion</li><li>j-th region</li><li>we have $R_j^t$</li><li>apply EMAï¼š$S_j^t = \alpha S_j^(t-1) + (1-\alpha )R_j^t$</li><li>$\hat GD(g) = S_{ind(g)} M$</li></ul></li><li>è¿™æ ·gradient densityä¼šæ›´smooth and insensitive to extreme data</li></ul></li></ul></li><li><p>GHM-R loss</p><ul><li><p>smooth L1ï¼š</p><p>  <img src="/2021/08/31/GHM/SL1.png" width="40%;"></p><ul><li>é€šå¸¸åˆ†ç•Œç‚¹è®¾ç½®æˆ$\frac{1}{9}$</li><li>SL1åœ¨çº¿æ€§éƒ¨åˆ†çš„å¯¼æ•°æ°¸è¿œæ˜¯å¸¸æ•°ï¼Œæ²¡æ³•å»distinguishing of examples</li><li>ç”¨$|d|$ä½œä¸ºgradient normåˆ™å­˜åœ¨inf</li></ul></li><li><p>æ‰€ä»¥å…ˆæ”¹é€ smooth L1ï¼šAuthentic Smooth L1</p><p>  <img src="/2021/08/31/GHM/ASL.png" width="40%;"></p><p>  <img src="/2021/08/31/GHM/ASL1.png" width="40%;"></p><ul><li>$\mu=0.02$</li><li>æ¢¯åº¦èŒƒå›´æ­£å¥½åœ¨[0,1)</li></ul></li><li><p>define gradient norm as $gr = |\frac{d}{\sqrt{d^2+\mu^2}}|$</p><ul><li><p>è§‚å¯Ÿconverged modelâ€˜s gradient norm for ASL1ï¼Œå‘ç°å¤§é‡æ˜¯outliers</p><p>  <img src="/2021/08/31/GHM/gr.png" width="40%;"></p></li><li><p>åŒæ ·ç”¨gradient densityè¿›è¡Œreweighting</p><p>  <img src="/2021/08/31/GHM/GHMR.png" width="40%;"></p></li><li><p>æ”¶æ•›çŠ¶æ€ä¸‹ï¼Œä¸åŒç±»å‹çš„æ ·æœ¬å¯¹æ¨¡å‹çš„gradient contribution</p><p>  <img src="/2021/08/31/GHM/rcontrib.png" width="40%;"></p><ul><li>regressionæ˜¯å¯¹æ‰€æœ‰æ­£æ ·æœ¬è¿›è¡Œè®¡ç®—ï¼Œä¸»è¦æ˜¯é’ˆå¯¹ç¦»ç¾¤ç‚¹è¿›è¡Œdownweighting</li><li>è¿™é‡Œé¢çš„ä¸€ä¸ªè§‚ç‚¹æ˜¯ï¼šåœ¨regression taské‡Œé¢ï¼Œå¹¶éæ‰€æœ‰easyæ ·æœ¬éƒ½æ˜¯ä¸é‡è¦çš„ï¼Œåœ¨åˆ†ç±»taské‡Œé¢ï¼Œeasyæ ·æœ¬å¤§éƒ¨åˆ†éƒ½æ˜¯ç®€å•çš„èƒŒæ™¯ç±»ï¼Œä½†æ˜¯regressionåˆ†æ”¯é‡Œé¢çš„easy sampleæ˜¯å‰æ™¯boxï¼Œè€Œä¸”still deviated from ground truthï¼Œä»æ—§å…·æœ‰å……åˆ†çš„ä¼˜åŒ–ä»·å€¼</li><li>æ‰€ä»¥GHM-Rä¸»è¦æ˜¯upweight the important part of easy samples and downweight the outliers</li></ul></li></ul></li></ul></li></ul></li><li><p>å®éªŒ</p></li></ol>]]></content>
      
      
        <tags>
            
            <tag> single-stage detector, data imbalance, loss </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>R-FCN</title>
      <link href="/2021/08/31/R-FCN/"/>
      <url>/2021/08/31/R-FCN/</url>
      <content type="html"><![CDATA[<p>referenceï¼š<a href="https://zhuanlan.zhihu.com/p/32903856" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32903856</a></p><p>å¼•ç”¨é‡ï¼š4193</p><h2 id="R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks"><a href="#R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks" class="headerlink" title="R-FCN: Object Detection via Region-based Fully Convolutional Networks"></a>R-FCN: Object Detection via Region-based Fully Convolutional Networks</h2><ol><li><p>åŠ¨æœº</p><ul><li>region-basedï¼š<ul><li>å…ˆæ¡†å®šregion of interestçš„æ£€æµ‹ç®—æ³•</li><li>previous methodsï¼šFast/Faster R-CNNï¼Œapply costly per-region subnetwork hundreds of times</li></ul></li><li>fully convolutional <ul><li>æ—¨åœ¨è§£å†³Faster R-CNNç¬¬äºŒé˜¶æ®µè®¡ç®—ä¸å…±äº«ï¼Œæ•ˆç‡ä½çš„é—®é¢˜</li></ul></li><li>we propose position-sensitive score maps<ul><li>translation-invariance in image classification</li><li>translation-variance in object detection</li></ul></li><li>verified on PASCAL VOC</li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>ä¸»æµçš„ä¸¤é˜¶æ®µæ£€æµ‹æ¶æ„</p><ul><li><p>two subnetworks</p><ul><li>a shared fully convolutionalï¼šè¿™ä¸€éƒ¨åˆ†æå–é€šç”¨ç‰¹å¾ï¼Œä½œç”¨äºå…¨å›¾</li><li>an RoI-wise subnetworkï¼šè¿™ä¸€éƒ¨åˆ†ä¸èƒ½å…±äº«è®¡ç®—ï¼Œä½œç”¨äºproposalsï¼Œå› ä¸ºæ˜¯è¦é’ˆå¯¹æ¯ä¸ªä½ç½®çš„ROIè¿›è¡Œåˆ†ç±»å’Œå›å½’</li><li><p>ä¹Ÿå°±æ˜¯è¯´ï¼Œç¬¬ä¸€éƒ¨åˆ†æ˜¯ä½ç½®ä¸æ•æ„Ÿçš„ï¼Œç¬¬äºŒéƒ¨åˆ†æ˜¯ä½ç½®æ•æ„Ÿçš„</p><p><img src="/2021/08/31/R-FCN/region-based.png" width="70%;"></p></li></ul></li><li><p>ç½‘ç»œè¶Šæ·±è¶Štranslation invariantï¼Œç›®æ ‡æ€ä¹ˆæ‰­æ›²ã€å¹³ç§»æœ€ç»ˆçš„åˆ†ç±»ç»“æœéƒ½ä¸å˜ï¼Œå¤šå±‚poolingåçš„å°feature mapä¸Šä¹Ÿæ„ŸçŸ¥ä¸åˆ°å°ä½ç§»ï¼Œå¹³ç§»å¯å˜æ€§ï¼ˆtranslation varianceï¼‰ï¼Œå¯¹å®šä½ä»»åŠ¡ä¸å‹å¥½</p></li><li><p>æ‰€ä»¥resnet-back-detectoræˆ‘ä»¬æ˜¯æŠŠROI Poolingæ”¾åœ¨stage4åé¢ï¼Œè·Ÿä¸€ä¸ªRoI-wiseçš„stage5</p><ul><li>improves accuracy</li><li>lower speed due to RoI-wise</li></ul></li></ul></li><li><p>R-FCN</p><ul><li>è¦è§£å†³çš„æ ¹æœ¬é—®é¢˜æ˜¯RoI-wiseéƒ¨åˆ†ä¸å…±äº«ï¼Œé€Ÿåº¦æ…¢ï¼š300ä¸ªproposalè¦è®¡ç®—300æ¬¡</li><li>å•çº¯åœ°å°†ç½‘ç»œæå‰æ”¾åˆ°shared backé‡Œé¢ä¸è¡Œï¼Œä¼šé€ æˆtranslation invariantï¼Œä½ç½®ç²¾åº¦ä¼šä¸‹é™</li><li><p>å¿…é¡»é€šè¿‡å…¶ä»–æ–¹æ³•åŠ å¼ºç½‘ç»œçš„å¹³ç§»å¯å˜æ€§ï¼Œæ‰€ä»¥æå‡ºäº†position-sensitive score map</p><ul><li>å°†å…¨å›¾åˆ’åˆ†ä¸ºkxkä¸ªåŒºåŸŸ</li><li>position-sensitive score mapï¼šç”Ÿæˆkxkx(C+1)ä¸ªç‰¹å¾å›¾</li><li>æ¯ä¸ªä½ç½®å¯¹åº”C+1ä¸ªç‰¹å¾å›¾</li><li>åšRoIPoolingçš„æ—¶å€™ï¼Œæ¯ä¸ªbinæ¥è‡ªæ¯ä¸ªpositionå¯¹åº”çš„C+1ä¸ªmapï¼ˆè¿™å’‹æƒ³çš„ï¼Œspace dimåˆ°channel dimå†åˆ°space dimï¼Ÿï¼‰</li></ul><p><img src="/2021/08/31/R-FCN/score map.png" width="60%;"></p></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>overview</p><p> <img src="/2021/08/31/R-FCN/RFCN.png" width="70%;"></p><ul><li>two-stage<ul><li>region proposalï¼šRPN</li><li>region classificationï¼šthe R-FCN</li></ul></li><li>R-FCN<ul><li>å…¨å·ç§¯</li><li>è¾“å‡ºconvå±‚æœ‰kxkx(C+1)ä¸ªchannel<ul><li>kxkå¯¹åº”grid positions</li><li>C+1å¯¹åº”Cä¸ªå‰æ™¯+background</li></ul></li><li>æœ€åæ˜¯position-sensitive RoI pooling layer<ul><li>aggregates from last conv and RPNï¼Ÿ</li><li>generate scores for each RoI</li><li>each bin aggregates responses fromå¯¹åº”çš„positionçš„channel score mapsï¼Œè€Œä¸æ˜¯å…¨éƒ¨é€šé“</li><li>forceæ¨¡å‹åœ¨é€šé“ä¸Šå½¢æˆå¯¹ä¸åŒä½ç½®çš„æ•æ„Ÿèƒ½åŠ›</li></ul></li></ul></li></ul></li><li><p>R-FCN architecture</p><ul><li>backï¼šResNet-101ï¼Œpre-trained on ImageNetï¼Œblock5 è¾“å‡ºæ˜¯2048-d</li><li>ç„¶åæ¥äº†random initialized 1x1 convï¼Œé™ç»´</li><li>cls brach<ul><li>æ¥$k^2(C+1)$çš„convç”Ÿæˆscore maps</li><li>ç„¶åæ˜¯Position-sensitive RoI pooling<ul><li>å°†æ¯ä¸ªROIå‡åŒ€åˆ‡åˆ†æˆkxkä¸ªbins</li><li>æ¯ä¸ªbinåœ¨å¯¹åº”çš„Position-sensitive score mapsä¸­æ‰¾åˆ°å”¯ä¸€çš„é€šé“ï¼Œè¿›è¡Œaverage pooling</li><li>æœ€ç»ˆå¾—åˆ°kxkçš„pooling mapï¼ŒC+1ä¸ªé€šé“</li><li>å°†pooling map performs average poolingï¼Œå¾—åˆ°C+1çš„vectorï¼Œç„¶åsoftmax</li></ul></li></ul></li><li>box branch<ul><li>æ¥$4k^2$çš„convç”Ÿæˆscore maps</li><li>Position-sensitive RoI pooling<ul><li>å¾—åˆ°kxkçš„pooling mapï¼Œ4ä¸ªé€šé“</li><li>average poolingï¼Œå¾—åˆ°4d vectorï¼Œä½œä¸ºå›å½’å€¼$(t_x,t_y,t_w,t_h)$</li></ul></li></ul></li><li>there is no learnable layer after the ROI layerï¼Œenable nearly cost-free region-wise computation</li></ul></li><li><p>Training</p><ul><li>R-FCN positives / negativesï¼šå’Œgt boxçš„IoU&gt;0.5çš„proposasl</li><li>adopt OHEM</li><li>sort all ROI loss and select the highest 128</li><li>å…¶ä»–settingsåŸºæœ¬å’ŒFaster-RCNNä¸€è‡´</li></ul></li><li>Atrous and stride<ul><li>ç‰¹åˆ«åœ°ï¼Œå¯¹resnetçš„block5è¿›è¡Œäº†æ”¹å˜</li><li>stride2æ”¹æˆstride1</li><li>æ‰€æœ‰çš„convæ”¹æˆç©ºæ´å·ç§¯</li><li>RPNæ˜¯æ¥åœ¨block4çš„è¾“å‡ºä¸Šï¼Œæ‰€ä»¥ä¸å—ç©ºæ´å·ç§¯çš„å½±å“ï¼Œåªå½±å“R-FCN head</li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ç›®æ ‡æ£€æµ‹ï¼Œå…¨å·ç§¯ï¼Œregion-basedï¼Œtwo-stage detector </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Meta Pseudo Labels</title>
      <link href="/2021/08/23/Meta-Pseudo-Labels/"/>
      <url>/2021/08/23/Meta-Pseudo-Labels/</url>
      <content type="html"><![CDATA[<p>papers</p><ul><li><p>[MPL 2021] Meta Pseudo Labels</p></li><li><p>[UDA 2009] Unsupervised Data Augmentation for Consistency Training</p></li><li><p>[Entropy Minimization 2004] Semi-supervised Learning by Entropy Minimization</p></li></ul><h2 id="Meta-Pseudo-Labels"><a href="#Meta-Pseudo-Labels" class="headerlink" title="Meta Pseudo Labels"></a>Meta Pseudo Labels</h2><ol><li><p>åŠ¨æœº</p><ul><li>semi-supervised learning<ul><li>Pseudo Labelsï¼šfixed teacher</li><li>Meta Pseudo Labelsï¼šconstantly adapted teacher by the feedback of the student</li></ul></li><li>SOTA on ImageNetï¼štop-1 acc 90.2%</li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>Pseudo Labels methods</p><ul><li>teacher generates pseudo labels on unlabeled images</li><li>pseudo labeled images are then combined with labeled images to train the student</li><li>confirmation bias problemï¼šstudentçš„ç²¾åº¦å–å†³äºä¼ªæ ‡ç­¾çš„è´¨é‡</li></ul></li><li><p>we propose Meta Pseudo Labels</p><ul><li>teacher observes how its pseudo labels affect the student</li><li>then correct the bias</li><li>the feedback signal is the performance of the student on the labeled dataset</li><li>æ€»çš„æ¥è¯´ï¼Œteacherå’Œstudentæ˜¯train in parallelçš„<ul><li>student learns from pseudo labels from the teacher</li><li>teacher learns from reward signal from how well student perform on labeled set</li></ul></li><li>dataset<ul><li>ImageNet as labeled set</li><li>JFT-300M as unlabeled set</li></ul></li><li>model<ul><li>teacherï¼šEfficientNet-L2</li><li>studentï¼šEfficientNet-L2</li></ul></li></ul></li><li><p>main difference</p><ul><li>Pseudo Labelsæ–¹æ³•ä¸­ï¼Œteacheråœ¨å•å‘çš„å½±å“student</li><li><p>Meta Pseudo Labelsæ–¹æ³•ä¸­ï¼Œteacherå’Œstudentæ˜¯äº¤äº’ä½œç”¨çš„</p><p><img src="/2021/08/23/Meta-Pseudo-Labels/diff.png" width="80%;"></p></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>notations</p><ul><li>models<ul><li>teacher model T &amp; $\theta_T$</li><li>student model S &amp; $\theta_S$</li></ul></li><li>data<ul><li>labeled set $(x_l, y_l)$</li><li>unlabeled set $(x_u)$</li></ul></li><li>predictions<ul><li>soft predictions by teacher $T(x_u, \theta_T)$</li><li>student  $S(x_u, \theta_S)$ &amp;  $S(x_l, \theta_S)$</li></ul></li><li>loss<ul><li>$CE(q,p)$ï¼Œå…¶ä¸­$q$æ˜¯one-hot labelï¼Œe.g. $CE(y_l, S(x_l, \theta_S))$</li></ul></li></ul></li><li><p>Pseudo Labels</p><ul><li>given a fixed teacher $\theta_T$</li><li><p>train the student model to minimize the cross-entropy loss on unlabeled data</p><script type="math/tex; mode=display">\theta_S^{PL} = argmin_{\theta_S}CE(T(x_u,\theta_T), S(x_u, \theta_S))</script></li><li><p>$\theta_S^{PL}$ also achieve a low loss on labeled data</p></li><li>$\theta_S^{PL}$ explicitly depends on $\theta_T$ï¼š$\theta_S^{PL}(\theta_T)$</li><li>student loss on labeled data is also a function of $\theta_T$ï¼š$L_l(\theta_S^{PL}(\theta_T))$</li></ul></li><li><p>Meta Pseudo Labels</p><ul><li><p>intuitionï¼šminimize $L_l$ with respect to $\theta_T$</p></li><li><p>ä½†æ˜¯å®é™…ä¸Šdependency of $\theta_S^{PL}(\theta_T)$ on $\theta_T$ éå¸¸å¤æ‚</p></li><li><p>å› ä¸ºæˆ‘ä»¬ç”¨äº†teacher predictionçš„hard labelså»è®­ç»ƒstudent</p></li><li><p>an alternating optimization procedure</p><p>  <img src="/2021/08/23/Meta-Pseudo-Labels/alternating.png" width="45%;"></p></li><li><p>teacherâ€™s auxiliary losses</p><ul><li>augment the teacherâ€™s training with a supervised learning objective and a semi-supervise learning objective</li><li>supervised objective<ul><li>train on labeled data</li><li>CE</li></ul></li><li>semi-supervised objective<ul><li>train on unlabeled data</li><li>UDA(Unsupervised Data Augmentation)ï¼šå°†æ ·æœ¬è¿›è¡Œç®€å•å¢å¼ºï¼Œé€šè¿‡è¡¡é‡ä¸€è‡´æ€§æŸå¤±ï¼Œæ¨¡å‹çš„æ³›åŒ–æ•ˆæœå¾—åˆ°æå‡</li><li>consistency training lossï¼šKLæ•£åº¦</li></ul></li></ul></li><li><p>finetuning student</p><ul><li>åœ¨meta pseudo labelsè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œstudent only learns from the unlabeled data</li><li>æ‰€ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ç»“æŸåï¼Œå¯ä»¥finetune it on labeled data to improve accuracy </li></ul></li><li><p>overall algorithm</p><p>  <img src="/2021/08/23/Meta-Pseudo-Labels/algorithm.png" width="90%;"></p><pre><code>  * è¿™é‡Œé¢æœ‰ä¸€å¤„ä¸‹æ ‡å†™é”™äº†ï¼Œå°±æ˜¯teacherçš„UDA gradientï¼Œæ˜¯åœ¨unlabeled dataä¸Šé¢ç®—çš„ï¼Œé‚£ä¸¤ä¸ª$x_l$å¾—æ”¹æˆ$x_u$  * UDA lossè®ºæ–‡é‡Œä½¿ç”¨ä¸¤ä¸ªpredicted logitsçš„æ•£åº¦ï¼Œè¿™é‡Œæ˜¯CE</code></pre></li></ul></li></ul></li></ol><h2 id="Unsupervised-Data-Augmentation-for-Consistency-Training"><a href="#Unsupervised-Data-Augmentation-for-Consistency-Training" class="headerlink" title="Unsupervised Data Augmentation for Consistency Training"></a>Unsupervised Data Augmentation for Consistency Training</h2><ol><li><p>åŠ¨æœº</p><ul><li>data augmentation in previous works<ul><li>èƒ½åœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®çš„é—®é¢˜</li><li>å¤šç”¨åœ¨supervised modelä¸Š</li><li>achieved limited gains</li></ul></li><li>we propose UDA<ul><li>apply data augmentation in semi-supervised learning setting</li><li>use harder and more realistic noise to generate the augmented samples</li><li>encourage the prediction to be consistent between unlabeled &amp; augmented unlabeled sample</li><li>åœ¨è¶Šå°çš„æ•°æ®é›†ä¸Šæå‡è¶Šå¤§</li></ul></li><li>verified on<ul><li>six language tasks</li><li>three vision tasks<ul><li>ImageNet-10%ï¼šï¼štop1/top5 68.7/88.5%</li><li>ImageNet-extra unlabeledï¼štop1/top5 79.0/94.5%</li></ul></li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>semi-supervised learning<ul><li>three categories<ul><li>graph-based label propagation via graph convolution and graph embeddings</li><li>modeling prediction target as latent variables</li><li>consistency / smoothness enforcing</li></ul></li><li>æœ€åè¿™ä¸€ç±»æ–¹æ³•shown to work wellï¼Œ<ul><li>enforce the model predictions on the two examples to be similar</li><li>ä¸»è¦åŒºåˆ«åœ¨äºperturbation functionçš„è®¾è®¡</li></ul></li></ul></li><li>we propose UDA<ul><li>use state-of-the-art data augmentation methods</li><li>we show that better augmentation methods(AutoAugment) lead to greater improvements</li><li>minimizes the KL divergence</li><li>can be applied even the class distributions of labeled and unlabeled data mismatch</li></ul></li><li>we propose TSA<ul><li>a training technique</li><li>prevent overfitting when much more unlabeled data is avaiable than labeled data</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>formulation</p><ul><li><p>given an input $x\in U$ and a small noise $\epsilon$</p></li><li><p>compute the output distribution $p_{\theta}(y|x)$ and $p_{\theta}(y|x,\epsilon)$</p></li><li><p>minimize the divergence between two predicted distributions $D(p_{\theta}(y|x)||p_{\theta}(y|x,\epsilon))$</p><p>  <img src="/2021/08/23/Meta-Pseudo-Labels/UDA.png" width="70%;"></p></li><li><p>add a CE loss on labeled data</p><p>  <img src="/2021/08/23/Meta-Pseudo-Labels/CE.png" width="70%;"></p></li><li><p>UDAçš„ä¼˜åŒ–ç›®æ ‡</p><ul><li>enforce the model to be insensitive to perturbation</li><li>thus smoother to the changes in the input space</li></ul></li><li><p>$\lambda=1$ for most experiments</p></li><li><p>use different batchsize for labeled &amp; unlabeled</p></li></ul></li><li><p>Augmentation Strategies for Different Tasks</p><ul><li>AutoAugment for Image Classification<ul><li>é€šè¿‡RLæœå‡ºæ¥çš„ä¸€ç»„optimal combination of aug operations</li></ul></li><li>Back translation for Text Classification</li><li>TF-IDF based word replacing for Text Classification</li></ul></li><li><p>Trade-off Between Diversity and Validity for Data Augmentation</p><ul><li>å¯¹åŸå§‹sampleåšå˜æ¢çš„æ—¶å€™ï¼Œæœ‰ä¸€å®šæ¦‚ç‡å¯¼è‡´gt labelå˜åŒ–</li><li>AutoAugmentå·²ç»æ˜¯optmial trade-offäº†ï¼Œæ‰€ä»¥ä¸ç”¨ç®¡</li><li>text taskséœ€è¦è°ƒèŠ‚temperature</li></ul></li><li><p>Additional Training Techniques</p><ul><li><p>TSA(Training Signal Annealing)</p><ul><li><p>situationï¼šunlabeled dataè¿œæ¯”labeled dataå¤šçš„æƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦large enough modelå»å……åˆ†åˆ©ç”¨å¤§æ•°æ®ï¼Œä½†åˆå®¹æ˜“å¯¹å°trainsetè¿‡æ‹Ÿåˆ</p></li><li><p>for each training step</p><ul><li>set a threshold $\frac{1}{K}\leq \eta_t\leq 1$ï¼ŒK is the number of categories</li><li>å¦‚æœæ ·æœ¬åœ¨gt clsä¸Šçš„é¢„æµ‹æ¦‚ç‡å¤§äºè¿™ä¸ªthresholdï¼Œå°±æŠŠè¿™ä¸ªæ ·æœ¬çš„losså»æ‰</li></ul></li><li><p>$\eta_t$ serves as a ceiling to prevent the model from over-training on examples that the model is already confident about</p></li><li><p>gradually release the training signals of the labeled examplesï¼Œç¼“è§£overfitting</p></li><li><p>schedules of $\eta_t$</p><ul><li>log-scheduleï¼š$\lambda_t = 1-exp(-\frac{t}{T}*5)$</li><li>linear-scheduleï¼š$\lambda_t = \frac{t}{T}$</li><li><p>exp-scheduleï¼š$\lambda_t = exp((\frac{t}{T}-1)*5)$</p></li><li><p>å¦‚æœæ¨¡å‹éå¸¸å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œç”¨exp-scheduleï¼Œåè¿‡æ¥ï¼ˆabundant labeled data/effective regularizationsï¼‰ï¼Œç”¨log-schedule</p><p><img src="/2021/08/23/Meta-Pseudo-Labels/TSA.png" width="80%;"></p></li></ul></li></ul></li><li><p>Sharpening Predictions</p><ul><li>situationï¼šthe predicted distributions on unlabeled examples tend to be over-flat across categoriesï¼Œtaskæ¯”è¾ƒå›°éš¾ï¼Œè®­ç»ƒæ•°æ®æ¯”è¾ƒå°‘æ—¶ï¼Œåœ¨unlabeled dataä¸Šæ¯ç±»çš„é¢„æµ‹æ¦‚ç‡éƒ½å·®ä¸å¤šä½ï¼Œæ²¡æœ‰å€¾å‘æ€§</li><li>è¿™æ—¶å€™KL divergenceçš„ç›‘ç£ä¿¡æ¯å°±å¾ˆå¼±</li><li>thus we need to sharpen the predicted distribution on unlabeled examples</li><li>Confidence-based maskingï¼šå°†current model not confident enough to predictçš„æ ·æœ¬è¿‡æ»¤æ‰ï¼Œåªä¿ç•™æœ€å¤§é¢„æµ‹æ¦‚ç‡å¤§äº0.6çš„æ ·æœ¬è®¡ç®—consistency loss</li><li>Entropy minimizationï¼šadd an entropy term to the overall objective</li><li>softmax temperatureï¼šåœ¨è®¡ç®—softmaxæ—¶å…ˆå¯¹logitsè¿›è¡Œrescaleï¼Œ$Softmax(logits/\tau)$ï¼Œa lower temperature corresponds to a sharper distribution</li><li>in practiceå‘ç°Confidence-based maskingå’Œsoftmax temperatureæ›´é€‚ç”¨äºå°labeled setï¼ŒEntropy minimizationé€‚ç”¨äºç›¸å¯¹å¤§ä¸€ç‚¹çš„labeled set</li></ul></li><li><p>Domain-relevance Data Filtering</p><ul><li>å…¶å®ä¹Ÿæ˜¯Confidence-based maskingï¼Œå…ˆç”¨labeled dataè®­ç»ƒä¸€ä¸ªbase modelï¼Œç„¶åinference the out-of-domain datasetï¼ŒæŒ‘å‡ºé¢„æµ‹æ¦‚ç‡è¾ƒå¤§çš„æ ·æœ¬</li></ul></li></ul></li></ul></li></ol><h2 id="Semi-supervised-Learning-by-Entropy-Minimization"><a href="#Semi-supervised-Learning-by-Entropy-Minimization" class="headerlink" title="Semi-supervised Learning by Entropy Minimization"></a>Semi-supervised Learning by Entropy Minimization</h2>]]></content>
      
      
        <tags>
            
            <tag> semi-supervised learning, teacher-student, classification </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Generalized Focal Loss</title>
      <link href="/2021/08/20/Generalized-Focal-Loss/"/>
      <url>/2021/08/20/Generalized-Focal-Loss/</url>
      <content type="html"><![CDATA[<h2 id="Generalized-Focal-Loss-Learning-Qualified-and-Distributed-Bounding-Boxes-for-Dense-Object-Detection"><a href="#Generalized-Focal-Loss-Learning-Qualified-and-Distributed-Bounding-Boxes-for-Dense-Object-Detection" class="headerlink" title="Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for  Dense Object Detection"></a>Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for  Dense Object Detection</h2><ol><li><p>åŠ¨æœº</p><ul><li>one-stage  detectors<ul><li>dense prediction</li><li>three fundamental elements<ul><li>class branch</li><li>box localization branch</li><li>an individual quality branch to estimate the quality of localization</li></ul></li></ul></li><li>current problems<ul><li>the inconsistent usage of the quality estimation in train &amp; test</li><li>the inflexible Dirac delta distributionï¼šå°†box regressionçš„valueå»ºæ¨¡æˆçœŸå€¼é™„è¿‘çš„è„‰å†²åˆ†å¸ƒï¼Œç”¨æ¥æè¿°è¾¹ç•Œä¸æ¸…æ™°/é®æŒ¡çš„caseå¯èƒ½ä¸å‡†ç¡®</li></ul></li><li>we design new representations for these three elements<ul><li>merge quality estimation into class predictionï¼šå°†objectness/centernessæ•´åˆè¿›cls predictionï¼Œç›´æ¥ç”¨ä½œNMS score</li><li>continout labels</li><li>propose GFL(Generalized Focal Loss) that generalizes Focal Loss from discrete form into continous version</li></ul></li><li>test on COCO<ul><li>ResNet-101-?-GFL: 45.0% AP </li><li>defeat ATSS</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>inconsistent usage of localization quality estimation and classification score</p><ul><li>è®­ç»ƒçš„æ—¶å€™qualityå’Œcls branchæ˜¯independent branch</li><li>box branchçš„supervisionåªä½œç”¨åœ¨positiveæ ·æœ¬ä¸Šï¼šwhich is unreliable on predicting negatives</li><li>æµ‹è¯•é˜¶æ®µå°†qualityå’Œcls scoreä¹˜èµ·æ¥æœ‰å¯èƒ½æ‹‰é«˜è´Ÿæ ·æœ¬çš„åˆ†æ•°ï¼Œä»¥è‡³äºåœ¨NMSé˜¶æ®µæŠŠä½åˆ†æ­£æ ·æœ¬æŒ¤æ‰</li></ul></li><li><p>inflexible representation of bounding boxes</p><ul><li>most methodå»ºæ¨¡æˆè„‰å†²åˆ†å¸ƒï¼šåªåœ¨IoUå¤§äºä¸€å®šé˜ˆå€¼çš„æ ¼å­ä¸Šæœ‰å“åº”ï¼Œåˆ«çš„æ ¼å­éƒ½æ˜¯0</li><li>some recent workå»ºæ¨¡æˆé«˜æ–¯åˆ†å¸ƒ</li><li>in fact the real distribution can be more arbitrary and flexibleï¼Œè¿ç»­ä¸”ä¸ä¸¥æ ¼é•œåƒ</li></ul></li><li><p>thus we propose</p><ul><li><p>merge the quality representation into the class branchï¼š</p><ul><li>class vectorçš„æ¯ä¸ªå…ƒç´ ä»£è¡¨äº†æ ¼å­çš„localization quality(å¦‚IoU score)</li><li><p>åœ¨inferenceé˜¶æ®µä¹Ÿæ˜¯ç›´æ¥ç”¨ä½œcls score</p><p><img src="/2021/08/20/Generalized-Focal-Loss/joint quality.png" width="80%"></p></li></ul></li><li><p>propose arbitrary/general distribution</p><ul><li>æœ‰æ˜ç¡®è¾¹ç•Œçš„ç›®æ ‡çš„è¾¹çš„åˆ†å¸ƒæ˜¯æ¯”è¾ƒsharpçš„</li><li><p>æ²¡æœ‰æ˜æ˜¾è¾¹ç•Œçš„è¾¹åˆ†å¸ƒå°±æ˜¯flattenä¸€ç‚¹</p><p><img src="/2021/08/20/Generalized-Focal-Loss/distribution.png" width="80%"></p></li></ul></li><li><p>Generalized Focal Loss (GFL)</p><ul><li>joint class representationæ˜¯continuous IoU label (0âˆ¼1) </li><li>imbalanceé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œä½†æ˜¯standart Focal Lossä»…æ”¯æŒ[0,1] sample</li><li>ä¿®æ”¹æˆcontinuouså½¢å¼ï¼ŒåŒæ—¶specialized into Quality Focal Loss (QFL) and Distribution Focal Loss (DFL) <ul><li>QFL for cls branchï¼šfocuses on a sparse set of hard examples</li><li>DFL for box branchï¼š focus on learning the probabilities of values around the continuous target locations</li></ul></li></ul></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>Focal Loss (FL)</p><p>  <img src="/2021/08/20/Generalized-Focal-Loss/FL.png" width="70%"></p><ul><li>standard CE partï¼š$-log(p_t)$</li><li>scaling factorï¼šdown-weights the easy examplesï¼Œfocus on hard examples</li></ul></li><li><p>Quality Focal Loss (QFL)</p><ul><li><p>soft one-hot labelï¼šæ­£æ ·æœ¬åœ¨å¯¹åº”ç±»åˆ«ä¸Šæœ‰ä¸ª(0,1]çš„float scoreï¼Œè´Ÿæ ·æœ¬å…¨0</p></li><li><p>float scoreå®šä¹‰ä¸ºé¢„æµ‹æ¡†å’Œgt boxçš„IoU score</p></li><li><p>we adopt multiple binary classification with sigmoid</p></li><li><p>modify FL</p><ul><li>CE part æ”¹æˆcomplete formï¼š$-ylog(\hat y)-(1-y)log(1-\hat y)$</li><li>scaling partç”¨vector distanceæ›¿æ¢å‡æ³•ï¼š$|y-\hat y |^{\beta}$</li><li><p>$\beta$ controls the down-weighting rate smoothly &amp; $\beta=2$ works best</p><p><img src="/2021/08/20/Generalized-Focal-Loss/QFL.png" width="70%"></p></li></ul></li></ul></li><li><p>Distribution Focal Loss (DFL)</p><ul><li><p>use relative offsets from the location to the four sides of a bounding box as the regression targets</p></li><li><p>å›å½’é—®é¢˜formulation</p><ul><li>è¿ç»­ï¼š$\hat y = \int_{y_0}^{y_n}P(x)xdx$</li><li>ç¦»æ•£åŒ–ï¼š$\hat y = \sum_{i=0}^n P(y_i)y_i$</li><li>P(x) can be easily implemented through a softmax layer containing n+1 unitsï¼š</li></ul></li><li><p>DFL</p><ul><li>force predictions to focus values near label $y$ï¼šexplicitly enlarge the probabilities of $y_i$å’Œ$y_{i+1}$ï¼Œgiven $y_i \leq y \leq y_{i+1}$</li><li>$log(S_i)$ force the probabilities </li><li><p>gap balance the ä¸Šä¸‹é™ï¼Œä½¿å¾—$\hat y$çš„global mininum solutionæ— é™é€¼è¿‘çœŸå€¼$y$ï¼Œå¦‚æœçœŸå€¼æ¥è¿‘çš„æ˜¯$\hat y_{i+1}$ï¼Œå¯ä»¥çœ‹åˆ°$log(S_i)$é‚£é¡¹è¢«downscaleäº†</p><p><img src="/2021/08/20/Generalized-Focal-Loss/DFL.png" width="70%"></p></li></ul></li></ul></li><li><p>Generalized Focal Loss (GFL)</p><p>  <img src="/2021/08/20/Generalized-Focal-Loss/GFL.png" width="80%"></p><ul><li><p>ä»¥å‰çš„cls preditionsåœ¨æµ‹è¯•é˜¶æ®µè¦ç»“åˆquality predictionsä½œä¸ºNMS scoreï¼Œç°åœ¨ç›´æ¥å°±æ˜¯</p></li><li><p>ä»¥å‰regression targetsæ¯ä¸ªå›å½’ä¸€ä¸ªå€¼ï¼Œç°åœ¨æ˜¯n+1ä¸ªå€¼</p></li><li><p>overall</p><p>  <img src="/2021/08/20/Generalized-Focal-Loss/GFL1.png" width="70%"></p><ul><li>ç¬¬ä¸€é¡¹cls lossï¼Œå°±æ˜¯QFLï¼Œdense on æ‰€æœ‰æ ¼å­ï¼Œç”¨æ­£æ ·æœ¬æ•°å»norm</li><li>ç¬¬äºŒé¡¹box lossï¼ŒGIoU loss + DFLï¼Œ$\lambda_0$é»˜è®¤2ï¼Œ$\lambda_1$é»˜è®¤1/4ï¼Œåªè®¡ç®—æœ‰IoUçš„æ ¼å­</li><li>we also utilize the quality scores to weight $L_B$ and $L_D$ during training</li></ul></li></ul></li></ul></li><li><p>å½©è›‹</p><ul><li><p>IoU branch always superior than centerness-branch</p><ul><li><p>centernesså¤©ç”Ÿå€¼è¾ƒå°ï¼Œå½±å“å¬å›ï¼ŒIoUçš„å€¼è¾ƒå¤§</p><p><img src="/2021/08/20/Generalized-Focal-Loss/centerness.png" width="80%"></p></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> one-stage detector, object-detection </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>soft teacher</title>
      <link href="/2021/08/12/soft-teacher/"/>
      <url>/2021/08/12/soft-teacher/</url>
      <content type="html"><![CDATA[<p>keywordsï¼šsemi-supervised, curriculum, pseudo labels, </p><h2 id="End-to-End-Semi-Supervised-Object-Detection-with-Soft-Teacher"><a href="#End-to-End-Semi-Supervised-Object-Detection-with-Soft-Teacher" class="headerlink" title="End-to-End Semi-Supervised Object Detection with Soft Teacher"></a>End-to-End Semi-Supervised Object Detection with Soft Teacher</h2><ol><li><p>åŠ¨æœº</p><ul><li><p>end-to-end trainingï¼šç›¸æ¯”è¾ƒäºå…¶ä»–æ–¹æ³•çš„multi-stage</p></li><li><p>semi-supervisedï¼šç”¨å¤–éƒ¨unlabeledæ•°æ®ï¼Œä»¥åŠpseudo-label based approach</p></li><li><p>propose two techniques</p><ul><li>soft teacher mechanismï¼špseudoæ ·æœ¬çš„classification lossç”¨teacher modelçš„prediction scoreæ¥åŠ æƒ</li><li>box jittering mechanismï¼šæŒ‘é€‰reliable pseudo boxes</li></ul></li><li><p>verified</p><ul><li>use SWIN-L as baseline</li><li>metric on COCOï¼š60.4 mAP</li><li><p>if pretrained with Object365ï¼š61.3 mAP</p><p><img src="/2021/08/12/soft-teacher/swin.png" width="90%"></p></li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>we present this end-to-end pseudo-label based semi-supervised object detection framework<ul><li>simultaneously performs<ul><li>pseudo-labelingï¼šteacher</li><li>training detector use the current pseudo-labels &amp; a few training sampleï¼šstudent</li></ul></li><li>teacher is an exponential moving average (EMA) of the student model</li><li>mutually enforce each other</li><li>soft teacher approach<ul><li>teacher modelçš„ä½œç”¨æ˜¯ç»™student modelç”Ÿæˆçš„box candidatesæ‰“åˆ†ï¼Œ</li><li>é«˜äºä¸€å®šé˜ˆå€¼çš„ä¸ºå‰æ™¯ï¼Œä½†æ˜¯å¯èƒ½æœ‰éƒ¨åˆ†å‰æ™¯è¢«å½’ç±»ä¸ºèƒŒæ™¯ï¼Œæ‰€ä»¥ç”¨è¿™ä¸ªscoreä½œä¸ºreliability measureï¼Œç»™æ ‡è®°ä¸ºèƒŒæ™¯æ¡†çš„cls lossè¿›è¡ŒåŠ æƒ</li></ul></li><li>reliability measure</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>overview </p><p> <img src="/2021/08/12/soft-teacher/overview.png" width="80%"></p><ul><li>ä¸¤ä¸ªmodelï¼šstudentå’Œteacher</li><li>teacher modelç”¨æ¥ç”Ÿæˆpseudo labelsï¼štwo set of pseudo boxesï¼Œä¸€ä¸ªç”¨äºclass branchï¼Œä¸€ä¸ªç”¨äºregression branch</li><li>student modelç”¨supervised&amp;unsupervised sampleçš„lossæ¥æ›´æ–°</li><li>teacher modelç”¨student modelçš„EMAæ¥æ›´æ–°</li><li>two crucial designs<ul><li>soft teacher</li><li>box jittering</li></ul></li><li>æ•´ä½“çš„å·¥ä½œæµç¨‹å°±æ˜¯ï¼Œæ¯ä¸ªtraining iterationï¼Œå…ˆæŒ‰ç…§ä¸€å®šæ¯”ä¾‹æŠ½å–labeled&amp;unlabeled sampleæ„æˆdata batchï¼Œç„¶åç”¨teacher modelç”Ÿæˆunlabeled dataçš„pseudo labelï¼ˆthousands of box candidates+NMS+score filterï¼‰ï¼Œç„¶åå°†å…¶ä½œä¸ºunlabeled sampleçš„ground truthï¼Œè®­ç»ƒstudent modelï¼Œoverall lossæ˜¯supervised losså’Œunsupervised lossçš„åŠ æƒå’Œ</li><li>åœ¨è®­ç»ƒå¼€å§‹é˜¶æ®µï¼Œä¸¤ä¸ªæ¨¡å‹éƒ½æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œteacheræ¨¡å‹éšç€studentæ¨¡å‹çš„æ›´æ–°è€Œæ›´æ–°</li><li>FixMatchï¼š<ul><li>è¾“å…¥ç»™teacheræ¨¡å‹çš„æ ·æœ¬ä½¿ç”¨weak aug</li><li>è¾“å…¥ç»™studentæ¨¡å‹çš„æ ·æœ¬ä½¿ç”¨strong aug</li></ul></li></ul></li><li><p>soft teacher</p><ul><li><p>detectorçš„pseudo-labelè´¨é‡å¾ˆé‡è¦</p></li><li><p>æ‰€ä»¥ç”¨score thresh=0.9å»å®šä¹‰box candidatesçš„å‰/èƒŒæ™¯</p></li><li><p>ä½†æ˜¯è¿™æ—¶å€™å¦‚æœç”¨ä¼ ç»Ÿçš„IoUæ¥å®šä¹‰student modelçš„box candidatesçš„pos/negï¼Œä¼šæœ‰ä¸€éƒ¨åˆ†å‰æ™¯æ¡†è¢«å½“ä½œèƒŒæ™¯</p></li><li><p>to alleviate </p><ul><li>assess the reliability of each student-generated box candidate to be a real background</li><li>given a student-generated box candidateï¼Œç”¨teacher modelçš„detection headå»é¢„æµ‹è¿™ä¸ªæ¡†çš„background score</li></ul></li><li><p>overall unsupervised cls loss</p><script type="math/tex; mode=display">  L_u^{cls} = \frac{1}{N_b^{fg}} \sum_i^{N_b^{fg}} l_{cls} (b_i^{fg}, G_{cls}) + \sum_j^{N_b^{bg}}w_j l_{cls} (b_j^{bg}, G_{cls})\\  w_j = \frac{r_j}{\sum_{k=1}^{N_b^{bg}}r_k}</script><ul><li>$G_{cls}$æ˜¯the set of boxes teacher generated for classificationï¼Œå°±æ˜¯teacher modelé¢„æµ‹çš„top1000ç»è¿‡nmså’Œscore filterä¹‹åçš„boxes</li><li>$b_i^{fg}$æ˜¯student candidatesä¸­è¢«assignä¸ºå‰æ™¯çš„æ¡†ï¼Œ$b_i^{bg}$æ˜¯student candidatesä¸­è¢«assignä¸ºèƒŒæ™¯çš„æ¡†ï¼Œassignçš„åŸåˆ™å°±æ˜¯score&gt;0.9</li><li>$w_j$æ˜¯å¯¹assignä¸ºèƒŒæ™¯çš„æ¡†çš„åŠ æƒ</li><li>$r_k$æ˜¯reliability scoreï¼Œæ˜¯student modelé€šè¿‡hard score thresh assignä¸ºèƒŒæ™¯çš„æ¡†ï¼Œç”¨teacher modelçš„detection headå»é¢„æµ‹çš„bg score</li></ul></li></ul></li><li><p>box jittering</p><ul><li><p>fg score threshå’Œbox iouå¹¶ä¸å‘ˆç°strong positive correlationï¼Œè¯´æ˜åŸºäºè¿™ä¸ªåŸåˆ™äº§ç”Ÿçš„æ¡†pseudo-labelså¹¶ä¸ä¸€å®šé€‚åˆbox regression</p></li><li><p>localization reliabilityï¼š</p><ul><li>è¡¡é‡ä¸€ä¸ªpseudo boxçš„consistency</li><li>given a pseudo boxï¼Œsampleä¸€ç³»åˆ—jitter box around itï¼Œå†ç”¨teacher modelå»é¢„æµ‹è¿™äº›jitter boxå¾—åˆ°refined boxes</li><li>refined boxå’Œpseudo boxçš„varianceè¶Šå°ï¼Œè¯´æ˜è¿™ä¸ªæ¡†çš„localization reliabilityè¶Šé«˜</li></ul><script type="math/tex; mode=display">\hat b_i = refine(jitter(b_i))\\\overline \sigma_i = \frac{1}{4} \sum_1^4 \hat \sigma_k\\\hat \sigma_k = \frac{\sigma_k}{0.5 (h(b_i)+w(b_i))}</script><ul><li>$\hat b_i$æ˜¯refined boxes</li><li>$\sigma_k$æ˜¯refine boxesçš„å››ä¸ªåæ ‡åŸºäºåŸå§‹boxçš„æ ‡å‡†å·®</li><li>$\hat \sigma_k$æ˜¯ä¸Šé¢é‚£ä¸ªæ ‡å‡†å·®åŸºäºåŸå§‹boxçš„å°ºåº¦è¿›è¡Œå½’ä¸€åŒ–</li><li>$\overline\sigma$æ˜¯refine boxeså››ä¸ªåæ ‡çš„normed stdçš„å¹³å‡å€¼</li><li>åªè®¡ç®—teacher box candidatesé‡Œé¢ï¼Œfg score&gt;0.5çš„é‚£éƒ¨åˆ†</li></ul></li><li><p>overall unsupervised reg loss</p><script type="math/tex; mode=display">  L_u^{reg} = \frac{1}{N_b^{fg}} \sum_1^{N_b^{fg}} l_{reg} (b_i^{fg}, G_{reg})</script><ul><li>$b_i^{fg}$æ˜¯student candidatesä¸­è¢«assignä¸ºå‰æ™¯çš„æ¡†ï¼Œå³cls score&gt;0.9é‚£äº›é¢„æµ‹æ¡†</li><li>$G_{cls}$æ˜¯the set of boxes teacher generated for regressionï¼Œå°±æ˜¯jittered reliabilityå¤§äºä¸€å®šé˜ˆå€¼çš„candidates</li></ul></li></ul></li><li><p>overall unsupervised lossï¼šcls losså’Œreg lossä¹‹å’Œï¼Œç„¶åç”¨æ ·æœ¬æ•°è¿›è¡Œnorm</p></li></ul></li><li><p>å®éªŒ</p></li></ol>]]></content>
      
      
        <tags>
            
            <tag> semi-supervised, object detection </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>GNN&amp;GCN</title>
      <link href="/2021/07/13/GNN-GCN/"/>
      <url>/2021/07/13/GNN-GCN/</url>
      <content type="html"><![CDATA[<h2 id="ç»¼è¿°"><a href="#ç»¼è¿°" class="headerlink" title="ç»¼è¿°"></a>ç»¼è¿°</h2><ol><li><p>referenceï¼š<a href="https://www.cnblogs.com/siviltaram/p/graph_neural_network_2.html" target="_blank" rel="noopener">https://www.cnblogs.com/siviltaram/p/graph_neural_network_2.html</a></p></li><li><p>key concepts</p><ul><li>å›¾ç¥ç»ç½‘ç»œï¼ˆGraph Neural Networkï¼ŒGNNï¼‰</li><li>å›¾å·ç§¯ç¥ç»ç½‘ç»œï¼ˆGraph Convolutional Neural Networkï¼‰</li><li>é¢‘åŸŸï¼ˆSpectral-domainï¼‰</li><li>ç©ºåŸŸï¼ˆSpatial-domainï¼‰</li></ul></li><li><p>å›¾ç¥ç»ç½‘ç»œ</p><ul><li><p>image &amp; graph</p><p>  <img src="/2021/07/13/GNN-GCN/image.png" width="45%;"></p></li><li><p>èŠ‚ç‚¹ï¼ˆNodeï¼‰</p><ul><li>æ¯ä¸ªèŠ‚ç‚¹æœ‰å…¶ç‰¹å¾ï¼Œç”¨$x_v$è¡¨ç¤º</li></ul></li><li><p>è¾¹ï¼ˆEdgeï¼‰</p><ul><li>è¿æ¥ä¸¤ä¸ªèŠ‚ç‚¹çš„è¾¹ä¹Ÿæœ‰å…¶ç‰¹å¾ï¼Œç”¨$x_{v,u}$è¡¨ç¤º</li></ul></li><li><p>éšè—çŠ¶æ€</p><ul><li>å›¾çš„å­¦ä¹ ç›®æ ‡æ˜¯è·å¾—æ¯ä¸ªèŠ‚ç‚¹çš„éšè—çŠ¶æ€</li></ul></li><li><p>å±€éƒ¨è¾“å‡ºå‡½æ•°</p><ul><li>é€‰å–ä¸€ä¸ªèŠ‚ç‚¹</li></ul></li></ul></li><li><p>å›¾å·ç§¯</p><ul><li><p>ä¸€å¼ å›¾ç‰‡å°±å¯ä»¥çœ‹ä½œä¸€ä¸ªéå¸¸ç¨ å¯†çš„å›¾ï¼Œé˜´å½±éƒ¨åˆ†ä»£è¡¨å·ç§¯æ ¸ï¼Œå³ä¾§åˆ™æ˜¯ä¸€ä¸ªæ™®é€šçš„å›¾ï¼Œå’Œå›¾å·ç§¯æ ¸</p><ul><li>åœ¨imageä¸ºä»£è¡¨çš„æ¬§å¼ç©ºé—´ä¸­ï¼Œç»“ç‚¹çš„é‚»å±…æ•°é‡éƒ½æ˜¯å›ºå®šçš„ï¼Œä½†åœ¨graphè¿™ç§éæ¬§ç©ºé—´ä¸­ï¼Œç»“ç‚¹æœ‰å¤šå°‘é‚»å±…å¹¶ä¸å›ºå®š</li><li><p>ä¼ ç»Ÿçš„å·ç§¯æ ¸ä¸èƒ½ç›´æ¥ç”¨äºæŠ½å–å›¾ä¸Šç»“ç‚¹çš„ç‰¹å¾</p><p><img src="/2021/07/13/GNN-GCN/gcn.png" width="45%;"></p></li></ul></li><li><p>ä¸¤ä¸ªä¸»æµæ€è·¯</p><ul><li>æŠŠéæ¬§ç©ºé—´çš„å›¾è½¬æ¢æˆæ¬§å¼ç©ºé—´ï¼Œç„¶åä½¿ç”¨ä¼ ç»Ÿå·ç§¯</li><li>æ‰¾å‡ºä¸€ç§å¯å¤„ç†å˜é•¿é‚»å±…ç»“ç‚¹çš„å·ç§¯æ ¸åœ¨å›¾ä¸ŠæŠ½å–ç‰¹å¾</li></ul></li><li></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> å›¾ç¥ç»ç½‘ç»œï¼Œå›¾å·ç§¯ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>few-shot</title>
      <link href="/2021/06/22/few-shot/"/>
      <url>/2021/06/22/few-shot/</url>
      <content type="html"><![CDATA[<h2 id="ç»¼è¿°"><a href="#ç»¼è¿°" class="headerlink" title="ç»¼è¿°"></a>ç»¼è¿°</h2><ol><li><p>few-shot</p><ul><li>few-shot learningï¼šé€šè¿‡å°‘é‡æ ·æœ¬å­¦ä¹ è¯†åˆ«æ¨¡å‹</li><li>é—®é¢˜ï¼šè¿‡æ‹Ÿåˆ&amp;æ³›åŒ–æ€§ï¼Œæ•°æ®å¢å¼ºå’Œæ­£åˆ™èƒ½åœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£ä½†ä¸è§£å†³ï¼Œè¿˜æ˜¯æ¨èä»å¤§æ•°æ®ä¸Šè¿ç§»å­¦ä¹ </li><li>å…±è¯†ï¼š<ul><li>æ ·æœ¬é‡æœ‰é™çš„æƒ…å†µä¸‹ï¼Œä¸ä¾é å¤–éƒ¨æ•°æ®å¾ˆéš¾å¾—åˆ°ä¸é”™çš„ç»“æœï¼Œå½“ä¸‹æ‰€æœ‰çš„è§£å†³æ–¹æ¡ˆéƒ½æ˜¯å€ŸåŠ©å¤–éƒ¨æ•°æ®ä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼Œæ„é€ å­¦ä¹ ä»»åŠ¡</li><li>è¿ç§»æ•°æ®ä¹Ÿä¸æ˜¯éšä¾¿æ‰¾çš„ï¼Œæ•°æ®é›†çš„domain differenceè¶Šå¤§ï¼Œè¿ç§»æ•ˆæœè¶Šå·®ï¼ˆe.g. ç”¨miniImagenetåšç±»é—´è¿ç§»ï¼Œæ•ˆæœä¸é”™ï¼Œä½†æ˜¯ç”¨miniImagenetåšbase classç”¨CUBåšnovel classï¼Œå­¦ä¹ æ•ˆæœä¼šæ˜æ˜¾ä¸‹é™ï¼‰</li></ul></li><li>æ•°æ®é›†ï¼š<ul><li>miniImagenetï¼šè‡ªç„¶å›¾åƒï¼Œ600å¼ ï¼Œ100ç±»</li><li>Omniglotï¼šæ‰‹å†™å­—ç¬¦ï¼Œ1623å¼ ï¼Œ50ç±»</li><li>CUBï¼šé¸Ÿé›†ï¼Œ11788å¼ ï¼Œ200ç±»ï¼Œå¯ç”¨äºç»†ç²’åº¦ï¼Œå¯ä»¥ç”¨äºzero-shot</li></ul></li></ul></li><li><p>methods</p><ul><li><p>pretraining + finetuning</p><ul><li>pretrainingé˜¶æ®µç”¨base classè®­ç»ƒä¸€ä¸ªfeature extractor</li><li><p>finetuningé˜¶æ®µfix feature extractoré‡æ–°è®­ç»ƒä¸€ä¸ªclassifier</p><p><img src="/2021/06/22/few-shot/pretraining.png" width="60%;"></p></li></ul></li><li><p>åŸºäºåº¦é‡å­¦ä¹ </p><ul><li>å¼•å…¥distance metricå…¶å®éƒ½ç®—åº¦é‡å­¦ä¹ ï¼Œæ‰€ä»¥ä¸Šé¢ï¼ˆpretraining+finetuningï¼‰å’Œä¸‹é¢ï¼ˆmeta learningï¼‰çš„æ–¹æ³•éƒ½æœ‰å±äºåº¦é‡å­¦ä¹ çš„æ–¹æ³•</li></ul></li><li><p>åŸºäºå…ƒå­¦ä¹ </p><p><img src="/2021/06/22/few-shot/metalearning.png" width="60%;"></p><ul><li>base class&amp;novel classï¼šbase classæ˜¯å·²æœ‰çš„å¤§æ•°æ®é›†ï¼Œå¤šç±»åˆ«ï¼Œå¤§æ ·æœ¬é‡ï¼Œnovel classæ˜¯æˆ‘ä»¬è¦è§£å†³çš„å°æ•°æ®é›†ï¼Œç±»åˆ«å°‘ï¼Œæ¯ç±»æ ·æœ¬ä¹Ÿå°‘</li><li>N-way-K-shotï¼šåŸºäºnovel classå…ˆåœ¨base classä¸Šæ„å»ºå¤šä¸ªå­ä»»åŠ¡ï¼ŒN-wayå°±æ˜¯æ„å»ºéšæœºNä¸ªç±»åˆ«çš„åˆ†ç±»ä»»åŠ¡ï¼ŒK-shotå°±æ˜¯æ¯ä¸ªç±»åˆ«å¯¹åº”æ ·æœ¬é‡ä¸ºK</li><li><p>supportset S &amp; queryset Qï¼šN-way-K-shotçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œæ¥è‡ªbase classä¸­ç›¸åŒçš„ç±»åˆ«ï¼Œå‡ç”¨äºtraining procedure</p></li><li><p>ä¸ä¼ ç»Ÿåˆ†ç±»ä»»åŠ¡å¯¹æ¯”ï¼š</p><p><img src="/2021/06/22/few-shot/meta1.jpeg" width="45%;">        <img src="/2021/06/22/few-shot/meta2.jpeg" width="45%;"></p></li></ul></li><li><p>leaderboardï¼š<a href="https://few-shot.yyliu.net/miniimagenet.html" target="_blank" rel="noopener">https://few-shot.yyliu.net/miniimagenet.html</a></p></li></ul></li><li><p>papers</p><ul><li>[2015 siamese]ï¼šSiamese Neural Networks for One-shot Image Recognitionï¼Œæ ¸å¿ƒæ€æƒ³å°±æ˜¯åŸºäºå­ªç”Ÿç½‘ç»œæ„å»ºsimilarityä»»åŠ¡ï¼Œç”¨ä¸€ä¸ªå¤§æ•°æ®é›†æ„é€ çš„same/diff pairså»è®­ç»ƒï¼Œç„¶åç›´æ¥ç”¨åœ¨novel setä¸Šï¼Œmetricæ˜¯reweighted L1</li><li>[2016 MatchingNet]ï¼šMatching Networks for One Shot Learningï¼Œæœ¬è´¨ä¸Šä¹Ÿæ˜¯å­ªç”Ÿç½‘ç»œ+metric learningï¼Œç›‘ç£çš„æ˜¯support set Så’Œtest set Bçš„ç›¸ä¼¼åº¦â€”â€”åœ¨Sä¸‹è®­ç»ƒçš„æ¨¡å‹åœ¨Bçš„é¢„æµ‹ç»“æœè¯¯å·®æœ€å°ï¼Œç½‘ç»œä¸Šçš„åˆ›æ–°æ˜¯ç”¨äº†memory&amp;attentionï¼Œtrain procedureçš„åˆ›æ–°åœ¨äºâ€œtest and train conditions must match N-way-K-shotâ€ï¼Œ</li><li>[2017 ProtoNet]ï¼šPrototypical Networks for Few-shot Learningï¼Œ</li><li>[2019 few-shotç»¼è¿°]ï¼šA CLOSER LOOK AT FEW-SHOT CLASSIFICATION</li></ul></li></ol><h2 id="Siamese-Neural-Networks-for-One-shot-Image-Recognition"><a href="#Siamese-Neural-Networks-for-One-shot-Image-Recognition" class="headerlink" title="Siamese Neural Networks for One-shot Image Recognition"></a>Siamese Neural Networks for One-shot Image Recognition</h2><ol><li><p>åŠ¨æœº</p><ul><li>learning good features is expensive</li><li>when little data is availableï¼šä¸€ä¸ªå…¸å‹ä»»åŠ¡one-shot learning</li><li>we desire<ul><li>generalize to the new distribution without extensive retraining</li></ul></li><li>we propose<ul><li>train a siamese network to rank similarity between inputs</li><li>capitalize on powerful discriminative features</li><li>generalize the network to new data/new classes</li></ul></li><li>experiment on<ul><li>character recognition</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>general strategy</p><ul><li>learn image representationï¼šsupervised metric-based approachï¼Œsiamese neural network</li><li><p>reuse the feature extractorï¼šon new dataï¼Œwithout any retraining</p><p><img src="/2021/06/22/few-shot/siamese.png" width="45%;"></p></li></ul></li><li><p>why siamese</p><ul><li>we hypothesize that networks which do well at verification tasks should generalize to one-shot classification</li></ul></li><li><p>siamese nets</p><ul><li>twin networks accept distinct inputs that are joined by an energy function at the top</li><li>twin back shares the weightsï¼šsymmetric</li><li>åŸå§‹è®ºæ–‡ç”¨äº†contrastive energy functionï¼šcontains dual terms to increase like-pairs energy &amp; decrease unlike-pairs energy</li><li>in this paper we use weighted L1 + sigmoid</li></ul></li><li><p>model</p><ul><li><p>conv-relu-maxpoolingï¼šconv of varying sizes</p></li><li><p>æœ€åä¸€ä¸ªconv-reluå®Œäº†æ¥flatten-fc-sigmoidå¾—åˆ°å½’ä¸€åŒ–çš„feature vector</p></li><li><p>ç„¶åæ˜¯joined layerï¼šè®¡ç®—ä¸¤ä¸ªfeature vectorçš„L1 distanceålearnable reweighting</p><p>  <img src="/2021/06/22/few-shot/reweighting.png" width="45%;"></p></li><li><p>ç„¶åæ¥sigmoid</p><p><img src="/2021/06/22/few-shot/architecture.png" width="85%;"></p></li></ul></li><li><p>loss</p><ul><li>binary classifier</li><li>regularized CE</li><li>loss functioné‡Œé¢åŠ äº†layer-wise-L2æ­£åˆ™</li><li>bpçš„æ—¶å€™ä¸¤ä¸ªå­ªç”Ÿç½‘ç»œçš„bp gradientæ˜¯additiveçš„</li></ul></li><li><p>weight initialization</p><ul><li>conv weightsï¼šmean 0 &amp; std var 0.01</li><li>conv biasï¼šmean 0.5 &amp; std var 0.01</li><li>fc weightsï¼šmean 0 &amp; std var 0.2</li><li>fc biasï¼šmean 0.5 &amp; std var 0.01</li></ul></li><li><p>learning schedule</p><ul><li>uniform lr decay 0.01</li><li>individual lr rate &amp; momentum</li><li>annealing</li></ul></li><li><p>augmentation</p><ul><li>individual affine distortions</li><li>æ¯ä¸ªaffine paramçš„probability 0.5</li></ul></li></ul></li></ol><pre><code>  &lt;img src=&quot;few-shot/affine.png&quot; width=&quot;45%;&quot; /&gt;</code></pre><ol><li><p>å®éªŒ</p><ul><li>dataset<ul><li>Omniglotï¼š50ä¸ªå­—æ¯ï¼ˆinternational/lesser known/fictitiousï¼‰</li><li>è®­ç»ƒç”¨çš„å­é›†ï¼š60% of the total dataï¼Œ12ä¸ªdraweråˆ›å»ºçš„30ä¸ªå­—æ¯ï¼Œæ¯ç±»æ ·æœ¬æ•°ä¸€æ ·å¤š</li><li>validationï¼š4ä¸ªdrawerçš„10ä¸ªå­—æ¯</li><li>testï¼š4ä¸ªdrawerçš„10ä¸ªå­—æ¯</li><li>8ä¸ªaffine transformsï¼š9å€æ ·æœ¬é‡ï¼Œsame&amp;different pairs</li><li>åœ¨ä¸ç»è¿‡å¾®è°ƒè®­ç»ƒçš„æ¡ä»¶ä¸‹ï¼Œæ¨¡å‹ç›´æ¥åº”ç”¨åœ¨MNISTæ•°æ®é›†ï¼Œä»æœ‰70%çš„å‡†ç¡®ç‡ï¼šæ³›åŒ–èƒ½åŠ›</li></ul></li></ul></li><li><p>è¯„ä»·</p><ul><li>å­ªç”Ÿç½‘ç»œå¯¹äºä¸¤ä¸ªå›¾åƒä¹‹é—´çš„å·®å¼‚æ˜¯éå¸¸æ•æ„Ÿçš„<ul><li>ä¸€åªé»„è‰²çš„çŒ«å’Œé»„è‰²çš„è€è™ä¹‹é—´çš„å·®åˆ«è¦æ¯”ä¸€åªé»„è‰²çš„çŒ«å’Œé»‘è‰²çš„çŒ«ä¹‹é—´çš„å·®åˆ«æ›´å°</li><li>ä¸€ä¸ªç‰©ä½“å‡ºç°åœ¨å›¾åƒçš„å·¦ä¸Šè§’å’Œå›¾åƒçš„å³ä¸‹è§’æ—¶å…¶æå–åˆ°çš„ç‰¹å¾ä¿¡æ¯å¯èƒ½æˆªç„¶ä¸åŒ</li><li>å°¤å…¶ç»è¿‡å…¨è¿æ¥å±‚ï¼Œç©ºé—´ä½ç½®ä¿¡æ¯è¢«ç ´å</li></ul></li><li>æ‰‹å†™å­—ç¬¦æ•°æ®é›†ç›¸æ¯”è¾ƒäºImageNetå¤ªç®€å•äº†<ul><li>ä¼˜åŒ–ç½‘ç»œç»“æ„ï¼šMatchingNet</li><li>æ›´å¥½çš„è®­ç»ƒç­–ç•¥ï¼šmeta learning</li></ul></li><li>ç°åœ¨å»å¤ç°å·²ç»æ²¡å•¥æ„ä¹‰ï¼Œç®—æ˜¯metric learningåœ¨å°æ ·æœ¬å­¦ä¹ ä¸Šçš„ä¸€ä¸ªstartupå§</li></ul></li></ol><h2 id="MatchingNet-Matching-Networks-for-One-Shot-Learning"><a href="#MatchingNet-Matching-Networks-for-One-Shot-Learning" class="headerlink" title="MatchingNet: Matching Networks for One Shot Learning"></a>MatchingNet: Matching Networks for One Shot Learning</h2><ol><li><p>åŠ¨æœº</p><ul><li>learning new concepts rapidly from little data</li><li>employ ideas<ul><li>metric learning</li><li>memory cell</li></ul></li><li>define one-shot learning problems<ul><li>Omniglot</li><li>ImageNet</li><li>language tasks</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>parametric models learns slow and require large datasets</li><li>non-parametric models rapidly assimilate new examples</li><li>we aim to incorporate both</li><li>we propose Matching Nets<ul><li>uses recent advances in attention and memory that enable rapid learning</li><li>test and train conditions must matchï¼šå¦‚æœè¦æµ‹è¯•ä¸€ä¸ªnç±»çš„æ–°åˆ†å¸ƒï¼Œå°±è¦åœ¨mç±»å¤§æ•°æ®é›†ä¸Šè®­ç±»ä¼¼çš„minibatchâ€”â€”æŠ½nä¸ªç±»ï¼Œæ¯ç±»show a few examples</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>build one-shot learning within the set-to-set framework</p><ul><li><p>è®­ç»ƒä»¥åçš„æ¨¡å‹ä¸éœ€è¦è¿›ä¸€æ­¥tuningå°±èƒ½produce sensible test labels for unobserved classes</p></li><li><p>given a small support set $S=\{(x_i,y_i)\}^k_{i=0}$</p></li><li><p>train a classifier $c_S$</p></li><li><p>given a test example $\hat x$ï¼šwe get a probability distribution $\hat y=c_S(\hat x)$</p></li><li><p>define the mappingï¼š$S \rightarrow c_S $ to be $P(\hat y| \hat x ,S)$</p></li><li><p>when given a new support set $S^{â€˜}=\{\hat x\}$ï¼šç›´æ¥ç”¨æ¨¡å‹På»é¢„æµ‹$\hat y$å°±å¯ä»¥äº†</p></li><li><p>simplest formï¼š</p><script type="math/tex; mode=display">  \hat y = \sum_{i=1}^k a(\hat x, x_i)y_i</script><ul><li>aæ˜¯attention mechanismï¼šå¦‚æœå’Œæµ‹è¯•æ ·æœ¬$\hat x$æœ€è¿œçš„bä¸ªæ”¯æŒæ ·æœ¬$x_i$çš„attentionæ˜¯0ï¼Œå…¶ä½™ä¸ºä¸€ä¸ªå®šå€¼ï¼Œè¿™å°±ç­‰ä»·äºä¸€ä¸ªk-b-NNæœºåˆ¶</li><li>$y_i$ act as memoriesï¼šå¯ä»¥æŠŠæ¯ä¸ª$y_i$çœ‹ä½œæ˜¯æ¯ä¸ª$x_i$æå–åˆ°çš„ä¿¡æ¯ä¿å­˜æˆmemory</li><li><p>workflowå®šä¹‰ï¼šgiven a inputï¼Œæˆ‘ä»¬åŸºäºattentioné”å®šcorresponding samples in the support setï¼Œå¹¶retrieve the label</p><p><img src="/2021/06/22/few-shot/formulation.png" width="55%;"></p></li></ul></li></ul></li><li><p>attention kernel</p><ul><li>ç”¨ä¸€ä¸ªembedding functionå…ˆå°†$\hat x$å’Œ$x_i$è½¬åŒ–æˆembeddings</li><li>ç„¶åè®¡ç®—å’Œæ¯ä¸ª$x_i$ embeddingçš„cosine distance</li><li>ç„¶åsoftmaxï¼Œå¾—åˆ°æ¯ä¸ªçš„attention value</li><li>softmaxä¹‹åçš„attention valueï¼Œå¤§éƒ¨åˆ†æ˜¯Né€‰1ï¼Œå¦‚æœæ¯ä¸ªattention valueéƒ½ä¸é«˜ï¼Œè¯´æ˜query sampleå’Œè®­ç»ƒé›†æ¯ç±»éƒ½ä¸åƒï¼Œæ˜¯ä¸ªnovel</li></ul></li><li><p>Full Context Embeddingsï¼ˆFCEï¼‰</p><ul><li><p>ç®€å•çš„æ¨¡å¼ä¸‹få’Œgå°±æ˜¯ä¸¤ä¸ªshared weightsçš„CNN feature extractorï¼ŒFCEæ˜¯æ¥åœ¨å¸¸è§„feature vectoråé¢ç²¾å¿ƒè®¾è®¡çš„ä¸€ä¸ªç»“æ„</p></li><li><p>è®¾è®¡æ€è·¯</p><ul><li>gï¼šsupport set donâ€™t get embedded individually</li><li>fï¼šsupport set modify how we embed the test image</li></ul></li><li><p>the first issueï¼š</p><ul><li><p>bidirectional Long-Short Term Memory</p></li><li><p>encoder the whole support set as contextsï¼Œeach time stepçš„è¾“å…¥æ˜¯$g^{â€˜}(x_i)$</p><p>  <img src="/2021/06/22/few-shot/biLSTM.png" width="35%;"></p></li><li><p>skip connection</p><script type="math/tex; mode=display">  g(x_i) = \overrightarrow{h_i}+\overleftarrow{h_i}+g^{'}(x_i)</script></li></ul></li><li><p>the second issue</p><ul><li><p>LSTM with read attention over the whole set S</p></li><li><p>$f(\hat x, S)=attLSTM(f^{â€˜}(\hat x), g(S), K)$</p><p>  <img src="/2021/06/22/few-shot/FCE.png" width="55%;"></p></li><li><p>$f^{â€˜}(\hat x)$æ˜¯query sampleçš„feature vectorï¼Œä½œä¸ºLSTM each time stepçš„è¾“å…¥</p></li><li><p>$K$æ˜¯fixed number of unrolling stepsï¼Œé™åˆ¶LSTMè®¡ç®—çš„stepï¼Œä¹Ÿå°±æ˜¯feature vectorå‚ä¸LSTMå¾ªç¯è®¡ç®—çš„æ¬¡æ•°ï¼Œæœ€ç»ˆçš„è¾“å‡ºæ˜¯$h_K$</p></li><li><p>skip connection as above</p></li><li><p>support set Sçš„å¼•å…¥ï¼š</p><ul><li>content based attention + softmax</li><li>$r_{k-1}$å’Œ$h_{k-1}$æ˜¯concatåˆ°ä¸€èµ·ï¼Œä½œä¸ºhidden statesï¼šã€QUESTIONã€‘è¿™æ ·lstm cellçš„hidden sizeå°±å˜äº†å•Šï¼Ÿï¼Ÿï¼Ÿ</li></ul></li></ul></li><li><p>attention of K fixed unrolling steps</p></li><li><p>encode $x_i$ in the context of the support set S</p></li></ul></li><li><p>training strategy</p><ul><li>the training procedure has to be chosen carefully so as to match the never seen</li><li>task defineï¼šä»å…¨é›†ä¸­é€‰å–few unique classes(e.g. 5)ï¼Œæ¯ä¸ªç±»åˆ«é€‰å–few examples(e.g. 1-5)ï¼Œæ„æˆsupport set Sï¼Œå†ä»å¯¹åº”ç±»åˆ«æŠ½ä¸€ä¸ªbatch Bï¼Œè®­ç»ƒç›®æ ‡å°±æ˜¯minimise the error predicting the labels in the batch B conditioned on the support set S</li><li><p>batch Bçš„é¢„æµ‹è¿‡ç¨‹å°±æ˜¯figure1ï¼šéœ€è¦$g(S(x_i,y_i))$å’Œ$f(\hat x)$è®¡ç®—$P(\hat y|\hat x, S)$ï¼Œç„¶åå’Œ$gt(\hat y)$è®¡ç®—log loss</p><p><img src="/2021/06/22/few-shot/matchingnetloss.png" width="55%;"></p></li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li><p>æ¨¡å¼</p><ul><li>N-way-K-shot train</li><li>one-shot testï¼šç”¨å”¯ä¸€çš„one-shot novel sampleç”Ÿæˆå¯¹åº”ç±»åˆ«çš„feature vectorï¼Œç„¶åå¯¹æ¯ä¸ªtest sampleè®¡ç®—cosine distanceï¼Œé€‰æ‹©æœ€è¿‘çš„ä½œä¸ºå…¶ç±»åˆ«</li></ul></li><li><p>comparing methods</p><ul><li>baseline classifier + NN</li><li>MANN</li><li>Convolutional Siamese Net + NN</li><li>further finetuningï¼šone-shot</li></ul></li><li>ç»“è®º<ul><li>using more examples for k-shot classification helps all models</li><li>5-way is easier than 20-way</li><li>siamese netåœ¨5-shotçš„æ—¶å€™è·Ÿour methodå·®ä¸å¤šï¼Œä½†æ˜¯one-shot degrades rapidly </li><li>FCEåœ¨ç®€å•æ•°æ®é›†ï¼ˆOmniglotï¼‰ä¸Šæ²¡å•¥ç”¨ï¼Œåœ¨harder taskï¼ˆ<em>mini</em>ImageNetï¼‰æ˜¾è‘—æå‡</li></ul></li></ul></li></ol><h2 id="A-CLOSER-LOOK-AT-FEW-SHOT-CLASSIFICATION"><a href="#A-CLOSER-LOOK-AT-FEW-SHOT-CLASSIFICATION" class="headerlink" title="A CLOSER LOOK AT FEW-SHOT CLASSIFICATION"></a>A CLOSER LOOK AT FEW-SHOT CLASSIFICATION</h2><ol><li><p>åŠ¨æœº</p><ul><li>ä¸ºä¸»æµæ–¹æ³•æä¾›ä¸€ä¸ªconsistent comparative analysisï¼Œå¹¶ä¸”å‘ç°ï¼š<ul><li>deeper backbones significantly reduce differences</li><li>reducing intra-class variation is an important factor when shallow backbone</li></ul></li><li>propose a modified baseline method<ul><li>achieves com- petitive performance</li><li>verified on miniImageNet &amp; CUB</li></ul></li><li>in realistic cross-domain settings<ul><li>generalization analysis</li><li>baseline method with standard fine-tuning win</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>three main categories of methods</p><ul><li>initialization based<ul><li>aims to learn good model initialization</li><li>to achieve rapid adaption with a limited number of training samples</li><li>have difficulty in handling domain shifts</li></ul></li><li>metric learning based<ul><li>è®­ç»ƒç›®æ ‡æ˜¯learn to compare</li><li>if a model can determine the similarity of two images, it can classify an unseen input image with the labeled instancesï¼šæœ¬è´¨æ˜¯similarityè®¡ç®—å™¨ï¼Œè„±ç¦»label level</li><li>èŠ±å¼è®­ç»ƒç­–ç•¥ï¼šmeta learning/graph</li><li>èŠ±å¼è·ç¦»metricï¼šcosine/Euclidean</li><li>turns outå¤§å¯ä¸å¿…ï¼š<ul><li>a simple baseline method with a distance- based classifier is competitive to the sophisticated algorithms</li><li>simply reducing intra-class variation in a baseline method leads to competitive performance</li></ul></li></ul></li><li>hallucination based<ul><li>ç”¨base classè®­ç»ƒä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼Œç„¶åç”¨ç”Ÿæˆæ¨¡å‹ç»™novel classé€ å‡æ•°æ®</li><li>é€šå¸¸å’Œmetric-basedæ¨¡å‹ç»“åˆèµ·æ¥ç”¨ï¼Œä¸å•ç‹¬åˆ†æ</li></ul></li></ul></li><li><p>two main challenges æ²¡æ³•ç»Ÿä¸€æ¨ªå‘æ¯”è¾ƒ</p><ul><li>implementation detailsæœ‰å·®å¼‚ï¼Œbaseline approachè¢«under-estimatedï¼šæ— æ³•å‡†ç¡®é‡åŒ–the relative performance gain</li><li>lack of domain shift between base &amp; novel datasetsï¼šmakes the evaluation scenarios unrealistic</li></ul></li><li>our work<ul><li>é’ˆå¯¹ä»£è¡¨æ€§æ–¹æ³•conduct consistent comparative experiments on common ground<ul><li>discoveries on deeper backbones</li></ul></li><li>è½»å¾®æ”¹åŠ¨baseline methodè·å¾—æ˜¾è‘—æå‡<ul><li>replace the linear classifier with distance-based classifier</li></ul></li><li>practical sceneries with domain shift<ul><li>å‘ç°è¿™ç§ç°å®åœºæ™¯ä¸‹ï¼Œé‚£äº›ä»£è¡¨æ€§çš„few-shot methodsåè€Œå¹²ä¸è¿‡baseline method</li></ul></li><li>open source codeï¼š<a href="https://github.com/wyharveychen/CloserLookFewShot" target="_blank" rel="noopener">https://github.com/wyharveychen/CloserLookFewShot</a></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li>baseline<ul><li>standard transfer learningï¼špre-training + fine-tuning</li><li>training stage<ul><li>train a feature extractor $f_{\theta}$ and a classifier $C_{W_b}$</li><li>use abundant base class labeled data</li><li>standard CE loss</li></ul></li><li>fine-tuning stage<ul><li>fix feature extractor $f_{\theta}$</li><li>train a new classifier $C_{W_n}$</li><li>use the few labeled novel samples</li><li>standard CE loss</li></ul></li></ul></li><li>baseline++<ul><li>variant of the baselineï¼šå”¯ä¸€çš„ä¸åŒå°±åœ¨äºclassifier design</li><li>æ˜¾å¼åœ°reduce intra-class varation among features during trainingï¼Œå’Œcenter lossæ€è·¯æœ‰ç‚¹åƒï¼Œä½†æ˜¯center lossçš„è´¨å¿ƒæ˜¯æ»‘åŠ¨å¹³å‡çš„ï¼Œè¿™é‡Œé¢çš„è´¨å¿ƒæ˜¯learnableçš„</li><li>training stage<ul><li>write the weight matrix $W_b$ as $[w_1, w_2, â€¦, w_c]$ï¼Œç±»ä¼¼æ¯ç±»çš„ç°‡å¿ƒ</li><li>for an input featureï¼Œcompute cosine similarity</li><li>multiply a class-wise learnable scalar to adjust origin [-1,1] value to fit softmax</li><li>ç„¶åç”¨softmaxå¯¹similarity vectorè¿›è¡Œå½’ä¸€åŒ–ï¼Œä½œä¸ºpredict label</li><li>the softmax function prevents the learned weight vectors collapsing to zerosï¼šæ¯ç±»çš„é¢„æµ‹distanceéƒ½æ˜¯0æ˜¯ç½‘ç»œæ¯”è¾ƒå®¹æ˜“é™·å…¥çš„å±€éƒ¨æœ€ä¼˜è§£</li></ul></li><li>ã€in fine-tuning stageï¼Ÿï¼Ÿã€‘</li></ul></li><li>meta-learning algorithms<ul><li>three distance metric based methodsï¼šMatchingNetï¼ŒProtoNetï¼ŒRelationNet</li><li>one initialization based methodï¼šMAML</li><li>meta-training stage<ul><li>a collection of N-way-K-shot tasks</li><li>ä½¿å¾—æ¨¡å‹$M(*|S)$å­¦ä¼šçš„æ˜¯ä¸€ç§å­¦ä¹ æ¨¡å¼â€”â€”åœ¨æœ‰é™æ•°æ®ä¸‹åšé¢„æµ‹</li></ul></li><li>meta-testing stage<ul><li>æ‰€æœ‰çš„novel dataéƒ½ä½œä¸ºå¯¹åº”ç±»åˆ«çš„support set</li><li>(class mean)</li><li>æ¨¡å‹å°±ç”¨è¿™ä¸ªæ–°çš„support setæ¥è¿›è¡Œé¢„æµ‹</li></ul></li><li>Different meta-learning methodsä¸»è¦åŒºåˆ«åœ¨äºå¦‚ä½•åŸºäºsupport setåšé¢„æµ‹ï¼Œä¹Ÿå°±æ˜¯classifierçš„è®¾è®¡<ul><li>MatchingNetè®¡ç®—çš„æ˜¯queryå’Œsupport setçš„æ¯ä¸ªcosine distanceï¼Œç„¶åmean per class</li><li>ProtoNetæ˜¯å…ˆå¯¹support featuresæ±‚class meanï¼Œç„¶åEuclidean distance</li><li>RelationNetå…ˆå¯¹support featuresæ±‚class meanï¼Œç„¶åå°†è·ç¦»è®¡ç®—æ¨¡å—æ›¿æ¢æˆlearnable relation module</li></ul></li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li><p>three scenarios</p><ul><li>generic object recognitionï¼šmini-ImageNetï¼Œ100ç±»ï¼Œ600å¼ per classï¼Œã€64-baseï¼Œ16-valï¼Œ20-novelã€‘</li><li>fine-grained image classificationï¼šCUB-200-2011ï¼Œ200ç±»ï¼Œæ€»å…±11,788å¼ ï¼Œã€random 100-baseï¼Œ50-valï¼Œ50-novelã€‘</li><li>cross-domain adaptationï¼šmini-ImageNet â€”&gt; CUBï¼Œã€100-mini-ImageNet-baseï¼Œ50-CUB-valï¼Œ50-CUB-testã€‘</li></ul></li><li><p>training details</p><ul><li>baselineå’Œbaseline++æ¨¡å‹ï¼štrain 400 epochsï¼Œbatch size 16</li><li>meta learning methodsï¼š<ul><li>train 60000 episodes for 5-way-1-shot tasksï¼Œtrain 40000 episodes for 5-way-5-shot tasks</li><li>use validation set to select the training episodes with the best acc</li><li>k-shot for support setï¼Œ16 instances for query set</li></ul></li><li>Adam with 1e-3 initial lr</li><li>standard data augmentationï¼šcropï¼Œleft-right flipï¼Œcolor jitter</li></ul></li><li><p>testing stage</p><ul><li>average over 600 experiments</li><li>each experiment randomly choose 5-way-k-shot support set + 16 instances query set</li><li>meta learning methodsç›´æ¥åŸºäºsupport setç»™å‡ºå¯¹query setçš„é¢„æµ‹ç»“æœ</li><li>baseline methodsåŸºäºsupport setè®­ç»ƒä¸€ä¸ªæ–°çš„åˆ†ç±»å¤´ï¼Œ100 iterationsï¼Œbatch size 4</li></ul></li><li><p>æ¨¡å‹details</p><ul><li>baseline++çš„similarityä¹˜ä¸Šäº†class-wise learnable scalar </li><li>MachingNetç”¨äº†FCE classification layer without fine-tuningç‰ˆæœ¬ï¼Œä¹Ÿä¹˜äº†class-wise learnable scalar </li><li>RelationNetå°†L2 normæ›¿æ¢æˆsoftmaxåŠ é€Ÿè®­ç»ƒ</li><li>MAMLä½¿ç”¨äº†ä¸€é˜¶æ¢¯åº¦è¿‘ä¼¼for efficiency</li></ul></li><li><p>åˆæ­¥ç»“æœ</p><ul><li><p>4-layer conv backbone</p></li><li><p>input size 84x84</p></li><li><p>originå’Œre-implementationçš„ç²¾åº¦å¯¹æ¯”</p><ul><li>åŸå§‹çš„baselineæ²¡åŠ data augmentationï¼Œæ‰€ä»¥è¿‡æ‹Ÿåˆäº†ç²¾åº¦å·®ï¼Œè¢«underestimatedäº†</li><li>MatchingNetåŠ äº†é‚£ä¸ªscalar shiftçš„æ”¹è¿›ä»¥åç²¾åº¦æœ‰æ˜¾è‘—æå‡</li><li><p>ProtoNetåŸè®ºæ–‡æ˜¯20-shot&amp;30-shotï¼Œæœ¬æ–‡ä¸»è¦æ¯”è¾ƒ1-shotå’Œ5-shotï¼Œç²¾åº¦éƒ½æ”¾å‡ºæ¥äº†</p><p><img src="/2021/06/22/few-shot/re-implement.png" width="75%;"></p></li></ul></li><li><p>our experiment settingä¸‹å„æ¨¡å‹çš„ç²¾åº¦å¯¹æ¯”</p><ul><li>baseline++å¤§å¹…æå‡ç²¾åº¦ï¼Œå·²ç»è·Ÿmeta learning methodså·®ä¸å¤šäº†</li><li>è¯´æ˜few-shotçš„key factoræ˜¯reduce intra-class variation</li><li><p>ä½†æ˜¯è¦æ³¨æ„çš„æ˜¯è¿™æ˜¯åœ¨4-layer-convçš„backbone settingä¸‹ï¼Œdeeper backbone can inherently reduce intra-class variation</p><p><img src="/2021/06/22/few-shot/cmp.png" width="75%;"></p></li></ul></li><li><p>å¢åŠ ç½‘ç»œæ·±åº¦</p><ul><li>ä¸Šé¢è¯´äº†ï¼Œdeeper backboneèƒ½å¤Ÿéšå¼åœ°é™ä½ç±»å†…è·ç¦»</li><li>deeper models<ul><li>conv4</li><li>conv6ï¼šç›¸å¯¹äºconv4é‚£ä¸ªæ¨¡å‹ï¼ŒåŠ äº†ä¸¤å±‚conv blocks without pooling</li><li>resnet10ï¼šç®€åŒ–ç‰ˆresnet18ï¼Œr18é‡Œé¢conv blockçš„ä¸¤å±‚å·ç§¯æ¢æˆä¸€å±‚</li><li>resnet18ï¼šorigin paper</li><li>resnet34ï¼šorigin paper</li></ul></li><li><p>éšç€ç½‘ç»œåŠ æ·±ï¼Œå„æ–¹æ³•çš„ç²¾åº¦å·®å¼‚ç¼©å°ï¼Œbaselineæ–¹æ³•ç”šè‡³åè¶…äº†ä¸€äº›meta learningæ–¹æ³•</p><p><img src="/2021/06/22/few-shot/tendency.png" width="75%;"></p></li></ul></li><li><p>effect of domain shift</p><ul><li>ä¸€ä¸ªç°å®åœºæ™¯ï¼šmini-ImageNet â€”&gt; CUBï¼Œæ”¶é›†general class dataç›¸å¯¹å®¹æ˜“ï¼Œæ”¶é›†fine-grainedæ•°æ®é›†åˆ™æ›´å›°éš¾</li><li>ç”¨resnet18å®éªŒ</li><li>Baseline outperforms all meta-learning methods under this scenario</li><li>å› ä¸ºmeta learning methodsçš„å­¦ä¹ å®Œå…¨ä¾èµ–äºbase support classï¼Œnot able to adapt</li><li>éšç€domain difference get largerï¼ŒBaselineç›¸å¯¹äºå…¶ä»–æ–¹æ³•çš„gapä¹Ÿé€æ¸æ‹‰å¤§</li><li><p>è¯´æ˜äº†åœ¨domain shiftåœºæ™¯ä¸‹ï¼Œadaptation based methodçš„å¿…è¦æ€§</p><p><img src="/2021/06/22/few-shot/domain shift.png" width="75%;"></p></li></ul></li><li><p>further adapt meta-learning methods</p><ul><li>MatchingNet &amp; ProtoNetï¼šè·Ÿbaselineæ–¹æ³•ä¸€æ ·ï¼Œfix feature extractorï¼Œç„¶åç”¨novel set train a new classifier</li><li>MAMLï¼šnot feasible to fix the featureï¼Œç”¨novel set finetuneæ•´ä¸ªç½‘ç»œ</li><li>RelationNetï¼šfeaturesæ˜¯conv mapsè€Œä¸æ˜¯vectorï¼Œrandomly splitä¸€éƒ¨åˆ†novel setä½œä¸ºè®­ç»ƒé›†</li><li><p>MatchingNet &amp; MAMLéƒ½æœ‰å¤§å¹…ç²¾åº¦æå‡ï¼Œå°¤å…¶åœ¨domain shiftåœºæ™¯ä¸‹ï¼Œä½†æ˜¯ProtoNetä¼šæ‰ç‚¹ï¼Œè¯´æ˜adaptationæ˜¯å½±å“ç²¾åº¦çš„key factorï¼Œä½†æ˜¯è¿˜æ²¡æœ‰å®Œç¾è§£å†³æ–¹æ¡ˆ</p><p><img src="/2021/06/22/few-shot/adaptation.png" width="75%;"></p></li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> å°æ ·æœ¬ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ATSS</title>
      <link href="/2021/06/17/ATSS/"/>
      <url>/2021/06/17/ATSS/</url>
      <content type="html"><![CDATA[<h2 id="ATSS-Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection"><a href="#ATSS-Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection" class="headerlink" title="ATSS: Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection"></a>ATSS: Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</h2><ol><li><p>åŠ¨æœº</p><ul><li>anchor-basedå’Œanchor-freeæ–¹æ³•çš„æœ¬è´¨åŒºåˆ«æ˜¯å¯¹æ­£è´Ÿæ ·æœ¬çš„å®šä¹‰ï¼Œè¿™ä¹Ÿç›´æ¥å¯¼è‡´äº†performance gap</li><li>we propose ATSS<ul><li>adaptive training sample selection</li><li>automatically select positive and negative samples according to statistical characteristics of objects</li><li>anchor-based&amp;anchor-freeæ¨¡å‹ä¸Šéƒ½æ¶¨ç‚¹</li></ul></li><li>discuss tiling multiple anchors</li></ul></li><li><p>è®ºç‚¹</p><ul><li>ä¸»æµanchor-basedæ–¹æ³•<ul><li>one-stage/two-stage</li><li>tile a large number of preset anchors on the image</li><li>output these refined anchors as detection results</li></ul></li><li>anchor-free detectorsä¸»è¦åˆ†æˆä¸¤ç§<ul><li>key-point basedï¼šé¢„æµ‹è§’ç‚¹/è½®å»“ç‚¹/heatmapï¼Œç„¶åboundè½®å»“å¾—åˆ°æ¡†</li><li>center-basedï¼šé¢„æµ‹ä¸­å¿ƒç‚¹ï¼Œç„¶ååŸºäºä¸­å¿ƒç‚¹å›å½’4ä¸ªè·ç¦»</li><li>æ¶ˆé™¤pre-defined anchorsçš„hyper-paramsï¼šå¼ºåŒ–generalization ability</li></ul></li><li>ä¸¾ä¾‹å¯¹æ¯”RetinaNet&amp;FCOS<ul><li>RetinaNetï¼šone-stage anchor-based</li><li>FCOSï¼šcenter-based anchor-free</li><li>ä¸»è¦åŒºåˆ«1ï¼šanchoræ•°é‡ï¼ŒRetinaNetæ˜¯hxwx9ï¼ŒFCOSæ˜¯hxwx1</li><li>ä¸»è¦åŒºåˆ«2ï¼šæ­£æ ·æœ¬å®šä¹‰ï¼ŒRetinaNetæ˜¯ä¸gt boxçš„IOUå¤§äºä¸€å®šé˜ˆå€¼çš„anchorï¼ŒFCOSæ˜¯featuremapä¸Šæ‰€æœ‰è½è¿›æ¡†å†…çš„æ ¼å­ç‚¹</li><li>ä¸»è¦åŒºåˆ«3ï¼šå›å½’æ–¹å¼ï¼ŒRetinaNetæ˜¯å›å½’gtç›¸å¯¹pos anchorçš„ç›¸å¯¹åç§»é‡ï¼ŒFCOSæ˜¯é¢„æµ‹å››æ¡è¾¹ç›¸å¯¹ä¸­å¿ƒç‚¹çš„ç»å¯¹è·ç¦»</li></ul></li></ul></li><li><p>Difference Analysis of Anchor-based and Anchor-free Detection</p><ul><li><p>we focus on the last two differencesï¼šæ­£è´Ÿæ ·æœ¬å®šä¹‰ &amp; å›å½’starting status</p></li><li><p>è®¾å®šRetinaNetä¹Ÿæ˜¯one square anchor per locationï¼Œå’ŒFCOSä¿æŒä¸€è‡´</p></li><li><p>experiment setting</p><ul><li>MS COCOï¼š80ç±»å‰æ™¯ï¼Œcommon split</li><li>ImageNet pretrained ResNet-50</li><li>resize input</li><li>SGDï¼Œ90K iterationsï¼Œ0.9 momentumï¼Œ1e-4 weight decayï¼Œ16 batch sizeï¼Œ0.01 lr with 0.1 lr decay/60K</li><li>testingï¼š<ul><li>0.05 score to filter out bg boxes</li><li>output top 1000 detections per feature pyramid</li><li>0.6 IoU thresh per class NMS to give final top 100 detections per image </li></ul></li></ul></li><li><p>inconsistency removal</p><ul><li>äº”å¤§improvementsåŠ åœ¨FCOSä¸Šè¿›ä¸€æ­¥boost the gap</li><li><p>æˆ‘ä»¬å°†å…¶é€æ­¥åŠ åœ¨RetinaNetä¸Šï¼Œèƒ½å¤Ÿæ‹‰åˆ°37%ï¼Œå’ŒFCOSè¿˜æœ‰0.8ä¸ªç‚¹çš„å·®è·</p><p><img src="/2021/06/17/ATSS/inconsistency.png" width="45%;"></p></li></ul></li><li><p>åˆ†æessential difference</p><ul><li><p>è®­ç»ƒä¸€ä¸ªæ£€æµ‹æ¨¡å‹ï¼Œé¦–å…ˆè¦åˆ†å‡ºæ­£è´Ÿæ ·æœ¬ï¼Œç„¶åç”¨æ­£æ ·æœ¬æ¥å›å½’</p></li><li><p>Classification</p><ul><li>RetinaNetç”¨anchor boxesä¸gt boxçš„IoUå†³å®šæ­£è´Ÿæ ·æœ¬ï¼šbest match anchorå’Œå¤§äºä¸€å®šIoU threshçš„anchoræ˜¯æ­£æ ·æœ¬ï¼Œå°äºä¸€å®šIoU threshçš„anchoræ˜¯è´Ÿæ ·æœ¬ï¼Œå…¶ä»–çš„æ˜¯ignoreæ ·æœ¬</li><li><p>FCOSç”¨spatial and scale constraintsé€‰æ‹©æ­£è´Ÿæ ·æœ¬ï¼šgt boxä»¥å†…çš„æ‰€æœ‰åƒç´ ä½œä¸ºå€™é€‰æ­£æ ·æœ¬ï¼Œç„¶åå»æ‰éƒ¨åˆ†å°ºåº¦ä¸åŒ¹é…çš„å€™é€‰æ ·æœ¬ï¼Œæ­£æ ·æœ¬ä»¥å¤–éƒ½æ˜¯è´Ÿæ ·æœ¬ï¼Œæ²¡æœ‰ignore</p><p><img src="/2021/06/17/ATSS/samples1.png" width="45%;"></p></li><li><p>ä¸¤ä¸ªæ¨¡å‹åœ¨ä¸¤ç§æ ·æœ¬é€‰æ‹©ç­–ç•¥ä¸Šå®éªŒï¼šSpatial and Scale Constraintç›¸æ¯”è¾ƒäºIoUéƒ½ä¼šæ˜¾è‘—æç‚¹</p></li><li><p>å½“ä¸¤ç§æ–¹æ³•éƒ½ä½¿ç”¨Spatial and Scale Constraintç­–ç•¥é€‰æ‹©æ­£è´Ÿæ ·æœ¬ï¼Œæ¨¡å‹ç²¾åº¦å°±æ²¡å•¥å·®åˆ«äº†</p><p><img src="/2021/06/17/ATSS/samples.png" width="45%;"></p></li></ul></li><li><p>Regression</p><ul><li>RetinaNet regresses from the anchor box with 4 offsetsï¼šå›å½’gt boxç›¸å¯¹äºanchor boxçš„åç§»é‡ï¼Œregression starting statusæ˜¯ä¸ªbox</li><li><p>FCOS regresses from the anchor point with 4 distancesï¼šå›å½’gt boxå››æ¡è¾¹ç›¸å¯¹äºanchor centerçš„è·ç¦»ï¼Œregression starting statusæ˜¯ä¸ªpoint</p><p><img src="/2021/06/17/ATSS/regress.png" width="45%;"></p></li><li><p>ä¸Šé¢é‚£ä¸ªè¡¨è¯´æ˜äº†é€‰æ‹©åŒæ ·çš„æ­£è´Ÿæ ·æœ¬ï¼Œregression starting statuså°±æ˜¯ä¸ªæ— å…³é¡¹ï¼Œä¸å½±å“ç²¾åº¦</p></li></ul></li></ul></li></ul></li><li><p>Adaptive Training Sample Selection ï¼ˆATSSï¼‰</p><ul><li><p>å½±å“æ£€æµ‹æ¨¡å‹ç²¾åº¦çš„essential differenceåœ¨äºhow to define positive and negative training samples</p></li><li><p>previous strategieséƒ½æœ‰sensitive hyperparametersï¼ˆanchors/scaleï¼‰ï¼Œsome outer objects may be neglected</p></li><li><p>we propose ATSS</p><ul><li>almost no hyper</li><li><p>divides pos/neg samples according to data statistical characteristics </p><p><img src="/2021/06/17/ATSS/ATSS.png" width="45%;"></p></li><li><p>å¯¹æ¯ä¸ªgt boxï¼Œé¦–å…ˆåœ¨æ¯ä¸ªlevelä¸Šï¼ŒåŸºäºL2 center distanceï¼Œæ‰¾åˆ°k-closest anchorâ€”â€”k*Lä¸ªcandidates per gt box</p></li><li>è®¡ç®—æ¯ä¸ªcandidatesçš„mean &amp; var</li><li>åŸºäºmean &amp; var è®¡ç®—è¿™ä¸ªgt boxçš„IoU thresh</li><li>åœ¨candidatesé‡Œé¢é€‰å–å¤§äºç­‰äºIoU threshï¼ŒåŒæ—¶anchor centeråœ¨gt boxå†…çš„ï¼Œç•™ä½œæ­£æ ·æœ¬</li><li>å¦‚æœä¸€ä¸ªacnhor boxåŒ¹é…äº†å¤šä¸ªgt boxï¼Œé€‰æ‹©IoUå¤§çš„é‚£ä¸ªä½œä¸ºæ ‡ç­¾</li></ul></li><li><p>åŸºäºcenter distanceé€‰æ‹©anchor boxï¼šå› ä¸ºè¶Šé è¿‘ç›®æ ‡ä¸­å¿ƒï¼Œè¶Šå®¹æ˜“produceé«˜å“è´¨æ¡†</p></li><li><p>ç”¨mean+varä½œä¸ºIoU threshï¼š</p><ul><li>higher mean indicates high-quality candidatesï¼Œå¯¹åº”çš„IoU threshåº”è¯¥é«˜ä¸€ç‚¹</li><li>higher variation indicates level specificï¼Œmean+varä½œä¸ºthreshèƒ½å°†candidatesé‡Œé¢IoUè¾ƒé«˜çš„ç­›é€‰å‡ºæ¥</li></ul></li><li><p>limit the anchor center in objectï¼šanchorä¸­å¿ƒä¸åœ¨ç›®æ ‡æ¡†å†…æ˜¾ç„¶ä¸æ˜¯ä¸ªå¥½æ¡†ï¼Œç”¨äºç­›æ‰å‰ä¸¤æ­¥é‡Œçš„æ¼ç½‘ä¹‹é±¼ï¼ŒåŒä¿é™©</p></li><li><p>fair between different objects</p><ul><li>ç»Ÿè®¡ä¸‹æ¥æ¯ç±»ç›®æ ‡éƒ½æœ‰å·®ä¸å¤š0.2kLä¸ªæ­£æ ·æœ¬ï¼Œä¸å°ºåº¦æ— å…³</li><li>ä½†æ˜¯RetinaNetå’ŒFCOSéƒ½æ˜¯å¤§ç›®æ ‡æ­£æ ·æœ¬å¤šï¼Œå°ç›®æ ‡æ­£æ ·æœ¬å°‘</li></ul></li><li><p>hyperparam-freeï¼šåªæœ‰ä¸€ä¸ªkï¼Œã€è¿˜æœ‰anchor-settingå‘¢ï¼Ÿï¼Ÿï¼Ÿã€‘</p></li><li><p>verification</p><ul><li>lite versionï¼šè¢«FCOSå®˜æ–¹å¼•ç”¨å¹¶ç§°ä½œcenter samplingï¼Œscale limit still exists in this version</li><li>full versionï¼šæœ¬æ–‡ç‰ˆæœ¬</li><li><p>ä¸¤ä¸ªæ–¹æ³•é€‰æ‹©candidatesçš„æ–¹æ³•å®Œå…¨ä¸€è‡´ï¼Œå°±æ˜¯select final postivesçš„æ–¹æ³•ä¸åŒ</p><p><img src="/2021/06/17/ATSS/verification.png" width="45%;"></p></li></ul></li><li><p>hyperparamçš„é²æ£’æ€§</p><ul><li><p>kåœ¨ä¸€å®šèŒƒå›´å†…ï¼ˆ7-17ï¼‰ç›¸å¯¹insensitiveï¼Œå¤ªå¤šäº†ä½è´¨é‡æ¡†å¤ªå¤šï¼Œå¤ªå°‘äº†less statistical</p><p>  <img src="/2021/06/17/ATSS/kvalue.png" width="45%;"></p></li><li><p>å°è¯•ä¸åŒçš„fix-ratio anchor scaleå’Œfix-scale anchor ratioï¼Œå‘ç°ç²¾åº¦ç›¸å¯¹ç¨³å®šï¼Œè¯´æ˜robust to anchor settings</p><p>  <img src="/2021/06/17/ATSS/anchor setting.png" width="45%;"></p></li><li><p>multi-anchors settings</p><ul><li><p>RetinaNetåœ¨ä¸åŒçš„anchor settingä¸‹ï¼Œç²¾åº¦åŸºæœ¬ä¸å˜ï¼Œè¯´æ˜ä¸»è¦æ­£æ ·æœ¬é€‰çš„å¥½ï¼Œä¸ç®¡ä¸€ä¸ªlocationç»‘å®šå‡ ä¸ªanchorç»“æœéƒ½ä¸€æ ·</p><p><img src="/2021/06/17/ATSS/multi.png" width="45%;"></p></li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ç›®æ ‡æ£€æµ‹ï¼Œanchor-free&amp;anchor-basedï¼Œtricks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>re-labeling</title>
      <link href="/2021/05/27/re-labeling/"/>
      <url>/2021/05/27/re-labeling/</url>
      <content type="html"><![CDATA[<h2 id="Re-labeling-ImageNet-from-Single-to-Multi-Labels-from-Global-to-Localized-Labels"><a href="#Re-labeling-ImageNet-from-Single-to-Multi-Labels-from-Global-to-Localized-Labels" class="headerlink" title="Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels"></a>Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels</h2><ol><li><p>åŠ¨æœº</p><ul><li>label noise<ul><li>single-label benchmark</li><li>but contains multiple classes in one sample</li><li>a random crop may contain an entirely different object from the gt label</li><li>exhaustive multi-label annotations per image is too cost</li></ul></li><li>mismatch<ul><li>researches refine the validation set with multi-labels</li><li>propose new multi-label evaluation metrics</li><li>ä½†æ˜¯é€ æˆæ•°æ®é›†çš„mismatch</li></ul></li><li>we propose<ul><li>re-label</li><li>use a strong image classifier trained on extra source of data to generate the multi-labels</li><li>use pixel-wise multi-label predictions before GAPï¼šaddtional location-specific supervision</li><li>then trained on re-labeled samples</li><li>further boost with CutMix</li></ul></li><li>from single to multi-labelsï¼šå¤šæ ‡ç­¾</li><li><p>from global to localizedï¼šdense prediction map</p><p><img src="/2021/05/27/re-labeling/re-labeling.png" width="40%;"></p></li></ul></li><li><p>è®ºç‚¹</p><ul><li>single-label<ul><li>å’Œmulti-label validation setçš„mismatch</li><li>random crop augmentationåŠ å‰§äº†é—®é¢˜</li><li>é™¤äº†å¤šç›®æ ‡è¿˜æœ‰å‰èƒŒæ™¯ï¼Œåªæœ‰23%çš„random crops IOU&gt;0.5</li></ul></li><li>ideally label<ul><li>the full set of classesâ€”â€”multi-label</li><li>where each objectsâ€”â€”localized label</li><li>results in a dense pixel labeling $L\in \{0,1\}^{HWC}$</li></ul></li><li><p>we propose a re-labeling strategy</p><ul><li>ReLabel <ul><li>strong classifier</li><li>external training data</li><li>generate feature map predictions</li></ul></li><li>LabelPooling<ul><li>with dense labels &amp; random crop</li><li>pooling the label scores from crop region</li></ul></li></ul></li><li><p>evaluations</p><ul><li>baseline r50ï¼š77.5%</li><li>r50 + ReLabelï¼š78.9%</li><li>r50 + ReLabel + CutMixï¼š80.2%</li></ul></li><li><p>ã€QUESTIONã€‘åŒæ ·æ˜¯å¼•å…¥å¤–éƒ¨æ•°æ®å®ç°æ— ç—›é•¿ç‚¹ï¼Œä¸noisy studentçš„åŒºåˆ«/å¥½åï¼Ÿï¼Ÿï¼Ÿ</p><ul><li><p>ç›®å‰è®ºæ–‡æåˆ°çš„å°±åªæœ‰efficiencyï¼ŒReLabelæ˜¯one-time costçš„ï¼ŒçŸ¥è¯†è’¸é¦æ˜¯iterative&amp;on-the-flyçš„</p><p><img src="/2021/05/27/re-labeling/differences.png" width="40%;"></p></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>Re-labeling</p><ul><li><p>super annotator</p><ul><li>state-of-the-art classifier</li><li>trained on super large dataset</li><li>fine-tuned on ImageNet</li><li>and predict ImageNet labels</li></ul></li><li><p>we use open-source trained weights as annotators</p><ul><li>though trained with single-label supervision</li><li>still tend to make multi-label predictions</li><li>EfficientNet-L2</li><li>input size 475</li><li>feature map size 15x15x5504</li><li><p>output dense label size 15x15x1000</p><p><img src="/2021/05/27/re-labeling/relabel.png" width="60%;"></p></li></ul></li><li><p>location-specific labels</p><ul><li>remove GAP heads</li><li>add a 1x1 conv</li><li>è¯´ç™½äº†å°±æ˜¯ä¸€ä¸ªfcn</li><li>original classifierçš„fcå±‚æƒé‡ä¸æ–°æ·»åŠ çš„1x1 convå±‚çš„æƒé‡æ˜¯ä¸€æ ·çš„</li><li><p>labelçš„æ¯ä¸ªchannelå¯¹åº”äº†ä¸€ä¸ªç±»åˆ«çš„heatmapï¼Œå¯ä»¥çœ‹åˆ°disjointly located at each objectâ€™s position</p><p><img src="/2021/05/27/re-labeling/labelmap.png" width="80%;"></p></li></ul></li></ul></li><li><p>LabelPooling</p><p>  <img src="/2021/05/27/re-labeling/labelpooling.png" width="40%;"></p><ul><li>loads the pre-computed label map</li><li>region pooling (RoIAlign) on the label map</li><li>GAP + softmax to get multi-label vector</li><li>train a classifier with the multi-label vector</li><li><p>uses CE</p><p><img src="/2021/05/27/re-labeling/algorithm.png" width="90%;"></p></li></ul></li><li><p>choices</p><ul><li><p>space consumption</p><ul><li>ä¸»è¦æ˜¯å­˜å‚¨label mapçš„ç©ºé—´</li><li>store only top-5 predictions per imageï¼š10GB</li></ul></li><li><p>time consumption</p><ul><li>ä¸»è¦æ˜¯è¯´ç”Ÿæˆlabel mapçš„one-shot-inference timeå’ŒlabelPoolingå¼•å…¥çš„é¢å¤–è®¡ç®—æ—¶é—´</li><li>relabelingï¼š10-GPU hours</li><li>labelPoolingï¼š0.5% additiona training time</li><li>more efficient than KD</li></ul></li><li><p>annotators</p><ul><li><p>æ ‡æ³¨å·¥å…·å“ªå®¶å¼ºï¼šç›®å‰çœ‹ä¸‹æ¥eff-L2çš„supervisionæ•ˆæœæœ€å¼º</p><p><img src="/2021/05/27/re-labeling/annotators.png" width="40%;"></p></li><li><p>supervision confidence</p><ul><li>éšç€image cropä¸å‰æ™¯ç‰©ä½“çš„IOUå¢å¤§ï¼Œconfidenceé€æ¸å¢åŠ </li><li><p>è¯´æ˜supervision provides some uncertainty when low IOU</p><p><img src="/2021/05/27/re-labeling/supervision.png" width="45%;"></p></li></ul></li></ul></li></ul></li></ul></li><li><p>å®éªŒ</p></li></ol>]]></content>
      
      
        <tags>
            
            <tag> pretaining </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>mlpç³»åˆ—</title>
      <link href="/2021/05/27/mlp%E7%B3%BB%E5%88%97/"/>
      <url>/2021/05/27/mlp%E7%B3%BB%E5%88%97/</url>
      <content type="html"><![CDATA[<p>[papers]</p><ul><li>[MLP-Mixer] MLP-Mixer: An all-MLP Architecture for Visionï¼ŒGoogle</li><li>[ResMLP] ResMLP: Feedforward networks for image classification with data-efficient trainingï¼ŒFacebook</li></ul><p>[references]</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzUyMjE2MTE0Mw==&amp;mid=2247493478&amp;idx=1&amp;sn=2be608d776b2469b3357da30c42d9770&amp;chksm=f9d2b9fecea530e8cbf07847c2029a1dabb131dbc1d6bd91ed227e41a396dd333afc83b64cf8&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzUyMjE2MTE0Mw==&amp;mid=2247493478&amp;idx=1&amp;sn=2be608d776b2469b3357da30c42d9770&amp;chksm=f9d2b9fecea530e8cbf07847c2029a1dabb131dbc1d6bd91ed227e41a396dd333afc83b64cf8&amp;scene=21#wechat_redirect</a></p><p><a href="https://mp.weixin.qq.com/s/8f9yC2P3n3HYygsOo_5zww" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/8f9yC2P3n3HYygsOo_5zww</a></p><h2 id="MLP-Mixer-An-all-MLP-Architecture-for-Vision"><a href="#MLP-Mixer-An-all-MLP-Architecture-for-Vision" class="headerlink" title="MLP-Mixer: An all-MLP Architecture for Vision"></a>MLP-Mixer: An all-MLP Architecture for Vision</h2><ol><li><p>åŠ¨æœº</p><ul><li>image classification task</li><li>neither of [CNN, attention] are necessary</li><li>our proposed MLP-Mixer<ul><li>ä»…åŒ…å«multi-layer-perceptrons</li><li>independently to image patches</li><li>repeated applied across either spatial locations or feature channels</li><li>two types<ul><li>applied independently to image patches</li><li>applied across patches</li></ul></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>overview</p><p>  <img src="/2021/05/27/mlpç³»åˆ—/mixer.png" width="70%;"></p><ul><li>è¾“å…¥æ˜¯token sequences<ul><li>non-overlapping image patches</li><li>linear projected to dimension C</li></ul></li><li>Mixer Layer<ul><li>maintain the input dimension</li><li>channel-mixing MLP<ul><li>operate on each token independently</li><li>å¯ä»¥çœ‹ä½œæ˜¯1x1çš„conv</li></ul></li><li>token-mixing MLP<ul><li>operate on each channel independently</li><li>take each spatial vectors (hxw)x1 as inputs</li><li>å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªglobal depth-wise convï¼Œs1ï¼Œsame padï¼Œkernel sizeæ˜¯(h,w)</li></ul></li></ul></li><li>æœ€åå¯¹token embeddingåšGAPï¼Œæå–sequence vecï¼Œç„¶åè¿›è¡Œç±»åˆ«é¢„æµ‹</li></ul></li><li><p>idea behind Mixer</p><ul><li>clearly separate the per-location operations &amp; cross-location operations </li><li>CNNæ˜¯åŒæ—¶è¿›è¡Œè¿™ä¿©çš„</li><li>transformerçš„MSAåŒæ—¶è¿›è¡Œè¿™ä¿©ï¼ŒMLPåªè¿›è¡Œper-location operations</li></ul></li><li><p>Mixer Layer</p><ul><li><p>two MLP blocks</p></li><li><p>given input $X\in R^{SC}$ï¼ŒS for spatial dimï¼ŒC for channel dim</p><p>  <img src="/2021/05/27/mlpç³»åˆ—/blocks.png" width="70%;"></p></li><li><p>å…ˆæ˜¯token-mixing MLP</p><ul><li>acts on S dim</li><li>maps $R^S$ to $R^S$</li><li>share across C-axis</li><li>LN-FC-GELU-FC-residual</li></ul></li><li><p>ç„¶åæ˜¯channel-mixing MLP</p><ul><li>acts on C dim</li><li>maps $R^C$ to $R^C$</li><li>share across S-axis</li><li>LN-FC-GELU-FC-residual</li></ul></li><li><p>fixed widthï¼Œæ›´æ¥è¿‘transformer/RNNï¼Œè€Œä¸æ˜¯CNNé‚£ç§é‡‘å­—å¡”ç»“æ„</p></li><li><p>ä¸ä½¿ç”¨positional embeddings</p><ul><li>the token-mixing MLPs are sensitive to the order of the input tokens</li><li>may learn to represent locations</li></ul></li></ul></li></ul></li><li><p>å®éªŒ</p></li></ol><h2 id="ResMLP-Feedforward-networks-for-image-classification-with-data-efficient-training"><a href="#ResMLP-Feedforward-networks-for-image-classification-with-data-efficient-training" class="headerlink" title="ResMLP: Feedforward networks for image classification with data-efficient training"></a>ResMLP: Feedforward networks for image classification with data-efficient training</h2><ol><li><p>åŠ¨æœº</p><ul><li>entirely build upon MLP</li><li>alternates from a simple residual network<ul><li>a linear layer to interact with image patches</li><li>a two-layer FFN to interact independently with each patch</li><li>affine transformæ›¿ä»£LNæ˜¯ä¸€ä¸ªç‰¹åˆ«ä¹‹å¤„</li></ul></li><li>trained with modern strategy<ul><li>heavy data-augmentation </li><li>optionally distillation</li></ul></li><li>show good performace on ImageNet classification</li></ul></li><li><p>è®ºç‚¹</p><ul><li>strongly inspired by ViT but simpler<ul><li>æ²¡æœ‰attentionå±‚ï¼Œåªæœ‰fcå±‚+gelu</li><li>æ²¡æœ‰normå±‚ï¼Œå› ä¸ºmuch more stable to trainï¼Œä½†æ˜¯ç”¨äº†affine transformation</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>overview</p><p>  <img src="/2021/05/27/mlpç³»åˆ—/ResMLP.png" width="70%;"></p><ul><li>takes flattened patches as inputs<ul><li>typically N=16ï¼š16x16</li></ul></li><li>linear project the patches into embeddings<ul><li>form $N^2$ d-dim embeddings</li></ul></li><li>ResMLP Layer<ul><li>main the dim throughout $[N^2,d]$</li><li>a simple linear layer<ul><li>interaction between the patches</li><li>applied to all channels independently</li><li>ç±»ä¼¼depth-wise conv with global kernelçš„ä¸œè¥¿ï¼Œçº¿æ€§ï¼ï¼</li></ul></li><li>a two-layer-mlp<ul><li>fc-GELU-fc</li><li>independently applied to all patches</li><li>éçº¿æ€§ï¼ï¼</li></ul></li></ul></li><li>average pooled $[d-dim]$ + linear classifier $cls-dim$</li></ul></li><li><p>Residual Multi-Layer Perceptron Layer</p><ul><li>a linear layer + a FFN layer</li><li>each layer is paralleled with a skip-connection</li><li><p>æ²¡ç”¨LNï¼Œä½†æ˜¯ç”¨äº†learnable affine transformation</p><ul><li>$Aff_{\alpha, \beta} (x) = Diag(\alpha) x + \beta$</li><li>rescale and shifts the input component-wiseï¼šå¯¹æ¯ä¸ªpatchï¼Œåˆ†åˆ«åšaffineå˜æ¢</li><li>åœ¨æ¨ç†é˜¶æ®µå¯ä»¥ä¸ä¸Šä¸€å±‚çº¿æ€§å±‚åˆå¹¶ï¼šno cost</li><li>ç”¨äº†ä¸¤æ¬¡<ul><li>ç¬¬ä¸€ä¸ªç”¨åœ¨main pathä¸Šç”¨æ¥æ›¿ä»£LNï¼šåˆå€¼ä¸ºidentity transform(1,0)</li><li>ç¬¬äºŒä¸ªåœ¨residual pathé‡Œé¢ï¼Œdown scale to boostï¼Œç”¨ä¸€ä¸ªsmall valueåˆå§‹åŒ–</li></ul></li></ul></li><li><p>given inputï¼š $d\times N^2$ matrix $X$</p><p>  <img src="/2021/05/27/mlpç³»åˆ—/mlp1.png" width="50%;"></p><ul><li>affineåœ¨d-dimä¸Šåš</li><li>ç¬¬ä¸€ä¸ªLinear layeråœ¨$N^2-dim$ä¸Šåšï¼šå‚æ•°é‡$N^2 \times N^2$</li><li><p>ç¬¬äºŒã€ä¸‰ä¸ªLinear layeråœ¨$d-dim$ä¸Šåšï¼šå‚æ•°é‡$d \times 4d$ &amp; $4d \times d$</p><p><img src="/2021/05/27/mlpç³»åˆ—/mlp2.png" width="50%;"></p></li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> mlp </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>torch-note</title>
      <link href="/2021/05/24/torch-note/"/>
      <url>/2021/05/24/torch-note/</url>
      <content type="html"><![CDATA[<ol><li><p>torch.cuda.amp</p><p> è‡ªåŠ¨æ··åˆç²¾åº¦ï¼šFloatTensor &amp; HalfTensor</p></li><li><p>torch.jit.script</p><p> å°†æ¨¡å‹ä»çº¯Pythonç¨‹åºè½¬æ¢ä¸ºèƒ½å¤Ÿç‹¬ç«‹äºPythonè¿è¡Œçš„TorchScriptç¨‹åº</p></li><li><p>torch.nn.DataParallel</p></li><li><p>torch.flatten(input, start_dim=0, end_dim=-1)</p><p> å±•å¼€start_dimåˆ°end_dimä¹‹é—´çš„dimæˆä¸€ç»´</p></li><li></li></ol>]]></content>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>LV-ViT</title>
      <link href="/2021/05/21/LV-ViT/"/>
      <url>/2021/05/21/LV-ViT/</url>
      <content type="html"><![CDATA[<p>[LV-ViT 2021] Token Labeling: Training an 85.4% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNetï¼Œæ–°åŠ å¡å›½ç«‹&amp;å­—èŠ‚ï¼Œä¸»ä½“ç»“æ„è¿˜æ˜¯ViTï¼Œdeeper+narrower+multi-layer-cnn-patch-projection+auxiliary label&amp;loss</p><p>åŒç­‰å‚æ•°é‡ä¸‹ï¼Œèƒ½å¤Ÿè¾¾åˆ°ä¸CNNç›¸å½“çš„åˆ†ç±»ç²¾åº¦</p><ul><li>26Mâ€”â€”84.4% ImageNet top1 acc</li><li>56Mâ€”â€”85.4% ImageNet top1 acc</li><li>150Mâ€”â€”86.2% ImageNet top1 acc</li></ul><p><img src="/2021/05/21/LV-ViT/recent.png" width="70%;"></p><p>ImageNet &amp; ImageNet-1kï¼šThe ImageNet dataset consists of more than 14M images, divided into approximately 22k different labels/classes. However the <strong>ImageNet challenge is conducted on just 1k high-level categories</strong> (probably because 22k is just too much)</p><h2 id="Token-Labeling-Training-an-85-4-Top-1-Accuracy-Vision-Transformer-with-56M-Parameters-on-ImageNet"><a href="#Token-Labeling-Training-an-85-4-Top-1-Accuracy-Vision-Transformer-with-56M-Parameters-on-ImageNet" class="headerlink" title="Token Labeling: Training an 85.4% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNet"></a>Token Labeling: Training an 85.4% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNet</h2><ol><li><p>åŠ¨æœº</p><ul><li>develop a bag of training techniques on vision transformers</li><li>slightly tune the structure</li><li>introduce token labelingâ€”â€”a new training objective</li><li>ImageNet classificaiton task</li></ul></li><li><p>è®ºç‚¹</p><ul><li>former ViTs<ul><li>ä¸»è¦é—®é¢˜å°±æ˜¯éœ€è¦å¤§æ•°æ®é›†pretrainï¼Œä¸ç„¶ç²¾åº¦ä¸Šä¸å»</li><li>ç„¶åæ¨¡å‹ä¹Ÿæ¯”è¾ƒå¤§ï¼Œneed huge computation resources</li><li>DeiTå’ŒT2T-ViTæ¢ç´¢äº†data augmentation/å¼•å…¥additional tokenï¼Œèƒ½å¤Ÿåœ¨æœ‰é™çš„æ•°æ®é›†ä¸Šæ‹‰ç²¾åº¦</li></ul></li><li>our work<ul><li>rely on purely ImageNet-1k data</li><li>rethink the way of performing patch embedding</li><li>introduce inductive bias</li><li>we add a token labeling objective loss beside cls token predition</li><li>provide practical advice on adjusting vision transformer structures</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>overview &amp; comparison</p><ul><li>ä¸»ä½“ç»“æ„ä¸å˜ï¼Œå°±æ˜¯å¢åŠ äº†ä¸¤é¡¹</li><li>a MixToken method</li><li><p>a token labeling objective</p><p><img src="/2021/05/21/LV-ViT/cmp.png" width="75%;"></p></li></ul></li><li><p>review the vision transformer</p><ul><li>patch embedding<ul><li>å°†å›ºå®šå°ºå¯¸çš„å›¾ç‰‡è½¬æ¢æˆpatch sequenceï¼Œä¾‹å¦‚224x224çš„å›¾ç‰‡ï¼Œpatch size=16ï¼Œé‚£å°±æ˜¯14x14ä¸ªsmall patches</li><li>å°†æ¯ä¸ªpatch(16x16x3=768-dim) linear projectæˆä¸€ä¸ªtoken(embedding-dim)</li><li>concat a class tokenï¼Œæ„æˆå…¨éƒ¨çš„input tokens</li></ul></li><li>position encoding<ul><li>added to input tokens</li><li>fixed sinusoidal / learnable</li></ul></li><li>multi-head self-attention<ul><li>ç”¨æ¥å»ºç«‹long-range dependency</li><li>multi-headsï¼šæ‰€æœ‰attention headsçš„è¾“å‡ºåœ¨channel-dimä¸Šconcatï¼Œç„¶ålinear projectå›å•ä¸ªheadçš„channel-dim</li></ul></li><li>feed-forward layers<ul><li>fc1-activation-fc2</li></ul></li><li>score predition layer<ul><li>åªç”¨äº†cls tokenå¯¹åº”çš„è¾“å‡ºembeddingï¼Œå…¶ä»–çš„discard</li></ul></li></ul></li><li><p>training techniques</p><ul><li><p>network depth</p><ul><li>add more transformer blocks</li><li>åŒæ—¶decrease the hidden dim of FFN</li></ul></li><li><p>explicit inductive bias</p><ul><li>CNNé€æ­¥æ‰©å¤§æ„Ÿå—é‡ï¼Œæ“…é•¿æå–å±€éƒ¨ç‰¹å¾ï¼Œå…·æœ‰å¤©ç„¶çš„å¹³ç§»ä¸å˜æ€§ç­‰</li><li>transformerè¢«å‘ç°failed to capture the low-level and local structures</li><li>we use convolutions with a smaller stride to provide an overlapped information for each nearby tokens</li><li>åœ¨patch embeddingçš„æ—¶å€™ä¸æ˜¯independent cropï¼Œè€Œæ˜¯æœ‰overlap</li><li>ç„¶åç”¨å¤šå±‚convï¼Œé€æ­¥æ‰©å¤§æ„Ÿå—é‡ï¼Œsmaller kernel sizeåŒæ—¶é™ä½äº†è®¡ç®—é‡</li></ul></li><li><p>rethinking residual connection</p><ul><li><p>ç»™æ®‹å·®åˆ†æ”¯add a smaller ratio $\alpha$</p><p>  <img src="/2021/05/21/LV-ViT/residual.png" width="40%;"></p></li><li><p>enhance the residual connection since less information will go to the residual branch</p></li><li><p>improve the generalization ability</p></li></ul></li><li><p>re-labeling</p><ul><li>label is not always accurate after cropping</li><li><p>situations are worse on smaller images</p></li><li><p>re-assign each image with a K-dim score mapï¼Œåœ¨1kç±»æ•°æ®é›†ä¸ŠK=1000</p></li><li>cheap operation compared to teacher-student</li><li>è¿™ä¸ªlabelæ˜¯é’ˆå¯¹whole imageçš„labelï¼Œæ˜¯é€šè¿‡å¦ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹è·å–</li></ul></li><li><p>token-labeling</p><ul><li>based on the dense score map provided by re-labelingï¼Œwe can assign each patch an individual label</li><li>auxiliary token labeling loss<ul><li>æ¯ä¸ªtokenéƒ½å¯¹åº”äº†ä¸€ä¸ªK-dim score map</li><li>å¯ä»¥è®¡ç®—ä¸€ä¸ªce</li></ul></li><li>given<ul><li>outputs of the transformer $[X^{cls}, X^1, â€¦, X^N]$</li><li>K-dim score map $[y^1, y^2, â€¦, y^N]$ </li><li>whole image label $y^{cls}$</li></ul></li><li>loss<ul><li>auxiliary token labeling lossï¼š$L_{aux} = \frac{1}{N} \sum_1^N CE(X^i, y^i)$</li><li>cls lossï¼š$L_{cls} = CE(X^{cls}, y^{cls})$</li><li>total lossï¼š$L_{total} = L_{cls}+\beta L_{aux}$ï¼Œ$\beta=0.5$</li></ul></li></ul></li><li><p>MixToken</p><ul><li>ä»Mixup&amp;CutMixå¯å‘æ¥çš„</li><li>ä¸ºäº†ç¡®ä¿each token have clear contentï¼Œæˆ‘ä»¬åŸºäºtoken embeddingè¿›è¡Œmixup</li><li>given<ul><li>token sequence $T_1=[t^1_1, t^2_1, â€¦, t^N_1]$ &amp; $T_2=[t^1_2, t^2_2, â€¦, t^N_2]$</li><li>token labels $y_1=[y^1_1, y^2_1, â€¦, y^N_1]$ &amp; $Y_2=[y^1_2, y^2_2, â€¦, y^N_2]$</li><li>binary mask M</li></ul></li><li>MixToken<ul><li>mixed token sequenceï¼š$\hat T = T_1 \odot M + T_2 \odot (1-M)$</li><li>mixed labelsï¼š$\hat Y = Y_1 \odot M + Y_2 \odot (1-M)$</li><li>mixed cls labelï¼š$\hat {Y^{cls}} = \overline M y_1^{cls} + (1-\overline M) y_2^{cls}$ï¼Œ$\overline M$ is the average of $M$</li></ul></li></ul></li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li><p>training details</p><ul><li>AdamW</li><li>linear lr scalingï¼šlarger when use token labeling</li><li>weight decay</li><li><p>dropoutï¼šhurts small modelsï¼Œuse Stochastic Depth instead</p><p><img src="/2021/05/21/LV-ViT/hyper.png" width="50%;"></p></li></ul></li><li><p>Training Technique Analysis</p><ul><li><p>more convs in patch embedding</p><p>  <img src="/2021/05/21/LV-ViT/convs.png" width="50%;"></p></li><li><p>enhanced residual</p><ul><li><p>smaller scaling factor</p><ul><li>the weight get larger gradients in residual branch</li><li>more information can be preserved in main branch</li><li>better performance</li><li>faster convergence</li></ul><p><img src="/2021/05/21/LV-ViT/enhanced residual.png" width="50%;"></p></li></ul></li><li><p>re-labeling</p><ul><li>use NFNet-F6 to re-label the ImageNet dataset and obtain the 1000-dimensional score map for each image</li><li>NFNet-F6 is trained from scratch</li><li>given input 576x576ï¼Œè·å¾—çš„score mapæ˜¯18x18x1000ï¼ˆs32ï¼‰</li><li>store the top5 probs for each position to save storage</li></ul></li><li><p>MixToken</p><ul><li>æ¯”baselineçš„CutMix methodè¦å¥½</li><li><p>åŒæ—¶çœ‹åˆ°token labelingæ¯”relabelingè¦å¥½</p><p><img src="/2021/05/21/LV-ViT/MixToken.png" width="50%;"></p></li></ul></li><li><p>token labeling</p><ul><li>relabelingæ˜¯åœ¨whole imageä¸Š</li><li>token labelingæ˜¯è¿›ä¸€æ­¥åœ°ï¼Œåœ¨token levelæ·»åŠ labelå’Œloss</li></ul></li><li><p>augmentation techniques</p><ul><li><p>å‘ç°MixUpä¼šhurt</p><p><img src="/2021/05/21/LV-ViT/aug.png" width="50%;"></p></li></ul></li><li><p>Model Scaling</p><ul><li><p>è¶Šå¤§è¶Šå¥½</p><p><img src="/2021/05/21/LV-ViT/scaling.png" width="50%;"></p></li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> visual transformer </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>memory bank</title>
      <link href="/2021/05/19/memory-bank/"/>
      <url>/2021/05/19/memory-bank/</url>
      <content type="html"><![CDATA[<ul><li>2018å¹´çš„paper</li><li>official codeï¼š<a href="https://github.com/zhirongw/lemniscate.pytorch" target="_blank" rel="noopener">https://github.com/zhirongw/lemniscate.pytorch</a></li><li>memory bank</li><li>NCE</li></ul><h2 id="Unsupervised-Feature-Learning-via-Non-Parametric-Instance-Discrimination"><a href="#Unsupervised-Feature-Learning-via-Non-Parametric-Instance-Discrimination" class="headerlink" title="Unsupervised Feature Learning via Non-Parametric Instance Discrimination"></a>Unsupervised Feature Learning via Non-Parametric Instance Discrimination</h2><ol><li><p>åŠ¨æœº</p><ul><li>unsupervised learning<ul><li>can we learn good feature representation that captures apparent similarity among instances instead of classes</li><li>formulate a non-parametric classification problem at instance-level</li><li>use noise contrastive estimation</li></ul></li><li>our non-parametric model<ul><li>highly compactï¼š128-d feature per imageï¼Œonly 600MB storage in total</li><li>enable fast nearest neighbour retrieval</li></ul></li><li>ã€QUESTIONã€‘æ— ç±»åˆ«æ ‡ç­¾ï¼Œå•é similarityï¼Œæœ€ç»ˆçš„åˆ†ç±»æ¨¡å‹æ˜¯å¦‚ä½•å»ºç«‹çš„ï¼Ÿ</li><li>verified on<ul><li>ImageNet 1K classification</li><li>semi-supervised learning</li><li>object detection tasks</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>observations<ul><li>ImageNet top-5 errè¿œæ¯”top-1 errå°</li><li>second highest responding class is more likely to be visually related</li><li>è¯´æ˜æ¨¡å‹éšå¼åœ°å­¦åˆ°äº†similarity</li><li>apparent similarity is learned not from se- mantic annotations, but from the visual data themselves</li></ul></li><li>å°†class-wise supervisionæ¨åˆ°ä¸€ä¸ªæé™<ul><li>å°±å˜æˆäº†instance-level</li><li>ç±»åˆ«æ•°å˜æˆäº†the whole training setï¼šsoftmax to many more classes becomes infeasible<ul><li>approximate the full softmax distribution with noise-contrastive estimation(NCE)</li><li>use a proximal regularization to stablize the learning process</li></ul></li></ul></li><li>train &amp; test<ul><li>é€šå¸¸çš„åšæ³•æ˜¯learned representationsåŠ ä¸€ä¸ªçº¿æ€§åˆ†ç±»å™¨</li><li>e.g. SVMï¼šä½†æ˜¯trainå’Œtestçš„feature spaceæ˜¯ä¸ä¸€è‡´çš„</li><li>æˆ‘ä»¬ç”¨äº†KNNï¼šsame metric space </li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>overview</p><ul><li>to learn a embedding function $f_{\theta}$</li><li>distance metric $d_{\theta}(x,y)  = ||f_{\theta}(x)-f_{\theta}(y)||$</li><li>to map visually similar images closer</li><li><p>instance-levelï¼što distinct between instances</p><p><img src="/2021/05/19/memory-bank/pipeline.png" width="70%;"></p></li></ul></li><li><p>Non-Parametric Softmax Classifier</p><ul><li><p>common parametric classifier</p><ul><li>givenç½‘ç»œé¢„æµ‹çš„N-dim representation $v=f_{\theta}(x)$</li><li>è¦é¢„æµ‹C-classesçš„æ¦‚ç‡ï¼Œéœ€è¦ä¸€ä¸ª$W^{NC}$çš„projectionï¼š$P(i|v) = \frac{exp (W^T_iv)}{\sum exp (W^T_jv)}$</li></ul></li><li><p>Non-Parametric version</p><ul><li>enforce $||v||=1$ via L2 norm</li><li>replace $W^T$ with $v^T$</li><li>then the probabilityï¼š$P(i|v) = \frac{exp (v^T_iv/\tau)}{\sum exp (v^T_jv / \tau)}$</li><li>temperature param $\tau$ï¼šcontrols the concentration level of the distribution </li><li><p>the goal is to minimize the negative log-likelihood</p></li><li><p>æ„ä¹‰ï¼šL2 normå°†æ‰€æœ‰çš„representationæ˜ å°„åˆ°äº†ä¸€ä¸ª128-d unit sphereä¸Šé¢ï¼Œ$v_i^T v_j$åº¦é‡äº†ä¸¤ä¸ªprojection vecçš„similarityï¼Œæˆ‘ä»¬å¸Œæœ›åŒç±»çš„vecå°½å¯èƒ½é‡åˆï¼Œä¸åŒç±»çš„vecå°½å¯èƒ½æ­£äº¤</p><ul><li>class weights $W$ are not generalized to new classes</li><li>but feature representations $V$ does</li></ul></li></ul></li><li><p>memory bank</p><ul><li>å› ä¸ºæ˜¯instance levelï¼ŒC-classeså¯¹åº”æ•´ä¸ªtraining setï¼Œä¹Ÿå°±æ˜¯è¯´${v_i}$ for all the images are needed for loss</li><li>Let $V={v_i}$ è¡¨ç¤ºmemory bankï¼Œåˆå§‹ä¸ºunit random vectors</li><li>every learning iterations<ul><li>$f_\theta$ is optimized by SGD</li><li>è¾“å…¥$x_i$æ‰€å¯¹åº”çš„$f_i$æ›´æ–°åˆ°$v_i$ä¸Š</li><li>ä¹Ÿå°±æ˜¯åªæœ‰mini-batchä¸­åŒ…å«çš„æ ·æœ¬ï¼Œåœ¨è¿™ä¸€ä¸ªstepï¼Œæ›´æ–°projection vec</li></ul></li></ul></li></ul></li><li><p>Noise-Contrastive Estimation</p><ul><li><p>non-parametric softmaxçš„è®¡ç®—é‡éšç€æ ·æœ¬é‡çº¿æ€§å¢é•¿ï¼Œmillions levelæ ·æœ¬é‡çš„æƒ…å†µä¸‹ï¼Œè®¡ç®—å¤ªheavyäº†</p></li><li><p>we use NCE to approximate the full softmax</p></li><li><p>assume</p><ul><li>noise samplesçš„uniform distributionï¼š$P_n =\frac{1}{n}$</li><li>noise samples are $m$ times frequent than data samples</li></ul></li><li><p>é‚£ä¹ˆsample $i$ matches vec $v$çš„åéªŒæ¦‚ç‡æ˜¯ï¼š$h(i,v)=\frac{P(i|v)}{P(i|v)+mP_n}$</p><ul><li>approximated training object is to minimize the negative log-likelihood of $h(i,v)$</li><li><img src="/2021/05/19/memory-bank/NCE.png" width="40%;"></li></ul></li><li><p>normalizing constant $Z$çš„è¿‘ä¼¼</p><ul><li><p><img src="/2021/05/19/memory-bank/norm.png" width="40%;"></p></li><li><p>ä¸»è¦å°±æ˜¯åˆ†æ¯è¿™ä¸ª$Z_i$çš„è®¡ç®—æ¯”è¾ƒheavyï¼Œæˆ‘ä»¬ç”¨Monte Carloé‡‡æ ·æ¥è¿‘ä¼¼ï¼š</p><p>  <img src="/2021/05/19/memory-bank/monte.png" width="40%;"></p></li><li><p>${j_k}$ is a random subset of indicesï¼šéšæœºæŠ½äº†memory bankçš„ä¸€ä¸ªå­é›†æ¥approxå…¨é›†çš„åˆ†æ¯ï¼Œå®éªŒå‘ç°å–batch sizeå¤§å°çš„å­é›†å°±å¯ä»¥ï¼Œm=4096</p></li></ul></li></ul></li><li><p>Proximal Regularization</p><ul><li><p>the learning process oscillates a lot</p><ul><li>we have one instance per class</li><li>during each training epoch each class is only visited once</li></ul></li><li><p>we introduce an additional term</p><ul><li>overall workflowï¼šåœ¨æ¯ä¸€ä¸ªiteration tï¼Œfeature representationæ˜¯$v_i^t=f_{\theta}(x_i)$ï¼Œè€Œmemory banké‡Œé¢çš„representationsæ¥è‡ªä¸Šä¸€ä¸ªiteration step $V={v^{t-1}}$ï¼Œæˆ‘ä»¬ä»memory banké‡Œé¢é‡‡æ ·ï¼Œå¹¶è®¡ç®—NCE lossï¼Œç„¶åbpæ›´æ–°ç½‘ç»œæƒé‡ï¼Œç„¶åå°†è¿™ä¸€è½®fpçš„representations updateåˆ°memory bankçš„æŒ‡å®šæ ·æœ¬ä¸Šï¼Œç„¶åä¸‹ä¸€è½®</li><li>å¯ä»¥å‘ç°ï¼Œåœ¨åˆå§‹randomé˜¶æ®µï¼Œæ¢¯åº¦æ›´æ–°ä¼šæ¯”è¾ƒå¿«è€Œä¸”ä¸ç¨³å®š</li><li><p>æˆ‘ä»¬ç»™positive sampleçš„lossä¸Šé¢å¤–åŠ äº†ä¸€ä¸ª$\lambda ||v_i^t-v_i^{t-1}||^2_2$ï¼Œæœ‰ç‚¹ç±»ä¼¼weight decayé‚£ç§ä¸œè¥¿ï¼Œå¼€å§‹é˜¶æ®µl2 lossä¼šå ä¸»å¯¼ï¼Œå¼•å¯¼ç½‘ç»œæ”¶æ•›</p></li><li><p><img src="/2021/05/19/memory-bank/proximal.png" width="50%;"></p></li><li><p>stabilize</p></li><li>speed up convergence</li><li>improve the learned representations </li></ul></li></ul></li><li><p>Weighted k-Nearest Neighbor Classifier</p><ul><li>a test timeï¼Œå…ˆè®¡ç®—feature representationï¼Œç„¶åè·Ÿmemory bankçš„vectorsåˆ†åˆ«è®¡ç®—cosine similarity $s_i=cos(v_i, f)$ï¼Œé€‰å‡ºtopk neighbours $N_k$ï¼Œç„¶åè¿›è¡Œweighted voting</li><li>weighted votingï¼š<ul><li>å¯¹æ¯ä¸ªclass cï¼Œè®¡ç®—å®ƒåœ¨topk neighboursçš„total weightï¼Œ$w_c =\sum_{i \in N_k} \alpha_i 1(c_i=c)$</li><li>$\alpha_i = exp(s_i/\tau)$</li></ul></li><li>k = 200</li><li>$\tau = 0.07$</li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> Unsupervised Learning, NCE </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>MoCoç³»åˆ—</title>
      <link href="/2021/04/30/MoCo%E7%B3%BB%E5%88%97/"/>
      <url>/2021/04/30/MoCo%E7%B3%BB%E5%88%97/</url>
      <content type="html"><![CDATA[<p>papersï¼š</p><p>[2019 MoCo v1] Momentum Contrast for Unsupervised Visual Representation Learningï¼Œkaiming</p><p>[2020 SimCLR] A Simple Framework for Contrastive Learning of Visual Representationsï¼ŒGoogle Brainï¼Œæ··è¿›æ¥æ˜¯å› ä¸ºå®ƒimprove based on MoCo v1ï¼Œè€ŒMoCo v2/v3åˆéƒ½æ˜¯åŸºäºå®ƒæ”¹è¿›</p><p>[2020 MoCo v2] Improved Baselines with Momentum Contrastive Learningï¼Œkaiming</p><p>[2021 MoCo v3] An Empirical Study of Training Self-Supervised Visual Transformersï¼Œkaiming</p><h2 id="preview-è‡ªç›‘ç£å­¦ä¹ -Self-supervised-Learning"><a href="#preview-è‡ªç›‘ç£å­¦ä¹ -Self-supervised-Learning" class="headerlink" title="preview: è‡ªç›‘ç£å­¦ä¹  Self-supervised Learning"></a>preview: è‡ªç›‘ç£å­¦ä¹  Self-supervised Learning</h2><ol><li><p>referenceï¼š<a href="https://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html" target="_blank" rel="noopener">https://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html</a></p></li><li><p>overview</p><p><img src="/2021/04/30/MoCoç³»åˆ—/æ— ç›‘ç£.png" width="40%;"></p><ul><li>å°±æ˜¯æ— ç›‘ç£</li><li>é’ˆå¯¹çš„ç—›ç‚¹ï¼ˆæœ‰ç›‘ç£è®­ç»ƒæ¨¡å‹ï¼‰<ul><li>æ ‡æ³¨æˆæœ¬é«˜</li><li>è¿ç§»æ€§å·®</li></ul></li><li>ä¼šåŸºäºæ•°æ®ç‰¹ç‚¹ï¼Œè®¾ç½®Pretext tasksï¼ˆæœ€å¸¸è§çš„ä»»åŠ¡å°±æ˜¯ç”Ÿæˆ/é‡å»ºï¼‰ï¼Œæ„é€ Pesdeo Labelsæ¥è®­ç»ƒç½‘ç»œ</li><li>é€šå¸¸æ¨¡å‹ç”¨æ¥ä½œä¸ºå…¶ä»–å­¦ä¹ ä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹</li><li>è¢«è®¤ä¸ºæ˜¯ç”¨æ¥å­¦ä¹ å›¾åƒçš„é€šç”¨è§†è§‰è¡¨ç¤º</li></ul></li><li><p>methods</p><ul><li><p>ä»ç»“æ„ä¸ŠåŒºåˆ†ä¸»è¦å°±æ˜¯ä¸¤å¤§ç±»æ–¹æ³•</p><ul><li>ç”Ÿæˆå¼ï¼šé€šè¿‡encoder-decoderç»“æ„è¿˜åŸè¾“å…¥ï¼Œç›‘ç£ä¿¡å·æ˜¯è¾“å…¥è¾“å‡ºå°½å¯èƒ½ç›¸ä¼¼<ul><li>é‡å»ºä»»åŠ¡å¼€é”€å¤§</li><li>æ²¡æœ‰å»ºç«‹ç›´æ¥çš„è¯­ä¹‰å­¦ä¹ </li><li>å¤–åŠ GANçš„åˆ¤åˆ«å™¨ä½¿å¾—ä»»åŠ¡æ›´åŠ å¤æ‚éš¾è®­</li></ul></li><li>åˆ¤åˆ«å¼ï¼šè¾“å…¥ä¸¤å¼ å›¾ç‰‡ï¼Œé€šè¿‡encoderç¼–ç ï¼Œç›‘ç£ä¿¡å·æ˜¯åˆ¤æ–­ä¸¤å¼ å›¾æ˜¯å¦ç›¸ä¼¼ï¼Œåˆ¤åˆ«å¼æ¨¡å‹ä¹Ÿå«Contrastive Learning</li></ul><p><img src="/2021/04/30/MoCoç³»åˆ—/self-supervised.png" width="70%;"></p></li><li><p>ä»Pretext tasksä¸Šåˆ’åˆ†ä¸»è¦åˆ†ä¸ºä¸‰ç±»</p><ul><li>åŸºäºä¸Šä¸‹æ–‡ï¼ˆContext basedï¼‰ ï¼šå¦‚bertçš„MLMï¼Œåœ¨å¥å­/å›¾ç‰‡ä¸­éšæœºæ‰£æ‰ä¸€éƒ¨åˆ†ï¼Œç„¶åæ¨åŠ¨æ¨¡å‹åŸºäºä¸Šä¸‹æ–‡/è¯­ä¹‰ä¿¡æ¯é¢„æµ‹è¿™éƒ¨åˆ†/ç›¸å¯¹ä½ç½®å…³ç³»</li><li>åŸºäºæ—¶åºï¼ˆTemporal Basedï¼‰ï¼šå¦‚bertçš„NSPï¼Œè§†é¢‘/è¯­éŸ³ï¼Œåˆ©ç”¨ç›¸é‚»å¸§çš„ç›¸ä¼¼æ€§ï¼Œæ„å»ºä¸åŒæ’åºçš„åºåˆ—ï¼Œåˆ¤æ–­Bæ˜¯å¦æ˜¯Açš„ä¸‹ä¸€å¥/æ˜¯å¦ç›¸é‚»å¸§</li><li>åŸºäºå¯¹æ¯”ï¼ˆContrastive Basedï¼‰ï¼šæ¯”è¾ƒæ­£è´Ÿæ ·æœ¬ï¼Œæœ€å¤§åŒ–ç›¸ä¼¼åº¦çš„lossåœ¨è¿™é‡Œé¢è¢«å«åšInfoNCE</li></ul></li></ul></li><li><p>memory-bank</p><ul><li>Contrastive Basedæ–¹æ³•æœ€å¸¸è§çš„æ–¹å¼æ˜¯åœ¨ä¸€ä¸ªbatchä¸­æ„å»ºæ­£è´Ÿæ ·æœ¬è¿›è¡Œå¯¹æ¯”å­¦ä¹ <ul><li>end-to-end</li><li>æ¯ä¸ªmini-batchä¸­çš„å›¾åƒå¢å¼ºå‰åçš„ä¸¤å¼ å›¾ç‰‡äº’ä¸ºæ­£æ ·æœ¬</li><li>å­—å…¸å¤§å°å°±æ˜¯minibatchå¤§å°</li></ul></li><li>memory bankåŒ…å«æ•°æ®é›†ä¸­æ‰€æœ‰æ ·æœ¬ç¼–ç åç‰¹å¾<ul><li>éšæœºé‡‡æ ·ä¸€éƒ¨åˆ†ä½œä¸ºkeys</li><li>æ¯ä¸ªè¿­ä»£åªæ›´æ–°è¢«é‡‡æ ·çš„æ ·æœ¬ç¼–ç </li><li>å› ä¸ºæ ·æœ¬ç¼–ç æ¥è‡ªä¸åŒçš„training stepï¼Œä¸€è‡´æ€§å·®</li></ul></li><li><p>MoCo</p><ul><li>åŠ¨æ€ç¼–ç åº“ï¼šout-of-dateçš„ç¼–ç å‡ºåˆ—</li><li>momentum updateï¼šä¸€è‡´æ€§æå‡</li></ul><p><img src="/2021/04/30/MoCoç³»åˆ—/memory-bank.png" width="60%;"></p></li></ul></li><li><p>InfoNCE</p><ul><li><p>deep mindåœ¨CPC(Contrastive Predictive Coding)æå‡ºï¼Œè®ºæ–‡ä»¥åæœ‰æœºä¼šå†å±•å¼€</p><ul><li>unsupervised</li><li>encoderï¼šencode x into latent space representations zï¼Œresnet blocks</li><li>autoregressive modelï¼šsummarize each time-step set of {z} into a context representation cï¼ŒGRUs</li><li><p>probabilistic contrastive loss</p><ul><li>Noise-Contrastive Estimation</li><li>Importance Sampling</li></ul><p><img src="/2021/04/30/MoCoç³»åˆ—/CPC.png" width="60%;"></p></li></ul></li><li><p>è®­ç»ƒç›®æ ‡æ˜¯è¾“å…¥æ•°æ®xå’Œcontext vector cä¹‹é—´çš„mutual information</p><ul><li>æ¯æ¬¡ä»$p(x_{t+k}|c_t)$ä¸­é‡‡æ ·ä¸€ä¸ªæ­£æ ·æœ¬ï¼šæ­£æ ·æœ¬æ˜¯è¿™ä¸ªåºåˆ—æ¥ä¸‹æ¥é¢„æµ‹çš„ä¸œè¥¿ï¼Œå’Œcçš„ç›¸ä¼¼æ€§è‚¯å®šè¦é«˜äºä¸æƒ³å¹²çš„token</li><li>ä»$p(x_{t+k})$ä¸­é‡‡æ ·N-1ä¸ªè´Ÿæ ·æœ¬ï¼šè´Ÿæ ·æœ¬æ˜¯åˆ«çš„åºåˆ—é‡Œé¢éšæœºé‡‡æ ·çš„ä¸œè¥¿</li><li><p>ç›®æ ‡æ˜¯è®©æ­£æ ·æœ¬ä¸contextç›¸å…³æ€§é«˜ï¼Œè´Ÿæ ·æœ¬ä½</p><p><img src="/2021/04/30/MoCoç³»åˆ—/InfoNCE.png" width="30%;"></p></li></ul></li></ul></li></ol><h2 id="MoCo-v1-Momentum-Contrast-for-Unsupervised-Visual-Representation-Learning"><a href="#MoCo-v1-Momentum-Contrast-for-Unsupervised-Visual-Representation-Learning" class="headerlink" title="MoCo v1: Momentum Contrast for Unsupervised Visual Representation Learning"></a>MoCo v1: Momentum Contrast for Unsupervised Visual Representation Learning</h2><ol><li><p>åŠ¨æœº</p><ul><li>unsupervised visual representation learning </li><li>contrastive learning </li><li><p>dynamic dictionary</p><ul><li>large</li><li>consisitent</li></ul></li><li><p>verified on</p><ul><li>7 down-stream tasks</li><li>ImageNet classification</li><li>VOC &amp; COCO det/seg</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>Unsupervised representation learning </p><ul><li>highly successful in NLPï¼Œin CV supervised is still the main-stream</li><li>ä¸¤ä¸ªæ ¸å¿ƒ<ul><li>pretext tasks</li><li>loss functions</li></ul></li><li>loss functions<ul><li>ç”Ÿæˆå¼æ–¹æ³•çš„lossæ˜¯åŸºäºpredictionå’Œä¸€ä¸ªfix targetæ¥è®¡ç®—çš„</li><li>contrastive-basedçš„key targetåˆ™æ˜¯vary on-the-fly during training</li><li>Adversarial lossesæ²¡å±•å¼€</li></ul></li><li>pretext tasks<ul><li>tasks involving recoverï¼šauto-encoder</li><li>task involving pseudo-labelsï¼šé€šå¸¸æœ‰ä¸ªexemplar/anchorï¼Œç„¶åè®¡ç®—contrastive loss </li></ul></li><li>contrastive learning VS pretext tasks<ul><li>å¤§é‡pretext taskså¯ä»¥é€šè¿‡è®¾è®¡ä¸€äº›contrastive lossæ¥å®ç°</li></ul></li></ul></li><li><p>recent approaches using contrastive loss</p><ul><li>dynamic dictionaries<ul><li>ç”±keysç»„æˆï¼šsampled from data &amp; represented by an encoder </li></ul></li><li>train the encoder to perform dictionary look-up<ul><li>given an encoded query</li><li>similar to its matching key and dissimilar to others</li></ul></li></ul></li><li><p>desirable dictionary </p><ul><li>largeï¼šbetter sample</li><li>consistentï¼štraining target consistent</li></ul></li><li><p>MoCoï¼šMomentum Contrast</p><ul><li>queue</li><li>æ¯ä¸ªit stepçš„mini-batchçš„ç¼–ç å…¥åº“</li><li>the oldest are dequeued</li><li><p>EMAï¼š</p><ul><li>a slowly progressing key encoder</li><li>momentum-based moving average of the query encoder</li></ul><p><img src="/2021/04/30/MoCoç³»åˆ—/MoCo.png" width="50%;"></p></li><li><p>similarçš„å®šä¹‰ï¼šq &amp; k are from the same image</p></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>contrastive learning</p><ul><li>a encoded query $q$</li><li>a set of encoded samples $\{k_0, k_1, â€¦\}$</li><li>assumeï¼šthere is a single key $k_+$ in the dictionary that $q$ matches</li><li>similarity measurementï¼šdot product</li><li>InfoNCEï¼š<ul><li>$L_q = -log \frac{exp(qk_+/\tau)}{\sum_0^K exp(qk/\tau)}$</li><li>1 positive &amp; K negtive samples</li><li>æœ¬è´¨ä¸Šæ˜¯ä¸ªsoftmax-based classifierï¼Œå°è¯•å°†$q$åˆ†ç±»æˆ$k_+$</li></ul></li><li>unsupervised workflow<ul><li>with a encoder network $f_q$ &amp; $f_k$</li><li>thus we have query &amp; sample representation $q=f_q(x^q)$ &amp; $k=f_k(x^k)$</li><li>inputs $x$ can be images/patches/context(patches set)</li><li>$f_q$ &amp; $f_k$ can be identical/partially shared/different</li></ul></li></ul></li><li><p>momentum contrast</p><ul><li><p>dictionary as a key</p><ul><li>the dictionary always represents a sampled subset of all data</li><li>the current mini-batchå…¥åˆ—</li><li>the oldest mini-batchå‡ºåˆ—</li></ul></li><li><p>momentum update</p><ul><li><p>large dictionaryæ²¡æ³•å¯¹keysè¿›è¡Œback-propagationï¼šå› ä¸ºsampleå¤ªå¤šäº†</p></li><li><p>only $f_q$ are updated by back-propagationï¼šmini-batch</p></li><li><p>naive solutionï¼šcopy $f_q$çš„å‚æ•°ç»™$f_k$ï¼Œyields poor resultsï¼Œå› ä¸ºkey encoderå‚æ•°å˜åŒ–å¤ªé¢‘ç¹äº†ï¼Œrepresentation inconsistent issue</p></li><li><p>momentum updateï¼š$f_k = mf_k + (1-m)f_q$ï¼Œ$m=0.999$</p></li><li><p>ä¸‰ç§æ›´æ–°æ–¹å¼å¯¹æ¯”</p><p>  <img src="/2021/04/30/MoCoç³»åˆ—/key encoder.png" width="80%;"></p><ul><li>ç¬¬ä¸€ç§end-to-end methodï¼š<ul><li>use samples in current mini-batch as the dictionary</li><li>keys are consistently encoded</li><li>dictionary size is limited</li></ul></li><li>ç¬¬äºŒç§memory bank<ul><li>A memory bank consists of the representations of all samples in the dataset</li><li>the dictionary for each mini-batch is randomly sampled from the memory bankï¼Œä¸è¿›è¡Œbpï¼Œthus enables large dictionary</li><li>key representation is updated when it was last seenï¼šinconsistent</li><li>æœ‰äº›ä¹Ÿç”¨momentum updateï¼Œä½†æ˜¯æ˜¯ç”¨åœ¨representationä¸Šï¼Œè€Œä¸æ˜¯encoderå‚æ•°</li></ul></li></ul></li></ul></li><li><p>pretext task</p><ul><li>define positive pairï¼šif the query and the key come from the same image</li><li>æˆ‘ä»¬ä»å›¾ä¸Štake two random views under random augmentation to form a positive pair</li><li>ç„¶åç”¨å„è‡ªçš„encoderç¼–ç æˆq &amp; k</li><li>æ¯ä¸€å¯¹è®¡ç®—similarityï¼špos similarity</li><li>ç„¶åå†è®¡ç®—input querieså’Œdictionaryçš„similarityï¼šneg similarity</li><li>è®¡ç®—ceï¼Œupdate $f_q$</li><li>ç”¨$f_q$ update $f_k$</li><li>æŠŠkåŠ å…¥dictionaryé˜Ÿåˆ—</li><li><p>æŠŠæœ€æ—©çš„mini-batchå‡ºåˆ—</p><p><img src="/2021/04/30/MoCoç³»åˆ—/MoCo code.png" width="50%;"></p></li><li><p>æŠ€æœ¯ç»†èŠ‚</p><ul><li>resnetï¼šlast fc dim=128ï¼ŒL2 norm</li><li>temperature $\tau=0.07$</li><li>augmentation<ul><li>random resize + random(224,224) crop</li><li>random color jittering</li><li>random horizontal flip</li><li>random grayscale conversion</li></ul></li><li>shuffling BN<ul><li>å®éªŒå‘ç°ä½¿ç”¨resneté‡Œé¢çš„BNä¼šå¯¼è‡´ä¸å¥½çš„ç»“æœï¼šçŒœæµ‹æ˜¯intra-batch communicationå¼•å¯¼æ¨¡å‹å­¦ä¹ äº†ä¸€ç§cheatingçš„low-loss solution</li><li>å…·ä½“åšæ³•æ˜¯ç»™$f_k$çš„è¾“å…¥mini-batchå…ˆshuffle the orderï¼Œç„¶åè¿›è¡Œfpï¼Œç„¶åå†shuffle backï¼Œè¿™æ ·$f_q$å’Œ$f_k$çš„BNè®¡ç®—çš„mini-batchçš„staticså°±æ˜¯ä¸åŒçš„</li></ul></li></ul></li></ul></li></ul></li></ul></li><li><p>å®éªŒ</p></li></ol><h2 id="SimCLR-A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations"><a href="#SimCLR-A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations" class="headerlink" title="SimCLR: A Simple Framework for Contrastive Learning of Visual Representations"></a>SimCLR: A Simple Framework for Contrastive Learning of Visual Representations</h2><ol><li><p>åŠ¨æœº</p><ul><li>simplify recently proposed contrastive self-supervised learning algorithms </li><li>systematically study the major components <ul><li>data augmentations</li><li>learnable unlinear prediction head</li><li>larger batch size and more training steps</li></ul></li><li><p>outperform previous self-supervised &amp; semi-supervised learning methods on ImageNet</p><p><img src="/2021/04/30/MoCoç³»åˆ—/acc.png" width="40%;"></p></li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>discriminative approaches based on contrastive learning</p><ul><li>maximizing agreement between differently augmented views of the same data sample </li><li>via a contrastive loss in the latent space</li></ul></li><li><p>major components &amp; conclusions</p><ul><li>æ•°æ®å¢å¼ºå¾ˆé‡è¦ï¼Œunsupervisedæ¯”supervised benefits more</li><li>å¼•å…¥çš„learnable nonlinear transformationæå‡äº†representation quality</li><li>contrastive cross entropy losså—ç›Šäºnormalized embeddingå’Œadjusted temperature parameter</li><li>larger batch size and more training stepså¾ˆé‡è¦ï¼Œunsupervisedæ¯”supervised benefits more</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>common framework</p><p>  <img src="/2021/04/30/MoCoç³»åˆ—/framework.png" width="40%;"></p><ul><li><p>4 major components</p><ul><li>éšæœºæ•°æ®å¢å¼º<ul><li>results in two views of the same sampleï¼Œæ„æˆpositive pair</li><li>crop + resize back + color distortions + gaussian blur</li></ul></li><li>base encoder<ul><li>ç”¨å•¥éƒ½è¡Œï¼Œæœ¬æ–‡ç”¨äº†resnet including the GAP</li></ul></li><li>a projection head<ul><li>å°†representation dimæ˜ å°„åˆ°the space where contrastive loss is appliedï¼ˆgiven 1 pos pair &amp; N neg pairï¼Œå°±æ˜¯N+1 dimï¼‰</li><li>ä¹‹å‰æœ‰æ–¹æ³•ç›´æ¥ç”¨linear projection</li><li>æˆ‘ä»¬ç”¨äº†å¸¦ä¸€ä¸ªhidden layerçš„MLPï¼šfc-bn-relu-fc</li></ul></li><li>a contrastive loss</li></ul></li><li><p>overall workflow</p><ul><li>random sample a minibatch of N</li><li>random augmentation results in 2N data points</li><li>å¯¹æ¯ä¸ªæ ·æœ¬æ¥è®²ï¼Œæœ‰1ä¸ªpositive pairï¼Œå…¶ä½™2(N-1)ä¸ªdata pointséƒ½æ˜¯negative samples</li><li>set cosine similarity $sim(u,v)=u^Tv/|u||v|$</li><li>given positive pair $(i,j)$ then the loss is $l_{i,j} = -log \frac{exp(s_{i,j}/\tau)}{\sum_{k\neq i}^{2N} exp(s_{i,k}/\tau)}$</li><li>å¯¹æ¯ä¸ªpositive pairéƒ½è®¡ç®—ï¼ŒåŒ…æ‹¬$(i,j)$ å’Œ$(j,i)$ï¼Œå«é‚£ä¸ªsymmetrized loss</li><li><p>update encoder</p><p><img src="/2021/04/30/MoCoç³»åˆ—/SimCLR.png" width="40%;"></p></li></ul></li></ul></li><li><p>training with large batch size</p><ul><li>batch 8192ï¼Œnegatives 16382</li><li>å¤§batchæ—¶ï¼Œlinear learning rate scalingå¯èƒ½ä¸ç¨³å®šï¼Œæ‰€ä»¥ç”¨äº†LARS optmizer</li><li>global BNï¼Œaggregate BN mean &amp; variance over all devices</li><li>TPU</li></ul></li></ul></li></ol><h2 id="MoCo-v2-Improved-Baselines-with-Momentum-Contrastive-Learning"><a href="#MoCo-v2-Improved-Baselines-with-Momentum-Contrastive-Learning" class="headerlink" title="MoCo v2: Improved Baselines with Momentum Contrastive Learning"></a>MoCo v2: Improved Baselines with Momentum Contrastive Learning</h2><ol><li><p>åŠ¨æœº</p><ul><li>still working on contrastive unsupervised learning</li><li>simple modifications on MoCo<ul><li>introduce two effective SimCLRâ€™s designsï¼š</li><li>an MLP head</li><li>more data augmentation</li><li>requires smaller batch size than SimCLRï¼Œmaking it possible to run on GPU</li></ul></li><li>verified on<ul><li>ImageNet classification </li><li>VOC detection</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>MoCo &amp; SimCLR <ul><li>contrastive unsupervised learning frameworks</li><li>MoCo v1 shows promising</li><li>SimCLR further reduce the gap</li><li>we found two design imrpovements in SimCLR åœ¨ä¸¤ä¸ªæ–¹æ³•ä¸­éƒ½workï¼Œè€Œä¸”ç”¨åœ¨MoCoä¸­shows better transfer learning results<ul><li>an MLP projection head</li><li>stronger data augmentation</li></ul></li><li>åŒæ—¶MoCo frameworkç›¸æ¯”è¾ƒäºSimCLR ï¼Œè¿œä¸éœ€è¦large training batches<ul><li>SimCLR based on end-to-end mechanismï¼Œéœ€è¦æ¯”è¾ƒå¤§çš„batch sizeï¼Œæ¥æä¾›è¶³å¤Ÿå¤šçš„negative pair</li><li>MoCoåˆ™ç”¨äº†åŠ¨æ€é˜Ÿåˆ—ï¼Œæ‰€ä»¥ä¸é™åˆ¶batch size</li></ul></li></ul></li><li>SimCLR <ul><li>improves the end-to-end method</li><li>larger batchï¼što provide more negative samples</li><li>output layerï¼šreplace fc with a MLP head</li><li>stronger data augmentation</li></ul></li><li><p>MoCo</p><ul><li>a large number of negative samples are readily available</li><li>æ‰€ä»¥å°±æŠŠåä¸¤é¡¹å¼•å…¥è¿›æ¥äº†</li></ul><p><img src="/2021/04/30/MoCoç³»åˆ—/optimization.png" width="50%;"></p></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>MLP head</p><ul><li>2-layer MLP(hidden dim=2048, ReLU)</li><li>ä»…å½±å“unsupervised trainingï¼Œæœ‰ç›‘ç£transfer learningçš„æ—¶å€™æ¢å¤´</li><li><p>temperature paramè°ƒæ•´ï¼šä»default 0.07 è°ƒæ•´æˆoptimal value 0.2</p><p><img src="/2021/04/30/MoCoç³»åˆ—/temperature.png" width="50%;"></p></li></ul></li><li><p>augmentation</p><ul><li>add blur</li><li>SimCLRè¿˜ç”¨äº†stronger color distortionï¼šwe found stronger color distortion in SimCLR hurts in our MoCoï¼Œæ‰€ä»¥æ²¡åŠ </li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li><p>ablation</p><ul><li>MLPï¼šåœ¨åˆ†ç±»ä»»åŠ¡ä¸Šçš„æå‡æ¯”æ£€æµ‹å¤§</li><li><p>augmentationï¼šåœ¨æ£€æµ‹ä¸Šçš„æå‡æ¯”åˆ†ç±»å¤§</p><p><img src="/2021/04/30/MoCoç³»åˆ—/ablation.png" width="50%;"></p></li></ul></li><li><p>comparison</p><p>  <img src="/2021/04/30/MoCoç³»åˆ—/comparison.png" width="50%;"></p><ul><li>large batches are not necessary for good accï¼šSimCLR longer trainingé‚£ä¸ªç‰ˆæœ¬ç²¾åº¦æ›´é«˜</li><li>end-to-endçš„æ–¹æ³•è‚¯å®šmore costly in memory and timeï¼šå› ä¸ºè¦bpä¸¤ä¸ªencoder</li></ul></li></ul></li></ol><h2 id="MoCo-v3-An-Empirical-Study-of-Training-Self-Supervised-Visual-Transformers"><a href="#MoCo-v3-An-Empirical-Study-of-Training-Self-Supervised-Visual-Transformers" class="headerlink" title="MoCo v3: An Empirical Study of Training Self-Supervised Visual Transformers"></a>MoCo v3: An Empirical Study of Training Self-Supervised Visual Transformers</h2><ol><li><p>åŠ¨æœº</p><ul><li>self-supervised frameworks that based on Siamese network, including MoCo</li><li>ViTï¼šstudy the fundamental components for training self-supervised ViT</li><li>MoCo v3ï¼šan incremental improvement of MoCo v1/2ï¼Œstriking for a better balance of simplicity &amp; accuracy &amp; scalability</li><li>instability is a major issue</li><li>scaling up ViT models<ul><li>ViT-Large</li><li>ViT-Huge</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>we go back to the basics and investigate the fundamental components of training deep neural networks<ul><li>batch size</li><li>learning rate</li><li>optmizer</li></ul></li><li>instability <ul><li>instability is a major issue that impacts self-supervised ViT training</li><li>but may not result in catastrophic failureï¼Œåªä¼šå¯¼è‡´ç²¾åº¦æŸå¤±</li><li>æ‰€ä»¥ç§°ä¹‹ä¸ºhidden degradation</li><li>use a simple trick to improve stabilityï¼šfreeze the patch projection layer in ViT</li><li>and observes increasement in acc</li></ul></li><li>NLPé‡Œé¢åŸºäºmasked auto-encodingçš„frameworkæ•ˆæœè¦æ¯”åŸºäºcontrastvieçš„frameworkå¥½ï¼Œå›¾åƒæ­£å¥½åè¿‡æ¥</li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>MoCo v3</p><ul><li>take two crops for each image under random augmentation</li><li>encoded by two encoders $f_q$ &amp; $f_k$ into vectors $q$ &amp; $k$</li><li>we use the keys that naturally co-exist in the same batch<ul><li>abandon the memory queueï¼šå› ä¸ºå‘ç°batch sizeè¶³å¤Ÿå¤§ï¼ˆ4096ï¼‰çš„æ—¶å€™ï¼Œmemory queueå°±æ²¡å•¥acc gainäº†</li><li>å›å½’åˆ°batch-based sample pair</li></ul></li><li>ä½†æ˜¯encoder kä»æ—§ä¸å›ä¼ æ¢¯åº¦ï¼Œè¿˜æ˜¯åŸºäºencoder qè¿›è¡ŒåŠ¨é‡æ›´æ–°</li><li><p>symmetrized lossï¼š</p><ul><li>$ctr(q_1, k_2) + ctr(q_2,k_1)$</li><li>InfoNCE</li><li>temperature</li><li>ä¸¤ä¸ªcropsåˆ†åˆ«è®¡ç®—ctr</li></ul><p><img src="/2021/04/30/MoCoç³»åˆ—/MoCo v3.png" width="50%;"></p></li></ul></li><li><p>encoder </p><ul><li>encoder $f_q$<ul><li>a backbone</li><li>a projection head</li><li>an extra prediction head</li></ul></li><li>encoder $f_k$<ul><li>a backbone</li><li>a projection head</li></ul></li><li>encoder $f_k$ is updated by the moving average of $f_q$ï¼Œexcluding the prediction head</li></ul></li><li><p>baseline acc</p><p>  <img src="/2021/04/30/MoCoç³»åˆ—/v3 base.png" width="50%;"></p><ul><li>basic settingsï¼Œä¸»è¦å˜åŠ¨å°±æ˜¯ä¸¤ä¸ªï¼š<ul><li>dynamic queueæ¢æˆlarge batch</li><li>encoder $f_q$çš„extra prediction head</li></ul></li></ul></li><li><p>use ViT</p><ul><li><p>ç›´æ¥ç”¨ViTæ›¿æ¢resnet back met instability issue</p></li><li><p>batch size</p><ul><li><p>ViTé‡Œé¢çš„ä¸€ä¸ªè§‚ç‚¹å°±æ˜¯ï¼Œmodelæœ¬èº«æ¯”è¾ƒheavyï¼Œæ‰€ä»¥large batch is desirable</p></li><li><p>å®éªŒå‘ç°</p><ul><li>a batch of 1k &amp; 2k produces reasonably smooth curvesï¼šIn this regime, the larger batch improves accuracy thanks to more negative samples</li><li>a batch of 4k æœ‰æ˜æ˜¾çš„untable dipsï¼š</li><li><p>a batch of 6k has worse failure patternsï¼šæˆ‘ä»¬è§£è¯»ä¸ºåœ¨è·³æ°´ç‚¹ï¼Œtraining is partially restarted and jumps out of the current local optimum</p><p><img src="/2021/04/30/MoCoç³»åˆ—/batch.png" width="50%;"></p></li></ul></li></ul></li><li><p>learning rate</p><ul><li>lrè¾ƒå°ï¼Œtrainingæ¯”è¾ƒç¨³å®šï¼Œä½†æ˜¯å®¹æ˜“æ¬ æ‹Ÿåˆ</li><li>lrè¿‡å¤§ï¼Œä¼šå¯¼è‡´unstableï¼Œä¹Ÿä¼šå½±å“acc</li><li><p>æ€»ä½“æ¥è¯´ç²¾åº¦è¿˜æ˜¯å†³å®šäºstability</p><p><img src="/2021/04/30/MoCoç³»åˆ—/lr.png" width="50%;"></p></li></ul></li><li><p>optimizer</p><ul><li>default adamWï¼Œbatch size 4096</li><li>æœ‰äº›æ–¹æ³•ç”¨äº†LARS &amp; LAMB for large-batch training</li><li><p>LAMB </p><ul><li>sensitive to lr</li><li>optmal lr achieves slightly better accuracy than AdamW</li><li>ä½†æ˜¯lrä¸€æ—¦è¿‡å¤§ï¼Œaccæé€Ÿdrop</li><li>ä½†æ˜¯training curves still smoothï¼Œè™½ç„¶ä¸­é—´è¿‡ç¨‹æœ‰dropï¼šæˆ‘ä»¬è§£è¯»ä¸ºLAMB can avoid sudden change in the gradientsï¼Œä½†æ˜¯é¿å…ä¸äº†negative compactï¼Œè¿˜æ˜¯ä¼šç´¯åŠ </li></ul><p><img src="/2021/04/30/MoCoç³»åˆ—/lamb.png" width="45%;"></p></li></ul></li><li><p>a trick for improving stability</p><ul><li>we found a spike in gradient causes a dip in the training curve</li><li>we also observe that gradient spikes happen earlier in the first layer (patch projection)</li><li><p>æ‰€ä»¥å°è¯•freezing the patch projection layer during trainingï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªrandomçš„patch projection layer</p><ul><li>This stability benefits the final accuracy</li><li>The improvement is bigger for a larger lr</li><li>åœ¨åˆ«çš„ViT-back-frameworkä¸Šä¹Ÿæœ‰æ•ˆï¼ˆSimCLRã€BYOLï¼‰</li></ul><p><img src="/2021/04/30/MoCoç³»åˆ—/random.png" width="45%;"></p></li></ul></li><li><p>we also tried BNï¼ŒWNï¼Œgradient clip</p><ul><li>BN/WN does not improve</li><li>gradient clipåœ¨thresholdè¶³å¤Ÿå°çš„æ—¶å€™æœ‰ç”¨ï¼Œæ¨åˆ°æé™å°±æ˜¯freezingäº†</li></ul></li></ul></li><li><p>implementation details</p><ul><li>AdamW</li><li>batch size 4096</li><li>lrï¼šwarmup 40 eps then cosine decay</li></ul></li><li><p>MLP heads</p><ul><li>projection headï¼š3-layersï¼Œ4096-BN-ReLU-4096-BN-ReLU-256</li><li>prediction headï¼š2-layersï¼Œ4096-BN-ReLU-256</li></ul></li><li><p>loss</p><ul><li>ctré‡Œé¢æœ‰ä¸ªscaleçš„å‚æ•°ï¼Œ$2\tau$</li><li>makes it less sensitive to $\tau$ value</li><li>$\tau=0.2$</li></ul></li><li><p>ViT architecture</p><p>  <img src="/2021/04/30/MoCoç³»åˆ—/ViT.png" width="45%;"></p><ul><li>è·ŸåŸè®ºæ–‡ä¿æŒä¸€è‡´</li><li>è¾“å…¥æ˜¯224x244çš„imageï¼Œåˆ’åˆ†æˆ16x16/14x14çš„patch sequenceï¼Œprojectæˆ256d/196dçš„embedding</li><li>åŠ ä¸Šsine-cosine-2Dçš„PE</li><li>å†concatä¸€ä¸ªcls token</li><li>ç»è¿‡ä¸€ç³»åˆ—transformer blocks</li><li>The class token after the last block (and after the final LayerNorm) is treated as the output of the backboneï¼Œand is the input to the MLP heads</li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> self-supervised learning, transformer, contrastive loss </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>optimizersä¼˜åŒ–å™¨</title>
      <link href="/2021/03/15/optimizers%E4%BC%98%E5%8C%96%E5%99%A8/"/>
      <url>/2021/03/15/optimizers%E4%BC%98%E5%8C%96%E5%99%A8/</url>
      <content type="html"><![CDATA[<h2 id="0-overview"><a href="#0-overview" class="headerlink" title="0. overview"></a>0. overview</h2><p>keywordsï¼šSGD, moment, Nesterov, adaptive, ADAM, Weight decay</p><ol><li>ä¼˜åŒ–é—®é¢˜Optimization<ul><li>to minimizeç›®æ ‡å‡½æ•°</li><li>grandient decent<ul><li>gradient<ul><li>numericalï¼šæ•°å€¼æ³•ï¼Œapproxï¼Œslow</li><li>analyticalï¼šè§£ææ³•ï¼Œexactï¼Œfast</li></ul></li><li>Stochastic<ul><li>ç”¨minibatchçš„æ¢¯åº¦æ¥approximateå…¨é›†</li><li>$\theta_{k+1} = \theta_k - v_{t+1}(x_i,y_i)$</li></ul></li><li>classic optimizersï¼šSGDï¼ŒMomentumï¼ŒNesterovâ€˜s momentum</li><li>adaptive optimizersï¼šAdaGradï¼ŒAdadeltaï¼ŒRMSPropï¼ŒAdam</li></ul></li><li>Newton</li></ul></li></ol><ul><li>modern optimizers for large-batch<pre><code>  * AdamW</code></pre><ul><li>LARS</li><li>LAMB</li></ul></li></ul><ol><li><p>common updating steps</p><p> for current step tï¼š</p><p> step1ï¼šè®¡ç®—ç›´æ¥æ¢¯åº¦ï¼Œ$g_t = \nabla f(w_t)$</p><p> step2ï¼šè®¡ç®—ä¸€é˜¶åŠ¨é‡å’ŒäºŒé˜¶åŠ¨é‡ï¼Œ$m_t \&amp; V_t$</p><p> step3ï¼šè®¡ç®—å½“å‰æ—¶åˆ»çš„ä¸‹é™æ¢¯åº¦ï¼Œ$\eta_t = \alpha m_t/\sqrt {V_t}$</p><p> step4ï¼šå‚æ•°æ›´æ–°ï¼Œ$w_{t+1} = w_t - \eta_t$</p><ul><li>å„ç§ä¼˜åŒ–ç®—æ³•çš„ä¸»è¦å·®åˆ«åœ¨step1å’Œstep2ä¸Š</li></ul></li><li><p>æ»‘åŠ¨å¹³å‡/æŒ‡æ•°åŠ æƒå¹³å‡/moving average/EMA</p><ul><li>å±€éƒ¨å‡å€¼ï¼Œä¸ä¸€æ®µæ—¶é—´å†…çš„å†å²ç›¸å…³</li><li>$v_t = \beta v_{t-1}+(1-\beta)\theta_t$ï¼Œå¤§è‡´ç­‰äºè¿‡å»$1/(1-\beta)$ä¸ªæ—¶åˆ»çš„$\theta$çš„å¹³å‡å€¼ï¼Œä½†æ˜¯åœ¨èµ·å§‹ç‚¹é™„è¿‘åå·®è¾ƒå¤§</li><li>$v_{tbiased} = \frac{v_t}{1-\beta^t}$ï¼Œåšäº†bias correction</li><li>tè¶Šå¤§ï¼Œè¶Šä¸éœ€è¦ä¿®æ­£ï¼Œä¸¤ä¸ªæ»‘åŠ¨å‡å€¼çš„ç»“æœè¶Šæ¥è¿‘</li><li><p>ä¼˜ç¼ºç‚¹ï¼šä¸ç”¨ä¿å­˜å†å²ï¼Œä½†æ˜¯è¿‘ä¼¼</p><p><img src="/2021/03/15/optimizersä¼˜åŒ–å™¨/moving average.png" width="50%;"></p></li></ul></li><li><p>SGD</p><ul><li>SGDæ²¡æœ‰åŠ¨é‡çš„æ¦‚å¿µï¼Œ$m_t=g_t$ï¼Œ$V_t=I^2$ï¼Œ$w_{t+1} = w_t - \alpha g_t$</li></ul></li></ol><ul><li>ä»…ä¾èµ–å½“å‰è®¡ç®—çš„æ¢¯åº¦<ul><li>ç¼ºç‚¹ï¼šä¸‹é™é€Ÿåº¦æ…¢ï¼Œå¯èƒ½é™·åœ¨local optimaä¸ŠæŒç»­éœ‡è¡</li></ul></li></ul><ol><li><p>SGDW (with weight decay)</p><ul><li>åœ¨æƒé‡æ›´æ–°çš„åŒæ—¶è¿›è¡Œæƒé‡è¡°å‡</li><li>$w_{t+1} = (1-\lambda)w_t - \alpha g_t$</li><li>åœ¨SGD formçš„ä¼˜åŒ–å™¨ä¸­weight decayç­‰ä»·äºåœ¨lossä¸ŠL2 regularization</li><li>ä½†æ˜¯åœ¨adaptive formçš„ä¼˜åŒ–å™¨ä¸­æ˜¯ä¸ç­‰ä»·çš„ï¼ï¼å› ä¸ºhistorical funcï¼ˆERMï¼‰ä¸­regularizerå’Œgradientä¸€èµ·è¢«downscaleäº†ï¼Œå› æ­¤not as much as they would get regularized in SGDW</li></ul></li><li><p>SGD with Momentum</p><ul><li>å¼•å…¥ä¸€é˜¶åŠ¨é‡ï¼Œ$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$ï¼Œä½¿ç”¨æ»‘åŠ¨å‡å€¼ï¼ŒæŠ‘åˆ¶éœ‡è¡</li><li>æ¢¯åº¦ä¸‹é™çš„ä¸»è¦æ–¹å‘æ˜¯æ­¤å‰ç´¯ç§¯çš„ä¸‹é™æ–¹å‘ï¼Œç•¥å¾®å‘å½“å‰æ—¶åˆ»çš„æ–¹å‘è°ƒæ•´</li></ul></li><li><p>SGD with Nesterov Acceleration</p><ul><li>look ahead SGD-momentum</li><li>åœ¨local minimaçš„æ—¶å€™ï¼Œå››å‘¨æ²¡æœ‰ä¸‹é™çš„æ–¹å‘ï¼Œä½†æ˜¯å¦‚æœèµ°ä¸€æ­¥å†çœ‹ï¼Œå¯èƒ½å°±ä¼šæ‰¾åˆ°ä¼˜åŒ–æ–¹å‘</li><li>å…ˆè·Ÿç€ç´¯ç§¯åŠ¨é‡èµ°ä¸€æ­¥ï¼Œæ±‚æ¢¯åº¦ï¼š$g_t = \nabla f(w_t-\alpha m_{t-1}/\sqrt {V_{t-1}})$</li><li>ç”¨è¿™ä¸ªç‚¹çš„æ¢¯åº¦æ–¹å‘æ¥è®¡ç®—æ»‘åŠ¨å¹³å‡ï¼Œå¹¶æ›´æ–°æ¢¯åº¦</li></ul></li><li><p>Adagrad</p><ul><li>å¼•å…¥äºŒé˜¶åŠ¨é‡ï¼Œå¼€å¯â€œè‡ªé€‚åº”å­¦ä¹ ç‡â€ï¼Œ$V_t = \sum_0^t g_k^2$ï¼Œåº¦é‡å†å²æ›´æ–°é¢‘ç‡</li><li>å¯¹äºç»å¸¸æ›´æ–°çš„å‚æ•°ï¼Œæˆ‘ä»¬å·²ç»ç§¯ç´¯äº†å¤§é‡å…³äºå®ƒçš„çŸ¥è¯†ï¼Œä¸å¸Œæœ›è¢«å•ä¸ªæ ·æœ¬å½±å“å¤ªå¤§ï¼Œå¸Œæœ›å­¦ä¹ é€Ÿç‡æ…¢ä¸€äº›ï¼›å¯¹äºå¶å°”æ›´æ–°çš„å‚æ•°ï¼Œæˆ‘ä»¬äº†è§£çš„ä¿¡æ¯å¤ªå°‘ï¼Œå¸Œæœ›èƒ½ä»æ¯ä¸ªå¶ç„¶å‡ºç°çš„æ ·æœ¬èº«ä¸Šå¤šå­¦ä¸€äº›ï¼Œå³å­¦ä¹ é€Ÿç‡å¤§ä¸€äº›</li><li>$\eta_t = \alpha m_t / \sqrt{V_t}$ï¼Œæœ¬è´¨ä¸Šä¸ºæ¯ä¸ªå‚æ•°ï¼Œå¯¹å­¦ä¹ ç‡åˆ†åˆ«rescale</li><li>ç¼ºç‚¹ï¼šäºŒé˜¶åŠ¨é‡å•è°ƒé€’å¢ï¼Œå¯¼è‡´å­¦ä¹ ç‡å•è°ƒè¡°å‡ï¼Œå¯èƒ½ä¼šä½¿å¾—è®­ç»ƒè¿‡ç¨‹æå‰ç»“æŸ</li></ul></li><li><p>AdaDelta/RMSProp</p><ul><li>å‚è€ƒmomentumï¼Œå¯¹äºŒé˜¶åŠ¨é‡ä¹Ÿè®¡ç®—æ»‘åŠ¨å¹³å‡ï¼Œ$V_t = \beta_2 V_{t-1} + (1-\beta_2)g_t^2$</li><li>é¿å…äº†äºŒé˜¶åŠ¨é‡æŒç»­ç´¯ç§¯ã€å¯¼è‡´è®­ç»ƒè¿‡ç¨‹æå‰ç»“æŸ</li></ul></li><li><p>Adam</p><ul><li>é›†å¤§æˆè€…ï¼šæŠŠä¸€é˜¶åŠ¨é‡å’ŒäºŒé˜¶åŠ¨é‡éƒ½ç”¨èµ·æ¥ï¼ŒAdaptive Momentum<ul><li>SGD-Måœ¨SGDåŸºç¡€ä¸Šå¢åŠ äº†ä¸€é˜¶åŠ¨é‡</li><li>AdaGradå’ŒAdaDeltaåœ¨SGDåŸºç¡€ä¸Šå¢åŠ äº†äºŒé˜¶åŠ¨é‡</li></ul></li><li>ä¸€é˜¶åŠ¨é‡æ»‘åŠ¨å¹³å‡ï¼š$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$</li><li>äºŒé˜¶åŠ¨é‡æ»‘åŠ¨å¹³å‡ï¼š$V_t = \beta_2 V_{t-1} + (1-\beta_2)g_t^2$</li></ul></li><li><p>Nadam</p><ul><li>look ahead Adam</li><li>æŠŠNesterovçš„one step tryåŠ ä¸Šï¼š$g_t = \nabla f(w_t-\alpha m_{t-1}/\sqrt {V_{t-1}})$</li><li>å†Adamæ›´æ–°ä¸¤ä¸ªåŠ¨é‡</li></ul></li><li><p>ç»éªŒè¶…å‚</p><ul><li>$momentum=0.9$</li><li>$\beta_1=0.9$</li><li>$\beta_2=0.999$</li><li>$m_0 = 0$</li><li>$V_0 = 0$</li><li>ä¸Šé¢çš„å›¾ä¸Šå¯ä»¥çœ‹å‡ºï¼ŒåˆæœŸçš„$m_t$å’Œ$V_t$ä¼šæ— é™æ¥è¿‘äº0ï¼Œæ­¤æ—¶å¯ä»¥è¿›è¡Œè¯¯å·®ä¿®æ­£ï¼š$factor=\frac{1}{1-\beta^t}$</li></ul></li><li><p>AdamW</p><ul><li>åœ¨adaptive methodsä¸­ï¼Œè§£è€¦weight-decayå’Œloss-based gradientåœ¨ERMè¿‡ç¨‹ä¸­çš„ç»‘å®šdownscaleçš„å…³ç³»</li><li>å®è´¨å°±æ˜¯å°†å¯¼æ•°é¡¹åç§»</li></ul><p><img src="/2021/03/15/optimizersä¼˜åŒ–å™¨/AdamW.png" width="80%;"></p></li></ol>]]></content>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>regnet</title>
      <link href="/2021/03/11/regnet/"/>
      <url>/2021/03/11/regnet/</url>
      <content type="html"><![CDATA[<h2 id="RegNet-Designing-Network-Design-Spaces"><a href="#RegNet-Designing-Network-Design-Spaces" class="headerlink" title="RegNet: Designing Network Design Spaces"></a>RegNet: Designing Network Design Spaces</h2><ol><li><p>åŠ¨æœº</p><ul><li>study the network design principles</li><li>design RegNet </li><li>outperforms efficientNet and 5x faster<ul><li>top1 errorï¼š20.1 ï¼ˆeff-b5ï¼š21.5ï¼‰</li><li>larger batch size</li><li>1/4 çš„ train/test latency</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>manual network design<ul><li>AlexNet, ResNet family, DenseNet, MobileNet</li><li>focus on discovering new design choices that improve acc </li></ul></li><li>the recent popular approach NAS<ul><li>search the best in a fixed search space of possible networks</li><li>limitationsï¼šgeneralize to new settingsï¼Œlack of interpretability  </li></ul></li><li>network scaling<ul><li>ä¸Šé¢ä¸¤ä¸ªfocus on æ‰¾å‡ºä¸€ä¸ªbasenet for a specific regime</li><li>scaling rules aims at tuning the optimal network in any target regime </li></ul></li><li>comparing networks<ul><li>the reliable comparison metric to guide the design process</li></ul></li><li>our method<ul><li>combines the disadvantages of manual design and NAS</li><li>first AnyNet</li><li>then RegNet</li></ul></li></ul></li><li><p>æ–¹æ³•</p></li></ol>]]></content>
      
      
    </entry>
    
    <entry>
      <title>mongodb</title>
      <link href="/2021/03/09/mongodb/"/>
      <url>/2021/03/09/mongodb/</url>
      <content type="html"><![CDATA[<ol><li><p>downloadï¼š<a href="https://www.mongodb.com/try/download/enterprise" target="_blank" rel="noopener">https://www.mongodb.com/try/download/enterprise</a></p></li><li><p>install</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> å°†è§£å‹ä»¥åçš„æ–‡ä»¶å¤¹æ”¾åœ¨/usr/<span class="built_in">local</span>ä¸‹</span></span><br><span class="line">sudo mv mongodb-osx-x86_64-4.0.9/ /usr/local/</span><br><span class="line">sudo ln -s mongodb-macos-x86_64-4.4.4 mongodb</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ENV PATH</span></span><br><span class="line">export PATH=/usr/local/mongodb/bin:$PATH</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> åˆ›å»ºæ—¥å¿—åŠæ•°æ®å­˜æ”¾çš„ç›®å½•</span></span><br><span class="line">sudo mkdir -p /usr/local/var/mongodb</span><br><span class="line">sudo mkdir -p /usr/local/var/log/mongodb</span><br><span class="line">sudo chown [amber] /usr/local/var/mongodb</span><br><span class="line">sudo chown [amber] /usr/local/var/log/mongodb</span><br></pre></td></tr></table></figure></li><li><p>configuration</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> åå°å¯åŠ¨</span></span><br><span class="line">mongod --dbpath /usr/local/var/mongodb --logpath /usr/local/var/log/mongodb/mongo.log --fork</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> æ§åˆ¶å°å¯åŠ¨</span></span><br><span class="line">mongod --config /usr/local/etc/mongod.conf</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> æŸ¥çœ‹çŠ¶æ€</span></span><br><span class="line">ps aux | grep -v grep | grep mongod</span><br></pre></td></tr></table></figure></li><li><p>run</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> åœ¨dbç¯å¢ƒä¸‹å¯åŠ¨ä¸€ä¸ªç»ˆç«¯</span></span><br><span class="line">cd /usr/local/mongodb/bin </span><br><span class="line">./mongo</span><br></pre></td></tr></table></figure></li><li><p>original settings</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> æ˜¾ç¤ºæ‰€æœ‰æ•°æ®çš„åˆ—è¡¨</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> show dbs</span></span><br><span class="line">admin   0.000GB</span><br><span class="line">config  0.000GB</span><br><span class="line">local   0.000GB</span><br><span class="line"><span class="meta">#</span><span class="bash"> ä¸‰ä¸ªç³»ç»Ÿä¿ç•™çš„ç‰¹æ®Šæ•°æ®åº“</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> è¿æ¥/åˆ›å»ºä¸€ä¸ªæŒ‡å®šçš„æ•°æ®åº“</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> use <span class="built_in">local</span></span></span><br><span class="line">switched to db local</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> æ˜¾ç¤ºå½“å‰æ•°æ®åº“, å¦‚æœæ²¡useé»˜è®¤ä¸º<span class="built_in">test</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db</span></span><br><span class="line">test</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ã€ï¼ï¼é‡è¦ã€‘å…³é—­æœåŠ¡</span></span><br><span class="line">ä¹‹å‰æœåŠ¡å™¨è¢«kill -9å¼ºåˆ¶å…³é—­ï¼Œæ•°æ®åº“ä¸¢å¤±äº†</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> use admin</span></span><br><span class="line">switched to db admin</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.shutdownServer()</span></span><br><span class="line">server should be down...</span><br></pre></td></tr></table></figure></li><li><p>concepts</p><p> <img src="/2021/03/09/mongodb/concepts.png" width="90%;"></p></li><li><p>æ–‡æ¡£document</p><p> ä¸€ç»„key-valueå¯¹ï¼Œå¦‚ä¸Šé¢å·¦å›¾ä¸­çš„ä¸€è¡Œè®°å½•ï¼Œå¦‚ä¸Šé¢å³å›¾ä¸­çš„ä¸€ä¸ªdict</p></li><li><p>é›†åˆcollection</p><p> ä¸€å¼ è¡¨ï¼Œå¦‚ä¸Šé¢å·¦å›¾å’Œä¸Šé¢å³å›¾</p></li><li><p>ä¸»é”®primary key</p><p> å”¯ä¸€ä¸»é”®ï¼ŒObjectIdç±»å‹ï¼Œè‡ªå®šç”Ÿæˆï¼Œæœ‰æ ‡å‡†æ ¼å¼</p></li><li><p>å¸¸ç”¨å‘½ä»¤</p><p>10.1 åˆ›å»º/åˆ é™¤/é‡å‘½ådb</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> åˆ‡æ¢è‡³æ•°æ®åº“test1</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> use test1</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> æ’å…¥ä¸€æ¡doc, db.COLLECTION_NAME.insert(document)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> dbè¦åŒ…å«è‡³å°‘ä¸€æ¡æ–‡æ¡£ï¼Œæ‰èƒ½åœ¨show dbsçš„æ—¶å€™æ˜¾ç¤ºï¼ˆæ‰çœŸæ­£åˆ›å»ºï¼‰</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.sheet1.insert(&#123;<span class="string">'name'</span>: img0&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> æ˜¾ç¤ºå½“å‰å·²æœ‰æ•°æ®åº“</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> show dbs</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> åˆ é™¤æŒ‡å®šæ•°æ®åº“</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> use test1</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.dropDatabase()</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> æ—§ç‰ˆæœ¬(before4.0)é‡å‘½åï¼šå…ˆæ‹·è´ä¸€ä»½ï¼Œåœ¨åˆ é™¤æ—§çš„</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.copyDatabase(<span class="string">'OLDNAME'</span>, <span class="string">'NEWNAME'</span>);</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> use old_name</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.dropDatabase()</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> æ–°ç‰ˆæœ¬é‡å‘½åï¼šdump&amp;restoreï¼Œè¿™ä¸ªä¸œè¥¿åœ¨mongodb toolsé‡Œé¢ï¼Œè¦å¦å¤–ä¸‹è½½ï¼Œå¯æ‰§è¡Œæ–‡ä»¶æ”¾åœ¨binä¸‹</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> mongodump   <span class="comment"># å°†æ‰€æœ‰æ•°æ®åº“å¯¼å‡ºåˆ°bin/dump/ä»¥æ¯ä¸ªdbåå­—å‘½åçš„æ–‡ä»¶å¤¹ä¸‹</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> mongodump -h dbhost -d dbname -o dbdirectory</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -h: æœåŠ¡å™¨åœ°å€:ç«¯å£å·</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -d: éœ€è¦å¤‡ä»½çš„æ•°æ®åº“</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -o: å­˜æ”¾ä½ç½®ï¼ˆéœ€è¦å·²å­˜åœ¨ï¼‰</span></span><br><span class="line">mongodump -d test -o tmp/</span><br><span class="line"><span class="meta">#</span><span class="bash"> åœ¨æ¢å¤å¤‡ä»½æ•°æ®åº“çš„æ—¶å€™æ¢ä¸ªåå­—ï¼šmongorestore -h dbhost -d dbname path</span></span><br><span class="line">mongorestore -d test_bkp tmp/test</span><br><span class="line"><span class="meta">#</span><span class="bash"> è¿™æ—¶å€™å¯ä»¥çœ‹åˆ°ä¸€ä¸ªæ–°å¢äº†ä¸€ä¸ªå«test_bkpçš„db</span></span><br></pre></td></tr></table></figure><p>10.2 åˆ›å»º/åˆ é™¤/é‡å‘½åcollection</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> åˆ›å»ºï¼šdb.createCollection(name, options)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.createCollection(<span class="string">'case2img'</span>)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> æ˜¾ç¤ºå·²æœ‰tables</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> show collections</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ä¸ç”¨æ˜¾ç¤ºåˆ›å»ºï¼Œåœ¨db insertçš„æ—¶å€™ä¼šè‡ªåŠ¨åˆ›å»ºé›†åˆ</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.sheet2.insert(&#123;<span class="string">"name"</span> : <span class="string">"img2"</span>&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> åˆ é™¤ï¼šdb.COLLECTION_NAME.drop()</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.sheet2.drop()</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> é‡å‘½åï¼šdb.COLLECTION_NAME.renameCollection(<span class="string">'NEWNAME'</span>)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.sheet2.renameCollection(<span class="string">'sheet3'</span>)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> å¤åˆ¶ï¼šdb.COLLECTION_NAME.aggregate(&#123;<span class="variable">$out</span>: <span class="string">'NEWNAME'</span>&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.sheet2.aggregate(&#123; <span class="variable">$out</span> : <span class="string">"sheet3"</span> &#125;)</span></span><br></pre></td></tr></table></figure><p>10.3 æ’å…¥/æ˜¾ç¤º/æ›´æ–°/åˆ é™¤document</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> æ’å…¥</span></span><br><span class="line">db.COLLECTION_NAME.insert(document)</span><br><span class="line">db.COLLECTION_NAME.save(document)</span><br><span class="line">db.COLLECTION_NAME.insertOne()</span><br><span class="line">db.COLLECTION_NAME.insertMany()</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> æ˜¾ç¤ºå·²æœ‰doc</span></span><br><span class="line">db.COLLECTION_NAME.find()</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> æ›´æ–°docçš„éƒ¨åˆ†å†…å®¹</span></span><br><span class="line">db.COLLECTION_NAME.update(</span><br><span class="line">   &lt;query&gt;,   # æŸ¥è¯¢æ¡ä»¶</span><br><span class="line">   &lt;update&gt;,  # æ›´æ–°æ“ä½œ</span><br><span class="line">   &#123;</span><br><span class="line">     upsert: &lt;boolean&gt;,     # if true å¦‚æœä¸å­˜åœ¨åˆ™æ’å…¥</span><br><span class="line">     multi: &lt;boolean&gt;,      # find fist/all match</span><br><span class="line">     writeConcern: &lt;document&gt;</span><br><span class="line">   &#125;</span><br><span class="line">)</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.insert(&#123;<span class="string">"case"</span>: <span class="string">"s0"</span>, <span class="string">"name"</span>: <span class="string">"img0"</span>&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.insert(&#123;<span class="string">"case"</span>: <span class="string">"s1"</span>, <span class="string">"name"</span>: <span class="string">"img1"</span>&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find()</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.update(&#123;<span class="string">'case'</span>: <span class="string">'s1'</span>&#125;, &#123;<span class="variable">$set</span>: &#123;<span class="string">'case'</span>: <span class="string">'s2'</span>, <span class="string">'name'</span>: <span class="string">'img2'</span>&#125;&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find()</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ç»™docçš„æŸä¸ªkeyé‡å‘½å</span></span><br><span class="line">db.COLLECTION_NAME.updateMany(</span><br><span class="line">&#123;&#125;,</span><br><span class="line">&#123;'$rename': &#123;"old_key": "new_key"&#125;&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> æ›´æ–°æ•´æ¡æ–‡æ¡£by object_id</span></span><br><span class="line">db.COLLECTION_NAME.save(</span><br><span class="line">   &lt;document&gt;,</span><br><span class="line">   &#123;</span><br><span class="line">     writeConcern: &lt;document&gt;</span><br><span class="line">   &#125;</span><br><span class="line">)</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.save(&#123;<span class="string">"_id"</span>: ObjectId(<span class="string">"60474e4b77e21bad9bd4655a"</span>), <span class="string">"case"</span>:<span class="string">"s3"</span>, <span class="string">"name"</span>:<span class="string">"img3"</span>&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> åˆ é™¤æ»¡è¶³æ¡ä»¶çš„doc</span></span><br><span class="line">db.COLLECTION_NAME.remove(</span><br><span class="line">   &lt;query&gt;,</span><br><span class="line">   &#123;</span><br><span class="line">     justOne: &lt;boolean&gt;,   # find fist/all match</span><br><span class="line">     writeConcern: &lt;document&gt;</span><br><span class="line">   &#125;</span><br><span class="line">)</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.remove(&#123;<span class="string">"case"</span>: <span class="string">"s0"</span>&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> åˆ é™¤æ‰€æœ‰doc</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.remove(&#123;&#125;)</span></span><br></pre></td></tr></table></figure><p>10.4 ç®€å•æŸ¥è¯¢find</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.insert(&#123;<span class="string">"case"</span>: <span class="string">"s0"</span>, <span class="string">"name"</span>: <span class="string">"img0"</span>&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.insert(&#123;<span class="string">"case"</span>: <span class="string">"s1"</span>, <span class="string">"name"</span>: <span class="string">"img1"</span>&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.insert(&#123;<span class="string">"case"</span>: <span class="string">"s2"</span>, <span class="string">"name"</span>: <span class="string">"img2"</span>&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.insert(&#123;<span class="string">"case"</span>: <span class="string">"s2"</span>, <span class="string">"name"</span>: <span class="string">"img3"</span>&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> æŸ¥è¯¢è¡¨ä¸­çš„docï¼šdb.COLLECTION_NAME.find(&#123;query&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find(&#123;<span class="string">'case'</span>: s2&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find(&#123;<span class="string">'case'</span>: <span class="string">'s1'</span>&#125;, &#123;<span class="string">"name"</span>:1&#125;)   <span class="comment"># projectionçš„valueåœ¨å¯¹åº”çš„key-valueæ˜¯listçš„æ—¶å€™æœ‰æ„ä¹‰</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> æ ¼å¼åŒ–æ˜¾ç¤ºæŸ¥è¯¢ç»“æœï¼šdb.COLLECTION_NAME.find(&#123;query&#125;).pretty()</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find(&#123;<span class="string">'case'</span>: s2&#125;).pretty()</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> è¯»å–æŒ‡å®šæ•°é‡çš„æ•°æ®è®°å½•ï¼šdb.COLLECTION_NAME.find(&#123;query&#125;).<span class="built_in">limit</span>(NUMBER)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find(&#123;<span class="string">'case'</span>: &#123;<span class="variable">$type</span>: <span class="string">'string'</span>&#125;&#125;).<span class="built_in">limit</span>(1)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> è·³è¿‡æŒ‡å®šæ•°é‡çš„æ•°æ®ï¼šdb.COLLECTION_NAME.find(&#123;query&#125;).skip(NUMBER)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find(&#123;<span class="string">'case'</span>: &#123;<span class="variable">$type</span>: <span class="string">'string'</span>&#125;&#125;).skip(1)</span></span><br></pre></td></tr></table></figure><p>10.5 æ¡ä»¶æ“ä½œç¬¦</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">(&gt;</span><span class="bash">) å¤§äº - <span class="variable">$gt</span></span></span><br><span class="line">(&lt;) å°äº - $lt</span><br><span class="line"><span class="meta">(&gt;</span><span class="bash">=) å¤§äºç­‰äº - <span class="variable">$gte</span></span></span><br><span class="line">(&lt;=) å°äºç­‰äº - $lte</span><br><span class="line">(or) æˆ– - $or</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.update(&#123;<span class="string">'case'</span>:<span class="string">'s1'</span>&#125;, &#123;<span class="variable">$set</span>: &#123;<span class="string">"name"</span>:<span class="string">'img1'</span>, <span class="string">'size'</span>:100&#125;&#125;)</span></span><br><span class="line">WriteResult(&#123; "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 &#125;)</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.update(&#123;<span class="string">'case'</span>:<span class="string">'s2'</span>&#125;, &#123;<span class="variable">$set</span>: &#123;<span class="string">"name"</span>:<span class="string">'img2'</span>, <span class="string">'size'</span>:200&#125;&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> æŸ¥è¯¢size&gt;150çš„doc</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find(&#123;<span class="string">'size'</span>: &#123;<span class="variable">$gt</span>: 150&#125;&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> æŸ¥è¯¢æ»¡è¶³ä»»æ„ä¸€ä¸ªæ¡ä»¶çš„doc</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find(&#123;<span class="string">'$or'</span>: [&#123;<span class="string">'case'</span>:<span class="string">'s1'</span>&#125;, &#123;<span class="string">'size'</span>: &#123;<span class="variable">$gt</span>: 150&#125;&#125;]&#125;)</span></span><br></pre></td></tr></table></figure><p>10.6 æ•°æ®ç±»å‹æ“ä½œç¬¦</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">type(KEY)ç­‰äº - $type</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> æ¯”è¾ƒå¯¹è±¡å¯ä»¥æ˜¯å­—ç¬¦ä¸²/å¯¹åº”çš„reflect NUM</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find(&#123;<span class="string">'case'</span>: &#123;<span class="variable">$type</span>: <span class="string">'string'</span>&#125;&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find(&#123;<span class="string">'case'</span>: &#123;<span class="variable">$type</span>: <span class="string">'0'</span>&#125;&#125;)</span></span><br></pre></td></tr></table></figure><p>10.7 æ’åºfind().sort</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> é€šè¿‡æŒ‡å®šå­—æ®µ&amp;æŒ‡å®šå‡åº/é™åºæ¥å¯¹æ•°æ®æ’åºï¼šdb.COLLECTION_NAME.find().sort(&#123;KEY:1/-1&#125;)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.find().sort(&#123;<span class="string">'name'</span>:1&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> skip(), limilt(), sort()ä¸‰ä¸ªæ”¾åœ¨ä¸€èµ·æ‰§è¡Œçš„æ—¶å€™ï¼Œæ‰§è¡Œçš„é¡ºåºæ˜¯å…ˆ sort(), ç„¶åæ˜¯ skip()ï¼Œæœ€åæ˜¯æ˜¾ç¤ºçš„ <span class="built_in">limit</span>()ã€‚</span></span><br></pre></td></tr></table></figure><p>10.8 ç´¢å¼•</p><p>skip</p><p>10.9 èšåˆaggregate</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ç”¨äºç»Ÿè®¡ db.COLLECTION_NAME.aggregate(AGGREGATE_OPERATION)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> by group</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.aggregate([&#123;<span class="variable">$group</span>: &#123;_id: <span class="string">'$case'</span>, img_num:&#123;<span class="variable">$sum</span>:1&#125;&#125;&#125;])</span></span><br><span class="line">group by key value 'case'</span><br><span class="line">count number of items in each group</span><br><span class="line">refer to the number as img_num</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.aggregate([&#123;<span class="variable">$group</span>: &#123;_id: <span class="string">'$case'</span>, img_num:&#123;<span class="variable">$sum</span>:<span class="string">'$size'</span>&#125;&#125;&#125;])</span></span><br><span class="line">è®¡ç®—æ¯ä¸€ä¸ªgroupå†…ï¼Œsizeå€¼çš„æ€»å’Œ</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> by match</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.aggregate([&#123;<span class="variable">$match</span>: &#123;<span class="string">'size'</span>: &#123;<span class="variable">$gt</span>:150&#125;&#125;&#125;, </span></span><br><span class="line"> &#123;$group:&#123;_id: null, totalsize: &#123;$sum: '$size'&#125;&#125;&#125;])</span><br><span class="line">ç±»ä¼¼shellçš„ç®¡é“ï¼Œmatchç”¨æ¥ç­›é€‰æ¡ä»¶ï¼Œç¬¦åˆæ¡ä»¶çš„é€å…¥ä¸‹ä¸€æ­¥ç»Ÿè®¡</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> db.case2img.aggregate([&#123;<span class="variable">$skip</span>: 4&#125;, </span></span><br><span class="line"> &#123;$group:&#123;_id: null, totalsize: &#123;$sum: '$size'&#125;&#125;&#125;])</span><br></pre></td></tr></table></figure></li><li><p>å¿«é€Ÿç»Ÿè®¡distinct</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db.case2img.distinct(TAG_NAME)</span><br><span class="line"><span class="meta">#</span><span class="bash"> æ³¨æ„å¦‚æœdistinctçš„å†…å®¹å¤ªé•¿ï¼Œè¶…è¿‡16Mï¼Œä¼šæŠ¥distinct too bigçš„errorï¼Œæ¨èç”¨èšåˆæ¥åšç»Ÿè®¡</span></span><br></pre></td></tr></table></figure></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">12. pymongo</span><br><span class="line"></span><br><span class="line">   ç”¨pythonä»£ç æ¥æ“ä½œæ•°æ®åº“</span><br><span class="line"></span><br><span class="line">   å…ˆå®‰è£…ï¼špip install pymongo</span><br><span class="line"></span><br><span class="line">   11.1 è¿æ¥client</span><br><span class="line"></span><br><span class="line">   ```python</span><br><span class="line">   from pymongo import MongoClient</span><br><span class="line">   Client = MongoClient()</span><br></pre></td></tr></table></figure><p>   11.2 è·å–æ•°æ®åº“</p>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db = Client.DB_NAME</span><br><span class="line">db = Client[<span class="string">'DB_NAME'</span>]</span><br></pre></td></tr></table></figure><p>   11.3 è·å–collection</p>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">collection = db.COLLECTION_NAME</span><br><span class="line">collection = db[<span class="string">'COLLECTION_NAME'</span>]</span><br></pre></td></tr></table></figure><p>   11.4 æ’å…¥doc</p>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># insert one</span></span><br><span class="line">document1 = &#123;<span class="string">'x'</span>:<span class="number">1</span>&#125;</span><br><span class="line">document2 = &#123;<span class="string">'x'</span>:<span class="number">2</span>&#125;</span><br><span class="line">post_1 = collection.insert_one(document1).inserted_id</span><br><span class="line">post_2 = collection.insert_one(document2).inserted_id</span><br><span class="line">print(post_1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># insert many</span></span><br><span class="line">new_document = [&#123;<span class="string">'x'</span>:<span class="number">1</span>&#125;,&#123;<span class="string">'x'</span>:<span class="number">2</span>&#125;]</span><br><span class="line"><span class="comment"># new_document = [document1,document2]  æ³¨æ„docæ˜¯ç¥æ‹·è´ï¼Œåªèƒ½ä½œä¸ºä¸€æ¡docè¢«æ’å…¥ä¸€æ¬¡</span></span><br><span class="line">result = collection.insert_many(new_document).inserted_ids</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><p>   11.5 æŸ¥æ‰¾</p>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bson.objectid <span class="keyword">import</span> ObjectId</span><br><span class="line"></span><br><span class="line"><span class="comment"># find one è¿”å›ä¸€æ¡doc</span></span><br><span class="line">result = collection.find_one()</span><br><span class="line">result = collection.find_one(&#123;<span class="string">'case'</span>: <span class="string">'s0'</span>&#125;)</span><br><span class="line">result = collection.find_one(&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'604752f277e21bad9bd46560'</span>)&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># find è¿”å›ä¸€ä¸ªè¿­ä»£å™¨</span></span><br><span class="line"><span class="keyword">for</span> _, item <span class="keyword">in</span> enumerate(collection.find()):</span><br><span class="line">    print(item)</span><br></pre></td></tr></table></figure><p>   11.6 æ›´æ–°</p>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># update one</span></span><br><span class="line">collection.update_one(&#123;<span class="string">'case'</span>:<span class="string">'s1'</span>&#125;,&#123;<span class="string">'$set'</span>:&#123;<span class="string">'size'</span>:<span class="number">300</span>&#125;&#125;)</span><br><span class="line">collection.update_one(&#123;<span class="string">'case'</span>:<span class="string">'s1'</span>&#125;,&#123;<span class="string">'$push'</span>:&#123;<span class="string">'add'</span>:<span class="number">1</span>&#125;&#125;)    <span class="comment"># è¿½åŠ æ•°ç»„å†…å®¹</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># update many</span></span><br><span class="line">collection.update_many(&#123;<span class="string">'case'</span>:<span class="string">'s1'</span>&#125;,&#123;<span class="string">'$set'</span>:&#123;<span class="string">'size'</span>:<span class="number">300</span>&#125;&#125;)</span><br></pre></td></tr></table></figure><p>   11.7 åˆ é™¤</p>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># åœ¨mongo shellé‡Œé¢æ˜¯removeæ–¹æ³•ï¼Œåœ¨pymongoé‡Œé¢è¢«deprecatedæˆdeleteæ–¹æ³•</span></span><br><span class="line">collection.delete_one(&#123;<span class="string">"case"</span>: <span class="string">"s2"</span>&#125;)</span><br><span class="line">collection.delete_many(&#123;<span class="string">"case"</span>: <span class="string">"s1"</span>&#125;)</span><br></pre></td></tr></table></figure><p>   11.8 ç»Ÿè®¡</p>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># è®¡æ•°ï¼šcountæ–¹æ³•å·²ç»è¢«é‡æ„</span></span><br><span class="line">print(collection.count_documents(&#123;<span class="string">'case'</span>:<span class="string">'s0'</span>&#125;))</span><br><span class="line"></span><br><span class="line"><span class="comment"># uniqueï¼šdistinctæ–¹æ³•</span></span><br><span class="line">print(collection.distinct(<span class="string">'case'</span>))</span><br></pre></td></tr></table></figure><p>â€‹    11.9 æ­£åˆ™</p><p>â€‹    mongo shellå‘½ä»¤è¡Œé‡Œçš„æ­£åˆ™å’Œpymongoè„šæœ¬é‡Œçš„æ­£åˆ™å†™æ³•æ˜¯ä¸ä¸€æ ·çš„ï¼Œå› ä¸ºpythoné‡Œé¢æœ‰å°è£…æ­£åˆ™æ–¹æ³•ï¼Œç„¶åé€šè¿‡bsonå°†pythonçš„æ­£åˆ™è½¬æ¢æˆæ•°æ®åº“çš„æ­£åˆ™</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pymongo</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> bson</span><br><span class="line"></span><br><span class="line">pattern = re.compile(<span class="string">r'(.*)-0[345]-(.*)'</span>)</span><br><span class="line">regex = bson.regex.Regex.from_native(pattern)</span><br><span class="line">result = collection.aggregate([&#123;<span class="string">'$match'</span>: &#123;<span class="string">'date'</span>: regex&#125;&#125;])</span><br><span class="line"></span><br><span class="line"><span class="comment"># mongo shell</span></span><br><span class="line">&gt; db.collection.find(&#123;date:&#123;$regex:<span class="string">"(.*)-0[345]-(.*)"</span>&#125;&#125;)</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> æ•°æ®åº“ï¼ŒNoSQL </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>docker</title>
      <link href="/2021/03/04/docker/"/>
      <url>/2021/03/04/docker/</url>
      <content type="html"><![CDATA[<ol><li><p>shartup</p><ul><li><p>éƒ¨ç½²æ–¹æ¡ˆ</p><ul><li><p>å¤æ—©å¹´ä»£</p><p>  <img src="/2021/03/04/docker/old.png" width="50%;"></p></li><li><p>è™šæ‹Ÿæœº</p><p>  <img src="/2021/03/04/docker/vm.png" width="50%;"></p></li><li><p>docker</p><p>  <img src="/2021/03/04/docker/docker.png" width="50%;"></p></li></ul></li><li><p>imageé•œåƒ &amp; containerå®¹å™¨ &amp; registryä»“åº“</p><ul><li>é•œåƒï¼šç›¸å½“äºæ˜¯ä¸€ä¸ª root æ–‡ä»¶ç³»ç»Ÿï¼Œæä¾›å®¹å™¨è¿è¡Œæ—¶æ‰€éœ€çš„ç¨‹åºã€åº“ã€èµ„æºã€é…ç½®ç­‰</li><li>å®¹å™¨ï¼šé•œåƒè¿è¡Œæ—¶çš„å®ä½“ï¼Œå¯ä»¥è¢«åˆ›å»ºã€å¯åŠ¨ã€åœæ­¢ã€åˆ é™¤ã€æš‚åœç­‰</li><li>ä»“åº“ï¼šç”¨æ¥ä¿å­˜é•œåƒ<ul><li>å®˜æ–¹ä»“åº“ï¼šdocker hubï¼š<a href="https://hub.docker.com/r/floydhub/tensorflow/tags?page=1&amp;ordering=last_updated" target="_blank" rel="noopener">https://hub.docker.com/r/floydhub/tensorflow/tags?page=1&amp;ordering=last_updated</a></li></ul></li></ul></li></ul></li><li><p>å¸¸ç”¨å‘½ä»¤</p><ul><li>æ‹‰é•œåƒ<ul><li>docker pull [é€‰é¡¹] [Docker Registry åœ°å€[:ç«¯å£å·]/]ä»“åº“å[:æ ‡ç­¾]</li><li>åœ°å€å¯ä»¥æ˜¯å®˜æ–¹åœ°å€ï¼Œä¹Ÿå¯ä»¥æ˜¯ç¬¬ä¸‰æ–¹ï¼ˆå¦‚Harborï¼‰</li><li>ä»“åº“åç”±ä½œè€…åå’Œè½¯ä»¶åç»„æˆï¼ˆå¦‚zhangruiming/skinï¼‰</li><li>æ ‡ç­¾ç”¨æ¥æŒ‡å®šæŸä¸ªç‰ˆæœ¬çš„imageï¼Œçœç•¥åˆ™é»˜è®¤latest</li></ul></li><li>åˆ—å‡ºæ‰€æœ‰é•œåƒ<ul><li>docker  images</li></ul></li><li>åˆ é™¤é•œåƒ<ul><li>docker rmi [-f] [é•œåƒid]</li><li>åˆ é™¤é•œåƒä¹‹å‰è¦kill/rmæ‰€æœ‰ä½¿ç”¨è¯¥é•œåƒçš„containerï¼šdocker rm [å®¹å™¨id]</li></ul></li><li>è¿è¡Œé•œåƒå¹¶åˆ›å»ºä¸€ä¸ªå®¹å™¨<ul><li>docker run [-it] [ä»“åº“å] [å‘½ä»¤]</li><li>é€‰é¡¹ -itï¼šä¸ºå®¹å™¨é…ç½®ä¸€ä¸ªäº¤äº’ç»ˆç«¯</li><li>é€‰é¡¹ -dï¼šåå°è¿è¡Œå®¹å™¨ï¼Œå¹¶è¿”å›å®¹å™¨IDï¼ˆä¸ç›´æ¥è¿›å…¥ç»ˆç«¯ï¼‰</li><li>é€‰é¡¹ â€”name=â€™xxxâ€™ï¼šä¸ºå®¹å™¨æŒ‡å®šä¸€ä¸ªåç§°</li><li>é€‰é¡¹-v /host_dir:/container_dirï¼šå°†ä¸»æœºä¸ŠæŒ‡å®šç›®å½•æ˜ å°„åˆ°å®¹å™¨çš„æŒ‡å®šç›®å½•</li><li>[å‘½ä»¤]å‚æ•°å¿…é¡»è¦åŠ ï¼Œè€Œä¸”è¦æ˜¯é‚£ç§ä¸€ç›´æŒ‚èµ·çš„å‘½ä»¤ï¼ˆ/bin/bashï¼‰ï¼Œå¦‚æœæ˜¯ls/cd/ç›´æ¥ä¸å¡«ï¼Œé‚£ä¹ˆå‘½ä»¤è¿è¡Œå®Œå®¹å™¨å°±ä¼šåœæ­¢è¿è¡Œï¼Œdocker ps -aæŸ¥çœ‹çŠ¶æ€ï¼Œå‘ç°éƒ½æ˜¯Exited</li></ul></li><li>åˆ›å»ºå®¹å™¨<ul><li>docker run</li></ul></li><li>æŸ¥çœ‹æ‰€æœ‰å®¹å™¨<ul><li>docker ps</li></ul></li><li>å¯åŠ¨ä¸€ä¸ªå·²ç»åœæ­¢çš„å®¹å™¨/åœæ­¢æ­£åœ¨è¿è¡Œçš„å®¹å™¨<ul><li>docker start [å®¹å™¨id]</li><li>docker stop [å®¹å™¨id]</li></ul></li><li>è¿›å…¥å®¹å™¨<ul><li>docker exec -it [å®¹å™¨id] [linuxå‘½ä»¤]</li></ul></li><li>åˆ é™¤å®¹å™¨<ul><li>docker rm [å®¹å™¨id]</li></ul></li><li>åˆ é™¤æ‰€æœ‰ä¸æ´»è·ƒçš„å®¹å™¨<ul><li>docker container prune</li></ul></li><li>æäº¤é•œåƒåˆ°è¿œç«¯ä»“åº“<ul><li>docker tag [é•œåƒid] [ç”¨æˆ·å]/[ä»“åº“]:[æ ‡ç­¾]    # é‡å‘½å</li><li>docker login    # ç™»é™†ç”¨æˆ·</li><li>docker push </li></ul></li></ul></li><li><p>æ¡ˆä¾‹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> æ‹‰é•œåƒ</span></span><br><span class="line">docker pull åœ°å€/ä»“åº“:æ ‡ç­¾</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> æ˜¾ç¤ºé•œåƒ</span></span><br><span class="line">docker images</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> è¿è¡ŒæŒ‡å®šé•œåƒ</span></span><br><span class="line">docker run -itd --name='test' åœ°å€/ä»“åº“:æ ‡ç­¾</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> æŸ¥çœ‹è¿è¡Œçš„å®¹å™¨</span></span><br><span class="line">docker ps</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> è¿›å…¥å®¹å™¨</span></span><br><span class="line">docker exec -it å®¹å™¨idæˆ–name /bin/bash</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ä¸€é¡¿æ“ä½œå®Œé€€å‡ºå®¹å™¨</span></span><br><span class="line">exit</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> å°†ä¿®æ”¹åçš„å®¹å™¨ä¿å­˜ä¸ºé•œåƒ</span></span><br><span class="line">docker commit å®¹å™¨idæˆ–name æ–°é•œåƒåå­—</span><br><span class="line">docker imageså¯ä»¥çœ‹åˆ°è¿™ä¸ªé•œåƒäº†</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ä¿å­˜é•œåƒåˆ°æœ¬åœ°</span></span><br><span class="line">docker save -o tf_torch.rar  tf_torch</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> è¿˜åŸé•œåƒ</span></span><br><span class="line">docker load --input tf_torch.tar</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> é‡å‘½åé•œåƒ</span></span><br><span class="line">docker tag 3db0b2f40a70 amberzzzz/tf1.14_torch1.4_cuda10.0:v1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> æäº¤é•œåƒ</span></span><br><span class="line">docker push amberzzzz/tf1.14-torch0.5-cuda10.0:v1</span><br></pre></td></tr></table></figure><ol><li><p>dockerfile</p><ul><li><p>Dockerfile æ˜¯ç”¨æ¥è¯´æ˜å¦‚ä½•è‡ªåŠ¨æ„å»º docker image çš„æŒ‡ä»¤é›†æ–‡ä»¶</p></li><li><p>å¸¸ç”¨å‘½ä»¤</p><ul><li>FROM image_nameï¼ŒæŒ‡å®šä¾èµ–çš„é•œåƒ</li><li>RUN commandï¼Œåœ¨ shell æˆ–è€… exec çš„ç¯å¢ƒä¸‹æ‰§è¡Œçš„å‘½ä»¤</li><li>ADD srcfile_path_inhost dstfile_incontainerï¼Œå°†æœ¬æœºæ–‡ä»¶å¤åˆ¶åˆ°å®¹å™¨ä¸­</li><li>CMD [â€œexecutableâ€,â€param1â€,â€param2â€]ï¼ŒæŒ‡å®šå®¹å™¨å¯åŠ¨é»˜è®¤æ‰§è¡Œçš„å‘½ä»¤</li><li>WORKDIR path_incontainerï¼ŒæŒ‡å®š RUNã€CMD ä¸ ENTRYPOINT å‘½ä»¤çš„å·¥ä½œç›®å½•</li><li>VOLUME [â€œ/dataâ€]ï¼Œæˆæƒè®¿é—®ä»å®¹å™¨å†…åˆ°ä¸»æœºä¸Šçš„ç›®å½•</li></ul></li><li><p>basic image</p><ul><li><p>ä»nvidia dockerå¼€å§‹ï¼š<a href="https://hub.docker.com/r/nvidia/cuda/tags?page=1&amp;ordering=last_updated&amp;name=10" target="_blank" rel="noopener">https://hub.docker.com/r/nvidia/cuda/tags?page=1&amp;ordering=last_updated&amp;name=10</a>.</p></li><li><p>é€‰ä¸€ä¸ªå–œæ¬¢çš„ï¼šå¦‚docker pull nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04</p></li><li><p>ç„¶åç¼–è¾‘dockerfile</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">FROM nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04</span><br><span class="line">MAINTAINER amber &lt;amber.zhang@tum.de&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> install basic dependencies</span></span><br><span class="line">RUN apt-get update </span><br><span class="line">RUN apt-get install -y wget vim cmake</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> install Anaconda3</span></span><br><span class="line">RUN wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.2.0-Linux-x86_64.sh -O ~/anaconda3.sh</span><br><span class="line">RUN bash ~/anaconda3.sh -b -p /home/anaconda3 &amp;&amp; rm ~/anaconda3.sh </span><br><span class="line">ENV PATH /home/anaconda3/bin:$PATH</span><br><span class="line"><span class="meta">#</span><span class="bash"> RUN <span class="built_in">echo</span> <span class="string">"export PATH=/home/anaconda3/bin:<span class="variable">$PATH</span>"</span> &gt;&gt; ~/.bashrc &amp;&amp; /bin/bash -c <span class="string">"source /root/.bashrc"</span> </span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> change mirror</span></span><br><span class="line">RUN mkdir ~/.pip \</span><br><span class="line">    &amp;&amp; cd ~/.pip    </span><br><span class="line">RUN echo '[global]\nindex-url = https://pypi.tuna.tsinghua.edu.cn/simple/' &gt;&gt; ~/.pip/pip.conf</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> install tensorflow</span></span><br><span class="line">RUN /home/anaconda3/bin/pip install tensorflow-gpu==1.8.0</span><br></pre></td></tr></table></figure></li><li><p>ç„¶åbuild dockerfile</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t &lt;docker_name&gt; .</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ol></li></ol>]]></content>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>layer norm</title>
      <link href="/2021/03/02/layer-norm/"/>
      <url>/2021/03/02/layer-norm/</url>
      <content type="html"><![CDATA[<h2 id="ç»¼è¿°"><a href="#ç»¼è¿°" class="headerlink" title="ç»¼è¿°"></a>ç»¼è¿°</h2><ol><li>papers</li></ol><p>[batch norm 2015] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shiftï¼ŒinceptionV2ï¼ŒGoogle Teamï¼Œå½’ä¸€åŒ–å±‚çš„å§‹ç¥–ï¼ŒåŠ é€Ÿè®­ç»ƒ&amp;æ­£åˆ™ï¼ŒBNè¢«åè¾ˆè¿½ç€æ‰“çš„ä¸»è¦ç—›ç‚¹ï¼šapproximation by mini-batchï¼Œtest phase frozen</p><p>[layer norm 2016] Layer Normalizationï¼ŒToronto+Googleï¼Œé’ˆå¯¹BNä¸é€‚ç”¨small batchå’ŒRNNçš„é—®é¢˜ï¼Œä¸»è¦ç”¨äºRNNï¼Œåœ¨CNNä¸Šä¸å¥½ï¼Œåœ¨testçš„æ—¶å€™ä¹Ÿæ˜¯activeçš„ï¼Œå› ä¸ºmean&amp;varianceç”±äºå½“å‰æ•°æ®å†³å®šï¼Œæœ‰è´Ÿè´£rescaleå’Œreshiftçš„layer params</p><p>[weight norm 2016] Weight normalization: A simple reparameterization to accelerate training of deep neural networksï¼ŒOpenAIï¼Œ</p><p>[cosine norm 2017] Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networksï¼Œä¸­ç§‘é™¢ï¼Œ</p><p>[instance norm 2017] Instance Normalization: The Missing Ingredient for Fast Stylizationï¼Œé«˜æ ¡reportï¼Œé’ˆå¯¹é£æ ¼è¿ç§»ï¼ŒINåœ¨testçš„æ—¶å€™ä¹Ÿæ˜¯activeçš„ï¼Œè€Œä¸æ˜¯freezeçš„ï¼Œå•çº¯çš„instance-independent normï¼Œæ²¡æœ‰layer params</p><p>[group norm 2018] Group Normalizationï¼ŒFAIR Kaimingï¼Œé’ˆå¯¹BNåœ¨small batchä¸Šæ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºbatch-independentçš„</p><p>[weight standardization 2019] Weight Standardizationï¼ŒJohns Hopkinsï¼Œ</p><p>[batch-channel normalization &amp; weight standardization 2020] BCN&amp;WS: Micro-Batch Training with Batch-Channel Normalization and Weight Standardizationï¼ŒJohns Hopkinsï¼Œ</p><ol><li><p>why Normalization</p><ul><li><p>ç‹¬ç«‹åŒåˆ†å¸ƒï¼šindependent and identically distribute</p></li><li><p>ç™½åŒ–ï¼šwhiteningï¼ˆ[PCA whitening][<a href="http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/]ï¼‰" target="_blank" rel="noopener">http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/]ï¼‰</a></p><ul><li>å»é™¤ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§</li><li>ä½¿æ‰€æœ‰ç‰¹å¾å…·æœ‰ç›¸åŒçš„å‡å€¼å’Œæ–¹å·®</li></ul></li><li><p>æ ·æœ¬åˆ†å¸ƒå˜åŒ–ï¼šInternal Covariate Shift</p><ul><li>å¯¹äºç¥ç»ç½‘ç»œçš„å„å±‚è¾“å…¥ï¼Œç”±äºstacking internel byproductï¼Œæ¯å±‚çš„åˆ†å¸ƒæ˜¾ç„¶å„ä¸ç›¸åŒï¼Œä½†æ˜¯å¯¹äºæŸä¸ªç‰¹å®šçš„æ ·æœ¬è¾“å…¥ï¼Œä»–ä»¬æ‰€æŒ‡ç¤ºçš„labelæ˜¯ä¸å˜çš„</li><li><p>å³æºç©ºé—´å’Œç›®æ ‡ç©ºé—´çš„æ¡ä»¶æ¦‚ç‡æ˜¯ä¸€è‡´çš„ï¼Œä½†æ˜¯è¾¹ç¼˜æ¦‚ç‡æ˜¯ä¸åŒçš„</p><script type="math/tex; mode=display">P_s(Y|X=x) = P_t(Y|X=x) \\P_s(X) \neq P_t(X)</script></li><li><p>æ¯ä¸ªç¥ç»å…ƒçš„æ•°æ®ä¸å†æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒï¼Œç½‘ç»œéœ€è¦ä¸æ–­é€‚åº”æ–°çš„åˆ†å¸ƒï¼Œä¸Šå±‚ç¥ç»å…ƒå®¹æ˜“é¥±å’Œï¼šç½‘ç»œè®­ç»ƒåˆæ…¢åˆä¸ç¨³å®š</p></li></ul></li></ul></li><li><p>how to Normalization</p><ul><li><p>preparation</p><ul><li>unitï¼šä¸€ä¸ªç¥ç»å…ƒï¼ˆä¸€ä¸ªopï¼‰ï¼Œè¾“å…¥[b,N,C_in]ï¼Œè¾“å‡º[b,N,1]</li><li>layerï¼šä¸€å±‚çš„ç¥ç»å…ƒï¼ˆä¸€ç³»åˆ—opï¼Œ$W\in R^{M*N}$ï¼‰ï¼Œåœ¨channel-dimä¸Šconcatå½“å‰å±‚æ‰€æœ‰unitçš„è¾“å‡º[b,N,C_out]</li><li>dims<ul><li>bï¼šbatch dimension</li><li>Nï¼šspatial dimensionï¼Œ1/2/3-dims</li><li>Cï¼šchannel dimension</li></ul></li><li>unified representationï¼šæœ¬è´¨ä¸Šéƒ½æ˜¯å¯¹æ•°æ®åœ¨è§„èŒƒåŒ–<ul><li>$h = f(g*\frac{x-\mu}{\sigma}+b)$ï¼šå…ˆå½’ä¸€åŒ–ï¼Œåœ¨rescale &amp; reshift</li><li>$\mu$ &amp; $\sigma$ï¼šcompute fromä¸Šä¸€å±‚çš„ç‰¹å¾å€¼</li><li>$g$ &amp; $b$ï¼šlearnable paramsåŸºäºå½“å‰å±‚</li><li>$f$ï¼šneuronsâ€™ weighting operation</li><li>å„æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºmean &amp; varianceçš„è®¡ç®—ç»´åº¦</li></ul></li></ul></li><li><p>å¯¹æ•°æ®</p><ul><li>BNï¼šä»¥ä¸€å±‚æ¯ä¸ªç¥ç»å…ƒçš„è¾“å‡ºä¸ºå•ä½ï¼Œå³æ¯ä¸ªchannelçš„mean&amp;varç›¸äº’ç‹¬ç«‹</li><li>LNï¼šä»¥ä¸€å±‚æ‰€æœ‰ç¥ç»å…ƒçš„è¾“å‡ºä¸ºå•ä½ï¼Œå³æ¯ä¸ªsampleçš„mean&amp;varç›¸äº’ç‹¬ç«‹</li><li>INï¼šä»¥æ¯ä¸ªsampleåœ¨æ¯ä¸ªç¥ç»å…ƒçš„è¾“å‡ºä¸ºå•ä½ï¼Œæ¯ä¸ªsampleåœ¨æ¯ä¸ªchannelçš„mean&amp;varéƒ½ç›¸äº’ç‹¬ç«‹</li><li>GNï¼šä»¥æ¯ä¸ªsampleåœ¨ä¸€ç»„ç¥ç»å…ƒçš„è¾“å‡ºä¸ºå•ä½ï¼Œä¸€ç»„åŒ…å«ä¸€ä¸ªç¥ç»å…ƒçš„æ—¶å€™å˜æˆINï¼Œä¸€ç»„åŒ…å«ä¸€å±‚æ‰€æœ‰ç¥ç»å…ƒçš„æ—¶å€™å°±æ˜¯LN</li><li><p>ç¤ºæ„å›¾ï¼š</p><p><img src="/2021/03/02/layer-norm/methods.png" width="70%;"></p></li></ul></li><li><p>å¯¹æƒé‡</p><ul><li>WNï¼šå°†æƒé‡åˆ†è§£ä¸ºå•ä½å‘é‡å’Œä¸€ä¸ªå›ºå®šæ ‡é‡ï¼Œç›¸å½“äºç¥ç»å…ƒçš„ä»»æ„è¾“å…¥vecç‚¹ä¹˜äº†ä¸€ä¸ªå•ä½vecï¼ˆdownscaleï¼‰ï¼Œå†rescaleï¼Œè¿›ä¸€æ­¥åœ°ç›¸å½“äºæ²¡æœ‰åšshiftå’Œreshiftçš„æ•°æ®normalization</li><li>WSï¼šå¯¹æƒé‡åšå…¨å¥—ï¼ˆå½’ä¸€åŒ–å†recaleï¼‰ï¼Œæ¯”WNå¤šäº†shiftï¼Œâ€œzero-center is the keyâ€ </li></ul></li><li><p>å¯¹op</p><ul><li><p>CosNï¼š</p><ul><li><p>å°†çº¿æ€§å˜æ¢opæ›¿æ¢æˆcos opï¼š$f_w(x) = cos <w,x> = \frac{w \cdot x}{|w||x|}$</w,x></p></li><li><p>æ•°å­¦æœ¬è´¨ä¸Šåˆé€€åŒ–æˆäº†åªæœ‰downscaleçš„å˜æ¢ï¼Œè¡¨å¾èƒ½åŠ›ä¸è¶³</p></li></ul></li></ul></li></ul></li></ol><h2 id="Whiteningç™½åŒ–"><a href="#Whiteningç™½åŒ–" class="headerlink" title="Whiteningç™½åŒ–"></a>Whiteningç™½åŒ–</h2><ol><li><p>purpose</p><ul><li>imagesçš„adjacent pixel values are highly correlatedï¼Œthus redundant</li><li>linearly move the origin distributionï¼Œmaking the inputs share the same mean &amp; variance</li></ul></li><li><p>method</p><ul><li><p>é¦–å…ˆè¿›è¡ŒPCAé¢„å¤„ç†ï¼Œå»æ‰correlation</p><ul><li><p>mean on sampleï¼ˆæ³¨æ„ä¸æ˜¯mean on imageï¼‰</p><script type="math/tex; mode=display">  \overline x = \frac{1}{N}\sum_{i=1}^N x_i\\  x^{'} = x - \overline x</script></li><li><p>åæ–¹å·®çŸ©é˜µ</p><script type="math/tex; mode=display">  X \in R^{d*N}\\  S = \frac{1}{N}XX^T</script></li><li><p>å¥‡å¼‚å€¼åˆ†è§£svd(S)</p><script type="math/tex; mode=display">  S = U \Sigma V^T</script><ul><li>$\Sigma$ä¸ºå¯¹è§’çŸ©é˜µï¼Œå¯¹è§’ä¸Šçš„å…ƒç´ ä¸ºå¥‡å¼‚å€¼</li><li>$U=[u_1,u_2,â€¦u_N]$ä¸­æ˜¯å¥‡å¼‚å€¼å¯¹åº”çš„æ­£äº¤å‘é‡</li></ul></li><li><p>æŠ•å½±å˜æ¢</p><script type="math/tex; mode=display">  X^{'} = U_p^T X</script><ul><li>å–æŠ•å½±çŸ©é˜µ$U_p$ from $U$ï¼Œ$U_p \in R^{N*d}$è¡¨ç¤ºå°†æ•°æ®ç©ºé—´ä»Nç»´æŠ•å½±åˆ°$U_p$æ‰€åœ¨çš„dç»´ç©ºé—´ä¸Š</li></ul></li><li><p>recoverï¼ˆæŠ•å½±é€†å˜æ¢ï¼‰</p><script type="math/tex; mode=display">  X^{''} = U_p^T X^{'}</script></li></ul></li></ul></li></ol><pre><code>        * å–æŠ•å½±çŸ©é˜µ$U_r=U_p^T$ï¼Œå°±æ˜¯å°† æ•°æ®ç©ºé—´ä»dç»´ç©ºé—´å†æŠ•å½±å›Nç»´ç©ºé—´ä¸Š* PCAç™½åŒ–ï¼š    * å¯¹PCAæŠ•å½±åçš„æ–°åæ ‡ï¼Œåšå½’ä¸€åŒ–å¤„ç†ï¼šåŸºäºç‰¹å¾å€¼è¿›è¡Œç¼©æ”¾        $$        X_{PCAwhite} = \Sigma^{-\frac{1}{2}}X^{&#39;} =  \Sigma^{-\frac{1}{2}}U^TX        $$    * $X_{PCAwhite}$çš„åæ–¹å·®çŸ©é˜µ$S_{PCAwhite} = I$ï¼Œå› æ­¤æ˜¯å»äº†correlationçš„* ZCAç™½åŒ–ï¼šåœ¨ä¸Šä¸€æ­¥åšå®Œä¹‹åï¼Œå†æŠŠå®ƒå˜æ¢åˆ°åŸå§‹ç©ºé—´ï¼Œæ‰€ä»¥ZCAç™½åŒ–åçš„ç‰¹å¾å›¾æ›´æ¥è¿‘åŸå§‹æ•°æ®    * å¯¹PCAç™½åŒ–åçš„æ•°æ®ï¼Œå†åšä¸€æ­¥recover        $$        X_{ZCAwhite} = U X_{PCAwhite}        $$    * åæ–¹å·®çŸ©é˜µä»æ—§æ˜¯Iï¼Œåˆæ³•ç™½åŒ–</code></pre><h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><ol><li><p>åŠ¨æœº</p><ul><li><p>BN reduces training time</p><ul><li>compute by each neuron</li><li>require moving average</li><li>depend on mini-batch size</li><li>how to apply to recurrent neural nets</li></ul></li><li><p>propose layer norm</p><ul><li>[unlike BN] compute by each layer</li><li>[like BN] with adaptive bias &amp; gain</li><li>[unlike BN] perform the same computation at training &amp; test time</li><li>[unlike BN] straightforward to apply to recurrent nets</li><li>work well for RNNs</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>BN<ul><li>reduce training time &amp; serves as regularizer</li><li>require moving averageï¼šintroduce dependencies between training cases </li><li>the approxmation of mean &amp; variance expectations constraints on the size of a mini-batch</li></ul></li><li>intuition<ul><li>norm layeræå‡è®­ç»ƒé€Ÿåº¦çš„æ ¸å¿ƒæ˜¯é™åˆ¶ç¥ç»å…ƒè¾“å…¥è¾“å‡ºçš„å˜åŒ–å¹…åº¦ï¼Œç¨³å®šæ¢¯åº¦</li><li>åªè¦æ§åˆ¶æ•°æ®åˆ†å¸ƒï¼Œå°±èƒ½ä¿æŒè®­ç»ƒé€Ÿåº¦</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li>compute over all hidden units in the same layer</li><li>different training cases have different normalization terms</li><li>æ²¡å•¥å¥½è¯´çš„ï¼Œå°±æ˜¯åœ¨channelç»´åº¦è®¡ç®—norm</li><li>furtherçš„GNæŠŠchannelç»´åº¦åˆ†ç»„åšnormï¼ŒINåœ¨ç›´æ¥æ¯ä¸ªç‰¹å¾è®¡ç®—norm</li><li>gain &amp; bias<ul><li>ä¹Ÿæ˜¯åœ¨å¯¹åº”ç»´åº¦ï¼š(hwd)c-dim</li><li><a href="https://tobiaslee.top/2019/11/21/understanding-layernorm/" target="_blank" rel="noopener">https://tobiaslee.top/2019/11/21/understanding-layernorm/</a></li><li>åç»­æœ‰å®éªŒå‘ç°ï¼Œå»æ‰ä¸¤ä¸ªlearnable rescale paramsåè€Œæç‚¹</li><li>è€ƒè™‘æ˜¯åœ¨training setä¸Šçš„è¿‡æ‹Ÿåˆ</li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li>RNNä¸Šæœ‰ç”¨</li><li>CNNä¸Šæ¯”æ²¡æœ‰norm layerå¥½ï¼Œä½†æ˜¯æ²¡æœ‰BNå¥½ï¼šå› ä¸ºchannelæ˜¯ç‰¹å¾ç»´åº¦ï¼Œç‰¹å¾ç»´åº¦ä¹‹é—´æœ‰æ˜æ˜¾çš„æœ‰ç”¨/æ²¡ç”¨ï¼Œä¸èƒ½ç®€å•çš„norm</li></ul></li></ol><h2 id="Weight-Normalization-A-Simple-Reparameterization-to-Accelerate-Training-of-Deep-Neural-Networks"><a href="#Weight-Normalization-A-Simple-Reparameterization-to-Accelerate-Training-of-Deep-Neural-Networks" class="headerlink" title="Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks"></a>Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks</h2><ol><li>åŠ¨æœº<ul><li>reparameterizing the weights<ul><li>decouple length &amp; direction</li><li>no dependency between samples which suits well for<ul><li>recurrent</li><li>reinforcement</li><li>generative</li></ul></li></ul></li><li>no additional memory and computation</li><li>testified on<ul><li>MLP with CIFAR</li><li>generative model VAE &amp; DRAW</li><li>reinforcement DQN</li></ul></li></ul></li><li>è®ºç‚¹<ul><li>a neuronï¼š<ul><li>get inputs from former layers(neurons)</li><li>weighted sum over the inputs</li><li>add a bias</li><li>elementwise nonlinear transformation</li><li>batch outputsï¼šone value per sample</li></ul></li><li>intuition of normalizationï¼š<ul><li>give gradients that are more like whitened natural gradients</li><li>BNï¼šmake the outputs of each neuronæœä»std norm</li><li>our WNï¼š<ul><li>inspired by BN</li><li>does not share BNâ€™s across-sample property</li><li>no addition memory and tiny addition computation</li></ul></li><li></li></ul></li></ul></li></ol><h2 id="Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization"><a href="#Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization" class="headerlink" title="Instance Normalization: The Missing Ingredient for Fast Stylization"></a>Instance Normalization: The Missing Ingredient for Fast Stylization</h2><ol><li><p>åŠ¨æœº</p><ul><li>stylizationï¼šé’ˆå¯¹é£æ ¼è¿ç§»ç½‘ç»œ</li><li>with a small changeï¼šswapping BN with IN</li><li>achieve qualitative improvement</li></ul></li><li><p>è®ºç‚¹</p><ul><li>stylized image <ul><li>a content image + a style image</li><li>both style and content statistics are obtained from a pretrained CNN for image classification</li><li>methods<ul><li>optimization-basedï¼šiterative thus computationally inefficient</li><li>generator-basedï¼šsingle pass but never as good as </li></ul></li></ul></li><li>our work<ul><li>revisit the feed-forward  method</li><li>replace BN in the generator with IN</li><li>keep them at test time as opposed to freeze</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>formulation</p><ul><li>given a fixed stype image $x_0$</li><li>given a set of content images $x_t, t= 1,2,â€¦,n$</li><li>given a pre-trained CNN</li><li>with a variable z controlling the generation of stylization results</li><li>compute the stylied image g($x_t$, z)</li><li>compare the statisticsï¼š$min_g \frac{1}{n} \sum^n_{t=1} L(x_0, x_t, g(x_t, z))$</li><li>comparing targetï¼šthe contrast of the stylized image is similar to the constrast of the style image</li></ul></li><li><p>observations</p><ul><li>the more training examples, the poorer the qualitive results</li><li>the result of stylization still depent on the constrast of the content image</li></ul></li><li><p>intuition </p><ul><li>é£æ ¼è¿ç§»æœ¬è´¨ä¸Šå°±æ˜¯å°†style imageçš„contrastç”¨åœ¨content imageçš„ï¼šä¹Ÿå°±æ˜¯rescale content imageçš„contrast</li><li><p>constrastæ˜¯per sampleçš„ï¼š$\frac{pixel}{\sum pixels\ on\ the\ map}$</p></li><li><p>BNåœ¨normçš„æ—¶å€™å°†batch samplesæ…åˆåœ¨äº†ä¸€èµ·</p></li></ul></li><li><p>IN</p><ul><li>instance-specfic normalization</li><li><p>also known as contrast normalization</p></li><li><p>å°±æ˜¯per imageåšæ ‡å‡†åŒ–ï¼Œæ²¡æœ‰trainable/frozen paramsï¼Œåœ¨test phaseä¹Ÿä¸€æ ·ç”¨</p><p>  <img src="/2021/03/02/layer-norm/IN.png" width="80%;"></p></li></ul></li></ul></li></ol><h2 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a>Group Normalization</h2><ol><li><p>åŠ¨æœº</p><ul><li>for small batch size</li><li>do normalization in channel groups</li><li>batch-independent</li><li>behaves stably over different batch sizes</li><li><p>approach BNâ€™s accuracy </p><p><img src="/2021/03/02/layer-norm/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/group-normalization/error.png" width="40%;"></p></li></ul></li><li><p>è®ºç‚¹</p><ul><li>BN<ul><li>requires sufficiently large batch size (e.g. 32)</li><li>Mask R-CNN frameworks use a batch size of 1 or 2 images because of higher resolution, where BN is â€œfrozenâ€ by transforming to a linear layer </li><li>synchronized BN ã€BR</li></ul></li><li>LN &amp; IN<ul><li>effective for training sequential models or generative models </li><li>but have limited success in visual recognition </li><li>GNèƒ½è½¬æ¢æˆLNï¼IN</li></ul></li><li>WN<ul><li>normalize the filter weights, instead of operating on features</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>group</p><ul><li>it is not necessary to think of deep neural network features as unstructured vectors <ul><li>ç¬¬ä¸€å±‚å·ç§¯æ ¸é€šå¸¸å­˜åœ¨ä¸€ç»„å¯¹ç§°çš„filterï¼Œè¿™æ ·å°±èƒ½æ•è·åˆ°ç›¸ä¼¼ç‰¹å¾</li><li>è¿™äº›ç‰¹å¾å¯¹åº”çš„channel can be normalized together</li></ul></li></ul></li><li><p>normalization</p><ul><li><p>transform the feature xï¼š$\hat x_i = \frac{1}{\sigma}(x_i-\mu_i)$</p></li><li><p>the mean and the standard deviationï¼š</p><script type="math/tex; mode=display">  \mu_i=\frac{1}{m}\sum_{k\in S_i}x_k\\  \sigma_i=\sqrt {\frac{1}{m}\sum_{k\in S_i}(x_k-\mu_i)^2+\epsilon}</script></li><li><p>the set $S_i$</p><ul><li>BNï¼š<ul><li>$S_i=\{k|k_C = i_C\}$</li><li>pixels sharing the same channel index are normalized together </li><li>for each channel, BN computes Î¼ and Ïƒ along the (N, H, W) axes </li></ul></li><li>LN<ul><li>$S_i=\{k|k_N = i_N\}$</li><li>pixels sharing the same batch index (per sample) are normalized together </li><li>LN computes Î¼ and Ïƒ along the (C,H,W) axes for each sample </li></ul></li><li>IN<ul><li>$S_i=\{k|k_N = i_N, k_C=i_C\}$</li><li>pixels sharing the same batch index and the same channel index are normalized together </li><li>LN computes Î¼ and Ïƒ along the (H,W) axes for each sample </li></ul></li><li>GN<ul><li>$S_i=\{k|k_N = i_N, [\frac{k_C}{C/G}]=[\frac{i_C}{C/G}]\}$</li><li>computes Î¼ and Ïƒ along the (H, W ) axes and along a group of C/G channels </li></ul></li></ul></li><li><p>linear transform  </p><ul><li>to keep representational ability </li><li><strong>per channel</strong></li><li>scale and shiftï¼š$y_i = \gamma \hat x_i + \beta$</li></ul><p><img src="/2021/03/02/layer-norm/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/group-normalization/gn.png" width="80%;"></p></li></ul></li><li><p>relation</p><ul><li>to LN<ul><li>LN assumes <em>all</em> channels in a layer make â€œsimilar contributionsâ€ </li><li>which is less valid with the presence of convolutions </li><li>GN improved representational power over LN</li></ul></li><li>to IN<ul><li>IN can only rely on the spatial dimension for computing the mean and variance </li><li>it misses the opportunity of exploiting the channel dependence</li><li>ã€QUESTIONã€‘BNä¹Ÿæ²¡è€ƒè™‘é€šé“é—´çš„è”ç³»å•Šï¼Œä½†æ˜¯è®¡ç®—meanå’Œvarianceæ—¶è·¨äº†sample</li></ul></li></ul></li><li><p>implementation</p><ul><li>reshape</li><li>learnable $\gamma \&amp; \beta$</li><li>computable mean &amp; var</li></ul><p><img src="/2021/03/02/layer-norm/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/group-normalization/code.png" width="50%;"></p></li></ul></li><li><p>å®éªŒ</p><ul><li>GNç›¸æ¯”äºBNï¼Œtraining erroræ›´ä½ï¼Œä½†æ˜¯val errorç•¥é«˜äºBN<ul><li>GN is effective for easing optimization</li><li>loses some regularization ability </li><li>it is possible that GN combined with a suitable regularizer will improve results </li></ul></li><li>é€‰å–ä¸åŒçš„groupæ•°ï¼Œæ‰€æœ‰çš„group&gt;1å‡å¥½äºgroup=1ï¼ˆLNï¼‰</li><li>é€‰å–ä¸åŒçš„channelæ•°ï¼ˆCï¼Gï¼‰ï¼Œæ‰€æœ‰çš„channel&gt;1å‡å¥½äºchannel=1ï¼ˆINï¼‰</li><li>Object Detection  <ul><li>frozenï¼šå› ä¸ºhigher resolutionï¼Œbatch sizeé€šå¸¸è®¾ç½®ä¸º2/GPUï¼Œè¿™æ—¶çš„BN frozenæˆä¸€ä¸ªçº¿æ€§å±‚$y=\gamma(x-\mu)/\sigma+beta$ï¼Œå…¶ä¸­çš„$\mu$å’Œ$sigma$æ˜¯loadäº†pre-trained modelä¸­ä¿å­˜çš„å€¼ï¼Œå¹¶ä¸”frozenæ‰ï¼Œä¸å†æ›´æ–°</li><li>denote as BN*</li><li>replace BN* with GN during fine-tuning </li><li>use a weight decay of 0 for the Î³ and Î² parameters </li></ul></li></ul></li></ol><h2 id="WS-Weight-Standardization"><a href="#WS-Weight-Standardization" class="headerlink" title="WS: Weight Standardization"></a>WS: Weight Standardization</h2><ol><li><p>åŠ¨æœº</p><ul><li>accelerate training</li><li>micro-batchï¼š<ul><li>ä»¥BN with large-batchä¸ºåŸºå‡†</li><li>ç›®å‰BN with micro-batchåŠå…¶ä»–normalization methodséƒ½ä¸èƒ½matchè¿™ä¸ªbaseline</li></ul></li><li>operates on weights instead of activations</li><li>æ•ˆæœ<ul><li>match or outperform BN</li><li>smooth the loss</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>two facts</p><ul><li>BNçš„performance gainä¸reduction of internal covariate shiftæ²¡ä»€ä¹ˆå…³ç³»</li><li>BNä½¿å¾—optimization landscape significantly smoother</li><li>å› æ­¤our target is to find another technique<ul><li>achieves smooth landscape</li><li>work with micro-batch</li></ul></li></ul></li><li><p>normalization methods </p><ul><li>focus on activations<ul><li>ä¸å±•å¼€</li></ul></li><li><p>focus on weights</p><ul><li>WNï¼šjust length-direction decoupling</li></ul><p><img src="/2021/03/02/layer-norm/methods2.png" width="40%;"></p></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>Lipschitz constants </p><ul><li>BN reduces the Lipschitz constants of the loss function</li><li>makes the gradient more Lipschitz</li><li>BN considers the Lipschitz constants with respect to activationsï¼Œnot the weights that the optimizer is directly optimizing</li></ul></li><li><p>our inspiration</p><ul><li>standardize the weightsä¹ŸåŒæ ·èƒ½å¤Ÿsmooth the landscape</li><li>æ›´ç›´æ¥</li><li>smoothing effects on activations and weightsæ˜¯å¯ä»¥ç´¯ç§¯çš„ï¼Œå› ä¸ºæ˜¯çº¿æ€§è¿ç®—</li></ul></li><li><p>Weight Standardization </p><ul><li>reparameterize the original weights $W$<ul><li>å¯¹å·ç§¯å±‚çš„æƒé‡å‚æ•°åšå˜æ¢ï¼Œno bias</li><li>$W \in R^{O * I}$</li><li>$O=C_{out}$</li><li>$I=C_{in}*kernel_size$</li></ul></li><li>optimize the loss on $\hat W$</li><li>compute mean &amp; var on I-dim</li><li><p>åªåšæ ‡å‡†åŒ–ï¼Œæ— éœ€affineï¼Œå› ä¸ºé»˜è®¤åç»­è¿˜è¦æ¥ä¸€ä¸ªnormalization layerå¯¹ç¥ç»å…ƒè¿›è¡Œrefine</p><p><img src="/2021/03/02/layer-norm/WS.png" width="40%;"></p></li></ul></li><li><p>WS normalizes gradients </p><ul><li><p>æ‹†è§£ï¼š</p><ul><li>eq5ï¼š$W$ to $\dot W$ï¼Œå‡å‡å€¼ï¼Œzero-centered</li><li>eq6ï¼š$\dot W$ to $\hat W$ï¼Œé™¤æ–¹å·®ï¼Œone-varianced</li><li>eq8ï¼š$\delta \hat W$ç”±å‰ä¸€æ­¥çš„æ¢¯åº¦normalizeå¾—åˆ°</li><li>eq9ï¼š$\delta \dot W$ä¹Ÿç”±å‰ä¸€æ­¥çš„æ¢¯åº¦normalize</li><li><p>æœ€ç»ˆç”¨äºæ¢¯åº¦æ›´æ–°çš„æ¢¯åº¦æ˜¯zero-centered</p><p><img src="/2021/03/02/layer-norm/gradients.png" width="40%;"></p></li></ul></li></ul></li><li><p>WS smooths landscape </p><ul><li>åˆ¤å®šæ˜¯å¦smoothå°±çœ‹Lipschitz constantçš„å¤§å°</li><li>eq5å’Œeq6éƒ½èƒ½reduce the Lipschitz constant</li><li>å…¶ä¸­eq5 makes the major improvements </li><li>eq6 slightly improvesï¼Œå› ä¸ºè®¡ç®—é‡ä¸å¤§ï¼Œæ‰€ä»¥ä¿ç•™</li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li><p>ImageNet</p><ul><li>BNçš„batchsizeæ˜¯64ï¼Œå…¶ä½™éƒ½æ˜¯1ï¼Œå…¶ä½™çš„æ¢¯åº¦æ›´æ–°iterationsæ”¹æˆ64â€”â€”ä½¿å¾—å‚æ•°æ›´æ–°æ¬¡æ•°åŒæ­¥</li><li>æ‰€æœ‰çš„normalization methodsåŠ ä¸ŠWSéƒ½æœ‰æå‡</li><li>è£¸çš„normalization methodsé‡Œé¢batchsize1çš„GNæœ€å¥½ï¼Œæ‰€ä»¥é€‰ç”¨GN+WSåšè¿›ä¸€æ­¥å®éªŒ</li><li><p>GN+WS+AFï¼šåŠ ä¸Šconv weightçš„affineä¼šharm</p><p><img src="/2021/03/02/layer-norm/imageNet.png" width="40%;"></p></li></ul></li></ul></li><li><p>code</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># official release</span></span><br><span class="line"><span class="comment"># æ”¾åœ¨WSConv2Då­ç±»çš„callé‡Œé¢</span></span><br><span class="line">kernel_mean = tf.math.reduce_mean(kernel, axis=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], keepdims=<span class="keyword">True</span>, name=<span class="string">'kernel_mean'</span>)</span><br><span class="line">kernel = kernel - kernel_mean</span><br><span class="line">kernel_std = tf.keras.backend.std(kernel, axis=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], keepdims=<span class="keyword">True</span>)</span><br><span class="line">kernel = kernel / (kernel_std + <span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> CNN, layer, normalization </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>NFNet</title>
      <link href="/2021/02/22/NFNet/"/>
      <url>/2021/02/22/NFNet/</url>
      <content type="html"><![CDATA[<h2 id="NFNet-High-Performance-Large-Scale-Image-Recognition-Without-Normalization"><a href="#NFNet-High-Performance-Large-Scale-Image-Recognition-Without-Normalization" class="headerlink" title="NFNet: High-Performance Large-Scale Image Recognition Without Normalization"></a>NFNet: High-Performance Large-Scale Image Recognition Without Normalization</h2><ol><li><p>åŠ¨æœº</p><ul><li>NFï¼š<ul><li>normalization-free</li><li>aims to match the test acc of batch-normalized networks<ul><li>attain new SOTA 86.5%</li><li>pre-training + fine-tuningä¸Šä¹Ÿè¡¨ç°æ›´å¥½89.2%</li></ul></li></ul></li><li>batch normalization<ul><li>ä¸æ˜¯å®Œç¾è§£å†³æ–¹æ¡ˆ</li><li>depends on batch size</li></ul></li><li>non-normalized networks<ul><li>accuracy</li><li>instabilitiesï¼šdevelop adaptive gradient clipping</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>vast majority models<ul><li>variants of deep residual + BN</li><li>allow deeper, stable and regularizing</li></ul></li><li>disadvantages of batch normalization<ul><li>computational expensive</li><li>introduces discrepancy between training &amp; testing models &amp; increase params</li><li>breaks the independence among samples</li></ul></li><li>methods seeks to replace BN<ul><li>alternative normalizers </li><li>study the origin benefits of BN</li><li>train deep ResNets without normalization layers  </li></ul></li><li>key theme when removing normalization<ul><li>suppress the scale of the residual branch</li><li>simplest wayï¼šapply a learnable scalar</li><li>recent workï¼šsuppress the branch at initialization &amp; apply Scaled Weight Standardizationï¼Œèƒ½è¿½ä¸ŠResNetå®¶æ—ï¼Œä½†æ˜¯æ²¡è¿½ä¸ŠEffå®¶æ—</li></ul></li><li>our NFNetsâ€™ main contributions <ul><li>propose AGCï¼šè§£å†³unstableé—®é¢˜ï¼Œallow larger batch size and stronger augmentatons</li><li>NFNetså®¶æ—åˆ·æ–°SOTAï¼šåˆå¿«åˆå‡†</li><li>pretraining + finetuningçš„æˆç»©ä¹Ÿæ¯”batch normed modelså¥½</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>Understanding Batch Normalization </p><ul><li>four main benefits<ul><li>downscale the residual branchï¼šä»initializationå°±ä¿è¯æ®‹å·®åˆ†æ”¯çš„scaleæ¯”è¾ƒå°ï¼Œä½¿å¾—ç½‘ç»œhas well-behaved gradients early in trainingï¼Œä»è€Œefficient optimization </li><li>eliminates mean-shiftï¼šReLUæ˜¯ä¸å¯¹ç§°çš„ï¼Œstacking layersä»¥åæ•°æ®åˆ†å¸ƒä¼šç´¯ç§¯åç§»</li><li>regularizing effectï¼šmini-batchä½œä¸ºsubsetå¯¹äºå…¨é›†æ˜¯æœ‰åçš„ï¼Œè¿™ç§noiseå¯ä»¥çœ‹ä½œæ˜¯regularizer</li><li>allows efficient large-batch trainingï¼šæ•°æ®åˆ†å¸ƒç¨³å®šæ‰€ä»¥losså˜åŒ–ç¨³å®šï¼ŒåŒæ—¶å¤§batchæ›´æ¥è¿‘çœŸå®åˆ†å¸ƒï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ›´å¤§çš„learning rateï¼Œä½†æ˜¯è¿™ä¸ªpropertyä»…åœ¨ä½¿ç”¨å¤§batch sizeçš„æ—¶å€™æœ‰æ•ˆ</li></ul></li></ul></li><li><p>NF-ResNets </p><ul><li>recovering the benefits of BNï¼šå¯¹residual branchè¿›è¡Œscaleå’Œmean-shift</li></ul></li></ul><ul><li><p>residual blockï¼š$h_{i+1} = h_i + \alpha f_i (h_i/\beta_i)$</p></li><li><p>$\beta_i = Var(h_i)$ï¼šå¯¹è¾“å…¥è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆæ–¹å·®ä¸º1ï¼‰ï¼Œè¿™æ˜¯ä¸ªexpected valueï¼Œä¸æ˜¯ç®—å‡ºæ¥çš„ï¼Œç»“æ„å®šæ­»å°±å®šæ­»äº†</p></li><li><p>Scaled Weight Standardization &amp; scaled activation</p><ul><li>æ¯”åŸç‰ˆçš„WSå¤šäº†ä¸€ä¸ª$\sqrt N$çš„åˆ†æ¯<ul><li>æºç å®ç°ä¸­æ¯”åŸç‰ˆWSè¿˜å¤šäº†learnable affine gain</li></ul></li><li>ä½¿å¾—conv-reluä»¥åè¾“å‡ºè¿˜æ˜¯æ ‡å‡†åˆ†å¸ƒ</li></ul><p><img src="/2021/02/22/NFNet/weight.png" width="45%;"></p></li><li><p>$\alpha=0.2$ï¼šrescale</p></li><li><p>residual branchä¸Šï¼Œæœ€ç»ˆçš„è¾“å‡ºä¸º$\alpha*$æ ‡å‡†åˆ†å¸ƒï¼Œæ–¹å·®æ˜¯$\alpha^2$</p></li><li><p>id pathä¸Šï¼Œè¾“å‡ºè¿˜æ˜¯$h_{i}$ï¼Œæ–¹å·®æ˜¯$Var(h_i)$</p><ul><li><p>updateè¿™ä¸ªblockè¾“å‡ºçš„æ–¹å·®ä¸º$Var(h_{i+1}) = Var(h_i)+\alpha^2$ï¼Œæ¥æ›´æ–°ä¸‹ä¸€ä¸ªblockçš„ $\beta$</p></li><li><p>variance reset</p><ul><li>æ¯ä¸ªtransition blockä»¥åï¼ŒæŠŠvarianceé‡æ–°è®¾å®šä¸º$1+\alpha^2$<ul><li>åœ¨æ¥ä¸‹æ¥çš„non-transition blockä¸­ï¼Œç”¨ä¸Šé¢çš„updateå…¬å¼æ›´æ–°expected std</li></ul></li></ul></li><li><p>å†åŠ ä¸Šadditional regularizationï¼ˆDropoutå’ŒStochastic Depthä¸¤ç§æ­£åˆ™æ‰‹æ®µï¼‰ï¼Œå°±æ»¡è¶³äº†BN benefitsçš„å‰ä¸‰æ¡</p><ul><li>åœ¨batch sizeè¾ƒå°çš„æ—¶å€™èƒ½å¤Ÿcatch upç”šè‡³è¶…è¶Šbatch normalized models<ul><li>ä½†æ˜¯large batch sizeçš„æ—¶å€™perform worse</li></ul></li></ul></li><li><p>å¯¹äºä¸€ä¸ªæ ‡å‡†çš„conv-bn-reluï¼Œä»workflowä¸Šçœ‹</p><ul><li>originï¼šinputâ€”â€”ä¸€ä¸ªfreeçš„conv weightingâ€”â€”BNï¼ˆnorm &amp; rescaleï¼‰â€”â€”activation<ul><li>NFNetï¼šinputâ€”â€”standard normâ€”â€”normed weighting &amp; activationâ€”â€”rescale</li></ul></li></ul></li></ul></li></ul></li></ol><ul><li><p>Adaptive Gradient Clipping for Efficient Large-Batch Training</p><ul><li><p>æ¢¯åº¦è£å‰ªï¼š</p><ul><li>clip by normï¼šç”¨ä¸€ä¸ªclipping threshold $\lambda$ è¿›è¡Œrescaleï¼Œtraining stability was extremely sensitive to è¶…å‚çš„é€‰æ‹©ï¼Œsettingsï¼ˆmodel depth, the batch size, or the learning rateï¼‰ä¸€å˜è¶…å‚å°±è¦é‡æ–°è°ƒ</li></ul></li></ul></li><li><p>clip by valueï¼šç”¨ä¸€ä¸ªclipping valueè¿›è¡Œä¸Šä¸‹é™æˆªæ–­</p></li><li><p>AGC</p><ul><li><p>given æŸå±‚çš„æƒé‡$W \in R^{N<em>M}$ å’Œ å¯¹åº”æ¢¯åº¦$G \in R^{N</em>M}$</p></li><li><p>ratio $\frac{||G||_F}{||W||_F}$ å¯ä»¥çœ‹ä½œæ˜¯æ¢¯åº¦å˜åŒ–å¤§å°çš„measurement</p></li><li><p>æ‰€ä»¥æˆ‘ä»¬ç›´è§‚åœ°æƒ³åˆ°å°†è¿™ä¸ªratioè¿›è¡Œé™å¹…ï¼šæ‰€è°“çš„adaptiveå°±æ˜¯åœ¨æ¢¯åº¦è£å‰ªçš„æ—¶å€™ä¸æ˜¯å¯¹æ‰€æœ‰æ¢¯åº¦ä¸€åˆ€åˆ‡ï¼Œè€Œæ˜¯è€ƒè™‘å…¶å¯¹åº”æƒé‡å¤§å°ï¼Œä»è€Œè¿›è¡Œæ›´åˆç†çš„è°ƒèŠ‚</p><ul><li><p>ä½†æ˜¯å®éªŒä¸­å‘ç°unit-wiseçš„gradient normè¦æ¯”layer-wiseçš„å¥½ï¼šæ¯ä¸ªunitå°±æ˜¯æ¯è¡Œï¼Œå¯¹äºconv weightså°±æ˜¯(hxwxCin)ä¸­çš„ä¸€ä¸ª</p><p><img src="/2021/02/22/NFNet/clipping.png" width="45%;"></p></li></ul></li><li><p>scalar hyperparameter $\lambda$</p><pre><code>  * the optimal value may depend on the choice of optimizer, learning rate and batch size  * empirically we found $\lambda$ should be smaller for larger batches</code></pre><ul><li><p>ablations for AGC</p><p><img src="/2021/02/22/NFNet/agc.png" width="50%;"></p><ul><li>ç”¨pre-activation NF-ResNet-50 å’Œ NF-ResNet-200 åšå®éªŒï¼Œbatch sizeé€‰æ‹©ä»256åˆ°4096ï¼Œå­¦ä¹ ç‡ä»0.1å¼€å§‹åŸºäºbatch sizeçº¿æ€§å¢é•¿ï¼Œè¶…å‚$\lambda$çš„å–å€¼è§å³å›¾<ul><li>å·¦å›¾ç»“è®º1ï¼šåœ¨batch sizeè¾ƒå°çš„æƒ…å†µä¸‹ï¼ŒNF-Netsèƒ½å¤Ÿè¿½ä¸Šç”šè‡³è¶…è¶Šnormed modelsçš„ç²¾åº¦ï¼Œä½†æ˜¯batch sizeä¸€å¤§ï¼ˆ2048ï¼‰æƒ…å†µå°±æ¶åŒ–äº†ï¼Œä½†æ˜¯æœ‰AGCçš„NF-Netsåˆ™èƒ½å¤Ÿmaintaining performance comparable or better thanï½ï½ï½</li><li>å·¦å›¾ç»“è®º2ï¼šthe benefits of using AGC are smaller when the batch size is small</li></ul></li></ul></li><li>å³å›¾ç»“è®º1ï¼šè¶…å‚$\lambda$çš„å–å€¼æ¯”è¾ƒå°çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯¹æ¢¯åº¦çš„clippingæ›´strongï¼Œè¿™å¯¹äºä½¿ç”¨å¤§batch sizeè®­ç»ƒçš„ç¨³å®šæ€§æ¥è¯´éå¸¸é‡è¦</li></ul></li><li><p>whether or not AGC is beneficial for all layers</p><pre><code>      * it is always better to not clip the final linear layer   * æœ€å¼€å§‹çš„å·ç§¯ä¸åšæ¢¯åº¦è£å‰ªä¹Ÿèƒ½ç¨³å®šè®­ç»ƒ</code></pre><ul><li>æœ€ç»ˆwe apply AGC to every layer except for the final linear layer</li></ul></li><li><p>Normalizer-Free Architectures</p></li><li><p>begin with SE-ResNeXt-D model</p></li><li><p>about group width</p><pre><code>      * set group width to 128</code></pre><ul><li>the reduction in compute density means that åªå‡å°‘äº†ç†è®ºä¸Šçš„FLOPsï¼Œæ²¡æœ‰å®é™…åŠ é€Ÿ</li></ul></li><li><p>about stages</p><pre><code>      * Rç³»åˆ—æ¨¡å‹åŠ æ·±çš„æ—¶å€™æ˜¯éçº¿æ€§å¢é•¿ï¼Œç–¯ç‹‚å åŠ stage3çš„blockæ•°ï¼Œå› ä¸ºè¿™ä¸€å±‚resolutionä¸å¤§ï¼Œchannelä¹Ÿä¸æ˜¯æœ€å¤šï¼Œå…¼é¡¾äº†ä¸¤ä¾§è®¡ç®—é‡</code></pre><ul><li>æˆ‘ä»¬ç»™F0è®¾ç½®ä¸º[1,2,6,3]ï¼Œç„¶ååœ¨deeper variantsä¸­å¯¹æ¯ä¸ªstageçš„blockæ•°ç”¨ä¸€ä¸ªscalar Nçº¿å½¢å¢é•¿</li></ul></li><li><p>about width</p><pre><code>      * ä»æ—§å¯¹stage3ä¸‹æ‰‹ï¼Œ[256,512,1536,1536]      * roughly preserves the training speed</code></pre><ul><li>ä¸€ä¸ªè®ºç‚¹ï¼šstage3 is the best place to add capacityï¼Œå› ä¸ºdeeper enoughåŒæ—¶have access to deeper levelsåŒæ—¶åˆæ¯”æœ€åä¸€å±‚æœ‰slightly higher resolution</li></ul></li><li><p>about block</p><pre><code>  * å®éªŒå‘ç°æœ€æœ‰ç”¨çš„æ“ä½œæ˜¯adding an additional 3 Ã— 3 grouped conv after the first  * overview</code></pre><p>  <img src="/2021/02/22/NFNet/NFblock.png" width="50%;"></p></li><li><p>about scaling variants</p><pre><code>      * effç³»åˆ—é‡‡ç”¨çš„æ˜¯Rã€Wã€Dä¸€èµ·å¢é•¿ï¼Œå› ä¸ºeffçš„blockæ¯”è¾ƒè½»é‡</code></pre><ul><li><p>ä½†æ˜¯å¯¹Rç³»åˆ—æ¥è¯´ï¼Œåªå¢é•¿Då’ŒRå°±å¤Ÿäº†</p></li><li><p>è¡¥å……ç»†èŠ‚</p><pre><code>  * åœ¨inferenceé˜¶æ®µä½¿ç”¨æ¯”è®­ç»ƒé˜¶æ®µslightly higher resolution</code></pre><ul><li>éšç€æ¨¡å‹åŠ å¤§increase the regularization strengthï¼š<ul><li>scale the drop rate of Dropout</li><li>è°ƒæ•´stochastic depth rateå’Œweight decayåˆ™not effective</li></ul></li><li><p>se-blockçš„scaleä¹˜ä¸ª2</p></li><li><p>SGD params:</p><ul><li>Nesterov=True, momentum=0.9, clipnorm=0.01</li><li>lrï¼š<ul><li>å…ˆwarmupå†ä½™å¼¦é€€ç«ï¼šincrease from 0 to 1.6 over 5 epochs, then decay to zero with cosine annealing</li><li>ä½™å¼¦é€€ç«cosine annealing<ul><li></li></ul></li></ul></li></ul><p><img src="/2021/02/22/NFNet/NFNets.png" width="45%;"></p></li><li><p>summary</p><ul><li>æ€»ç»“æ¥è¯´ï¼Œå°±æ˜¯æ‹¿æ¥ä¸€ä¸ªSE-ResNeXt-D</li><li>å…ˆåšç»“æ„ä¸Šçš„è°ƒæ•´ï¼Œmodified width and depth patternsä»¥åŠa second spatial convolutionï¼Œè¿˜æœ‰drop rateï¼Œresolution</li><li>å†åšå¯¹æ¢¯åº¦çš„è°ƒæ•´ï¼šé™¤äº†æœ€åä¸€ä¸ªçº¿å½¢åˆ†ç±»å±‚ä»¥å¤–ï¼Œå…¨ç”¨AGCï¼Œ$\lambda=0.01$</li></ul></li><li><p>æœ€åæ˜¯è®­ç»ƒä¸Šçš„trickï¼šstrong regularization and data augmentation</p><p>  <img src="/2021/02/22/NFNet/ablation.png" width="45%;"></p></li></ul></li><li><p>detailed view of NFBlocks</p><ul><li>transition blockï¼šæœ‰ä¸‹é‡‡æ ·çš„block<ul><li>æ®‹å·®branchä¸Šï¼Œbottleneckçš„narrow ratioæ˜¯0.5</li><li>æ¯ä¸ªstageçš„3x3 convçš„group widthæ°¸è¿œæ˜¯128ï¼Œè€Œgroupæ•°ç›®æ˜¯åœ¨éšç€block widthå˜çš„</li><li>skip pathæ¥åœ¨ $\beta$ downscaling ä¹‹å</li><li>skip pathä¸Šæ˜¯avg pooling + 1x1 conv</li></ul></li><li><p>non-transition blockï¼šæ— ä¸‹é‡‡æ ·çš„block</p><ul><li>bottleneck-ratioä»æ—§æ˜¯0.5</li><li>3x3convçš„group widthä»æ—§æ˜¯128</li><li>skip pathæ¥åœ¨$\beta$ downscaling ä¹‹å‰</li><li>skip pathå°±æ˜¯id</li></ul><p><img src="/2021/02/22/NFNet/NFblocks.png" width="45%;"></p></li></ul></li></ul></li></ul></li></ul><ol><li><p>å®éªŒ</p><ul><li></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> CNN, classification </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>repVGG</title>
      <link href="/2021/02/09/repVGG/"/>
      <url>/2021/02/09/repVGG/</url>
      <content type="html"><![CDATA[<h2 id="RepVGG-Making-VGG-style-ConvNets-Great-Again"><a href="#RepVGG-Making-VGG-style-ConvNets-Great-Again" class="headerlink" title="RepVGG: Making VGG-style ConvNets Great Again"></a>RepVGG: Making VGG-style ConvNets Great Again</h2><ol><li><p>åŠ¨æœº</p><ul><li>plain ConvNets <ul><li>simply efficient but poor performance</li></ul></li><li>propose a CNN architecture RepVGG<ul><li>èƒ½å¤Ÿdecoupleä¸ºtraining-timeå’Œinference-timeä¸¤ä¸ªç»“æ„</li><li>é€šè¿‡structure re-paramterization technique</li><li>inference-time architecture has a VGG-like plain body</li></ul></li><li>faster<ul><li>83% faster than ResNet-50 or 101% faster than ResNet-101</li></ul></li><li>accuracy-speed trade-off<ul><li>reaches over 80% top-1 accuracy</li><li>outperforms ResNets by a large margin</li></ul></li><li>verify on classification &amp; semantic segmentation tasks</li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>well-designed CNN architectures </p><ul><li>Inceptionï¼ŒResNetï¼ŒDenseNetï¼ŒNAS models</li><li>deliver higher accuracy</li><li>drawbacks <ul><li>multi-branch designsï¼šslow down inference and reduce memory utilizationï¼Œå¯¹é«˜å¹¶è¡ŒåŒ–çš„è®¾å¤‡ä¸å‹å¥½</li><li>some componentsï¼šdepthwise &amp; channel shuffleï¼Œincrease memory access cost </li></ul></li><li>MAC(memory access cost) constitutes a large time usage in groupwise convolutionï¼šæˆ‘çš„groupconvå®ç°é‡Œcardinalityç»´åº¦ä¸Šè®¡ç®—ä¸å¹¶è¡Œ</li><li>FLOPså¹¶ä¸èƒ½precisely reflect actual speedï¼Œä¸€äº›ç»“æ„çœ‹ä¼¼æ¯”old fashioned VGG/resnetçš„FLOPså°‘ï¼Œä½†å®é™…å¹¶æ²¡æœ‰å¿«</li></ul></li><li><p>multi-branch</p><ul><li>é€šå¸¸multi-branch modelè¦æ¯”plain modelè¡¨ç°å¥½</li><li>å› ä¸ºmakes the model an implicit ensemble of numerous shallower models</li><li>so that avoids gradient vanishing</li><li>benefits are all for training</li><li>drawbacks are undesired for inference</li></ul></li><li><p>the proposed RepVGG</p><ul><li>advantages<ul><li>plain architectureï¼šno branches</li><li>3x3 conv &amp; ReLUç»„æˆ</li><li>æ²¡æœ‰è¿‡é‡çš„äººå·¥è®¾è®¡ç—•è¿¹</li></ul></li><li>training time use identity &amp; 1x1 conv branches</li><li><p>at inference time</p><ul><li>identity å¯ä»¥çœ‹åšdegraded 1x1 conv</li><li>1x1 conv å¯ä»¥çœ‹åšdegraded 3x3 conv</li><li>æœ€ç»ˆæ•´ä¸ªconv-bn branchesèƒ½å¤Ÿæ•´åˆæˆä¸€ä¸ª3x3 conv</li><li>inference-time modelåªåŒ…å«convå’ŒReLUï¼šæ²¡æœ‰max poolingï¼ï¼</li><li>fewer memory unitsï¼šåˆ†æ”¯ä¼šå å†…å­˜ï¼Œç›´åˆ°åˆ†æ”¯è®¡ç®—ç»“æŸï¼Œplainç»“æ„çš„memoryåˆ™æ˜¯immediately released</li></ul><p><img src="/2021/02/09/repVGG/memory.png" width="40%;"></p></li></ul></li></ul></li><li><p>æ–¹æ³•</p><p> <img src="/2021/02/09/repVGG/repvgg.png" width="45%;"></p><ul><li><p>training-time</p><ul><li>ResNet-like block<ul><li>id + 1x1 conv + 3x3 conv multi-branches</li><li>use BN in each branch</li><li>with n blocks, the model can be interpreted as an ensemble of $3^n$ models</li><li>stride2çš„blockåº”è¯¥æ²¡æœ‰id pathå§ï¼Ÿï¼Ÿ</li></ul></li><li>simply stack serveral blocks to construct the training model</li></ul></li><li><p>inference-time</p><ul><li><p>re-param</p><ul><li>inference-time BNä¹Ÿæ˜¯ä¸€ä¸ªçº¿æ€§è®¡ç®—</li><li>ä¸¤ä¸ª1x1 convéƒ½å¯ä»¥è½¬æ¢æˆä¸­é€šçš„3x3 kernelï¼Œæœ‰æƒ/æ— æƒ</li><li><p>è¦æ±‚å„branch has the same strides &amp; padding pixelè¦å¯¹é½</p><p><img src="/2021/02/09/repVGG/reparam.png" width="40%;"></p></li></ul></li><li><p>architectural specification </p><ul><li>varietyï¼šdepth and width</li><li>does not use maxpoolingï¼šåªæœ‰ä¸€ç§operatorï¼š3x3 conv+relu</li><li>headï¼šGAP + fc / task specific</li><li>5 stages<ul><li>ç¬¬ä¸€ä¸ªstageå¤„ç†high resolutionï¼Œstride2</li><li>ç¬¬äº”ä¸ªstage shall have more channelsï¼Œæ‰€ä»¥åªç”¨ä¸€å±‚ï¼Œsave parameters</li><li>ç»™å€’æ•°ç¬¬äºŒä¸ªstageæœ€å¤šå±‚ï¼Œè€ƒè™‘paramså’Œcomputationçš„balance</li></ul></li><li>RepVGG-Aï¼š[1,2,4,14,1]ï¼Œç”¨æ¥compete againstè½»é‡å’Œä¸­é‡çº§model</li><li><p>RepVGG-Bï¼šdeeper in s2,3,4ï¼Œ[1,4,6,16,1]ï¼Œç”¨æ¥compete against high-performance ones</p><p><img src="/2021/02/09/repVGG/architecture.png" width="45%;"></p></li><li><p>basic widthï¼š[64, 128, 256, 512]  </p><ul><li>width multiplier a &amp; b</li><li>aæ§åˆ¶å‰4ä¸ªstageå®½åº¦ï¼Œbæ§åˆ¶æœ€åä¸€ä¸ªstage</li><li>[64a, 128a, 256a, 512b]  </li><li><p>ç¬¬ä¸€ä¸ªstageçš„å®½åº¦åªæ¥å—å˜å°ä¸æ¥å—å˜å¤§ï¼Œå› ä¸ºå¤§resolutionå½±å“è®¡ç®—é‡ï¼Œmin(64,64a)</p><p><img src="/2021/02/09/repVGG/variety.png" width="45%;"></p></li></ul></li><li><p>further reduce params &amp; computation</p><ul><li>groupwise 3x3 conv</li><li>è·³ç€å±‚æ¢ï¼šä»ç¬¬ä¸‰å¼€å§‹ï¼Œç¬¬ä¸‰ã€ç¬¬äº”ã€</li><li>number of groupsï¼š1ï¼Œ2ï¼Œ4 globally </li></ul></li></ul></li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li><p>åˆ†æ”¯çš„ä½œç”¨</p><p>  <img src="/2021/02/09/repVGG/branch.png" width="45%;"></p></li><li><p>ç»“æ„ä¸Šçš„å¾®è°ƒ</p><ul><li>id pathå»æ‰BN</li><li>æŠŠæ‰€æœ‰çš„BNç§»åŠ¨åˆ°addçš„åé¢</li><li><p>æ¯ä¸ªpathåŠ ä¸Šrelu</p><p><img src="/2021/02/09/repVGG/variants.png" width="45%;"></p></li></ul></li><li><p>ImageNetåˆ†ç±»ä»»åŠ¡ä¸Šå¯¹æ ‡å…¶ä»–æ¨¡å‹</p><ul><li><p>simple augmentation </p><p>  <img src="/2021/02/09/repVGG/simple.png" width="45%;"></p></li><li><p>strongï¼šAutoaugment, label smoothing and mixup </p><p>  <img src="/2021/02/09/repVGG/strong.png" width="45%;"></p></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>transform in CNN</title>
      <link href="/2021/02/03/transform-in-CNN/"/>
      <url>/2021/02/03/transform-in-CNN/</url>
      <content type="html"><![CDATA[<h2 id="ç»¼è¿°"><a href="#ç»¼è¿°" class="headerlink" title="ç»¼è¿°"></a>ç»¼è¿°</h2><ol><li>å‡ ä½•å˜æ¢<ul><li>STNï¼š<ul><li>æ™®é€šçš„CNNèƒ½å¤Ÿéšå¼çš„å­¦ä¹ ä¸€å®šçš„å¹³ç§»ã€æ—‹è½¬ä¸å˜æ€§ï¼Œè®©ç½‘ç»œèƒ½å¤Ÿé€‚åº”è¿™ç§å˜æ¢ï¼šé™é‡‡æ ·ç»“æ„æœ¬èº«èƒ½å¤Ÿä½¿å¾—ç½‘ç»œå¯¹å˜æ¢ä¸æ•æ„Ÿ</li><li>ä»æ•°æ®è§’åº¦å‡ºå‘ï¼Œæˆ‘ä»¬è¿˜ä¼šå¼•å…¥å„ç§augmentationï¼Œå¼ºåŒ–ç½‘ç»œå¯¹å˜åŒ–çš„ä¸å˜èƒ½åŠ›</li><li>deepMindä¸ºç½‘ç»œè®¾è®¡äº†ä¸€ä¸ªæ˜¾å¼çš„å˜æ¢æ¨¡å—æ¥å­¦ä¹ å„ç§å˜åŒ–ï¼Œå°†distortedçš„è¾“å…¥å˜æ¢å›å»ï¼Œè®©ç½‘ç»œå­¦ä¹ æ›´ç®€å•çš„ä¸œè¥¿</li><li>å‚æ•°é‡ï¼šå°±æ˜¯å˜æ¢çŸ©é˜µçš„å‚æ•°ï¼Œé€šå¸¸æ˜¯2x3çš„çººå°„å˜åŒ–çŸ©é˜µï¼Œä¹Ÿå°±æ˜¯6ä¸ªå‚æ•°</li></ul></li><li>deformable convï¼š<ul><li>based on STN</li><li>é’ˆå¯¹åˆ†ç±»å’Œæ£€æµ‹åˆ†åˆ«æå‡ºdeformable convolutionå’Œdeformable RoI poolingï¼š</li><li>æ„Ÿè§‰deformable RoI poolingå’Œguiding anchoré‡Œé¢çš„feature adaptionæ˜¯ä¸€ä¸ªä¸œè¥¿</li><li>å‚æ•°é‡ï¼šregular kernel params 3x3 + deformable offsets 3x3x2</li><li>whatâ€™s newï¼Ÿ<ul><li>ä¸ªäººè®¤ä¸ºå¼•å…¥æ›´å¤šçš„å‚æ•°å¼•å…¥çš„å˜åŒ–</li><li>é¦–å…ˆSTNæ˜¯ä»outputåˆ°inputçš„æ˜ å°„ï¼Œä½¿ç”¨å˜æ¢çŸ©é˜µMé€šå¸¸åªèƒ½è¡¨ç¤ºdepictable transformationï¼Œä¸”å…¨å›¾åªæœ‰1ä¸ªtransformation</li><li>å…¶æ¬¡STNçš„sampling kernelä¹Ÿæ˜¯é¢„å®šä¹‰çš„ç®—æ³•ï¼Œå¯¹kernelå†…çš„æ‰€æœ‰pixelä½¿ç”¨ç›¸åŒçš„å˜åŒ–ï¼Œä¹Ÿå°±æ˜¯1ä¸ªweight factor</li><li>deformable convæ˜¯ä»inputåˆ°outputçš„æ˜ å°„ï¼Œæ˜ å°„å¯ä»¥æ˜¯ä»»æ„çš„transformationï¼Œä¸”3x3x2çš„å‚æ•°æœ€å¤šå¯ä»¥åŒ…å«3x3ç§transformation</li><li>sampling kernelå¯¹kernelå†…çš„æ¯ä¸ªç‚¹ï¼Œä¹Ÿå¯ä»¥æœ‰ä¸åŒçš„æƒé‡ï¼Œä¹Ÿå°±æ˜¯3x3ä¸ªweight factor</li></ul></li></ul></li><li>è¿˜æœ‰å•¥è·Ÿå½¢å˜ç›¸å…³çš„</li></ul></li><li>attentionæœºåˆ¶<ul><li>spatial attentionï¼šSTNï¼ŒsSE</li><li>channel attentionï¼šSENet</li><li>åŒæ—¶ä½¿ç”¨ç©ºé—´attentionå’Œé€šé“attentionæœºåˆ¶ï¼šCBAM</li></ul></li><li><p>papers</p><ul><li>[STN] STN: Spatial Transformer Networksï¼ŒSTNçš„å˜æ¢æ˜¯pre-definedçš„ï¼Œæ˜¯é’ˆå¯¹å…¨å±€featuremapçš„å˜æ¢</li><li>[DCN 2017] Deformable Convolutional Networks ï¼ŒDCNçš„å˜æ¢æ˜¯æ›´éšæœºçš„ï¼Œæ˜¯é’ˆå¯¹å±€éƒ¨kernelåˆ†åˆ«è¿›è¡Œçš„å˜åŒ–ï¼ŒåŸºäºå·ç§¯æ ¸æ·»åŠ location-specific shift</li><li>[DCNv2 2018] Deformable ConvNets v2: More Deformable, Better Resultsï¼Œè¿›ä¸€æ­¥æ¶ˆé™¤irrelevant contextï¼ŒåŸºäºå·ç§¯æ ¸æ·»åŠ weighted-location-specific shiftï¼Œæå‡performance</li><li>[attentionç³»åˆ—paper] [SENet &amp;SKNet &amp; CBAM &amp; GC-Net][<a href="https://amberzzzz.github.io/2020/03/13/attention%E7%B3%BB%E5%88%97/">https://amberzzzz.github.io/2020/03/13/attention%E7%B3%BB%E5%88%97/</a>]</li></ul></li></ol><h2 id="STN-Spatial-Transformer-Networks"><a href="#STN-Spatial-Transformer-Networks" class="headerlink" title="STN: Spatial Transformer Networks"></a>STN: Spatial Transformer Networks</h2><ol><li><p>åŠ¨æœº</p><ul><li>ä¼ ç»Ÿå·ç§¯ï¼šlack the ability of spacially invariant</li><li>propose a new learnable module<ul><li>can be inserted into CNN</li><li>spatially manipulate the data</li><li>without any extra supervision</li><li>models learn to be invariant to transformations</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>spatially invariant<ul><li>the ability of being invariant to large transformations of the input data</li></ul></li><li>max-pooling<ul><li>åœ¨ä¸€å®šç¨‹åº¦ä¸Šspatially invariant</li><li>å› ä¸ºreceptive fields are fixed and local and small</li><li>å¿…é¡»å åŠ åˆ°æ¯”è¾ƒæ·±å±‚çš„æ—¶å€™æ‰èƒ½å®ç°ï¼Œintermediate feature layerså¯¹large transformationsä¸å¤ªè¡Œ</li><li>æ˜¯ä¸€ç§pre-defined mechanismï¼Œè·Ÿsampleæ— å…³</li></ul></li><li>spatial transformation module<ul><li>conditioned on individual samples</li><li>dynamic mechanism</li><li>produce a transformation and perform it on the entire feature map </li></ul></li><li>taskåœºæ™¯<ul><li>distorted digitsåˆ†ç±»ï¼šå¯¹è¾“å…¥åštranformèƒ½å¤Ÿsimplifyåé¢çš„åˆ†ç±»ä»»åŠ¡</li><li>co-localisationï¼š</li><li>spatial attention</li></ul></li><li>related work<ul><li>ç”Ÿæˆå™¨ç”¨æ¥ç”Ÿæˆtransformed imagesï¼Œä»è€Œåˆ¤åˆ«å™¨èƒ½å¤Ÿå­¦ä¹ åˆ†ç±»ä»»åŠ¡from transformation supervision</li><li>ä¸€äº›methodsè¯•å›¾ä»ç½‘ç»œç»“æ„ã€feature extractorsçš„è§’åº¦çš„è·å¾—invariant representationsï¼Œwhile STN aims to achieve this by manipulating the data</li><li>manipulating the dataé€šå¸¸å°±æ˜¯åŸºäºattention mechanismï¼Œcropæ¶‰åŠdifferentiableé—®é¢˜</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>formulation </p><ul><li>localisation networkï¼špredict transform parameters</li><li>grid generatorï¼šåŸºäºpredicted paramsç”Ÿæˆsampling grid</li><li><p>samplerï¼šelement-multiply</p><p><img src="/2021/02/03/transform-in-CNN/spacial transformer.png" width="80%;"></p></li></ul></li><li><p>localisation network</p><ul><li>input feature map $U \in R^{h<em>w</em>c}$</li><li>same transformation is applied to each channel</li><li>generate parameters of transformation $\theta$ï¼š1-d vector</li><li>fc / conv + final regression layer</li></ul></li><li><p>parameterised sampling grid</p><ul><li><p>sampling kernel</p></li><li><p>applied by pixel</p></li><li><p>general affine transformationï¼šcroppingï¼Œtranslationï¼Œrotationï¼Œscaleï¼Œskew</p></li><li><p>ouput mapä¸Šä»»æ„ä¸€ç‚¹ä¸€å®šæ¥è‡ªå˜æ¢å‰çš„æŸä¸€ç‚¹ï¼Œåä¹‹ä¸ä¸€å®šï¼Œinput mapä¸ŠæŸä¸€ç‚¹å¯èƒ½æ˜¯bgï¼Œè¢«cropæ‰äº†ï¼Œæ‰€ä»¥pointwise transformationå†™æˆåè¿‡æ¥çš„ï¼š</p><p>  <img src="/2021/02/03/transform-in-CNN/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/transform-in-CNN/pointwise transformation.png" width="50%;"></p><p>  <img src="/2021/02/03/transform-in-CNN/transformation.png" width="70%;"></p></li><li><p>target pointsæ„æˆçš„ç‚¹é›†å°±æ˜¯sampling points on the input feature map</p></li></ul></li><li><p>differentiable image sampling </p><ul><li><p>é€šè¿‡ä¸Šä¸€æ­¥çš„çŸ©é˜µtransformationï¼Œå¾—åˆ°input mapä¸Šéœ€è¦ä¿ç•™çš„source point set</p></li><li><p>å¯¹ç‚¹é›†ä¸­æ¯ä¸€ç‚¹apply kernel</p></li><li><p>é€šç”¨çš„æ’å€¼è¡¨è¾¾å¼ï¼š</p><p>  <img src="/2021/02/03/transform-in-CNN/interpolation.png" width="70%;"></p></li><li><p>æœ€è¿‘é‚»kernelæ˜¯ä¸ªpulseå‡½æ•°</p><p>  <img src="/2021/02/03/transform-in-CNN/nearest.png" width="70%;"></p></li><li><p>bilinear kernelæ˜¯ä¸ªdistance&gt;1çš„å…¨muteæ‰ï¼Œåˆ†æ®µå¯å¯¼</p><p>  <img src="/2021/02/03/transform-in-CNN/bilinear.png" width="70%;"></p></li></ul></li><li><p>STNï¼šSpatial Transformer Networks </p><ul><li>æŠŠspatial transformeråµŒè¿›CNNå»ï¼šlearn how to actively transform the features to help minimize the overall cost</li><li>computationally fast</li><li>å‡ ç§ç”¨æ³•<ul><li>feed the output of the localization network $\theta$ to the rest of the networkï¼šå› ä¸ºtransformå‚æ•°explicitly encodesç›®æ ‡çš„ä½ç½®å§¿æ€ä¿¡æ¯</li><li>place multiple spatial transformers at increasing depthï¼šä¸²è¡Œèƒ½å¤Ÿè®©æ·±å±‚çš„transformerå­¦ä¹ æ›´æŠ½è±¡çš„å˜æ¢</li><li>place multiple spatial transformers in parallelï¼šå¹¶è¡Œçš„å˜æ¢ä½¿å¾—æ¯ä¸ªå˜æ¢é’ˆå¯¹ä¸åŒçš„object</li></ul></li></ul></li></ul></li><li><p>å®éªŒ</p><p> <img src="/2021/02/03/transform-in-CNN/error.png" width="70%;"></p><ul><li>Rã€RTSã€Pã€Eï¼šdistortion ahead</li><li>affã€projã€TPSï¼štransformer predefined<ul><li>affï¼šç»™å®šè§’åº¦ï¼Ÿï¼Ÿ</li><li>TPSï¼šè–„æ¿æ ·æ¡æ’å€¼</li></ul></li></ul></li></ol><h2 id="Deformable-Convolutional-Networks"><a href="#Deformable-Convolutional-Networks" class="headerlink" title="Deformable Convolutional Networks"></a>Deformable Convolutional Networks</h2><ol><li><p>åŠ¨æœº</p><ul><li>CNNï¼šfixed geometric structures</li><li>enhance the transformation modeling capability<ul><li>deformable convolution</li><li>deformable RoI pooling</li></ul></li><li>without additional supervision</li><li>share similiar spirit with STN</li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>to accommodate geometric variations </p><ul><li>data augmentation is limited to model large, unknown transformations </li><li>fixed receptive fields is undesirable for high level CNN layers that encode the semantics </li><li>ä½¿ç”¨å¤§é‡å¢å¹¿çš„æ•°æ®ï¼Œæšä¸¾ä¸å…¨ï¼Œè€Œä¸”æ”¶æ•›æ…¢ï¼Œæ‰€éœ€ç½‘ç»œå‚æ•°é‡å¤§</li><li>å¯¹äºæå–è¯­ä¹‰ç‰¹å¾çš„é«˜å±‚ç½‘ç»œæ¥è®²ï¼Œå›ºå®šçš„æ„Ÿå—é‡å¯¹ä¸åŒç›®æ ‡ä¸å‹å¥½</li></ul></li><li><p>introduce two new modules </p><ul><li>deformable convolution<ul><li>learning offsets for each kernel via additional convolutional layers </li></ul></li><li><p>deformable RoI pooling</p><ul><li>learning offset for each bin partition of the previous RoI pooling </li></ul><p><img src="/2021/02/03/transform-in-CNN/deform.png" width="40%;"></p></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>overview</p><ul><li>operate on the 2D spatial domain </li><li>remains the same across the channel dimension </li></ul></li><li><p>deformable convolution</p><ul><li>æ­£å¸¸çš„å·ç§¯ï¼š<ul><li>$y(p_0) = \sum w(p_n)*x(p_0 + p_n)$</li><li>$p_n \in R\{(-1,-1),(-1,0),â€¦, (0,0), (1,1)\}$</li></ul></li><li>deformable convï¼šwith offsets $\Delta p_n$<ul><li>$y(p_0) = \sum w(p_n)*x(p_0 + p_n + \Delta p_n)$</li><li>offset value is typically fractional </li><li>bilinear interpolationï¼š<ul><li>$x(p) = \sum_q G(q,p)x(q)$</li><li>å…¶ä¸­$G(q,p)$æ˜¯æ¡ä»¶ï¼š$G(q,p)=max(0, 1-|q_x-p_x|)*max(0, 1-|q_y-p_y|)$</li><li>åªè®¡ç®—å’Œoffsetç‚¹è·ç¦»å°äº1ä¸ªå•ä½çš„é‚»è¿‘ç‚¹</li></ul></li></ul></li><li>å®ç°<ul><li><img src="/2021/02/03/transform-in-CNN/conv.png" width="40%;"></li><li>offsets convå’Œç‰¹å¾æå–convæ˜¯ä¸€æ ·çš„kernelï¼šsame spatial resolution and dilationï¼ˆNä¸ªpositionï¼‰</li><li>the channel dimension 2Nï¼šå› ä¸ºæ˜¯xå’Œyä¸¤ä¸ªæ–¹å‘çš„offset</li></ul></li></ul></li><li><p>deformable RoI pooling</p><ul><li><p>RoI pooling converts an input feature map of arbitrary size into fixed size features </p></li><li><p>å¸¸è§„çš„RoI pooling</p><ul><li>divides ROI into k*k bins and for each binï¼š$y(i,j) = \sum_{p \in bin(i,j)} x(p_0+p)/n_{ij}$</li><li>å¯¹feature mapä¸Šåˆ’åˆ†åˆ°æ¯ä¸ªbiné‡Œé¢æ‰€æœ‰çš„ç‚¹</li></ul></li><li><p>deformable RoI poolingï¼šwith offsets $\Delta p_{ij}$</p><ul><li>$y(i,j) = \sum_{p \in bin(i,j)} x(p_0+p+\Delta p_{ij})/n_{ij}$</li><li>scaled normalized offsetsï¼š$\Delta p_{ij} = \gamma \Delta p_{ij} (w,h) $</li><li>normalized offset value is fractional </li><li>bilinear interpolation on the pooled map as above</li></ul></li><li><p>å®ç°</p><ul><li><img src="/2021/02/03/transform-in-CNN/pooling.png" width="40%;"></li><li>fc layerï¼šk*k*2ä¸ªelementï¼ˆsigmoidï¼Ÿï¼‰</li></ul></li><li><p>position sensitive RoI Pooling</p><ul><li>fully convolutional  </li><li>input feature mapå…ˆé€šè¿‡å·ç§¯æ‰©å±•æˆk*k*(C+1)é€šé“</li><li><p>å¯¹æ¯ä¸ªC+1(åŒ…å«k<em>kä¸ªfeature map)ï¼Œconvå‡ºå…¨å›¾çš„offset(2\</em>k*kä¸ª)</p><p><img src="/2021/02/03/transform-in-CNN/pspooling.png" width="40%;"></p></li></ul></li></ul></li><li><p>deformable convNets</p><ul><li>initialized with zero weights</li><li>learning rates are set to $\beta$ times of the learning rate for the existing layers <ul><li>$\beta=1.0$ for conv</li><li>$\beta=0.01$ for fc</li></ul></li><li>feature extraction<ul><li>backï¼šResNet-101 &amp; Aligned-Inception-ResNet </li><li>withoutTopï¼šA randomly initialized 1x1 conv is added at last to reduce the channel dimension to 1024 </li><li>last block<ul><li>stride is changed from 2 to 1</li><li>the dilation of all the convolution filters with kernel size&gt;1 is changed from 1 to 2</li></ul></li><li>Optionally last block<ul><li>use deformable conv in res5a,b,c</li></ul></li></ul></li><li>segmentation and detection<ul><li>deeplab predicts 1x1 score maps</li><li>Category-Aware RPN run region proposal with specific class</li><li>modified faster R-CNNï¼šadd ROI pooling at last conv</li><li>optional faster R-CNNï¼šuse deformable ROI pooling</li><li>R-FCNï¼šstate-of-the-art detector</li><li>optional R-FCNï¼šuse deformable ROI pooling</li></ul></li></ul><p><img src="/2021/02/03/transform-in-CNN/Illustration.png" width="80%;"></p></li></ul></li><li><p>å®éªŒ</p><ul><li><p>Accuracy steadily improves when more deformable convolution layers are usedï¼šä½¿ç”¨è¶Šå¤šå±‚deform convè¶Šå¥½ï¼Œç»éªŒå–äº†3</p><p>  <img src="/2021/02/03/transform-in-CNN/table1.png" width="80%;"></p></li><li><p>the learned offsets are highly adaptive to the image contentï¼šå¤§ç›®æ ‡çš„é—´è·å¤§ï¼Œå› ä¸ºreception fieldå¤§ï¼Œconsistent in different layers </p><p>  <img src="/2021/02/03/transform-in-CNN/table2.png" width="45%;"></p></li><li><p>atrous convolution also improvesï¼šdefault networks have too small receptive fieldsï¼Œä½†æ˜¯dilationéœ€è¦æ‰‹è°ƒåˆ°æœ€ä¼˜</p></li><li><p>using deformable RoI pooling alone already produces noticeable performance gains, using both obtains significant accuracy improvements</p><p>  <img src="/2021/02/03/transform-in-CNN/table3.png" width="80%;"></p></li></ul></li></ol><h2 id="Deformable-ConvNets-v2-More-Deformable-Better-Results"><a href="#Deformable-ConvNets-v2-More-Deformable-Better-Results" class="headerlink" title="Deformable ConvNets v2: More Deformable, Better Results"></a>Deformable ConvNets v2: More Deformable, Better Results</h2><ol><li><p>åŠ¨æœº</p><ul><li>DCNèƒ½å¤Ÿadaptä¸€å®šçš„geometric variationsï¼Œä½†æ˜¯ä»å­˜åœ¨extend beyond image contentçš„é—®é¢˜</li><li>to focus on pertinent image regions<ul><li>increased modeling power<ul><li>more deformable layers</li><li>updated DCNv2 modules</li></ul></li><li>stronger training<ul><li>propose feature mimicking scheme</li></ul></li></ul></li><li>verified on<ul><li>incorporated into Faster-RCNN &amp; Mask RCNN</li><li>COCO for det &amp; set</li></ul></li><li>still lightweight and easy to incorporate</li></ul></li><li><p>è®ºç‚¹</p><ul><li>DCNv1<ul><li>deformable convï¼šåœ¨standard convçš„åŸºç¡€ä¸Šgenerate location-specific offsets which are learned from the preceding feature maps</li><li>deformable poolingï¼šoffsets are learned for the bin positions in RoIpooling</li><li>é€šè¿‡å¯è§†åŒ–æ•£ç‚¹å›¾å‘ç°æœ‰éƒ¨åˆ†æ•£ç‚¹è½åœ¨ç›®æ ‡å¤–å›´</li></ul></li><li>propose DCNv2<ul><li>equip more convolutional layers with offset</li><li>modified module<ul><li>each sample not only undergoes a learned offset</li><li>but also a learned feature amplitude</li></ul></li><li>effective trainin<ul><li>use RCNN as the teacher network since RCNN learns features unaffected by irrelevant info outside the ROI</li><li>feature mimicking loss</li></ul></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>stacking more deformable conv layers</p><ul><li>replace more regular conv layers by their deformable counterpartsï¼š<ul><li>resnet50çš„stage3ã€4ã€5çš„3x3convéƒ½æ›¿æ¢æˆdeformable convï¼š13ä¸ªconv layer</li><li>DCNv1æ˜¯æŠŠstage5çš„3ä¸ªresblockçš„3x3 convæ›¿æ¢æˆdeformable convï¼š3ä¸ªdeconv layer</li></ul></li><li>å› ä¸ºDCNv1é‡Œé¢åœ¨PASCALä¸Šé¢å®éªŒå‘ç°å†å¤šçš„deconvç²¾åº¦å°±é¥±å’Œäº†ï¼Œä½†æ˜¯DCNv2æ˜¯åœ¨harder dataset COCOä¸Šé¢çš„best-acc-efficiency-tradeoff</li></ul></li><li><p>modulated deformable conv</p><ul><li>modulate the input feature amplitudes from different spacial locations/bins<ul><li>set the learnable offset &amp; scalar for the k-th locationï¼š$\Delta p_k$å’Œ$\Delta m_k$</li><li>set the conv kernel dilationï¼š$p_k$ï¼Œresneté‡Œé¢éƒ½æ˜¯1</li><li>the value for location p isï¼š$y(p) = \sum_{k=1}^K w_k x(p+p_k+\Delta p_k)\Delta m_k$ï¼Œbilinear interpolation</li></ul></li><li>ç›®çš„æ˜¯æŠ‘åˆ¶æ— å…³ä¿¡å·</li><li>learnable offset &amp; scalar obtained via a separate conv layer over the same input feature map x</li><li>è¾“å‡ºæœ‰3Kä¸ªchannelï¼š2K for xy-offsetï¼ŒK for scalar<ul><li>offsetçš„convåé¢æ²¡æ¿€æ´»å‡½æ•°ï¼Œå› ä¸ºèŒƒå›´æ— é™</li><li>scalarçš„convåé¢æœ‰ä¸ªsigmoidï¼Œå°†rangeæ§åˆ¶åœ¨[0,1]</li><li>ä¸¤ä¸ªconvå…¨0åˆå§‹åŒ–</li><li>ä¸¤ä¸ªconv layerçš„learning rateæ¯”existing layerså°ä¸€ä¸ªæ•°é‡çº§</li></ul></li></ul></li><li><p>modulated deformable RoIpooling</p><ul><li>given an input ROI</li><li>split into K(7x7) spatial bins</li><li>average pooling over the sampling points for each binè®¡ç®—binçš„value</li><li>the bin value isï¼š$y(k) = \sum_{j=1}^{n_k} x(p_{kj}+\Delta p_k)\Delta m_k /n_k$ï¼Œbilinear interpolation</li><li>a sibling branch<ul><li>2ä¸ª1024d-fcï¼šgaussian initialization with 0.01 std dev</li><li>1ä¸ª3Kd-fcï¼šå…¨0åˆå§‹åŒ–</li><li>last K channels + sigmoid</li><li>learning rateè·Ÿexisting layersä¿æŒä¸€è‡´</li></ul></li></ul></li><li><p>RCNN feature mimicking</p><ul><li>å‘ç°æ— è®ºæ˜¯convè¿˜æ˜¯deconvï¼Œerror-boundéƒ½å¾ˆå¤§</li><li>å°½ç®¡ä»è®¾è®¡æ€è·¯ä¸Šï¼ŒDCNv2æ˜¯å¸¦æœ‰mute irrelevantçš„èƒ½åŠ›çš„ï¼Œä½†æ˜¯äº‹å®ä¸Šå¹¶æ²¡åšåˆ°</li><li>è¯´æ˜such representation cannot be learned well through standard FasterRCNN training procedureï¼š<ul><li>è¯´ç™½äº†å°±æ˜¯supervisionåŠ›åº¦ä¸å¤Ÿ</li><li>éœ€è¦additional guidance</li></ul></li></ul></li><li><p>feature mimic loss</p><ul><li><p>enforced only on positive ROIsï¼šå› ä¸ºèƒŒæ™¯ç±»å¾€å¾€éœ€è¦æ›´é•¿è·ç¦»/æ›´å¤§èŒƒå›´çš„contextä¿¡æ¯</p></li><li><p>architecture</p><ul><li>add an additional RCNN branch</li><li>RCNN input cropped imagesï¼Œgenerate 14x14 featuremapsï¼Œç»è¿‡ä¸¤ä¸ªfcå˜æˆ1024-d</li><li>å’ŒFasterRCNNé‡Œå¯¹åº”çš„counterpartï¼Œè®¡ç®—cosine similarity</li><li><p>è¿™ä¸ªå¤ªæ‰¯äº†ä¸å±•å¼€äº†</p><p><img src="/2021/02/03/transform-in-CNN/mimic.png" width="50%;"></p></li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> å‡ ä½•å˜æ¢ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>spineNet</title>
      <link href="/2021/01/28/spineNet/"/>
      <url>/2021/01/28/spineNet/</url>
      <content type="html"><![CDATA[<h2 id="SpineNet-Learning-Scale-Permuted-Backbone-for-Recognition-and-Localization"><a href="#SpineNet-Learning-Scale-Permuted-Backbone-for-Recognition-and-Localization" class="headerlink" title="SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization"></a>SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</h2><ol><li><p>åŠ¨æœº</p><ul><li>object detection task<ul><li>requiring simultaneous recognition and localization</li><li>solely encoder performs not well</li><li>while encoder-decoder architectures are ineffective</li></ul></li><li><p>propose SpineNet</p><ul><li>scale-permuted intermediate features</li><li>cross-scale connections</li><li>searched by NAS on detection COCO</li><li>can transfer to classification tasks</li><li><p>åœ¨è½»é‡å’Œé‡é‡backçš„ä¸€é˜¶æ®µç½‘ç»œä¸­éƒ½æ¶¨ç‚¹é¢†å…ˆ</p><p><img src="/2021/01/28/spineNet/scale-permuted.png" width="40%;"></p></li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>scale-decreasing backbone <ul><li>throws away the spatial information by down-sampling</li><li>challenging to recover</li><li>æ¥ä¸€ä¸ªè½»é‡çš„FPNï¼š</li></ul></li><li>scale-permuted model <ul><li>scales of features can increase/decrease anytimeï¼šretain the spacial information </li><li>connections go across scalesï¼šmulti-scale fusion</li><li>searched by NAS</li><li>æ˜¯ä¸€ä¸ªå®Œæ•´çš„FPNï¼Œä¸æ˜¯encoder-decoderé‚£ç§å¯åˆ†çš„å½¢å¼</li><li>directly connect to classification and bounding box regression subnets</li><li>base on ResNet50<ul><li>use bottleneck feature blocks</li><li>two inputs for each feature blocks</li><li>roughly the same computation</li></ul></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>formulation</p><ul><li>overall architecture<ul><li>stemï¼šscale-decreased architecture </li><li>scale-permuted network</li><li>blocks in the stem network can be candidate inputs for the following scale-permuted network </li></ul></li><li>scale-permuted network<ul><li>building blocksï¼š$B_k$</li><li>feature levelï¼š$L_3 - L_7$</li><li>output featuresï¼š1x1 convï¼Œ$P_3 - P_7$</li></ul></li></ul></li><li><p>search space</p><ul><li><p>scale-permuted networkï¼š</p><ul><li>blockåªèƒ½ä»å‰å¾€åconnect</li><li>based on resNet blocks</li><li>channel 256 for $L_5, L_6, L_7$</li></ul></li><li><p>cross-scale connectionsï¼š</p><ul><li><p>two input connections for each block</p></li><li><p>from lower ordering block / stem</p></li><li><p>resampling</p><ul><li>narrow factor  $\alpha$ï¼š1x1 conv</li><li>ä¸Šé‡‡æ ·ï¼šinterpolation</li><li>ä¸‹é‡‡æ ·ï¼š3x3 s2 conv</li><li><p>element-wise add</p><p><img src="/2021/01/28/spineNet/resampling.png" width="50%;"></p></li></ul></li></ul></li><li><p>block adjustment</p><ul><li>intermediate blocks can adjust its scale level &amp; type</li><li>level from {-1, 0, 1, 2}</li><li>select from bottleneck / residual block</li></ul></li></ul></li><li><p>family of models</p><ul><li>R[N] - SP[M]ï¼šN feature layers in stem &amp; M feature layers in scale-permuted layers</li><li>gradually shift from stem to SP</li><li><p>with size decreasing </p><p><img src="/2021/01/28/spineNet/shifting.png" width="45%;"></p><p><img src="/2021/01/28/spineNet/family.png" width="75%;"></p></li></ul></li><li><p>spineNet family</p><ul><li>basicï¼šspineNet-49</li><li>spineNet-49Sï¼šchannelæ•°scaled down by 0.65</li><li>spineNet-96ï¼šdouble the number of blocks</li><li>spineNet-143ï¼šrepeat 3 timesï¼Œfusion narrow factor $\alpha=1$</li><li>spineNet-190ï¼šrepeat 4 timesï¼Œfusion narrow factor $\alpha=1$ï¼Œchannelæ•°scaled up by 1.3</li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li><p>åœ¨mid/heavyé‡çº§ä¸Šï¼Œæ¯”resnet-family-FPNæ¶¨å‡ºä¸¤ä¸ªç‚¹</p><p>  <img src="/2021/01/28/spineNet/heavy.png" width="45%;"></p></li><li><p>åœ¨lighté‡çº§ä¸Šï¼Œæ¯”mobileNet-family-FPNæ¶¨å‡ºä¸€ä¸ªç‚¹</p><p>  <img src="/2021/01/28/spineNet/light.png" width="45%;"></p></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> backbone </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>guided anchoring</title>
      <link href="/2021/01/27/guided-anchoring/"/>
      <url>/2021/01/27/guided-anchoring/</url>
      <content type="html"><![CDATA[<p>åŸä½œè€…çŸ¥ä¹referenceï¼š<a href="https://zhuanlan.zhihu.com/p/55854246" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/55854246</a></p><ul><li>ä¸å®Œå…¨æ˜¯anchor-freeï¼Œå› ä¸ºè¿˜æ˜¯æœ‰decision grid to choose fromçš„ï¼Œåº”è¯¥è¯´æ˜¯adaptive anchor instead of hand-picked</li><li>ä¸ºäº†ç‰¹å¾å’Œadaptive anchorå¯¹é½ï¼Œå¼•å…¥deformable conv</li></ul><h2 id="Region-Proposal-by-Guided-Anchoring"><a href="#Region-Proposal-by-Guided-Anchoring" class="headerlink" title="Region Proposal by Guided Anchoring"></a>Region Proposal by Guided Anchoring</h2><ol><li><p>åŠ¨æœº</p><ul><li>most methods<ul><li>predefined anchors</li><li>do a uniformed dense prediction </li></ul></li><li>our method<ul><li>use sematic features to guide the anchoring</li><li>anchor sizeä¹Ÿæ˜¯ç½‘ç»œé¢„æµ‹å‚æ•°ï¼Œcompute from feature map</li><li>arbitrary aspect ratios </li></ul></li><li>feature inconsistency<ul><li>ä¸åŒçš„anchor locéƒ½æ˜¯å¯¹åº”feature mapä¸ŠæŸä¸€ä¸ªç‚¹</li><li>å˜åŒ–çš„anchor sizeå’Œå›ºå®šçš„ä½ç½®å‘é‡ä¹‹é—´å­˜åœ¨inconsistency</li><li>å¼•å…¥feature adaption module</li></ul></li><li>use high-quality proposals<ul><li>GA-RPNæå‡äº†proposalçš„è´¨é‡</li><li>å› æ­¤æˆ‘ä»¬å¯¹proposalè¿›å…¥stage2çš„æ¡ä»¶æ›´ä¸¥æ ¼</li></ul></li><li>adopt in Fast R-CNN, Faster R-CNN and RetinaNetå‡æ¶¨ç‚¹<ul><li>RPNæå‡æ˜¾è‘—ï¼š9.1</li><li>MAPä¹Ÿæœ‰æ¶¨ç‚¹ï¼š1.2-2.7</li></ul></li><li>è¿˜å¯ä»¥boosting trained models<ul><li>boosting a two-stage detector by a fine-tuning schedule</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>alignment &amp; consistency</p><ul><li><p>æˆ‘ä»¬ç”¨feature mapçš„pixelsä½œä¸ºanchor representationsï¼Œé‚£ä¹ˆanchor centerså¿…é¡»è·Ÿfeature pixelsä¿æŒalign</p></li><li><p>ä¸åŒpixelçš„reception fieldå¿…é¡»è·Ÿå¯¹åº”çš„anchor sizeä¿æŒåŒ¹é…</p></li><li>previous sliding window schemeå¯¹æ¯ä¸ªpixeléƒ½åšä¸€æ ·çš„æ“ä½œï¼Œç”¨åŒæ ·ä¸€ç»„anchorï¼Œå› æ­¤æ˜¯alignå’Œconsistçš„</li><li>previous progressly refining schemeå¯¹anchorçš„ä½ç½®å¤§å°åšäº†refinementï¼Œignore the alignment &amp; consistency issueï¼Œæ˜¯ä¸å¯¹çš„ï¼ï¼</li></ul></li><li><p>disadvantage of predefined anchors</p><ul><li>hard hyperparams</li><li>huge pos/neg imbalance &amp; computation </li></ul></li><li><p>we propose GA-RPN</p><ul><li>learnable anchor shapes to mitigate the hand-picked issue</li><li>feature adaptation to solve the consistency issue</li><li>key concerns in this paper<ul><li>learnable anchors</li><li>joint anchor distribution </li><li>alignment &amp; consistency</li><li>high-quality proposals </li></ul></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>formulation</p><ul><li>$p(x,y,w,h|I) = p(x,y|I)p(w,h|x,y,I)$</li><li>å°†é—®é¢˜è§£è€¦æˆä½ç½®å’Œå°ºå¯¸çš„é¢„æµ‹ï¼Œé¦–å…ˆanchorçš„locæœä»full imageçš„å‡åŒ€åˆ†å¸ƒï¼Œanchorçš„sizeå»ºç«‹åœ¨locå­˜åœ¨çš„åŸºç¡€ä¸Š</li><li>two branches for loc &amp; shape prediction<ul><li>locï¼šbinary classificationï¼Œhxwx1</li><li>shapeï¼šlocation-dependent shapesï¼Œhxwx2</li><li>anchorsï¼šloc probabilities above a certain threshold &amp; correponding â€˜most probableâ€™ anchor shape</li></ul></li><li>multi-scale<ul><li>the anchor generation parameters are shared</li></ul></li><li><p>feature adaptation module</p><ul><li>adapts the feature according to the anchor shape </li></ul><p><img src="/2021/01/27/guided-anchoring/guided anchoring.png" width="80%;"></p></li></ul></li><li><p>anchor location prediction</p><ul><li>indicates the probability of an objectâ€™s center  </li><li>ä¸€å±‚å·ç§¯ï¼š1x1 convï¼Œchannel1ï¼Œsigmoid</li><li>transform backï¼šeach grid(i,j) corresponds to coords ((i+0.5)*s, (j+0.5)*s) on the origin map</li><li>filter out 90% of the regions</li><li>thus replace the ensuing conv layers by masked convs</li><li>groud truth<ul><li>binary label map </li><li>each levelï¼šcenter region &amp; ignore region &amp; outside regionï¼ŒåŸºäºobject centerçš„æ–¹æ¡†<ul><li>$\sigma_1=0.2ï¼Œ\sigma_2=0.5$ï¼šregion boxçš„é•¿å®½ç³»æ•°</li><li>ï¼Ÿï¼Ÿï¼Ÿç”¨centerNetçš„heatmapä¼šä¸ä¼šæ›´å¥½ï¼Ÿï¼Ÿï¼Ÿ</li></ul></li></ul></li><li>focal loss $L_{loc}$</li></ul></li><li>anchor shape prediction<ul><li>predicts the best shape for each location</li><li>best shapeï¼ša shape that lead to best iou with the nearest gt box</li><li>ä¸€å±‚å·ç§¯ï¼š1x1 convï¼Œchannel2ï¼Œ[-1,1]</li><li>transform layerï¼štransform direct [-1,1] outputs to real box shape<ul><li>$w = \sigma <em> s </em> e^{dw}$</li><li>$h = \sigma <em> s </em> e^{dh}$</li><li>sï¼šstride</li><li>$\sigma$ï¼šç»éªŒå‚æ•°ï¼Œ8 in experiments</li></ul></li><li>set 9 pairs of (w,h) as RetinaNetï¼Œcalculate the IoU of these sampled anchors with gtï¼Œtake the max as target value</li><li>bounded iou lossï¼š$L_{shape} = L_1(1-min(\frac{w}{w_g}, \frac{w_g}{w})) + L_1(1-min(\frac{h}{h_g}, \frac{h_g}{h}))$ </li></ul></li><li>feature adaptation<ul><li>intuitionï¼šthe feature corresponding to different size of anchor shapesåº”è¯¥encode different content region</li><li>inputsï¼šfeature map &amp; anchor shape</li><li>location-dependent transformationï¼š3x3 deformable conv</li><li>deformable convçš„offsetæ˜¯anchor shapeå¾—åˆ°çš„</li><li>outputsï¼šadapted features</li></ul></li><li>with adapted features<ul><li>then perform further classification and bounding-box regression </li></ul></li><li>training<ul><li>jointly optimizeï¼š$L = \lambda_1 L_{loc} + \lambda_2 L_{shape} + L_{cls} + L_{reg}$</li><li>$\lambda_1=0.2ï¼Œ\lambda_2=0.5$</li><li>each level of feature map should only target objects of a specific scale rangeï¼šä½†æ˜¯ASFFè®ºæ–‡ä¸»å¼ è¯´è¿™ç§arrange by scaleçš„æ¨¡å¼ä¼šå¼•å…¥å‰èƒŒæ™¯inconsistencyï¼Ÿï¼Ÿ</li></ul></li><li>High-quality Proposals <ul><li>set a higher positive/negative threshold </li><li>use fewer samples  </li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ç›®æ ‡æ£€æµ‹ï¼Œone/two-stage </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ASFF</title>
      <link href="/2021/01/25/ASFF/"/>
      <url>/2021/01/25/ASFF/</url>
      <content type="html"><![CDATA[<h2 id="Learning-Spatial-Fusion-for-Single-Shot-Object-Detection"><a href="#Learning-Spatial-Fusion-for-Single-Shot-Object-Detection" class="headerlink" title="Learning Spatial Fusion for Single-Shot Object Detection"></a>Learning Spatial Fusion for Single-Shot Object Detection</h2><ol><li><p>åŠ¨æœº</p><ul><li>inconsistency when fuse across different feature scales</li><li>propose ASFF<ul><li>suppress the inconsistency </li><li>spatially filter conflictive informationï¼šæƒ³æ³•åº”è¯¥è·ŸSSE-blockç±»ä¼¼</li></ul></li><li><p>build on yolov3</p><ul><li>introduce a bag of tricks</li><li>anchor-free pipeline</li></ul><p><img src="/2021/01/25/ASFF/ASFF.png" width="70%;"></p></li></ul></li><li>è®ºç‚¹<ul><li>ssd is one of the first to generate pyramidal feature representations<ul><li>deeper layers reuse the formers</li><li>bottom-up path</li><li>small instances suffers low acc because containing insufficient semanic info</li></ul></li><li>FPN use top-down path<ul><li>shares rich semantics at all levels </li><li>improvementï¼šmore strengthening feature fusion </li></ul></li><li>åœ¨ä½¿ç”¨FPNæ—¶ï¼Œé€šå¸¸ä¸åŒscaleçš„ç›®æ ‡ç»‘å®šåˆ°ä¸åŒçš„levelä¸Šé¢<ul><li>inconsistencyï¼šå…¶ä»–levelçš„feature mapå¯¹åº”ä½ç½®çš„ä¿¡æ¯åˆ™ä¸ºèƒŒæ™¯</li><li>some methods set ignore region in adjacent features</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li>introduce advanced techniques  <ul><li>mixup</li><li>cosine learning rate schedule </li><li>sync-bn</li><li>an anchor-free branch to run jointly with anchor-based ones</li><li>L1 loss + IoU loss</li></ul></li><li>fusion<ul><li>å…¨è”æ¥è€Œéadjacent mergeï¼šä¸‰ä¸ªlevelçš„fuse mapéƒ½æ¥è‡ªä¸‰ä¸ªlevelçš„feature map</li><li>ä¸Šé‡‡æ ·ï¼š<ul><li>1x1 convï¼šå¯¹é½channel</li><li>upsamp with interpolation</li></ul></li><li>ä¸‹é‡‡æ ·ï¼š<ul><li>s2ï¼š3x3 s2 conv</li><li>s4ï¼šmaxpooling + 3x3 s2 conv</li></ul></li><li>adaptive fusion<ul><li>pixel levelçš„reweight</li><li>shared across channelsï¼šhxwx1</li><li>å¯¹æ¥è‡ªä¸‰ä¸ªlevelçš„feature mapï¼Œresolutionå¯¹é½ä»¥åï¼Œåˆ†åˆ«1x1convï¼Œchannel 1</li><li>norm the weightsï¼šsoftmax</li><li>ä¸ºå•¥èƒ½suppress inconsistencyï¼šä¸‰ä¸ªlevelçš„åƒç´ ç‚¹ï¼Œåªæ¿€æ´»ä¸€ä¸ªå¦å¤–ä¸¤ä¸ªæ˜¯0çš„æƒ…å†µæ˜¯ç»å¯¹ä¸harmçš„ï¼Œç›¸å½“äºä¸Šé¢ignoreé‚£ä¸ªæ–¹æ³•æ‹“å±•æˆadaptive</li></ul></li></ul></li><li>training<ul><li>apply mixup on the classification pretraining of D53</li><li>turn off mixup augmentation for the last 30 epochs. </li></ul></li><li>inference<ul><li>the detection header at each level first predicts the shape of anchorsï¼Ÿï¼Ÿï¼Ÿè¿™ä¸ªä¸å¤ªæ‡‚</li></ul></li><li>ASFF &amp; ASFF*<ul><li>enhanced version of ASFF by integrating other lightweight modules </li><li>dropblock &amp; RFB</li></ul></li></ul></li><li><p>å®ç°</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ASFF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, level, activate, rfb=False, vis=False)</span>:</span></span><br><span class="line">        super(ASFF, self).__init__()</span><br><span class="line">        self.level = level</span><br><span class="line">        self.dim = [<span class="number">512</span>, <span class="number">256</span>, <span class="number">128</span>]</span><br><span class="line">        self.inter_dim = self.dim[self.level]</span><br><span class="line">        <span class="keyword">if</span> level == <span class="number">0</span>:</span><br><span class="line">            self.stride_level_1 = conv_bn(<span class="number">256</span>, self.inter_dim, kernel=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">            self.stride_level_2 = conv_bn(<span class="number">128</span>, self.inter_dim, kernel=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">            self.expand = conv_bn(self.inter_dim, <span class="number">512</span>, kernel=<span class="number">3</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">elif</span> level == <span class="number">1</span>:</span><br><span class="line">            self.compress_level_0 = conv_bn(<span class="number">512</span>, self.inter_dim, kernel=<span class="number">1</span>)</span><br><span class="line">            self.stride_level_2 = conv_bn(<span class="number">128</span>, self.inter_dim, kernel=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">            self.expand = conv_bn(self.inter_dim, <span class="number">256</span>, kernel=<span class="number">3</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">elif</span> level == <span class="number">2</span>:</span><br><span class="line">            self.compress_level_0 = conv_bn(<span class="number">512</span>, self.inter_dim, kernel=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">            self.compress_level_1= conv_bn(<span class="number">256</span>,self.inter_dim,kernel=<span class="number">1</span>,stride=<span class="number">1</span>)</span><br><span class="line">            self.expand = conv_bn(self.inter_dim, <span class="number">128</span>, kernel=<span class="number">3</span>, stride=<span class="number">1</span>)</span><br><span class="line">        compress_c = <span class="number">8</span> <span class="keyword">if</span> rfb <span class="keyword">else</span> <span class="number">16</span>  </span><br><span class="line">        self.weight_level_0 = conv_bn(self.inter_dim, compress_c, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        self.weight_level_1 = conv_bn(self.inter_dim, compress_c, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        self.weight_level_2 = conv_bn(self.inter_dim, compress_c, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        self.weight_levels = conv_bias(compress_c * <span class="number">3</span>, <span class="number">3</span>, kernel=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.vis = vis</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_level_0, x_level_1, x_level_2)</span>:</span></span><br><span class="line">      <span class="comment"># è·Ÿè®ºæ–‡æè¿°ä¸€æ ·ï¼šä¸Šé‡‡æ ·å…ˆ1x1convå¯¹é½ï¼Œå†upinterpï¼Œä¸‹é‡‡æ ·3x3 s2 conv</span></span><br><span class="line">        <span class="keyword">if</span> self.level == <span class="number">0</span>:</span><br><span class="line">            level_0_resized = x_level_0</span><br><span class="line">            level_1_resized = self.stride_level_1(x_level_1)</span><br><span class="line">            level_2_downsampled_inter = F.max_pool2d(x_level_2, <span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">            level_2_resized = self.stride_level_2(level_2_downsampled_inter)</span><br><span class="line">        <span class="keyword">elif</span> self.level == <span class="number">1</span>:</span><br><span class="line">            level_0_compressed = self.compress_level_0(x_level_0)</span><br><span class="line">            sh = torch.tensor(level_0_compressed.shape[<span class="number">-2</span>:])*<span class="number">2</span></span><br><span class="line">            level_0_resized = F.interpolate(level_0_compressed, tuple(sh), <span class="string">'nearest'</span>)</span><br><span class="line">            level_1_resized = x_level_1</span><br><span class="line">            level_2_resized = self.stride_level_2(x_level_2)</span><br><span class="line">        <span class="keyword">elif</span> self.level == <span class="number">2</span>:</span><br><span class="line">            level_0_compressed = self.compress_level_0(x_level_0)</span><br><span class="line">            sh = torch.tensor(level_0_compressed.shape[<span class="number">-2</span>:])*<span class="number">4</span></span><br><span class="line">            level_0_resized = F.interpolate(level_0_compressed, tuple(sh), <span class="string">'nearest'</span>)</span><br><span class="line">            level_1_compressed = self.compress_level_1(x_level_1)</span><br><span class="line">            sh = torch.tensor(level_1_compressed.shape[<span class="number">-2</span>:])*<span class="number">2</span></span><br><span class="line">            level_1_resized = F.interpolate(level_1_compressed, tuple(sh),<span class="string">'nearest'</span>)</span><br><span class="line">            level_2_resized = x_level_2</span><br><span class="line">        <span class="comment"># è¿™é‡Œå¾—åˆ°çš„resizedç‰¹å¾å›¾ä¸ç›´æ¥è½¬æ¢æˆä¸€é€šé“çš„weighting mapï¼Œ</span></span><br><span class="line">        <span class="comment"># è€Œæ˜¯å…ˆ1x1convé™ç»´åˆ°8/16ï¼Œç„¶åconcatï¼Œç„¶å3x3ç”Ÿæˆ3é€šé“çš„weighting map</span></span><br><span class="line">        <span class="comment"># weighting mapç›¸å½“äºä¸€ä¸ªprediction headï¼Œæ‰€ä»¥æ˜¯conv_bias_softmaxï¼Œæ— bn</span></span><br><span class="line">        level_0_weight_v = self.weight_level_0(level_0_resized)</span><br><span class="line">        level_1_weight_v = self.weight_level_1(level_1_resized)</span><br><span class="line">        level_2_weight_v = self.weight_level_2(level_2_resized)</span><br><span class="line">        levels_weight_v = torch.cat((level_0_weight_v, level_1_weight_v, level_2_weight_v), <span class="number">1</span>)</span><br><span class="line">        levels_weight = self.weight_levels(levels_weight_v)</span><br><span class="line">        levels_weight = F.softmax(levels_weight, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># reweighting</span></span><br><span class="line">        fused_out_reduced = level_0_resized * levels_weight[:, <span class="number">0</span>:<span class="number">1</span>, :, :] + \</span><br><span class="line">                            level_1_resized * levels_weight[:, <span class="number">1</span>:<span class="number">2</span>, :, :] + \</span><br><span class="line">                            level_2_resized * levels_weight[:, <span class="number">2</span>:, :, :]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3x3çš„convï¼Œæ˜¯ç‰¹å¾å›¾å¹³æ»‘</span></span><br><span class="line">        out = self.expand(fused_out_reduced)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.vis:</span><br><span class="line">            <span class="keyword">return</span> out, levels_weight, fused_out_reduced.sum(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ç›®æ ‡æ£€æµ‹ï¼Œyolov3 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>VoVNet</title>
      <link href="/2021/01/22/VoVNet/"/>
      <url>/2021/01/22/VoVNet/</url>
      <content type="html"><![CDATA[<h2 id="An-Energy-and-GPU-Computation-Efficient-Backbone-Network-for-Real-Time-Object-Detection"><a href="#An-Energy-and-GPU-Computation-Efficient-Backbone-Network-for-Real-Time-Object-Detection" class="headerlink" title="An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection"></a>An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection</h2><ol><li><p>åŠ¨æœº</p><ul><li>denseNet<ul><li>dense pathï¼šdiverse receptive fields</li><li>heavy memory cost &amp; low efficiency </li></ul></li><li>we propose a backbone<ul><li>preserve the benefit of concatenation</li><li>improve denseNet efficiency</li><li>VoVNet comprised of One-Shot Aggregation (OSA)</li></ul></li><li>apply to one/two stage object detection tasks<ul><li>outperforms denseNet &amp; resNet based ones</li><li>better small object detection performance</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>main difference between resNet &amp; denseNet<ul><li>aggregationï¼šsummation &amp; concatenation <ul><li>summation would washed out the early features</li><li>concatenation last as it preserves</li></ul></li></ul></li><li>GPU parallel computation <ul><li>computing utilization is maximized when operand tensor is larger </li><li>many 1x1 convs for reducing dimension</li><li>dense connections in intermediate layers are inducing the inefficiencies </li></ul></li><li><p>VoVNet </p><ul><li>hypothesize that the dense connections are redundant</li><li>OSAï¼šaggregates intermediate features at once</li><li>test as object detection backboneï¼šoutperforms DenseNet &amp; ResNet with better energy efficiency and speed</li></ul></li><li><p>factors for efficiency</p><ul><li>FLOPS and model sizes are indirect metrics</li><li>energy per image and frame per second are more practical</li><li>MACï¼š<ul><li>memory accesses costï¼Œ$hw(c_i+c_o) + k^2 c_ic_o$</li><li>memory usageä¸æ­¢è·Ÿå‚æ•°é‡æœ‰å…³ï¼Œè¿˜è·Ÿç‰¹å¾å›¾å°ºå¯¸ç›¸å…³</li><li>MAC can be minimized when input channel size equals the output</li></ul></li><li>FLOPs/s<ul><li>splitting a large convolution operation into several fragmented smaller operations makes GPU computation inefficient as fewer computations are processed in parallel</li><li>æ‰€ä»¥depthwise/bottleneckç†è®ºä¸Šé™ä½äº†è®¡ç®—é‡FLOPï¼Œä½†æ˜¯ä»GPUå¹¶è¡Œçš„è§’åº¦efficiencyé™ä½ï¼Œå¹¶æ²¡æœ‰æ˜¾è‘—æé€Ÿï¼šcause more sequential computations</li><li>ä»¥æ—¶é—´ä¸ºå•ä½çš„FLOPsæ‰æ˜¯fairçš„</li></ul></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>hypothesize </p><ul><li>dense connection makes similar between neighbor layers</li><li>redundant</li></ul></li><li><p>OSA</p><ul><li>dense connectionï¼šformer features concats in every following features</li><li><p>one-shot connectionï¼šformer features concats once in the last feature</p><p><img src="/2021/01/22/VoVNet/OSA.png" width="40%;"></p></li><li><p>æœ€å¼€å§‹è·Ÿdense blockä¿æŒå‚æ•°ä¸€è‡´ï¼šä¸€ä¸ªblocké‡Œé¢12ä¸ªlayersï¼Œchannel20ï¼Œå‘ç°æ·±å±‚ç‰¹å¾contributes lessï¼Œæ‰€ä»¥æ¢æˆæµ…å±‚ï¼Œ5ä¸ªlayersï¼Œchannel43ï¼Œå‘ç°æœ‰æ¶¨ç‚¹ï¼šimplies that building deep intermediate feature via dense connection is less effective than expected </p><p><img src="/2021/01/22/VoVNet/weight_mat.png" width="40%;"></p></li><li><p>in/out channelæ•°ç›¸åŒ</p><ul><li>much less MACï¼š<ul><li>denseNet40ï¼š3.7M</li><li>OSAï¼š5layersï¼Œchannel43ï¼Œ2.5M</li><li>å¯¹äºhigher resolutionçš„detectionä»»åŠ¡impies more fast and energy efficient </li></ul></li><li>GPU efficiency <ul><li>ä¸éœ€è¦é‚£å¥½å‡ åä¸ª1x1</li></ul></li></ul></li></ul></li><li><p>architecture</p><ul><li>stemï¼š3ä¸ª3x3conv</li><li>downsampï¼šs2çš„maxpooling</li><li>stagesï¼šincreasing channels enables more rich semantic high-level informationï¼Œbetter feature representation</li><li><p>deeperï¼šmakes more modules in stage3/4</p><p><img src="/2021/01/22/VoVNet/VoVNet.png" width="80%;"></p></li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li>one-stageï¼šrefineDet</li><li>two-stageï¼šMask-RCNN</li></ul></li></ol>]]></content>
      
      
    </entry>
    
    <entry>
      <title>GCN</title>
      <link href="/2021/01/18/GCN/"/>
      <url>/2021/01/18/GCN/</url>
      <content type="html"><![CDATA[<p>referenceï¼š<a href="https://mp.weixin.qq.com/s/SWQHgogAP164Kr082YkF4A" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/SWQHgogAP164Kr082YkF4A</a></p><ol><li><p>å›¾</p><ul><li>$G = (V,E)$ï¼šèŠ‚ç‚¹ &amp; è¾¹ï¼Œè¿é€šå›¾ &amp; å­¤ç«‹ç‚¹</li><li>é‚»æ¥çŸ©é˜µAï¼šNxNï¼Œæœ‰å‘ &amp; æ— å‘</li><li>åº¦çŸ©é˜µDï¼šNxNå¯¹è§’çŸ©é˜µï¼Œæ¯ä¸ªèŠ‚ç‚¹è¿æ¥çš„èŠ‚ç‚¹</li><li>ç‰¹å¾çŸ©é˜µXï¼šNxFï¼Œæ¯ä¸ª1-dim Fæ˜¯æ¯ä¸ªèŠ‚ç‚¹çš„ç‰¹å¾å‘é‡</li></ul></li><li><p>ç‰¹å¾å­¦ä¹ </p><ul><li>å¯ä»¥ç±»æ¯”CNNï¼šå¯¹å…¶é‚»åŸŸï¼ˆkernelï¼‰å†…ç‰¹å¾è¿›è¡Œçº¿æ€§å˜æ¢ï¼ˆwåŠ æƒï¼‰ï¼Œç„¶åæ±‚å’Œï¼Œç„¶åæ¿€æ´»å‡½æ•°</li><li>$H^{k+1} = f(H^{k},A) = \sigma(AH^{k}W^{k})$<ul><li>Hï¼šrunning updating ç‰¹å¾çŸ©é˜µï¼ŒNxFk</li><li>Aï¼š0-1é‚»æ¥çŸ©é˜µï¼ŒNxN</li><li>Wï¼šæƒé‡ï¼Œ$F_k$x$F_{k+1}$</li></ul></li><li>æƒé‡æ‰€æœ‰èŠ‚ç‚¹å…±äº«</li><li>èŠ‚ç‚¹çš„é‚»æ¥èŠ‚ç‚¹å¯ä»¥çœ‹åšæ„Ÿå—é‡</li><li>ç½‘ç»œåŠ æ·±ï¼Œæ„Ÿå—é‡å¢å¤§ï¼šèŠ‚ç‚¹çš„ç‰¹å¾èåˆäº†æ›´å¤šèŠ‚ç‚¹çš„ä¿¡æ¯</li></ul></li><li><p>å›¾å·ç§¯</p><ul><li><p>Aä¸­æ²¡æœ‰è€ƒè™‘è‡ªå·±çš„ç‰¹å¾ï¼šæ·»åŠ è‡ªè¿æ¥</p><ul><li>A = A + I</li></ul></li><li><p>åŠ æ³•è§„åˆ™å¯¹åº¦å¤§çš„èŠ‚ç‚¹ï¼Œç‰¹å¾ä¼šè¶Šæ¥è¶Šå¤§ï¼šå½’ä¸€åŒ–</p><ul><li><p>ä½¿å¾—é‚»æ¥çŸ©é˜µæ¯è¡Œå’Œä¸º1ï¼šå·¦ä¹˜åº¦çŸ©é˜µçš„é€†</p></li><li><p>æ•°å­¦å®è´¨ï¼šæ±‚å¹³å‡</p></li><li><p>one step furtherï¼šä¸å•å¯¹è¡Œåšå¹³å‡ï¼Œå¯¹åº¦è¾ƒå¤§çš„é‚»æ¥èŠ‚ç‚¹ä¹Ÿåšpunish</p><p>  <img src="/2021/01/18/GCN/graph-conv-layer.png" width="60%;"></p></li></ul></li><li><p>GCNç½‘ç»œ</p><p>  <img src="/2021/01/18/GCN/gcn.png" width="60%;"></p></li></ul></li><li><p>å®ç°</p><ul><li><p>weightsï¼šin x outï¼Œkaiming_uniform_initialize</p></li><li><p>biasï¼šoutï¼Œzero_initialize</p></li><li><p>activationï¼šrelu</p></li><li><p>A x H x Wï¼šå·¦ä¹˜æ˜¯ç³»æ•°çŸ©é˜µä¹˜æ³•</p></li><li><p>é‚»æ¥çŸ©é˜µçš„ç»“æ„ä»è¾“å…¥å¼€å§‹å°±ä¸å˜äº†ï¼Œå’Œæ¯å±‚çš„ç‰¹å¾çŸ©é˜µä¸€èµ·ä½œä¸ºè¾“å…¥ï¼Œä¼ å…¥GCN</p></li><li><p>åˆ†ç±»å¤´ï¼šæœ€åä¸€å±‚é¢„æµ‹Nxn_classçš„ç‰¹å¾å‘é‡ï¼Œæå–æ„Ÿå…´è¶£èŠ‚ç‚¹F(n_class)ï¼Œç„¶åsoftmaxï¼Œå¯¹å…¶åˆ†ç±»</p></li><li><p>å½’ä¸€åŒ–</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># å¯¹ç§°å½’ä¸€åŒ–</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize_adj</span><span class="params">(adj)</span>:</span></span><br><span class="line">    <span class="string">"""compute L=D^-0.5 * (A+I) * D^-0.5"""</span></span><br><span class="line">    adj += sp.eye(adj.shape[<span class="number">0</span>])</span><br><span class="line">    degree = np.array(adj.sum(<span class="number">1</span>))</span><br><span class="line">    d_hat = sp.diags(np.power(degree, <span class="number">-0.5</span>).flatten())</span><br><span class="line">    norm_adj = d_hat.dot(adj).dot(d_hat)</span><br><span class="line">    <span class="keyword">return</span> norm_adj</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># å‡å€¼å½’ä¸€åŒ–</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize_adj</span><span class="params">(adj)</span>:</span></span><br><span class="line">    <span class="string">"""compute L=D^-1 * (A+I)"""</span></span><br><span class="line">    adj += sp.eye(adj.shape[<span class="number">0</span>])</span><br><span class="line">    degree = np.array(adj.sum(<span class="number">1</span>))</span><br><span class="line">    d_hat = sp.diags(np.power(degree, <span class="number">-1</span>).flatten())</span><br><span class="line">    norm_adj = d_hat.dot(adj)</span><br><span class="line">    <span class="keyword">return</span> norm_adj</span><br></pre></td></tr></table></figure></li></ul></li><li><p>åº”ç”¨åœºæ™¯</p><p> [åŠç›‘ç£ GCN]ï¼šSEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</p><p> [skin GCN]ï¼šLearning Differential Diagnosis of Skin Conditions with Co-occurrence Supervision using Graph Convolutional Networks</p></li></ol><h2 id="Learning-Differential-Diagnosis-of-Skin-Conditions-with-Co-occurrence-Supervision-using-Graph-Convolutional-Networks"><a href="#Learning-Differential-Diagnosis-of-Skin-Conditions-with-Co-occurrence-Supervision-using-Graph-Convolutional-Networks" class="headerlink" title="Learning Differential Diagnosis of Skin Conditions with Co-occurrence Supervision using Graph Convolutional Networks"></a>Learning Differential Diagnosis of Skin Conditions with Co-occurrence Supervision using Graph Convolutional Networks</h2><ol><li><p>åŠ¨æœº</p><ul><li>çš®è‚¤ç—…ï¼šå‘ç—…ç‡é«˜ï¼Œexpertså°‘</li><li>differential diagnosisï¼šé‰´åˆ«è¯Šæ–­ï¼Œå°±æ˜¯ä»ä¼—å¤šç–¾ç—…ç±»åˆ«ä¸­è·³å‡ºæ­£ç¡®ç±»åˆ«</li><li>timely and accurate</li><li>propose a DLS(deep learning system)<ul><li>clinical images</li><li>multi-label classification</li><li>80 conditions</li><li>incomplete image labels</li><li>GCN</li><li>Co-occurrence supervision</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>googleçš„DLS<ul><li>26ä¸­ç–¾ç—…</li><li>å»ºæ¨¡æˆmulti-class classification problemï¼šç ´åäº†ç±»åˆ«é—´çš„correlation</li></ul></li><li>our DLS<ul><li>multi-label classification task over 80 conditions</li><li>incomplete image labelsï¼šGCN that characterizes label co-occurrence supervision</li><li>combine the classification network with the GCN</li><li>æ•°æ®é‡ï¼š136,462 clinical images</li><li>ç²¾åº¦ï¼štest on 12,378 user taken imagesï¼Œtop-5 acc 93.6%</li></ul></li><li>GCN<ul><li>original applicationï¼š<ul><li>nodes classificationï¼Œonly a small subset of nodes had their labels available</li><li>the graph structure is contructed from data</li></ul></li><li>ML-GCNï¼š<ul><li>multi-label classification task</li><li>å›¾ç½‘ç»œæ¥æ”¶çš„è¾“å…¥æ˜¯CNNå­¦åˆ°çš„featureï¼Œè€Œä¸æ˜¯raw image</li></ul></li><li>graph structure<ul><li>èŠ‚ç‚¹ä»£è¡¨labels</li><li>è¾¹label_i â€”&gt; label_jè¡¨ç¤ºå½“label_iæ¿€æ´»çš„æ—¶å€™ï¼Œlabel_jä¹Ÿæ¿€æ´»</li></ul></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>overview of our GCN-CNN approach</p><ul><li>ä¸€ä¸ªtrainableçš„CNNï¼Œå°†å›¾ç‰‡è½¬åŒ–æˆfeature vector</li><li>ä¸€ä¸ªGCN branchï¼Œpropagates label co-occurrence and semantic embedding</li><li><p>ç„¶åä¸¤ä¸ªfeature vectorç›¸ä¹˜ï¼Œç»™å‡ºæœ€ç»ˆç»“æœ</p><p><img src="/2021/01/18/GCN/GCN-CNN.png" width="60%;"></p></li></ul></li><li><p>GCN branch </p><ul><li>two graph convolutional (GC) layers</li><li>use the k-th order filter of spectral graph convolution</li><li>GCN-1 takes order 1ï¼Œconvolves on direct neighbors</li><li>GCN-2 takes order 2ï¼Œconvolves on direct neighbors &amp; indirect neighbors</li><li>avoids over smoothing labe nodes</li><li>build co-occurence graph using only training data<ul><li>node embed semantic meaning to labels</li><li>è¾¹çš„å€¼å®šä¹‰æœ‰ç‚¹åƒç±»åˆ«é—´çš„ç›¸å…³æ€§å¼ºåº¦ï¼š$e_{ij} = 1(\frac{C(i,j)}{C(i)+C(j)} \geq t)$ï¼Œåˆ†å­æ˜¯æœ‰ä¸¤ç§æ ‡ç­¾çš„æ ·æœ¬é‡ï¼Œåˆ†æ¯æ˜¯å„è‡ªæ ·æœ¬é‡</li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> å›¾å·ç§¯ï¼Œgraph-conv </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>transformers</title>
      <link href="/2021/01/18/transformers/"/>
      <url>/2021/01/18/transformers/</url>
      <content type="html"><![CDATA[<h2 id="startup"><a href="#startup" class="headerlink" title="startup"></a>startup</h2><p>reference1ï¼š<a href="https://mp.weixin.qq.com/s/Rm899vLhmZ5eCjuy6mW_HA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/Rm899vLhmZ5eCjuy6mW_HA</a></p><p>reference2ï¼š<a href="https://zhuanlan.zhihu.com/p/308301901" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/308301901</a></p><ol><li><p>NLP &amp; RNN</p><ul><li>æ–‡æœ¬æ¶‰åŠä¸Šä¸‹æ–‡å…³ç³»</li><li>RNNæ—¶åºä¸²è¡Œï¼Œå»ºç«‹å‰åå…³ç³»</li><li>ç¼ºç‚¹ï¼šå¯¹è¶…é•¿ä¾èµ–å…³ç³»å¤±æ•ˆï¼Œä¸å¥½å¹¶è¡ŒåŒ–</li></ul></li><li><p>NLP &amp; CNN</p><ul><li>æ–‡æœ¬æ˜¯1ç»´æ—¶é—´åºåˆ—</li><li>1D CNNï¼Œå¹¶è¡Œè®¡ç®—</li><li>ç¼ºç‚¹ï¼šCNNæ“…é•¿å±€éƒ¨ä¿¡æ¯ï¼Œå·ç§¯æ ¸å°ºå¯¸å’Œé•¿è·ç¦»ä¾èµ–çš„balance</li></ul></li><li><p>NLP &amp; transformer</p><ul><li>å¯¹æµå…¥çš„æ¯ä¸ªå•è¯ï¼Œå»ºç«‹å…¶å¯¹è¯åº“çš„æƒé‡æ˜ å°„ï¼Œæƒé‡ä»£è¡¨attention</li><li>è‡ªæ³¨æ„åŠ›æœºåˆ¶</li><li><p>å»ºç«‹é•¿è·ç¦»ä¾èµ–</p><p><img src="/2021/01/18/transformers/transformer.png" width="30%;"></p></li></ul></li><li><p>put in CV</p><ul><li>æ’å…¥ç±»ä¼¼çš„è‡ªæ³¨æ„åŠ›å±‚</li><li>å®Œå…¨æŠ›å¼ƒå·ç§¯å±‚ï¼Œä½¿ç”¨Transformers</li></ul></li><li><p>RNN &amp; LSTM &amp; GRU cell</p><ul><li><p>æ ‡å‡†è¦ç´ ï¼šè¾“å…¥xã€è¾“å‡ºyã€éšå±‚çŠ¶æ€h</p></li><li><p>RNN</p><ul><li>RNN cellæ¯æ¬¡æ¥æ”¶ä¸€ä¸ªå½“å‰è¾“å…¥$x_t$ï¼Œå’Œå‰ä¸€æ­¥çš„éšå±‚è¾“å‡º$h_{t-1}$ï¼Œç„¶åäº§ç”Ÿä¸€ä¸ªæ–°çš„éšå±‚çŠ¶æ€$h_t$ï¼Œä¹Ÿæ˜¯å½“å‰çš„è¾“å‡º$y_t$</li><li>formulationï¼š$y_t, h_t = f(x_t, h_{t-1})$</li><li>same parameters for each time stepï¼šåŒä¸€ä¸ªcellæ¯ä¸ªtime stepçš„æƒé‡å…±äº«</li><li><p>ä¸€ä¸ªé—®é¢˜ï¼šæ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸</p><ul><li>è€ƒè™‘hidden statesâ€™ chainçš„ç®€åŒ–å½¢å¼ï¼š$h_t = \theta^t h_0$ï¼Œä¸€ä¸ªsequence forwardä¸‹å»å°±æ˜¯same weights multiplied over and over again</li><li>å¦å¤–tanhä¹Ÿæ˜¯ä¼šè®©ç¥ç»å…ƒæ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸</li></ul><p><img src="/2021/01/18/transformers/RNN.png" width="40%;"></p></li></ul></li><li><p>LSTM</p><ul><li><p>key ingredient</p><ul><li>cellï¼šå¢åŠ äº†ä¸€æ¡cell state workflowï¼Œä¼˜åŒ–æ¢¯åº¦æµ</li><li>gateï¼šé€šè¿‡é—¨ç»“æ„åˆ é€‰æºå¸¦ä¿¡æ¯ï¼Œä¼˜åŒ–é•¿è·ç¦»å…³è”</li></ul><p><img src="/2021/01/18/transformers/LSTM.png" width="25%;"></p></li><li><p>å¯ä»¥çœ‹åˆ°LSTMçš„å¾ªç¯çŠ¶æ€æœ‰ä¸¤ä¸ªï¼šç»†èƒçŠ¶æ€$c_t$å’Œéšå±‚çŠ¶æ€$h_t$ï¼Œè¾“å‡ºçš„$y_t$ä»æ—§æ˜¯$h_t$</p></li></ul></li><li><p>GRU</p><ul><li><p>LSTMçš„å˜ä½“ï¼Œä»æ—§æ˜¯é—¨ç»“æ„ï¼Œæ¯”LSTMç»“æ„ç®€å•ï¼Œå‚æ•°é‡å°ï¼Œæ®è¯´æ›´å¥½è®­ç»ƒ</p><p><img src="/2021/01/18/transformers/GRU.png" width="25%;"></p></li></ul></li></ul></li><li><p>papers</p><p>[ä¸€ä¸ªåˆ—äº†å¾ˆå¤šè®ºæ–‡çš„ä¸»é¡µ] <a href="https://github.com/dk-liang/Awesome-Visual-Transformer" target="_blank" rel="noopener">https://github.com/dk-liang/Awesome-Visual-Transformer</a></p><p>[ç»å…¸è€ƒå¤]</p><p>â€‹    * [Seq2Seq 2014] Sequence to Sequence Learning with Neural Networksï¼ŒGoogleï¼Œæœ€æ—©çš„encoder-decoder stacking LSTMç”¨äºæœºç¿»</p><p>â€‹    * [self-attention/Transformer 2017] Transformer: Attention Is All You Needï¼ŒGoogleï¼Œ</p><p>â€‹    * [bert 2019] BERT: Pre-training of Deep Bidirectional Transformers for Language Understandingï¼ŒGoogleï¼ŒNLPï¼Œè¾“å…¥single sentence/patched sentencesï¼Œç”¨Transformer encoderæå–bidirectional cross sentence representationï¼Œç”¨è¾“å‡ºçš„ç¬¬ä¸€ä¸ªlogitè¿›è¡Œåˆ†ç±»</p><p>[ç»¼è¿°]</p><p>â€‹    * [ç»¼è¿°2020] Efficient Transformers: A Surveyï¼ŒGoogleï¼Œ</p><p>â€‹    * [ç»¼è¿°2021] Transformers in Vision: A Surveyï¼Œè¿ªæ‹œï¼Œ</p><p>â€‹    * [ç»¼è¿°2021] A Survey on Visual Transformerï¼Œåä¸ºï¼Œ</p><p>[classification]</p><p>â€‹    * [ViT 2020] AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALEï¼ŒGoogleï¼Œåˆ†ç±»ä»»åŠ¡ï¼Œç”¨transformerçš„encoderæ›¿æ¢CNNå†åŠ åˆ†ç±»å¤´ï¼Œæ¯ä¸ªfeature patchä½œä¸ºä¸€ä¸ªinput embeddingï¼Œchannel dimæ˜¯vector dimï¼Œå¯ä»¥çœ‹åˆ°è·ŸbertåŸºæœ¬ä¸€æ ·ï¼Œå°±æ˜¯input sequenceæ¢æˆpatchï¼Œåç»­åŸºäºå®ƒçš„æå‡æœ‰DeiTã€LV-ViT</p><p>â€‹    * [BotNet 2021] Bottleneck Transformers for Visual Recognitionï¼ŒGoogleï¼Œå°†CNN backboneæœ€åå‡ ä¸ªstageæ›¿æ¢æˆMSA</p><p>â€‹    * [CvT 2021] CvT: Introducing Convolutions to Vision Transformersï¼Œå¾®è½¯ï¼Œ</p><p>[detection]</p><p>â€‹    * [DeTR 2020] DeTR: End-to-End Object Detection with Transformersï¼ŒFacebookï¼Œç›®æ ‡æ£€æµ‹ï¼ŒCNN+transformer(en-de)+é¢„æµ‹å¤´ï¼Œæ¯ä¸ªfeature pixelä½œä¸ºä¸€ä¸ªinput embeddingï¼Œchannel dimæ˜¯vector dim</p><p>â€‹    * [Swin 2021] Swin Transformer: Hierarchical Vision Transformer using Shifted Windowsï¼Œå¾®è½¯</p><p>[segmentation]</p><p>â€‹    * [SETR] Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformersï¼Œå¤æ—¦ï¼Œæ°´ï¼Œæ„Ÿè§‰å°±æ˜¯æŠŠFCNçš„backæ¢æˆtransformer</p><p>[Unet+Transformer]ï¼š</p><p>â€‹    * [UNETR 2021] UNETR: Transformers for 3D Medical Image Segmentationï¼Œè‹±ä¼Ÿè¾¾ï¼Œç›´æ¥ä½¿ç”¨transformer encoderåšunet encoder</p><p>â€‹    * [TransUNet 2021] TransUNet: Transformers Make Strong Encoders for Medical Image Segmentationï¼Œencoder streamé‡Œé¢åŠ transformer block</p><p>â€‹    * [TransFuse 2021] TransFuse: Fusing Transformers and CNNs for Medical Image Segmentationï¼Œå¤§å­¦ï¼ŒCNN featureå’ŒTransformer featureè¿›è¡Œbifusion</p></li></ol><h2 id="Sequence-to-Sequence"><a href="#Sequence-to-Sequence" class="headerlink" title="Sequence to Sequence"></a>Sequence to Sequence</h2><ol><li><p>[a keras tutorial][<a href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" target="_blank" rel="noopener">https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html</a>]</p><ul><li><p>general case</p><ul><li>extract the information of the entire input sequence</li><li>then start generate the output sequence</li></ul></li><li><p>seq2seq model workflow</p><ul><li>a (stacking of) RNN layer acts as encoder<ul><li>processes the input sequence</li><li>returns its own internal stateï¼šä¸è¦RNNçš„outputsï¼Œåªè¦internal states</li><li>encoderç¼–ç å¾—åˆ°çš„ä¸œè¥¿å«Context Vector</li></ul></li><li>a (stacking of) RNN layer acts as decoder<ul><li>given previous characters of the target sequence</li><li>it is trained to predict the next characters of the target sequence</li><li>teacher forcingï¼š<ul><li>è¾“å…¥æ˜¯target sequenceï¼Œè®­ç»ƒç›®æ ‡æ˜¯ä½¿æ¨¡å‹è¾“å‡ºoffset by one timestepçš„target sequence</li><li>ä¹Ÿå¯ä»¥ä¸teacher forcingï¼šç›´æ¥æŠŠé¢„æµ‹ä½œä¸ºnext stepçš„è¾“å…¥</li></ul></li><li>Context Vectorçš„åŒè´¨æ€§ï¼šæ¯ä¸ªstepï¼Œdecoderéƒ½è¯»å–ä¸€æ ·çš„Context Vectorä½œä¸ºinitial_state</li></ul></li><li>when inference<ul><li>ç¬¬ä¸€æ­¥è·å–input sequenceçš„state vectors</li><li>repeat<ul><li>ç»™decoderè¾“å…¥input stateså’Œout sequence(begin with a èµ·å§‹ç¬¦)</li><li>ä»predictionä¸­æ‹¿åˆ°next character</li></ul></li><li>append the character to the output sequence</li><li>untilï¼šå¾—åˆ°end character / hit the character limit</li></ul></li></ul></li><li><p>implementation</p><p>  <a href="https://github.com/AmberzzZZ/transformer/blob/master/seq2seq.py" target="_blank" rel="noopener">https://github.com/AmberzzZZ/transformer/blob/master/seq2seq.py</a></p></li></ul></li><li><p>one step further</p><ul><li>æ”¹è¿›æ–¹å‘<ul><li>bi-directional RNNï¼šç²—æš´åè½¬åºåˆ—ï¼Œæœ‰æ•ˆæ¶¨ç‚¹</li><li>attentionï¼šæœ¬è´¨æ˜¯å°†encoderçš„è¾“å‡ºContext VectoråŠ æƒ</li><li>ConvS2Sï¼šè¿˜æ²¡çœ‹</li></ul></li><li>ä¸»è¦éƒ½æ˜¯é’ˆå¯¹RNNçš„ç¼ºé™·æå‡º</li></ul></li><li><p>åŠ¨æœº</p><ul><li>present a general end-to-end sequence learning approach<ul><li>multi-layered LSTMs</li><li>encode the input seq to a fix-dim vector</li><li>decode the target seq from the fix-dim vector</li></ul></li><li>LSTM did not have difficulty on long sentences </li><li><p>reversing the order of the words improved performance</p><p><img src="/2021/01/18/transformers/seq2seq.png" width="65%;"></p></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>standard  RNN</p><ul><li><p>given a sequence $(x_1, x_2, â€¦, x_T)$</p></li><li><p>iteratingï¼š</p><script type="math/tex; mode=display">  h_t = sigm(W^{hx} x_t + W^{hh}h_{t-1})\\  y_t = W^{yh} h_t</script></li><li><p>å¦‚æœè¾“å…¥ã€è¾“å‡ºçš„é•¿åº¦äº‹å…ˆå·²çŸ¥ä¸”å›ºå®šï¼Œä¸€ä¸ªRNNç½‘ç»œå°±èƒ½å»ºæ¨¡seq2seq modeläº†</p></li><li><p>å¦‚æœè¾“å…¥ã€è¾“å‡ºçš„é•¿åº¦ä¸åŒã€å¹¶ä¸”æœä»ä¸€äº›æ›´å¤æ‚çš„å…³ç³»ï¼Ÿå°±å¾—ç”¨ä¸¤ä¸ªRNNç½‘ç»œï¼Œä¸€ä¸ªå°†input seqæ˜ å°„æˆfixed-sized vectorï¼Œå¦ä¸€ä¸ªå°†vectoræ˜ å°„æˆoutput seqï¼Œbut long-term-dependency issue</p></li></ul></li><li><p>LSTM</p><ul><li>LSTMæ˜¯å§‹ç»ˆå¸¦ç€å…¨éƒ¨seqçš„ä¿¡æ¯çš„ï¼Œå¦‚ä¸Šå›¾é‚£æ ·</li></ul></li><li><p>our actual model</p><ul><li>use two LSTMsï¼šencoder-decoderèƒ½å¤Ÿå¢åŠ å‚æ•°é‡</li><li>an LSTM with four layersï¼šdeeper</li><li>input sequenceå€’åºï¼šçœŸæ­£çš„å¥é¦–æ›´æ¥è¿‘transçš„å¥é¦–ï¼Œmakes it easy for SGD to establish communication</li></ul></li><li><p>training details</p><ul><li>LSTMï¼š4 layersï¼Œ1000 cells</li><li>word-embeddingï¼š1000-dimï¼Œ(input vocab 160,000, output vocab 80,000)</li><li>naive softmax</li><li>uniform initializationï¼š(-0.08, 0.08)</li><li>SGDï¼Œlr=0.7ï¼Œhalf by every half epochï¼Œtotal 7.5 epochs</li><li>gradient norm [10, 25]</li><li>all sentences in a minibatch are roughly of the same length</li></ul></li></ul></li></ol><h2 id="Transformer-Attention-Is-All-You-Need"><a href="#Transformer-Attention-Is-All-You-Need" class="headerlink" title="Transformer: Attention Is All You Need"></a>Transformer: Attention Is All You Need</h2><ol><li><p>åŠ¨æœº</p><ul><li>sequence2sequence models<ul><li>encoder + decoder</li><li>RNN / CNN + an attention path</li></ul></li><li>we propose Transformer<ul><li>base solely on attention mechanisms</li><li>more parallelizable and less training time</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>sequence modeling<ul><li>ä¸»æµï¼šRNNï¼ŒLSTMï¼Œgated<ul><li>align the positions to computing time steps</li><li>sequentialæœ¬è´¨é˜»ç¢å¹¶è¡ŒåŒ–</li></ul></li><li>Attention mechanisms acts as a integral part<ul><li>in previous work used in conjunction with the RNN</li></ul></li><li>ä¸ºäº†å¹¶è¡ŒåŒ–<ul><li>some methods use CNN as basic building blocks</li><li>difficult to learn dependencies between distant positions</li></ul></li></ul></li><li>we propose Transformer<ul><li>rely entirely on an attention mechanism</li><li>draw global dependencies</li></ul></li><li>self-attention<ul><li>relating different positions of a single sequence</li><li>to generate a overall representation of the sequence</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>encoder-decoder </p><ul><li>encoderï¼šdoc2emb<ul><li>given an input sequence of symbol representation $(x_1, x_2, â€¦, x_n)$</li><li>map to a sequence of continuous representations  $(z_1, z_2, â€¦, z_n)$ï¼Œ(embeddings)</li></ul></li><li>decoderï¼šhidden layers<ul><li>given embeddings z</li><li>generate an output sequence  $(y_1, y_2, â€¦, y_m)$ one element at a time</li><li>the previous generated symbols are served as additional input when computing the current time step</li></ul></li></ul></li><li><p>Transformer Architecture</p><ul><li><p>Transformer use </p><ul><li>for both encoder and decoder</li><li><p>stacked self-attention and point-wise fully-connected layers</p><p><img src="/2021/01/18/transformers/architecture.png" width="40%;"></p></li></ul></li><li><p>encoder</p><ul><li>N=6 identical layers</li><li>each layer has 2 sub-layers<ul><li>multi-head self-attention mechanism</li><li>postision-wise fully connected layer</li></ul></li><li>residual<ul><li>for two sub-layers independently</li><li>add &amp; layer norm</li></ul></li><li>d=512</li></ul></li><li><p>decoder</p><ul><li>N=6 identical layers</li><li>3 sub-layers<ul><li>[new] masked multi-head self-attentionï¼šcombineäº†å…ˆéªŒçŸ¥è¯†ï¼Œoutput embeddingåªèƒ½åŸºäºåœ¨å®ƒä¹‹å‰çš„time-stepçš„embeddingè®¡ç®—</li><li>multi-head self-attention mechanism</li><li>postision-wise fully connected layer</li></ul></li><li>residual</li></ul></li><li><p>attention</p><ul><li>referenceï¼š<a href="https://bbs.cvmart.net/articles/4032" target="_blank" rel="noopener">https://bbs.cvmart.net/articles/4032</a></li><li>step1ï¼šproject embedding to query-key-value pairs<ul><li>$Q = W_Q^{d<em>d} A^{d</em>N}$</li><li>$K = W_K^{d<em>d} A^{d</em>N}$</li><li>$V = W_V^{d<em>d} A^{d</em>N}$</li></ul></li><li>step2ï¼šscaled dot-product attention<ul><li>$A^{N*N}=softmax(K^TQ/\sqrt{d})$</li><li>$B^{d<em>N} = V^{d</em>N}A^{N*N}$</li></ul></li><li>multi-head attention <ul><li>ä»¥ä¸Šçš„step1&amp;step2æ“ä½œperforms a single attention function</li><li>äº‹å®ä¸Šæˆ‘ä»¬å¯ä»¥ç”¨å¤šç»„projectionå¾—åˆ°å¤šç»„$\{Q,K,V\}^h$ï¼Œin parallelåœ°æ‰§è¡Œattentionè¿ç®—ï¼Œå¾—åˆ°å¤šç»„$\{B^{d*N}\}^h$</li><li>concat &amp; project<ul><li>concat in d-dimï¼š$B\in R^{(d<em>h)</em>N}$</li><li>linear projectï¼š$out = W^{d<em>(d</em>h)} B$</li></ul></li><li>h=8</li><li>$d_{in}/h=64$ï¼šembeddingçš„dim</li><li>$d_{out}=64$ï¼šquery-key-valueçš„dim</li></ul></li></ul></li><li><p>positional encoding</p><ul><li><p>æ•°å­¦æœ¬è´¨æ˜¯ä¸€ä¸ªhand-craftedçš„æ˜ å°„çŸ©é˜µ$W^P$å’Œone-hotçš„ç¼–ç å‘é‡$p$ï¼š</p><script type="math/tex; mode=display">  \left[ \begin{array}{ccc}  a\\  e  \end{array}   \right ]  =  [W^I, W^P]   \left[ \begin{array}{ccc}  x\\  p  \end{array}   \right ]</script></li><li><p>ç”¨PEè¡¨ç¤ºe</p><p>  <img src="/2021/01/18/transformers/PE.png" width="35%;"></p><ul><li>posæ˜¯sequence xä¸Šçš„position</li><li>2iå’Œ2i+1æ˜¯embedding aä¸Šçš„idx</li></ul></li></ul></li><li><p>point-wise feed-forward network</p><ul><li>fc-ReLU-fc</li><li>dim_fc=2048</li><li>dim_in &amp; dim_out = 512</li></ul></li></ul></li><li><p>è¿è¡Œè¿‡ç¨‹</p><ul><li><p>encoderæ˜¯å¯ä»¥å¹¶è¡Œè®¡ç®—çš„</p><ul><li>è¾“å…¥æ˜¯sequence embeddingå’Œpositional embeddingï¼š$A\in R^{d*N}$</li><li>ç»è¿‡repeated blocks</li><li>è¾“å‡ºæ˜¯å¦å¤–ä¸€ä¸ªsequenceï¼š$B\in R^{d*N}$</li><li>self-attentionï¼šQã€Kã€Væ˜¯ä¸€ä¸ªä¸œè¥¿</li><li>encoderçš„æœ¬è´¨å°±æ˜¯åœ¨è§£æè‡ªæ³¨æ„åŠ›ï¼š</li><li>å¹¶è¡Œçš„å…¨å±€ä¸¤ä¸¤æ¯”è¾ƒï¼Œä¸€æ­¥åˆ°ä½<ul><li>RNNè¦by step</li></ul></li><li>CNNè¦stack layers</li></ul></li></ul></li></ul></li></ol><ul><li><p>decoderæ˜¯åœ¨è®­ç»ƒé˜¶æ®µæ˜¯å¯ä»¥å¹¶è¡Œçš„ï¼Œåœ¨inferenceé˜¶æ®µby step</p><ul><li><p>è¾“å…¥æ˜¯encoderçš„è¾“å‡ºå’Œä¸Šä¸€ä¸ªtime-step decoderçš„è¾“å‡ºembedding</p></li><li><p>è¾“å‡ºæ˜¯å½“å‰time-stepå¯¹åº”positionçš„è¾“å‡ºè¯çš„æ¦‚ç‡</p></li><li><p>ç¬¬ä¸€ä¸ªattention layeræ˜¯out embeddingçš„self-attentionï¼šè¦å®ç°åƒRNNä¸€æ ·ä¾æ¬¡è§£ç å‡ºæ¥ï¼Œæ¯ä¸ªtime stepè¦ç”¨åˆ°ä¸Šä¸€ä¸ªä½ç½®çš„è¾“å‡ºä½œä¸ºè¾“å…¥â€”â€”masking</p><ul><li>givenè¾“å…¥sequenceæ˜¯\ I have a catï¼Œ5ä¸ªå…ƒç´ </li><li>é‚£ä¹ˆmaskå°±æ˜¯$R^{5*5}$çš„ä¸‹ä¸‰è§’çŸ©é˜µ</li><li><p>è¾“å…¥embeddingç»è¿‡transformationå˜æˆQã€Kã€Vä¸‰ä¸ªçŸ©é˜µ</p></li><li><p>ä»æ—§æ˜¯$A=K^TQ$è®¡ç®—attention</p></li><li><p>è¿™é‡Œæœ‰ä¸€äº›attentionæ˜¯éæ³•çš„ï¼šä½ç½®é å‰çš„queryåªèƒ½ç”¨åˆ°æ¯”ä»–ä½ç½®æ›´é å‰çš„queryï¼Œå› æ­¤è¦ä¹˜ä¸ŠmaskçŸ©é˜µï¼š$A=M A$</p></li><li><p>softmaxï¼š$A=softmax(A)$</p></li><li><p>scaleï¼š$B = VA$</p></li><li><p>concat &amp; projection</p><p><img src="/2021/01/18/transformers/mask.png" width="60%;"></p><ul><li>ç¬¬äºŒä¸ªattention layeræ˜¯in &amp; out sequenceçš„æ³¨æ„åŠ›ï¼Œå…¶keyå’Œvalueæ¥è‡ªencoderï¼Œqueryæ¥è‡ªä¸Šä¸€ä¸ªdecoder blockçš„è¾“å‡º</li></ul></li></ul><p><img src="/2021/01/18/transformers/flow.jpeg" width="45%;"></p></li></ul></li></ul><ol><li><p>why self-attention</p><ul><li>è¡¡é‡ç»´åº¦<ul><li>total computational complexity per layer</li><li>amount of computation that can be parallelized</li><li>path-length between long-range dependencies</li></ul></li><li>given input sequence with length N &amp; dim $d_{in}$ï¼Œoutput sequence with dim $d_{out}$<ul><li>RNN need N sequencial operations of $W\in R^{d_{in} * d_{out}}$</li><li>CNN need N/k stacking layers of $d_{in}<em>d_{out}$ sequence operations of $W\in R^{k</em>k}$ï¼Œgenerallyæ˜¯RNNçš„kå€</li></ul></li></ul></li><li><p>training</p><ul><li>optimizerï¼š$Adam(lr, \beta_1=0.9, \beta_2=0.98, \epsilon=10^{-9})$</li><li>lrscheduleï¼šwarmup by 4000 stepsï¼Œthen decay</li><li><p>dropout</p><ul><li>residual dropoutï¼šå°±æ˜¯stochastic depth</li><li>dropout to the sum of embeddings &amp; PE for both encoder and decoder</li><li>drop_rate = 0.1</li></ul></li><li><p>label smoothingï¼šsmooth_factor = 0.1</p></li></ul></li><li><p>å®éªŒ</p><ul><li>Aï¼švary the number of attention headsï¼Œå‘ç°å¤šäº†å°‘äº†éƒ½hurts</li><li>Bï¼šreduce the dim of attention keyï¼Œå‘ç°hurts</li><li>C &amp; Dï¼šå¤§æ¨¡å‹+dropout helps</li><li>Eï¼šlearnable &amp; sincos PEï¼šnearly identical</li><li><p>æœ€åæ˜¯big modelçš„å‚æ•°</p><p><img src="/2021/01/18/transformers/transformer_arch.png" width="65%;"></p></li></ul></li></ol><h2 id="BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding"><a href="#BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding" class="headerlink" title="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"></a>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</h2><ol><li><p>åŠ¨æœº</p><ul><li>BERTï¼šBidirectional Encoder Representations from Transformers<ul><li>Bidirectional</li><li>Encoder</li><li>Representations</li><li>Transformers</li></ul></li><li>workflow<ul><li>pretrain bidirectional representations from unlabeled text</li><li>tune with one additional output layer to obtain the model</li></ul></li><li>SOTA<ul><li>GLUE score 80.5%</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>pretraining is effective in NLP tasks<ul><li>feature-based methodï¼šuse task-specfic architecturesï¼Œä»…ä½¿ç”¨pretrained modelçš„ç‰¹å¾</li><li>fine-tuining methodï¼šç›´æ¥fine-tuneé¢„è®­ç»ƒæ¨¡å‹</li><li>ä¸¤ç§æ–¹æ³•åœ¨é¢„è®­ç»ƒé˜¶æ®µè®­ç»ƒç›®æ ‡ä¸€è‡´ï¼šuse unidirectional language models to learn general language representations</li><li>reduce the need for many heavily-engineered task- specific architectures</li></ul></li><li>current methodsâ€™ limitations<ul><li>unidirectionalï¼š<ul><li>limit the choice of architectures</li><li>äº‹å®ä¸Štokençš„ä¸Šä¸‹æ–‡éƒ½å¾ˆé‡è¦ï¼Œä¸èƒ½åªçœ‹ä¸Šæ–‡</li></ul></li><li>ç®€å•çš„concatä¸¤ä¸ªindependentçš„L2Rå’ŒR2Læ¨¡å‹ï¼ˆbiRNNï¼‰<ul><li>independent</li><li>shallow concat</li></ul></li></ul></li><li>BERT<ul><li>masked language modelï¼šåœ¨ä¸€ä¸ªsequenceä¸­é¢„æµ‹è¢«é®æŒ¡çš„è¯</li><li>next sentence predictionï¼štrains text-pair representations</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>two steps</p><ul><li>pre-training<ul><li>unlabeled data</li><li>different pretraining tasks</li></ul></li><li>fine-tuning<ul><li>labeled data of the downstream tasks</li><li>fine-tune all the params</li></ul></li><li><p>ä¸¤ä¸ªé˜¶æ®µçš„æ¨¡å‹ï¼Œåªæœ‰è¾“å‡ºå±‚ä¸åŒ</p><ul><li>ä¾‹å¦‚é—®ç­”æ¨¡å‹</li><li>pretrainingé˜¶æ®µï¼Œè¾“å…¥æ˜¯ä¸¤ä¸ªsentenceï¼Œè¾“å…¥çš„èµ·å§‹æœ‰ä¸€ä¸ªCLS symbolï¼Œä¸¤ä¸ªå¥å­çš„åˆ†éš”æœ‰ä¸€ä¸ªSEP symbol</li><li>fine-tuningé˜¶æ®µï¼Œè¾“å…¥åˆ†åˆ«æ˜¯é—®å’Œç­”ï¼Œã€è¾“å‡ºæ˜¯å•¥ï¼Ÿã€‘</li></ul><p><img src="/2021/01/18/transformers/bert.png" width="65%;"></p></li></ul></li><li><p>architecture</p><ul><li><p>multi-layer bidirectional Transformer encoder</p><ul><li>number of transfomer blocks L</li><li>hidden size H</li><li>number of self-attention heads A</li><li>FFN dim 4H</li></ul></li><li><p>Bert baseï¼šL=12ï¼ŒH=768ï¼ŒA=12</p></li><li><p>Bert largeï¼šL=24ï¼ŒH=1024ï¼ŒA=16</p></li><li><p>input/output representations</p><ul><li>a single sentence / two packed up sentenceï¼š<ul><li>æ‹¼æ¥çš„sentenceç”¨ç‰¹æ®Štoken SEPè¡”æ¥</li><li>segment embeddingï¼šåŒæ—¶add a learned embedding to every token indicating who it belongs</li></ul></li><li>use WordPiece embeddings with 30000 token vocabulary</li><li>è¾“å…¥sequenceçš„ç¬¬ä¸€ä¸ªtokenæ°¸è¿œæ˜¯ä¸€ä¸ªç‰¹æ®Šç¬¦å·CLSï¼Œå®ƒå¯¹åº”çš„final stateè¾“å‡ºä½œä¸ºsentenceæ•´ä½“çš„representationï¼Œç”¨äºåˆ†ç±»ä»»åŠ¡</li><li><p>overallç½‘ç»œçš„input representationæ˜¯é€šè¿‡å°†token embeddingsæ‹¼æ¥ä¸Šä¸Šç‰¹æ®Šç¬¦å·ï¼ŒåŠ ä¸ŠSEå’ŒPEå¾—åˆ°</p><p><img src="/2021/01/18/transformers/bert input.png" width="75%;"></p></li></ul></li></ul></li><li><p>pre-training</p><ul><li>two unsupervised tasks<ul><li>Masked LM (MLM)<ul><li>mask some percentage of the input tokens at randomï¼š15%<ul><li>80%çš„æ¦‚ç‡ç”¨MASK tokenæ›¿æ¢</li><li>10%çš„æ¦‚ç‡ç”¨random tokenæ›¿æ¢</li><li>10%çš„æ¦‚ç‡unchanged</li></ul></li><li>then predict those masked tokens</li><li>the final hidden states corresponding to the masked tokens are fed into a softmax</li><li>ç›¸æ¯”è¾ƒäºä¼ ç»Ÿçš„left2right/right2left/concatæ¨¡å‹<ul><li>æ—¢æœ‰å‰æ–‡åˆæœ‰åæ–‡</li><li>åªé¢„æµ‹masked tokenï¼Œè€Œä¸æ˜¯å…¨å¥é¢„æµ‹</li></ul></li></ul></li><li>Next Sentence Prediction (NSP)<ul><li>å¯¹äºrelationship between sentencesï¼š<ul><li>ä¾‹å¦‚question&amp;answerï¼Œå¥å­æ¨æ–­</li><li>not direatly captured by language modelingï¼Œæ¨¡å‹ç›´è§‚å­¦ä¹ çš„æ˜¯token relationship</li></ul></li><li>binarized next sentence prediction task<ul><li>é€‰å–sentence A&amp;Bï¼š<ul><li>50%çš„æ¦‚ç‡æ˜¯çœŸçš„ä¸Šä¸‹æ–‡ï¼ˆIsNextï¼‰</li><li>50%çš„æ¦‚ç‡æ˜¯randomï¼ˆNotNextï¼‰</li></ul></li><li>æ„æˆäº†ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ï¼šä»æ—§ç”¨CLS tokenå¯¹åº”çš„hidden state Cæ¥é¢„æµ‹</li></ul></li></ul></li></ul></li></ul></li><li><p>fine-tuning</p><ul><li>BERTå…¼å®¹many downstream tasksï¼šsingle text or text pairs</li><li>ç›´æ¥ç»„å¥½è¾“å…¥ï¼Œend-to-end fine-tuningå°±è¡Œ</li><li>è¾“å‡ºè¿˜æ˜¯ç”¨CLS tokenå¯¹åº”çš„hidden state Cæ¥é¢„æµ‹ï¼Œæ¥åˆ†ç±»å¤´</li></ul></li></ul></li></ol><h2 id="A-Survey-on-Visual-Transformer"><a href="#A-Survey-on-Visual-Transformer" class="headerlink" title="A Survey on Visual Transformer"></a>A Survey on Visual Transformer</h2><ol><li><p>åŠ¨æœº</p><ul><li>provide a comprehensive overview of the recent advances in visual transformers </li><li>discuss the potential directions for further improvement</li><li><p>develop timeline</p><p><img src="/2021/01/18/transformers/timeline.png" width="75%;"></p></li></ul></li><li><p>æŒ‰ç…§åº”ç”¨åœºæ™¯åˆ†ç±»</p><ul><li>backboneï¼šåˆ†ç±»</li><li>high/mid-level visionï¼šé€šå¸¸æ˜¯è¯­ä¹‰ç›¸å…³çš„ï¼Œæ£€æµ‹/åˆ†å‰²/å§¿æ€ä¼°è®¡</li><li>low-level visionï¼šå¯¹å›¾åƒæœ¬èº«è¿›è¡Œæ“ä½œï¼Œè¶…åˆ†/å›¾åƒç”Ÿæˆï¼Œç›®å‰åº”ç”¨è¾ƒå°‘</li><li><p>video processing</p><p><img src="/2021/01/18/transformers/category.png" width="85%;"></p></li></ul></li><li><p>revisiting transformer</p><ul><li><p>key-conceptsï¼šsentenceã€embeddingã€positional encodingã€encoderã€decoderã€self-attention layerã€encoder-decoder attention layerã€multi-head attentionã€feed-forward neural network</p><p><img src="/2021/01/18/transformers/revisit transformer.png" width="45%;"></p></li><li><p>self-attention layer</p><ul><li>input vector is transformed into 3 vectors<ul><li>input vector is embedding+PE(pos,i)ï¼šposæ˜¯wordåœ¨sequenceä¸­çš„ä½ç½®ï¼Œiæ˜¯PE-elementåœ¨embedding vecä¸­çš„ä½ç½®</li><li>query vec q</li><li>key vec k</li><li>value vec v</li><li>$d_q = d_k = d_v = d_{model} = 512$</li></ul></li><li>then calculateï¼š$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$</li><li>encoder-decoder attention layer<ul><li>Kå’ŒVæ˜¯ä»encoderä¸­æ‹¿åˆ°</li><li>Qæ˜¯ä»å‰ä¸€å±‚æ‹¿åˆ°</li><li>è®¡ç®—æ˜¯ç›¸ä¼¼çš„</li></ul></li></ul></li><li>multi-head attention<ul><li>ä¸€ä¸ªattentionæ˜¯ä¸€ä¸ªsoftmaxï¼Œå¯¹åº”äº†ä¸€å¯¹å¼ºç›¸å…³ï¼ŒåŒæ—¶æŠ‘åˆ¶äº†å…¶ä»–wordçš„ç›¸å…³æ€§</li><li>è€ƒè™‘ä¸€ä¸ªè¯å¾€å¾€ä¸å‡ ä¸ªè¯å¼ºç›¸å…³ï¼Œè¿™å°±éœ€è¦å¤šä¸ªattention</li><li>multi-headï¼šdifferent QKV matrices are used for different heads</li><li>given a input vectorï¼Œthe number of heads h<ul><li>å…ˆäº§ç”Ÿhä¸ª<q,k,v> pairs</q,k,v></li><li>$d_q=d_k=d_v=d_{model}/h=64$</li><li>è¿™hä¸ªpairï¼Œåˆ†åˆ«è®¡ç®—attention vectorï¼Œå¾—åˆ°hä¸ª[b,d]çš„context vector</li><li>concat along-d-axis and linear projection to final [b,d] vector</li></ul></li></ul></li><li>residual &amp; layer-normï¼šlayer-normåœ¨residual-addä»¥å</li><li>feed-forward network<ul><li>fc-GeLU-fc</li><li>$d_h=2048$</li></ul></li><li>final-layer in decoder<ul><li>dense+softmax</li><li>$d_{words}=$ number of words in the vocabulary </li></ul></li><li>when applied in CV tasks<ul><li>most transformers adopt the original transformerâ€™s encoder module</li><li>used as a feature selector</li><li>ç›¸æ¯”è¾ƒäºCNNï¼Œèƒ½å¤Ÿcapture long-distance characteristicsï¼Œderive global information</li><li>ç›¸æ¯”è¾ƒäºRNNï¼Œèƒ½å¤Ÿå¹¶è¡Œè®¡ç®—</li></ul></li><li>è®¡ç®—é‡<ul><li>é¦–å…ˆæ˜¯ä¸‰ä¸ªçº¿æ€§å±‚ï¼šçº¿æ€§æ—¶é—´å¤æ‚åº¦O(n)ï¼Œè®¡ç®—é‡ä¸$d_{model}$æˆæ­£æ¯”</li><li>ç„¶åæ˜¯self-attentionå±‚ï¼šQKVçŸ©é˜µä¹˜æ³•è¿ç®—ï¼Œå¹³æ–¹æ—¶é—´å¤æ‚åº¦O(n^2)</li><li>multi-headçš„è¯ï¼Œè¿˜æœ‰ä¸€ä¸ªçº¿æ€§å±‚ï¼šå¹³æ–¹æ—¶é—´å¤æ‚åº¦O(n^2)</li></ul></li></ul></li><li><p>revisiting transformers for NLP</p><ul><li>æœ€æ—©æœŸçš„RNN + attentionï¼šrnnçš„sequentialæœ¬è´¨å½±å“äº†é•¿è·ç¦»/å¹¶è¡ŒåŒ–/å¤§æ¨¡å‹</li><li><p>transformerçš„solely attentionç»“æ„ï¼šè§£å†³ä»¥ä¸Šé—®é¢˜ï¼Œä¿ƒè¿›äº†large pre-trained models (PTMs) for NLP</p></li><li><p>BERT and its variants </p><ul><li>are a series of PTMs built on the multi-layer transformer encoder architecture </li><li>pre-trained<ul><li>Masked language modeling </li><li>Next sentence prediction </li></ul></li><li>fine-tuned<ul><li>add an output layer</li></ul></li></ul></li><li>Generative Pre-trained Transformer models (GPT)<ul><li>are another type of PTMs based on the transformer decoder architecture</li><li>masked self-attention mechanisms </li><li>pre-trained<ul><li>ä¸BERTæœ€å¤§çš„ä¸åŒæ˜¯æœ‰å‘æ€§</li></ul></li></ul></li></ul></li><li><p>visual transformer</p><ul><li><p>ã€category1ã€‘: backbone for image classification</p><ul><li>transformerçš„è¾“å…¥æ˜¯tokensï¼Œåœ¨NLPé‡Œæ˜¯embeddingå½¢å¼çš„åˆ†è¯åºåˆ—ï¼Œåœ¨CVé‡Œå°±æ˜¯representing a certain semantic conceptçš„visual token<ul><li>visual tokenå¯ä»¥æ¥è‡ªCNNçš„feature</li><li>ä¹Ÿå¯ä»¥ç›´æ¥æ¥è‡ªimageçš„å°patch</li></ul></li><li><p>purely use transformeræ¥åšimage classificationä»»åŠ¡çš„æ¨¡å‹æœ‰iGPTã€ViTã€DeiT</p></li><li><p>iGPT</p><ul><li>pretraining stage + finetuning stage</li><li>pre-training stage<ul><li>self-supervisedï¼šè‡ªç›‘ç£ï¼Œæ‰€ä»¥ç»“æœè¾ƒå·®</li><li>given an unlabeled dataset</li><li>train the model by minimizing the -log(density)ï¼Œæ„Ÿè§‰æ˜¯åœ¨forceå…‰æ …æ’åºæ­£ç¡®</li></ul></li><li>fine-tuning stage<ul><li>average pool + fc + softmax</li><li>jointly train with L_gen &amp; L_CE</li></ul></li></ul></li><li>ViT<ul><li>pre-trained on large datasets<ul><li>standard transformerâ€™s encoder + MLP head</li><li>treats all patches equally</li><li>æœ‰ä¸€ä¸ªç±»ä¼¼BERT class tokençš„ä¸œè¥¿<ul><li>ä»è®­ç»ƒçš„è§’åº¦ï¼Œgather knowledge of the entire class</li><li>inferenceçš„æ—¶å€™ï¼Œåªæ‹¿äº†è¿™ç¬¬ä¸€ä¸ªlogitç”¨æ¥åšé¢„æµ‹</li></ul></li></ul></li><li>fine-tuning<ul><li>æ¢ä¸€ä¸ªzero-initializedçš„MLP head</li><li>use higher resolution &amp; æ’å€¼pe</li></ul></li></ul></li><li>DeiT<ul><li>Data-efficient image transformer </li><li>better performance with <ul><li>a more cautious training strategy </li><li>and a token-based distillation</li></ul></li></ul></li></ul></li><li><p>ã€category2ã€‘: High/Mid-level Vision </p></li><li><p>ã€category3ã€‘: Low-level Vision </p></li><li><p>ã€category4ã€‘: Video Processing </p></li><li><p>efficient transformerï¼šç˜¦èº«&amp;åŠ é€Ÿ</p><ul><li>Pruning and Decomposition </li><li>Knowledge Distillation </li><li>Quantization </li><li><p>Compact Architecture Design </p><p><img src="/2021/01/18/transformers/efficient.png" width="45%;"></p></li></ul></li></ul></li></ol><h2 id="ViT-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE"><a href="#ViT-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE" class="headerlink" title="ViT: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"></a>ViT: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</h2><ol><li><p>åŠ¨æœº</p><ul><li>attention in vision<ul><li>either in conjunction with CNN</li><li>or replace certain part of a CNN</li><li>overalléƒ½è¿˜æ˜¯CNN-based</li></ul></li><li>use a pure transformer to sequence of image patches</li><li>verified on image classification tasks in supervised fashion</li></ul></li><li><p>è®ºç‚¹</p><ul><li>transformer lack some inductive biases inherent to CNNsï¼Œæ‰€ä»¥åœ¨insufficient dataä¸Šnot generalize well</li><li>however large scale training trumps inductive biasï¼Œå¤§æ•°æ®é›†ä¸ŠViTæ›´å¥½</li><li>naive application of self-attention<ul><li>å»ºç«‹pixelä¹‹é—´çš„ä¸¤ä¸¤å…³è”ï¼šè®¡ç®—é‡å¤ªå¤§äº†</li><li>éœ€è¦approximationï¼šlocal/æ”¹å˜size</li></ul></li><li>we use transformer<ul><li>wih global self-attention</li><li>to full-sized images</li></ul></li></ul></li><li><p>æ–¹æ³•</p><p> <img src="/2021/01/18/transformers/ViT.png" width="65%;"></p><ul><li><p>input 1D-embedding sequence</p><ul><li>å°†image $x\in R^{HWC}$ å±•å¼€æˆpatches $\{x_p \in R^{P^2C}\}$</li><li>thus sequence length $N=HW/P^2$</li><li>patch embeddingï¼š<ul><li>use a trainable linear projection</li><li>fixed dimension size through-all</li></ul></li><li>position embeddingï¼š<ul><li>add to patch embedding</li><li>standard learnable 1D position embedding</li></ul></li><li>prepended embeddingï¼š<ul><li>å‰ç½®çš„learnable embedding $x_{class}$</li><li>similar to BERTâ€™s class token</li></ul></li><li>ä»¥ä¸Šä¸‰ä¸ªembeddingç»„åˆèµ·æ¥ï¼Œä½œä¸ºè¾“å…¥sequence</li></ul></li><li><p>transformer encoder</p><ul><li>follow the original Transformer </li><li>äº¤æ›¿çš„MSAå’ŒMLP</li><li>layer norm LN</li><li>residual</li><li><p>GELU</p><p><img src="/2021/01/18/transformers/ViT formulation.png" width="70%;"></p></li></ul></li><li><p>hybrid architecture</p><ul><li>input sequenceä¹Ÿå¯ä»¥æ¥æºäºCNNçš„feature maps</li><li>patch sizeå¯ä»¥æ˜¯1x1</li></ul></li><li><p>classification head</p><ul><li>attached to $z_L^0$ï¼šæ˜¯class tokenç”¨æ¥åšé¢„æµ‹</li><li>pre-trainingçš„æ—¶å€™æ˜¯MLP</li><li>fine-tuningçš„æ—¶å€™æ¢ä¸€ä¸ªzero-initializedçš„single linear layer</li></ul></li><li><p>workflow</p><ul><li>typicallyå…ˆpre-train on large datasets</li><li>å†fine-tune to downstream tasks</li><li>fine-tuneçš„æ—¶å€™æ›¿æ¢ä¸€ä¸ªzero-initializedçš„æ–°çº¿æ€§åˆ†ç±»å¤´</li><li>when feeding images with higher resolution<ul><li>keep the patch size</li><li>results in larger sequence length</li><li>è¿™æ—¶å€™pre-trained PEå°±no longer meaningfuläº†</li><li>we therefore perform 2D interpolationåŸºäºå®ƒåœ¨åŸå›¾ä¸Šçš„ä½ç½®</li></ul></li></ul></li><li><p>training details</p><ul><li>Adamï¼š$\beta_1=0.9ï¼Œ\beta_2=0.999$</li><li>batch size 4096</li><li>high weight decay 0.1</li><li>linear lr warmup &amp; decay</li></ul></li><li><p>fine-tuning details</p><ul><li>SGDM</li><li>cosine LR</li><li>no weight decay</li><li>ã€ï¼Ÿï¼Ÿï¼Ÿï¼Ÿã€‘average 0.9999</li></ul></li></ul></li></ol><h2 id="Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows"><a href="#Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows" class="headerlink" title="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"></a>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</h2><ol><li><p>åŠ¨æœº</p><ul><li>use Transformer as visual tasksâ€™ backbone</li><li>challenges of Transformer in vision domain<ul><li>large variations of scales of the visual entities</li><li>high resolution of pixels</li></ul></li><li>we propose hierarchical Transformer<ul><li>shifted windows</li><li>self-attention in local windows</li><li>cross-window connection</li></ul></li><li>verified on<ul><li>classificationï¼šImageNet top1 acc 86.4</li><li>detectionï¼šCOCO box-MAP 58.7</li><li>segmentationï¼šADE20K</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>when transfer Transformerâ€™s high performance in NLP domain to CV domain<ul><li>differences between the two modalities <ul><li>scaleï¼šNLPé‡Œé¢ï¼Œword tokens serves as the basic elementï¼Œä½†æ˜¯CVé‡Œé¢ï¼Œpatchçš„å½¢æ€å¤§å°éƒ½æ˜¯å¯å˜çš„ï¼Œprevious methodsé‡Œé¢ï¼Œéƒ½æ˜¯ç»Ÿä¸€è®¾å®šå›ºå®šå¤§å°çš„patch token</li><li>resolutionï¼šä¸»è¦é—®é¢˜å°±æ˜¯self-attentionçš„è®¡ç®—å¤æ‚åº¦ï¼Œæ˜¯image sizeçš„å¹³æ–¹</li></ul></li><li>we propose Swin Transformer<ul><li>hierarchial feature maps</li><li>linear computatoinal complexity to image size</li></ul></li></ul></li><li>hierarchical<ul><li>start from small patches</li><li>merge in deeper layers</li><li>æ‰€ä»¥å¯¹ä¸åŒå°ºåº¦çš„ç‰¹å¾patchè¿›è¡Œäº†èåˆ</li></ul></li><li><p>linear complexity</p><ul><li>compute self-attention locally in each window</li><li>æ¯ä¸ªwindowçš„number of patchesæ˜¯è®¾å®šå¥½çš„ï¼Œwindowæ•°æ˜¯ä¸image sizeæˆæ­£æ¯”çš„</li><li>æ‰€ä»¥æ˜¯çº¿æ€§</li></ul><p><img src="/2021/01/18/transformers/hierarchical.png" width="45%;"></p></li><li><p>shifted window approach</p><ul><li>è·¨å±‚çš„window shiftï¼Œå»ºç«‹èµ·ç›¸é‚»windowé—´çš„æ¡¥æ¢</li><li><p>ã€QUESTIONã€‘all query patches within a window share the same key set</p><p><img src="/2021/01/18/transformers/shifted window.png" width="45%;"></p></li></ul></li><li><p>previous attemptations of Transformer</p><ul><li>self-attention based backbone architectures<ul><li>å°†éƒ¨åˆ†/å…¨éƒ¨conv layersæ›¿æ¢æˆself-attention</li><li>æ¨¡å‹ä¸»ä½“æ¶æ„è¿˜æ˜¯ResNet</li><li>slightly better acc</li><li>larger latency caused by self-att</li></ul></li><li>self-attention complement CNNs<ul><li>ä½œä¸ºadditional blockï¼Œç»™åˆ°backbone/headï¼Œæä¾›é•¿è·ç¦»ä¿¡æ¯</li><li>æœ‰äº›æ£€æµ‹/åˆ†å‰²ç½‘ç»œä¹Ÿå¼€å§‹ç”¨äº†transformerçš„encoder-decoderç»“æ„</li></ul></li><li>transformer-based vision backbones<ul><li>ä¸»è¦å°±æ˜¯ViTåŠå…¶è¡ç”Ÿå“</li><li>ViT requires large-scale training sets</li><li>DeiT introduces training strategies</li><li>ä½†æ˜¯è¿˜å­˜åœ¨high resolutionè®¡ç®—é‡çš„é—®é¢˜</li></ul></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>overview</p><p>  <img src="/2021/01/18/transformers/Swin.png" width="75%;"></p><ul><li>Swin-Tï¼štiny version</li><li>ç¬¬ä¸€æ­¥æ˜¯patch partitionï¼š<ul><li>å°†RGBå›¾åˆ‡æˆnon-overlapping patches</li><li>patchesï¼štokenï¼Œbasic element</li><li>feature input dimï¼šwith patch size 4x4ï¼Œdim=4x4x3=48</li></ul></li><li>ç„¶åæ˜¯linear embedding layer<ul><li>å°†raw feature re-projectionåˆ°æŒ‡å®šç»´åº¦</li><li>æŒ‡å®šç»´åº¦Cï¼šdefault=96</li></ul></li><li>æ¥ä¸‹æ¥æ˜¯Swin Transformer blocks<ul><li>the number of tokens maintain</li></ul></li><li>patch merging layersè´Ÿè´£reduce the number of tokens<ul><li>ç¬¬ä¸€ä¸ªpatch merging layer concat æ‰€æœ‰2x2çš„neighbor patchesï¼š4C-dim vec each</li><li>ç„¶åç”¨äº†ä¸€ä¸ªçº¿æ€§å±‚re-projection</li><li>number of tokensï¼ˆresolutionï¼‰ï¼šï¼ˆH/4*W/4ï¼‰/4 = ï¼ˆH/8*W/8ï¼‰ï¼Œè·Ÿå¸¸è§„çš„CNNä¸€æ ·å˜åŒ–çš„</li><li>token dimsï¼š2C</li><li>åé¢æ¥ä¸Šä¸€ä¸ªTransformer blocks</li><li>åˆèµ·æ¥å«stage2ï¼ˆstage3ã€stage4ï¼‰</li></ul></li></ul></li><li><p>Swin Transformer blocks</p><ul><li><p>è·ŸåŸå§‹çš„Transformer blockæ¯”ï¼Œå°±æ˜¯æŠŠåŸå§‹çš„MSAæ›¿æ¢æˆäº†window-basedçš„MSA</p></li><li><p>åŸå§‹çš„attentionï¼šglobal computation leads to quadratic complexity</p></li><li><p>window-based attentionï¼š</p><ul><li>attentionçš„è®¡ç®—åªå‘ç”Ÿåœ¨æ¯ä¸ªwindowå†…éƒ¨</li><li>non-overlapping partition</li><li>å¾ˆæ˜¾ç„¶lacks connections across windows </li></ul></li><li><p>shifted window partitioning in successive blocks</p><ul><li><p>ä¸¤ä¸ªattention block</p></li><li><p>ç¬¬ä¸€ä¸ªç”¨å¸¸è§„çš„window partitioning strategyï¼šä»å·¦ä¸Šè§’å¼€å§‹ï¼Œtake M=4ï¼Œwindow size 4x4ï¼ˆä¸€ä¸ªwindowé‡Œé¢åŒ…å«4x4ä¸ªpatchï¼‰</p></li><li><p>ç¬¬äºŒå±‚çš„windowï¼ŒåŸºäºå‰ä¸€å±‚ï¼Œå„å¹³ç§»M/2</p></li><li><p>introduce connections between neighbor non-overlapping windows in the previous layer</p></li><li><p>efficient computation</p><ul><li><p>shifted windowä¼šå¯¼è‡´windowå°ºå¯¸ä¸ä¸€è‡´ï¼Œä¸åˆ©äºå¹¶è¡Œè®¡ç®—</p><p><img src="/2021/01/18/transformers/efficient-SW.png" width="55%;"></p></li></ul></li></ul></li><li><p>relative position bias</p><ul><li>æˆ‘ä»¬åœ¨MxMçš„windowå†…éƒ¨è®¡ç®—local attentionï¼šä¹Ÿå°±æ˜¯input sequenceçš„time-stepæ˜¯$M^2$</li><li>Qã€Kã€V $\in R ^ {M^2 d}$</li><li>$Attention(Q,K,V)=Softmax(QK^T/\sqrt{d}+B)V$</li><li>è¿™ä¸ªBä½œä¸ºlocalçš„position biasï¼Œåœ¨äºŒç»´ä¸Šï¼Œåœ¨æ¯ä¸ªè½´ä¸Šçš„å˜åŒ–èŒƒå›´[-M+1,M-1]</li><li>we parameterized a smaller-sized bias matrix $\hat B\in R ^{(2M-1)*(2M-1)}$</li><li>values in $B \in R ^ {M^2*M^2}$ are taken from $\hat B$</li><li>the learnt relative position biaså¯ä»¥ç”¨æ¥initialize fine-tuned model</li></ul></li></ul></li><li><p>Architecture variants</p><ul><li><p>base modelï¼šSwin-Bï¼Œå‚æ•°é‡å¯¹æ ‡ViT-B</p></li><li><p>Swin-Tï¼š0.25xï¼Œå¯¹æ ‡ResNet-50 (DeiT-S) </p></li><li><p>Swin-Sï¼š0.5xï¼Œå¯¹æ ‡ResNet-101</p></li><li><p>Swin-Lï¼š2x</p></li><li><p>window sizeï¼šM=7</p></li><li><p>query dimï¼šd=32ï¼Œï¼ˆæ¯ä¸ªstageçš„input sequence dimé€æ¸x2ï¼Œheads numé€æ¸x2ï¼‰</p></li><li><p>MLPï¼šexpansion ratio=4</p></li><li><p>channel number Cï¼šç¬¬ä¸€ä¸ªstageçš„embdding dimï¼Œï¼ˆåç»­é€æ¸x2ï¼‰</p></li><li><p>hypersï¼š</p><p>  <img src="/2021/01/18/transformers/swins.png" width="40%;"></p></li><li><p>acc</p><p>  <img src="/2021/01/18/transformers/swinsacc.png" width="45%;"></p></li></ul></li></ul></li></ol><h2 id="DETR-End-to-End-Object-Detection-with-Transformers"><a href="#DETR-End-to-End-Object-Detection-with-Transformers" class="headerlink" title="DETR: End-to-End Object Detection with Transformers"></a>DETR: End-to-End Object Detection with Transformers</h2><ol><li><p>åŠ¨æœº</p><ul><li>new task formulationï¼ša direct set prediction problem</li><li>main gradients<ul><li>a set-based global loss</li><li>a transformer en-de architecture</li><li>remove the hand-designed componets like nms &amp; anchor</li></ul></li><li>acc &amp; run-time on par with Faster R-CNN on COCO<ul><li>significantly better performance on large objects</li><li>lower performances on small objects</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>modern detectors run object detection in an indirect way</p><ul><li>åŸºäºæ ¼å­/anchor/proposalsè¿›è¡Œå›å½’å’Œåˆ†ç±»</li><li>ç®—æ³•æ€§èƒ½å—åˆ¶äºnmsæœºåˆ¶ã€anchorè®¾è®¡ã€target-anchorçš„åŒ¹é…æœºåˆ¶</li></ul></li><li><p>end-to-end approach</p><p>  <img src="/2021/01/18/transformers/DETR.png" width="65%;"></p><ul><li>transformerçš„self-attentionæœºåˆ¶ï¼Œexplicitly model all pairwise interactions between elementsï¼šå†…å«äº†å»é‡ï¼ˆnmsï¼‰çš„èƒ½åŠ›</li><li>bipartite matchingï¼šset loss functionï¼Œå°†é¢„æµ‹å’Œgtçš„boxä¸€ä¸€åŒ¹é…ï¼Œrun in parallel</li><li>DETR does not require any customized layers, thus can be reproduced easily</li><li>expand to segmentation taskï¼ša simple segmentation head trained on top of a pre-trained DETR</li></ul></li><li><p>set predictionï¼što predict a set of bounding boxes and the categories for each</p><ul><li>basicï¼šmultilabel classification</li><li>detection task has near-duplicates issues</li><li>set predictionæ˜¯postprocessing-freeçš„ï¼Œå®ƒçš„global inference schemesèƒ½å¤Ÿavoid redundancy</li><li>usual lossï¼šbipartite match</li></ul></li><li><p>object detection</p><ul><li>set-based loss<ul><li>modern detectors use non-unique assignment rules together with NMS</li><li>bipartite matchingæ˜¯targetå’Œpredä¸€ä¸€å¯¹åº”</li></ul></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>overall</p><p>  <img src="/2021/01/18/transformers/DETR pipeline.png" width="65%;"></p><ul><li>three main components<ul><li>a CNN backbone</li><li>an encoder-decoder transformer</li><li>a simple FFN</li></ul></li></ul></li><li><p>backbone</p><ul><li>conventional r50</li><li>inputï¼š$[H_0, W_0, 3]$</li><li>outputï¼š$[H,W,C], H=\frac{H_0}{32}, W=\frac{W_0}{32}, C=2048$</li></ul></li><li><p>transformer encoder</p><ul><li>reduce channel dim to $d$ï¼š1x1 convï¼Œ$d=512$</li><li>collapse the spatial dimensionsï¼šfeature sequence [d, HW]ï¼Œæ¯ä¸ªspatial pixelä½œä¸ºä¸€ä¸ªfeature</li><li>fixed positional encodingsï¼š<ul><li>added to the input of each attention layer</li><li>ã€QUESTIONã€‘åŠ åœ¨Kå’ŒQä¸Šè¿˜æ˜¯embeddingä¸Šï¼Ÿ</li></ul></li></ul></li><li><p>transformer decoder</p><ul><li>è¾“å…¥Nä¸ªdim=dçš„embedding<ul><li>å«object queriesï¼šè¡¨ç¤ºæˆ‘ä»¬é¢„æµ‹å›ºå®šå€¼Nä¸ªç›®æ ‡</li><li>å› ä¸ºdecoderä¹Ÿæ˜¯permutation-invariantçš„ï¼ˆå› ä¸ºall sharedï¼‰ï¼Œæ‰€ä»¥è¦è¾“å…¥Nä¸ªä¸ä¸€æ ·çš„embedding</li><li>learnt positional encodings</li><li>add them to the input of each attention layer</li></ul></li><li>decodes the N objects in parallel</li></ul></li><li><p>prediction FFN</p><ul><li>3 layerï¼ŒReLUï¼Œ</li><li>box predictionï¼šnormalized center coords &amp; height &amp; width</li><li>class predictionï¼š<ul><li>an additional class label $\varnothing$ è¡¨ç¤ºno object</li></ul></li></ul></li><li><p>auxiliary losses</p><ul><li>each decoder layeråé¢éƒ½æ¥ä¸€ä¸ªFFN predictionå’ŒHungarian loss</li><li>shared FFN</li><li>an additional shared LN to norm the inputs of FFN</li><li>three components of the loss<ul><li>class lossï¼šCE loss</li><li>box loss<ul><li>GIOU loss</li><li>L1 loss</li></ul></li></ul></li></ul></li><li><p>technical details</p><ul><li>AdamWï¼š<ul><li>initial transformer lr=10e-4</li><li>initial backbone lr=10e-5</li><li>weight decay=10e-4</li></ul></li><li>Xavier init</li><li>imagenet-pretrained resnet weights with frozen batchnorm layersï¼šr50 &amp; r101ï¼ŒDETR &amp; DETR-R101</li><li>a variantï¼š<ul><li>increase feature resolution version</li><li>remove stage5â€™s stride and add a dilation</li><li>DETR-DC5 &amp; DETR-DC5-R101</li><li>improve performance for small objects</li><li>overall 2x computation increase</li></ul></li><li>augmentation<ul><li>resize input</li><li>random cropï¼šwith 0.5 prob then resize</li></ul></li><li>transformer default dropout 0.1</li><li>lr schedule<ul><li>300 epochs</li><li>drop by factor 10 after 200 epochs</li></ul></li><li>4 images per GPUï¼Œtotal batch 64</li></ul></li><li><p>for segmentation taskï¼šå…¨æ™¯åˆ†å‰²</p><ul><li>ç»™decoder outputsåŠ mask head</li><li>compute multi-head attention among<ul><li>decoder box predictions</li><li>encoder outputs</li></ul></li><li>generate M attention heatmaps per object</li><li>add a FPN styled CNN to recover resolution</li><li><p>pixel-wise argmax</p><p><img src="/2021/01/18/transformers/DETRseg.png" width="75%;"></p></li></ul></li></ul></li></ol><h2 id="UNETR-Transformers-for-3D-Medical-Image-Segmentation"><a href="#UNETR-Transformers-for-3D-Medical-Image-Segmentation" class="headerlink" title="UNETR: Transformers for 3D Medical Image Segmentation"></a>UNETR: Transformers for 3D Medical Image Segmentation</h2><ol><li><p>åŠ¨æœº</p><ul><li>unetç»“æ„ç”¨äºåŒ»å­¦åˆ†å‰²<ul><li>encoder learns global context</li><li>decoder utilize the representations to predict the semanic ouputs</li><li>the locality of CNN limits long-range spatial dependency</li></ul></li><li>our method<ul><li>use a pure transformer as the encoder</li><li>learn sequence representations of the input volume</li><li>global</li><li>multi-scale</li><li>encoder directly connects to decoder with skip connections</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>unetç»“æ„<ul><li>encoderç”¨æ¥æå–å…¨å›¾ç‰¹å¾</li><li>decoderç”¨æ¥recover</li><li>skip connectionsç”¨æ¥è¡¥å……spatial information that is lost during downsampling</li><li>localized receptive fieldsï¼š<ul><li>disadvantage in capturing multi-scale contextual information</li><li>å¦‚ä¸åŒå°ºå¯¸çš„è„‘è‚¿ç˜¤</li><li>ç¼“å’Œæ‰‹æ®µï¼šatrous convsï¼Œstill limited</li></ul></li></ul></li><li>transformer<ul><li>self-attention mechanism in NLP<ul><li>highlight the important features of word sequences</li><li>learn its long-range dependencies</li></ul></li><li>in ViT<ul><li>an image is represented as a patch embedding sequence</li></ul></li></ul></li><li>our method<ul><li>formulation<ul><li>1D seq2seq problem</li><li>use embedded patches</li></ul></li><li>the first completely transformer-based encoder</li></ul></li><li>other unet- transformer methods<ul><li>2D (ours 3D)</li><li>employ only in the bottleneck (ours pure transformer)</li><li>CNN &amp; transformer in separate streams and fuse</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>overview</p><p>  <img src="/2021/01/18/transformers/UNETR.png" width="75%;"></p></li><li><p>transformer encoder</p><ul><li>inputï¼š1D sequence of input embeddings</li><li>given 3D volume $x \in R^{HWDC}$</li><li>divide into flattened uniform non-overlapping patches $x\in R^{LCN^3}$<ul><li>$L=HWD/N^3$ï¼šthe sequence length</li><li>$N^3$ï¼špatch dimension</li></ul></li><li>linear projection to K-dim $E \in R^{LCK}$ï¼šremain constant through transformer</li><li>1D learnable positional embedding $E_{pos} \in R^LD$</li><li>12 self-att blocksï¼šMSA + MLP</li></ul></li><li>decoder &amp;skip connections<ul><li>é€‰å–encoderç¬¬{3,6,9,12}ä¸ªblockçš„è¾“å‡º</li><li>reshape back to 3D volume $[\frac{H}{N},\frac{W}{N},\frac{D}{N},C]$</li><li>consecutive 3x3x3 conv+BN+ReLU</li><li>bottleneck<ul><li>deconv by 2 to increase resolution</li><li>then concat with the previous resized feature</li><li>then jointly consecutive conv</li><li>then upsample with deconvâ€¦</li></ul></li><li>concatåˆ°åŸå›¾resolutionä»¥åï¼Œconsecutive convä»¥åï¼Œå†1x1x1 conv+softmax</li></ul></li><li>loss<ul><li>dice loss<ul><li>diceï¼šfor each class channelï¼Œè®¡ç®—diceï¼Œç„¶åæ±‚ç±»å¹³å‡</li><li>1-dice</li></ul></li><li>ce loss<ul><li>for each pixelï¼Œæ±‚bceï¼Œç„¶åæ±‚æ‰€æœ‰pixelçš„å¹³å‡</li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> transformer, self-attention </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>pre-training &amp; self-training</title>
      <link href="/2021/01/17/pre-training-self-training/"/>
      <url>/2021/01/17/pre-training-self-training/</url>
      <content type="html"><![CDATA[<p>[pre-training] Rethinking ImageNet Pre-trainingï¼ŒHe Kaimingï¼ŒimageNet pre-trainingå¹¶æ²¡æœ‰çœŸæ­£helps accï¼Œåªæ˜¯speedupï¼Œrandom initializationèƒ½å¤Ÿreach no worseçš„ç»“æœï¼Œå‰ææ˜¯æ•°æ®å……è¶³å¢å¼ºå¤ŸçŒ›ï¼Œå¯¹å°é—¨å°æˆ·è¿˜æ˜¯æ²¡å•¥ç”¨ï¼Œæˆ‘ä»¬å¸Œæœ›speedup</p><p>[pre-training &amp; self-training] Rethinking Pre-training and Self-trainingï¼ŒGoogle Brainï¼Œæå‡ºtask-specificçš„pseudo labelè¦æ¯”pre-trainingä¸­æå‡ºæ¥çš„å„ç§æ ‡ç­¾è¦å¥½ï¼Œå‰æè¿˜æ˜¯å †æ•°æ®ï¼Œå¯¹å°é—¨å°æˆ·æ²¡å•¥ç”¨ï¼Œlow-dataä¸‹è¿˜æ˜¯pre-trainä¿å¹³å®‰</p><p>æ€»ä½“ä¸Šéƒ½æ˜¯é’ˆå¯¹è·¨ä»»åŠ¡ä¸‹ï¼ŒimageNet pre-trainingæ„ä¹‰çš„æ¢è®¨ï¼Œ</p><ul><li>åˆ†ç±»é—®é¢˜è¿˜æ˜¯å¯ä»¥ç»§ç»­pretrained</li><li>kaimingè¿™ä¸ªåªæ˜¯factï¼Œæ²¡æœ‰ç°å®æŒ‡å¯¼æ„ä¹‰</li><li>googleè¿™ä¸ªone step furtherï¼Œæå‡ºäº†self-trainingåœ¨ç°å®æ¡ä»¶ä¸­å¯ä»¥ä¸€è¯•</li></ul><h2 id="Rethinking-Pre-training-and-Self-training"><a href="#Rethinking-Pre-training-and-Self-training" class="headerlink" title="Rethinking Pre-training and Self-training"></a>Rethinking Pre-training and Self-training</h2><ol><li><p>åŠ¨æœº</p><ul><li>given factï¼šImageNet pre-training has limited impact on COCO object detection </li><li>investigate self-training to utilize the additional data</li></ul></li><li><p>è®ºç‚¹</p><ul><li>common practice pre-training<ul><li>supervised pre-training<ul><li>é¦–å…ˆè¦æ±‚æ•°æ®æœ‰æ ‡ç­¾</li><li>pre-train the backbone on ImageNet as a classification task</li></ul></li><li>å¼±ç›‘ç£å­¦ä¹ <ul><li>with pseudo/noisy label</li><li>kaimingï¼šExploring the limits of weakly supervised pretraining</li></ul></li><li>self-supervised pre-training<ul><li>æ— æ ‡ç­¾çš„æµ·é‡æ•°æ®</li><li>æ„é€ å­¦ä¹ ç›®æ ‡ï¼šautoencoderï¼Œcontrastiveï¼Œâ€¦</li><li><a href="https://zhuanlan.zhihu.com/p/108906502" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/108906502</a></li></ul></li></ul></li><li>self-training paradigm on COCO<ul><li>train an object detection model on COCO </li><li>generate pseudo labels on ImageNet</li><li>both labeled data are combined to train a new model</li><li>åŸºæœ¬åŸºäºnoisy studentçš„æ–¹æ³•</li></ul></li><li>observations<ul><li>with stronger data augmentation, pre-training hurts the accuracy, but helps in self-training</li><li>both supervised and self-supervised pre-training methods fails</li><li>the benefit of pre-training does not cancel out the gain by self-training</li><li>flexible about unlabeled data sources, model architectures and computer vision tasks</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li>data augmentation<ul><li>vary the strength of data augmentation as 4 levels</li></ul></li><li><p>pre-training</p><ul><li>efficientNet-B7</li><li>AutoAugment weights &amp; noisy student weights</li></ul><p><img src="/2021/01/17/pre-training-self-training/notations.png" width="70%;"></p></li><li><p>self-training</p><ul><li>noisy student scheme</li><li>å®éªŒå‘ç°self-training with this standard loss function can be unstable </li><li>implement a loss normalization technique </li></ul></li><li>experimental settings<ul><li>object detection<ul><li>COCO dataset for supervised learning</li><li>unlabeled ImageNet and OpenImages dataset for self-trainingï¼šscore thresh 0.5 to generate pesudo labels</li><li>retinaNet &amp; spineNet</li><li>batchï¼šhalf supervised half pesudo</li></ul></li><li>semantic segmentation<ul><li>PASCAL VOC 2012 for supervised learning</li><li>augmented PASCAL &amp; COCO &amp; ImageNet for self-trainingï¼šscore thresh 0.5 to generate pesudo masks &amp; multi-scale</li><li>NAS-FPN</li></ul></li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li><p>pre-training</p><ul><li>Pre-training hurts performance when stronger data augmentation is usedï¼šå› ä¸ºä¼šsharpenæ•°æ®å·®å¼‚ï¼Ÿ</li><li>More labeled data diminishes the value of pre-trainingï¼šé€šå¸¸æˆ‘ä»¬çš„å®éªŒæ•°æ®fractionéƒ½æ¯”è¾ƒå°çš„ç›¸å¯¹imageNetï¼Œæ‰€ä»¥ç†è®ºä¸Šä¸ä¼šharmï¼Ÿ</li><li><p>self-supervised pre-trainingä¹Ÿä¼šä¸€æ ·harmï¼Œåœ¨augmentåŠ å¼ºçš„æ—¶å€™</p><p><img src="/2021/01/17/pre-training-self-training/pre-training.png" width="60%;"></p></li></ul></li><li><p>self-training</p><ul><li><p>Self-training helps in high data/strong augmentation regimes, even when pre-training hurtsï¼šä¸åŒçš„augment levelï¼Œself-trainingå¯¹æœ€ç»ˆç»“æœéƒ½æœ‰åŠ æˆ</p><p><img src="/2021/01/17/pre-training-self-training/self1.png" width="60%;"></p></li><li><p>Self-training works across dataset sizes and is additive to pre-trainingï¼šä¸åŒçš„æ•°æ®é‡ï¼Œä¹Ÿéƒ½æœ‰åŠ æˆï¼Œä½†æ˜¯low data regimeä¸‹enjoys the biggest gain</p><p><img src="/2021/01/17/pre-training-self-training/self2.png" width="60%;"></p></li></ul></li><li><p>discussion</p><ul><li>weak performance of pre-training is that pre-training is not aware of the task of interest and can fail to adapt </li><li>jointly training also helpsï¼šaddress the mismatch between two dataset</li><li>noisy labeling is worse than targeted pseudo labeling</li></ul></li><li><p>æ€»ä½“ç»“è®ºï¼šå°æ ·æœ¬é‡çš„æ—¶å€™ï¼Œpre-trainingè¿˜æ˜¯æœ‰åŠ æˆçš„ï¼Œå†åŠ ä¸Šself-trainingè¿›ä¸€æ­¥æå‡ï¼Œæ ·æœ¬å¤šçš„æ—¶å€™å°±ç›´æ¥self-training</p></li></ul></li></ol><h2 id="Rethinking-ImageNet-Pre-training"><a href="#Rethinking-ImageNet-Pre-training" class="headerlink" title="Rethinking ImageNet Pre-training"></a>Rethinking ImageNet Pre-training</h2><ol><li>åŠ¨æœº<ul><li>thinking random initialization &amp; pre-training</li><li>ImageNet pre-training<ul><li>speed up</li><li>but not necessarily improving</li></ul></li><li>random initialization<ul><li>can achieve no worse result</li><li>robust to data size, models, tasks and metrics</li></ul></li><li>rethink current paradigm of â€˜pre- training and fine-tuningâ€™</li></ul></li><li>è®ºç‚¹<ul><li>no fundamental obstacle preventing us from training from scratch <ul><li>if use normalization techniques appropriately </li><li>if train sufficiently long </li></ul></li><li>pre-training<ul><li>speed up</li><li>when fine-tuning on small dataset new hyper-parameters must be selected to avoid overfitting</li><li>localization-sensitive task benefits limited from pre-training</li><li>aimed at communities that donâ€™t have enough data or computational resources</li></ul></li></ul></li><li>æ–¹æ³•<ul><li>normalization<ul><li>form<ul><li>normalized parameter initialization </li><li>normalization layers </li></ul></li><li>BN layers makes training from scratch difficult <ul><li>small batch size degrade the acc of BN</li><li>fine-tuningå¯ä»¥freeze BN</li><li>alternatives<ul><li>GNï¼šå¯¹batch sizeä¸æ•æ„Ÿ</li><li>syncBN</li></ul></li></ul></li><li>with appropriately normalized initializationå¯ä»¥train from scratch VGGè¿™ç§ä¸ç”¨BNå±‚çš„</li></ul></li><li>convergence<ul><li>pre-training model has learned low-level features that do not need to be re-learned during </li><li>random-initial training need more iterations to learn both low-level and semantic features</li></ul></li></ul></li><li>å®éªŒ<ul><li>investigate maskRCNN<ul><li>æ›¿æ¢BNï¼šGN/sync-BN</li><li>learning rateï¼š<ul><li>training longer for the first (large) learning rate is useful</li><li>but training for longer on small learning rates often leads to overfitting </li></ul></li></ul></li><li>10k COCOå¾€ä¸Šï¼Œtrain from scratch resultsèƒ½å¤Ÿcatch up pretraining resultsï¼Œåªè¦è®­çš„å¤Ÿä¹…</li><li>1kå’Œ3.5kçš„COCOï¼Œconverges show no worseï¼Œä½†æ˜¯åœ¨éªŒè¯é›†ä¸Šå·®ä¸€äº›ï¼šstrong overfitting due to lack of data</li><li>PASCALçš„ç»“æœä¹Ÿå·®ä¸€ç‚¹ï¼Œå› ä¸ºinstanceå’Œcategoryéƒ½æ›´å°‘ï¼Œnot directly comparable to the same number of COCO imagesï¼šfewer instances and categories has a similar negative impact as insufficient training data</li></ul></li></ol>]]></content>
      
      
    </entry>
    
    <entry>
      <title>long-tailed</title>
      <link href="/2021/01/11/long-tailed/"/>
      <url>/2021/01/11/long-tailed/</url>
      <content type="html"><![CDATA[<p>[bag of tricks] Bag of Tricks for Long-Tailed Visual Recognition with Deep Convolutional Neural Networksï¼šç»“è®ºå°±æ˜¯ä¸¤é˜¶æ®µï¼Œinput mixup + CAM-based DRS + muted mixup fine-tuningç»„åˆä½¿ç”¨æœ€å¥½</p><p>[balanced-meta softmax] Balanced Meta-Softmax for Long-Tailed Visual Recognitionï¼šå•†æ±¤</p><p>[eql] Equalization Loss for Long-Tailed Object Recognition </p><p>[eql2] Equalization Loss v2:  A New Gradient Balance Approach for Long-tailed Object Detection </p><p>[Class Rectification Loss] Imbalanced Deep Learning by Minority Class Incremental Rectificationï¼šæå‡ºCRLä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«åˆ†å¸ƒç¨€ç–çš„å°ç±»ä»¬çš„è¾¹ç•Œï¼Œä»¥æ­¤é¿å…å¤§ç±»ä¸»å¯¼çš„å½±å“</p><h2 id="Bag-of-Tricks-for-Long-Tailed-Visual-Recognition-with-Deep-Convolutional-Neural-Networks"><a href="#Bag-of-Tricks-for-Long-Tailed-Visual-Recognition-with-Deep-Convolutional-Neural-Networks" class="headerlink" title="Bag of Tricks for Long-Tailed Visual Recognition with Deep Convolutional Neural Networks"></a>Bag of Tricks for Long-Tailed Visual Recognition with Deep Convolutional Neural Networks</h2><ol><li><p>åŠ¨æœº</p><ul><li>to give a detailed experimental guideline of common tricks</li><li>to obtain the effective combinations of these tricks</li><li>propose a novel data augmentation approach</li></ul></li><li><p>è®ºç‚¹</p><ul><li>long-tailed datasets <ul><li>poor accuray on the under-presented minority</li><li>long-tailed CIFARï¼š<ul><li>æŒ‡æ•°å‹è¡°å‡</li><li>imbalance factorï¼š50/100</li><li>test set unchanged</li></ul></li><li>ImageNet-LT  <ul><li>sampling the origin set follow the pareto distribution</li><li>test set is balanced</li></ul></li><li>iNaturalist<ul><li>extremely imbalanced real world dataset</li><li>fine-grained problem</li></ul></li></ul></li><li>different learning paradigms <ul><li>metric learning </li><li>meta learning </li><li>knowledge transfer  </li><li>suffer from high sensitivity to hyper-parameters</li></ul></li><li>training tricks<ul><li>re-weighting</li><li>re-sample</li><li>mixup</li><li>two-stage training</li><li>different tricks might hurt each other</li><li>propose a novel data augmentation approach based on CAMï¼šgenerate images with transferred foreground and unchanged background</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>start from baseline</p><p> <img src="/2021/01/11/long-tailed/baseline.png" width="70%;"></p></li><li><p>re-weighting</p><ul><li>baselineï¼šCE</li><li>re-weighting methodsï¼š<ul><li>cost-sensitive CEï¼šæŒ‰ç…§æ ·æœ¬é‡çº¿æ€§åŠ æƒ$\frac{n_c}{n_{min}}$</li><li>focal lossï¼šå›°éš¾æ ·æœ¬åŠ æƒ</li><li>class-balanced lossï¼š<ul><li>effective number rather than æ ·æœ¬é‡$n_c$</li><li>hyperparameter $\beta$ and weighting factorï¼š$\frac{1-\beta}{1-\beta^{n_c}}$</li></ul></li><li>åœ¨cifar10ä¸Šæœ‰æ•ˆï¼Œä½†æ˜¯cifar100ä¸Šå°±ä¸å¥½äº†<ul><li>directly application in training procedure is not a proper choice</li><li>especially whenç±»åˆ«å¢å¤šï¼ŒimbalanceåŠ å‰§çš„æ—¶å€™</li></ul></li></ul></li><li><img src="/2021/01/11/long-tailed/reweighting.png" width="50%;"></li></ul></li><li><p>re-sampling</p><ul><li>re-sampling methods<ul><li>over-samplingï¼š<ul><li>éšæœºå¤åˆ¶minority</li><li>might leads to overfitting</li></ul></li><li>under-sampling<ul><li>éšæœºå»æ‰ä¸€äº›majority</li><li>be preferable to over-sampling </li></ul></li><li>æœ‰è§„å¾‹åœ°sampling<ul><li>å¤§ä½“éƒ½æ˜¯imbalancedå‘ç€lighter imbalancedå‘ç€balancedæ¨åŠ¨</li></ul></li><li>artificial sampling methods<ul><li>create artificial samples </li><li>sample based on gradients and features </li><li>likely to introduce noisy data </li></ul></li></ul></li><li>è§‚å¯Ÿåˆ°æå‡æ•ˆæœä¸æ˜æ˜¾</li><li><img src="/2021/01/11/long-tailed/resampling.png" width="50%;"></li></ul></li><li><p>mixup</p><ul><li>input mixupï¼šinput mixup can be further improved if we remove the mixup in last several epochs</li><li>manifold mixupï¼šon only one layer </li><li><p>è§‚å¯Ÿåˆ°ä¸¤ç§mixupåŠŸæ•ˆå·®ä¸å¤šï¼Œåé¢å‘ç°input mixupæ›´å¥½äº›</p><p><img src="/2021/01/11/long-tailed/mixup.png" width="50%;"></p><ul><li>input mixupå»æ‰å†finetuningå‡ ä¸ªepochç»“æœåˆæå‡ï¼Œmanifoldåˆ™ä¼šå˜å·®</li></ul><p><img src="/2021/01/11/long-tailed/mixup1.png" width="50%;"></p></li></ul></li><li><p>two-stage training</p><ul><li>imbalanced training + balanced fine-tuning</li><li>vanilla training schedule on imbalanced data<ul><li>å…ˆå­¦ç‰¹å¾</li></ul></li><li>fine-tune on balanced subsets<ul><li>å†è°ƒæ•´recognition accuracy </li><li>deferred re-balancing by re-sampling (DRS)  ï¼špropose CAM-based sampling</li><li>deferred re-balancing by re-weighting (DRW) </li></ul></li><li>proposed CAM-based sampling<ul><li>DRS only replicate or remove</li><li>for each sampled image, apply the trained model &amp; its ground truth label  to generate CAM</li><li>ç”¨heatmapçš„å¹³å‡å€¼ä½œä¸ºé˜ˆå€¼æ¥åŒºåˆ†å‰èƒŒæ™¯</li><li>å¯¹å‰æ™¯apply transformations <ul><li>horizontal flipping</li><li>translation</li><li>rotating</li><li>scaling</li></ul></li></ul></li><li>å‘ç°fine-tuningæ—¶å€™å†resampleæ¯”ç›´æ¥resampleçš„ç»“æœå¥½</li><li>proposed CAM-based samplingå¥½äºå…¶ä»–samplingï¼Œå…¶ä¸­CAM-based balance- samplingæœ€å¥½</li><li><p>ImageTrans balance-samplingåªåšå˜æ¢ï¼Œä¸ç”¨CAMåŒºåˆ†å‰èƒŒæ™¯ï¼Œç»“æœä¸å¦‚CAM-basedï¼Œè¯æ˜CAMæœ‰ç”¨</p><p><img src="/2021/01/11/long-tailed/DRS.png" width="50%;"></p></li><li><p>å‘ç°fine-tuningæ—¶å€™å†reweightæ¯”ç›´æ¥reweightçš„ç»“æœå¥½</p></li><li><p>å…¶ä¸­CSCEï¼ˆæŒ‰ç…§æ ·æœ¬é‡çº¿æ€§åŠ æƒï¼‰æœ€å¥½</p><p><img src="/2021/01/11/long-tailed/DRW.png" width="50%;"></p></li><li><p>æ•´ä½“æ¥çœ‹DRSçš„ç»“æœç¨å¾®æ¯”DRWå¥½ä¸€ç‚¹</p></li></ul></li><li><p>trick combinations</p><ul><li>two-stageçš„CAM-based DRSç•¥å¥½äºDRWï¼Œä¸¤ä¸ªåŒæ—¶ç”¨ä¸ä¼šfurther improve</li><li>å†åŠ ä¸Šmixupçš„è¯ï¼Œinputæ¯”manifoldå¥½ä¸€äº›</li><li><p>ç»“è®ºå°±æ˜¯ï¼šinput mixup + CAM-based DRS + mute fine-tuningï¼Œapply the tricks incrementally</p><p><img src="/2021/01/11/long-tailed/bag.png" width="50%;"></p></li></ul></li></ul></li></ol><h2 id="Balanced-Meta-Softmax-for-Long-Tailed-Visual-Recognition"><a href="#Balanced-Meta-Softmax-for-Long-Tailed-Visual-Recognition" class="headerlink" title="Balanced Meta-Softmax for Long-Tailed Visual Recognition"></a>Balanced Meta-Softmax for Long-Tailed Visual Recognition</h2><ol><li><p>åŠ¨æœº</p><ul><li>long-tailedï¼šmismatch between training and testing distributions </li><li>softmaxï¼šbiased gradient estimation under the long-tailed setup</li><li>propose <ul><li>Balanced Softmaxï¼šan elegant unbiased extension of Softmax</li><li>apply a complementary Meta Samplerï¼šoptimal sample rate</li></ul></li><li>classification &amp; segmentation</li></ul></li><li><p>è®ºç‚¹</p><ul><li>raw baselineï¼ša model that minimizes empirical risk on long-tailed training datasets often underperforms on a class-balanced test set</li><li>most methods use re-sampling or re-weighting<ul><li>to simulate a balanced dataset</li><li>may under-class the majority or have gradient issue</li></ul></li><li>meta-learning<ul><li>optimize the weight per sample</li><li>need a clean and unbiased dataset</li></ul></li><li>decoupled training<ul><li>å°±æ˜¯ä¸Šé¢ä¸€ç¯‡è®ºæ–‡ä¸­çš„ä¸¤é˜¶æ®µï¼Œç¬¬ä¸€é˜¶æ®µå…ˆå­¦è¡¨å¾ï¼Œç¬¬äºŒé˜¶æ®µè°ƒæ•´åˆ†å¸ƒfine-tuning</li><li>not adequate for datasets with extremely high imbalance factor</li></ul></li><li>LDAM<ul><li>Label-Distribution-Aware Margin Loss </li><li>larger generalization error bound for minority</li><li>suit for binary classification</li></ul></li><li>we propose BALMS <ul><li>Balanced Meta-Softmax</li><li>theoretically equivalent with generalization error bound</li><li>for datasets with high imbalance factors should combine Meta Sampler  </li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>balanced softmax</p><ul><li>biasedï¼šä»è´å¶æ–¯æ¡ä»¶æ¦‚ç‡å…¬å¼çœ‹ï¼Œstandard softmaxä¸Šé»˜è®¤äº†å‡åŒ€é‡‡æ ·çš„p(y)ï¼Œåœ¨é•¿å°¾åˆ†å¸ƒçš„æ—¶å€™ï¼Œå°±æ˜¯æœ‰åçš„</li><li><p>åŠ æƒï¼š</p><ul><li>åŠ åœ¨softmaxé¡¹é‡Œé¢</li><li>åŸºäºæ ·æœ¬é‡çº¿æ€§åŠ æƒ</li></ul><p><img src="/2021/01/11/long-tailed/bsoft.png" width="40%;"></p></li><li><p>æ•°å­¦æ„ä¹‰ä¸Šï¼šwe need to focus on minimizing the training loss of the tail classes</p></li></ul></li><li><p>meta sampler</p><ul><li>resampleå’Œreweightç›´æ¥combineå¯èƒ½ä¼šworsen performance</li><li>class balance resampleå¯èƒ½æœ‰over-balance issue</li></ul></li><li><p>combination procedures</p><p>  <img src="/2021/01/11/long-tailed/meta.png" width="70%;"></p><ul><li>å¯¹å½“å‰åˆ†å¸ƒï¼Œå…ˆè®¡ç®—balanced-softmaxï¼Œä¿å­˜ä¸€ä¸ªæ¢¯åº¦æ›´æ–°åçš„æ¨¡å‹</li><li>è®¡ç®—è¿™ä¸ªä¸´æ—¶æ¨¡å‹åœ¨meta setä¸Šçš„CEï¼Œå¯¹åˆ†å¸ƒembeddingè¿›è¡Œæ¢¯åº¦æ›´æ–°ï¼šè¯„ä¼°å½“å‰åˆ†å¸ƒå’‹æ ·ï¼Œå¾€ä¸€å®šæ–¹å‘çŸ«æ­£</li><li>å¯¹çœŸæ­£çš„æ¨¡å‹ï¼Œç”¨æœ€æ–°çš„åˆ†å¸ƒï¼Œè®¡ç®—balanced-softmaxï¼Œè¿›è¡Œæ¢¯åº¦æ›´æ–°ï¼šç”¨ä¼˜åŒ–åçš„åˆ†å¸ƒï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ </li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li>CEçš„ç»“æœå‘ˆç°æ˜æ˜¾çš„é•¿å°¾åŒåˆ†å¸ƒè¶‹åŠ¿</li><li>CBSæœ‰ç¼“è§£</li><li>BSæ›´å¥½</li><li>BS+CBSä¼šover sample</li><li><p>BS+metaæœ€å¥½</p><p><img src="/2021/01/11/long-tailed/test.png" width="70%;"></p></li></ul></li></ol><h2 id="Imbalanced-Deep-Learning-by-Minority-Class-Incremental-Rectification"><a href="#Imbalanced-Deep-Learning-by-Minority-Class-Incremental-Rectification" class="headerlink" title="Imbalanced Deep Learning by Minority Class Incremental Rectification"></a>Imbalanced Deep Learning by Minority Class Incremental Rectification</h2><ol><li><p>åŠ¨æœº</p><ul><li>significantly imbalanced training data </li><li>propose<ul><li>batch-wise incremental minority class rectification model  </li><li>Class Rectification Loss (CRL) </li></ul></li><li><p>bring benefits to both minority and majority class boundary learning </p><p><img src="/2021/01/11/long-tailed/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/data-aug/CRL.png" width="80%"></p></li></ul></li><li><p>è®ºç‚¹</p><ul><li>Most methods produce learning bias towards the majority classes  <ul><li>to eliminate bias<ul><li>lifting the importance of minority classesï¼šover-sampling can easily cause model overfittingï¼Œå¯èƒ½é€ æˆå¯¹å°ç±»åˆ«çš„è¿‡åˆ†å…³æ³¨ï¼Œè€Œå¯¹å¤§ç±»åˆ«ä¸å¤Ÿé‡è§†ï¼Œå½±å“æ¨¡å‹æ³›åŒ–èƒ½åŠ›</li><li>cost-sensitive learningï¼šdifficult to optimise </li><li>threshold-adjustment techniqueï¼šgiven by experts </li></ul></li></ul></li><li>previous methods mainly investigate single-label binary-class with small imbalance ratio</li><li>real data <ul><li>large ratioï¼špower-law distributions </li><li>Subtle appearance discrepancy </li></ul></li><li>hard sample mining<ul><li>hard negatives are more informative than easy negatives as they violate a model class boundary  </li><li>we only consider hard mining on the minority classes for efficiency </li><li>our batch-balancing hard mining strategyï¼šeliminating exhaustive searching  </li></ul></li><li>LMLE <ul><li>å”¯ä¸€çš„ç«å“ï¼šè€ƒè™‘äº†data imbalanceçš„ç»†ç²’åº¦åˆ†ç±»</li><li>not end-to-end </li><li>global hard mining </li><li>computationally complex and expensive </li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>CRL overview</p><ul><li>explicitly imposing structural discrimination of minority classes </li><li>batch-wise</li><li>operate on CE</li><li>forcus on minority class onlyï¼šthe conventional CE loss can already model the majority classes well </li></ul></li><li><p>limitations of CE</p><ul><li>CE treat the individual samples and classes as equally important </li><li>the learned model is suboptimal</li><li>boundaries are biased towards majority classes  </li></ul></li><li><p>profile the class distribution for each class</p><ul><li>hard mining</li><li><p>overview</p><p><img src="/2021/01/11/long-tailed/data-aug/CRL2.png" width="50%"></p></li></ul></li><li><p>minority class hard sample mining</p><ul><li><p>selectively â€œborrowingâ€ majority class samples from class decision boundary </p></li><li><p>to minority classâ€™s perspectiveï¼šmining both hard-positive and hard-negative samples </p></li><li><p>define minority classï¼šselected in each mini-batch</p></li><li><p>Incremental refinementï¼š</p><ul><li>eliminates the LMLEâ€™s drawback in assuming that local group structures of all classes can be estimated reliably by offline global clustering </li><li>mini-batchçš„data distributionå’Œè®­ç»ƒé›†ä¸æ˜¯å®Œå…¨ä¸€è‡´çš„</li></ul></li><li><p>steps</p><ul><li><p>profile the minority and majority classes per label in each training mini-batch</p><ul><li>for each sampleï¼Œfor each class $j$ï¼Œfor each pred class $k$ï¼Œwe have $h^j=[h_1^j, â€¦, h_k^j, â€¦, h_{n_cls}^j]$</li><li>sort $h_k^j$ in descent orderï¼Œdefine the minority classes for each class with $C_{min}^j = \sum_{k\in C_{min}^j}h_k^j \leq \rho * n_{bs}$ï¼Œwith $\rho=0.5$</li></ul></li><li><p>hard mining</p><ul><li><p>hardness</p><ul><li>score basedï¼šprediction scoreï¼Œclass-level</li><li>feature basedï¼šfeature distanceï¼Œinstance-level</li></ul></li><li><p>class-levelï¼Œfor class c</p><ul><li>hard-positivesï¼šsame gt classï¼Œbut low prediction</li><li>hard-negativeï¼šdifferent gt classï¼Œwith high prediction</li></ul></li><li><p>instance-levelï¼Œfor each sample in class c</p><ul><li>hard-positivesï¼šsame gt classï¼Œlarge distance with current sample</li><li>hard-negativeï¼šdifferent gt classï¼Œsmall distance with current sample</li></ul></li><li><p>top-k mining</p><ul><li>hard-positivesï¼šbottom-k scored on c/top-k distance on c</li><li>hard-negativeï¼štop-k scored on c/bottom-k distance on c</li></ul></li><li><p>score-based yields superior to distance-based</p><p>  <img src="/2021/01/11/long-tailed/data-aug/hard.png" width="50%"></p></li></ul></li></ul></li></ul></li><li><p>CRL</p><ul><li>final weighted lossï¼š$L = \alpha L_{crl}+(1-\alpha)L_{ce}$ï¼Œ$\alpha=\eta\Omega_{imbalance}$</li><li>class imbalance measure $\Omega$ï¼šmore weighting  is assigned to more imbalanced labels </li><li>form<ul><li>triplet lossï¼šç±»å†…+ç±»é—´</li><li>contrastive lossï¼šç±»å†…</li><li>modelling the distribution relationship of positive and negative pairsï¼šæ²¡çœ‹æ‡‚</li></ul></li></ul></li></ul></li><li><p>æ€»ç»“</p><p> å°±æ˜¯å¥—ç”¨ç°æœ‰çš„metric learningï¼Œå®šä¹‰äº†ä¸€ä¸ªå˜åŒ–çš„minority classï¼Œåƒåœ¾ã€‚</p><p> è¯´åˆ°åº•å°±æ˜¯å¤§æ•°æ®â€”â€”CEï¼Œå°æ•°æ®â€”â€”metric learningã€‚</p></li></ol>]]></content>
      
      
        <tags>
            
            <tag> é•¿å°¾åˆ†å¸ƒ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>refineDet</title>
      <link href="/2021/01/08/refineDet/"/>
      <url>/2021/01/08/refineDet/</url>
      <content type="html"><![CDATA[<p>å’ŒrefineNetæ²¡æœ‰ä»»ä½•å…³ç³»</p><h2 id="RefineDet-Single-Shot-Refinement-Neural-Network-for-Object-Detectio"><a href="#RefineDet-Single-Shot-Refinement-Neural-Network-for-Object-Detectio" class="headerlink" title="RefineDet: Single-Shot Refinement Neural Network for Object Detectio"></a>RefineDet: Single-Shot Refinement Neural Network for Object Detectio</h2><ol><li><p>åŠ¨æœº</p><ul><li>inherit the merits of both two-stage and one-stageï¼šaccuracy and efficiency</li><li>single-shot</li><li>multi-task</li><li>refineDet<ul><li>anchor refinement module (ARM)  </li><li>object detection module (ODM)</li><li>transfer connection block (TCB) </li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>three advantages that two-stage superior than one-stage<ul><li>RPNï¼šhandle class imbalance</li><li>two step regressï¼šcoarse to refine</li><li>two stage featureï¼šRPNä»»åŠ¡å’Œregressionä»»åŠ¡æœ‰å„è‡ªçš„feature</li></ul></li><li>æ¨¡æ‹ŸäºŒé˜¶æ®µæ£€æµ‹çš„RPNï¼ŒæŠŠclassifierä»»åŠ¡ä¸­çš„å¤§é‡é˜´æ€§æ¡†å…ˆæ’æ‰ï¼Œä½†ä¸æ˜¯ä»¥ä¸¤ä¸ªé˜¶æ®µçš„å½¢å¼ï¼Œè€Œæ˜¯multi-taskå¹¶è¡Œ</li><li>å°†ä¸€é˜¶æ®µæ£€æµ‹çš„objectnesså’Œbox regressionä»»åŠ¡è§£è€¦ï¼Œä¸¤ä¸ªä»»åŠ¡é€šè¿‡transfer blockè¿æ¥</li><li>ARM<ul><li>remove negative anchors to reduce search space for the classifier </li><li>coarsely adjust the locations and sizes of anchors to provide better initialization for regression</li></ul></li><li>ODM <ul><li>further improve the regression </li><li>predict multi labels</li></ul></li><li><p>TCB </p><ul><li>transfer the features in the ARM to handle the more challenging tasks in the ODM </li></ul><p><img src="/2021/01/08/refineDet/refineDet.png" width="70%;"></p></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>Transfer Connection Block</p><ul><li><p>æ²¡ä»€ä¹ˆæ–°çš„ä¸œè¥¿ï¼Œä¸Šé‡‡æ ·ç”¨äº†deconvï¼Œconv-reluï¼Œelement-wise add</p><p><img src="/2021/01/08/refineDet/TCB.png" width="50%;"></p></li></ul></li><li><p>Two-Step Cascaded Regression </p><ul><li>fisrt step ARM prediction <ul><li>for each cellï¼Œfor each predefined anchor boxesï¼Œpredict 4 offsets and 2 scores</li><li>obtain refined anchor boxes </li></ul></li><li>second step ODM prediction<ul><li>with justified feature mapï¼Œwith refined anchor boxes </li><li>generate accurate boxes offset to refined boxes and multi-class scoresï¼Œc+4</li></ul></li></ul></li><li><p>Negative Anchor Filtering </p><ul><li>reject well-classified negative anchors </li><li>if the negative confidence is larger than 0.99ï¼Œdiscard it in training the ODM </li><li>ODMæ¥æ”¶æ‰€æœ‰pred positiveå’Œhard negative</li></ul></li><li><p>Training and Inference details</p><ul><li>backï¼šVGG16 &amp; resnet101<ul><li>fc6 &amp; fc7å˜æˆä¸¤ä¸ªconv</li><li>different feature scales </li><li>L2 norm</li><li>two extra convolution layers and one extra residual block </li></ul></li><li>4 feature strides<ul><li>each levelï¼š1 scale &amp; 3 ratios</li><li>ensures that different scales of anchors have the same tiling density on the image</li></ul></li><li>matching<ul><li>æ¯ä¸ªGT box matchä¸€ä¸ªscoreæœ€é«˜çš„anchor box</li><li>ä¸ºæ¯ä¸ªanchor boxæ‰¾åˆ°æœ€åŒ¹é…çš„iouå¤§äº0.5çš„gt box</li><li>ç›¸å½“äºæŠŠignoreé‚£éƒ¨åˆ†ä¹Ÿä½œä¸ºæ­£æ ·æœ¬äº†</li></ul></li><li>Hard Negative Mining<ul><li>select negative anchor boxes with top loss values </li><li>n &amp; p ratioï¼š3:1</li></ul></li><li>Loss Function<ul><li>ARM loss<ul><li>binary classï¼šåªè®¡ç®—æ­£æ ·æœ¬ï¼Ÿï¼Ÿï¼Ÿ</li><li>boxï¼šåªè®¡ç®—æ­£æ ·æœ¬</li></ul></li><li>ODM loss<ul><li>pass the refined anchors with the negative confidence less than the threshold </li><li>multi-classï¼šè®¡ç®—å‡è¡¡çš„æ­£è´Ÿæ ·æœ¬</li><li>boxï¼šåªè®¡ç®—æ­£æ ·æœ¬</li></ul></li><li>æ­£æ ·æœ¬æ•°ä¸º0çš„æ—¶å€™ï¼Œlosså‡ä¸º0ï¼šçº¯é˜´æ€§æ ·æœ¬æ— æ•ˆï¼Ÿï¼Ÿ</li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ç›®æ ‡æ£€æµ‹ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CPNDet</title>
      <link href="/2021/01/05/CPNDet/"/>
      <url>/2021/01/05/CPNDet/</url>
      <content type="html"><![CDATA[<h2 id="Corner-Proposal-Network-for-Anchor-free-Two-stage-Object-Detection"><a href="#Corner-Proposal-Network-for-Anchor-free-Two-stage-Object-Detection" class="headerlink" title="Corner Proposal Network for Anchor-free, Two-stage Object Detection"></a>Corner Proposal Network for Anchor-free, Two-stage Object Detection</h2><ol><li><p>åŠ¨æœº</p><ul><li>anchor-free</li><li>two-stage<ul><li>å…ˆæ‰¾potential corner keypoints</li><li>classify each proposal</li></ul></li><li>corner-basedæ–¹æ³•ï¼šå¯¹äºobjects of various scalesæœ‰æ•ˆï¼Œåœ¨è®­ç»ƒä¸­é¿å…äº§ç”Ÿè¿‡å¤šçš„å†—ä½™false-positive proposalsï¼Œä½†æ˜¯åœ¨ç»“æœä¸Šä¼šå‡ºç°æ›´å¤šçš„fp</li><li>å¾—åˆ°çš„æ˜¯competitive results</li></ul></li><li><p>è®ºç‚¹</p><ul><li>anchor-based methodså¯¹å½¢çŠ¶å¥‡æ€ªçš„ç›®æ ‡å®¹æ˜“æ¼æ£€</li><li><p>anchor-free methodså®¹æ˜“å¼•å…¥å‡é˜³caused by mistakely grouping</p><ul><li>thus an individual classifier is strongly required  </li></ul></li><li><p>Corner Proposal Network (CPN) </p><ul><li>use key-point detection in CornerNet</li><li>ä½†æ˜¯groupé˜¶æ®µä¸å†ç”¨embedding distanceè¡¡é‡ï¼Œè€Œæ˜¯ç”¨a binary classifier </li><li>ç„¶åæ˜¯multi-class classifierï¼Œoperate on the survived objects</li><li>æœ€åsoft-NMS</li></ul></li><li></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ç›®æ ‡æ£€æµ‹ï¼Œtwo-stageï¼Œanchor-free </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>refineNet</title>
      <link href="/2021/01/05/refineNet/"/>
      <url>/2021/01/05/refineNet/</url>
      <content type="html"><![CDATA[<h2 id="RefineNet-Multi-Path-Refinement-Networks-for-High-Resolution-Semantic-Segmentation"><a href="#RefineNet-Multi-Path-Refinement-Networks-for-High-Resolution-Semantic-Segmentation" class="headerlink" title="RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation"></a>RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</h2><p>å¼•ç”¨é‡1452ï¼Œä½†æ˜¯æ²¡æœ‰å‡ ç¯‡æŠ€æœ¯åšå®¢ï¼Ÿï¼Ÿ</p><ol><li><p>åŠ¨æœº</p><ul><li>è¯­ä¹‰åˆ†å‰²<ul><li>dense classification on every single pixel</li></ul></li><li>refineNet<ul><li>long-range residual connections</li><li>chained residual pooling</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>pooling/conv strideï¼š<ul><li>losing finer image structure</li><li>deconv is not able to recover the lost info</li></ul></li><li>atrous<ul><li>high resoï¼šlarge computation</li><li>dilated convï¼šcoarse sub-sampling of feature</li></ul></li><li>FCN<ul><li>fuse features from all levels </li><li>stage-wise rather than end-to-end???å­˜ç–‘</li></ul></li><li><p>this paper</p><ul><li>main ideaï¼šeffectively exploit middle layer features </li><li>RefineNet<ul><li>fuse all level feature</li><li>residual connections with identity skip</li><li>chained residual pooling to capture background contextï¼šçœ‹æè¿°æ„Ÿè§‰åƒinception downsamp</li><li>end-to-end</li><li>æ˜¯æ•´ä¸ªåˆ†å‰²ç½‘ç»œä¸­çš„ä¸€ä¸ªcomponent</li></ul></li></ul><p><img src="/2021/01/05/refineNet/dense.png" width="80%;"></p></li></ul></li><li><p>æ–¹æ³•</p><p> <img src="/2021/01/05/refineNet/refineNet.png" width="70%;"></p><ul><li>backbone<ul><li>pretrained resnet</li><li>4 blocksï¼šx4 - x32ï¼Œeach blockï¼špool-residual</li><li>connectionï¼šæ¯ä¸ªè¾“å‡ºè¿æ¥ä¸€ä¸ªRefineNet unit</li></ul></li><li>4-cascaded architecture<ul><li>final ouputï¼š<ul><li>high-resolution feature maps</li><li>dense soft-max </li><li>bilinear interpolation to origin resolution</li></ul></li><li>cascade inputs<ul><li>output from backbone block</li><li>ouput from previous refineNet block</li></ul></li></ul></li><li>refineNet block<ul><li>adapt convï¼š<ul><li>to adapt the dimensionality and refine special task</li><li>BN layers are removed</li><li>channel 512 for R4ï¼Œchannel 256 for the rest</li></ul></li><li>fusionï¼š<ul><li>å…ˆç”¨conv to adapt dimension and recale the paths</li><li>ç„¶åupsamp</li><li>summation </li><li>å¦‚æœsingle inputï¼šwalk through and stay unchanged</li></ul></li><li>chained residual poolingï¼š<ul><li>aims to capture background context from a large image region </li><li>chainedï¼šefficiently pool features with multiple window sizes  </li><li>pooling blocksï¼šs1 maxpooling+conv</li><li>in practiceç”¨äº†ä¸¤ä¸ªpooling blocks</li><li>use one ReLU in the chained residual pooling block</li></ul></li><li>output convï¼š<ul><li>ä¸€ä¸ªresidualï¼što employ non-linearity</li><li>dimension remains unchanged</li><li>final levelï¼štwo additional RCUs before the final softmax prediction </li></ul></li></ul></li><li>residual identity mappings <ul><li>a clean information path not block by any non-linearityï¼šæ‰€æœ‰reluéƒ½åœ¨residual pathé‡Œé¢</li><li>åªæœ‰chained residual poolingæ¨¡å—èµ·å§‹æ—¶å€™æœ‰ä¸ªReLUï¼šone single ReLU in each RefineNet block does not noticeably reduce the effectiveness of gradient flow </li><li>linear operationsï¼š<ul><li>within the fusion block </li><li>dimension reduction operations</li><li>upsamp operations</li></ul></li></ul></li></ul></li><li><p>å…¶ä»–ç»“æ„</p><p> <img src="/2021/01/05/refineNet/vari.png" width="70%;"></p><ul><li>çº§è”çš„å°±å«cascaded</li><li>ä¸€ä¸ªblockå°±å«single</li><li>å¤šä¸ªinput resolutionå°±å«mult-scale</li></ul></li><li><p>å®éªŒ</p><ul><li>4-cascaded works better than 1-cas &amp; 2-cas</li><li>2-scale works better than 1-scale</li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> è¯­ä¹‰åˆ†å‰² </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>centerNet</title>
      <link href="/2020/12/29/centerNet/"/>
      <url>/2020/12/29/centerNet/</url>
      <content type="html"><![CDATA[<p>[papers]</p><ul><li><p>[centerNet] çœŸcenterNet: Objects as Pointsï¼Œutexasï¼Œè¿™ä¸ªæ˜¯çœŸçš„centerNetï¼ŒåŸºäºåˆ†å‰²æ¶æ„ï¼Œé¢„æµ‹ä¸­å¿ƒç‚¹çš„heatmapï¼Œä»¥åŠ2-Nä¸ªchannelå…¶ä»–ç›¸å…³å‚æ•°çš„å›å½’</p></li><li><p>[cornet-centerNet] centerNet: Keypoint Triplets for Object Detectionï¼Œè¿™ä¸ªæŠ¢å…ˆå«äº†centerNetï¼Œä½†æ˜¯æˆ‘è§‰å¾—å«corner-centerNetæ›´åˆé€‚ï¼Œå®ƒæ˜¯åŸºäºcornerNetè¡ç”Ÿçš„ï¼Œåœ¨cornerNetçš„åŸºç¡€ä¸Šå†åŠ ä¸€åˆ€åˆ¤å®šï¼ŒåŸºäºè§’ç‚¹pairçš„ä¸­å¿ƒç‚¹æ˜¯å¦æ˜¯å‰æ™¯æ¥å†³å®šæ˜¯å¦ä¿ç•™è¿™ä¸ªæ¡†</p></li><li><p>[centerNet2] Probabilistic two-stage detectionï¼Œutexasï¼Œ</p></li></ul><h2 id="centerNet-Objects-as-Points"><a href="#centerNet-Objects-as-Points" class="headerlink" title="centerNet: Objects as Points"></a>centerNet: Objects as Points</h2><ol><li><p>åŠ¨æœº</p><ul><li><p>anchor-based</p><ul><li>exhaustive list of potential locations</li><li>wasteful, inefficient, requires additional post-processing</li></ul></li><li><p>our detector</p><ul><li>centerï¼šuse keypoint estimation to find center points</li><li>other propertiesï¼šregress</li></ul></li><li><p>tasks</p><ul><li>object detection</li><li>3d object detection</li><li><p>multi-person human pose estimation </p><p><img src="/2020/12/29/centerNet/tasks.png" width="40%;"></p></li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>ç›¸æ¯”è¾ƒäºä¼ ç»Ÿä¸€é˜¶æ®µã€äºŒé˜¶æ®µæ£€æµ‹<ul><li>anchorï¼š<ul><li>box &amp; kpï¼šä¸€ä¸ªæ˜¯æ¡†ï¼Œä¸€ä¸ªæ˜¯å‡»ä¸­æ ¼å­</li><li>nmsï¼štake local peaksï¼Œno need of nms</li><li>larger resolutionï¼šhourglassæ¶æ„ï¼Œè¾“å‡ºx4çš„heatmapï¼Œeliminates the need for multiple anchors </li></ul></li></ul></li><li>ç›¸æ¯”è¾ƒäºkey point estimantion network<ul><li>themï¼šrequire grouping stage</li><li>ourï¼šåªå®šä½ä¸€ä¸ªcenter pointï¼Œno need for group or post-processing</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>loss</p><ul><li><p>å…³é”®ç‚¹loss</p><ul><li><p>center pointå…³é”®ç‚¹å®šä¹‰ï¼šæ¯ä¸ªç›®æ ‡çš„gt pointåªæœ‰ä¸€ä¸ªï¼Œä»¥å®ƒä¸ºä¸­å¿ƒï¼Œåšobject size-adaptiveçš„é«˜æ–¯penalty reductionï¼Œoverlapçš„åœ°æ–¹å–max</p></li><li><p>focal lossï¼šåŸºæœ¬ä¸cornetNetä¸€è‡´</p><script type="math/tex; mode=display">  L_k = \frac{-1}{N}\sum_{x,y,c}  \begin{cases}  (1-\hat Y)^\alpha log(\hat Y), if Y=1\\  (1-Y)^\beta \hat Y^\alpha log(1-\hat Y), otherwise  \end{cases}</script><ul><li>$\alpha=2, \beta=4$</li><li>background pointsæœ‰penaltyï¼Œæ ¹æ®gtçš„é«˜æ–¯è¡°å‡æ¥çš„</li></ul></li></ul></li><li><p>offset loss</p><ul><li>åªæœ‰ä¸¤ä¸ªé€šé“(x_offset &amp; y_offset)ï¼šshared among categories</li><li>gtçš„offsetæ˜¯åŸå§‹resolution/output strideå‘ä¸‹å–æ•´å¾—åˆ°</li><li>L1 loss</li></ul></li></ul></li><li><p>centerNet</p><ul><li><p>output</p><ul><li>ç¬¬ä¸€ä¸ªéƒ¨åˆ†ï¼šä¸­å¿ƒç‚¹ï¼Œ[h,w,c]ï¼Œbinary mask for each category</li><li>ç¬¬äºŒä¸ªéƒ¨åˆ†ï¼šoffsetï¼Œ[h,w,2]ï¼Œshared among</li><li>ç¬¬ä¸‰ä¸ªéƒ¨åˆ†ï¼šsizeï¼Œ[h,w,2]ï¼Œshared among<ul><li>L1 lossï¼Œuse raw pixel coordinates</li></ul></li><li>overall<ul><li>C+4 channelsï¼Œè·Ÿä¼ ç»Ÿæ£€æµ‹çš„formulationæ˜¯ä¸€è‡´çš„ï¼Œåªä¸è¿‡ä¼ ç»Ÿæ£€æµ‹gtæ˜¯åŸºäºanchorè®¡ç®—çš„ç›¸å¯¹å€¼ï¼Œæœ¬æ–‡ç›´æ¥å›å½’ç»å¯¹å€¼</li><li>$L_{det} = L_k + \lambda_{size} L_{size} + \lambda_{off} L_{off}$</li><li>å…¶ä»–taskçš„formulationçœ‹ç¬¬ä¸€å¼ å›¾</li></ul></li></ul></li><li><p>inference workflow</p><ul><li>local peaksï¼š<ul><li>for each category channel</li><li>all responses greater or equal to its 8-connected neighborsï¼š3x3 max pooling</li><li>keep the top100</li></ul></li><li>generate bounding boxes<ul><li>ç»„åˆoffset &amp; size predictions</li><li>ï¼Ÿï¼Ÿï¼Ÿï¼Ÿæ²¡æœ‰åå¤„ç†äº†ï¼Ÿï¼Ÿï¼Ÿå‡é˜³ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ</li></ul></li></ul></li><li><p>encoder-decoder backboneï¼šx4</p><ul><li>hourglass104<ul><li>stemï¼šx4</li><li>modulesï¼šä¸¤ä¸ª</li></ul></li><li>resnet18/101+deformable conv upsampling<ul><li>3x3 deformable conv, 256/128/64</li><li>bilinear interpolation</li></ul></li><li><p>DLA34+deformable conv upsampling</p><p><img src="/2020/12/29/centerNet/backs.jpg" width="80%;"></p></li></ul></li><li><p>heads</p><ul><li>independent heads</li><li>one 3x3 convï¼Œ256</li><li>1x1 conv for prediction</li></ul></li></ul></li></ul></li><li><p>æ€»ç»“</p><p> ä¸ªäººæ„Ÿè§‰ï¼ŒcenterNetå’Œanchor-basedçš„formulationå…¶å®æ˜¯ä¸€æ ·çš„ï¼Œ</p><ul><li>centerçš„å›å½’å¯¹æ ‡confidenceçš„å›å½’ï¼ŒåŒºåˆ«åœ¨äºé«˜æ–¯/[0,1]/[0,-1,1]</li><li>sizeçš„å›å½’å˜æˆäº†raw pixelï¼Œä¸å†åŸºäºanchor</li><li>hourglassç»“æ„å°±æ˜¯fpnï¼Œçº§è”çš„hourglasså¯ä»¥å¯¹æ ‡bi-fpn</li><li>å¤šå°ºåº¦å˜æˆäº†å•ä¸€å¤§resolutionç‰¹å¾å›¾ï¼Œä¹Ÿå¯ä»¥ç”¨å¤šå°ºåº¦é¢„æµ‹ï¼Œéœ€è¦åŠ NMS</li></ul></li></ol><h2 id="centerNet2-Probabilistic-two-stage-detection"><a href="#centerNet2-Probabilistic-two-stage-detection" class="headerlink" title="centerNet2: Probabilistic two-stage detection"></a>centerNet2: Probabilistic two-stage detection</h2><ol><li><p>åŠ¨æœº</p><ul><li>two-stage</li><li>probabilistic interpretation</li><li>the suggested pipeline<ul><li>stage1ï¼šinfer proper object-backgroud likelihoodï¼Œä¸“æ³¨å‰èƒŒæ™¯åˆ†ç¦»</li><li>stage2ï¼šinform the overall score</li></ul></li><li>verified on COCO<ul><li>faster and more accurate than both one and two stage detectors</li><li>outperform yolov4</li><li>extreme large modelï¼š56.4 mAP</li><li>standard ResNeXt- 32x8d-101-DCN backï¼š50.2 mAP</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>one-stage detectors<ul><li>dense predict</li><li>jointly predict class &amp; location</li><li>anchor-basedï¼šRetinaNetç”¨focal lossæ¥deal with å‰èƒŒæ™¯imbalance</li><li>anchor-freeï¼šFCOS &amp; CenterNetä¸åŸºäºanchoråŸºäºgridï¼Œç¼“è§£imbalance</li><li>deformable convï¼šAlignDetåœ¨outputå‰é¢åŠ ä¸€å±‚deformable conv to get richer features</li><li>sound probablilistic interpretation</li><li>heavier separate classification and regression branches than two-stage modelsï¼šå¦‚æœç±»åˆ«ç‰¹åˆ«å¤šçš„æƒ…å†µï¼Œå¤´ä¼šéå¸¸é‡ï¼Œä¸¥é‡å½±å“æ€§èƒ½</li><li>misaligned issueï¼šä¸€é˜¶æ®µé¢„æµ‹æ˜¯åŸºäºlocal featureï¼Œæ„Ÿå—é‡ã€anchor settingséƒ½ä¼šå½±å“ä¸ç›®æ ‡çš„å¯¹é½ç¨‹åº¦</li></ul></li><li>two-stage detectors<ul><li>first RPN generates coarse object proposals</li><li>then per-region head to classify and refine</li><li>ROI headsï¼šFaster-RCNNç”¨äº†ä¸¤ä¸ªfcå±‚ä½œä¸ºROI heads</li><li>cascadedï¼šCascadeRCNNç”¨äº†ä¸‰ä¸ªè¿ç»­çš„Faster-RCNNï¼Œwith a different positive threshold</li><li>semantic branchï¼šHTCç”¨äº†é¢å¤–çš„åˆ†å‰²åˆ†æ”¯enhance the inter-stage feature flow</li><li>decoupleï¼šTSDå°†cls&amp;posä¸¤ä¸ªROI headsè§£è€¦</li><li>weak RPNï¼šå› ä¸ºå°½å¯èƒ½æå‡å¬å›ç‡ï¼Œproposal scoreä¹Ÿä¸å‡†ï¼Œä¸§å¤±äº†ä¸€ä¸ªclearçš„probabilistic interpretation</li><li>independent probabilistic interpretationï¼šä¸¤ä¸ªé˜¶æ®µå„è®­å„çš„ï¼Œæœ€åçš„cls scoreä»…ç”¨ç¬¬äºŒé˜¶æ®µçš„</li><li>slowï¼šproposalså¤ªå¤šäº†æ‰€ä»¥slow down</li></ul></li><li>other detectors<ul><li>point-basedï¼šcornetNeté¢„æµ‹&amp;ç»„åˆä¸¤ä¸ªè§’ç‚¹ï¼ŒcenterNeté¢„æµ‹ä¸­å¿ƒç‚¹å¹¶åŸºäºå®ƒå›å½’é•¿å®½</li><li>transformerï¼šDETRç›´æ¥é¢„æµ‹a set of bounding boxesï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„ç»“æ„åŒ–çš„dense output</li></ul></li><li><p>ç½‘ç»œç»“æ„</p><ul><li>one/two-stage detectorsï¼šimage classification network + lightweight upsampling layers + heads</li><li>point-basedï¼šFCNï¼Œæœ‰symmetric downsampling and upsampling layerï¼Œé¢„æµ‹ä¸€ä¸ªå°strideçš„heatmap</li><li>DETRï¼šfeature extraction + transformer decoder</li></ul></li><li><p>our method</p><p>  <img src="/2020/12/29/centerNet/centerNet2.png" width="40%;"></p><ul><li>ç¬¬ä¸€ä¸ªé˜¶æ®µ<ul><li>åšäºŒåˆ†ç±»çš„one-stage detectorï¼Œæå‰æ™¯ï¼Œ</li><li>å®ç°ä¸Šå°±ç”¨region-level feature+classifierï¼ˆFCN-basedï¼‰</li></ul></li><li>ç¬¬äºŒé˜¶æ®µ<ul><li>åšposition-basedç±»åˆ«é¢„æµ‹</li><li>å®ç°ä¸Šæ—¢å¯ä»¥ç”¨ä¸€ä¸ªFaster-RCNNï¼Œä¹Ÿå¯ä»¥ç”¨classifier</li></ul></li><li>æœ€ç»ˆçš„lossç”±ä¸¤ä¸ªé˜¶æ®µåˆå¹¶å¾—åˆ°ï¼Œè€Œä¸æ˜¯åˆ†é˜¶æ®µè®­ç»ƒ</li><li>è·Ÿformer two-stage frameworkçš„ä¸»è¦ä¸åŒæ˜¯<ul><li>åŠ äº†joint probabilistic objective over both stages</li><li>ä»¥å‰çš„äºŒé˜¶æ®µRPNçš„ç”¨é€”ä¸»è¦æ˜¯æœ€å¤§åŒ–recallï¼Œdoes not produce accurate likelihoods</li></ul></li><li>faster and more accurate<ul><li>é¦–å…ˆæ˜¯ç¬¬ä¸€ä¸ªé˜¶æ®µçš„proposalæ›´å°‘æ›´å‡†</li><li>å…¶æ¬¡æ˜¯ç¬¬äºŒä¸ªé˜¶æ®µmakes full use of years of progress in two-stage detectionï¼ŒäºŒé˜¶æ®µçš„è®¾è®¡ç«™åœ¨ä¼Ÿäººçš„è‚©è†€ä¸Š</li></ul></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li>joint class distributionï¼šå…ˆä»‹ç»æ€ä¹ˆå°†ä¸€äºŒé˜¶æ®µè”åŠ¨<ul><li>ã€ç¬¬ä¸€é˜¶æ®µçš„å‰èƒŒæ™¯scoreã€‘ ä¹˜ä¸Š ã€ç¬¬äºŒé˜¶æ®µçš„class scoreã€‘</li><li>$P(C_k) = \sum_o P(C_k|O_k=o)P(O_k=o)$</li><li>maximum likelihood estimation<ul><li>for annotated objects<ul><li>é€€é˜¶æˆindependent maximum-likelihood</li><li>$log P(C_k) = log P(C_k|O_k=1) + log P(O_k=1)$</li></ul></li><li>for background class<ul><li>ä¸åˆ†è§£</li><li>$log P(bg) = log( P(bg|O_k=1) * P(O_k=1) + P(O_k=0))$</li><li>lower boundsï¼ŒåŸºäºjensenä¸ç­‰å¼å¾—åˆ°ä¸¤ä¸ªä¸ç­‰å¼</li><li>$log P(bg) \ge P(O_k=1) * log( P(bg|O_k=1))$ï¼šå¦‚æœä¸€é˜¶æ®µå‰æ™¯ç‡è´¼å¤§ï¼Œé‚£ä¹ˆå°±</li><li>$log P(bg) \ge P(O_k=0)$ï¼š</li><li>optimize both bounds jointly works better</li></ul></li></ul></li></ul></li><li><p>network designï¼šä»‹ç»æ€ä¹ˆåœ¨one-stage detectorçš„åŸºç¡€ä¸Šæ”¹é€ å‡ºä¸€ä¸ªtwo-stage probabilistic detector</p><ul><li>experiment with 4 different designs for first-stage RPN</li><li>RetinaNet<ul><li>RetinaNetå…¶å®å’Œtwo-stageçš„RPNé«˜åº¦ç›¸ä¼¼æ ¸å¿ƒåŒºåˆ«åœ¨äºï¼š<ul><li>a heavier head designï¼š4-conv vs 1-conv<ul><li>RetinaNetæ˜¯backbone+fpn+individual heads</li><li>RPNæ˜¯backbone+fpn+shared convs+individual heads</li></ul></li><li>a stricter positive and negative anchor definitionï¼šéƒ½æ˜¯IoU-based anchor selectionï¼Œthreshä¸ä¸€æ ·</li><li>focal loss</li></ul></li><li>first-stage design<ul><li>ä»¥ä¸Šä¸‰ç‚¹éƒ½åœ¨probabilistic modelé‡Œé¢ä¿ç•™</li><li>ç„¶åå°†separated headsæ”¹æˆshared heads</li></ul></li></ul></li><li>centerNet<ul><li>æ¨¡å‹å‡çº§<ul><li>å‡çº§æˆmulti-scaleï¼šuse ResNet-FPN backï¼ŒP3-P7</li><li>å¤´æ˜¯FCOSé‚£ç§å¤´ï¼šindividual headsï¼Œä¸share convï¼Œç„¶åcls branché¢„æµ‹centerness+clsï¼Œreg branché¢„æµ‹regress params</li><li>æ­£æ ·æœ¬ä¹Ÿæ˜¯æŒ‰ç…§FCOSç­–ç•¥ï¼šposition &amp; scale-based</li></ul></li><li>å‡çº§æ¨¡å‹è¿›è¡Œone-stage &amp; two-stageå®éªŒï¼šcenterNet*</li></ul></li><li>ATSS<ul><li>æ˜¯ä¸€ä¸ªadaptive IoU threshçš„æ–¹æ³•ï¼Œcenternessæ¥è¡¨ç¤ºä¸€ä¸ªæ ¼å­çš„score</li><li>æˆ‘ä»¬å°†centerness*classification scoreå®šä¹‰ä¸ºè¿™ä¸ªæ¨¡å‹çš„proposal score</li><li>å¦å¤–å°±æ˜¯two-stageä¸‹è¿˜æ˜¯å°†RPNçš„cls &amp; reg headsåˆå¹¶</li></ul></li><li>GFLï¼šè¿˜æ²¡çœ‹è¿‡ï¼Œå…ˆè·³è¿‡å§</li><li>second-stage designsï¼šFasterRCNN &amp; CascadeR- CNN</li><li>deformable convï¼šè¿™ä¸ªåœ¨centerNetv1çš„ResNetå’ŒDLA backé‡Œé¢éƒ½ç”¨äº†ï¼Œåœ¨v2é‡Œé¢ï¼Œä¸»è¦æ˜¯ç”¨ResNeXt-32x8d-101-DCNï¼Œ</li></ul></li><li><p>hyperparameters for two-stage probabilistic model</p><ul><li>ä¸¤é˜¶æ®µæ¨¡å‹é€šå¸¸æ˜¯ç”¨P2-P6ï¼Œä¸€é˜¶æ®µé€šå¸¸ç”¨P3-P7ï¼šæˆ‘ä»¬ç”¨P3-P7</li><li>increase the positive IoU thresholdï¼š0.5 to [0.6,0.7,0.8]</li><li>maximum of 256 proposals ï¼ˆå¯¹æ¯”originçš„1kï¼‰</li><li>increase nms threshold from 0.5 to 0.7</li><li>SGDï¼Œ90K iterations</li><li>base learning rateï¼š0.02 for two-stage &amp; 0.01 for one-stageï¼Œ0.1 decay</li><li>multi-scale trainingï¼šçŸ­è¾¹[640,800]ï¼Œé•¿è¾¹ä¸è¶…è¿‡1333</li><li>fix-scale testingï¼šçŸ­è¾¹ç”¨800ï¼Œé•¿è¾¹ä¸è¶…è¿‡1333</li><li>first stage loss weightï¼š0.5ï¼Œå› ä¸ºone-stage detectoré€šå¸¸ç”¨0.01 lrå¼€å§‹è®­ç»ƒ</li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li><p>4ç§designçš„å¯¹æ¯”</p><ul><li>æ‰€æœ‰çš„probabilistic modeléƒ½æ¯”one-stage modelå¼ºï¼Œç”šè‡³è¿˜å¿«ï¼ˆå› ä¸ºç®€åŒ–äº†è„‘è¢‹ï¼‰</li><li>æ‰€æœ‰çš„probabilistic FasterRCNNéƒ½æ¯”åŸå§‹çš„RPN-based FasterRCNNå¼ºï¼Œä¹Ÿå¿«ï¼ˆå› ä¸ºP3-P7æ¯”P2-P6çš„è®¡ç®—é‡å°ä¸€åŠï¼Œè€Œä¸”ç¬¬äºŒé˜¶æ®µfewer proposalsï¼‰</li><li><p>CascadeRCNN-CenterNet design performs the bestï¼šæ‰€ä»¥ä»¥åå°±æŠŠå®ƒå«CenterNet2</p><p><img src="/2020/12/29/centerNet/designs.png" width="40%;"></p></li></ul></li><li><p>å’Œå…¶ä»–real-time modelså¯¹æ¯”</p><ul><li>å¤§å¤šæ˜¯real-time modelséƒ½æ˜¯ä¸€é˜¶æ®µæ¨¡å‹</li><li><p>å¯ä»¥çœ‹åˆ°äºŒé˜¶æ®µä¸ä»…èƒ½å¤Ÿæ¯”ä¸€é˜¶æ®µæ¨¡å‹è¿˜å¿«ï¼Œç²¾åº¦è¿˜æ›´é«˜</p><p><img src="/2020/12/29/centerNet/real-time.png" width="40%;"></p></li></ul></li><li><p>SOTAå¯¹æ¯”</p><ul><li>æŠ¥äº†ä¸€ä¸ª56.4%çš„sotaï¼Œä½†æ˜¯å¤§å®¶å£ç¢‘ä¸Šå¥½åƒæ•ˆæœå¾ˆå·®</li><li>ä¸æ”¾å›¾äº†</li></ul></li></ul></li></ol><h2 id="corner-centerNet-Keypoint-Triplets-for-Object-Detection"><a href="#corner-centerNet-Keypoint-Triplets-for-Object-Detection" class="headerlink" title="corner-centerNet: Keypoint Triplets for Object Detection"></a>corner-centerNet: Keypoint Triplets for Object Detection</h2><ol><li><p>åŠ¨æœº</p><ul><li><p>based on cornerNet</p></li><li><p>triplet</p><ul><li>corner keypointsï¼šweak grouping ability cause false positives</li><li><p>correct predictions can be determined by checking the central parts</p><p><img src="/2020/12/29/centerNet/center.png" width="40%;"></p></li></ul></li><li><p>cascade corner pooling and center poolling</p></li></ul></li><li><p>è®ºç‚¹</p><ul><li>whats new in CenterNet<ul><li>triplet inference workflow<ul><li>after a proposal is generated as a pair of corner keypoints</li><li>checking if there is a center keypoint of the same class </li></ul></li><li>center pooling<ul><li>for predicting center keypoints </li><li>by making the center keypoints on feature map having the max sum Hori+Verti responses</li></ul></li><li>cascade corner pooling<ul><li>equips the original corner pooling module with the ability of perceiving internal information </li><li>not only consider the boundary but also the internal directions </li></ul></li></ul></li><li>CornetNetç—›ç‚¹<ul><li>fp rateé«˜</li><li>small objectçš„fp rateå°¤å…¶é«˜</li><li>ä¸€ä¸ªideaï¼šcornerNet based RPN<ul><li>ä½†æ˜¯åŸç”ŸRPNéƒ½æ˜¯å¤ç”¨çš„</li><li>è®¡ç®—æ•ˆç‡ï¼Ÿ</li></ul></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>center pooling</p><ul><li>geometric centers &amp; semantic centers</li><li><p>center poolingèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†è¯­ä¹‰ä¿¡æ¯æœ€ä¸°å¯Œçš„ç‚¹ï¼ˆsemantic centersï¼‰ä¼ è¾¾åˆ°ç‰©ç†ä¸­å¿ƒç‚¹ï¼ˆgeometric centersï¼‰ï¼Œä¹Ÿå°±æ˜¯central region</p><p><img src="/2020/12/29/centerNet/central.png" width="40%;"></p></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ç›®æ ‡æ£€æµ‹ï¼Œanchor-free, one-stage&amp;two-stage </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>equlization loss</title>
      <link href="/2020/12/21/equlization-loss/"/>
      <url>/2020/12/21/equlization-loss/</url>
      <content type="html"><![CDATA[]]></content>
      
      
    </entry>
    
    <entry>
      <title>megDet</title>
      <link href="/2020/12/18/megDet/"/>
      <url>/2020/12/18/megDet/</url>
      <content type="html"><![CDATA[<h2 id="MegDet-A-Large-Mini-Batch-Object-Detector"><a href="#MegDet-A-Large-Mini-Batch-Object-Detector" class="headerlink" title="MegDet: A Large Mini-Batch Object Detector"></a>MegDet: A Large Mini-Batch Object Detector</h2><ol><li><p>åŠ¨æœº</p><ul><li>past methods mainly come from novel framework or loss design</li><li><p>this paper studies the mini-batch size</p><ul><li>enable training with a large mini-batch size</li><li>warmup learning rate policy</li><li>cross-gpu batch normalization</li></ul></li><li><p>faster &amp; better acc</p></li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>potential drawbacks with small mini-batch sizes</p><ul><li><p>long training time</p></li><li><p>inaccurate statistics for BNï¼šprevious methods use fixed statistics from ImageNet which is a sub-optimal trade-off</p></li><li><p>positive &amp; negative training examples are more likely imblanced</p></li><li><p>åŠ å¤§batch sizeä»¥åï¼Œæ­£è´Ÿæ ·æœ¬æ¯”ä¾‹æœ‰æå‡ï¼Œæ‰€ä»¥yolov3ä¼šå…ˆé”ç€backå¼€å¤§batchsizeåšwarmup</p><p>  <img src="/2020/12/18/megDet/ratio.png" width="40%;"></p></li></ul></li><li><p>learning rate dilemma</p><ul><li>large min-batch size usually requires large learning rate </li><li>large learning rate is likely leading to convergence failure</li><li>a smaller learning rate often obtains inferior results </li></ul></li><li><p>solution of the paper</p><ul><li>linear scaling rule </li><li>warmup</li><li>Cross-GPU Batch Normalization (CGBN) </li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>warmup</p><ul><li>set up the learning rate small enough at the be- ginning</li><li>then increase the learning rate with a constant speed after every iteration, until fixed</li></ul></li><li><p>Cross-GPU Batch Normalization </p><ul><li>ä¸¤æ¬¡åŒæ­¥</li><li><p>tensorpacké‡Œé¢æœ‰</p><p><img src="/2020/12/18/megDet/CGBN.png" width="40%;"></p><p><img src="/2020/12/18/megDet/Cross-GPU.png" width="40%;"></p></li></ul></li></ul></li><li><p>ä¸€æ¬¡åŒæ­¥</p><ul><li><p>å¼‚æ­¥BNï¼šbatch size è¾ƒå°æ—¶ï¼Œæ¯å¼ å¡è®¡ç®—å¾—åˆ°çš„ç»Ÿè®¡é‡å¯èƒ½ä¸æ•´ä½“æ•°æ®æ ·æœ¬å…·æœ‰è¾ƒå¤§å·®å¼‚</p><p> <img src="/2020/12/18/megDet/unsync.png" width="40%;"></p></li><li><p>åŒæ­¥ï¼š</p></li><li><p>éœ€è¦åŒæ­¥çš„æ˜¯æ¯å¼ å¡ä¸Šè®¡ç®—çš„ç»Ÿè®¡é‡ï¼Œå³BNå±‚ç”¨åˆ°çš„å‡å€¼$\mu$å’Œæ–¹å·®$\sigma^2$</p></li><li><p>è¿™æ ·å¤šå¡è®­ç»ƒç»“æœæ‰ä¸å•å¡è®­ç»ƒæ•ˆæœç›¸å½“</p></li><li><p>ä¸¤æ¬¡åŒæ­¥ï¼š</p></li><li><p>ç¬¬ä¸€æ¬¡åŒæ­¥å‡å€¼ï¼šè®¡ç®—å…¨å±€å‡å€¼</p></li><li><p>ç¬¬äºŒæ¬¡åŒæ­¥æ–¹å·®ï¼šåŸºäºå…¨å±€å‡å€¼è®¡ç®—å„è‡ªæ–¹å·®ï¼Œå†å–å¹³å‡</p></li><li><p>ä¸€æ¬¡åŒæ­¥ï¼š</p><ul><li><p>æ ¸å¿ƒåœ¨äºæ–¹å·®çš„è®¡ç®—</p></li><li><p>é¦–å…ˆå‡å€¼ï¼š$\mu = \frac{1}{m} \sum_{i=1}^m x_i$</p><ul><li>ç„¶åæ˜¯æ–¹å·®ï¼š<script type="math/tex; mode=display">  \sigma^2 = \frac{1}{m} \sum_{i=1}^m (x_i-\mu)^2 = \frac{1}{m} \sum_{i=1}^m x_i^2 - \mu^2\\  =\frac{1}{m} \sum_{i=1}^m x_i^2 - (\frac{1}{m} \sum_{i=1}^m x_i)^2</script></li></ul></li></ul></li></ul></li></ol><pre><code>  * è®¡ç®—æ¯å¼ å¡çš„$\sum x_i$å’Œ$\sum x_i^2$ï¼Œå°±å¯ä»¥ä¸€æ¬¡æ€§ç®—å‡ºæ€»å‡å€¼å’Œæ€»æ–¹å·®</code></pre>]]></content>
      
      
        <tags>
            
            <tag> ç›®æ ‡æ£€æµ‹ï¼Œlarge mini-batch </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>RFB</title>
      <link href="/2020/12/16/RFB/"/>
      <url>/2020/12/16/RFB/</url>
      <content type="html"><![CDATA[<h2 id="RFB-Receptive-Field-Block-Net-for-Accurate-and-Fast-Object-Detection"><a href="#RFB-Receptive-Field-Block-Net-for-Accurate-and-Fast-Object-Detection" class="headerlink" title="RFB: Receptive Field Block Net for Accurate and Fast Object Detection"></a>RFB: Receptive Field Block Net for Accurate and Fast Object Detection</h2><ol><li><p>åŠ¨æœº</p><ul><li>RF blockï¼šReceptive Fields </li><li>strengthen the lightweight features using a hand-crafted mechanismï¼šè½»é‡ï¼Œç‰¹å¾è¡¨è¾¾èƒ½åŠ›å¼º</li><li>assemble RFB to the top of SSD </li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>lightweight </p><ul><li>enhance feature representation </li></ul></li><li><p>äººç±»</p><ul><li>ç¾¤æ™ºæ„Ÿå—é‡ï¼ˆpRFï¼‰çš„å¤§å°æ˜¯å…¶è§†ç½‘è†œå›¾ä¸­åå¿ƒç‡çš„å‡½æ•°</li><li>æ„Ÿå—é‡éšç€åå¿ƒç‡è€Œå¢åŠ </li><li>æ›´é è¿‘ä¸­å¿ƒçš„åŒºåŸŸåœ¨è¯†åˆ«ç‰©ä½“æ—¶æ‹¥æœ‰æ›´é«˜çš„æ¯”é‡æˆ–ä½œç”¨</li><li><p>å¤§è„‘åœ¨å¯¹äºå°çš„ç©ºé—´å˜åŒ–ä¸æ•æ„Ÿ</p><p><img src="/2020/12/16/RFB/human.png" width="60%;"></p></li></ul></li><li><p>fixed sampling grid (conv)</p><ul><li>probably induces some loss in the feature discriminability as well as robustness </li></ul></li><li><p>inception</p><ul><li>RFs of multiple sizes </li><li>but at the same center</li></ul></li><li><p>ASPP</p><ul><li>with different atrous rates </li><li>the resulting feature tends to be less distinctive </li></ul></li><li><p>Deformable CNN </p><ul><li>sampling grid is flexible</li><li><p>but all pixels in an RF contribute equally </p><p><img src="/2020/12/16/RFB/RF.png" width="60%;"></p></li></ul></li><li><p>RFB</p><ul><li>varying kernel sizes</li><li>applies dilated convolution layers to control their eccentricities </li><li>ç»„åˆæ¥æ¨¡æ‹Ÿhuman visual system </li><li>concat</li><li><p>1x1 conv for fusion</p><p><img src="/2020/12/16/RFB/rfb.png" width="60%;"></p></li></ul></li><li><p>main contributions </p><ul><li>RFB module: enhance deep features of lightweight CNN networks</li><li>RFB Net: gain on SSD</li><li>assemble on MobileNet </li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>Receptive Field Block </p><ul><li>ç±»ä¼¼inceptionçš„multi-branch</li><li><p>dilated pooling or convolution layer</p><p><img src="/2020/12/16/RFB/RFBlock.png" width="80%;"></p></li></ul></li><li><p>RFB Net </p><ul><li><p>SSD-base</p><p>  <img src="/2020/12/16/RFB/ssd.png" width="80%;"></p></li><li><p>å¤´ä¸Šæœ‰è¾ƒå¤§åˆ†è¾¨ç‡çš„ç‰¹å¾å›¾çš„convå±‚are replaced by the RFB module</p></li><li><p>ç‰¹åˆ«å¤´ä¸Šçš„convå±‚å°±ä¿ç•™äº†ï¼Œå› ä¸ºtheir feature maps are too small to apply filters with large kernels like 5 Ã— 5</p><p><img src="/2020/12/16/RFB/rfbnet.png" width="70%;"></p></li><li><p>stride2 moduleï¼šæ¯ä¸ªconv stride2ï¼Œé‚£id pathå¾—å˜æˆ1x1 convï¼Ÿ</p></li></ul></li><li></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ç›®æ ‡æ£€æµ‹ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PANet</title>
      <link href="/2020/12/02/PANet/"/>
      <url>/2020/12/02/PANet/</url>
      <content type="html"><![CDATA[<h2 id="PANet-Path-Aggregation-Network-for-Instance-Segmentation"><a href="#PANet-Path-Aggregation-Network-for-Instance-Segmentation" class="headerlink" title="PANet: Path Aggregation Network for Instance Segmentation"></a>PANet: Path Aggregation Network for Instance Segmentation</h2><ol><li><p>åŠ¨æœº</p><ul><li>boost the information flow</li><li>bottom-up path<ul><li>shorten information path </li><li>enhance  accurate localization </li></ul></li><li>adaptive feature pooling <ul><li>aggregate all levels</li><li>avoiding arbitrarily assigned results </li></ul></li><li>mask prediction head<ul><li>fcn + fc</li><li>captures different views, possess complementary properties </li></ul></li><li>subtle extra computational</li></ul></li><li><p>è®ºç‚¹</p><ul><li>previous skills: fcn, fpn, residual, dense</li><li>findings<ul><li>é«˜å±‚ç‰¹å¾ç±»åˆ«å‡†ï¼Œåº•å±‚ç‰¹å¾å®šä½å‡†ï¼Œä½†æ˜¯é«˜å±‚å’Œåº•å±‚ç‰¹å¾ä¹‹é—´çš„pathå¤ªé•¿äº†ï¼Œä¸åˆ©äºåŒé«˜</li><li>past proposals make predictions based on one level</li></ul></li><li>PANet <ul><li>bottom-up path<ul><li>shorten information path </li><li>enhance  accurate localization </li></ul></li><li>adaptive feature pooling <ul><li>aggregate all levels</li><li>avoiding arbitrarily assigned results </li></ul></li><li>mask prediction head<ul><li>fcn + fc</li><li>captures different views, possess complementary properties </li></ul></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>framework</p><p> <img src="/2020/12/02/PANet/PAN.png" width="80%;"></p><ul><li>b: bottom-up path</li><li>c: adaptive feature pooling </li><li>e: fusion mask branch</li></ul></li><li><p>bottom-up path</p><ul><li>fpnâ€™s top-down path:<ul><li>to propagate strong semantical information</li><li>to ensure reasonable classification capability </li><li>long path: red line, 100+ layers</li></ul></li><li>bottom-up path:  <ul><li>enhances the localization capability  </li><li>short path: green line, less than 10 layers</li></ul></li><li><p>for each level $N_l$</p><ul><li>input: $N_{l+1}$ &amp; $P_l$</li><li>$N_{l+1}$ 3x3 conv &amp; $P_l$ id path - add - 3x3 conv</li><li>channel 256</li><li>ReLU after conv</li></ul><p><img src="/2020/12/02/PANet/bottom-up.png" width="50%;"></p></li></ul></li><li><p>adaptive feature pooling </p><ul><li><p>pool features from all levels, then fuse, then predict</p></li><li><p>steps</p><ul><li>map each proposal to all feature levels</li><li>roi align</li><li>go through one layer of the following sub-networks independently</li><li>fusion operation (element-wise max or sum) </li><li><p>ä¾‹å¦‚ï¼Œbox branchæ˜¯ä¸¤ä¸ªfcå±‚ï¼Œæ¥è‡ªå„ä¸ªlevelçš„roi alignä¹‹åçš„proposal featuresï¼Œå…ˆå„è‡ªç»è¿‡ä¸€ä¸ªfcå±‚ï¼Œå†share the following till the headï¼Œmask branchæ˜¯4ä¸ªconvå±‚ï¼Œæ¥è‡ªå„ä¸ªlevelçš„roi alignä¹‹åçš„proposal featuresï¼Œå…ˆå„è‡ªç»è¿‡ä¸€ä¸ªconvå±‚ï¼Œå†share the following till the head</p><p><img src="/2020/12/02/PANet/fusion.png" width="50%;"></p></li></ul></li><li><p>fusion mask branch</p><ul><li>fc layers are location sensitive </li><li>helpful to differentiate instances and recognize separate parts belonging to the same object </li><li>convåˆ†æ”¯<ul><li>4ä¸ªè¿ç»­conv+1ä¸ªdeconvï¼š3x3 convï¼Œchannel256ï¼Œdeconv factor=2</li><li>predict mask of each classï¼šoutput channel n_classes </li></ul></li><li>fcåˆ†æ”¯<ul><li>from convåˆ†æ”¯çš„conv3è¾“å‡º</li><li>2ä¸ªè¿ç»­convï¼Œchannel256ï¼Œchannel128</li><li>fcï¼Œdim=28x28ï¼Œç‰¹å¾å›¾å°ºå¯¸ï¼Œç”¨äºå‰èƒŒæ™¯åˆ†ç±»</li></ul></li><li><p>final maskï¼šadd</p><p><img src="/2020/12/02/PANet/mask.png" width="50%;"></p></li></ul></li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li>heavier head<ul><li>4 consecutive 3x3 convs</li><li>shared among reg &amp; cls</li><li>åœ¨multi-taskçš„æƒ…å†µä¸‹ï¼Œå¯¹boxçš„é¢„æµ‹æœ‰æ•ˆ</li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> å®ä¾‹åˆ†å‰² </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CSPNet</title>
      <link href="/2020/11/17/CSPNet/"/>
      <url>/2020/11/17/CSPNet/</url>
      <content type="html"><![CDATA[<h2 id="CSPNET-A-NEW-BACKBONE-THAT-CAN-ENHANCE-LEARNING-CAPABILITY-OF-CNN"><a href="#CSPNET-A-NEW-BACKBONE-THAT-CAN-ENHANCE-LEARNING-CAPABILITY-OF-CNN" class="headerlink" title="CSPNET: A NEW BACKBONE THAT CAN ENHANCE LEARNING CAPABILITY OF CNN"></a>CSPNET: A NEW BACKBONE THAT CAN ENHANCE LEARNING CAPABILITY OF CNN</h2><ol><li><p>åŠ¨æœº</p><ul><li>propose a network from the respect of the variability of the gradients</li><li>reduces computations </li><li>superior accuracy while being lightweightening</li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>CNN architectures design </p><ul><li>ResNeXtï¼šcardinality can be more effective than width and depth </li><li>DenseNetï¼šreuse features </li><li>partial ResNetï¼šhigh cardinality and sparse connectionï¼Œthe concept of gradient combination </li></ul></li><li><p>introduce Cross Stage Partial Network (CSPNet) </p><ul><li>strengthening learning ability of a CNNï¼šsufficient accuracy while being lightweightening </li><li>removing computational bottlenecksï¼šhoping evenly distribute the amount of computation at each layer in CNN </li><li><p>reducing memory costsï¼šadopt cross-channel pooling during fpn</p><p><img src="/2020/11/17/CSPNet/CSP.png" width="90%;"></p></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>ç»“æ„</p><ul><li>Partial Dense Blockï¼šèŠ‚çœä¸€åŠè®¡ç®—</li><li>Partial Transition Layerï¼šfusion lastèƒ½å¤Ÿsave computationåŒæ—¶ç²¾åº¦ä¸æ‰å¤ªå¤š</li></ul><p><img src="/2020/11/17/CSPNet/fusion.png" width="90%;"></p><ul><li>è®ºæ–‡è¯´fusion firstä½¿å¾—å¤§é‡æ¢¯åº¦å¾—åˆ°é‡ç”¨ï¼Œcomputation cost is significantly droppedï¼Œfusion lastä¼šæŸå¤±éƒ¨åˆ†æ¢¯åº¦é‡ç”¨ï¼Œä½†æ˜¯ç²¾åº¦æŸå¤±ä¹Ÿæ¯”è¾ƒå°(0.1)ã€‚</li><li>it is obvious that if one can effectively reduce the repeated gradient information, the learning ability of a network will be greatly improved. </li></ul><p><img src="/2020/11/17/CSPNet/acc.png" width="80%;"></p></li></ul></li></ol><ul><li><p>Apply CSPNet to Other Architectures </p><ul><li>å› ä¸ºåªæœ‰ä¸€åŠçš„channelå‚ä¸resnet blockçš„è®¡ç®—ï¼Œæ‰€ä»¥æ— éœ€å†å¼•å…¥bottleneckç»“æ„äº†</li><li><p>æœ€åä¸¤ä¸ªpathçš„è¾“å‡ºconcat</p><p><img src="/2020/11/17/CSPNet/cspresnet.png" width="40%;"></p></li></ul></li><li><p>EFM</p><p>  <img src="/2020/11/17/CSPNet/ERM.png" width="80%;"></p></li><li><p>fusion</p><pre><code>  * ç‰¹å¾é‡‘å­—å¡”ï¼ˆFPNï¼‰ï¼šèåˆå½“å‰å°ºåº¦å’Œä»¥å‰å°ºåº¦çš„ç‰¹å¾ã€‚  * å…¨å±€èåˆæ¨¡å‹ï¼ˆGFMï¼‰ï¼šèåˆæ‰€æœ‰å°ºåº¦çš„ç‰¹å¾ã€‚  * ç²¾ç¡®èåˆæ¨¡å‹ï¼ˆEFMï¼‰ï¼šèåˆanchorå°ºå¯¸ä¸Šçš„ç‰¹å¾ã€‚</code></pre><ul><li>EFM<ul><li>assembles features from the three scalesï¼šå½“å‰å°ºåº¦&amp;ç›¸é‚»å°ºåº¦</li><li>åŒæ—¶åˆåŠ äº†ä¸€ç»„bottom-upçš„èåˆ</li><li>Maxout techniqueå¯¹ç‰¹å¾æ˜ å°„è¿›è¡Œå‹ç¼©</li></ul></li></ul></li></ul><ol><li><p>ç»“è®º</p><p> ä»å®éªŒç»“æœæ¥çœ‹ï¼Œ</p><ul><li>åˆ†ç±»é—®é¢˜ä¸­ï¼Œä½¿ç”¨CSPNetå¯ä»¥é™ä½è®¡ç®—é‡ï¼Œä½†æ˜¯å‡†ç¡®ç‡æå‡å¾ˆå°ï¼›</li><li>åœ¨ç›®æ ‡æ£€æµ‹é—®é¢˜ä¸­ï¼Œä½¿ç”¨CSPNetä½œä¸ºBackboneå¸¦æ¥çš„æå‡æ¯”è¾ƒå¤§ï¼Œå¯ä»¥æœ‰æ•ˆå¢å¼ºCNNçš„å­¦ä¹ èƒ½åŠ›ï¼ŒåŒæ—¶ä¹Ÿé™ä½äº†è®¡ç®—é‡ã€‚æœ¬æ–‡æ‰€æå‡ºçš„EFMæ¯”GFMæ…¢2fpsï¼Œä½†APå’ŒAP50åˆ†åˆ«æ˜¾è‘—æé«˜äº†2.1%å’Œ2.4%ã€‚</li></ul></li></ol>]]></content>
      
      
    </entry>
    
    <entry>
      <title>nms</title>
      <link href="/2020/10/29/nms/"/>
      <url>/2020/10/29/nms/</url>
      <content type="html"><![CDATA[<p>Non-maximum suppressionï¼šéæå¤§å€¼æŠ‘åˆ¶ç®—æ³•ï¼Œæœ¬è´¨æ˜¯æœç´¢å±€éƒ¨æå¤§å€¼ï¼ŒæŠ‘åˆ¶éæå¤§å€¼å…ƒç´ </p><p>[nms]ï¼šstandard nmsï¼Œå½“ç›®æ ‡æ¯”è¾ƒå¯†é›†ã€å­˜åœ¨é®æŒ¡æ—¶ï¼Œæ¼æ£€ç‡é«˜</p><p>[soft nms]ï¼šæ”¹å˜nmsçš„hard thresholdï¼Œç”¨è¾ƒä½çš„åˆ†æ•°æ›¿ä»£0ï¼Œæå‡recall</p><p>[softer nms]ï¼šå¼•å…¥box position confidenceï¼Œé€šè¿‡åå¤„ç†æé«˜å®šä½ç²¾åº¦</p><p>[DIoU nms]ï¼šé‡‡ç”¨DIoUçš„è®¡ç®—æ–¹å¼æ›¿æ¢IoUï¼Œå› ä¸ºDIoUçš„è®¡ç®—è€ƒè™‘åˆ°äº†ä¸¤æ¡†ä¸­å¿ƒç‚¹ä½ç½®çš„ä¿¡æ¯ï¼Œæ•ˆæœæ›´ä¼˜</p><p>[fast nms]ï¼šYOLOACTå¼•å…¥çŸ©é˜µä¸‰è§’åŒ–ï¼Œä¼šæ¯”Traditional NMSæŠ‘åˆ¶æ›´å¤šçš„æ¡†ï¼Œæ€§èƒ½ç•¥å¾®ä¸‹é™</p><p>[cluster nms]ï¼š<a href="https://github.com/Zzh-tju/CIoU" target="_blank" rel="noopener">CIoUæå‡º</a>ï¼Œå¼¥è¡¥Fast NMSçš„æ€§èƒ½ä¸‹é™ï¼Œè¿ç®—æ•ˆç‡æ¯”Fast NMSä¸‹é™äº†ä¸€äº›</p><p>[mask nms]ï¼šmask iouè®¡ç®—æœ‰ä¸å¯å¿½ç•¥çš„å»¶è¿Ÿï¼Œå› æ­¤æ¯”box nmsæ›´è€—æ—¶</p><p>[matrix nms]ï¼šSOLOå°†mask IoUå¹¶è¡ŒåŒ–ï¼Œæ¯”FAST-NMSè¿˜å¿«ï¼Œæ€è·¯å’ŒFAST-NMSä¸€æ ·ä»ä¸Šä¸‰è§’IoUçŸ©é˜µå‡ºå‘ï¼Œå¯èƒ½é€ æˆè¿‡å¤šæŠ‘åˆ¶ã€‚</p><p>[WBF]ï¼šåŠ æƒæ¡†èåˆï¼ŒKaggleèƒ¸ç‰‡å¼‚ç‰©æ¯”èµ›claimæœ‰ç”¨ï¼Œé€Ÿåº¦æ…¢ï¼Œå¤§æ¦‚æ¯”æ ‡å‡†NMSæ…¢3å€ï¼ŒWBFå®éªŒä¸­æ˜¯åœ¨å·²ç»å®ŒæˆNMSçš„æ¨¡å‹ä¸Šè¿›è¡Œçš„</p><ol><li>nms<ul><li>è¿‡æ»¤+è¿­ä»£+éå†+æ¶ˆé™¤<ul><li>é¦–å…ˆè¿‡æ»¤æ‰å¤§é‡ç½®ä¿¡åº¦è¾ƒä½çš„æ¡†ï¼Œå¤§äºconfidence threshçš„boxä¿ç•™</li><li>å°†æ‰€æœ‰æ¡†çš„å¾—åˆ†æ’åºï¼Œé€‰ä¸­æœ€é«˜åˆ†çš„æ¡†</li><li>éå†å…¶ä½™çš„æ¡†ï¼Œå¦‚æœå’Œå½“å‰æœ€é«˜åˆ†æ¡†çš„IOUå¤§äºä¸€å®šé˜ˆå€¼(nms thresh)ï¼Œå°±å°†æ¡†åˆ é™¤(score=0)</li><li>ä»æœªå¤„ç†çš„æ¡†ä¸­ç»§ç»­é€‰ä¸€ä¸ªå¾—åˆ†æœ€é«˜çš„ï¼Œé‡å¤ä¸Šè¿°è¿‡ç¨‹</li></ul></li><li>when evaluation<ul><li>iou threshï¼šç•™ä¸‹çš„boxé‡Œé¢ï¼Œä¸gt boxçš„iouå¤§äºiou threshçš„boxä½œä¸ºæ­£ä¾‹ï¼Œç”¨äºè®¡ç®—å‡ºAPå’ŒmAPï¼Œé€šè¿‡è°ƒæ•´confidence threshå¯ä»¥ç”»å‡ºPRæ›²çº¿</li></ul></li></ul></li></ol><ol><li><p>softnms</p><ul><li><p>åŸºæœ¬æµç¨‹è¿˜æ˜¯nmsçš„è´ªå©ªæ€è·¯ï¼Œè¿‡æ»¤+è¿­ä»£+éå†+è¡°å‡</p><p>  <img src="/2020/10/29/nms/softnms.png" width="40%;"></p></li><li><p>re-score functionï¼šhigh overlap decays more</p><ul><li>linearï¼š<ul><li>for each $iou(M,b_i)&gt;th$ï¼Œ $s_i=s_i(1-iou)$</li><li>not continuousï¼Œsudden penalty </li></ul></li><li>gaussianï¼š<ul><li>for all remaining detection boxesï¼Œ$s_i=s_i e^{-\frac{iou(M,b_i)}{\sigma}}$</li></ul></li></ul></li><li><p>ç®—æ³•æµç¨‹ä¸Šæœªåšä¼˜åŒ–ï¼Œæ˜¯é’ˆå¯¹ç²¾åº¦çš„ä¼˜åŒ–</p></li></ul></li></ol><ol><li><p>softer nms</p><ul><li>è·Ÿsoft nmsæ²¡å…³ç³»</li><li>å…·æœ‰é«˜åˆ†ç±»ç½®ä¿¡åº¦çš„è¾¹æ¡†å…¶ä½ç½®å¹¶ä¸æ˜¯æœ€ç²¾å‡†çš„</li><li>æ–°å¢åŠ äº†ä¸€ä¸ªå®šä½ç½®ä¿¡åº¦çš„é¢„æµ‹ï¼Œä½¿å…¶æœä»é«˜æ–¯åˆ†å¸ƒ</li><li>inferé˜¶æ®µè¾¹æ¡†çš„æ ‡å‡†å·®å¯ä»¥è¢«çœ‹åšè¾¹æ¡†çš„ä½ç½®ç½®ä¿¡åº¦ï¼Œä¸åˆ†ç±»ç½®ä¿¡åº¦åšåŠ æƒå¹³å‡ï¼Œä½œä¸ºtotal score</li><li>ç®—æ³•æµç¨‹ä¸Šæœªåšä¼˜åŒ–ï¼Œå®Œå…¨æ˜¯ç²¾åº¦çš„ä¼˜åŒ–</li></ul></li></ol><ol><li><p>DIoU nms</p><ul><li><p>ä¹Ÿæ˜¯ä¸ºäº†è§£å†³hard nmsåœ¨å¯†é›†åœºæ™¯ä¸­æ¼æ£€ç‡é«˜çš„é—®é¢˜</p></li><li><p>ä½†æ˜¯ä¸åŒäºsoft nmsçš„æ˜¯ï¼ŒDçš„æ”¹è¿›åœ¨iouè®¡ç®—ä¸Šï¼Œè€Œä¸æ˜¯åœ¨score</p></li><li><p>diouçš„è®¡ç®—ï¼š$diou = iou-\frac{\rho^2(b_1, b_2)}{c^2}$</p><p>  <img src="/2020/10/29/nms/diou.png" width="60%;"></p></li><li><p>ç®—æ³•æµç¨‹ä¸Šæœªåšä¼˜åŒ–ï¼Œä»æ—§æ˜¯ç²¾åº¦çš„ä¼˜åŒ–</p></li></ul></li></ol><ol><li><p>fast nms</p><ul><li><p>yoloactæå‡º</p></li><li><p>ä¸»è¦æ•ˆç‡æå‡åœ¨äºç”¨çŸ©é˜µæ“ä½œæ›¿æ¢éå†ï¼Œæ‰€æœ‰æ¡†åŒæ—¶è¢«filteræ‰ï¼Œè€Œéä¾æ¬¡éå†åˆ é™¤</p></li><li><p>iouä¸Šä¸‰è§’çŸ©é˜µ</p><ul><li>iouä¸Šä¸‰è§’çŸ©é˜µçš„æ¯ä¸€ä¸ªå…ƒç´ éƒ½æ˜¯è¡Œå·å°äºåˆ—å·</li><li>iouä¸Šä¸‰è§’çŸ©é˜µçš„æ¯ä¸€ä¸ªè¡Œï¼Œå¯¹åº”ä¸€ä¸ªbnd boxï¼Œä¸å…¶ä»–æ‰€æœ‰scoreå°äºå®ƒçš„bnd boxçš„iou</li><li>iouä¸Šä¸‰è§’çŸ©é˜µçš„æ¯ä¸€ä¸ªåˆ—ï¼Œå¯¹åº”ä¸€ä¸ªbnd boxï¼Œä¸å…¶ä»–æ‰€æœ‰scoreå¤§äºå®ƒçš„bnd boxçš„iou</li><li>fast nmsåœ¨iouçŸ©é˜µæ¯ä¸€åˆ—ä¸Šæ±‚æœ€å¤§å€¼ï¼Œå¦‚æœè¿™ä¸ªæœ€å¤§å€¼å¤§äºiou threshï¼Œè¯´æ˜å½“å‰åˆ—å¯¹åº”çš„bnd boxï¼Œå­˜åœ¨ä¸€ä¸ªscoreå¤§äºå®ƒï¼Œä¸”å’Œå®ƒé‡å åº¦è¾ƒé«˜çš„bnd boxï¼Œå› æ­¤è¦æŠŠè¿™ä¸ªboxè¿‡æ»¤æ‰</li></ul></li><li><p>æœ‰ç²¾åº¦æŸå¤±</p><ul><li><p>åœºæ™¯ï¼š</p><p><img src="/2020/10/29/nms/fastnms1.jpg" width="60%;"></p></li><li><p>å¦‚æœæ˜¯hard nmsçš„è¯ï¼Œé¦–å…ˆéå†b1çš„å…¶ä»–boxï¼Œb2å°±è¢«åˆ é™¤äº†ï¼Œè¿™æ˜¯b3å°±ä¸å­˜åœ¨é«˜é‡å æ¡†äº†ï¼Œb3å°±ä¼šè¢«ç•™ä¸‹ï¼Œä½†æ˜¯åœ¨fast nmsåœºæ™¯ä¸‹ï¼Œæ‰€æœ‰æ¡†è¢«åŒæ—¶åˆ é™¤ï¼Œå› æ­¤b2ã€b3éƒ½æ²¡äº†ã€‚</p><p><img src="/2020/10/29/nms/fastnms2.png" width="60%;"></p></li></ul></li></ul></li></ol><ol><li><p>cluster nms</p><ul><li><p>é’ˆå¯¹fast nmsæ€§èƒ½ä¸‹é™çš„å¼¥è¡¥</p></li><li><p>fast nmsæ€§èƒ½ä¸‹é™ï¼Œä¸»è¦é—®é¢˜åœ¨äºè¿‡åº¦æŠ‘åˆ¶ï¼Œå¹¶è¡Œæ“ä½œæ— æ³•åŠæ—¶æ¶ˆé™¤high scoreæ¡†æŠ¹æ‰å¯¹åç»­low scoreæ¡†åˆ¤æ–­çš„å½±å“</p></li><li><p>ç®—æ³•æµç¨‹ä¸Šï¼Œå°†fast nmsçš„ä¸€æ¬¡é˜ˆå€¼æ“ä½œï¼Œè½¬æ¢æˆå°‘æ•°å‡ æ¬¡çš„è¿­ä»£æ“ä½œï¼Œæ¯æ¬¡éƒ½æ˜¯ä¸€ä¸ªfast nms</p><p>  <img src="/2020/10/29/nms/clusternms.png" width="80%;"></p><ul><li>å›¾ä¸­Xè¡¨ç¤ºiouçŸ©é˜µï¼Œbè¡¨ç¤ºnmsé˜ˆå€¼äºŒå€¼åŒ–ä»¥åçš„å‘é‡ï¼Œä¹Ÿå°±æ˜¯fast nmsé‡Œé¢é‚£ä¸ªä¿ç•™ï¼æŠ‘åˆ¶å‘é‡</li><li>æ¯æ¬¡è¿­ä»£ï¼Œç®—æ³•å°†bå±•å¼€æˆä¸€ä¸ªå¯¹è§’çŸ©é˜µï¼Œç„¶åå·¦ä¹˜iouçŸ©é˜µ</li><li>ç›´åˆ°å‡ºç°æŸä¸¤æ¬¡è¿­ä»£åï¼Œ bä¿æŒä¸å˜äº†ï¼Œé‚£ä¹ˆè¿™å°±æ˜¯æœ€ç»ˆçš„b</li></ul></li><li><p>cluster nmsçš„è¿­ä»£æ“ä½œï¼Œå…¶å®å°±æ˜¯åœ¨çœç•¥ä¸Šä¸€æ¬¡Fast NMSè¿­ä»£ä¸­è¢«æŠ‘åˆ¶çš„æ¡†å¯¹å…¶ä»–æ¡†çš„å½±å“</p></li><li><p>æ•°å­¦å½’çº³æ³•è¯æ˜ï¼Œcluster nmsçš„ç»“æœä¸hard nmså®Œå…¨ä¸€è‡´ï¼Œè¿ç®—æ•ˆç‡æ¯”fast nmsä¸‹é™äº†ä¸€äº›ï¼Œä½†æ˜¯æ¯”hard nmså¿«å¾—å¤š</p></li><li><p>cluster nmsçš„è¿ç®—æ•ˆç‡ä¸ä¸clusteræ•°é‡æœ‰å…³ï¼Œåªä¸éœ€è¦è¿­ä»£æ¬¡æ•°æœ€å¤šçš„é‚£ä¸€ä¸ªclusteræœ‰å…³</p></li></ul></li></ol><ol><li><p>mask nms</p><ul><li>ä»æ£€æµ‹æ¡†å½¢çŠ¶çš„è§’åº¦æ‹“å±•å‡ºæ¥ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºmask nmsã€polygon nmsä»¥åŠinclined nms</li><li>iouçš„è®¡ç®—æ–¹å¼æœ‰ä¸€ç§æ˜¯mmiï¼š$mmi=max(\frac{I}{I_A}, \frac{I}{I_B})$</li></ul></li></ol><ol><li><p>matrix nms</p><ul><li><p>å­¦ä¹ soft nmsï¼šdecay factor</p></li><li><p>one step furtherï¼šè¿­ä»£æ”¹å¹¶è¡Œ</p></li><li><p>å¯¹äºæŸä¸ªobject $m_j$çš„scoreè¿›è¡Œpenaltyçš„æ—¶å€™è€ƒè™‘ä¸¤éƒ¨åˆ†å½±å“</p><ul><li>è¿­ä»£æŸä¸ª$m_i$æ—¶ï¼Œå¯¹åç»­lower scoreçš„$m_j$çš„å½±å“</li><li>ä¸€æ˜¯æ­£é¢å½±å“$f(iou_{i,j})\ linear/guassian$ï¼šè¿™ä¸ªæ¡†ä¿ç•™ï¼Œé‚£ä¹ˆåç»­æ¡†éƒ½è¦åŸºäºä¸å…¶çš„iouåšdecay</li><li>äºŒæ˜¯åå‘å½±å“$f(iou_{*,i})=max_{\forall s_k&gt;s_i}f(iou_{k,i})$ï¼šå¦‚æœè¿™ä¸ªæ¡†ä¸ä¿ç•™ï¼Œé‚£ä¹ˆå¯¹äºåç»­æ¡†æ¥è®²ï¼Œåº”è¯¥æ¶ˆé™¤è¿™ä¸ªæ¡†å¯¹å…¶çš„decayï¼Œé€‰æœ€å¤§å€¼çš„æ„ä¹‰æ˜¯å½“å‰maskè¢«æŠ‘åˆ¶æœ€æœ‰å¯èƒ½å°±æ˜¯å’Œä»–é‡å åº¦æœ€å¤§çš„é‚£ä¸ªmaskå¹²çš„ï¼ˆå› ä¸ºå¯¹åº”çš„æ­£é¢å½±å“1-iouæœ€å°ï¼‰</li></ul></li></ul></li></ol><ul><li><p>final decay factorï¼š$decay_j=min_{\forall s_i &gt; s_j}\frac{f(iou_{i,j})}{f(iou_{*,i})}$</p></li><li><p>ç®—æ³•æµç¨‹</p><pre><code>  &lt;img src=&quot;nms/matrixnms.png&quot; width=&quot;50%;&quot; /&gt;</code></pre></li></ul><pre><code>* æŒ‰ç…§åŸè®ºæ–‡çš„å®ç°ï¼Œdecayæ°¸è¿œå¤§äºç­‰äº1ï¼Œå› ä¸ºæ¯ä¸€åˆ—çš„iou_cmaxæ°¸è¿œå¤§äºç­‰äºiouï¼Œä»è®ºæ–‡çš„æ€è·¯æ¥çœ‹ï¼Œæ¯ä¸ªmaskçš„decayæ˜¯å®ƒä¹‹å‰æ‰€æœ‰maskçš„å½±å“å åŠ åœ¨ä¸€èµ·ï¼Œæ‰€ä»¥åº”è¯¥æ˜¯ä¹˜ç§¯è€Œä¸æ˜¯minï¼š    <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># åŸè®ºæ–‡å®ç°</span></span><br><span class="line"><span class="keyword">if</span> method==<span class="string">'gaussian'</span>:</span><br><span class="line">    decay = np.exp(-(np.square(iou)-np.square(iou_cmax))/sigma)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    decay = (<span class="number">1</span>-iou)/(<span class="number">1</span>-iou_cmax)</span><br><span class="line">decay = np.min(decay, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ”¹è¿›å®ç°</span></span><br><span class="line"><span class="keyword">if</span> method==<span class="string">'gaussian'</span>:</span><br><span class="line">    decay = np.exp(-(np.sum(np.square(iou),axis=<span class="number">0</span>)-np.square(iou_cmax))/sigma)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    decay = np.prod(<span class="number">1</span>-iou)/(<span class="number">1</span>-iou_cmax)</span><br></pre></td></tr></table></figure></code></pre>]]></content>
      
      
        <tags>
            
            <tag> ç›®æ ‡æ£€æµ‹ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>MimicDet</title>
      <link href="/2020/10/14/MimicDet/"/>
      <url>/2020/10/14/MimicDet/</url>
      <content type="html"><![CDATA[<p>[MimicDet] ResNeXt-101 backbone on the COCO: 46.1 mAP </p><h2 id="MimicDet-Bridging-the-Gap-Between-One-Stage-and-Two-Stage-Object-Detection"><a href="#MimicDet-Bridging-the-Gap-Between-One-Stage-and-Two-Stage-Object-Detection" class="headerlink" title="MimicDet: Bridging the Gap Between One-Stage and Two-Stage Object Detection"></a>MimicDet: Bridging the Gap Between One-Stage and Two-Stage Object Detection</h2><ol><li><p>åŠ¨æœº</p><ul><li>mimic taskï¼šknowledge distillation </li><li>mimic the two-stage features <ul><li>a shared backbone </li><li>two heads for mimicking </li></ul></li><li>end-to-end training</li><li>specialized designs to facilitate mimicking <ul><li>dual-path mimicking</li><li>staggered feature pyramid </li></ul></li><li>reach two-stage accuracy </li></ul></li><li><p>è®ºç‚¹</p><ul><li>one-stage detectors adopt a straightforward fully convolutional architecture </li><li>two-stage detectors use RPN + R-CNN</li><li>advantages of two-stage detectors <ul><li>avoid class imbalance </li><li>less proposals enables larger cls net and richer features </li><li>RoIAlign extracts location consistent feature -&gt; better represenation</li><li>regress the object location twice -&gt; better refined</li></ul></li><li>one-stage detectorsâ€™ imitation<ul><li>RefineDetï¼šcascade detection flow  </li><li>AlignDetï¼šRoIConv layer </li><li>still leaves a big gap </li></ul></li><li>network mimicking<ul><li>knowledge distillation </li><li>use a well-trained large teacher model to supervise </li><li>difference<ul><li>mimic in heads instead of backbones </li><li>teacher branch instead of model</li><li>trained jointly </li></ul></li></ul></li><li>this method<ul><li>not only mimic the structure design, but also imitate in the feature level</li><li>contains both one-stage detection head and two-stage detection head during training <ul><li>share the same backbone </li><li>two-stage detection head, called T-head </li><li>one-stage detection head, called S-head </li><li>similarity loss for matching featureï¼šguided deformable conv layer </li><li>together with detection losses </li></ul></li><li>specialized designs <ul><li>decomposed detection heads</li><li>conduct mimicking in classification and regression branches individually</li><li>staggered feature pyramid  </li></ul></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>overview</p><p>  <img src="/2020/10/14/MimicDet/mimicdet.png" width="70%;"></p></li><li><p>back &amp; fpn</p><ul><li>RetinaNet fpnï¼šwith P6 &amp; P7</li><li>crucial modificationï¼šP2 ï½ P7</li><li>staggered feature pyramid <ul><li>high-res set {P2 to P6}ï¼šfor T-head &amp; accuray</li><li>low-res set {P3 to P7}ï¼šfor S-head &amp; computation speed</li></ul></li></ul></li><li><p>refinement module</p><ul><li>filter out easy negativesï¼šmitigate the class imbalance issue </li><li>adjust the location and size of pre-defined anchor boxesï¼šanchor initialization  </li><li>module<ul><li>on top of the feature pyramid </li><li>one 3x3 conv</li><li>two sibling 1x1 convs<ul><li>binary classificationï¼šbce loss</li><li>bounding box regressionï¼šthe same as Faster R-CNNï¼ŒL1 loss</li></ul></li><li>top-ranked boxes transferred to T-head and S-head </li></ul></li><li>one anchor on each positionï¼šavoid feature sharing among proposals</li><li>assign the objects to feature pyramid according to their scale </li><li>positive areaï¼š0.3 times shrinking of gt boxes from center</li><li>positive sampleï¼š<ul><li>valid scale rangeï¼šgt target belongs to this level</li><li>central point of anchor lies in the positive area</li></ul></li></ul></li><li><p>detection heads</p><ul><li>T-head<ul><li>heavy head </li><li>run on a sparse set of anchor boxes </li><li>use the staggered feature pyramid </li><li>generate 7x7 location-sensitive features for each anchor box </li><li>cls branch<ul><li>two 1024-d fc layers</li><li><strong>one 81-d fc layer</strong> + softmaxï¼šce loss</li></ul></li><li>reg branch<ul><li>four 3x3 convsï¼Œch256</li><li>flatten</li><li><strong>1024-d fc</strong></li><li>4-d fcï¼šL1 loss</li></ul></li><li>mimicking target <ul><li>81-d classification logits </li><li>1024-d regression feature </li></ul></li></ul></li><li>S-head<ul><li>light-weight  </li><li>directly dense detection on fpn</li><li>ã€ä¸å¤ªç†è§£ã€‘introducing the refinement module will break the location consistency between the anchor box and its corresponding featuresï¼šæˆ‘çš„ç†è§£æ˜¯refineä»¥åçš„anchorå’ŒåŸå§‹anchorå¯¹åº”çš„ç‰¹å¾å›¾misalignäº†ï¼ŒT-headç”¨çš„æ˜¯refined anchorï¼ŒS-headç”¨çš„æ˜¯original gridï¼Œæ‰€ä»¥misalign</li><li>use deformable convolution to capture the misaligned feature <ul><li>deformation offset is computed by a micro-network </li><li>takes the regression output of the refinement module as input </li><li>three 1x1 convsï¼Œch64/128ï¼18(50)</li><li>3x3 Dconv for P3 and 5x5 for othersï¼Œch256</li></ul></li><li>two sibling 1x1 convsï¼Œch1024<ul><li>cls branchï¼š1x1 convï¼Œch80</li><li>reg branchï¼š1x1 convï¼Œch4</li></ul></li></ul></li><li><img src="/2020/10/14/MimicDet/heads.png" width="70%;"></li></ul></li><li><p>head mimicking</p><ul><li>cosine similarity </li><li>cls logits &amp; refine params</li><li>To get the S-head feature of an adjusted anchor box <ul><li>trace back to its initial position </li><li>extract the pixel at that position in the feature map </li></ul></li><li>lossï¼š$L_{mimic} = 1 - cosine(F_i^T, F_i^S)$</li></ul></li><li>multi-task training loss  <ul><li>$L = L_R + L_S + L_T + L_{mimic}$</li><li>$L_R$ï¼šrefine module lossï¼Œbce+L1</li><li>$L_S$ï¼šS-head lossï¼Œce+L1</li><li>$L_T$ï¼šT-head lossï¼Œce+L1</li><li>$L_{mimic}$ï¼šmimic loss </li></ul></li><li>training details<ul><li>networkï¼šresnet50/101ï¼Œresize image with shorter side 800</li><li>refinement module<ul><li>run NMS with 0.8 IoU threshold on anchor boxes </li><li>select top 2000 boxes </li></ul></li><li>T-head<ul><li>sample 128 boxes from proposal</li><li>pï¼nï¼š1/3</li></ul></li><li>S-head<ul><li>hard miningï¼šselect 128 boxes with top loss value </li></ul></li></ul></li><li>inference<ul><li>take top 1000 boxes from refine module</li><li>NMS with 0.6 IoU threshold and 0.005 score threshold </li><li>ã€ï¼Ÿï¼Ÿã€‘finally top 100 scoring boxesï¼šè¿™å—ä¸å¤ªç†è§£ï¼Œæœ€ååº”è¯¥ä¸æ˜¯ç»“æ„åŒ–è¾“å‡ºäº†å•Šï¼Œåº”è¯¥æ˜¯ä¸€é˜¶æ®µæ£€æµ‹å¤´çš„re-refineè¾“å‡ºå•Š</li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ç›®æ ‡æ£€æµ‹ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>metrics</title>
      <link href="/2020/10/09/metrics/"/>
      <url>/2020/10/09/metrics/</url>
      <content type="html"><![CDATA[<h2 id="åˆ†ç±»æŒ‡æ ‡"><a href="#åˆ†ç±»æŒ‡æ ‡" class="headerlink" title="åˆ†ç±»æŒ‡æ ‡"></a>åˆ†ç±»æŒ‡æ ‡</h2><ul><li>recallï¼šå¬å›ç‡</li><li>precisionï¼šå‡†ç¡®ç‡</li><li>accuracyï¼šæ­£ç¡®ç‡</li><li>F-Measure</li><li>sensitivityï¼šçµæ•åº¦</li><li>specificityï¼šç‰¹å¼‚åº¦</li><li>TPR</li><li>FPR</li><li>ROC</li><li>AUC</li></ul><ol><li><p>æ··æ·†çŸ©é˜µ</p><p> |           |  gt is p   |   gt is n    |<br> | :â€”â€”â€”-: | :â€”â€”â€”â€”: | :â€”â€”â€”â€”â€”: |<br> | pred is p |     tp     | fpï¼ˆå‡é˜³æ€§ï¼‰ |<br> | pred is n | fnï¼ˆæ¼æ£€ï¼‰ |      tn      |</p><ul><li>æ³¨æ„åŒºåˆ†fpå’Œfn</li><li>fpï¼šè¢«é”™è¯¯åœ°åˆ’åˆ†ä¸ºæ­£ä¾‹çš„ä¸ªæ•°ï¼Œå³å®é™…ä¸ºè´Ÿä¾‹ä½†è¢«åˆ†ç±»å™¨åˆ’åˆ†ä¸ºæ­£ä¾‹çš„å®ä¾‹æ•°</li><li>fnï¼šè¢«é”™è¯¯åœ°åˆ’åˆ†ä¸ºè´Ÿä¾‹çš„ä¸ªæ•°ï¼Œå³å®é™…ä¸ºæ­£ä¾‹ä½†è¢«åˆ†ç±»å™¨åˆ’åˆ†ä¸ºè´Ÿä¾‹çš„å®ä¾‹æ•°</li></ul></li><li><p>recall</p><ul><li>è¡¡é‡æŸ¥å…¨ç‡</li><li>å¯¹gt is påšç»Ÿè®¡</li><li>$recall = \frac{tp}{tp+fn}$</li></ul></li><li><p>precision</p><ul><li>è¡¡é‡æŸ¥å‡†ç‡</li><li>å¯¹pred is påšç»Ÿè®¡</li><li>$precision = \frac{tp}{tp+fp}$</li></ul></li><li><p>accuracy</p><ul><li>å¯¹çš„é™¤ä»¥æ‰€æœ‰</li><li>$accuracy = \frac{tp+tn}{p+n}$</li></ul></li><li><p>sensitivity</p><ul><li>è¡¡é‡åˆ†ç±»å™¨å¯¹æ­£ä¾‹çš„è¯†åˆ«èƒ½åŠ›</li><li>å¯¹gt is påšç»Ÿè®¡</li><li>$sensitivity = \frac{tp}{p}=\frac{tp}{tp+fn}$</li></ul></li><li><p>specificity</p><ul><li>è¡¡é‡åˆ†ç±»å™¨å¯¹è´Ÿä¾‹çš„è¯†åˆ«èƒ½åŠ›</li><li>å¯¹gt is nåšç»Ÿè®¡</li><li>$specificity =\frac{tn}{n}= \frac{tn}{fp+tn}$</li></ul></li><li><p>F-measure</p><ul><li>ç»¼åˆè€ƒè™‘På’ŒRï¼Œæ˜¯Precisionå’ŒRecallåŠ æƒè°ƒå’Œå¹³å‡</li><li>$F = \frac{(a^2+1)<em>P</em>R}{a^2*P+R}$</li><li>$F_1 = \frac{2PR}{P+R}$</li></ul></li><li><p>TPR</p><ul><li>å°†æ­£ä¾‹åˆ†å¯¹çš„æ¦‚ç‡</li><li>å¯¹gt is tåšç»Ÿè®¡</li><li>$TPR = \frac{tp}{tp+fn}$</li></ul></li><li><p>FPR</p><ul><li>å°†è´Ÿä¾‹é”™åˆ†ä¸ºæ­£ä¾‹çš„æ¦‚ç‡</li><li>å¯¹gt is nåšç»Ÿè®¡</li><li>$FPR = \frac{fp}{fp+tn}$</li><li>FPR = 1 - ç‰¹å¼‚åº¦</li></ul></li><li><p>ROC</p><ul><li>æ¯ä¸ªç‚¹çš„æ¨ªåæ ‡æ˜¯FPRï¼Œçºµåæ ‡æ˜¯TPR</li><li>æç»˜äº†åˆ†ç±»å™¨åœ¨TPï¼ˆçœŸæ­£çš„æ­£ä¾‹ï¼‰å’ŒFPï¼ˆé”™è¯¯çš„æ­£ä¾‹ï¼‰é—´çš„trade-off</li><li>é€šè¿‡å˜åŒ–é˜ˆå€¼ï¼Œå¾—åˆ°ä¸åŒçš„åˆ†ç±»ç»Ÿè®¡ç»“æœï¼Œè¿æ¥è¿™äº›ç‚¹å°±å½¢æˆROC curve</li><li>æ›²çº¿åœ¨å¯¹è§’çº¿å·¦ä¸Šæ–¹ï¼Œç¦»å¾—è¶Šè¿œè¯´æ˜åˆ†ç±»æ•ˆæœå¥½</li><li>P/Rå’ŒROCæ˜¯ä¸¤ä¸ªä¸åŒçš„è¯„ä»·æŒ‡æ ‡å’Œè®¡ç®—æ–¹å¼ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæ£€ç´¢ç”¨å‰è€…ï¼Œåˆ†ç±»ã€è¯†åˆ«ç­‰ç”¨åè€…</li></ul></li><li><p>AUC</p><ul><li>AUCçš„å€¼å°±æ˜¯å¤„äºROC curveä¸‹æ–¹çš„é‚£éƒ¨åˆ†é¢ç§¯çš„å¤§å°</li><li>é€šå¸¸ï¼ŒAUCçš„å€¼ä»‹äº0.5åˆ°1.0ä¹‹é—´</li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> è¯„ä»·æŒ‡æ ‡ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>metric learningç³»åˆ—</title>
      <link href="/2020/09/25/metric-learning%E7%B3%BB%E5%88%97/"/>
      <url>/2020/09/25/metric-learning%E7%B3%BB%E5%88%97/</url>
      <content type="html"><![CDATA[<p>å‚è€ƒï¼š<a href="https://gombru.github.io/2019/04/03/ranking_loss/ï¼Œåšä¸»å®éªŒä¸‹æ¥è§‰å¾—Triplet" target="_blank" rel="noopener">https://gombru.github.io/2019/04/03/ranking_loss/ï¼Œåšä¸»å®éªŒä¸‹æ¥è§‰å¾—Triplet</a> Loss outperforms Cross-Entropy Loss</p><h2 id="ç»¼è¿°"><a href="#ç»¼è¿°" class="headerlink" title="ç»¼è¿°"></a>ç»¼è¿°</h2><ol><li><p>metric learning</p><ul><li>å¸¸è§„çš„cls lossç³»åˆ—(CEã€BCEã€MSE)çš„ç›®æ ‡æ˜¯predict a label</li></ul></li></ol><ul><li>metric lossç³»åˆ—çš„ç›®æ ‡åˆ™æ˜¯predict relative distances between inputs<ul><li>å¸¸ç”¨åœºæ™¯ï¼šäººè„¸ &amp; fine-grained</li></ul></li></ul><ol><li><p>relation between samples</p><ul><li>first get the embedded representation</li><li>then compute the similarity score<ul><li>binary (similar / dissimilar)</li><li>regression (euclidian distance)</li></ul></li></ul></li><li><p>å¤§ç±»ï¼šä¸ç®¡å«å•¥ï¼Œä¸»ä½“ä¸Šå°±ä¸¤ç±»ï¼ŒäºŒå…ƒç»„å’Œä¸‰å…ƒç»„</p><ul><li><p>common targetï¼šæ‹‰è¿‘ç±»å†…è·ç¦»ï¼Œæ‹‰å¤§ç±»é—´è·ç¦»</p></li><li><p>pairs</p><ul><li>anchor + sample<ul><li>positive pairsï¼šdistance â€”&gt; 0</li><li>negative pairsï¼šdisctance &gt; a margin</li></ul></li><li><img src="/2020/09/25/metric-learningç³»åˆ—/pairs.png" width="50%;"></li></ul></li><li>triplets<ul><li>anchor + pos sample + neg sample</li><li>targetï¼š(dissimilar distance - similar distance) â€”&gt; a margin</li></ul></li><li><img src="/2020/09/25/metric-learningç³»åˆ—/triplet.png" width="50%;"></li></ul></li><li><p>papers</p><p>[siamese network] Signature Verification using a â€˜Siameseâ€™ Time Delay Neural Networkï¼š1993ï¼Œlecunï¼Œå­ªç”Ÿç½‘ç»œå§‹ç¥–ï¼Œä¿©ä¸ªå­ç½‘ç»œsharing weightsï¼Œè·ç¦»ç”¨çš„æ˜¯cosine distanceï¼Œlossç›´æ¥ä¼˜åŒ–è·ç¦»ï¼Œä¼˜åŒ–targetæ˜¯ä¸ªå®šå€¼cosine=1.0/-1.0</p><p>[contrastive loss] Dimensionality Reduction by Learning an Invariant Mappingï¼š2006ï¼Œlecunï¼Œcontrastive losså§‹ç¥–ï¼Œç ”ç©¶çš„æ˜¯é«˜ç»´ç‰¹å¾å‘é‡å‘ä½ç»´æ˜ å°„çš„éçº¿æ€§å±‚ï¼Œè·ç¦»ç”¨çš„æ˜¯euclidean distanceï¼Œlossä¼˜åŒ–çš„æ˜¯squared distanceï¼Œä¼˜åŒ–targetæ˜¯0å’Œmï¼Œsimilar pairsä»æ—§ä¼šè¢«æ¨å‘ä¸€ä¸ªå®šç‚¹ï¼Œæ²¡æœ‰è§£å†³è®ºæ–‡å£°ç§°çš„uniform distribution</p><p>[triplet-loss] Learning Fine-grained Image Similarity with Deep Rankingï¼š2014ï¼ŒGoogleï¼Œç”¨äº†ä¸‰å…ƒç»„ï¼Œæå‡ºäº†triplet-loss</p><p>[facenet] FaceNet: A Unified Embedding for Face Recognition and Clusteringï¼š2015ï¼ŒGoogleï¼Œç”¨æ¥è¯†åˆ«äººè„¸ï¼Œç”¨äº†ä¸‰å…ƒç»„å’Œtriplet-lossï¼Œsquared euclidean distanceï¼Œä¼˜åŒ–ç›®æ ‡æ˜¯åŒç±»å’Œå¼‚ç±»pairä¹‹é—´çš„ç›¸å¯¹è·ç¦»ï¼Œå›°éš¾æ ·æœ¬ï¼ˆsemi-hard &amp; hardï¼‰å¯¹æ”¶æ•›èµ·ä½œç”¨ï¼ˆåŠ é€Ÿï¼local minimaï¼‰ï¼Œtriplet-lossè€ƒè™‘äº†ç±»é—´çš„ç¦»æ•£æ€§ï¼Œä½†æ²¡æœ‰è€ƒè™‘ç±»å†…çš„ç´§å‡‘æ€§</p><p>[center-loss] A Discriminative Feature Learning Approach for Deep Face Recognitionï¼š2016ï¼Œä¹Ÿæ˜¯ç”¨åœ¨äººè„¸ä»»åŠ¡ä¸Šï¼Œä¼˜åŒ–ç›®æ ‡æ˜¯ç±»å†…çš„ç»å¯¹è·ç¦»ï¼Œè€Œä¸æ˜¯å»ºæ¨¡ç›¸å¯¹å…³ç³»ï¼Œcenter-lossç›´æ¥ä¼˜åŒ–çš„æ˜¯ç±»é—´çš„é—´å‡‘æ€§ï¼Œç±»é—´çš„ç¦»æ•£æ€§é çš„æ˜¯softmax loss</p><p>[triplet-center-loss] Triplet-Center Loss for Multi-View 3D Object Retrievalï¼š2018ï¼Œä¸œæ‹¼è¥¿å‡‘æ°´è®ºæ–‡</p><p>[Hinge-loss] SVM margin</p><p>[circle-loss] Circle Loss: A Unified Perspective of Pair Similarity Optimizationï¼š2020CVPRï¼Œæ—·è§†ï¼Œæå‡ºäº†cls losså’Œmetric lossçš„ç»Ÿä¸€å½¢å¼$minimize(s_n - s_p+m)$ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šæå‡ºcircle lossä½œä¸ºä¼˜åŒ–ç›®æ ‡$(\alpha_n s_n - \alpha_p s_p) = m$ï¼Œåœ¨toy scenarioä¸‹å±•ç¤ºäº†åˆ†ç±»è¾¹ç•Œå’Œæ¢¯åº¦çš„æ”¹å–„ã€‚</p><p>ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½</p><p>[Hierarchical Similarity] Learning Hierarchical Similarity Metricsï¼š2012CVPRï¼Œ</p><p>[Hierarchical Triplet Loss] Deep Metric Learning with Hierarchical Triplet Lossï¼š2018ECCVï¼Œ</p><ul><li>Hierarchical classicificationåº”è¯¥å•ç‹¬åšä¸€ä¸ªç³»åˆ—ï¼Œtobeadded</li></ul></li><li><p>ä¸€äº›å¾…æ˜ç¡®çš„é—®é¢˜</p><ul><li>anchoræ€ä¹ˆé€‰ï¼šfacenetä¸­è¯´æ˜ï¼Œæ¯ä¸ªmini-batchä¸­æ¯ä¸ªç±»åˆ«å¿…é¡»éƒ½æœ‰</li><li>pairsæ€ä¹ˆå®ç°ï¼ˆå›°éš¾çš„å®šä¹‰ï¼‰ï¼šfacenetä¸­è¯´æ˜ï¼Œhard distance sample in mini-batch</li><li>hingeloss &amp; SVMæ¨å¯¼</li><li>å¸¸è§„ä½¿ç”¨ï¼Ÿç»“åˆcls losså’Œmetric lossè¿˜æ˜¯åªç”¨metric lossï¼šcls losså’Œmetric lossæœ¬è´¨ä¸Šæ˜¯ä¸€æ ·çš„ï¼Œéƒ½æ˜¯å¸Œæœ›åŒç±»æ ·æœ¬è¾“å‡ºä¸€æ ·ï¼Œä¸åŒç±»æ ·æœ¬è¾“å‡ºä¸ä¸€æ ·ï¼Œåªä¸è¿‡å‰è€…å…·æœ‰æ¦‚ç‡æ„ä¹‰ï¼Œåè€…å…·æœ‰è·ç¦»æ„ä¹‰ã€‚ä¸Šé¢åˆ—å‡ºæ¥çš„åªæœ‰center lossæ˜¯è¦è·Ÿcls lossç»“åˆèµ·æ¥ç”¨çš„ï¼Œå› ä¸ºä»–åªé’ˆå¯¹ç±»å†…ï¼Œä¸è¶³ä»¥æ¨åŠ¨æ•´ä¸ªæ¨¡å‹ã€‚</li></ul></li></ol><h2 id="Signature-Verification-using-a-â€˜Siameseâ€™-Time-Delay-Neural-Network"><a href="#Signature-Verification-using-a-â€˜Siameseâ€™-Time-Delay-Neural-Network" class="headerlink" title="Signature Verification using a â€˜Siameseâ€™ Time Delay Neural Network"></a>Signature Verification using a â€˜Siameseâ€™ Time Delay Neural Network</h2><ol><li>åŠ¨æœº<ul><li>verification of written signatures </li><li>propose Siamese <ul><li>two identical sub-networks </li><li>joined at their outputs </li><li>measure the distance</li></ul></li><li>verification process<ul><li>a stored feature vector </li><li>a chosen threshold</li></ul></li></ul></li><li>æ–¹æ³•<ul><li>network<ul><li>two inputsï¼šextracting features</li><li>two sub-networksï¼šshare the same weights</li><li>one outputï¼šcosine of the angle between two feature vectors </li><li>target<ul><li>two real signaturesï¼šcosine=1.0</li><li>with one forgeryï¼šcosine=-0.9 and cosine=-1.0</li></ul></li><li>dataset<ul><li>50% genuine:genuine pairs</li><li>40% genuine:forgery pairs</li><li>10% genuine:zero-effort pairs</li></ul></li></ul></li></ul></li></ol><p><img src="/2020/09/25/metric-learningç³»åˆ—/siamese.png" width="40%;"></p><h2 id="Dimensionality-Reduction-by-Learning-an-Invariant-Mapping"><a href="#Dimensionality-Reduction-by-Learning-an-Invariant-Mapping" class="headerlink" title="Dimensionality Reduction by Learning an Invariant Mapping"></a>Dimensionality Reduction by Learning an Invariant Mapping</h2><ol><li><p>åŠ¨æœº</p><ul><li>dimensionality reduction</li><li>propose Dimensionality Reduction by Learning an Invariant Mapping (DrLIM)<ul><li>globally co- herent non-linear function</li><li>relies solely on neighbor- hood relationships</li><li>invariant to certain transformations of the inputs</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>most existing dimensionality reduction techniques <ul><li>they do not produce a function (or a mapping) from input to manifold </li><li>new points with unknown relationships with training samples cannot be processed </li><li>they tend to cluster points in output space </li><li>a uniform distribution in the outer manifolds is desirable </li></ul></li><li>proposed DrLIM <ul><li>globally coherent non-linear function </li><li>neighborhood relationships that are independent from any distance metric</li><li>invariant to complicated non-linear trnasformations <ul><li>lighting changes</li><li>geometric distortions</li></ul></li><li>can be used to map new samples </li></ul></li><li>empoly contrastive loss <ul><li>neighbors are pulled together </li><li>non-neighbors are pushed apart</li></ul></li><li>energy based model <ul><li>euclidean distance  </li><li>approximates the â€œsemantic similarityâ€of the inputs in input space</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>contrastive loss </p><ul><li><p>conventional loss sum over samples</p></li><li><p>contrastive loss sum over pairs $(X_1, X_2, Y)$</p><ul><li>similar pairsï¼š$Y=0$</li><li>dissimilarï¼š$Y=1$</li></ul></li><li><p>euclidean distance </p><ul><li><p>$L = (1-Y)\sum L_s ||G_w(X_1)-G_w(X_2)||_2 + Y\sum L_d ||G_w(X_1)-G_w(X_2)||_2$</p></li><li><p>$L_s$ should results in low values for similar pairs </p></li><li>$L_d$ should results in high values for dissimilar pairs</li><li>exact formï¼š$L(W,Y,X_1,X_2) = (1-Y)\frac{1}{2}(D^2) + (Y)\frac{1}{2} \{max(0,m-D)\}^2$</li></ul><p><img src="/2020/09/25/metric-learningç³»åˆ—/contrastive loss.png" width="40%;"></p></li></ul></li><li><p>spring model analogy</p><ul><li>similar partial lossç›¸å½“äºç»™å¼¹ç°§æ–½åŠ äº†ä¸€ä¸ªæ’å®šçš„åŠ›ï¼Œå‘ä¸­å¿ƒç‚¹æŒ¤å‹</li><li><p>dissimilar partial lossåªå¯¹åœˆå†…çš„ç‚¹æ–½åŠ›ï¼Œæ¨å‡ºåœˆå¤–å°±ä¸ç®¡äº†</p><p><img src="/2020/09/25/metric-learningç³»åˆ—/spring.png" width="40%;"></p></li></ul></li></ul></li></ol><h2 id="FaceNet-A-Unified-Embedding-for-Face-Recognition-and-Clustering"><a href="#FaceNet-A-Unified-Embedding-for-Face-Recognition-and-Clustering" class="headerlink" title="FaceNet: A Unified Embedding for Face Recognition and Clustering"></a>FaceNet: A Unified Embedding for Face Recognition and Clustering</h2><ol><li><p>åŠ¨æœº</p><ul><li><p>face tasks</p><ul><li>face verification: is this the same person</li><li>face recognition: who is the person</li><li>clustering: find common people among the faces</li></ul></li><li><p>learn a mapping</p><ul><li>compact Euclidean space</li><li><p>where the Euclidean distance directly correspond to face similarity</p><p><img src="/2020/09/25/metric-learningç³»åˆ—/face similarity.png" width="40%;"></p></li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>traditionally training classification layer</p><ul><li><p>generalizes well to new facesï¼Ÿ                      <strong>indirectness</strong></p></li><li><p>large dimension feature representation         <strong>inefficiency</strong></p></li><li><p>use siamese pairs</p><ul><li>the loss encourages all faces of one identity to project onto a single point</li></ul></li></ul></li><li><p>this paper</p><ul><li>employ triplet loss<ul><li>targetï¼šseparate the positive pair from the negative by a distance margin </li><li>allows the faces of one identity to live on a manifold</li></ul></li><li>obtain face embedding<ul><li>l2 norm</li><li>a fixed d-dims hypersphere </li></ul></li><li>large dataset  <ul><li>to attain the appropriate invariances to pose, illumination, and other variational conditions</li></ul></li><li><p>architecture</p><ul><li>explore two different deep network</li></ul><p><img src="/2020/09/25/metric-learningç³»åˆ—/facenet.png" width="40%;"></p></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>inputï¼šä¸‰å…ƒç»„ï¼Œconsist of two matching face thumbnails and a non-matching one </p></li><li><p>ouputï¼šç‰¹å¾æè¿°ï¼Œa compact 128-D embedding <strong>living on the fixed hypersphere $||f(x)||_2=1$</strong></p></li><li><p>triple-loss</p><ul><li>targetï¼šall anchor-pos distances are smaller than any anchor-neg distances with a least margin $\alpha$</li><li>$L = \sum_i^N [||f(x_i^a) - f(x_i^p)||_2^2 - ||f(x_i^a) - f(x_i^n)||_2^2 + \alpha]$</li><li>hard triplets </li></ul></li><li><p>hard samples</p><ul><li>$argmax_{x_i^p}||f(x_i^a) - f(x_i^p)||_2^2$</li><li>$argmin_{x_i^n}||f(x_i^a) - f(x_i^n)||_2^2$</li><li>infeasible to compute over the whole setï¼šmislabelled and poorly imaged faces would dominate the hard positives and negatives </li><li>off-lineï¼šuse recent checkpoint to compute on a subset</li><li>onlineï¼šselect in mini-batch</li></ul></li><li><p>mini-batchï¼š</p><ul><li>æ¯ä¸ªç±»åˆ«éƒ½å¿…é¡»æœ‰æ­£æ ·æœ¬</li><li>è´Ÿæ ·æœ¬æ˜¯randomly sampled </li></ul></li><li><p>hard sample</p><ul><li>use all anchor-positive pairs  </li><li>selecting the hard negatives <ul><li>hardest negatives can lead to bad local minima in early stage</li><li>å…ˆpick semi-hardï¼š$||f(x_i^a) - f(x_i^p)||_2^2 &lt; ||f(x_i^a) - f(x_i^n)||_2^2$</li></ul></li></ul></li><li><p>network</p><ul><li>ä¸€ç§straightçš„ç½‘ç»œï¼Œå¼•å…¥äº†1x1 convå…ˆå‹ç¼©é€šé“</li><li>Inception modelsï¼š20x fewer paramsï¼Œ5x fewer FLOPS</li></ul></li><li><p>metric</p><ul><li>sameï¼differentæ˜¯ç”±a squared L2 distanceå†³å®š</li><li>å› æ­¤æµ‹è¯•ç»“æœæ˜¯dçš„å‡½æ•°</li><li>å®šä¹‰true acceptsï¼šåœˆå†…å¯¹çš„ï¼Œ$TA(d)=\{(i,j)\in P_{same}, with D(x_i,x_j)\leq d\}$</li><li><p>å®šä¹‰false acceptsï¼šåœˆå†…é”™çš„ï¼Œ$FA(d)=\{(i,j)\in P_{diff}, with D(x_i,x_j)\leq d\}$</p></li><li><p>å®šä¹‰validation rateï¼š$VAL(d) = \frac{|TA(d)|}{|P_{same}|}$</p></li><li>å®šä¹‰false accept rateï¼š$FAR(d) = \frac{|FA(d)|}{|P_{diff}|}$</li></ul></li></ul></li></ol><h2 id="A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition"><a href="#A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition" class="headerlink" title="A Discriminative Feature Learning Approach for Deep Face Recognition"></a>A Discriminative Feature Learning Approach for Deep Face Recognition</h2><ol><li><p>åŠ¨æœº</p><ul><li>enhance the discriminationative power of the deeply learned features</li><li>joint supervision <ul><li>softmax loss</li><li>center loss</li></ul></li><li>two key learning objectives<ul><li>inter-class dispension </li><li>intra-class compactness </li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>face recognition task requirement<ul><li>the learned features need to be not only separable but also discriminative</li><li>generalized enough for the new unseen samples</li></ul></li><li>the softmax loss only encourage the separability of features <ul><li>å¯¹åˆ†ç±»è¾¹ç•Œã€ç±»å†…ç±»é—´åˆ†å¸ƒæ²¡æœ‰ç›´æ¥çº¦æŸ</li></ul></li><li>contrastive loss &amp; triplet loss <ul><li>training pairs or triplets dramatically grows </li><li>slow convergence and instability </li></ul></li><li>we propose<ul><li>learn a center </li><li>simultaneously update the center and optimize the distances </li><li>joint supervision<ul><li>softmax loss forces the deep features of different classes staying apart </li><li>center loss efficiently pulls the deep features of the same class to their centers </li><li>to be more discriminationative</li></ul></li><li>the inter-class features differences are enlarged </li><li>the intra-class features variations are reduced</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>softmax vis</p><ul><li>æœ€åä¸€å±‚hidden layerä½¿ç”¨ä¸¤ä¸ªç¥ç»å…ƒ</li><li>so that we can directly plot</li><li><p>separable but still show significant intra-class variations</p><p><img src="/2020/09/25/metric-learningç³»åˆ—/softmax.png" width="50%;"></p></li></ul></li><li><p>center loss</p><ul><li><p>$L_c = \frac{1}{2} \sum_1^m ||x_i - c_{y_i}||_2^2$</p></li><li><p>update class center on mini-batchï¼š</p><script type="math/tex; mode=display">  \frac{\partial L_c}{\partial x_i} = x_i - c_{y_i}\\  \Delta c_j = \frac{\sum_i^m \delta (y_i=j) * (c_j - x_i)}{1+\sum_i^m \delta (y_i=j)}</script></li><li><p>joint supervisionï¼š</p><script type="math/tex; mode=display">  L = L_{CE} + \lambda L_c</script><p><img src="/2020/09/25/metric-learningç³»åˆ—/joint supervision.png" width="50%;"></p></li></ul></li><li><p>discussion</p><ul><li>necessity of joint supervision <ul><li>solely softmax loss â€”-&gt; large intra-class variations </li><li>solely center loss â€”-&gt; features and centers will degraded to zeros </li></ul></li><li>compared to contrastive loss and triplet loss<ul><li>using pairsï¼šsuffer from dramatic data expansion </li><li>hard miningï¼šcomplex recombination </li><li>optimizing targetï¼š<ul><li>center lossç›´æ¥é’ˆå¯¹intra-class compactnessï¼Œç±»å†…ç”¨è·ç¦»æ¥çº¦æŸï¼Œç±»é—´ç”¨softmaxæ¥çº¦æŸ</li><li>contrastive lossä¹Ÿæ˜¯ç›´æ¥ä¼˜åŒ–ç»å¯¹è·ç¦»ï¼Œç±»å†…&amp;ç±»é—´éƒ½ç”¨è·ç¦»æ¥çº¦æŸ</li><li>triplet lossæ˜¯å»ºæ¨¡ç›¸å¯¹å…³ç³»ï¼Œç±»å†…&amp;ç±»é—´éƒ½ç”¨è·ç¦»æ¥çº¦æŸ</li></ul></li></ul></li></ul></li><li><p>architecture </p><ul><li>local convolution layerï¼šå½“æ•°æ®é›†åœ¨ä¸åŒçš„åŒºåŸŸæœ‰ä¸åŒçš„ç‰¹å¾åˆ†å¸ƒæ—¶ï¼Œé€‚åˆç”¨local-Convï¼Œå…¸å‹çš„ä¾‹å­å°±æ˜¯äººè„¸è¯†åˆ«ï¼Œä¸€èˆ¬äººçš„é¢éƒ¨éƒ½é›†ä¸­åœ¨å›¾åƒçš„ä¸­å¤®ï¼Œå› æ­¤æˆ‘ä»¬å¸Œæœ›å½“convçª—å£æ»‘è¿‡è¿™å—åŒºåŸŸçš„æ—¶å€™ï¼Œæƒé‡å’Œå…¶ä»–è¾¹ç¼˜åŒºåŸŸæ˜¯ä¸åŒçš„</li><li><p>å‚æ•°é‡æš´å¢ï¼škernel_size <em> kernel_size </em> output_size <em> output_size </em> input_channel * output_channel</p><p><img src="/2020/09/25/metric-learningç³»åˆ—/centerlossnet.png" width="60%;"></p></li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li><p>hyperparamï¼š$\lambda$ and $\alpha$</p><ul><li>fix $\alpha=0.5$ and vary $\lambda$ from 0-0.1</li><li>fix $\lambda=0.003$ and vary $\alpha$ from 0.01-1</li><li><p>ç»“è®ºæ˜¯remains stable across a large rangeï¼Œæ²¡æœ‰ç»™å‡ºæœ€ä½³ï¼å»ºè®®</p><p><img src="/2020/09/25/metric-learningç³»åˆ—/hyper.png" width="50%;"></p></li></ul></li><li><p>æˆ‘çš„å®éªŒ</p><ul><li>åŠ æ¯”ä¸åŠ è®­ç»ƒæ…¢å¾—å¤š</li><li>åœ¨Mnistä¸Šæµ‹è¯•åŒæ ·çš„epochåŠ æ¯”ä¸åŠ å‡†ç¡®ç‡ä½</li><li>ä¹‹æ‰€ä»¥Center Lossæ˜¯é’ˆå¯¹äººè„¸è¯†åˆ«çš„Lossæ˜¯æœ‰åŸå› çš„ï¼Œä¸ªäººè®¤ä¸º<strong>äººè„¸çš„ä¸­å¿ƒæ€§æ›´å¼ºä¸€äº›</strong>ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸€ä¸ªäººçš„æ‰€æœ‰è„¸å–å¹³å‡å€¼ä¹‹åçš„äººè„¸æˆ‘ä»¬è¿˜æ˜¯å¯ä»¥è¾¨è¯†æ˜¯ä¸æ˜¯è¿™ä¸ªäººï¼Œæ‰€ä»¥Center Lossæ‰èƒ½å‘æŒ¥ä½œç”¨</li></ul></li></ul></li></ol><h2 id="Circle-Loss-A-Unified-Perspective-of-Pair-Similarity-Optimization"><a href="#Circle-Loss-A-Unified-Perspective-of-Pair-Similarity-Optimization" class="headerlink" title="Circle Loss: A Unified Perspective of Pair Similarity Optimization"></a>Circle Loss: A Unified Perspective of Pair Similarity Optimization</h2><ol><li><p>åŠ¨æœº</p><ul><li>pair similarity</li><li>circular decision boundary</li><li>unify cls-based &amp; metric-based data<ul><li>class-level labels  </li><li>pair-wise labels </li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>there is no intrinsic difference between softmax loss &amp; metric loss</p><ul><li>minimize between-class similarity $s_n$</li><li>maximize within- class similarity $s_p$</li><li>reduce $s_n - s_p$</li></ul></li><li><p>short-commings</p><ul><li>lack of flexibilityï¼š$s_p$å’Œ$s_n$çš„ä¼˜åŒ–é€Ÿåº¦å¯èƒ½ä¸åŒï¼Œä¸€ä¸ªå¿«æ”¶æ•›äº†ä¸€ä¸ªè¿˜å¾ˆå·®ï¼Œè¿™æ—¶å€™ç”¨åŒæ ·çš„æ¢¯åº¦å»æ›´æ–°å°±éå¸¸inefficient and irrationalï¼Œå°±å·¦å›¾æ¥è¯´ï¼Œä¸‹é¢çš„ç‚¹ç›¸å¯¹ä¸Šé¢çš„ç‚¹ï¼Œ$s_n$æ›´å°ï¼ˆæ›´æ¥è¿‘opï¼‰ï¼Œ$s_p$æ›´å°ï¼ˆæ›´è¿œç¦»opï¼‰ï¼Œvice versaï¼Œä½†æ˜¯å†³ç­–å¹³é¢å¯¹ä¸‰ä¸ªç‚¹ç›¸å¯¹äº$s_n$å’Œ$s_p$çš„æ¢¯åº¦éƒ½æ˜¯ä¸€æ ·çš„ï¼ˆ1å’Œ-1ï¼‰ã€‚</li><li><p>ambiguous convergence statusï¼šç”¨ä¸€ä¸ªhard distance marginæ¥æè¿°decision boundaryè¿˜ä¸å¤Ÿdiscriminativeï¼Œhard decision boundaryä¸Šå„ç‚¹å…¶å®è¿˜æ˜¯æœ‰å·®åˆ«çš„ï¼Œå‡è®¾å­˜åœ¨ä¸€ä¸ªoptimumï¼ˆ$s_p=1 \ \&amp; \ s_n=0$ï¼‰ï¼Œé‚£ä¹ˆå·¦å›¾å†³ç­–å¹³é¢ä¸Šä¸¤ä¸ªç‚¹ï¼Œç›¸å¯¹optimumçš„æ„ä¹‰æ˜æ˜¾ä¸ä¸€æ ·ï¼Œå†³ç­–å¹³é¢åº”è¯¥æ˜¯ä¸ªå›´ç»•optimumçš„åœ†åœˆã€‚</p><p><img src="/2020/09/25/metric-learningç³»åˆ—/circle loss.png" width="40%;"></p></li></ul></li><li><p>propose circle loss</p><ul><li>independent weighting factorsï¼šç¦»optimumè¶Šè¿œçš„penalty strengthè¶Šå¤§ï¼Œè¿™ä¸€é¡¹ç›´æ¥ä»¥è·ç¦»ä¸ºä¼˜åŒ–ç›®æ ‡çš„losséƒ½æ˜¯æ»¡è¶³çš„</li><li>different penalty strengthï¼š$s_p$å’Œ$s_n$ learn at different pacesï¼Œç±»å†…åŠ æƒï¼ŒåŠ æƒç³»æ•°æ˜¯learnable params</li><li>$(\alpha_n s_n - \alpha_p s_p) = m$ï¼šyielding a circle shape </li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>æ ¸å¿ƒï¼š$(\alpha_n s_n - \alpha_p s_p) = m$</p></li><li><p>self-paced weighting</p><ul><li><p>given optimum $O_p$ and $O_n$ï¼Œfor each similarity scoreï¼š</p><script type="math/tex; mode=display">  \begin{cases}  a_p^i = [O_p - s_p^i]_+ \\  a_n^j = [s_n^j - O_n]_+  \end{cases}</script></li><li><p>cut-off at zero </p></li><li><p>å¯¹äºè¿œç¦»optimumçš„ç‚¹æ¢¯åº¦æ”¾å¤§ï¼Œæ¥è¿‘optimumçš„ç‚¹ï¼ˆå¿«æ”¶æ•›ï¼‰æ¢¯åº¦ç¼©å°</p></li><li><p>softmaxé‡Œé¢é€šå¸¸ä¸ä¼šå¯¹åŒç±»æ ·æœ¬é—´åšè¿™ç§rescalingçš„ï¼Œå› ä¸ºå®ƒå¸Œæœ›æ‰€æœ‰æ ·æœ¬valueéƒ½è¾¾åˆ°è´¼å¤§</p></li><li><p>Circle loss abandons the interpretation of classifying a sample to its target class with a large probability </p></li></ul></li><li><p>margin</p><ul><li>adding a margin m reinforces the optimization </li><li>take toy scenario <ul><li>æœ€ç»ˆæ•´ç†æˆï¼š$(s_n-0)^2 + (s_p-1)^2 = 2m^2$</li><li>op targetï¼š$s_p &gt; 1-m$ï¼Œ$s_n &lt; m$</li><li>relaxation factor $m$ï¼šcontrols the radius of the decision boundary </li></ul></li></ul></li><li><p>unified perspective </p><ul><li>tranverse all the similarity pairsï¼š$\{s_p^i\}^K$å’Œ$\{s_n^j\}^N$</li><li>to reduce $(s_n^j - s_p^i)$ï¼š$L_{uni}=log[1+\sum^K_i \sum^N_j exp(\lambda (s_n^j - s_p^i + m))]$</li><li>è§£è€¦ï¼ˆä¸ä¼šåŒæ—¶æ˜¯$s_p$å’Œ$s_n$ï¼‰ï¼š$L_{uni}=log[1+\sum^N_j exp(\lambda (s_n^j + m))\sum^K_i exp(\lambda (-s_p^i))]$</li><li>given class labelsï¼š<ul><li>we get $(N-1)$ between-class similarity scores and $(1)$ within-class similarity score </li><li>åˆ†æ¯ç¿»ä¸Šå»ï¼š$L = -log \frac{exp(\lambda (s_p-m))}{exp(\lambda (s_p-m)) + \sum^{N-1}_j exp(\lambda (s_n^j))}$</li><li>å°±æ˜¯softmax</li></ul></li><li>given pair-wise labelsï¼š<ul><li>triplet loss with hard miningï¼šfind pairs with large $s_n$ and low $s_p$</li><li>use infiniteï¼š$L=lim_{\lambda \to \inf} \frac{1}{\lambda} L_{uni}$</li></ul></li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li>Face recognition <ul><li>noisy and long-tailed dataï¼šå»å™ªå¹¶ä¸”å»æ‰ç¨€ç–æ ·æœ¬</li><li>resnet &amp; 512-d feature embeddings &amp; cosine distance </li><li>$\lambda=256$ï¼Œ$m=0.25$</li></ul></li><li>Person re-identification <ul><li>$\lambda=128$ï¼Œ$m=0.25$</li></ul></li><li>Fine-grained image retrieval <ul><li>è½¦é›†å’Œé¸Ÿé›†</li><li>bn-inception &amp; 512-d embeddings</li><li>P-K sampling</li><li>$\lambda=80$ï¼Œ$m=0.4$</li></ul></li><li>hyper-params<ul><li>the scale factor $\lambda$ï¼š<ul><li>determines the largest scale of each similarity score </li><li>Circle loss exhibits high robustness on $\lambda$</li><li>the other two becomes unstable with larger  $\lambda$</li><li>owing to the decay factor</li></ul></li><li>the relaxation factor mï¼š<ul><li>determines the radius of the circular decision boundary </li><li>surpasses the best performance of the other two in full range</li><li>robustness </li></ul></li><li><img src="/2020/09/25/metric-learningç³»åˆ—/circle hyper.png" width="50%;"></li></ul></li></ul></li><li><p>inference</p><ul><li>å¯¹äººè„¸ç±»ä»»åŠ¡ï¼Œé€šå¸¸ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆä¸€ä¸ªäººè„¸æ ‡å‡†åº•åº“ï¼Œç„¶åæ¯æ¬¡æ¨ç†çš„æ—¶å€™å¾—åˆ°æµ‹è¯•æ•°æ®çš„ç‰¹å¾å‘é‡ï¼Œå¹¶åœ¨æ ‡å‡†åº•åº“ä¸­æœç´¢ç›¸ä¼¼åº¦æœ€é«˜çš„ç‰¹å¾ï¼Œå®Œæˆäººè„¸è¯†åˆ«è¿‡ç¨‹ã€‚</li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> åº¦é‡å­¦ä¹ ï¼Œloss &amp; network </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>efficientå‘¨è¾¹</title>
      <link href="/2020/09/23/efficient%E5%91%A8%E8%BE%B9/"/>
      <url>/2020/09/23/efficient%E5%91%A8%E8%BE%B9/</url>
      <content type="html"><![CDATA[<p>å› ä¸ºä¸æ˜¯googlenetå®¶æ—å®˜æ–¹å‡ºå“ï¼Œæ‰€ä»¥æ”¾åœ¨å¤–é¢</p><p>[EfficientFCN] EfficientFCN: Holistically-guided Decoding for Semantic Segmentationï¼šå•†æ±¤ï¼Œä¸»è¦é’ˆå¯¹upsamplingæ˜¯å±€éƒ¨æ„Ÿå—é‡ï¼Œé‡å»ºå¤±çœŸå¤šï¼Œåˆ†å‰²ç²¾åº¦å·®çš„é—®é¢˜ï¼Œæå‡ºäº†Holistically-guided Decoder (HGD) ï¼Œç”¨æ¥recover the high-resolution (OS=8) feature mapsï¼Œæƒ³æ³•ä¸Šæ¥è¿‘SCSE-blockï¼Œæ•°å­¦è¡¨è¾¾ä¸Šæ¥è¿‘bilinear-CNNï¼Œæ€§èƒ½æå‡ä¸»è¦å½’å› äºeff backå§ã€‚</p><h2 id="EfficientFCN-Holistically-guided-Decoding-for-Semantic-Segmentation"><a href="#EfficientFCN-Holistically-guided-Decoding-for-Semantic-Segmentation" class="headerlink" title="EfficientFCN: Holistically-guided Decoding for Semantic Segmentation"></a>EfficientFCN: Holistically-guided Decoding for Semantic Segmentation</h2><ol><li><p>åŠ¨æœº</p><ul><li>Semantic Segmentation <ul><li>dilatedFCNï¼šcomputational complexity </li><li>encoder-decoderï¼šperformance</li></ul></li><li>proposed EfficientFCN <ul><li>common back without dilated convolution </li><li>holistically-guided decoder </li></ul></li><li>balance performance and efficiency</li></ul></li><li><p>è®ºç‚¹</p><ul><li>key elements for semantic segmentation<ul><li>high-resolution feature maps </li><li>pre-trained weights</li></ul></li><li>OS32 feature mapï¼šthe fine-grained structural information is discarded </li><li>dilated convolutionï¼šno extra parameters introduced but equire high computational complexity and memory consumption </li><li>encoder-decoder based methods<ul><li>repeated upsampling + skip connection procedure<ul><li>upsampling</li><li>concatï¼add</li><li>successive convs</li></ul></li><li>Even with the skip connections, lower-level high-resolution feature maps cannot provide abstractive enough features for achieving high- performance segmentation </li><li>The  bilinear upsampling or deconvolution operations are conducted in a local manner(from a limited receptive filed)</li><li>improvements<ul><li>reweightï¼šSE-block</li><li>scales each feature channel but maintains the original spatial size and structuresï¼šã€scse blockå¯¹spacialæœ‰åŠ æƒå•Šã€‘</li></ul></li></ul></li><li><p>propose EfficientFCN</p><ul><li>widely used classification model </li><li>Holistically-guided Decoder (HGD) <ul><li>take OS8, OS16, OS32 feature maps from backbone</li><li>OS8å’ŒOS16ç”¨æ¥spatially guiding the feature upsampling process </li><li>OS32ç”¨æ¥encode the global contextç„¶ååŸºäºguidanceè¿›è¡Œä¸Šé‡‡æ ·</li><li>linear assembly at each high-resolution spatial locationï¼šæ„Ÿè§‰å°±æ˜¯å¯¹ä¸Šé‡‡æ ·ç‰¹å¾å›¾åšäº†åŠ æƒ</li></ul></li></ul><p><img src="/2020/09/23/efficientå‘¨è¾¹/FCN.png" width="50%;"></p></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>Holistically-guided Decoder </p><ul><li>multi-scale feature fusion </li><li>holistic codebook generation<ul><li>from high-level feature maps </li><li>holistic codewordsï¼šwithout any spatial order </li></ul></li><li><p>codeword assembly </p><p><img src="/2020/09/23/efficientå‘¨è¾¹/hgd.png" width="70%;"></p></li></ul></li><li><p>multi-scale feature fusion </p><ul><li>we observe the fusion of multi-scale feature maps generally result in better performance </li><li>compressï¼šseparate 1x1 convs</li><li>bilinear downsampï¼upsamp</li><li>concatenate</li><li>fused OS32 $m_{32}$ &amp; fused OS8 $m_8$</li></ul></li><li><p>holistic codebook generation</p><ul><li>from $m_{32}$</li><li>two separate 1x1 conv<ul><li>a codeword based map $B \in R^{1024<em>(H/32)</em>(W/32)}$ï¼šæ¯ä¸ªä½ç½®ç”¨ä¸€ä¸ª1024-dimçš„vectoræ¥æè¿°</li><li>n spatial weighting map $A\in R^{n<em>(H/32)</em>(W/32)}$ï¼šhighlight ç‰¹å¾å›¾ä¸Šä¸åŒåŒºåŸŸ<ul><li>softmax norm in spatial-dim</li><li>$\widetilde A_i(x,y)=\frac{exp(A_i(x,y))}{\sum_{p,q} exp(A_i(p,q))}, i\in [0,n)$</li></ul></li></ul></li><li>codeword $c_i \in R^{1024}$<ul><li>global description for each weighting map </li><li>weighted average of B on all locations</li><li>$c_i = \sum_{p,q} \widetilde A_i(p,q) B(p,q)$</li><li>each codeword captures certain aspect of the global context </li></ul></li><li>orderless high-level global features $C \in R^{1024*n}$<ul><li>$C = [c_1, â€¦, c_n]$</li></ul></li></ul></li><li><p>codeword assembly </p><ul><li>raw guidance map $G \in R^{1024<em>(H/8)</em>(W/8)}$ï¼š1x1 conv on $m_8$</li><li>fuse semantic-rich feature map $\overline B \in R^{1024}$ï¼šglobal average vector </li><li>novel guidance feature map $\overline G = G \oplus \overline B $ï¼šlocation-wise additionã€ï¼Ÿï¼Ÿï¼Ÿï¼Ÿã€‘</li><li>linear assembly weights of the n codewords $W \in R^{n<em>(H/8)</em>(W/8)}$ï¼š1x1 conv on $\overline G$</li><li>holistically-guided upsampled feature $\tilde f_8 = W^T C$ï¼šreshape &amp; dot</li><li>final feature map $f_8$ï¼šconcat $\tilde f_8$ and $G$</li></ul></li><li><p>final segmentation</p><ul><li>1x1 conv</li><li>further upsampling </li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li><p>numer of holistic codewords</p><ul><li>32-512ï¼šincrease</li><li>512-1024ï¼šslight drop</li><li><p>we observe the number of codewords needed is approximately 4 times than the number of classes </p><p><img src="/2020/09/23/efficientå‘¨è¾¹/n.png" width="50%;"></p></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> è¯­ä¹‰åˆ†å‰² </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>data aug</title>
      <link href="/2020/09/18/data-aug/"/>
      <url>/2020/09/18/data-aug/</url>
      <content type="html"><![CDATA[<p>[mixup] mixup: BEYOND EMPIRICAL RISK MINIMIZATIONï¼šå¯¹ä¸åŒç±»åˆ«çš„æ ·æœ¬ï¼Œä¸ä»…å¯ä»¥ä½œä¸ºæ•°æ®å¢å¹¿æ‰‹æ®µï¼Œè¿˜å¯ä»¥ç”¨äºsemi-supervised learningï¼ˆMixMatchï¼‰</p><p>[mixmatch] MixMatch: A Holistic Approach to Semi-Supervised Learningï¼šé’ˆå¯¹åŠç›‘ç£æ•°æ®çš„æ•°æ®å¢å¹¿</p><p>[mosaic] from YOLOv4</p><p>[AutoAugment] AutoAugment: Learning Augmentation Policies from Dataï¼šgoogle</p><p>[RandAugment] RandAugment: Practical automated data augmentation with a reduced search spaceï¼šgoogle</p><h2 id="RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space"><a href="#RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space" class="headerlink" title="RandAugment: Practical automated data augmentation with a reduced search space"></a>RandAugment: Practical automated data augmentation with a reduced search space</h2><ol><li><p>åŠ¨æœº</p><ul><li>AutoAugment<ul><li>separate search phase</li><li>run on a subset of a huge dataset</li><li>unable to adjust the regularization strength based on model or dataset size</li></ul></li><li><p>RandAugment</p><ul><li>significantly reduced search space</li><li>can be used uniformly across tasks and datasets</li><li>match or exceeds the previous val acc</li></ul><p><img src="/2020/09/18/data-aug/acc.png" width="50%"></p></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>formulation</p><ul><li><p>always select a transformation with uniform prob $\frac{1}{K}$</p></li><li><p>given N transformations for an imageï¼šthere are $K^N$ potential policies</p></li><li><p>fixied magnitude schedule Mï¼šwe choose Constantï¼Œå› ä¸ºåªè¦ä¸€ä¸ªhyper</p><p>  <img src="/2020/09/18/data-aug/magnitude.png" width="50%"></p></li><li><p>run naive grid search</p></li></ul></li><li><p>ç–‘é—®ï¼šè¿™æ ·æ¯ä¸ªopç­‰æ¦‚ç‡ï¼Œå°±ä¸å†data-specificäº†ï¼Œä¹Ÿçœ‹ä¸å‡ºè‡ªç„¶å›¾åƒæ›´prefer color transformationè¿™ç§ç»“è®ºäº†</p></li></ul></li></ol><h2 id="AutoAugment-Learning-Augmentation-Policies-from-Data"><a href="#AutoAugment-Learning-Augmentation-Policies-from-Data" class="headerlink" title="AutoAugment: Learning Augmentation Policies from Data"></a>AutoAugment: Learning Augmentation Policies from Data</h2><ol><li><p>åŠ¨æœº</p><ul><li><p>search for data augmentation policies</p></li><li><p>propose AutoAugment</p><ul><li>create a search space composed of augmentation sub-policies<ul><li>one sub-policy is randomly choosed per image per mini-batch</li><li>a sub-policy consists of two base operations </li></ul></li><li><p>find the best policyï¼šyields the highest val acc on the target dataset</p></li><li><p>the learned policy can transfer</p></li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>data augmentation<ul><li>to teach a model about invariance</li><li>in data domain is easier than hardcoding it into model architecture</li><li>currently dataset-specific and often do not transferï¼š<ul><li>MNISTï¼šelastic distortions, scale, translation, and rotation </li><li>CIFAR &amp; ImageNetï¼šrandom cropping, image mirroring and color shifting / whitening </li><li>GANï¼šç›´æ¥ç”Ÿæˆå›¾åƒï¼Œæ²¡æœ‰å½’çº³policy</li></ul></li></ul></li><li>we aim to automate the process of finding an effective data augmentation policy for a target dataset<ul><li>each policyï¼š<ul><li>operations in certain order</li><li>probabilities after applying</li><li>magnitudes </li></ul></li><li>use reinforcement learning as the search algorithm</li></ul></li><li>contributions<ul><li>SOTA on CIFAR &amp; ImageNet &amp; SVHN </li><li>new insight on transfer learningï¼šä½¿ç”¨é¢„è®­ç»ƒæƒé‡æ²¡æœ‰æ˜¾è‘—æå‡çš„datasetä¸Šï¼Œä½¿ç”¨åŒæ ·çš„aug policiesåˆ™ä¼šæ¶¨ç‚¹</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>formulation</p><ul><li><p>search space of policies</p><ul><li>policyï¼ša policy consists of 5 sub-policies </li><li>sub-policyï¼šeach sub-policy consisting of two image operations</li><li>operationï¼šeach operation is also associated with two hyperparameters<ul><li>probabilityï¼šof applying the operationï¼Œuniformly discrete into 11 values</li><li>magnitudeï¼šof the operationï¼Œuniformly discrete into 10 values</li></ul></li><li>a mini-batch share the same chosen sub-policy</li></ul></li><li><p>operationsï¼š16 in totalï¼Œmainly use PIL</p><p>  <img src="/2020/09/18/data-aug/operations.png" width="80%"></p><ul><li><a href="https://blog.csdn.net/u011583927/article/details/104724419æœ‰å„ç§operationçš„å¯è§†åŒ–æ•ˆæœ" target="_blank" rel="noopener">https://blog.csdn.net/u011583927/article/details/104724419æœ‰å„ç§operationçš„å¯è§†åŒ–æ•ˆæœ</a></li><li>shearæ˜¯ç æ‰å›¾åƒä¸€ä¸ªè§’çš„ç•¸å˜</li><li>equalizeæ˜¯ç›´æ–¹å›¾å‡è¡¡åŒ–</li><li>solarizeæ˜¯åŸºäºä¸€å®šé˜ˆå€¼çš„invertï¼Œé«˜äºé˜ˆå€¼invertï¼Œä½äºé˜ˆå€¼ä¸å˜</li><li>posterizeä¹Ÿæ˜¯ä¸€ç§åƒç´ å€¼æˆªæ–­æ“ä½œ</li><li>coloræ˜¯è°ƒæ•´é¥±å’Œåº¦ï¼Œmag&lt;1è¶‹è¿‘ç°åº¦å›¾</li><li>sharpnesså†³å®šå›¾åƒæ¨¡ç³Š/é”åŒ–</li><li>sample pairingï¼šä¸¤å¼ å›¾åŠ æƒæ±‚å’Œï¼Œä½†æ˜¯ä¸æ”¹å˜æ ‡ç­¾</li></ul></li><li><p>searching goal</p><ul><li>with $(16<em>10</em>11)^2$ choices of sub-policies</li><li>we want 5</li></ul></li></ul></li><li><p>example</p><ul><li>ä¸€ä¸ªsub-policyåŒ…å«ä¸¤ä¸ªoperation</li><li>æ¯ä¸ªoperationæœ‰ä¸€å®šçš„possibilityåš/ä¸åš</li><li><p>æ¯ä¸ªoperationæœ‰ä¸€å®šçš„magnitudeå†³å®šåšåçš„æ•ˆæœ</p><p><img src="/2020/09/18/data-aug/example.png" width="60%"></p></li></ul></li></ul></li><li><p>ç»“è®º</p><ul><li><p>On CIFAR-10, AutoAugment picks mostly color-based transformations </p></li><li><p>on ImageNet, AutoAugment focus on color-based transformations as well, besides geometric transformation and rotate is commonly used </p><ul><li><p>one of the best policy</p><p>  <img src="/2020/09/18/data-aug/imagenet.png" width="80%"></p></li><li><p>overall results</p><p>  <img src="/2020/09/18/data-aug/imagenet2.png" width="60%"></p></li></ul></li></ul></li></ol><h2 id="mixup-BEYOND-EMPIRICAL-RISK-MINIMIZATION"><a href="#mixup-BEYOND-EMPIRICAL-RISK-MINIMIZATION" class="headerlink" title="mixup: BEYOND EMPIRICAL RISK MINIMIZATION"></a>mixup: BEYOND EMPIRICAL RISK MINIMIZATION</h2><ol><li><p>åŠ¨æœº</p><ul><li>classification task</li><li>memorization and sensitivity issue<ul><li>reduces the memorization of corrupt labels</li><li>increases the robustness to adversarial examples</li><li>improves the generalization </li><li>can be used to stabilize the training of GANs  </li></ul></li><li>propose convex combinations of pairs of examples and their labels</li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>ERM(Empirical Risk Minimization)ï¼šissue of generalization</p><ul><li>allows large neural networks to <em>memorize</em> (instead of generalize from) the training data even in the presence of strong regularization </li><li>neural networks change their predictions drastically when evaluated on examples just outside the training distribution </li></ul></li><li><p>VRM(Vicinal Risk Minimization)ï¼šintroduce data augmentation</p><ul><li>e.g. define the vicinity of one image as the set of its horizontal reflections, slight rotations, and mild scalings </li><li>vicinity share the same class </li><li>does not model the vicinity relation across examples of different classes </li></ul></li><li>ERMä¸­çš„training setå¹¶ä¸æ˜¯æ•°æ®çš„çœŸå®åˆ†å¸ƒï¼Œåªæ˜¯ç”¨æœ‰é™æ•°æ®æ¥è¿‘ä¼¼çœŸå®åˆ†å¸ƒï¼Œmemorizationä¹Ÿä¼šæœ€å°åŒ–training errorï¼Œä½†æ˜¯å¯¹training segä»¥å¤–çš„sampleå°±leads to undesirable behaviour </li><li>mixupå°±æ˜¯VRMçš„ä¸€ç§ï¼Œpropose a generic vicinal distributionï¼Œè¡¥å……vicinity relation across examples of different classes </li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>mixup</p><ul><li><p>constructs virtual training examples </p><script type="math/tex; mode=display">  x = \lambda x_i + (1-\lambda)x_j  \\  y = \lambda y_i + (1-\lambda)y_j</script></li><li><p>use two examples drawn at randomï¼šraw inputs &amp; raw one-hot labels</p></li><li><p>ç†è®ºåŸºç¡€ï¼šlinear interpolations of feature vectors should lead to linear interpolations of the associated targets </p></li><li><p>hyper-parameter $\alpha$</p><ul><li>$\lambda = np.random.beta(\alpha, \alpha)$</li></ul></li><li>controls the strength of interpolation  </li></ul></li><li><p>åˆæ­¥ç»“è®º</p></li><li><p>three or more examples mixup does not provide further gain but more computation </p></li><li><p>interpolating only between inputs with equal label did not lead to the performance gains </p></li><li><p><strong>key elemetsâ€”â€”two inputs with different label</strong></p></li><li><p>vis</p><ul><li><p>decision boundariesæœ‰äº†ä¸€ä¸ªçº¿æ€§è¿‡æ¸¡</p><p>  <img src="/2020/09/18/data-aug/boundary.png" width="30%"></p></li><li><p>æ›´å‡†ç¡® &amp; æ¢¯åº¦æ›´å°ï¼šerrorå°‘æ‰€ä»¥losså°æ‰€ä»¥æ¢¯åº¦å°ï¼Ÿï¼Ÿ</p><p>  <img src="/2020/09/18/data-aug/gradient.png" width="60%"></p></li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li><p>åˆæ­¥åˆ†ç±»å®éªŒ</p><ul><li>$\alpha \in [0.1, 0.4]$ leads to improved performanceï¼Œlargers leads to underfitting</li><li>models with higher capacities and/or longer training runs are the ones to benefit the most from mixup</li></ul></li><li><p>memorization of corrupted labels</p><ul><li>å°†æ•°æ®é›†ä¸­ä¸€éƒ¨åˆ†labelæ¢æˆrandom noise </li><li>ERMç›´æ¥è¿‡æ‹Ÿåˆï¼Œåœ¨corrupted sampleä¸Šé¢training erroræœ€å°ï¼Œæµ‹è¯•é›†ä¸Štest erroræœ€å¤§</li><li>dropoutæœ‰æ•ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä½†æ˜¯mixup outperformså®ƒ</li><li><p>corrupted labelå¤šçš„æƒ…å†µä¸‹ï¼Œdropout+mixup performs the best</p><p><img src="/2020/09/18/data-aug/memorization.png" width="60%"></p></li></ul></li><li><p>robustness to adversarial examples</p><ul><li>Adversarial examples are obtained by adding tiny (visually imperceptible) perturbations  </li><li>å¸¸è§„æ“ä½œdata augmentationï¼šproduce and train on adversarial examples </li><li>add significant computationalï¼šæ ·æœ¬æ•°é‡å¢å¤šï¼Œæ¢¯åº¦å˜åŒ–å¤§</li><li>mixup results in a smaller loss and gradient normï¼šå› ä¸ºmixupç”Ÿæˆçš„å‡æ ·æœ¬â€œæ›´åˆç†ä¸€ç‚¹â€ï¼Œæ¢¯åº¦å˜åŒ–æ›´å°</li></ul></li><li><p>ablation study</p><ul><li><p>mixup is the bestï¼šç»å¯¹é¢†å…ˆç¬¬äºŒmix input + label smoothing</p></li><li><p>the effect of regularization </p><ul><li>ERMéœ€è¦å¤§weight decayï¼Œmixupéœ€è¦å°çš„â€”â€”è¯´æ˜mixupæœ¬èº«çš„regularization effectsæ›´å¼º</li><li>é«˜å±‚ç‰¹å¾mixupéœ€è¦æ›´å¤§çš„weight decayâ€”â€”éšç€å±‚æ•°åŠ æ·±regularization effectså‡å¼±</li><li><p>AC+RPæœ€å¼º</p></li><li><p>label smoothingå’Œadd Gaussian noise to inputs ç›¸å¯¹æ¯”è¾ƒå¼±</p></li><li>mix inputs only(SMOTE) shows no gain</li></ul></li></ul><p><img src="/2020/09/18/data-aug/ablation.png" width="60%"></p></li></ul></li></ol><h2 id="MixMatch-A-Holistic-Approach-to-Semi-Supervised-Learning"><a href="#MixMatch-A-Holistic-Approach-to-Semi-Supervised-Learning" class="headerlink" title="MixMatch: A Holistic Approach to Semi-Supervised Learning"></a>MixMatch: A Holistic Approach to Semi-Supervised Learning</h2><ol><li><p>åŠ¨æœº</p><ul><li>semi-supervised learning</li><li>unify previous methods</li><li>proposed mixmatch<ul><li>guessing low-entropy labels </li><li>mixup labeled and unlabeled data </li></ul></li><li>useful for differentially private learning</li></ul></li><li><p>è®ºç‚¹</p><ul><li>semi-supervised learning add a loss term computed on unlabeled data and encourages the model to generalize better to unseen data </li><li>the loss term<ul><li>entropy minimizationï¼šdecision boundaryåº”è¯¥å°½å¯èƒ½è¿œç¦»æ•°æ®ç°‡ï¼Œå› æ­¤prediction on unlabeled dataä¹Ÿåº”è¯¥æ˜¯high confidence</li><li>consistency regularizationï¼šå¢å¼ºå‰åçš„unlabeled dataè¾“å‡ºåˆ†å¸ƒä¸€è‡´</li><li>generic regularizationï¼šweight decay &amp; mixup</li></ul></li><li>MixMatch unified all above<ul><li>introduces a unified loss term for unlabeled data  </li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>overview</p><ul><li>givenï¼ša batch of labeled examples $X$ and a batch of labeled examples $U$</li><li>augment+label guessï¼ša batch of augmented labeled examples $X^{â€˜}$ and a batch of augmented labeled examples $U^{â€˜}$</li><li>computeï¼šseparate labeled and unlabeled loss terms $L_X$ and $L_U$</li><li><p>combineï¼šweighted sum</p><p><img src="/2020/09/18/data-aug/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/data-aug/overview.png" width="50%;"></p></li></ul></li><li><p>MixMatch</p><ul><li><p>data augmentation</p><ul><li>å¸¸è§„augmentation</li><li>ä½œç”¨äºæ¯ä¸€ä¸ª$x_b$å’Œ$u_b$</li><li>$u_b$åš$K$æ¬¡å¢å¼º</li></ul></li><li><p>label guessing</p><ul><li>å¯¹å¢å¼ºçš„$K$ä¸ª$u_b$åˆ†åˆ«é¢„æµ‹ï¼Œç„¶åå–å¹³å‡</li><li>average class prediction </li></ul></li><li><p>sharpening</p><ul><li>reduce the entropy of the label distribution </li><li>æ‹‰é«˜æœ€å¤§predictionï¼Œæ‹‰å°å…¶ä»–çš„</li><li>$Sharpen (p, T)_i =\frac{p_i^{\frac{1}{T}}}{\sum^{N}_j p_j^{\frac{1}{T}}} $</li><li>$T$è¶‹è¿‘äº0çš„æ—¶å€™ï¼Œprocessed labelå°±æ¥è¿‘one-hotäº†</li></ul></li><li><p>mixup</p><ul><li><p>slightly modified form of mixup to make the generated sample being more closer to the original</p><p><img src="/2020/09/18/data-aug/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/data-aug/mixup.png" width="50%;"></p></li></ul></li><li><p>loss function</p><ul><li>labeled lossï¼štypical cross-entropy loss  </li></ul></li><li><p>unlabeled lossï¼š<strong>squared L2ï¼Œbounded and less sensitive to completely incorrect predictions</strong></p></li></ul></li><li><p>hyperparameters</p><ul><li><p>sharpening temperature $T$ï¼šfixed 0.5</p><ul><li>number of unlabeled augmentations $K$ï¼šfixed 2</li><li>MixUp Beta parameter $\alpha$ï¼š0.75 for start</li><li>unsupervised loss weight $\lambda_U$ï¼š100 for start</li></ul></li><li><p>Algorithm </p><p>  <img src="/2020/09/18/data-aug/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/data-aug/mixmatch.png" width="70%;"></p></li></ul></li></ul></li><li><p>å®éªŒ</p></li></ol><p>[mosaic] from YOLOv4</p>]]></content>
      
      
        <tags>
            
            <tag> æ•°æ®å¢å¼ºï¼Œæ ·æœ¬ä¸å¹³è¡¡ï¼ŒåŠç›‘ç£å­¦ä¹ ï¼Œåº¦é‡å­¦ä¹  </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hrnet</title>
      <link href="/2020/09/18/hrnet/"/>
      <url>/2020/09/18/hrnet/</url>
      <content type="html"><![CDATA[<ol><li><p>åŠ¨æœº</p><ul><li>human pose estimation</li><li>high-resolution representations through<ul><li>existing methods recover high-res feature from the low</li><li>this methods maintain the high-res from start to the end</li><li>repeated multi-scale fusions</li></ul></li><li>more accurate and spatially more precise</li><li>estimate on the high-res output</li></ul></li><li><p>è®ºç‚¹</p><ul><li>in parallel rather than in seriesï¼špotentially spatially more precise</li><li>repeated multi- scale fusionsï¼šboost both high&amp;low representationsï¼Œmore accurate</li><li>pose estimation <ul><li>probabilistic graphical model </li><li>regression</li><li>heatmap</li></ul></li><li>High-to-low and low-to-high <ul><li>Symmetric high-to-low and low-to-high </li><li>Heavy high-to-low and light low-to-high </li><li>Heavy high-to-low with dilated convolutions and further lighter low-to-high </li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>overview</p><p>  <img src="/2020/09/18/hrnet/hrnet.png" width="50%;"></p><ul><li>four stages </li><li>channels double when halve the res</li><li>1st stage<ul><li>4 residual unitsï¼Œbottleneck resblock</li><li>width=64</li><li>3x3 conv reducing width to C</li></ul></li><li>2ã€3ã€4 stages<ul><li>contain 1, 4, 3 exchange blocks respectively </li><li>exchange block<ul><li>convï¼š4 residual unitsï¼Œtwo 3x3 conv</li><li>exchange unit</li></ul></li></ul></li><li>width<ul><li>Cï¼šwidth of the high-resolution subnetworks in last three stages</li><li>other three parallel subnetworks <ul><li>HRNet-W32ï¼š64, 128, 256</li><li>HRNet-W48ï¼š96, 192, 384  </li></ul></li></ul></li></ul></li><li><p>parallel multi-resolution subnetworks</p><ul><li>for one stageï¼Œone produce features with same-res and a extra lower one</li><li><p>ä¸€ä¸ªparallelä»£è¡¨ä¸€ä¸ªscale</p><p><img src="/2020/09/18/hrnet/parallel.png" width="30%;"></p></li></ul></li><li><p>repeated multi-scale fusion</p><ul><li>exchange unit </li><li><p>multi-scale</p><ul><li>downsamplingï¼š3x3 s2 conv</li><li>upsamplingï¼šnearest neighbor+1x1 conv</li><li>identify connection </li><li>more factorsï¼šconsecutive layers</li></ul><p><img src="/2020/09/18/hrnet/exchange unit.png" width="50%;"></p></li><li><p>exchange block</p><ul><li>3 parallel convolution units + an exchange unit </li><li><p>å¯ä»¥è¿ç»­å‡ ä¸ªblock</p><p><img src="/2020/09/18/hrnet/exchange block.png" width="45%;"></p></li></ul></li></ul></li><li><p>heatmap estimation</p><ul><li>from the last high-res exchange unit</li><li>mse</li><li>gt gassian mapï¼šstd=1</li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> é«˜åˆ†è¾¨ç‡ï¼Œäººä½“å§¿æ€ä¼°è®¡ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>bilinear CNN</title>
      <link href="/2020/09/18/bilinear-CNN/"/>
      <url>/2020/09/18/bilinear-CNN/</url>
      <content type="html"><![CDATA[<p>17å¹´çš„paperï¼Œå¼•ç”¨é‡15ï¼Œæå‡ºäº†ç½‘è·¯ç»“æ„ï¼Œä½†æ˜¯æ²¡åˆ†æä¸ºå•¥æœ‰æ•ˆï¼Œåƒåœ¾</p><h2 id="Bilinear-CNNs-for-Fine-grained-Visual-Recognition"><a href="#Bilinear-CNNs-for-Fine-grained-Visual-Recognition" class="headerlink" title="Bilinear CNNs for Fine-grained Visual Recognition"></a>Bilinear CNNs for Fine-grained Visual Recognition</h2><ol><li><p>åŠ¨æœº</p><ul><li>fine-grained classification </li><li>propose a pooled outer product of features derived from two CNNs <ul><li>2 CNNs</li><li>a bilinear layer</li><li>a pooling layer</li></ul></li><li>outperform existing models and fairly efficient </li><li>effective at other image classification tasks such as material, texture, and scene recognition </li></ul></li><li><p>è®ºç‚¹</p><ul><li>fine-grained classification tasks require<ul><li>recognition of highly localized attributes of objects </li><li>while being invariant to their pose and location in the image </li></ul></li><li>previous techniques <ul><li>part-based models<ul><li>construct representations by localizing parts </li><li>more accurate but requires part annotations </li></ul></li><li>holistic models<ul><li>construct a representation of the entire image </li><li>texture descriptorsï¼šFVï¼ŒSIFT </li></ul></li><li>STNï¼šaugment CNNs with parameterized image transformations </li><li>attentionï¼šuse segmentation as a weakly-supervised manner  </li></ul></li><li>Our key insight is that several widely-used texture representations can be written as a pooled outer product of two suitably designed features <ul><li>several widely-used texture representations</li><li>two suitably designed features </li></ul></li><li>the bilinear features are highly redundant <ul><li>dimensionality reduction </li><li>trade-off between accuracy </li></ul></li><li><p>We also found that feature normalization and domain-specific fine-tuning offers additional benefits</p></li><li><p>combination</p><ul><li>concatenateï¼šadditional parameters to fuse</li><li>an outer productï¼šno parameters </li><li>sum productï¼šcan achieve similar approximations  </li></ul></li><li>â€œtwo-streamâ€ architectures <ul><li>one used to model two- factor variations such as â€œstyleâ€ and â€œcontentâ€ for images </li><li>in our case is to model two factor variations in location and appearance of partsï¼šä½†å¹¶ä¸æ˜¯explicit modelingå› ä¸ºæœ€ç»ˆæ˜¯ä¸ªåˆ†ç±»å¤´</li><li>one used to analyze videos modeling the temporal aspect and the spatial aspect </li></ul></li><li>dimension reduction <ul><li>two 512-dim feature results in 512x512-dim</li><li>earlier work projects one feature to a lower-dimensional space, e.g. 64-dimâ€”&gt;512x64-dim</li><li>we use compact bilinear pooling to generate low-dimensional embeddings (8-32x)</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>architecture</p><p>  <img src="/2020/09/18/bilinear-CNN/BCNN.png" width="50%;"></p><ul><li>input $(l,I)$ï¼štakes an image and a locationï¼Œlocation generally contains position and scale </li><li>quadruple $B=(f_A, f_B, P, C)$</li><li>Aã€Bä¸¤ä¸ªCNNï¼šconv+pooling layersï¼Œ</li><li>Pï¼špooling function<ul><li>combined A&amp;B outputs using the matrix outer product</li><li>average pooling</li></ul></li><li>Cï¼šlogistic regression or linear SVM <ul><li>we found that linear models are effective on top of bilinear features</li></ul></li></ul></li><li><p>CNN</p><ul><li><p>independentï¼partial sharedï¼fully shared</p><p><img src="/2020/09/18/bilinear-CNN/CNN.png" width="70%;"></p></li></ul></li><li><p>bilinear combination </p><ul><li><p>for each location</p></li><li><p>$bilinear(l,I,f_A,f_B)=f_A(l,I)^T f_B(l,I)$</p></li><li><p>pooling function combines bilinear features across all locations</p></li><li><p>$\Phi (I) = \sum_{l\in L} bilinear(l,I,f_A,f_B)$</p></li><li><p>same feature dimension K for A &amp; Bï¼Œe.g. KxM &amp; KxN respectivelyï¼Œ$\Phi(I)$ is size MxN</p></li><li><p>Normalization</p><ul><li>a signed square rootï¼š$y=sign(x)\sqrt {|x|}$</li><li><p>follow a l2 normï¼š$z = \frac{y}{||y||_2}$</p></li><li><p>improves performance in practice  </p></li></ul></li></ul></li><li><p>classification </p><ul><li>logistic regression or linear SVM  </li><li>we found that linear models are effective on top of bilinear features</li></ul></li><li><p>back propagation</p><ul><li><p>$\frac{dl}{dA}=B(\frac{dl}{dx})^T$ï¼Œ$\frac{dl}{dB}=A(\frac{dl}{dx})^T$</p></li><li><p><img src="/2020/09/18/bilinear-CNN/flow.png" width="50%;"></p></li></ul></li><li><p>Relation to classical texture representationsï¼šæ”¾åœ¨è¿™ä¸€èŠ‚æ’‘ç¯‡å¹…ï¼Ÿï¼Ÿ</p><ul><li>texture representations can be defined by the choice of the local features, the encoding function, the pooling function, and the normalization function <ul><li>choice of local featuresï¼šorderless aggregation with sumï¼max operation</li><li>encoding functionï¼šA non-linear encoding is typically applied to the local feature before aggregation</li><li>normalizationï¼šnormalization of the aggregated feature is done to increase invariance </li></ul></li><li>end-to-end trainable</li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ç»†ç²’åº¦ï¼Œç‰¹å¾èåˆ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>label smoothing</title>
      <link href="/2020/09/14/label-smoothing/"/>
      <url>/2020/09/14/label-smoothing/</url>
      <content type="html"><![CDATA[<ol><li><p>åŠ¨æœº</p><ul><li>to understand label smoothing<ul><li>improving generalization </li><li>improves model calibration </li><li>changes the representations learned by the penultimate layer of the network</li><li>effect on knowledge distillation of a student network</li></ul></li><li>soft targetsï¼ša hard target and the uniform distribution of other classes</li></ul></li><li><p>è®ºç‚¹</p><ul><li>label smoothing implicitly calibrates the learned models  <ul><li>èƒ½è®©confidencesæ›´æœ‰è§£é‡Šæ€§â€”â€”more aligned with the accuracies of their predictions</li><li>label smoothing impairs distillationâ€”â€”teacherç”¨äº†label smoothingï¼Œstudentä¼šè¡¨ç°å˜å·®ï¼Œthis adverse effect results from loss of information in the digits</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>modeling</p><ul><li>penultimate layerï¼šfc with activation<ul><li>$p_k = \frac{e^{wx}}{\sum e^{wx}}$</li></ul></li><li>outputsï¼šloss<ul><li>$H(y,p)=\sum_{k=1}^K -y_klog(p_k)$</li></ul></li><li>hard targetsï¼š$y_k$ is 1 for the correct class and 0 for the rest</li><li>label smoothingï¼š$y_k^{LS} = y_k(1-\alpha)+ \alpha /K$</li></ul></li><li><p>visualization schem </p><ul><li>å°†dimK activation vectoræŠ•å½±åˆ°æ­£äº¤å¹³é¢ä¸Šï¼Œa dim2 vector per example</li><li><p>clusters are much tighter because label smoothing encourages that each example in training set to be equidistant from all the other classâ€™s templates</p></li><li><p>3 classes shows triangle structure since â€˜equidistantâ€™</p></li><li>predictionsâ€˜ absolute values are much bigger without LM, representing over-confident</li><li>semantically similar classes are harder to separateï¼Œä½†æ˜¯æ€»ä½“ä¸Šclusterå½¢æ€è¿˜æ˜¯å¥½ä¸€ç‚¹</li><li>training without label smoothing there is continuous degree of change between two semantically similar classesï¼Œç”¨äº†LMä»¥åå°±è§‚å¯Ÿä¸åˆ°äº†â€”â€”ç›¸ä¼¼classä¹‹é—´çš„è¯­ä¹‰ç›¸å…³æ€§è¢«ç ´åäº†ï¼Œâ€™erasure of informationâ€™</li><li>have similar accuracies despite qualitatively different clusteringï¼Œå¯¹åˆ†ç±»ç²¾åº¦çš„æå‡ä¸æ˜æ˜¾ï¼Œä½†æ˜¯ä»clusterå½¢æ€ä¸Šçœ‹æ›´å¥½çœ‹</li></ul></li><li><p>model calibration </p><ul><li><p>making the confidence of its predictions more accurately represent their accuracy </p></li><li><p>metricï¼šexpected calibration error (ECE) </p></li><li><p>reliability diagram </p><p>  <img src="/2020/09/14/label-smoothing/calibration.png" width="70%"></p></li><li><p>better calibration compared to the unscaled network </p></li><li><p>Despite trying to collapse the training examples to tiny clusters, these networks generalize and are calibratedï¼šåœ¨è®­ç»ƒé›†ä¸Šçš„clusteråˆ†å¸ƒéå¸¸ç´§å‡‘ï¼Œencourageæ¯ä¸ªæ ·æœ¬éƒ½å’Œå…¶ä»–ç±»åˆ«çš„clusterä¿æŒç›¸åŒçš„è·ç¦»ï¼Œä½†æ˜¯åœ¨æµ‹è¯•é›†ä¸Šï¼Œæ ·æœ¬çš„åˆ†å¸ƒå°±æ¯”è¾ƒæ¾æ•£äº†ï¼Œä¸ä¼šé™å®šåœ¨å°å°çš„ä¸€å¨å†…ï¼Œè¯´æ˜ç½‘ç»œæ²¡æœ‰over-confidentï¼Œrepresenting the full range of confidences for each prediction </p></li></ul></li><li><p>knowledge distillation </p><ul><li><p>even when label smoothing improves the accuracy of the teacher network, teachers trained with label smoothing produce inferior student networks </p></li><li><p>As the representations collapse to small clusters of points, much of the information that could have helped distinguish examples is lost </p><p>  <img src="/2020/09/14/label-smoothing/knowledge.png" width="40%"></p></li><li><p>çœ‹training setçš„scatterï¼ŒLMä¼šå€¾å‘äºå°†ä¸€ç±»sampleé›†ä¸­æˆä¸ºç›¸ä¼¼çš„è¡¨å¾ï¼Œsampleä¹‹é—´çš„å·®å¼‚æ€§ä¿¡æ¯ä¸¢äº†ï¼šTherefore a teacher with better accuracy is not necessarily the one that distills better</p></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> åˆ†ç±»ï¼Œloss </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>noisy student</title>
      <link href="/2020/09/11/noisy-student/"/>
      <url>/2020/09/11/noisy-student/</url>
      <content type="html"><![CDATA[<h2 id="Self-training-with-Noisy-Student-improves-ImageNet-classification"><a href="#Self-training-with-Noisy-Student-improves-ImageNet-classification" class="headerlink" title="Self-training with Noisy Student improves ImageNet classification"></a>Self-training with Noisy Student improves ImageNet classification</h2><ol><li><p>åŠ¨æœº</p><ul><li>semi-supervised learningï¼ˆSSLï¼‰</li><li>semi-supervised approach when labeled data is abundant</li><li>use unlabeled images to improve SOTA model</li><li>improve self-training and distillation</li><li>accuracy and robustness  </li><li>better acc, mCE, mFR<ul><li>EfficientNet model on labeled images</li></ul></li><li>student<ul><li>even or larger student model</li><li>on labeled &amp; pseudo labeled images</li><li>noise, stochastic depth, data augmentation</li><li>generalizes better</li></ul></li><li>process iteration<ul><li>by putting back the student as the teacher</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>supervised learning which requires a large corpus of labeled images to work well </li><li>robustness<ul><li>noisy dataï¼šunlabeled images that do not belong to any category in ImageNet</li><li>large margins on much harder test sets </li></ul></li><li>training process<ul><li>teacher<ul><li>EfficientNet model on labeled images</li></ul></li><li>student<ul><li>even or larger student model</li><li>on labeled &amp; pseudo labeled images</li><li>noise, stochastic depth, data augmentation</li><li>generalizes better</li></ul></li><li>process iteration<ul><li>by putting back the student as the teacher</li></ul></li></ul></li><li>improve in two ways<ul><li>it makes the student largerï¼šå› ä¸ºç”¨äº†æ›´å¤šæ•°æ®</li><li>noised student is forced to learn harderï¼šå› ä¸ºlabelæœ‰pseudo labelsï¼Œinputæœ‰å„ç±»augmentationï¼Œç½‘ç»œæœ‰dropoutï¼stochastic depth</li></ul></li><li>main difference compared with Knowledge Distillation <ul><li>use noise â€”â€”â€” KD do not use</li><li>use equal/larger student â€”â€”â€” KD use smaller student to learn faster</li></ul></li><li>think of as Knowledge Expansion<ul><li>giving the student model enough capacity and difficult environments</li><li>want the student to be better than the teacher </li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li>algorithm<ul><li>train teacher use labeled images</li><li>use teacher to inference unlabedled images, generating pseudo labels, soft/one-hot</li><li>train student model use labeled &amp; unlabeld images</li><li>make student the new teacher, jump to the inter step</li></ul></li><li>noise<ul><li>enforcing invariancesï¼šè¦æ±‚studentç½‘ç»œèƒ½å¤Ÿå¯¹å„ç§å¢å¼ºåçš„æ•°æ®é¢„æµ‹labelä¸€æ ·ï¼Œensure consistency </li><li>required to mimic a more powerful ensemble modelï¼šteacherç½‘ç»œåœ¨inferenceé˜¶æ®µè¿›è¡Œdropoutå’Œstochastic depthï¼Œbehaves like an ensembleï¼Œwhereas the student behaves like a single modelï¼Œè¿™å°±push studentç½‘ç»œå»å­¦ä¹ ä¸€ä¸ªæ›´å¼ºå¤§çš„æ¨¡å‹</li></ul></li><li>other techniques<ul><li>data filtering  <ul><li>we filter images that the teacher model has low confidences </li><li>è¿™éƒ¨åˆ†dataä¸training dataçš„åˆ†å¸ƒèŒƒå›´å†…</li></ul></li><li>data balancing <ul><li>duplicate images in classes where there are not enough images </li><li>take the images with the highest confidence when there are too many</li></ul></li></ul></li><li>softï¼hard pseudo labels  <ul><li>both work</li><li>soft slightly better </li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li>dataset<ul><li>benchmarked datasetï¼šImageNet 2012 ILSVRC </li><li>unlabeled datasetï¼šJFT</li><li>fillter &amp; balancingï¼š<ul><li>use EfficientNet-B0</li><li>trained on ImageNetï¼Œinference over JFT</li><li>take images with confidence over 0.3</li><li>130M at most per class</li></ul></li></ul></li><li>models<ul><li>EfficientNet-L2<ul><li>further scale up EfficientNet-B7</li><li>wider &amp; deeper</li><li>lower resolution</li><li><img src="/2020/09/11/noisy-student/l2.png" width="50%"></li><li>train-test resolution discrepancy <ul><li>first perform normal training with a smaller resolution for 350 epochs</li><li>then finetune the model with a larger resolution for 1.5 epochs on unaugmented labeled images</li><li>shallow layers are fixed during finetuning</li></ul></li></ul></li><li>noise<ul><li>stochastic depthï¼šstochastic depth 0.8 for the final layer and follow the linear decay rule for other layers </li><li>dropoutï¼šdropout 0.5 for the final layer</li><li>RandAugmentï¼štwo random operations with magnitude set to 27</li></ul></li></ul></li><li><p>iterative training</p><ul><li>ã€teacherã€‘first trained an EfficientNet-B7 on ImageNet</li><li>ã€studentã€‘then trained an EfficientNet-L2 with the unlabeled batch size set to 14 times the labeled batch size</li><li>ã€new teacherã€‘trained a new EfficientNet-L2 </li><li>ã€new studentã€‘trained an EfficientNet-L2 with the unlabeled batch size set to 28 times the labeled batch size</li><li>ã€iterationã€‘â€¦</li><li><img src="/2020/09/11/noisy-student/iteration.png" width="50%"></li></ul></li><li><p>robustness test</p><ul><li>difficult images </li><li><p>common corruptions and perturbations  </p><p><img src="/2020/09/11/noisy-student/robustness.png" width="70%"></p></li><li><p>FGSM attack </p></li><li><p>metrics</p><ul><li>improves the top-1 accuracy </li><li><p>reduces mean corruption error (mCE) </p></li><li><p>reduces mean flip rate (mFR) </p></li></ul></li></ul></li><li><p>ablation study</p><ul><li>noisy<ul><li>å¦‚æœä¸noise the studentï¼Œå½“student modelçš„é¢„æµ‹å’Œteacheré¢„æµ‹çš„unlabeledæ•°æ®å®Œå…¨ä¸€æ ·çš„æƒ…å†µä¸‹ï¼Œlossä¸º0ï¼Œä¸å†å­¦ä¹ ï¼Œè¿™æ ·studentå°±ä¸èƒ½outperform teacheräº†</li><li>injecting noise to the student model enables the teacher and the student to make different predictions </li><li>The student performance consistently drops with noise function removed </li><li>removing noise leads to a smaller drop in training lossï¼Œè¯´æ˜noiseçš„ä½œç”¨ä¸æ˜¯ä¸ºäº†preventing overfittingï¼Œå°±æ˜¯ä¸ºäº†enhance model</li></ul></li><li>iteration<ul><li>iterative training is effective in producing increas- ingly better models </li><li>larger batch size ratio for latter iteration</li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> classification, semi-supervised, teacher-student </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>complement cross entropy</title>
      <link href="/2020/09/08/complement-cross-entropy/"/>
      <url>/2020/09/08/complement-cross-entropy/</url>
      <content type="html"><![CDATA[<ol><li>summary<ul><li>ä½¿ç”¨complement lossçš„ä¸»è¦åŠ¨æœºæ˜¯one-hotçš„labelä¸‹ï¼Œceåªå…³æ³¨æ‹‰é«˜æ­£æ ·æœ¬æ¦‚ç‡ï¼Œä¸§å¤±æ‰äº†å…¶ä»–incorrectç±»åˆ«çš„ä¿¡æ¯</li><li>äº‹å®ä¸Šå¯¹äºincorrectç±»åˆ«ï¼Œå¯ä»¥è®©å…¶è¾“å‡ºæ¦‚ç‡å€¼åˆ†å¸ƒçš„ç†µå°½å¯èƒ½çš„å¤§â€”â€”ä¹Ÿå°±æ˜¯å°†è¿™ä¸ªåˆ†å¸ƒå°½å¯èƒ½æ¨å‘å‡åŒ€åˆ†å¸ƒï¼Œè®©å®ƒä»¬ä¹‹é—´äº’ç›¸éåˆ¶ä»è€Œå‡¸æ˜¾å‡ºground truthçš„æ¦‚ç‡</li><li>ä½†è¿™æ˜¯å»ºç«‹åœ¨â€œå„ä¸ªæ ‡ç­¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹â€è¿™ä¸ªå‡è®¾ä¸Šï¼Œå¦‚æœç±»åˆ«é—´æœ‰hierarchicalçš„å…³ç³»ï¼multi-labelï¼Œå°±ä¸è¡Œäº†ã€‚</li><li>åœ¨æ•°å­¦è¡¨è¾¾ä¸Šï¼Œ<ul><li>é¦–å…ˆä»ç„¶æ˜¯ç”¨ceä½œç”¨äºcorrect labelï¼Œå¸Œæœ›æ­£æ ·æœ¬æ¦‚ç‡gt_predå°½å¯èƒ½æé«˜ï¼Œæ¥è¿‘çœŸå®å€¼</li><li>ç„¶åæ˜¯ä½œç”¨äºincorrect labelçš„cceï¼Œåœ¨é™¤äº†æ­£ä¾‹pred possibilityä»¥å¤–çš„å‡ ä¸ªæ¦‚ç‡ä¸Šï¼Œè®¡ç®—äº¤å‰ç†µï¼Œå¸Œæœ›è¿™å‡ ä¸ªæ¦‚ç‡å°½å¯èƒ½æœä»å‡åŒ€åˆ†å¸ƒï¼Œæ¦‚ç‡æ¥è¿‘$\frac{1-gt_pred}{K-1}$</li><li>æˆ‘æ„Ÿè§‰è¿™å°±æ˜¯label smoothingï¼Œä¸»è¦åŒºåˆ«å°±æ˜¯cceä¸Šæœ‰ä¸ªnormé¡¹ï¼Œlabel smoothinåœ¨è®¡ç®—ceçš„æ—¶å€™ï¼Œvectorä¸­æ¯ä¸€ä¸ªincorrect labelçš„ç†µéƒ½ä¸correct labelç­‰æƒé‡ï¼Œcceå¯¹æ•´ä¸ªincorrect vectorçš„æƒé‡ä¸correct labelç­‰åŒï¼Œä¸”å¯ä»¥è°ƒæ•´ã€‚</li></ul></li></ul></li></ol><h2 id="Imbalanced-Image-Classification-with-Complement-Cross-Entropy"><a href="#Imbalanced-Image-Classification-with-Complement-Cross-Entropy" class="headerlink" title="Imbalanced Image Classification with Complement Cross Entropy"></a>Imbalanced Image Classification with Complement Cross Entropy</h2><ol><li><p>åŠ¨æœº</p><ul><li>class-balanced datasets</li><li>motivated by COT(complement objective training)<ul><li>suppressing softmax probabilities on incorrect classes during training  </li></ul></li><li>propose cce<ul><li>keep ground truth probability overwhelm the other classes</li><li>neutralizing predicted probabilities on incorrect classes</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>class imbalace<ul><li>limits generalization </li><li>resample<ul><li>oversampling on minority classes </li><li>undersampling on majority classes  </li></ul></li><li>reweight<ul><li><strong>neglect the fact that samples on minority classes may have noise or false annotations</strong></li><li>might cause poor generalization </li></ul></li></ul></li><li>observed degradation in imbalanced datasets using CE<ul><li>cross entropy mostly ignores output scores on wrong classes</li><li>neutralizing predicted probabilities on incorrect classes helps improve accuracy of prediction for imbalanced image classification </li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>complement entropy</p><ul><li>calculated on incorrect classes </li><li>N samplesï¼ŒK-dims class vector</li><li>$C(y,\hat y)=-\frac{1}{N}\sum_{i=1}^N\sum_{j=1,j \neq g}^K \frac{\hat y^j}{1-\hat y^g}log\frac{\hat y^j}{1-\hat y^g} $</li><li>the purpose is to encourage larger gap between ground truth and other classes â€”â€” when the incorrect classes obey normal distribution it reaches optimal</li><li><img src="/2020/09/08/complement-cross-entropy/complement.png" width="40%;"></li></ul></li><li><p>balanced complement entropy</p><ul><li>add balancing factor</li><li>$C^{â€˜}(y,\hat y) = \frac{1}{K-1}C(y,\hat y)$</li></ul></li><li><p>forming COTï¼š</p><p>  <img src="/2020/09/08/complement-cross-entropy/COT.png" width="45%;"></p><ul><li>twice back-propagation per each iteration <ul><li>first cross entropy </li><li>second complement entropy</li></ul></li></ul></li><li><p>CCE (Complement Cross Entropy)</p><ul><li>add modulating factorï¼š$\tilde C(y, \hat y) = \frac{\gamma}{K-1}C(y, \hat y)$ï¼Œ$\gamma=-1$</li><li>combinationï¼šCE+CCE</li></ul></li></ul></li></ol><ol><li><p>å®éªŒ</p><ul><li><p>datasetï¼š</p><ul><li>cifar</li><li>class-balanced originally</li><li>construct imbalanced variants with imbalance ratio $\frac{N_{min}}{N_{max}}$</li><li><img src="/2020/09/08/complement-cross-entropy/imbalance.png" width="40%;"></li></ul></li><li><p>test acc</p><ul><li>è®ºæ–‡çš„å®éªŒç»“æœéƒ½æ˜¯åœ¨cifarä¸Šcceå¥½äºcotå¥½äºfocal lossï¼Œåœ¨roadä¸Šcceå¥½äºcotï¼Œæ²¡æ”¾fl</li><li>å’±ä¹Ÿä¸çŸ¥é“ã€‚ã€‚ã€‚</li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> loss </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>regression loss</title>
      <link href="/2020/09/07/regression-loss/"/>
      <url>/2020/09/07/regression-loss/</url>
      <content type="html"><![CDATA[<ol><li><p>æŸå¤±å‡½æ•°ç”¨æ¥è¯„ä»·æ¨¡å‹é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„ä¸ä¸€æ ·ç¨‹åº¦</p><p> ä¸¤ç³»æŸå¤±å‡½æ•°ï¼š</p><p> <img src="/2020/09/07/regression-loss/loss.png" width="30%;"></p></li></ol><ol><li><p>ç»å¯¹å€¼loss</p><ul><li>$L(Y,f(x))=|Y-f(x)|$</li><li><img src="/2020/09/07/regression-loss/L1.png" width="30%;"></li><li>å¹³å‡ç»å¯¹å€¼æŸå¤±ï¼ŒMAEï¼ŒL1</li><li>å¯¹å¼‚å¸¸ç‚¹æœ‰æ›´å¥½çš„é²æ£’æ€§</li><li>æ›´æ–°çš„æ¢¯åº¦å§‹ç»ˆç›¸åŒï¼Œå¯¹äºå¾ˆå°çš„æŸå¤±å€¼ï¼Œæ¢¯åº¦ä¹Ÿå¾ˆå¤§ï¼Œä¸åˆ©äºæ¨¡å‹å­¦ä¹ â€”â€”æ‰‹åŠ¨è¡°å‡å­¦ä¹ ç‡</li></ul></li><li><p>å¹³æ–¹å·®loss</p><ul><li>$L(Y, f(x)) = (Y-f(x))^2$</li><li><img src="/2020/09/07/regression-loss/L2.png" width="30%;"></li><li>å‡æ–¹è¯¯å·®æŸå¤±ï¼ŒMSEï¼ŒL2</li><li>å› ä¸ºå–äº†å¹³æ–¹ï¼Œä¼šèµ‹äºˆå¼‚å¸¸ç‚¹æ›´å¤§çš„æƒé‡ï¼Œä¼šä»¥ç‰ºç‰²å…¶ä»–æ ·æœ¬çš„è¯¯å·®ä¸ºä»£ä»·ï¼Œæœç€å‡å°å¼‚å¸¸ç‚¹è¯¯å·®çš„æ–¹å‘æ›´æ–°ï¼Œé™ä½æ¨¡å‹çš„æ•´ä½“æ€§èƒ½</li></ul></li><li><p>Huber loss</p><ul><li>$L = \begin{cases} \frac{1}{2}(y-f(x))^2,\text{   for }|y-f(x)|&lt;\delta,\\ \delta |y-f(x)|-\frac{1}{2}\delta^2,  \text{  otherwise} \end{cases} $</li><li><img src="/2020/09/07/regression-loss/huber.png" width="30%;"></li><li>è¶…å‚å†³å®šäº†å¯¹ä¸å¼‚å¸¸ç‚¹çš„å®šä¹‰ï¼Œåªå¯¹è¾ƒå°çš„å¼‚å¸¸å€¼æ•æ„Ÿ</li></ul></li><li><p>å¯¹æ•°loss</p><script type="math/tex; mode=display"> L(Y, P(Y|X)) = -log(P(Y|X))</script></li><li><p>cross-entropy loss</p><p> äºŒåˆ†ç±»åŒè¾¹è®¡ç®—ï¼š</p><script type="math/tex; mode=display"> L = ylna + (1-y)ln(1-a)</script><p> å¤šåˆ†ç±»å•è¾¹è®¡ç®—ï¼š</p><script type="math/tex; mode=display"> L = ylna</script></li></ol><ol><li><p>æŒ‡æ•°loss</p><script type="math/tex; mode=display"> L(Y, f(x)) = exp[-yf(x)]</script></li><li><p>Hinge loss</p><script type="math/tex; mode=display"> L(Y, f(x)) = max(0, 1-yf(x))</script></li><li><p>perceptron loss</p><script type="math/tex; mode=display"> L(Y, f(x)) = max(0, -yf(x))</script></li><li><p>cross-entropy loss</p><p> äºŒåˆ†ç±»åŒè¾¹è®¡ç®—ï¼š</p><script type="math/tex; mode=display"> L = ylna + (1-y)ln(1-a)</script><p> å¤šåˆ†ç±»å•è¾¹è®¡ç®—ï¼š</p><script type="math/tex; mode=display"> L = ylna</script></li></ol>]]></content>
      
      
        <tags>
            
            <tag> å›å½’ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>pseudo-3d</title>
      <link href="/2020/09/02/pseudo-3d/"/>
      <url>/2020/09/02/pseudo-3d/</url>
      <content type="html"><![CDATA[<p>[3d resnet] Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognitionï¼šçœŸ3dï¼Œfor comparisonï¼Œåˆ†ç±»</p><p>[C3d] Learning Spatiotemporal Features with 3D Convolutional Networksï¼šçœŸ3dï¼Œfor comparisonï¼Œåˆ†ç±»</p><p>[Pseudo-3D resnet] Learning Spatio-Temporal Representation with Pseudo-3D Residual Networksï¼šä¼ª3dï¼Œresblockï¼ŒSå’ŒTèŠ±å¼è¿æ¥ï¼Œåˆ†ç±»</p><p>[2.5d Unet] Automatic Segmentation of Vestibular Schwannoma from T2-Weighted MRI by Deep Spatial Attention with Hardness-Weighted Lossï¼špatchè¾“å…¥ï¼Œå…ˆ2då3dï¼Œé’ˆå¯¹å„å‘å¼‚æ€§ï¼Œåˆ†å‰²</p><p>[two-pathway U-Net] Combining analysis of multi-parametric MR images into a convolutional neural network: Precise target delineation for vestibular schwannoma treatment planningï¼špatchè¾“å…¥ï¼Œ3dç½‘ç»œï¼Œxyå’Œzå¹³é¢åˆ†åˆ«conv &amp; concatï¼Œåˆ†å‰²</p><p>[Projection-Based 2.5D U-net] Projection-Based 2.5D U-net Architecture for Fast Volumetric Segmentationï¼šmipï¼Œ2dç½‘ç»œï¼Œåˆ†å‰²ï¼Œé‡å»º</p><p>[New 2.5D Representation] A New 2.5D Representation for Lymph Node Detection using Random Sets of Deep Convolutional Neural Network Observationsï¼šæ¨ªå† çŸ¢ä¸‰ä¸ªå¹³é¢ä½œä¸ºä¸‰ä¸ªchannelè¾“å…¥ï¼Œ2dç½‘ç»œï¼Œæ£€æµ‹</p><h2 id="Learning-Spatio-Temporal-Representation-with-Pseudo-3D-Residual-Networks"><a href="#Learning-Spatio-Temporal-Representation-with-Pseudo-3D-Residual-Networks" class="headerlink" title="Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks"></a>Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks</h2><ol><li><p>åŠ¨æœº</p><ul><li>spatio-temporal video</li><li>the development of a very deep 3D CNN from scratch results in expensive computational cost and memory demand</li><li>new framework<ul><li>1x3x3 &amp; 3x1x1</li><li>Pseudo-3D Residual Net which exploits all the variants of blocks</li></ul></li><li>outperforms 3D CNN and frame-based 2D CNN</li></ul></li><li><p>è®ºç‚¹</p><ul><li>3d CNNçš„model sizeï¼šmaking it extremely difficult to train a very deep model</li><li>fine-tuning 2d å¥½äº train from scrach 3d</li><li>RNN builds only the temporal connections on the high-level featuresï¼Œleaving the correlations in the low-level forms not fully exploited</li><li>we propose<ul><li>1x3x3 &amp; 3x1x1 in parallel or cascaded </li><li>å…¶ä¸­çš„3x3 convå¯ä»¥ç”¨2d convæ¥åˆå§‹åŒ–</li><li>a family of bottleneck building blocksï¼šenhance the structural diversity  </li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>P3D Blocks  </p><ul><li>directï¼indirect influenceï¼šSå’ŒTä¹‹é—´æ˜¯ä¸²è”è¿˜æ˜¯å¹¶è”</li><li><p>directï¼indirect connected to the final outputï¼šSå’ŒTçš„è¾“å‡ºæ˜¯å¦ç›´æ¥ä¸identity pathç›¸åŠ </p><p><img src="/2020/09/02/pseudo-3d/P3D Blocks.png" width="40%"></p></li><li><p>bottleneckï¼š</p><ul><li>å¤´å°¾å„æ¥ä¸€ä¸ª1x1x1çš„conv</li><li>å¤´ç”¨æ¥narrow channelï¼Œå°¾ç”¨æ¥widen back</li><li><p>å¤´æœ‰reluï¼Œå°¾æ²¡æœ‰relu</p><p><img src="/2020/09/02/pseudo-3d/bottlenecks.png" width="80%"></p></li></ul></li></ul></li><li><p>Pseudo-3D ResNet </p><ul><li>mixing blocksï¼šå¾ªç¯ABC</li><li><p>better performance &amp; small increase in model size</p><p><img src="/2020/09/02/pseudo-3d/cmp.png" width="45%"></p></li><li><p>fine-tuning resnet50ï¼š</p><ul><li>randomly cropped 224x224</li><li>freeze all BN except for the first one</li><li>add an extra dropout layer with 0.9 dropout rate </li></ul></li><li>further fine-tuning P3D resnetï¼š<ul><li>initialize with r50 in last step</li><li>randomly cropped 16x160x160</li><li>horizontally flipped </li><li>mini-batch as 128 frames </li></ul></li></ul></li><li><p>future work</p><ul><li>attention mechanism will be incorporated </li></ul></li></ul></li></ol><h2 id="Projection-Based-2-5D-U-net-Architecture-for-Fast-Volumetric-Segmentation"><a href="#Projection-Based-2-5D-U-net-Architecture-for-Fast-Volumetric-Segmentation" class="headerlink" title="Projection-Based 2.5D U-net Architecture for Fast Volumetric Segmentation"></a>Projection-Based 2.5D U-net Architecture for Fast Volumetric Segmentation</h2><ol><li><p>åŠ¨æœº</p><ul><li><p>MIPï¼š2D images containing information of the full 3D image </p></li><li><p>faster, less memory, accurate</p></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>2d unet</p><ul><li>MIPï¼š$\alpha=36$</li><li>3x3 conv, s2 pooling, transpose conv, concat, BN, relu, </li><li>filtersï¼šbegin with 32, end with 512</li><li><p>dropoutï¼š0.5 in the deepest convolutional block and 0.2 in the second deepest blocks</p><p><img src="/2020/09/02/pseudo-3d/mra.png" width="50%"></p></li></ul></li><li><p>3d unet</p><ul><li>overfitting &amp; memory space</li><li>filtersï¼šbegin with 4, end with 16</li><li>dropoutï¼š0.5 in the deepest convolutional block and 0.4 in the second deepest blocks</li></ul></li><li><p>Projection-Based 2.5D U-net </p><ul><li><p>2d sliceï¼šloss of connection </p></li><li><p>2d mipï¼šdisappointing results </p></li><li><p>2d volumeï¼šlong training time</p></li><li><p>the proposed 2.5D U-netï¼š</p><script type="math/tex; mode=display">  N(x) = T R_p F_p   \left[   \begin{matrix}     U M_{\alpha_1}(x) \\     ... \\     U M_{\alpha_p}(x)    \end{matrix}    \right]</script><ul><li><p>$M_{i}$ï¼šMIPï¼Œp=12</p></li><li><p>$U$ï¼š2d-Unet like above</p></li><li><p>$F_p$ï¼šlearnable filtrationï¼Œ1x3 convï¼Œfor each projectionï¼ŒæŠ‘åˆ¶é‡å»ºä¼ªå½±</p></li><li><p>$R_p$ï¼šreconstruction operator</p><p>  <img src="/2020/09/02/pseudo-3d/recon.png" width="40%"></p></li><li><p>$T$ï¼šfine-tuning operatorï¼Œshift &amp; scale back to 0-1 mask</p></li></ul><p><img src="/2020/09/02/pseudo-3d/cmp2.png" width="60%"></p></li></ul></li></ul></li></ol><h2 id="Learning-Spatio-Temporal-Features-with-3D-Residual-Networks-for-Action-Recognition"><a href="#Learning-Spatio-Temporal-Features-with-3D-Residual-Networks-for-Action-Recognition" class="headerlink" title="Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition"></a>Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition</h2><ol><li><p>åŠ¨æœº</p><ul><li>3D kernels tend to overfit</li><li>3D CNNs is relatively shallow</li><li>propose a 3D CNNs based on ResNets<ul><li>better performance</li><li>not overfit</li><li>deeper than C3D</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>two-stream architectureï¼šconsists of RGB and optical flow streams is often used to represent spatio-temporal information </li><li>3D CNNsï¼štrained on relatively small video datasets performs worse than 2D CNNs pretrained on large datasets </li><li>Very deep 3D CNNsï¼šnot explored yet due to training difficulty  </li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>Network Architecture</p><ul><li>main differenceï¼škernel dimensions</li><li>stemï¼šstride2 for Sï¼Œstride1 for T</li><li>resblockï¼šconv_bn_relu&amp;conv + id</li><li>identity shortcutsï¼šuse zero-padding for increasing dimensionsï¼Œto avoid increasing the number of parameters</li><li>stride2 convï¼šconv3_1ã€ conv4_1ã€ conv5_1</li><li>input clipsï¼š3x16x112x112</li><li><strong>large learning rate</strong> and batch size was important </li></ul><p><img src="/2020/09/02/pseudo-3d/3d res.png" width="50%"></p></li></ul></li><li><p>å®éªŒ</p><ul><li>åœ¨å°æ•°æ®é›†ä¸Š3d-r18ä¸å¦‚C3Dï¼Œoverfitäº†ï¼šshallow architecture of the C3D and pretraining on the Sports-1M dataset prevent the C3D from overfitting </li><li>åœ¨å¤§æ•°æ®é›†ä¸Š3d-r34å¥½äºC3Dï¼ŒåŒæ—¶C3Dçš„val accæ˜æ˜¾é«˜äºtrain accâ€”â€”å¤ªshallowæ¬ æ‹Ÿåˆäº†ï¼Œr34åˆ™è¡¨ç°æ›´å¥½ï¼Œè€Œä¸”ä¸éœ€è¦é¢„è®­ç»ƒ</li><li>RGB-I3D achieved the best performance <ul><li>3d-r34æ˜¯æ›´deeperçš„</li><li>RGB-I3Dç”¨äº†æ›´å¤§çš„batch sizeï¼šLarge batch size is important to train good models with batch normalization </li><li>High resolutionsï¼š3x64x224x224</li></ul></li></ul></li></ol><h2 id="Learning-Spatiotemporal-Features-with-3D-Convolutional-Networks"><a href="#Learning-Spatiotemporal-Features-with-3D-Convolutional-Networks" class="headerlink" title="Learning Spatiotemporal Features with 3D Convolutional Networks"></a>Learning Spatiotemporal Features with 3D Convolutional Networks</h2><ol><li><p>åŠ¨æœº</p><ul><li>generic </li><li>efficient </li><li>simple </li><li>3d ConvNet with 3x3x3 conv &amp; a simple linear classifier</li></ul></li><li><p>è®ºç‚¹</p><ul><li>3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets</li><li>2D ConvNets lose temporal information of the input signal right after every convolution operation </li><li><p>2d convåœ¨channelç»´åº¦ä¸Šæƒé‡éƒ½æ˜¯ä¸€æ ·çš„ï¼Œç›¸å½“äºtemporal dimsä¸Šæ²¡æœ‰é‡è¦æ€§ç‰¹å¾æå–</p><p><img src="/2020/09/02/pseudo-3d/2d3d.png" width="80%"></p></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>basic network settings</p><ul><li>5 conv layers + 5 pooling layers + 2 fc layers + softmax</li><li>filtersï¼š[64ï¼Œ128ï¼Œ256ï¼Œ256ï¼Œ256]</li><li>fc dimsï¼š[2048ï¼Œ2048]</li><li>conv kernelï¼šdx3x3</li><li>pooling kernelï¼š2x2x2ï¼Œs2 except for the first layer <ul><li>with the intention of not to merge the temporal signal too early </li><li>also to satisfy the clip length of 16 frames </li></ul></li></ul></li><li><p>varing settings</p><ul><li>temporal kernel depth<ul><li>homogeneousï¼šdepth-1/3/5/7 throughout</li><li>varyingï¼šincreasing-3-3-5-5-7 &amp; decreasing-7- 5-5-3-3  </li></ul></li><li><p>depth-3 throughout performs the best</p><p><img src="/2020/09/02/pseudo-3d/depth.png" width="60%"></p></li><li><p>depth-1 is significantly worse  </p></li><li>We also verify that 3D ConvNet consistently performs better than 2D ConvNet <strong>on a large-scale internal dataset</strong></li></ul></li><li><p>C3D</p><ul><li>8 conv layers + 5 pooling layers + 2 fc layers + softmax</li><li>homogeneousï¼š3x3x3 s1 conv thtoughout</li><li>pool1ï¼š1x2x2 kernel size &amp; strideï¼Œrest 2x2x2</li><li><p>fc dimsï¼š4096</p><p><img src="/2020/09/02/pseudo-3d/c3d.png" width="80%"></p></li></ul></li><li><p>C3D video descriptorï¼šfc6 activations + L2-norm</p></li><li><p>deconvolution visualizingï¼š</p><ul><li>conv5b feature maps</li><li>starts by focusing on appearance in the first few frames </li><li>tracks the salient motion in the subsequent frames </li></ul></li><li><p>compactness </p><ul><li>PCA</li><li>å‹ç¼©åˆ°50-100dimä¸å¤ªæŸå¤±acc</li><li><p>å‹ç¼©åˆ°10dimä»æ—§æ˜¯æœ€é«˜acc</p><p><img src="/2020/09/02/pseudo-3d/PCA.png" width="60%"></p></li><li><p>projected to 2-dimensional space using t-SNE </p><ul><li>C3D features are semantically separable compared to Imagenet  </li><li><p>quantitatively observe that C3D is better than Imagenet</p><p><img src="/2020/09/02/pseudo-3d/2dvec.png" width="60%"></p></li></ul></li></ul></li></ul></li><li><p>Action Similarity Labeling </p><ul><li>predicting action similarity  </li><li>extract C3D features: prob, fc7, fc6, pool5 for each clip </li><li>L2 normalization</li><li>compute the 12 different distances for each featureï¼š48 in total</li><li>linear SVM is trained on these 48-dim feature vectors </li><li><p>C3D significantly outperforms the others</p><p><img src="/2020/09/02/pseudo-3d/similarity.png" width="50%"></p></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 3d CNN, 2.5d CNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SSD</title>
      <link href="/2020/08/13/SSD/"/>
      <url>/2020/08/13/SSD/</url>
      <content type="html"><![CDATA[<h2 id="SSD-Single-Shot-MultiBox-Detector"><a href="#SSD-Single-Shot-MultiBox-Detector" class="headerlink" title="SSD: Single Shot MultiBox Detector"></a>SSD: Single Shot MultiBox Detector</h2><ol><li><p>åŠ¨æœº</p><ul><li>single network</li><li>speed &amp; accuracy </li><li>59 FPS / 74.3% mAP</li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>prev methods</p><ul><li>two-stageï¼šç”Ÿæˆç¨€ç–çš„å€™é€‰æ¡†ï¼Œç„¶åå¯¹å€™é€‰æ¡†è¿›è¡Œåˆ†ç±»ä¸å›å½’</li><li>one-stageï¼šå‡åŒ€åœ°åœ¨å›¾ç‰‡çš„ä¸åŒä½ç½®ï¼Œé‡‡ç”¨ä¸åŒå°ºåº¦å’Œé•¿å®½æ¯”ï¼Œè¿›è¡Œå¯†é›†æŠ½æ ·ï¼Œç„¶ååˆ©ç”¨CNNæå–ç‰¹å¾åç›´æ¥è¿›è¡Œåˆ†ç±»ä¸å›å½’</li></ul></li><li><p>fundamental speed improvement </p><ul><li>eliminating bounding box proposals </li><li>eliminating feature resampling  </li></ul></li><li>other improvements <ul><li>small convolutional filter for bbox categories and offsetsï¼ˆé’ˆå¯¹yolov1çš„å…¨è¿æ¥å±‚è¯´ï¼‰</li><li>separate predictors by aspect ratio</li><li>multiple scales</li><li>è¿™äº›æ“ä½œéƒ½ä¸æ˜¯åŸåˆ›</li></ul></li><li>The core of SSD is predicting category scores and box offsets for a fixed set of default bounding boxes using small convolutional filters applied to feature maps. </li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>Model</p><ul><li><p>Multi-scale feature maps for detectionï¼šé‡‡ç”¨äº†å¤šå°ºåº¦çš„ç‰¹å¾å›¾ï¼Œé€æ¸ç”¨s2é™ç»´ï¼Œå¤§å°ºåº¦ç‰¹å¾å›¾ä¸Šæœ‰æ›´å¤šçš„å•å…ƒï¼Œç”¨æ¥å›å½’å°ç‰©ä½“</p></li><li><p>Convolutional predictors for detectionï¼šé’ˆå¯¹yolov1é‡Œé¢çš„fcå±‚</p></li><li><p>Default boxes and aspect ratiosï¼šä¸€ä¸ªå•å…ƒ4ç§sizeçš„å…ˆéªŒæ¡†ï¼Œå¯¹æ¯ä¸ªå…ˆéªŒæ¡†éƒ½é¢„æµ‹ä¸€ç»„4+(c+1)ï¼Œå…¶ä¸­çš„1å¯ä»¥çœ‹ä½œèƒŒæ™¯ç±»ï¼Œä¹Ÿå¯ä»¥çœ‹åšæ˜¯æœ‰æ— ç›®æ ‡çš„ç½®ä¿¡åº¦ï¼Œå„ç”¨ä¸€ä¸ªconv3x3çš„head</p></li><li><p>backbone</p><ul><li>å‚è€ƒï¼š<a href="https://www.cnblogs.com/sddai/p/10206929.html" target="_blank" rel="noopener">https://www.cnblogs.com/sddai/p/10206929.html</a></li></ul></li><li>VGG16å‰å››ä¸ªconv blockä¿ç•™<ul><li>æ— dropoutå’Œfc</li><li>conv5çš„æ± åŒ–ç”±2x2-s2å˜æˆ3x3-s1</li><li>conv6å’Œconv7æ˜¯3x3x1024å’Œ1x1x1024çš„ç©ºæ´å·ç§¯ï¼Œè¾“å‡º19x19x1024</li><li>conv8æ˜¯1x1x256å’Œ3x3x512 s2çš„convï¼Œè¾“å‡º10x10x512</li><li>conv9éƒ½æ˜¯1x1x128å’Œ3x3x256 s2çš„convï¼Œè¾“å‡º5x5x256</li><li>conv10ã€conv11éƒ½æ˜¯1x1x128å’Œ3x3x256 s1 p0çš„convï¼Œè¾“å‡º3x3x256ã€1x1x256</li></ul></li></ul></li><li>Training <ul><li>Matching strategyï¼šmatch default boxå’Œgt box<ul><li>é¦–å…ˆä¸ºæ¯ä¸€ä¸ªgt boxæ‰¾åˆ°ä¸€ä¸ªoverlapæœ€å¤§çš„default box</li><li>ç„¶åæ‰¾åˆ°æ‰€æœ‰ä¸gt boxçš„overlapå¤§äº0.5çš„default box</li><li>ä¸€ä¸ªgt boxå¯èƒ½å¯¹åº”å¤šä¸ªdefault box</li><li>ä¸€ä¸ªdefault boxåªèƒ½å¯¹åº”ä¸€ä¸ªgt boxï¼ˆoverlapæœ€å¤§çš„ï¼‰</li></ul></li><li>Objective loss <ul><li>loc lossï¼šsmooth L1ï¼Œoffsets like Faster R-CNN</li><li>cls lossï¼šsoftmax loss  </li><li>weighted sumï¼š$L = \frac{1}{N} (L_{cls} + \alpha L_{loc})$ï¼Œ<ul><li>N is the number of matched default boxes</li><li>loss=0 when N=0</li></ul></li></ul></li><li>Choosing scales and aspect ratios for default boxes <ul><li>æ¯ä¸ªlevelçš„feature mapæ„Ÿå—é‡ä¸åŒï¼Œdefault boxçš„å°ºå¯¸ä¹Ÿä¸åŒ</li><li>æ•°é‡ä¹Ÿä¸åŒï¼Œconv4ã€conv10å’Œconv11æ˜¯4ä¸ªï¼Œconv7ã€conv8ã€conv9æ˜¯6ä¸ª</li><li>ratioï¼š{1,2,3,1/2,1/3}ï¼Œ4ä¸ªçš„æ²¡æœ‰3å’Œ1/3</li><li>L2 normalization for conv4ï¼š<ul><li>$y_i = \frac{x_i}{\sqrt{\sum_{k=1}^n x_k^2}}$</li><li>ä½œç”¨æ˜¯å°†ä¸åŒå°ºåº¦çš„ç‰¹å¾éƒ½å½’ä¸€åŒ–æˆæ¨¡ä¸º1çš„å‘é‡</li><li>scaleï¼šå¯ä»¥æ˜¯å›ºå®šå€¼ï¼Œä¹Ÿå¯ä»¥æ˜¯å¯å­¦ä¹ å‚æ•°</li><li>ä¸ºå•¥åªé’ˆå¯¹conv4ï¼Ÿ<a href="https://zhuanlan.zhihu.com/p/39399799" target="_blank" rel="noopener">ä½œè€…çš„å¦ä¸€ç¯‡paper(ParseNet)ä¸­å‘ç°conv4å’Œå…¶ä»–å±‚ç‰¹å¾çš„scaleæ˜¯ä¸ä¸€æ ·çš„</a></li></ul></li></ul></li><li>predictions<ul><li>all default boxes with different scales and aspect ratio from all locations of many feature maps</li><li>significant imbalance for positive/negative</li><li>Hard negative mining<ul><li>sort using the highest confidence loss</li><li>pick the top ones with n/p at most 3:1</li><li>faster optimization and a more stable training</li></ul></li></ul></li><li>Data augmentation <ul><li>sample a patch with specific IoU</li><li>resize</li></ul></li></ul></li></ul></li><li>æ€§è´¨<ul><li>much worse performance on smaller objects, increasing the input size can help improve</li><li>Data augmentation is crucial, resulting in a 8.8% mAP improvement</li><li>Atrous is faster, ä¿ç•™pool5ä¸å˜çš„è¯ï¼Œthe result is about the same while the speed is about 20% slower</li><li></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ç›®æ ‡æ£€æµ‹ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>pythonå¤šçº¿ç¨‹&amp;å¤šè¿›ç¨‹</title>
      <link href="/2020/08/04/python%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
      <url>/2020/08/04/python%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E5%A4%9A%E8%BF%9B%E7%A8%8B/</url>
      <content type="html"><![CDATA[<p>Referenceï¼š</p><p><a href="https://www.cnblogs.com/kaituorensheng/p/4465768.html" target="_blank" rel="noopener">https://www.cnblogs.com/kaituorensheng/p/4465768.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/46368084" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46368084</a></p><p><a href="https://www.runoob.com/python3/python3-multithreading.html" target="_blank" rel="noopener">https://www.runoob.com/python3/python3-multithreading.html</a></p><ol><li><p>åè¯</p><ul><li><p>è¿›ç¨‹(process)å’Œçº¿ç¨‹(thread)</p><ul><li>cpuåœ¨å¤„ç†ä»»åŠ¡æ—¶ï¼ŒæŠŠæ—¶é—´åˆ†æˆè‹¥å¹²ä¸ªå°æ—¶é—´æ®µï¼Œè¿™äº›æ—¶é—´æ®µå¾ˆå°çš„ï¼Œç³»ç»Ÿä¸­æœ‰å¾ˆå¤šè¿›ç¨‹ï¼Œæ¯ä¸ªè¿›ç¨‹ä¸­åˆåŒ…å«å¾ˆå¤šçº¿ç¨‹ï¼Œåœ¨åŒä¸€æ—¶é—´æ®µ å†…ï¼Œç”µè„‘CPUåªèƒ½å¤„ç†ä¸€ä¸ªçº¿ç¨‹ï¼Œä¸‹ä¸€ä¸ªæ—¶é—´æ®µï¼Œå¯èƒ½åˆå»æ‰§è¡Œåˆ«çš„çº¿ç¨‹äº†ï¼ˆæ—¶é—´ç‰‡è½®è½¬ï¼Œä»è€Œå®ç°ä¼ªå¤šä»»åŠ¡ï¼‰ï¼Œå…·ä½“é¡ºåºå–å†³äºå…¶è°ƒåº¦é€»è¾‘</li><li>å¤šæ ¸cpuå¯ä»¥å®ç°çœŸæ­£çš„å¹¶è¡Œï¼ŒåŒä¸€ä¸ªæ—¶åˆ»æ¯ä¸ªcpuä¸Šéƒ½å¯ä»¥è·‘ä¸€ä¸ªä»»åŠ¡</li><li><p>å¤šè¿›ç¨‹ï¼šæ¯ä¸ªè¿›ç¨‹åˆ†åˆ«æ‰§è¡ŒæŒ‡å®šä»»åŠ¡ï¼Œè¿›ç¨‹é—´äº’ç›¸ç‹¬ç«‹ï¼Œæ¯ä¸ªæ—¶åˆ»å¹¶è¡Œçš„å®é™…è¿›ç¨‹æ•°å–å†³äºcpuæ•°é‡</p></li><li><p>å¤šçº¿ç¨‹ï¼šå•ä¸ªcpuåŒä¸€æ—¶åˆ»åªèƒ½å¤„ç†ä¸€ä¸ªçº¿ç¨‹ï¼Œä¸€ä¸ªä»»åŠ¡å¯èƒ½ç”±å¤šä¸ªå·¥äººæ¥å®Œæˆï¼Œå·¥äººä»¬ç›¸äº’ååŒï¼Œè¿™åˆ™æ˜¯å¤šçº¿ç¨‹</p></li></ul></li><li><p>pythonçš„å¤šè¿›ç¨‹ï¼šmultiprocessæ¨¡å—</p></li><li><p>pythonçš„å¤šçº¿ç¨‹ï¼šthreadingæ¨¡å—</p></li><li><p>æ¯ä¸ªè¿›ç¨‹åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­æ‹¥æœ‰ç‹¬ç«‹çš„å†…å­˜å•å…ƒï¼Œè€Œä¸€ä¸ªè¿›ç¨‹çš„å¤šä¸ªçº¿ç¨‹åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­å…±äº«å†…å­˜ã€‚</p></li></ul></li><li><p>å¤šè¿›ç¨‹multiprocess</p><ul><li>æ¯è¿›ç¨‹ï¼šå½“æˆ‘ä»¬æ‰§è¡Œä¸€ä¸ªpythonè„šæœ¬ï¼Œif mainä¸‹é¢å®é™…è¿è¡Œçš„ä¸»ä½“å°±æ˜¯æ¯è¿›ç¨‹</li><li>å­è¿›ç¨‹ï¼šæˆ‘ä»¬ä½¿ç”¨multiprocessæ˜¾å¼åˆ›å»ºçš„è¿›ç¨‹ï¼Œéƒ½æ˜¯å­è¿›ç¨‹</li><li>join()æ–¹æ³•ï¼šç”¨æ¥è®©æ¯è¿›ç¨‹é˜»å¡ï¼Œç­‰å¾…æ‰€æœ‰å­è¿›ç¨‹æ‰§è¡Œå®Œæˆå†ç»“æŸ</li></ul></li></ol><ul><li>ä½¿ç”¨multiprocessçš„å¤šè¿›ç¨‹ï¼Œå¯ä»¥é€šè¿‡processæ–¹æ³•å’Œpoolæ–¹æ³•<ul><li>processæ–¹æ³•ï¼šé€‚ç”¨è¿›ç¨‹è¾ƒå°‘æ—¶å€™ï¼Œæ— æ³•æ‰¹é‡å¼€å¯/å…³é—­</li><li>poolæ–¹æ³•ï¼šæ‰¹é‡ç®¡ç†</li><li>å‚æ•°ï¼šè¾“å…¥å‚æ•°éƒ½å·®ä¸å¤šï¼Œç¬¬ä¸€ä¸ªæ˜¯è¦æ‰§è¡Œçš„å‡½æ•°æ–¹æ³•target/funcï¼Œç¬¬äºŒä¸ªæ˜¯è¾“å…¥å‚æ•°args</li></ul></li></ul><p>  ğŸŒ°Processæ–¹æ³•ï¼š</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">long_time_task</span><span class="params">(i)</span>:</span></span><br><span class="line">    print(<span class="string">'å­è¿›ç¨‹: &#123;&#125; - ä»»åŠ¡&#123;&#125;'</span>.format(os.getpid(), i))</span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line">    print(<span class="string">"ç»“æœ: &#123;&#125;"</span>.format(<span class="number">8</span> ** <span class="number">20</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    print(<span class="string">'å½“å‰æ¯è¿›ç¨‹: &#123;&#125;'</span>.format(os.getpid()))</span><br><span class="line">    start = time.time()</span><br><span class="line">    p1 = Process(target=long_time_task, args=(<span class="number">1</span>,))</span><br><span class="line">    p2 = Process(target=long_time_task, args=(<span class="number">2</span>,))</span><br><span class="line">    print(<span class="string">'ç­‰å¾…æ‰€æœ‰å­è¿›ç¨‹å®Œæˆã€‚'</span>)</span><br><span class="line">    p1.start()</span><br><span class="line">    p2.start()</span><br><span class="line">    p1.join()</span><br><span class="line">    p2.join()</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">"æ€»å…±ç”¨æ—¶&#123;&#125;ç§’"</span>.format((end - start)))</span><br></pre></td></tr></table></figure><ul><li><p>processæ–¹æ³•ä½¿ç”¨Processå®ä¾‹åŒ–ä¸€ä¸ªè¿›ç¨‹å¯¹è±¡ï¼Œç„¶åè°ƒç”¨å®ƒçš„startæ–¹æ³•å¼€å¯è¿›ç¨‹</p><p>ğŸŒ°Poolæ–¹æ³•ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool, cpu_count</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">long_time_task</span><span class="params">(i)</span>:</span></span><br><span class="line">    print(<span class="string">'å­è¿›ç¨‹: &#123;&#125; - ä»»åŠ¡&#123;&#125;'</span>.format(os.getpid(), i))</span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line">    print(<span class="string">"ç»“æœ: &#123;&#125;"</span>.format(<span class="number">8</span> ** <span class="number">20</span>))</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">True</span>     <span class="comment"># ç”¨äºæ¼”ç¤ºpoolé€‚ç”¨äºæœ‰è¿”å›å€¼</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    print(<span class="string">"CPUå†…æ ¸æ•°:&#123;&#125;"</span>.format(cpu_count()))        <span class="comment"># 4</span></span><br><span class="line">    print(<span class="string">'å½“å‰æ¯è¿›ç¨‹: &#123;&#125;'</span>.format(os.getpid()))</span><br><span class="line">    start = time.time()</span><br><span class="line">    p = Pool(<span class="number">4</span>)</span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="comment"># p.apply_async(long_time_task, args=(i,))</span></span><br><span class="line">        results.append(p.apply_async(long_time_task, args=(i,)))</span><br><span class="line">    print(<span class="string">'ç­‰å¾…æ‰€æœ‰å­è¿›ç¨‹å®Œæˆã€‚'</span>)</span><br><span class="line">    p.close()</span><br><span class="line">    p.join()</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">"æ€»å…±ç”¨æ—¶&#123;&#125;ç§’"</span>.format((send - start)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># æŸ¥çœ‹è¿”å›å€¼</span></span><br><span class="line">    <span class="keyword">for</span> res <span class="keyword">in</span> results:</span><br><span class="line">      print(res.get())</span><br></pre></td></tr></table></figure></li><li><p>apply_async(func, args=(), kwds={}, callback=None)ï¼šå‘è¿›ç¨‹æ± æäº¤éœ€è¦æ‰§è¡Œçš„å‡½æ•°åŠå‚æ•°ï¼Œå„ä¸ªè¿›ç¨‹é‡‡ç”¨éé˜»å¡ï¼ˆå¼‚æ­¥ï¼‰çš„è°ƒç”¨æ–¹å¼ï¼Œå³æ¯ä¸ªå­è¿›ç¨‹åªç®¡è¿è¡Œè‡ªå·±çš„ï¼Œä¸ç®¡å…¶å®ƒè¿›ç¨‹æ˜¯å¦å·²ç»å®Œæˆã€‚</p></li><li>close()ï¼šå…³é—­è¿›ç¨‹æ± ï¼ˆpoolï¼‰ï¼Œä¸å†æ¥å—æ–°çš„ä»»åŠ¡ã€‚</li><li>join()ï¼šä¸»è¿›ç¨‹é˜»å¡ç­‰å¾…å­è¿›ç¨‹çš„é€€å‡ºï¼Œ è°ƒç”¨join()ä¹‹å‰å¿…é¡»å…ˆè°ƒç”¨close()æˆ–terminate()æ–¹æ³•ï¼Œä½¿å…¶ä¸å†æ¥å—æ–°çš„Processã€‚</li></ul><ol><li><p>å¤šçº¿ç¨‹threading</p><ul><li>pythonçš„å¤šçº¿ç¨‹æ˜¯ä¼ªå¤šçº¿ç¨‹ï¼Œå› ä¸ºä¸»è¿›ç¨‹åªæœ‰ä¸€ä¸ªï¼Œæ‰€ä»¥åªç”¨äº†å•æ ¸ï¼Œåªæ˜¯é€šè¿‡ç¢ç‰‡åŒ–è¿›ç¨‹ã€è°ƒåº¦ã€å…¨å±€é”ç­‰æ“ä½œï¼Œcpuåˆ©ç”¨ç‡æå‡äº†</li><li>æ‰€ä»¥æˆ‘æƒ³å¹¶è¡Œå¤„ç†ç™¾ä¸‡é‡çº§çš„æ•°æ®å…¥åº“æ“ä½œæ—¶ï¼Œå¤šè¿›ç¨‹çš„æ•ˆç‡æ˜æ˜¾é«˜äºå¤šçº¿ç¨‹</li><li><p>ã€é—®é¢˜ã€‘ä»æˆ‘è§‚å¯Ÿä¸Šçœ‹å¤šçº¿ç¨‹åŸºæœ¬å°±æ˜¯ä¸²è¡Œï¼Ÿï¼Ÿ</p><p>ğŸŒ°threading</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">long_time_task</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'å½“å­çº¿ç¨‹: &#123;&#125;'</span>.format(threading.current_thread().name))</span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line">    print(<span class="string">"ç»“æœ: &#123;&#125;"</span>.format(<span class="number">8</span> ** <span class="number">20</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line">    print(<span class="string">'è¿™æ˜¯ä¸»çº¿ç¨‹ï¼š&#123;&#125;'</span>.format(threading.current_thread().name))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        t = threading.Thread(target=long_time_task, args=())</span><br><span class="line">        t.setDaemon(<span class="keyword">True</span>)</span><br><span class="line">        t.start()</span><br><span class="line">        t.join()</span><br><span class="line"></span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">"æ€»å…±ç”¨æ—¶&#123;&#125;ç§’"</span>.format((end - start)))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment"># ç»§æ‰¿&amp;æœ‰è¿”å›å€¼çš„å†™æ³•</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">long_time_task</span><span class="params">(i)</span>:</span></span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">8</span>**<span class="number">20</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyThread</span><span class="params">(threading.Thread)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, func, args , name=<span class="string">''</span>, )</span>:</span></span><br><span class="line">        threading.Thread.__init__(self)</span><br><span class="line">        self.func = func</span><br><span class="line">        self.args = args</span><br><span class="line">        self.name = name</span><br><span class="line">        self.result = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'å¼€å§‹å­è¿›ç¨‹&#123;&#125;'</span>.format(self.name))</span><br><span class="line">        self.result = self.func(self.args[<span class="number">0</span>],)</span><br><span class="line">        print(<span class="string">"ç»“æœ: &#123;&#125;"</span>.format(self.result))</span><br><span class="line">        print(<span class="string">'ç»“æŸå­è¿›ç¨‹&#123;&#125;'</span>.format(self.name))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_result</span><span class="params">(self)</span>:</span></span><br><span class="line">        threading.Thread.join(self)  <span class="comment"># ç­‰å¾…çº¿ç¨‹æ‰§è¡Œå®Œæ¯•</span></span><br><span class="line">        <span class="keyword">return</span> self.result</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line">    threads = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">3</span>):</span><br><span class="line">        t = MyThread(long_time_task, (i,), str(i))</span><br><span class="line">        threads.append(t)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> threads:</span><br><span class="line">        t.start()</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> threads:</span><br><span class="line">        t.join()</span><br><span class="line"></span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">"æ€»å…±ç”¨æ—¶&#123;&#125;ç§’"</span>.format((end - start)))</span><br></pre></td></tr></table></figure></li><li><p>joinæ–¹æ³•ï¼šç­‰å¾…æ‰€æœ‰è¿›ç¨‹æ‰§è¡Œå®Œï¼Œä¸»è¿›ç¨‹å†æ‰§è¡Œå®Œ</p></li><li>setDaemon(True)ï¼šä¸»çº¿ç¨‹æ‰§è¡Œå®Œå°±é€€å‡º</li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>IoU</title>
      <link href="/2020/08/03/IoU/"/>
      <url>/2020/08/03/IoU/</url>
      <content type="html"><![CDATA[<p>reference: <a href="https://bbs.cvmart.net/articles/1396" target="_blank" rel="noopener">https://bbs.cvmart.net/articles/1396</a></p><ol><li><p>IoU</p><p> IoU = Intersection / Union</p><p> $Loss_{IoU} = 1 - IoU$</p><ul><li>[0,1]</li><li>æ— æ³•ç›´æ¥ä¼˜åŒ–æ²¡æœ‰é‡å çš„éƒ¨åˆ†ï¼šå¦‚æœä¸¤ä¸ªæ¡†æ²¡æœ‰äº¤é›†ï¼ŒIoU=0ï¼Œæ²¡æœ‰æ¢¯åº¦å›ä¼ ï¼Œæ— æ³•è¿›è¡Œå­¦ä¹ è®­ç»ƒ</li><li>å°ºåº¦ä¸æ•æ„Ÿ</li><li><p>æ— æ³•ç²¾ç¡®çš„åæ˜ ä¸¤è€…çš„é‡åˆè´¨é‡</p><p><img src="/2020/08/03/IoU/IoU.png" width="40%;"></p></li></ul></li><li><p>GIoU(Generalized Intersection over Union)</p><p> $GIoU = IoU - \frac{|A_c - U|}{|A_c|}$ï¼Œ$A_c$æ˜¯åŒ…å«ä¸¤ä¸ªæ¡†çš„æœ€å°å¤–æ¥æ¡†</p><p> $Loss_{GIoU} = 1 - GIoU$</p><p> <img src="/2020/08/03/IoU/GIoU.png" width="40%;"></p><ul><li>GIoUå€¾å‘äºå…ˆå¢å¤§bboxçš„å¤§å°æ¥å¢å¤§ä¸GTçš„äº¤é›†ï¼Œç„¶åé€šè¿‡IoUé¡¹å¼•å¯¼æœ€å¤§åŒ–bboxçš„é‡å åŒºåŸŸ</li><li>[-1,1]</li><li>èƒ½å¤Ÿå…³æ³¨åˆ°éé‡åˆåŒºåŸŸ</li><li>å°ºåº¦ä¸æ•æ„Ÿ</li><li>ä¸¤ä¸ªæ¡†ä¸ºåŒ…å«å…³ç³»æ—¶ï¼Œé€€åŒ–ä¸ºIoU</li><li>å¦‚æœä¹‹é—´ç”¨æ¥æ›¿æ¢mseï¼Œå‰æœŸæ”¶æ•›ä¼šæ¯”è¾ƒæ…¢</li><li><strong>ä¸€èˆ¬åœ°ï¼ŒGIoU lossä¸èƒ½å¾ˆå¥½åœ°æ”¶æ•›SOTAç®—æ³•ï¼Œåè€Œé€ æˆä¸å¥½çš„ç»“æœ</strong></li></ul></li><li><p>DIoU (Distance-IoU)</p><p> $DIoU = IoU - \frac{d^2}{c^2}$ï¼Œdæ˜¯ä¸¤ä¸ªä¸­å¿ƒç‚¹é—´çš„æ¬§å¼è·ç¦»ï¼Œcæ˜¯ä¸¤ä¸ªæ¡†çš„æœ€å°å¤–æ¥æ¡†çš„å¯¹è§’çº¿è·ç¦»</p><p> $Loss_{DIoU} = 1 - DIoU$</p><p> <img src="/2020/08/03/IoU/DIoU.png" width="30%;"></p></li></ol><pre><code>* ç›´æ¥æœ€å°åŒ–ä¸¤ä¸ªç›®æ ‡æ¡†çš„è·ç¦»ï¼Œæ”¶æ•›å¿«å¾—å¤š* èƒ½å¤Ÿå…³æ³¨åˆ°éé‡åˆåŒºåŸŸ* å¯¹äºåŒ…å«å…³ç³»çš„ä¸¤ä¸ªæ¡†ï¼Œä»æ—§æœ‰è·ç¦»æŸå¤±ï¼Œä¸ä¼šé€€åŒ–ä¸ºIoU* å¯ä»¥æ›¿æ¢NMSä¸­çš„IoUï¼šåŸå§‹çš„IoUä»…è€ƒè™‘äº†é‡å åŒºåŸŸï¼Œå¯¹åŒ…å«çš„æƒ…å†µæ²¡æœ‰å¾ˆå¥½çš„å¤„ç†    $$    score = score\text{ if }IoU - dis(box_{max}, box)&gt;\epsilon \text{, else } 0    $$* æ²¡æœ‰è€ƒè™‘å½¢çŠ¶ï¼ˆé•¿å®½æ¯”ï¼‰</code></pre><ol><li><p>CIoU (Complete-IoU)</p><p> $CIoU = IoU - \frac{d^2}{c^2}-av$ï¼Œåœ¨DIoUçš„åŸºç¡€ä¸Šæ–°å¢äº†æƒ©ç½šé¡¹avï¼Œaæ˜¯æƒé‡ç³»æ•°ï¼Œvç”¨æ¥è¯„ä»·é•¿å®½æ¯”ï¼š</p><script type="math/tex; mode=display"> v = \frac{4}{\pi ^ 2}(arctan\frac{w^{gt}}{h^{gt}}-arctan\frac{w}{h})^2\\ a = \frac{v}{1-IoU+v}</script><p> $Loss_{CIoU} = 1 - CIoU$</p><ul><li><p>vçš„æ¢¯åº¦ä¸­æœ‰$\frac{1}{w^2+h^2}$ï¼Œé•¿å®½åœ¨[0,1]ä¹‹é—´ï¼Œå¯èƒ½å¾ˆå°ï¼Œä¼šå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸ï¼Œç”¨çš„æ—¶å€™</p><ul><li>clampä¸€ä¸‹ä¸Šä¸‹é™</li><li><p>åˆ†æ¯ä¸­çš„$w^2+h^2$æ›¿æ¢æˆ1</p><script type="math/tex; mode=display">\frac{\partial v }{\partial w} = \frac{8}{\pi ^ 2}(arctan\frac{w^{gt}}{h^{gt}}-arctan\frac{w}{h})\frac{h}{w^2+h^2}\\\frac{\partial v }{\partial w} = \frac{8}{\pi ^ 2}(arctan\frac{w^{gt}}{h^{gt}}-arctan\frac{w}{h})\frac{w}{w^2+h^2}</script></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ç›®æ ‡æ£€æµ‹ï¼Œloss </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>YOLOACT</title>
      <link href="/2020/07/17/YOLOACT/"/>
      <url>/2020/07/17/YOLOACT/</url>
      <content type="html"><![CDATA[<ul><li>[YOLACT] Real-time Instance Segmentationï¼š33 FPS/30 mAP</li><li>[YOLACT++] Better Real-time Instance Segmentationï¼š33.5 FPS/34.1 mAP</li></ul><h2 id="YOLACT-Real-time-Instance-Segmentation"><a href="#YOLACT-Real-time-Instance-Segmentation" class="headerlink" title="YOLACT: Real-time Instance Segmentation"></a>YOLACT: Real-time Instance Segmentation</h2><ol><li><p>åŠ¨æœº</p><ul><li><p>create a real-time instance segmentation base on fast, one-stage detection model</p></li><li><p>forgoes an explicit localization step (e.g., feature repooling) </p><ul><li>doesnâ€™t depend on repooling (RoI Pooling)</li><li>produces very high-quality masks</li></ul></li><li><p>set two parallel subtasks</p><ul><li>prototypesâ€”â€”conv</li><li>mask coefficientsâ€”â€”fc</li><li>ä¹‹åå°†æ¨¡æ¿maskå’Œå®ä¾‹maskç³»æ•°è¿›è¡Œçº¿æ€§ç»„åˆæ¥è·å¾—å®ä¾‹çš„mask</li></ul></li></ul></li></ol><ul><li><p>â€˜prototypesâ€™: vocabulary </p></li><li><p>fully-convolutional</p><ul><li>localization is still translation variant</li></ul></li><li><p>Fast NMS</p></li></ul><ol><li><p>è®ºç‚¹</p><ul><li><p>State-of-the-art approaches to instance segmentation like Mask R- CNN and FCIS directly build off of advances in object detection like Faster R-CNNand R-FCN</p><ul><li>focus primarily on performance over speed </li><li>these methods â€œre-poolâ€ features in some bounding box region  </li><li>inherently sequential therefore difficult to accelerate </li></ul></li><li><p>One-stage instance segmentation methods generate position sensitive maps </p><ul><li>still require repooling or other non-trivial computations </li></ul></li><li><p>prototypes </p><ul><li>related works use prototypes to represent features (Bag of Feature)</li><li>we use them to assemble masks for instance segmentation </li><li>we learn prototypes that are specific to each image, rather than global prototypes shared across the entire dataset</li></ul></li><li><p>Bag of Feature</p><ul><li><p>BOFå‡è®¾å›¾åƒç›¸å½“äºä¸€ä¸ªæ–‡æœ¬ï¼Œå›¾åƒä¸­çš„ä¸åŒå±€éƒ¨åŒºåŸŸæˆ–ç‰¹å¾å¯ä»¥çœ‹ä½œæ˜¯æ„æˆå›¾åƒçš„è¯æ±‡(codebook)</p><p>  <img src="/2020/07/17/YOLOACT/BOF.png" width="60%"></p></li><li><p>æ‰€æœ‰çš„æ ·æœ¬å…±äº«ä¸€ä»½è¯æ±‡æœ¬ï¼Œé’ˆå¯¹æ¯ä¸ªå›¾åƒï¼Œç»Ÿè®¡æ¯ä¸ªå•è¯çš„é¢‘æ¬¡ï¼Œå³å¯å¾—åˆ°å›¾ç‰‡çš„ç‰¹å¾å‘é‡</p><p><img src="/2020/07/17/YOLOACT/BOF1.png" width="50%"></p></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>parallel tasks</p><ul><li>The first branch uses an FCN to produce a set of image-sized â€œprototype masksâ€ that do not depend on any one instance. </li><li>The second adds an extra head to the object detection branch to predict a vector of â€œmask coefficientsâ€ for each anchor that encode an instanceâ€™s rep- resentation in the prototype space.</li><li>linearly combining </li></ul></li><li><p>Rationale </p><ul><li>masks are spatially coherentï¼šmashæ˜¯ç©ºé—´ç›¸å…³çš„ï¼Œç›¸é‚»åƒç´ å¾ˆå¯èƒ½æ˜¯ä¸€ç±»</li><li>å·ç§¯å±‚èƒ½å¤Ÿåˆ©ç”¨åˆ°è¿™ç§ç©ºé—´ç›¸å…³æ€§ï¼Œä½†æ˜¯fcå±‚ä¸èƒ½</li><li>è€Œone-stageæ£€æµ‹å™¨çš„æ£€æµ‹å¤´é€šå¸¸æ˜¯fcå±‚ï¼Ÿï¼Ÿ</li><li>making use of fc layers, which are good at producing semantic vectors</li><li>and conv layers, which are good at producing spatially coherent masks</li></ul></li><li><p>Prototype </p><ul><li>åœ¨backbone feature layer P3ä¸Šæ¥ä¸€ä¸ªFCN<ul><li>taking protonet from deeper backbone features produces more robust masks</li><li>higher resolution prototypes result in both higher quality masks and better performance on smaller objects</li><li>upsampleåˆ°x4çš„å°ºåº¦to increase performance on small objects</li></ul></li><li><p>headåŒ…å«kä¸ªchannels</p><ul><li>æ¢¯åº¦å›ä¼ æ¥æºäºæœ€ç»ˆçš„final assembled maskï¼Œä¸æ˜¯å½“å‰è¿™ä¸ªå¤´</li><li>unboundedï¼šReLU or no nonlinearity</li><li>We choose ReLU for more interpretable prototypes</li></ul><p><img src="/2020/07/17/YOLOACT/Protonet.png" width="40%"></p></li></ul></li><li><p>Mask Coefficients </p><ul><li>a third branch in parallel with detection heads</li><li><p>nonlinearityï¼šè¦æœ‰æ­£è´Ÿï¼Œæ‰€ä»¥tanh </p><p><img src="/2020/07/17/YOLOACT/Coefficients.png" width="40%"></p></li></ul></li><li><p>Mask Assembly </p><ul><li>linear combination + sigmoid: $M=\sigma(PC^T)$</li><li>loss<ul><li>cls lossï¼šw=1, å’Œssdä¸€æ ·ï¼Œc+1 softmax</li><li>box reg lossï¼šw=1.5, å’Œssdä¸€æ ·ï¼Œsmooth-L1</li><li>mask lossï¼šw=6.125ï¼Œ BCE</li></ul></li><li>crop mask<ul><li>evalï¼šç”¨predict boxå»crop</li><li>trainï¼šç”¨gt boxå»cropï¼ŒåŒæ—¶è¿˜è¦ç»™mask lossé™¤ä»¥gt boxçš„é¢ç§¯ï¼Œto preserve small objects </li></ul></li></ul></li><li><p>Emergent Behavior </p><ul><li><p>ä¸cropä¹Ÿèƒ½åˆ†å‰²ä¸­å¤§ç›®æ ‡ï¼š</p><ul><li>YOLACT learns how to localize instances on its own via different activations in its prototypes</li><li>è€Œä¸æ˜¯é å®šä½ç»“æœ </li></ul></li><li><p>translation variant</p><ul><li>the consistent rim of padding in modern FCNs like ResNet gives the network the ability to tell how far away from the imageâ€™s edge a pixel isï¼Œæ‰€ä»¥ç”¨ä¸€å¼ çº¯è‰²çš„å›¾èƒ½å¤Ÿçœ‹å‡ºkernelå®é™…highlightçš„æ˜¯å“ªéƒ¨åˆ†ç‰¹å¾</li><li>åŒä¸€ç§kernelï¼ŒåŒä¸€ç§äº”è§’æ˜Ÿï¼Œåœ¨ç”»é¢ä¸åŒä½ç½®ï¼Œå¯¹åº”çš„å“åº”å€¼æ˜¯ä¸åŒçš„ï¼Œè¯´æ˜fcnæ˜¯èƒ½å¤Ÿæå–ç‰©ä½“ä½ç½®è¿™æ ·çš„è¯­ä¹‰ä¿¡æ¯çš„</li><li><p>prototypes are compressibleï¼š</p><ul><li>å¢åŠ æ¨¡ç‰ˆæ•°ç›®åè€Œä¸å¤ªæœ‰æ•ˆï¼Œbecause predicting coefficients is difficultï¼Œ</li><li>the network has to play a balancing act to produce the right coef- ficients, and adding more prototypes makes this harder, </li><li>We choose 32 for its mix of performance and speed</li></ul><p><img src="/2020/07/17/YOLOACT/Prototype.png" width="40%"></p></li></ul></li></ul></li><li><p>Network</p><ul><li>speed as well as feature richness</li><li>backboneå‚è€ƒRetinaNetï¼ŒResNet-101 + FPN <ul><li>550x550 inputï¼Œresize</li><li>å»æ‰P2ï¼Œadd P6&amp;P7</li><li>3 anchors per levelï¼Œ[1, 1/2, 2]</li><li>P3çš„anchorå°ºå¯¸æ˜¯24x24ï¼Œæ¥ä¸‹æ¥æ¯å±‚double the scale  </li><li>æ£€æµ‹å¤´ï¼šshared conv+parallel conv</li><li>OHEM </li></ul></li><li><p>single GPUï¼šbatch size 8 using ImageNet weightsï¼Œno extra bn layers</p><p><img src="/2020/07/17/YOLOACT/YOLACT.png" width="80%"></p></li></ul></li><li><p>Fast NMS </p><ul><li>æ„é€ cxnxnçš„çŸ©é˜µï¼Œcä»£è¡¨æ¯ä¸ªclass</li><li>ç„¶åææˆä¸Šä¸‰è§’ï¼Œæ±‚column-wise max</li><li>å†IoU threshold</li><li>15.0 ms faster with a performance loss of 0.3 mAP</li></ul></li><li>Semantic Segmentation Loss <ul><li>using modules not executed at test time </li><li>P3ä¸Š1x1 convï¼Œsigmoid and c channels </li><li>w=1</li><li>+0.4 mAP boost</li></ul></li></ul></li></ol><h2 id="YOLACT-Better-Real-time-Instance-Segmentation"><a href="#YOLACT-Better-Real-time-Instance-Segmentation" class="headerlink" title="YOLACT++: Better Real-time Instance Segmentation"></a>YOLACT++: Better Real-time Instance Segmentation</h2>]]></content>
      
      
        <tags>
            
            <tag> å®ä¾‹åˆ†å‰² </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>cornerNet</title>
      <link href="/2020/07/17/cornerNet/"/>
      <url>/2020/07/17/cornerNet/</url>
      <content type="html"><![CDATA[<h2 id="CornerNet-Detecting-Objects-as-Paired-Keypoints"><a href="#CornerNet-Detecting-Objects-as-Paired-Keypoints" class="headerlink" title="CornerNet: Detecting Objects as Paired Keypoints"></a>CornerNet: Detecting Objects as Paired Keypoints</h2><ol><li><p>åŠ¨æœº</p><ul><li>corner formulation <ul><li>top-left corner  </li><li>bottom-right corner </li></ul></li><li>anchor-free</li><li>corner pooling</li><li>no multi-scale</li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>anchor box drawbacks    </p><ul><li>huge set of anchors boxes to ensure sufficient overlapï¼Œcause huge imbalance</li><li>hyperparameters and design choices</li></ul></li><li><p>cornerNet</p><ul><li><p>detect and group</p><ul><li>heatmap to predict corners<ul><li>ä»æ•°å­¦è¡¨è¾¾ä¸Šçœ‹ï¼Œå…¨å›¾whä¸ªtl cornerï¼Œwhä¸ªbt cornerï¼Œå¯ä»¥è¡¨è¾¾wwhhä¸ªæ¡†</li></ul></li><li>anchor-basedï¼Œå…¨å›¾whä¸ªä¸­å¿ƒç‚¹ï¼Œ9ä¸ªanchor sizeï¼Œåªèƒ½è¡¨è¾¾æœ‰é™çš„æ¡†ï¼Œä¸”å¯èƒ½matchä¸ä¸Š</li><li>embeddings to group pairs of corners</li></ul><p><img src="/2020/07/17/cornerNet/cornerNet.png" width="80%;"></p></li><li><p>corner pooling</p><ul><li><p>better localize corners  which are usually out of the foreground</p><p><img src="/2020/07/17/cornerNet/cornerPooling.png" width="50%;"></p></li></ul></li><li><p>modifid hourglass architecture </p></li><li><p>add our novel variant of focal loss </p></li></ul></li></ul></li><li><p>æ–¹æ³•</p><p> <img src="/2020/07/17/cornerNet/overview.png" width="75%;"></p><ul><li><p>two prediction modules</p><ul><li><p>heatmaps </p><ul><li><p>C channels, C for number of categories </p></li><li><p>binary mask</p></li><li><p>each corner has only one ground-truth positive</p></li><li><p>penalty the neighbored negatives within a radius that still hold high iou (0.3 iou)</p><ul><li>determine the radius </li><li>penalty reduction $=e^{-\frac{x^2+y^2}{2\sigma^2}}$</li></ul></li><li><p>variant focal loss</p><ul><li><script type="math/tex; mode=display">  L_{det} = \frac{-1}{N} \sum^C \sum^H \sum^W   \begin{cases}  (1-p_{i,j})^\alpha log(p_{i,j}), \ \ if y_{ij}=1\\  (1-y_{ij})^\beta (p_{i,j})^\alpha log(1-p_{i,j}), \ \ otherwise  \end{cases}</script></li><li><p>$\alpha=2, \beta=4$</p></li><li><p>N is the number of gts</p></li></ul></li></ul></li><li><p>embeddings</p><ul><li>associative embedding </li><li>use 1-dimension embedding</li><li>pull and push loss on gt positives <ul><li>$L_{pull} = \frac{1}{N} \sum^N [(e_{tk}-e_k)^2 + (e_{bk}-e_k)^2]$</li><li>$L_{push} = \frac{1}{N(N-1)} \sum_j^N\sum_{k\neq j}^N max(0, \Delta -|e_k-e_j|)$</li><li>$e_k$ is the average of $e_{tk}$ and $e{bk}$</li><li>$\Delta$ = 1</li></ul></li></ul></li><li><p>offsets</p><ul><li>ä»heatmap resolution remappingåˆ°origin resolutionå­˜åœ¨ç²¾åº¦æŸå¤±<script type="math/tex; mode=display">  o_k = ï¼ˆ\frac{x_k}{n} - \lfloor \frac{x_k}{n} \rfloorï¼Œ \frac{y_k}{n} - \lfloor \frac{y_k}{n} \rfloorï¼‰</script></li></ul></li></ul></li><li><p>greatly affect the IoU of small bounding boxes  </p></li><li><p>shared among all categories</p></li><li><p>smooth L1 loss on gt positives </p><pre><code>      $$      L_{off} = \frac{1}{N} \sum^N SmoothL1(o_k, \hat o_k)</code></pre><p>  $$</p></li><li><p>corner pooling</p><ul><li>top-left pooling layerï¼š<pre><code>  * ä»å½“å‰ç‚¹(i,j)å¼€å§‹ï¼Œ  * å‘ä¸‹elementwise maxæ‰€æœ‰feature vecorï¼Œå¾—åˆ°$t_{i,j}$  * å‘å³elementwise maxæ‰€æœ‰feature vecorï¼Œå¾—åˆ°$l_{i,j}$  * æœ€åä¸¤ä¸ªvectorç›¸åŠ </code></pre><ul><li>bottom-right cornerï¼šå‘å·¦å‘ä¸Š</li></ul></li></ul><p><img src="/2020/07/17/cornerNet/prediction.png" width="80%;"></p></li><li><p>Hourglass Network </p><ul><li>hourglass modules<ul><li>series of convolution and max pooling layers </li><li>series of upsampling and convolution layers </li><li>skip layers </li></ul></li><li><p>multiple hourglass modules stackedï¼šreprocess the features to capture higher-level information</p><p><img src="/2020/07/17/cornerNet/hourglass.png" width="40%;"></p></li><li><p>intermediate supervision</p><ul><li><p>å¸¸è§„çš„ä¸­ç»§ç›‘ç£ï¼š</p><p>  <img src="/2020/07/17/cornerNet/intermediate.png" width="70%;"></p><p>  ä¸‹ä¸€çº§hourglass moduleçš„è¾“å…¥åŒ…æ‹¬ä¸‰ä¸ªéƒ¨åˆ†</p><ul><li>å‰ä¸€çº§è¾“å…¥</li><li>å‰ä¸€çº§è¾“å‡º</li><li>ä¸­ç»§ç›‘ç£çš„è¾“å‡º</li></ul></li><li><p>æœ¬æ–‡ä½¿ç”¨äº†ä¸­ç»§ç›‘ç£ï¼Œä½†æ˜¯æ²¡æŠŠè¿™ä¸ªç»“æœåŠ å›å»</p><ul><li>hourglass2 inputï¼š1x1 conv-BN to both input and output of hourglass1 + add + relu</li></ul></li></ul></li></ul></li><li><p>Our backbone</p><ul><li>2 hourglasses</li><li>5 times downsamp with channels [256,384,384,384,512]</li><li>use stride2 conv instead of max-pooling</li><li>upsampï¼š2 residual modules + nearest neighbor upsampling</li><li>skip connection: 2 residual modulesï¼Œadd</li><li>mid connection: 4 residual modules</li><li>stem: 7x7 stride2, ch128 + residual stride2, ch256</li><li>hourglass2 inputï¼š1x1 conv-BN to both input and output of hourglass1 + add + relu</li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li>training details<ul><li>randomly initialized, no pretrained</li><li>biasï¼šset the biases in the convolution layers that predict the corner heatmaps</li><li>inputï¼š511x511</li><li>outputï¼š128x128</li><li>apply PCA to the input image</li><li>full lossï¼š$L = L_{det} + \alpha L_{pull} + \beta L_{push} + \gamma L_{off}$<ul><li>é…å¯¹lossï¼š$\alpha=\beta=0.1$</li><li>offset lossï¼š$\gamma=1$</li></ul></li><li>batch size = 49 = 4+5x9</li></ul></li><li>test details<ul><li>NMSï¼š3x3 max pooling on heatmaps</li><li>pickï¼štop100 top-left corners &amp; top100 bottom-right corners</li><li>filter pairsï¼š<ul><li>L1 distance greater than 0.5</li><li>from different categories </li></ul></li><li>fusionï¼šcombine the detections from the original and flipped images  + soft nms</li></ul></li><li>Ablation Study <ul><li>corner pooling is especially helpful for medium and large objects</li><li>penalty reduction especially benefits medium and large objects</li><li>CornerNet achieves a much higher AP at 0.9 IoU than other detectorsï¼šæ›´æœ‰èƒ½åŠ›ç”Ÿæˆé«˜è´¨é‡æ¡†</li><li>error analysisï¼šthe main bottleneck is detecting corners</li></ul></li></ul></li></ol><h2 id="CornerNet-Lite-Efficient-Keypoint-Based-Object-Detection"><a href="#CornerNet-Lite-Efficient-Keypoint-Based-Object-Detection" class="headerlink" title="CornerNet-Lite: Efficient Keypoint-Based Object Detection"></a>CornerNet-Lite: Efficient Keypoint-Based Object Detection</h2><ol><li><p>åŠ¨æœº</p><ul><li><p>keypoint-based methods</p><ul><li>detecting and grouping</li><li>accuary but with processing cost</li></ul></li><li><p>propose CornerNet-Lite </p><ul><li>CornerNet-Saccadeï¼šattention mechanism </li><li>CornerNet-Squeezeï¼ša new compact backbone </li></ul></li><li><p>performance</p><p>  <img src="/2020/07/17/cornerNet/performance.png" width="60%;"></p></li></ul></li><li><p>è®ºç‚¹</p><ul><li>main drawback of cornerNet<ul><li>inference speed</li><li>reducing the number of scales or the image resolution cause a large accuracy drop </li></ul></li><li>two orthogonal directions <ul><li>reduce the number of pixels to processï¼šCornerNet-Saccade </li><li>reduce the amount of processing per pixelï¼š</li></ul></li><li>CornerNet-Saccade <ul><li>downsized attention map </li><li>select a subset of crops to examine in high resolution </li><li>for off-lineï¼šAP of 43.2% at 190ms per image </li></ul></li><li>CornerNet-Squeeze  <ul><li>inspired by squeezeNet and mobileNet</li><li>1x1 convs</li><li>bottleneck layers</li><li>depth-wise separable convolution</li><li>for real-timeï¼šAP of 34.4% at 30ms </li></ul></li><li>combined??<ul><li>CornerNet-Squeeze-Saccade turns out slower and less accurate than CornerNet- Squeeze </li></ul></li><li>Saccadesï¼šæ‰«è§†<ul><li>to generate interesting crops</li><li>RCNNç³»åˆ—ï¼šsingle-type &amp; single object</li><li>AutoFocusï¼šadd a branchè°ƒç”¨faster-RCNNï¼Œthus multi-type &amp; mixed-objectsï¼Œæœ‰single branchæœ‰multi branch</li><li>CornerNet-Saccadeï¼š<ul><li>single-type &amp; multi object</li><li>crops can be much smaller than number of objects</li></ul></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>CornerNet-Saccade </p><p>  <img src="/2020/07/17/cornerNet/Saccade.png" width="60%;"></p><ul><li><p>step1ï¼šobtain possible locations  </p><ul><li>downsizeï¼štwo scalesï¼Œ255 &amp; 192ï¼Œzero-padding</li><li>predicts 3 attention maps<ul><li>small objectï¼šlonger side&lt;32 pixels</li><li>medium objectï¼š32-96</li><li>large objectï¼š&gt;96</li><li>so that we can control the zoom-in factorï¼šzoom-in more for smaller objects</li><li>feature mapï¼šdifferent scales from the upsampling layers</li><li>attention mapï¼š3x3 conv-relu + 1x1 conv-sigmoid</li><li>process locations where scores &gt; 0.3</li></ul></li></ul></li><li><p>step2ï¼šfiner detection</p><ul><li>zoom-in scalesï¼š4ï¼Œ2ï¼Œ1 for smallã€mediumã€large objects</li><li>apply CornerNet-Saccade on the ROI<ul><li>255x255 window</li><li>centered at the location</li></ul></li></ul></li><li><p>step3ï¼šNMS</p><ul><li>soft-nms</li><li>remove the bounding boxes which touch the crop boundary</li></ul></li><li><p>CornerNet-Saccade uses the same network for attention maps and bounding boxes</p><ul><li>åœ¨ç¬¬ä¸€æ­¥çš„æ—¶å€™ï¼Œå¯¹ä¸€äº›å¤§ç›®æ ‡å·²ç»æœ‰äº†æ£€æµ‹æ¡†</li><li>ä¹Ÿè¦zoom-inï¼ŒçŸ«æ­£ä¸€ä¸‹</li></ul></li><li><p>efficiency</p><ul><li>regions/croped imageséƒ½æ˜¯processed in batch/parallel</li><li>resize/cropæ“ä½œåœ¨GPUä¸­å®ç°</li><li><p>suppress redundant regions using a NMS-similar policy before prediction</p><p><img src="/2020/07/17/cornerNet/efficiency.png" width="60%;"></p></li></ul></li></ul></li><li><p>new hourglass backbone</p><ul><li>3 hourglass moduleï¼Œdepth 54</li><li>downsize twice before hourglass modules</li><li>downsize 3 times in each moduleï¼Œwith channels [384,384,512]</li><li>one residual in both encoding path &amp; skip connection</li><li>mid connectionï¼šone residualï¼Œwith channels 512</li></ul></li><li><p>CornerNet-Squeeze </p><ul><li>to replace the heavy hourglass104</li><li>use fire module to replace residuals</li><li>downsizes 3 times before hourglass modules</li><li>downsize 4 times in each module</li><li>replace the 3x3 conv in prediction head with 1x1 conv</li><li><p>replace the nearest neighboor upsampling with 4x4 transpose conv</p><p><img src="/2020/07/17/cornerNet/fire.png" width="60%;"></p></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ç›®æ ‡æ£€æµ‹ï¼Œanchor-free </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SOLO</title>
      <link href="/2020/07/17/SOLO/"/>
      <url>/2020/07/17/SOLO/</url>
      <content type="html"><![CDATA[<p>[SOLO] SOLO: Segmenting Objects by Locationsï¼šå­—èŠ‚ï¼Œç›®å‰ç»å¤§å¤šæ•°æ–¹æ³•å®ä¾‹åˆ†å‰²çš„ç»“æ„éƒ½æ˜¯é—´æ¥å¾—åˆ°â€”â€”æ£€æµ‹æ¡†å†…è¯­ä¹‰åˆ†å‰²ï¼å…¨å›¾è¯­ä¹‰åˆ†å‰²èšç±»ï¼Œä¸»è¦åŸå› æ˜¯formulation issueï¼Œå¾ˆéš¾æŠŠå®ä¾‹åˆ†å‰²å®šä¹‰æˆä¸€ä¸ªç»“æ„åŒ–çš„é—®é¢˜</p><p>[SOLOv2] SOLOv2: Dynamic, Faster and Strongerï¼šbest 41.7% AP </p><h2 id="SOLO-Segmenting-Objects-by-Locations"><a href="#SOLO-Segmenting-Objects-by-Locations" class="headerlink" title="SOLO: Segmenting Objects by Locations"></a>SOLO: Segmenting Objects by Locations</h2><ol><li><p>åŠ¨æœº</p><ul><li>challengingï¼šarbitrary number of instances</li><li>form the task into a classification-solvable problem</li><li>direct &amp; end-to-end &amp; one-stage &amp; using mask annotations solely</li><li>on par accuracy with Mask R-CNN</li><li>outperforming recent single-shot instance segmenters</li></ul></li><li><p>è®ºç‚¹</p><ul><li>formulating<ul><li>Objects in an image belong to a fixed set of semantic categoriesâ€”â€”semantic segmentation can be easily formulated as a dense per-pixel classification problem</li><li>the number of instances varies</li></ul></li><li>existing methods<ul><li>æ£€æµ‹ï¼èšç±»ï¼šstep-wise and indirect</li><li>ç´¯ç§¯è¯¯å·®</li></ul></li><li>core idea <ul><li>in most cases two instances in an image either have different center locations or have different object sizes </li><li>locationï¼š<ul><li>think image as a divided grid of cells</li><li>an object instance is assigned to one of the grid cells as its center location category </li><li>encode center location categories as the channel axis </li></ul></li><li>size<ul><li>FPN</li><li>assign objects of different sizes to different levels of feature maps</li></ul></li><li>SOLO converts coordinate regression into classification by discrete quantization </li><li>One feat of doing so is the avoidance of heuristic coordination normalization and log-transformation typically used in detectorsã€ï¼Ÿï¼Ÿï¼Ÿä¸æ‡‚è¿™å¥è¯æƒ³è¡¨è¾¾å•¥ã€‘</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>problem formulation</p><ul><li>divided grids</li><li><p>simultaneous task</p><ul><li>category-aware prediction  </li><li>instance-aware mask generation</li></ul><p><img src="/2020/07/17/SOLO/solo.png" width="70%;"></p></li><li><p>category prediction</p><ul><li>predict instance for each gridï¼š$S<em>S</em>C$</li><li>grid sizeï¼š$S*S$</li><li>number of classesï¼š$C$</li><li>based on the assumption that each cell must belong to one individual instance</li><li>C-dim vec indicates the class probability for each object instance in each grid</li></ul></li><li>mask prediction<ul><li>predict instance mask for each positive cellï¼š$H<em>W</em>S^2$</li><li>the channel corresponding to the location</li><li>position sensitiveï¼šå› ä¸ºæ¯ä¸ªgridä¸­åˆ†å‰²çš„maskæ˜¯è¦æ˜ å°„åˆ°å¯¹åº”çš„channelçš„ï¼Œå› æ­¤æˆ‘ä»¬å¸Œæœ›ç‰¹å¾å›¾æ˜¯spatially variant<ul><li>è®©ç‰¹å¾å›¾spatially variantçš„æœ€ç›´æ¥åŠæ³•å°±æ˜¯åŠ ä¸€ç»´spatially variantçš„ä¿¡æ¯</li><li>inspired by CoordConvï¼šæ·»åŠ ä¸¤ä¸ªé€šé“ï¼Œnormed_xå’Œnormed_yï¼Œ[-1,1]</li><li>original feature tensor $H<em>W</em>D$ becomes $H<em>W</em>(D+2)$</li></ul></li></ul></li><li>final results<ul><li>gather category prediction &amp; mask prediction</li><li>NMS</li></ul></li></ul></li><li><p>network</p><ul><li>backboneï¼šresnet</li><li>FCNï¼š256-d</li><li><p>headsï¼šweights are shared across different levels except for the last 1x1 conv</p><p><img src="/2020/07/17/SOLO/head.png" width="40%;"></p></li></ul></li><li><p>learning</p><ul><li>positive gridï¼šfalls into a center region<ul><li>maskï¼šmask center $(c_x, c_y)$ï¼Œmask size $(h,w)$</li><li>center regionï¼š$(c_x,c_y,\epsilon w, \epsilon h)$ï¼Œset $\epsilon = 0.2$</li></ul></li><li>lossï¼š$L = L_{cate} + \lambda L_{seg}$<ul><li>cate lossï¼šfocal loss</li><li>seg lossï¼šdiceï¼Œ$L_{mask} = \frac{1}{N_{pos}}\sum_k 1_{p^<em>_{i,j}&gt;0} dice(m_k, m^</em>_k) $ï¼Œå¸¦æ˜Ÿå·çš„æ˜¯groud truth</li></ul></li></ul></li><li><p>inference</p><ul><li><p>use a confidence threshold of 0.1 to filter out low spacial predictions</p></li><li><p>use a threshold of 0.5 to binary the soft masks</p></li><li><p>select the top 500 scoring masks </p></li><li><p>NMS </p><ul><li>Only one instance will be activated at each grid</li><li><p>and one in- stance may be predicted by multiple adjacent mask channels </p><p><img src="/2020/07/17/SOLO/results.png" width="70%;"></p></li></ul></li><li><p>keep top 100</p></li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li><p>grid number</p><ul><li>é€‚å½“å¢åŠ æœ‰æå‡ï¼Œä¸»è¦æå‡è¿˜æ˜¯åœ¨FPN</li><li><img src="/2020/07/17/SOLO/grid.png" width="40%;"></li></ul></li><li><p>fpn</p><ul><li>äº”ä¸ªFPN pyramids </li><li><p>å¤§ç‰¹å¾å›¾ï¼Œå°æ„Ÿå—é‡ï¼Œç”¨æ¥åˆ†é…å°ç›®æ ‡ï¼Œgridæ•°é‡è¦å¢å¤§</p></li><li><p><img src="/2020/07/17/SOLO/fpn.png" width="40%;"></p></li></ul></li><li><p>feature alignment</p><ul><li>åœ¨åˆ†ç±»branchï¼Œ$H<em>W$ç‰¹å¾å›¾è¦è½¬æ¢æˆ$S</em>S$çš„ç‰¹å¾å›¾<ul><li>interpolationï¼šbilinear interpolating </li><li>adaptive-poolï¼šapply a 2D adaptive max-pool </li><li>region-grid- interpolationï¼šå¯¹æ¯ä¸ªcellï¼Œé‡‡æ ·å¤šä¸ªç‚¹åšåŒçº¿æ€§æ’å€¼ï¼Œç„¶åå–å¹³å‡</li></ul></li><li>is no noticeable performance gap between these variants </li><li>ï¼ˆå¯èƒ½å› ä¸ºæœ€ç»ˆæ˜¯åˆ†ç±»ä»»åŠ¡</li></ul></li><li><p>head depth</p><ul><li>4-7æœ‰æ¶¨ç‚¹</li><li>æ‰€ä»¥æœ¬æ–‡é€‰äº†7</li><li><img src="/2020/07/17/SOLO/depth.png" width="40%;"></li></ul></li></ul></li><li><p>decoupled SOLO</p><ul><li><p>mask branché¢„æµ‹çš„channelæ•°æ˜¯$S^2$ï¼Œå…¶ä¸­å¤§éƒ¨åˆ†channelå…¶å®æ˜¯æ²¡æœ‰è´¡çŒ®çš„ï¼Œç©ºå å†…å­˜</p></li><li><p>prediction is somewhat redundant as in most cases the objects are located sparsely in the image </p></li><li><p>element-wise multiplication </p><p>  <img src="/2020/07/17/SOLO/decoupled.png" width="40%;"></p></li><li><p>å®éªŒä¸‹æ¥</p><ul><li>achieves the same performance </li><li>efficient and equivalent variant  </li></ul></li></ul></li></ol><h2 id="SOLOv2-Dynamic-Faster-and-Stronger"><a href="#SOLOv2-Dynamic-Faster-and-Stronger" class="headerlink" title="SOLOv2: Dynamic, Faster and Stronger"></a>SOLOv2: Dynamic, Faster and Stronger</h2><ol><li><p>åŠ¨æœº</p><ul><li>take one step further on the mask head<ul><li>dynamically learning the mask head</li><li>decoupled into mask kernel branch and mask feature branch</li></ul></li><li>propose Matrix NMS<ul><li>faster &amp; better results</li></ul></li><li>try object detection  and panoptic segmentation</li></ul></li><li><p>è®ºç‚¹</p><ul><li>SOLO develop pure instance segmentation </li><li>instance segmentation <ul><li>requires instance-level and pixel-level predictions simultaneously </li><li>most existing instance segmentation methods build on the top of bounding boxes </li><li>SOLO develop pure instance segmentation </li></ul></li><li>SOLOv2 improve SOLO<ul><li>mask learningï¼šdynamic scheme </li><li>mask NMSï¼šparallel matrix operationsï¼Œoutperforms Fast NMS </li></ul></li><li>Dynamic Convolutions <ul><li>STNï¼šadaptively transform feature maps conditioned on the input </li><li>Deformable Convolutional Networksï¼šlearn location</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>revisit SOLOv1</p><ul><li>redundant mask prediction</li><li>decouple</li><li><p>dynamicï¼šdynamically pick the valid ones from predicted $s^2$ classifiers and perform the convolution </p><p><img src="/2020/07/17/SOLO/dynamic.png" width="80%;"></p></li></ul></li><li><p>SOLOv2</p><ul><li><p>dynamic mask segmentation head</p><ul><li>mask kernel branch</li><li>mask feature branch</li></ul></li><li><p>mask kernel branch</p><ul><li>prediction headsï¼š4 convs + 1 final convï¼Œshared across scale</li><li>no activation on the output</li><li>concat normalized coordinates in two additional input channels at start</li><li>ouputs D-dims kernel weights for each gridï¼še.g.  for 3x3 conv with E input channels, outputs $S<em>S</em>9E$</li></ul></li><li><p>mask feature branch</p><ul><li><p>predict instance-aware featureï¼š$F \in R^{H<em>W</em>E}$</p></li><li><p>unified and high-resolution mask featureï¼šåªè¾“å‡ºä¸€ä¸ªå°ºåº¦çš„ç‰¹å¾å›¾ï¼Œencoded x32 feature with coordinates info</p><ul><li>we feed normalized pixel coordinates to the deepest FPN level (at 1/32 scale)</li><li>repeated ã€3x3 conv, group norm, ReLU, 2x bilinear upsamplingã€‘</li><li>element-wise sum</li><li><p>last layerï¼š1x1 conv, group norm, ReLU</p><p><img src="/2020/07/17/SOLO/unified mask.png" width="50%;"></p></li></ul></li></ul></li><li><p>instance mask</p><ul><li>mask feature branch conved by the mask kernel branchï¼šfinal conv $H<em>W</em>S^2$</li><li>mask NMS</li></ul></li><li><p>train</p><ul><li>lossï¼š$L = L_{cate} + \lambda L_{seg}$<ul><li>cate lossï¼šfocal loss</li><li>seg lossï¼šdiceï¼Œ$L_{mask} = \frac{1}{N_{pos}}\sum_k 1_{p^<em>_{i,j}&gt;0} dice(m_k, m^</em>_k) $ï¼Œå¸¦æ˜Ÿå·çš„æ˜¯groud truth</li></ul></li></ul></li><li><p>inference</p><ul><li>category scoreï¼šfirst use a confidence threshold of 0.1 to filter out predictions with low confidence </li><li>mask branchï¼šrun convolution based on the filtered category map</li><li>sigmoid</li><li>use a threshold of 0.5 to convert predicted soft masks to binary masks </li><li>Matrix NMS</li></ul></li><li><p>Matrix NMS</p><ul><li>decremented functions  <ul><li>linearï¼š$f(iou_{i,j}=1-iou_{i,j})$</li><li>gaussianï¼š$f(iou_{i,j}=exp(-\frac{iou_{i,j}^2}{\sigma})$</li></ul></li><li>the most overlapped prediction for $m_i$ï¼šmax iou<ul><li>$f(iou_{*,i}) = min_{s_k}f(iou_{k,i})$</li></ul></li><li>decay factor  <ul><li>$decay_i = min \frac{f(iou_{i,j})}{f(iou_{*,i})}$</li></ul></li><li><img src="/2020/07/17/SOLO/matrix nms.png" width="60%;"></li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> å®ä¾‹åˆ†å‰² </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>polarMask</title>
      <link href="/2020/06/29/polarMask/"/>
      <url>/2020/06/29/polarMask/</url>
      <content type="html"><![CDATA[<h2 id="PolarMask-Single-Shot-Instance-Segmentation-with-Polar-Representation"><a href="#PolarMask-Single-Shot-Instance-Segmentation-with-Polar-Representation" class="headerlink" title="PolarMask: Single Shot Instance Segmentation with Polar Representation"></a>PolarMask: Single Shot Instance Segmentation with Polar Representation</h2><ol><li><p>åŠ¨æœº</p><ul><li>instance segmentation</li><li>anchor-free</li><li>single-shot</li><li>modified on FCOS</li></ul></li><li><p>è®ºç‚¹</p><ul><li>two-stage methods <ul><li>FCIS, Mask R-CNN</li><li>bounding box detection then semantic segmentation within each box</li></ul></li><li>single-shot method<ul><li>formulate the task as instance center classification and dense distance regression in a polar coordinate </li><li>FCOS can be regarded as a special case that the contours has only 4 directions</li></ul></li><li><p>this paper</p><ul><li>two parallel taskï¼š<ul><li>instance center classification</li><li>dense distance regression</li></ul></li><li>Polar IoU Loss can largely ease the optimization and considerably improve the accuary</li><li>Polar Centerness improves the original idea of â€œCentrenessâ€ in FCOS, leading to further performance boost</li></ul><p><img src="/2020/06/29/polarMask/PolarMask.png" width="80%;"></p></li></ul></li><li><p>æ–¹æ³•</p><ul><li>architecture <ul><li>back &amp; fpn are the same as FCOS </li><li>model the instance mask as one center and n rays<ul><li>conclude that mass-center is more advantageous than box center</li><li>the angle interval is pre-fixed, thus only the length of the rays is to be regressed</li><li>positive samplesï¼šfalls into 1.5xstrides of the area around the gt mass-centerï¼Œthat is 9-16 pixels around gt grid</li><li>distance regression<ul><li>å¦‚æœä¸€æ¡å°„çº¿ä¸Šå­˜åœ¨å¤šä¸ªäº¤ç‚¹ï¼Œå–æœ€é•¿çš„</li><li>å¦‚æœä¸€æ¡å°„çº¿ä¸Šæ²¡æœ‰äº¤ç‚¹ï¼Œå–æœ€å°å€¼$\epsilon=10^{-6}$</li></ul></li></ul></li></ul></li><li>potential issuse of the mask regression branch<ul><li>dense regression task with such as 36 rays, may cause imbalance between regression loss and classification  loss</li><li>n rays are relevant and should be trained as a whole rather than a set of independent valuesâ€”-&gt;iou loss</li></ul></li><li>inference<ul><li>multiply center-ness with classification to obtain final confidence scores, conf thresh=0.05</li><li>take top-1k predictions per fpn level</li><li>use the smallest bounding boxes to run NMS, nms thresh=0.5</li></ul></li><li>polar centerness<ul><li>to suppress low quality detected centers</li><li>$polar\ centerness=\sqrt{\frac{min(\{d_1,d_2, â€¦, d_n\})}{max(\{d_1,d_2, â€¦, d_n\})}}$</li><li>$d_{min}$å’Œ$d_{max}$è¶Šæ¥è¿‘ï¼Œè¯´æ˜ä¸­å¿ƒç‚¹è´¨é‡è¶Šå¥½</li><li>Experiments show that Polar Centerness improves accuracy especially under stricter localization metrics, such as $AP_{75}$</li></ul></li><li>polar IoU loss<ul><li>polar IoUï¼š$IoU=lim_{N\to\inf}\frac{\sum_{i=1}^N\frac{1}{2} d_{min}^2 \Delta \theta}{\sum_{i=1}^N\frac{1}{2} d_{max}^2 \Delta \theta}$</li><li>empirically observe that å»æ‰å¹³æ–¹é¡¹æ•ˆæœæ›´å¥½ï¼š$polar\ IoU=\frac{\sum_{i=1}^n d_{min}}{\sum_{i=1}^n d_{max}}$</li><li>polar iou lossï¼šbce of polar IoUï¼Œ$-log(\frac{\sum_{i=1}^n d_{min}}{\sum_{i=1}^n d_{max}})$</li><li>advantage<ul><li>differentiable, enable bp</li><li>regards the regression targets as a whole</li><li>keep balance with classification loss </li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> å®ä¾‹åˆ†å‰²ï¼Œæåæ ‡ï¼Œone-stage </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>FCOS</title>
      <link href="/2020/06/23/FCOS/"/>
      <url>/2020/06/23/FCOS/</url>
      <content type="html"><![CDATA[<h2 id="FCOS-Fully-Convolutional-One-Stage-Object-Detection"><a href="#FCOS-Fully-Convolutional-One-Stage-Object-Detection" class="headerlink" title="FCOS: Fully Convolutional One-Stage Object Detection"></a>FCOS: Fully Convolutional One-Stage Object Detection</h2><ol><li><p>åŠ¨æœº</p><ul><li>anchor free</li><li>proposal free</li><li>avoids the complicated computation related to anchor boxes<ul><li>calculating overlapping during training</li></ul></li><li>avoid all hyper-parameters related to anchor boxes<ul><li>size &amp; shape</li><li>positiveï¼ignoredï¼negative </li></ul></li><li>leverage as many foreground samples as possible</li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>anchor-based detectors</p><ul><li>detection performance is sensitive to anchor settings</li><li>encounter difficulties in cases with large shape variations</li><li>hamper the generalization ability of detectors </li><li>dense proposeï¼šthe excessive number of negative samples aggravates the imbalance </li><li>involve complicated computationï¼šsuch as calculating the IoU with gt boxes</li></ul></li><li><p>FCN-based detector</p><ul><li>predict a 4D vector plus a class category at each spatial location on a level of feature maps</li><li>do not work well when applied to overlapped bounding boxes</li><li><p><strong>with FPN this ambiguity can be largely eliminated</strong></p><p><img src="/2020/06/23/FCOS/fcn-based.png" width="40%;"></p></li></ul></li><li><p>anchor-free detector</p><ul><li>yolov1ï¼šonly the points near the center are usedï¼Œlow recall  </li><li>CornerNetï¼šcomplicated post-processing to match the pairs of corners</li><li>DenseBoxï¼šdifficulty in handling overlapping bounding boxes </li></ul></li><li><p>this methos</p><ul><li>use FPN to deal with ambiguity</li><li>dense predictï¼š<strong>use all points in a ground truth bounding box</strong> to predict the bounding box</li><li>introduce â€œcenter-nessâ€ branch to predict the deviation of a pixel to the center of its corresponding bounding box</li><li>can be used as a RPN in two-stage detectors and can achieve significantly better performance</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>ground truth boxesï¼Œ$B_i=(x_0, y_0, x_1, y_1, c)$ï¼Œcorners + cls</p></li><li><p>anchor-freeï¼šeach location (x,y)ï¼Œmap into abs input image (xs+[s/2], ys+[s/2])</p></li></ul></li></ol><ul><li><p>positive sampleï¼šif a location falls into any ground-truth box</p></li><li><p>ambiguous sampleï¼šlocation falls into multiple gt boxesï¼Œ<strong>choose the box with minimal area</strong></p></li><li><p>regression targetï¼šl t r b distanceï¼Œlocation to the four sides</p><ul><li><p>cls branch</p><ul><li>C binary classifiers </li><li>C-dims vector p</li></ul></li><li>focal loss<ul><li>$\frac{1}{N_{pos}} \sum_{x,y}L_{cls}(p_{x,y}, c_{x,y}^*)$</li></ul></li><li><p>calculate on both positive/negative samples</p></li><li><p>box reg branch</p><ul><li>4-dims vector t</li></ul></li><li>IOU loss<ul><li>$\frac{1}{N_{pos}} \sum_{x,y}1_{\{c_{x,y}^<em>&gt;0\}}L_{reg}(t_{x,y}, t_{x,y}^</em>)$</li></ul></li><li>calculate on positive samples</li></ul></li><li><p>inference</p><ul><li><p>choose the location with p &gt; 0.05 as positive samples</p></li><li><p>two possible issues  </p><ul><li>large stride makes BPR low, which is actually not a problem in FCOS</li></ul></li><li><p>overlaps gt boxes cause ambiguity, which can be greatly resolved with multi-level prediction </p></li><li><p>FPN</p><ul><li>P3, P4, P5ï¼š1x1 conv from C3, C4, C5, top-down connections</li></ul></li><li><p>P6, P7: stride2 conv from P5, P6</p></li><li><p>limit the bbox regression for each level</p><ul><li>$m_i$ï¼šmaximum distance for each level</li></ul></li><li>if a locationâ€™s gt bbox satifiesï¼š$max(l^<em>,t^</em>,r^<em>,b^</em>)&gt;m_i$ or $max(l^<em>,t^</em>,r^<em>,b^</em>)&lt;m_{i-1}$ï¼Œit is set as a negative sampleï¼Œnot regress at current level<ul><li>objects with different sizes are assigned to different feature levelsï¼šlargely alleviateä¸€éƒ¨åˆ†box overlappingé—®é¢˜</li></ul></li><li><p>for other overlapping casesï¼šsimply choose the gt box with minimal area</p></li><li><p>sharing heads between different feature levels </p></li><li><p>to regress different size rangeï¼šuse $exp(s_ix)$</p><ul><li>trainable scalar $s_i$</li></ul></li><li>slightly improve</li></ul></li><li><p>center-ness</p><ul><li><p>low-quality predicted bounding boxes are produced by locations far away from the center of an object</p><ul><li><p>predict the â€œcenter-nessâ€ of a location </p></li><li><p>normalized distance </p><script type="math/tex; mode=display">  centerness^* = \sqrt {\frac{min(l^*,r^*)}{max(l^*,r^*)}* \frac{min(t^*,b^*)}{max(t^*,b^*)}}</script></li></ul></li><li><p>sqrt to slow down the decay</p></li><li><p>[0,1] use bce loss</p></li><li><p>when inference center-ness is mutiplied with the class scoreï¼šcan down-weight the scores of bounding boxes far from the center of an object, then filtered out by NMS</p><ul><li>an alternative of the center-nessï¼šuse of only the central portion of ground-truth bounding box as positive samplesï¼Œå®éªŒè¯æ˜ä¸¤ç§æ–¹æ³•ç»“åˆæ•ˆæœæœ€å¥½</li></ul></li><li><p>architecture</p><ul><li>two minor differences from the standard RetinaNet<ul><li>use <strong>Group Normalization</strong> in the <strong>newly added convolutional layers</strong> except for the last prediction layers </li><li>use P5 instead of C5 to produce P6&amp;P7</li></ul></li></ul><p><img src="/2020/06/23/FCOS/architecture.png" width="70%;"></p><p>â€‹    </p></li></ul></li></ul>]]></content>
      
      
        <tags>
            
            <tag> ç›®æ ‡æ£€æµ‹ï¼Œå…¨å·ç§¯ï¼Œone-stageï¼Œcenternessï¼Œanchor free </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>FCIS</title>
      <link href="/2020/06/22/FCIS/"/>
      <url>/2020/06/22/FCIS/</url>
      <content type="html"><![CDATA[<h2 id="Fully-Convolutional-Instance-aware-Semantic-Segmentation"><a href="#Fully-Convolutional-Instance-aware-Semantic-Segmentation" class="headerlink" title="Fully Convolutional Instance-aware Semantic Segmentation"></a>Fully Convolutional Instance-aware Semantic Segmentation</h2><ol><li><p>åŠ¨æœº</p><ul><li>instance segmentationï¼š<ul><li>å®ä¾‹åˆ†å‰²æ¯”èµ·æ£€æµ‹ï¼Œéœ€è¦å¾—åˆ°ç›®æ ‡æ›´ç²¾ç¡®çš„è¾¹ç•Œä¿¡æ¯</li><li>æ¯”èµ·è¯­ä¹‰åˆ†å‰²ï¼Œéœ€è¦åŒºåˆ†ä¸åŒçš„ç‰©ä½“</li></ul></li><li>detects and segments simultanously</li><li>FCN + instance mask proposal</li></ul></li><li><p>è®ºç‚¹</p><ul><li>FCNs do not work for the instance-aware semantic segmentation task <ul><li>convolution is translation invariantï¼šæƒå€¼å…±äº«ï¼Œä¸€ä¸ªåƒç´ å€¼å¯¹åº”ä¸€ä¸ªå“åº”å€¼ï¼Œä¸ä½ç½®æ— å…³</li></ul></li><li>instance segmentation operates on region level <ul><li>the same pixel can have different semantics in different regions </li><li>Certain translation-variant property is required </li></ul></li><li>prevalent method<ul><li>step1: an FCN is applied on the whole image to generate shared feature maps </li><li>step2: a pooling layer warps each region of interest into fixed-size per-ROI feature maps </li><li>step3: use fc layers to convert the per-ROI feature maps to per-ROI masks </li><li><strong>the translation-variant property is introduced in the fc layer(s) in the last step</strong></li><li>drawbacks <ul><li>the ROI pooling step losses spatial details </li><li>the fc layers over-parametrize the task </li></ul></li></ul></li><li>InstanceFCN<ul><li>position-sensitive score maps</li><li>sliding windows </li><li>sub-tasks are separated and the solution is not end-to-end </li><li>blind to the object categoriesï¼šå‰èƒŒæ™¯åˆ†å‰²</li></ul></li><li><p>In this work</p><ul><li>extends InstanceFCN</li><li>end-to-end </li><li>fully convolutional </li><li>operates on box proposals instead of sliding windows</li><li>per-ROI computation does not involve any warping or resizing operations</li></ul><p><img src="/2020/06/22/FCIS/FCIS.png" width="60%"></p></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>position-sensitive score map</p><ul><li>FCN<ul><li>predict a single score map</li><li>predict each pixelâ€™s likelihood score of belonging to each category</li></ul></li><li>at instance level<ul><li>the same pixel can be foreground on one object but background on another </li><li>a single score map per-category is insufficient to distinguish these two cases</li></ul></li><li>a fully convolutional solution for instance mask proposal <ul><li>k x k evenly partitioned cells of object </li><li>thus obtain k x k position-sensitive score maps </li><li>Each score represents å½“å‰åƒç´ åœ¨<strong>å½“å‰ä½ç½®ï¼ˆscore mapåœ¨cellsä¸­çš„ä½ç½®ï¼‰</strong>ä¸Šå±äºæŸä¸ªç‰©ä½“å®ä¾‹çš„ä¼¼ç„¶å¾—åˆ†</li><li>assembling (copy-paste) </li></ul></li></ul></li><li><p>jointly and simultaneously</p><ul><li>The same set of score maps are shared for the two sub-tasks </li><li>For each pixel in a ROI, there are two tasks:  <ul><li>detectionï¼šwhether it belongs to an object bounding box </li><li>segmentationï¼šwhether it is inside an object instanceâ€™s boundary </li><li>separateï¼štwo 1x1 conv heads</li><li>fuseï¼šinside and outside <ul><li>high inside score and low outside scoreï¼šdetection+, segmentation+</li><li>low inside score and high outside scoreï¼šdetection+, segmentation-</li><li>low inside score and low outside scoreï¼šdetection-, segmentation-</li><li>detection score<ul><li>average pooling over all pixelsâ€˜ likelihoods for each class</li><li>max(detection score) represent the object</li></ul></li><li>segmentation <ul><li>softmax(inside, outside) for each pixel to distinguish fgï¼bg</li></ul></li></ul></li></ul></li><li><p>All the per-ROI components are implemented through convs</p><ul><li>local weight sharing propertyï¼ša regularization mechanism </li><li>without involving any feature warping, resizing or fc layers </li><li>the per-ROI computation cost is negligible</li></ul><p><img src="/2020/06/22/FCIS/fuse.png" width="70%"></p></li></ul></li><li><p>architecture </p><ul><li>ResNet back produce features with 2048 channels</li><li>a 1x1 conv reduces the dimension to 1024</li><li>x16 output strideï¼šconv5 stride is decreased from 2 to 1, the dilation is increased from 1 to 2</li><li>head1ï¼šjoint det conf &amp; segmentation<ul><li>1x1 convï¼Œgenerates $2k^2(C+1)$ score maps</li><li>2 for insideï¼outside</li><li>$k^2$ for $k^2$ä¸ªposition</li><li>$(C+1)$ for fgï¼bg</li></ul></li><li>head2ï¼šbbox regression <ul><li>1x1 convï¼Œ$4k^2$ channels</li></ul></li><li>RPN to generate ROIs</li><li>inference<ul><li>300 ROIs </li><li>pass through the bbox regression obtaining another 300 ROIs </li><li>pass through joint head to obtain detection score&amp;fg mask for all categories </li><li>mask votingï¼šæ¯ä¸ªROI (with max det score) åªåŒ…å«å½“å‰ç±»åˆ«çš„å‰æ™¯ï¼Œè¿˜è¦è¡¥ä¸Šæ¡†å†…å…¶ä»–ç±»åˆ«èƒŒæ™¯<ul><li>for current ROI, find all the ROIs (from the 600) with IoU scores higher than 0.5 </li><li>their fg masks are averaged per-pixel and weighted by the classification score </li></ul></li></ul></li><li><p>training</p><ul><li>ROI positiveï¼negativeï¼šIoU&gt;0.5</li><li>loss<ul><li>softmax detection loss over C+1 categories</li><li>softmax segmentation loss over the gt fg mask, on positive ROIs</li><li>bbox regression loss, , on positive ROIs</li></ul></li><li>OHEMï¼šamong the 300 proposed ROIs on one image, 128 ROIs with the highest losses are selected to back-propagate their error gradients</li><li>RPNï¼š<ul><li>9 anchors  </li><li>sharing feature between FCIS and RPN </li></ul></li></ul><p><img src="/2020/06/22/FCIS/architecture.png" width="70%"></p></li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li><p>metricï¼šmAP</p></li><li><p>FCIS (translation invariant)ï¼š</p><ul><li>set k=1ï¼Œachieve the worst mAP</li><li>indicating the position sensitive score map is vital for this method</li></ul></li><li><p>back</p><ul><li>50-101ï¼šincrease</li><li>101-152ï¼šsaturate</li></ul></li><li><p>tricks</p><p>  <img src="/2020/06/22/FCIS/tricks.png" width="40%"></p></li></ul></li></ol><pre><code>* r</code></pre>]]></content>
      
      
        <tags>
            
            <tag> å®ä¾‹åˆ†å‰²ï¼Œå…¨å·ç§¯ï¼Œå¸¦ä½ç½®ä¿¡æ¯çš„scoremap </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>å¤©æ± è„ŠæŸ±MRI</title>
      <link href="/2020/06/11/%E5%A4%A9%E6%B1%A0%E8%84%8A%E6%9F%B1MRI/"/>
      <url>/2020/06/11/%E5%A4%A9%E6%B1%A0%E8%84%8A%E6%9F%B1MRI/</url>
      <content type="html"><![CDATA[<ol><li><p>MRI</p><p> T1åŠ æƒç›¸ä¸Šé¢å¯†åº¦é«˜çš„éª¨å¤´ä¼šæ¯”è¾ƒäº®(å°±æ˜¯é«˜ä¿¡å·)ï¼Œè¿˜æœ‰è„‚è‚ªå’Œç”²çŠ¶è…ºä¹Ÿæ˜¯é«˜ä¿¡å·ï¼Œæ°´ä»½ä¸€èˆ¬éƒ½æ˜¯æ— ä¿¡å·ï¼Œ<br> T2åŠ æƒç›¸é‡Œæ°´æ˜¯é«˜ä¿¡å·æ‰€ä»¥æ°´æ¯”è¾ƒäº®ï¼Œå› ä¸ºå¾ˆå¤šçš„ç—…å˜æœ‰æ°´è‚¿ï¼Œæ‰€ä»¥T2åŠ æƒç›¸é€šä¿—å¯ä»¥è¯´æ˜¯çœ‹ç—…å˜(æ¯•ç«Ÿæ¯”è¾ƒæ˜æ˜¾)ï¼Œ</p><p> è§†è§‰ç›´è§‚ä¸Šæ¥çœ‹ï¼ŒT1çœ‹è§£å‰–ï¼ŒT2çœ‹ç—…å˜</p><p> â€”â€”æ€ä¹ˆfusionä¸€ä¸ªcaseï¼ˆæ ‡æ³¨åªæœ‰ä¸€å¼ ï¼‰</p></li><li><p>æ•°æ®é›†</p><p> T1ã€T2çŸ¢çŠ¶ä½ï¼ŒT2è½´çŠ¶ä½ï¼Œ</p><p> å…³é”®ç‚¹ï¼šåŸºäºT2çŸ¢çŠ¶ä½çš„ä¸­é—´å¸§ï¼Œ</p><p> æ ‡æ³¨èŒƒå›´ï¼šä»èƒ¸12ï¼ˆT12ï¼‰è…°1ï¼ˆL1ï¼‰é—´çš„æ¤é—´ç›˜å¼€å§‹ï¼Œåˆ°è…°5ï¼ˆL5ï¼‰éª¶1ï¼ˆS1ï¼‰é—´çš„æ¤é—´ç›˜ç»“æŸ</p><p> ç±»åˆ«ï¼šæ¤å—æœ‰ç¼–å·ï¼ˆT12åˆ°L5ï¼‰ï¼Œé—´ç›˜é€šè¿‡ä¸Šä¸‹æ¤å—çš„ç¼–å·è¡¨ç¤ºï¼ˆT12-L1åˆ°L5-S1ï¼‰</p><p> ç—…ç¶ï¼š</p><pre><code> * æ¤å—æœ‰ä¸¤ç±»ï¼šæ­£å¸¸V1å’Œé€€è¡Œæ€§ç—…å˜V2ï¼Œ * æ¤é—´ç›˜æœ‰7ç±»ï¼šæ­£å¸¸V1ï¼Œé€€è¡Œæ€§æ”¹å˜V2ï¼Œå¼¥æ¼«æ€§è†¨å‡ºï¼Œéå¯¹ç§°æ€§è†¨å‡ºï¼Œçªå‡ºï¼Œè„±å‡ºï¼Œç–å‡ºV7</code></pre><p> jsonç»“æ„ï¼š</p><ul><li>uidï¼Œdimï¼Œspacingç­‰ä¸€äº›header info</li><li>annotationï¼š<ul><li>sliceï¼šéš¾é“ä¸æ˜¯T2çŸ¢çŠ¶ä½çš„ä¸­é—´å¸§å—ï¼Ÿ</li><li>pointï¼šå…³é”®ç‚¹åæ ‡ï¼Œç—…ç¶ç±»åˆ«ï¼Œå…³é”®ç‚¹ç±»åˆ«</li></ul></li></ul></li><li><p>è¯„ä¼°æŒ‡æ ‡</p><ul><li><p>distance&lt;8mm</p></li><li><p>TPï¼šå¤šä¸ªå‘½ä¸­å–æœ€è¿‘çš„ï¼Œå…¶ä½™å¿½ç•¥</p></li><li>FPï¼šå‡é˜³æ€§ï¼Œdistanceè¶…å‡ºæ‰€æœ‰gtçš„8mmåœˆåœˆï¼è½è¿›åœˆåœˆä½†æ˜¯ç±»åˆ«é”™äº†</li><li>FNï¼šå‡é˜´æ€§ï¼Œgtç‚¹æ²¡æœ‰è¢«TP</li><li>precisionï¼šTP/(TP+FP)</li><li>recallï¼šTP/(TP+FN)</li><li>AP</li><li>MAP</li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> competition </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>group normalization</title>
      <link href="/2020/06/08/group-normalization/"/>
      <url>/2020/06/08/group-normalization/</url>
      <content type="html"><![CDATA[<h2 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a>Group Normalization</h2><ol><li><p>åŠ¨æœº</p><ul><li>for small batch size</li><li>do normalization in channel groups</li><li>batch-independent</li><li>behaves stably over different batch sizes</li><li>approach BNâ€™s accuracy </li></ul><p><img src="/2020/06/08/group-normalization/error.png" width="40%;"></p></li><li><p>è®ºç‚¹</p><ul><li>BN<ul><li>requires sufficiently large batch size (e.g. 32)</li><li>Mask R-CNN frameworks use a batch size of 1 or 2 images because of higher resolution, where BN is â€œfrozenâ€ by transforming to a linear layer </li><li>synchronized BN ã€BR</li></ul></li><li>LN &amp; IN<ul><li>effective for training sequential models or generative models </li><li>but have limited success in visual recognition </li><li>GNèƒ½è½¬æ¢æˆLNï¼IN</li></ul></li><li>WN<ul><li>normalize the filter weights, instead of operating on features</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>group</p><ul><li>it is not necessary to think of deep neural network features as unstructured vectors <ul><li>ç¬¬ä¸€å±‚å·ç§¯æ ¸é€šå¸¸å­˜åœ¨ä¸€ç»„å¯¹ç§°çš„filterï¼Œè¿™æ ·å°±èƒ½æ•è·åˆ°ç›¸ä¼¼ç‰¹å¾</li><li>è¿™äº›ç‰¹å¾å¯¹åº”çš„channel can be normalized together</li></ul></li></ul></li><li><p>normalization</p><ul><li><p>transform the feature xï¼š$\hat x_i = \frac{1}{\sigma}(x_i-\mu_i)$</p></li><li><p>the mean and the standard deviationï¼š</p><script type="math/tex; mode=display">  \mu_i=\frac{1}{m}\sum_{k\in S_i}x_k\\  \sigma_i=\sqrt {\frac{1}{m}\sum_{k\in S_i}(x_k-\mu_i)^2+\epsilon}</script></li><li><p>the set $S_i$</p><ul><li>BNï¼š<ul><li>$S_i=\{k|k_C = i_C\}$</li><li>pixels sharing the same channel index are normalized together </li><li>for each channel, BN computes Î¼ and Ïƒ along the (N, H, W) axes </li></ul></li><li>LN<ul><li>$S_i=\{k|k_N = i_N\}$</li><li>pixels sharing the same batch index (per sample) are normalized together </li><li>LN computes Î¼ and Ïƒ along the (C,H,W) axes for each sample </li></ul></li><li>IN<ul><li>$S_i=\{k|k_N = i_N, k_C=i_C\}$</li><li>pixels sharing the same batch index and the same channel index are normalized together </li><li>LN computes Î¼ and Ïƒ along the (H,W) axes for each sample </li></ul></li><li>GN<ul><li>$S_i=\{k|k_N = i_N, [\frac{k_C}{C/G}]=[\frac{i_C}{C/G}]\}$</li><li>computes Î¼ and Ïƒ along the (H, W ) axes and along a group of C/G channels </li></ul></li></ul></li><li><p>linear transform  </p><ul><li>to keep representational ability </li><li><strong>per channel</strong></li><li>scale and shiftï¼š$y_i = \gamma \hat x_i + \beta$</li></ul><p><img src="/2020/06/08/group-normalization/gn.png" width="80%;"></p></li></ul></li><li><p>relation</p><ul><li>to LN<ul><li>LN assumes <em>all</em> channels in a layer make â€œsimilar contributionsâ€ </li><li>which is less valid with the presence of convolutions </li><li>GN improved representational power over LN</li></ul></li><li>to IN<ul><li>IN can only rely on the spatial dimension for computing the mean and variance </li><li>it misses the opportunity of exploiting the channel dependence</li><li>ã€QUESTIONã€‘BNä¹Ÿæ²¡è€ƒè™‘é€šé“é—´çš„è”ç³»å•Šï¼Œä½†æ˜¯è®¡ç®—meanå’Œvarianceæ—¶è·¨äº†sample</li></ul></li></ul></li><li><p>implementation</p><ul><li>reshape</li><li>learnable $\gamma \&amp; \beta$</li><li>computable mean &amp; var</li></ul></li></ul><p><img src="/2020/06/08/group-normalization/code.png" width="50%;"></p></li><li><p>å®éªŒ</p><ul><li>GNç›¸æ¯”äºBNï¼Œtraining erroræ›´ä½ï¼Œä½†æ˜¯val errorç•¥é«˜äºBN<ul><li>GN is effective for easing optimization</li><li>loses some regularization ability </li><li>it is possible that GN combined with a suitable regularizer will improve results </li></ul></li><li>é€‰å–ä¸åŒçš„groupæ•°ï¼Œæ‰€æœ‰çš„group&gt;1å‡å¥½äºgroup=1ï¼ˆLNï¼‰</li><li>é€‰å–ä¸åŒçš„channelæ•°ï¼ˆCï¼Gï¼‰ï¼Œæ‰€æœ‰çš„channel&gt;1å‡å¥½äºchannel=1ï¼ˆINï¼‰</li><li>Object Detection  <ul><li>frozenï¼šå› ä¸ºhigher resolutionï¼Œbatch sizeé€šå¸¸è®¾ç½®ä¸º2/GPUï¼Œè¿™æ—¶çš„BN frozenæˆä¸€ä¸ªçº¿æ€§å±‚$y=\gamma(x-\mu)/\sigma+beta$ï¼Œå…¶ä¸­çš„$\mu$å’Œ$sigma$æ˜¯loadäº†pre-trained modelä¸­ä¿å­˜çš„å€¼ï¼Œå¹¶ä¸”frozenæ‰ï¼Œä¸å†æ›´æ–°</li><li>denote as BN*</li><li>replace BN* with GN during fine-tuning </li><li>use a weight decay of 0 for the Î³ and Î² parameters </li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æ­£åˆ™åŒ–</title>
      <link href="/2020/06/02/regularization/"/>
      <url>/2020/06/02/regularization/</url>
      <content type="html"><![CDATA[<h2 id="ç»¼è¿°"><a href="#ç»¼è¿°" class="headerlink" title="ç»¼è¿°"></a>ç»¼è¿°</h2><ol><li><p>æ­£åˆ™</p><ul><li><p>æ­£åˆ™åŒ–æ˜¯ç”¨æ¥è§£å†³ç¥ç»ç½‘ç»œè¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œé€šè¿‡é™ä½æ¨¡å‹çš„å¤æ‚æ€§å’Œçº¦æŸæƒå€¼ï¼Œè¿«ä½¿ç¥ç»ç½‘ç»œå­¦ä¹ å¯æ³›åŒ–çš„ç‰¹å¾</p><ul><li>æ­£åˆ™åŒ–å¯ä»¥å®šä¹‰ä¸ºæˆ‘ä»¬ä¸ºäº†å‡å°‘æ³›åŒ–è¯¯å·®è€Œä¸æ˜¯å‡å°‘è®­ç»ƒè¯¯å·®è€Œå¯¹è®­ç»ƒç®—æ³•æ‰€åšçš„ä»»ä½•æ”¹å˜<ul><li>å¯¹æƒé‡è¿›è¡Œçº¦æŸ</li><li>å¯¹ç›®æ ‡å‡½æ•°æ·»åŠ é¢å¤–é¡¹ï¼ˆé—´æ¥çº¦æŸæƒå€¼ï¼‰ï¼šL1 &amp; L2æ­£åˆ™</li><li>æ•°æ®å¢å¼º</li><li>é™ä½ç½‘ç»œå¤æ‚åº¦ï¼šdropoutï¼Œstochastic depth</li><li>early stopping</li></ul></li><li>æˆ‘ä»¬åœ¨å¯¹ç½‘ç»œè¿›è¡Œæ­£åˆ™åŒ–æ—¶ä¸è€ƒè™‘ç½‘ç»œçš„biasï¼šæ­£åˆ™è¡¨è¾¾å¼åªæ˜¯æƒå€¼çš„è¡¨è¾¾å¼ï¼Œä¸åŒ…å«bias<ul><li>biasæ¯”weightå…·æœ‰æ›´å°‘çš„å‚æ•°é‡</li><li>å¯¹biasè¿›è¡Œæ­£åˆ™åŒ–å¯èƒ½å¼•å…¥å¤ªå¤šçš„æ–¹å·®ï¼Œå¼•å…¥å¤§é‡çš„æ¬ æ‹Ÿåˆ</li></ul></li></ul></li><li><p>L1 &amp; L2ï¼š</p><ul><li><p>è¦æƒ©ç½šçš„æ˜¯ç¥ç»ç½‘ç»œä¸­æ¯ä¸ªç¥ç»å…ƒçš„æƒé‡å¤§å°</p></li><li><p>L2å…³æ³¨çš„æ˜¯æƒé‡çš„å¹³æ–¹å’Œï¼Œæ˜¯è¦ç½‘ç»œä¸­çš„æƒé‡æ¥è¿‘0ä½†ä¸ç­‰äº0ï¼Œâ€œæƒé‡è¡°å‡â€</p><script type="math/tex; mode=display">  \frac{d}{dW}(\frac{\lambda}{2m}W^2) = \frac{\lambda}{m} W</script></li><li><p>L1å…³æ³¨çš„æ˜¯æƒé‡çš„ç»å¯¹å€¼ï¼Œæƒé‡å¯èƒ½è¢«å‹ç¼©æˆ0ï¼Œæƒé‡æ›´æ–°æ—¶æ¯æ¬¡å‡å»çš„æ˜¯ä¸€ä¸ªå¸¸é‡</p><script type="math/tex; mode=display">  \frac{d}{dW}(\frac{\lambda}{m}W) = \frac{\lambda}{m} sgn(W)</script></li><li><p>L1ä¼šè¶‹å‘äºäº§ç”Ÿå°‘é‡çš„ç‰¹å¾ï¼Œè€Œå…¶ä»–çš„ç‰¹å¾éƒ½æ˜¯0ï¼Œè€ŒL2ä¼šé€‰æ‹©æ›´å¤šçš„ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾éƒ½ä¼šæ¥è¿‘äº0</p></li></ul></li><li><p>dropout</p><ul><li>æ¯ä¸ªepochè®­ç»ƒçš„æ¨¡å‹éƒ½æ˜¯éšæœºçš„</li><li>åœ¨testçš„æ—¶å€™ç›¸å½“äºensembleå¤šä¸ªæ¨¡å‹</li></ul></li><li><p>æƒé‡å…±äº«</p></li><li><p>æ•°æ®å¢å¼º</p></li><li><p>éšå¼æ­£åˆ™åŒ–ï¼šå…¶å‡ºç°çš„ç›®çš„ä¸æ˜¯ä¸ºäº†æ­£åˆ™åŒ–ï¼Œè€Œæ­£åˆ™åŒ–çš„æ•ˆæœæ˜¯å…¶å‰¯äº§å“ï¼ŒåŒ…æ‹¬early stoppingï¼ŒBNï¼Œéšæœºæ¢¯åº¦ä¸‹é™</p></li></ul></li><li><p>dropout &amp; drop connectï¼ˆ[Reference][<a href="https://zhuanlan.zhihu.com/p/108024434]ï¼‰" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/108024434]ï¼‰</a></p><ul><li><p>dropoutï¼š</p><ul><li>2012å¹´Hintonæå‡ºï¼Œåœ¨æ¨¡å‹è®­ç»ƒæ—¶ä»¥æ¦‚ç‡péšæœºè®©éšå±‚èŠ‚ç‚¹çš„è¾“å‡ºå˜æˆ0ï¼Œæš‚æ—¶è®¤ä¸ºè¿™äº›èŠ‚ç‚¹ä¸æ˜¯ç½‘ç»œç»“æ„çš„ä¸€éƒ¨åˆ†ï¼Œä½†æ˜¯ä¼šæŠŠå®ƒä»¬çš„æƒé‡ä¿ç•™ä¸‹æ¥ï¼ˆä¸æ›´æ–°ï¼‰ã€‚</li><li><p>æ ‡å‡†dropoutç›¸å½“äºåœ¨ä¸€å±‚ç¥ç»å…ƒä¹‹åå†æ·»åŠ ä¸€ä¸ªé¢å¤–çš„å±‚ï¼Œè¿™äº›ç¥ç»å…ƒåœ¨è®­ç»ƒæœŸé—´ä»¥ä¸€å®šçš„æ¦‚ç‡å°†å€¼è®¾ç½®ä¸ºé›¶ï¼Œå¹¶åœ¨æµ‹è¯•æœŸé—´å°†å®ƒä»¬ä¹˜ä»¥pã€‚ </p><p><img src="/2020/06/02/regularization/dropout.jpg" width="70%"></p></li></ul></li><li><p>drop connectï¼š</p><ul><li>ä¸æ˜¯éšæœºçš„å°†éšå±‚èŠ‚ç‚¹çš„è¾“å‡ºå˜æˆ0ï¼Œè€Œæ˜¯å°†èŠ‚ç‚¹ä¸­çš„æ¯ä¸ªä¸å…¶ç›¸è¿çš„è¾“å…¥æƒå€¼ä»¥1-pçš„æ¦‚ç‡å˜æˆ0ã€‚ï¼ˆä¸€ä¸ªæ˜¯è¾“å‡ºä¸€ä¸ªæ˜¯è¾“å…¥ï¼‰</li><li>è®­ç»ƒé˜¶æ®µï¼Œå¯¹æ¯ä¸ªexampleï¼mini-batch, æ¯ä¸ªepochéƒ½éšæœºsampleä¸€ä¸ªmaskçŸ©é˜µ</li><li><p>Dropconnectåœ¨æµ‹è¯•æœŸé—´é‡‡ç”¨äº†ä¸æ ‡å‡†dropoutä¸åŒçš„æ–¹æ³•ã€‚ä½œè€…æå‡ºäº†dropconnectåœ¨æ¯ä¸ªç¥ç»å…ƒå¤„çš„é«˜æ–¯è¿‘ä¼¼ï¼Œç„¶åä»è¿™ä¸ªé«˜æ–¯å‡½æ•°ä¸­æŠ½å–ä¸€ä¸ªæ ·æœ¬å¹¶ä¼ é€’ç»™ç¥ç»å…ƒæ¿€æ´»å‡½æ•°ã€‚è¿™ä½¿å¾—dropconnectåœ¨æµ‹è¯•æ—¶å’Œè®­ç»ƒæ—¶éƒ½æ˜¯ä¸€ç§éšæœºæ–¹æ³•ã€‚</p><p><img src="/2020/06/02/regularization/dropconnect.png" width="40%"></p><p><img src="/2020/06/02/regularization/dropCinfer.jpg" width="80%"></p></li><li><p>ä¼¯åŠªåˆ©åˆ†å¸ƒï¼š0-1åˆ†å¸ƒ</p></li></ul></li></ul></li><li><p>dropout &amp; drop connect é€šå¸¸åªä½œç”¨äºå…¨è¿æ¥å±‚ä¸Šï¼šè¿™ä¿©æ˜¯ç”¨æ¥é˜²æ­¢è¿‡å¤šå‚æ•°å¯¼è‡´è¿‡æ‹Ÿåˆ</p><ul><li><p>å·ç§¯å±‚å‚æ•°è´¼å°‘ï¼Œæ‰€ä»¥æ²¡å¿…è¦ï¼Œ</p></li><li><p>é’ˆå¯¹å·ç§¯é€šé“æœ‰spacial dropoutï¼šæŒ‰ç…§channeléšæœºæ‰”</p></li></ul></li><li><p>dropblockï¼šæ˜¯é’ˆå¯¹å·ç§¯å±‚çš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œç›¸æ¯”è¾ƒäºdropoutçš„random muteï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°removeæ‰éƒ¨åˆ†è¯­ä¹‰ä¿¡æ¯ï¼Œblock size=1çš„æ—¶å€™é€€åŒ–æˆdropout</p><p><img src="/2020/06/02/regularization/dropblock.png" width="60%"></p></li><li><p>papers</p><p>[dropout] Improving neural networks by preventing co-adaptation of feature detectorsï¼Œä¸¢èŠ‚ç‚¹</p><p>[drop connect] Regularization of neural networks using dropconnectï¼Œä¸¢weight path</p><p>[Stochastic Depth] Deep Networks with Stochastic Depthï¼Œä¸¢layer</p><p>[DropBlock] A regularization method for convolutional networks</p></li><li><p>dropå¤§æ³•ä¸€å¥è¯æ±‡æ€»</p><ul><li>dropoutï¼šå„ç»´åº¦å®Œå…¨éšæœºæ‰”</li><li>spacial dropoutï¼šæŒ‰ç…§channeléšæœºæ‰”</li><li>stochastic depthï¼šæŒ‰ç…§res blockéšæœºæ‰”</li><li>dropblockï¼šåœ¨feature mapä¸ŠæŒ‰ç…§spacialå—éšæœºæ‰”</li><li>cutoutï¼šåœ¨input mapä¸ŠæŒ‰ç…§spacialå—éšæœºæ‰”</li><li>dropconnectï¼šæ‰”è¿æ¥ä¸æ‰”ç¥ç»å…ƒ</li></ul></li></ol><h2 id="Deep-Networks-with-Stochastic-Depth"><a href="#Deep-Networks-with-Stochastic-Depth" class="headerlink" title="Deep Networks with Stochastic Depth"></a>Deep Networks with Stochastic Depth</h2><ol><li><p>åŠ¨æœº</p><ul><li>propose a training procedureï¼šstochastic depthï¼Œtrain short and test deep</li><li>for each mini-batch<ul><li>randomly drop a subset of layers  </li><li>and bypass them with the identity function</li></ul></li><li>shortï¼šreduces training time </li><li>regï¼šimproves the test error </li><li>can increase the network depth </li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>deeper</p><ul><li>expressiveness </li><li>vanishing gradients </li><li>diminishing feature reuse  </li></ul></li><li><p>resnet </p><ul><li>skip connection</li><li><p>whenè¾“å…¥è¾“å‡ºchannelæ•°ä¸matchï¼šredefine id(Â·) as a linear projection to reduce the dimensions </p><p><img src="/2020/06/02/regularization/resnet.png" width="60%"></p></li></ul></li><li><p>dropout</p><ul><li>Dropout reduces the effect known as â€œco- adaptationâ€ of hidden nodes </li><li>Dropout loses effectiveness when used in combination with Batch Normalization </li></ul></li><li><p>our approach</p><ul><li>higher diversity </li><li>shorter instead of thinner </li><li>work with Batch Normalization</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>stochastic depth</p><ul><li>randomly dropping entire ResBlocks </li><li>$H_l = ReLU(b_l Res_l(H_{l-1}) + id(H_{l-1}))$</li></ul></li><li><p>survival probabilities </p><ul><li><p>$p_l = Pr(b_l=1)$</p></li><li><p>set uniformly / set following a linear decay rule </p></li><li><p>set $p_0=1, p_L=0.5$ï¼š</p><script type="math/tex; mode=display">  p_l = 1 - \frac{l}{L}(1-p_L)</script></li><li><p>intuitionï¼šthe earlier layers extract low-level features that will be used by later layers and should therefore be more reliably present </p><p><img src="/2020/06/02/regularization/stochastic.png" width="60%"></p></li></ul></li><li><p>Expected network depth </p><ul><li>$E(L) \approx 3L/4$</li><li>approximately 25% of training time could be saved </li></ul></li><li><p>during testing</p><ul><li>all res path are active  </li><li>each res path is weighted by its survival probability</li><li>$H_l^{Test} = ReLU(b_l Res_l(H_{l-1}, W_l) + id(H_{l-1}))$</li><li>è·Ÿdropoutä¸€æ ·</li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> æ­£åˆ™åŒ–ï¼Œdropoutï¼Œdropconnect </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>RetinaNet</title>
      <link href="/2020/05/30/RetinaNet/"/>
      <url>/2020/05/30/RetinaNet/</url>
      <content type="html"><![CDATA[<ol><li>[det] RetinaNet: Focal Loss for Dense Object Detection</li><li>[det+instance seg] RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free </li><li>[det+semantic seg] Retina U-Net: Embarrassingly Simple Exploitation of Segmentation Supervision for Medical Object Detection </li></ol><h2 id="Focal-Loss-for-Dense-Object-Detection"><a href="#Focal-Loss-for-Dense-Object-Detection" class="headerlink" title="Focal Loss for Dense Object Detection"></a>Focal Loss for Dense Object Detection</h2><ol><li><p>åŠ¨æœº</p><ul><li>dense prediction(one-stage detector)</li><li>focal lossï¼šaddress the class imbalance problem</li><li>RetinaNetï¼šdesign and train a simple dense detector </li></ul></li><li><p>è®ºç‚¹</p><ul><li>accuracy trailed<ul><li>two-stageï¼šclassifier is applied to a sparse set of candidate</li><li>one-stageï¼šdense sampling of possible object locations</li><li>the <strong>extreme foreground-background class imbalance</strong> encountered during training of dense detectors is the central cause</li></ul></li><li>loss<ul><li>standard cross entropy lossï¼šdown-weights the loss assigned to well-classified examples</li><li>proposed focal lossï¼šfocuses training on a sparse set of hard examples</li></ul></li><li>R-CNNç³»åˆ—two-stage framework <ul><li>proposal-driven </li><li>the first stage generates a sparse set of candidate object locations</li><li>the second stage classifies each candidate location as one of the foreground classes or as background</li><li>class imbalanceï¼šåœ¨stage1å¤§éƒ¨åˆ†èƒŒæ™¯è¢«filter outäº†ï¼Œstage2è®­ç»ƒçš„æ—¶å€™å¼ºåˆ¶å›ºå®šå‰èƒŒæ™¯æ ·æœ¬æ¯”ä¾‹ï¼Œå†åŠ ä¸Šå›°éš¾æ ·æœ¬æŒ–æ˜OHEM </li><li>fasterï¼šreducing input image resolution and the number of proposals </li><li>ever fasterï¼šone-stage</li></ul></li><li>one-stage detectors <ul><li>One stage detectors are applied over a regular, dense sampling of object locations, scales, and aspect ratios</li><li>denseï¼šregularly sampling(contrast to selection)ï¼ŒåŸºäºgridä»¥åŠanchorä»¥åŠå¤šå°ºåº¦</li><li>the training procedure is still dominated by easily classified background examples</li><li>class imbalanceï¼šé€šå¸¸å¼•å…¥bootstrappingå’Œhard example miningæ¥ä¼˜åŒ–</li></ul></li><li>Object Detectors<ul><li>Classicï¼šsliding-window+classifier based on HOGï¼Œdense predict</li><li>Two-stageï¼šselective Search+classifier based on CNNï¼Œshared network RPN</li><li>One-stageï¼šâ€˜anchorsâ€™ introduced by RPNï¼ŒFPN</li></ul></li><li>loss<ul><li>Huber lossï¼šdown-weighting the loss of outliers (hard examples)</li><li>focal lossï¼šdown-weighting inliers (easy examples)  </li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>focal loss</p><ul><li>CEï¼š$CE(p_t)=-log(p_t)$<ul><li>even examples that are easily classified ($p_t&gt;0.5$) incur a loss with non-trivial magnitude</li><li>summed CE loss over a large number of easy examples can overwhelm the rare class</li></ul></li><li>WCEï¼š$WCE(p_t)=-\alpha_t log(p_t)$<ul><li>balances the importance of positive/negative examples</li><li>does not differentiate between easy/hard examples </li></ul></li><li><p>FLï¼š$FL(p_t)=-\alpha_t(1-p_t)^\gamma log(p_t)$</p><ul><li>as $\gamma$ increases the modulating factor is likewise increased </li><li>$\gamma=2$ works best in our experiments </li></ul><p><img src="/2020/05/30/RetinaNet/loss.png" width="40%;"></p><p>â€‹    </p></li><li><p>two-stage detectorsé€šå¸¸ä¸ä¼šä½¿ç”¨WCEæˆ–FL</p><ul><li>cascade stageä¼šè¿‡æ»¤æ‰å¤§éƒ¨åˆ†easy negatives </li><li>ç¬¬äºŒé˜¶æ®µè®­ç»ƒä¼šåšbiased minibatch sampling </li><li>Online Hard Example Mining (OHEM)<ul><li>construct minibatches using high-loss examples</li><li>scored by loss + nms</li><li>completely discards easy examples</li></ul></li></ul></li></ul></li><li><p>RetinaNet</p><ul><li>composeï¼šbackbone network + two task-specific subnetworks</li><li>backboneï¼šconvolutional feature map over the entire input image </li><li>subnet1ï¼šobject classification </li><li><p>subnet2ï¼šbounding box regression </p><p><img src="/2020/05/30/RetinaNet/retinanet.png" width="60%;"></p></li><li><p>ResNet-FPN backbone</p><ul><li>rich, multi-scale feature pyramidï¼ŒäºŒé˜¶æ®µçš„RPNä¹Ÿç”¨äº†FPN</li><li>each level can be used for detecting objects at a different scale</li><li>P3 - P7ï¼š8x - 128x downsamp</li><li>FPN channelsï¼š256</li></ul></li><li><p>anchors</p><ul><li>anchor ratiosï¼š{1:2, 1:1, 2:1}ï¼Œé•¿å®½æ¯”</li><li>anchor scalesï¼š{$2^0$, $2^\frac{1}{3}$, $2^\frac{2}{3}$}ï¼Œå¤§å°ï¼ŒåŒä¸€ä¸ªscaleçš„anchorï¼Œé¢ç§¯ç›¸åŒï¼Œéƒ½æ˜¯size*sizeï¼Œé•¿å®½é€šè¿‡ratioæ±‚å¾—</li><li>anchor size per levelï¼š[32, 64, 128, 256, 512]ï¼ŒåŸºæœ¬çš„æ­£æ–¹å½¢anchorçš„è¾¹é•¿</li><li>total anchors per levelï¼šA=9</li><li>KAï¼šeach anchor is assigned a length K one-hot vector of classification targets </li><li>4Aï¼šand a 4-vector of box regression targets </li><li>anchors are assigned to ground-truth object boxes using an intersection-over-union (IoU) threshold of 0.5 </li><li>anchors are <strong>assigned background</strong> if their IoU is in [0, 0.4)</li><li>anchor is unassigned between [0.4, 0.5), which is ignored during training</li><li>each anchor is assigned to at most one object box </li><li><p>for each anchor</p><ul><li><p>classification targetsï¼šone-hot vector </p></li><li><p>box regression targetsï¼šeach anchorå’Œå…¶å¯¹åº”çš„gt boxçš„offset</p></li></ul></li></ul></li><li><p>rpn offsetï¼šä¸­å¿ƒç‚¹ã€å®½ã€é«˜</p><pre><code>  $$</code></pre><p>  t_x = (x - x_a) / w_a\\</p><pre><code>    t_y = (y - y_a) / h_a\\</code></pre><p>  t_w = log(w/ w_a)\\</p><pre><code>    t_h = log(h/ h_a)</code></pre><p>  $$</p></li><li><p>or omitted if there is no assignment</p></li><li><p>ã€QUESTIONã€‘æ‰€è°“çš„anchor state {-1:ignore, 0:negative, 1:positive} æ˜¯é’ˆå¯¹cls lossæ¥è¯´çš„ï¼Œç›¸å½“äºäººä¸ºä¸¢å¼ƒäº†ä¸€éƒ¨åˆ†åå‘ä¸­ç«‹çš„æ ·æœ¬ï¼Œè¿™å¯¹åˆ†ç±»æ•ˆæœæœ‰æå‡å—ï¼Ÿï¼Ÿ</p></li><li><p>classification subnet</p><ul><li><p>for each spatial positionï¼Œfor each anchorï¼Œpredict one among K classesï¼Œone-hot</p></li><li><p>inputï¼šC channels feature map from FPN</p></li><li><p>structureï¼šfour 3x3 conv + ReLUï¼Œeach with C filters </p></li><li><p>headï¼š3x3 conv + sigmoidï¼Œwith KA filters</p></li><li><p>share across levels</p></li></ul></li></ul></li><li><p>not share with box regression subnet </p></li><li><p>focal lossï¼š</p><ul><li>sum over all ï½100k anchors<pre><code>  * and normalized by the number of anchors assigned to a ground-truth box  * å› ä¸ºæ˜¯sumï¼Œæ‰€ä»¥è¦normailizeï¼Œnormé¡¹ç”¨çš„æ˜¯number of assigned anchorsï¼ˆè¿™æ˜¯åŒ…æ‹¬äº†å‰èƒŒæ™¯ï¼Ÿï¼‰  * vast majority of anchors are **easy negatives** and receive negligible loss values under the focal lossï¼ˆç¡®å®åŒ…å«èƒŒæ™¯æ¡†ï¼‰  * $\alpha$ï¼šIn general $alpha$ should be decreased slightly as $\gamma$ is increased </code></pre></li><li><p>strong effect on negativesï¼šFL can effectively discount the effect of easy negatives, focusing all attention on the hard negative examples </p><p><img src="/2020/05/30/RetinaNet/bg.png" width="80%;"></p></li><li><p>box regression subnet </p><ul><li>class-agnostic bounding box regressor  </li></ul></li></ul></li><li>same structureï¼šfour 3x3 conv + ReLUï¼Œeach with C filters <pre><code>  * headï¼š4A linear outputs   * L1 loss</code></pre></li></ul></li></ol><ul><li><p>inference</p><ul><li><p>keep top 1k predictions per FPN level </p><pre><code>  * all levels are merged and non-maximum suppression with a threshold of 0.5 </code></pre><ul><li><p>train</p><ul><li>initializationï¼š<ul><li>cls head bias initializationï¼Œencourage more foreground prediction at the start of training </li><li>prevents the large number of background anchors from generating a large, destabilizing loss</li></ul></li></ul></li></ul></li></ul></li><li><p>network design</p><ul><li><p>anchors</p><pre><code>      * one-stage detecors use fixed sampling grid to generate position      * use multiple â€˜anchorsâ€™ at each spatial position to cover boxes of various scales and aspect ratios       * beyond 6-9 anchors did not shown further gains in AP  * speed/accuracy trade-off        * outperforms all previous methods      * bigger resolution bigger AP      * Retina-101-600ä¸ResNet101-FRCNNçš„APæŒå¹³ï¼Œä½†æ˜¯æ¯”ä»–å¿«</code></pre><ul><li><p>gradientï¼š</p><ul><li>æ¢¯åº¦æœ‰ç•Œ</li></ul></li></ul></li></ul></li><li><p>the derivative is small as soon as $x_t &gt; 0$</p><pre><code>      &lt;img src=&quot;RetinaNet/gradient.png&quot; width=&quot;70%;&quot; /&gt;</code></pre></li></ul><pre><code>â€‹        </code></pre><h2 id="RetinaMask-Learning-to-predict-masks-improves-state-of-the-art-single-shot-detection-for-free"><a href="#RetinaMask-Learning-to-predict-masks-improves-state-of-the-art-single-shot-detection-for-free" class="headerlink" title="RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free"></a>RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</h2><ol><li><p>åŠ¨æœº</p><ul><li>improve single-shot detectors to the same level as current two-stage techniques</li><li>improve on RetinaNet<ul><li>integrating instance mask prediction</li><li>adaptive loss </li><li>additional hard examples</li><li>Group Normalization</li></ul></li><li><p>same computational cost as the original RetinaNet but more accurateï¼šåŒæ ·çš„å‚æ•°é‡çº§æ¯”orgin RetinaNetå‡†ï¼Œæ•´ä½“çš„å‚æ•°é‡çº§å¤§äºyolov3ï¼Œaccå¿«è¦æ¥è¿‘äºŒé˜¶æ®µçš„mask RCNNäº†</p><p><img src="/2020/05/30/RetinaNet/acc:time.png" width="40%;"></p></li></ul></li></ol><ol><li><p>è®ºç‚¹</p><ul><li>part of improvements of two-stage detectors is due to architectures like Mask R-CNN that involves multiple prediction heads</li><li>additional segmentation task had only been added to two-stage detectors in the past</li><li>two-stage detectors have the cost of resampling(ROI-Align) issueï¼šRPNä¹‹åè¦ç‰¹å¾å¯¹é½</li><li>add addtional heads in training keeps the structure of the detector at test time unchanged </li><li>potential improvement directions<ul><li>dataï¼šOHEM</li><li>contextï¼šFPN</li><li>additional taskï¼šsegmentation branch</li></ul></li><li>this paperâ€™s contribution<ul><li>add a mask prediction branch</li><li>propose a new self-adjusting loss function</li><li>include more of positive samplesâ€”&gt;those with low overlap</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>best matching policy</p><ul><li>speical caseï¼šoutlier gt boxï¼Œè·Ÿæ‰€æœ‰çš„anchor iouéƒ½ä¸å¤§äº0.5ï¼Œæ°¸è¿œä¸ä¼šè¢«å½“ä½œæ­£æ ·æœ¬</li><li>use best matching anchor with any nonzero overlap to replace the threshold</li></ul></li><li><p>self-adjusting Smooth L1 loss</p><ul><li><p>bbox regression</p></li><li><p>smooth L1ï¼š</p><ul><li>L1 loss is used beyond $\beta$ to <strong>avoid over-penalizing outliers</strong></li><li><p>the choice of control point is heuristic and is usually done by hyper parameter search </p><script type="math/tex; mode=display">f(x) = \begin{cases}0.5 \frac{x^2}{\beta} \text{,  if  } |x| < \beta \\|x| - 0.5\beta \text{,  otherwise  }\end{cases}</script></li></ul></li><li><p>self-adjusting control point</p><ul><li><p>running mean &amp; variance</p><script type="math/tex; mode=display">  \mu_B = \frac{1}{n}\sum_{i=1}^n |x_i|\\  \sigma_B^2 = \frac{1}{n}\sum_{i=1}^n(|x_i|-\mu_B)^2</script></li><li><p>minibatch updateï¼šm=0.9</p><script type="math/tex; mode=display">  \mu_R = \mu_R * m + \mu_B*(1-m)\\  \sigma_R^2 = \sigma_R^2*m+\sigma_B^2*(1-m)</script></li><li><p>control pointï¼š$[0, \hat \beta]$ clip to avoid unstable </p><script type="math/tex; mode=display">  \beta = max(0, min(\hat \beta, \mu_R-\sigma_R^2))</script></li></ul></li></ul></li><li><p>mask module</p><ul><li><p>detection predictions are treated as mask proposals </p></li><li><p>extract the top N scored predictions </p></li><li><p>distribute the mask proposals to sample features from the appropriate layers </p><script type="math/tex; mode=display">  k = [k_0 + log_2 \sqrt{wh}/224]</script><ul><li>$k_0=4$ï¼Œå¦‚æœsizeå°äº224*224ï¼Œproposalä¼šè¢«åˆ†é…ç»™P3ï¼Œå¦‚æœå¤§äº448*448ï¼Œproposalä¼šè¢«åˆ†é…ç»™P5</li><li>using more feature layers shows no performance boost</li></ul></li></ul></li><li><p>architecture </p><ul><li>r50&amp;r101 backï¼šfreezing all of the Batch Nor- malization layers </li><li>fpn feature channelï¼š256</li><li>classification branch<ul><li>4 conv layersï¼šconv3x3+reluï¼Œchannel256</li><li>headï¼šconv3x3+sigmoidï¼Œchannel n_anchors*n_classes</li></ul></li><li>regression branch<ul><li>4 conv layersï¼šconv3x3+reluï¼Œchannel256</li><li>headï¼šconv3x3ï¼Œchannel n_anchors*4</li></ul></li><li>aggregate the boxes to the FPN layers </li><li>ROI-Align yielding 14x14 resolution features </li><li><p>mask head</p><ul><li>4 conv layersï¼šconv3x3</li><li>a single transposed convolutional layerï¼šconvtranspose2d 2x2ï¼Œto 28*28 resolution</li><li>prediction headï¼šconv1x1</li></ul><p><img src="/2020/05/30/RetinaNet/retinaMask.png" width="70%;"></p></li></ul></li><li><p>training</p><ul><li>min side &amp; max sideï¼š800&amp;1333</li><li>limited GPUï¼šreduce the batch sizeï¼Œincreasing the number of training iterations and reducing the learning rate accordingly</li><li>positive/ignore/negativeï¼š0.5ï¼Œ0.4</li><li><p>focal loss for classification</p><ul><li><p>gaussian initialization</p></li><li><p>$\alpha=0.25, \lambda=2.0$</p></li><li><p>$FL=-\alpha_t(1-p_t)^\lambda log(p_t)$</p><script type="math/tex; mode=display"> FL =  \left\{ \begin{array}{lr} -\alpha (1-p)^{\gamma}log(p),  \  \ y=1\\ -(1-\alpha) p^{\gamma}log(1-p),  \  \ y=0\\ \end{array} \right.</script></li><li><p>gammaé¡¹æ§åˆ¶çš„æ˜¯ç®€å•æ ·æœ¬çš„è¡°å‡é€Ÿåº¦ï¼Œalphaé¡¹æ§åˆ¶çš„æ˜¯æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ï¼Œå¯ä»¥é»˜è®¤å€¼ä¸‹æ­£æ ·æœ¬çš„æƒé‡æ˜¯0.25ï¼Œè´Ÿæ ·æœ¬çš„æƒé‡æ˜¯0.75ï¼Œå’Œæƒ³è±¡ä¸­çš„ç»™æ­£æ ·æœ¬æ›´å¤šæƒé‡ä¸ä¸€æ ·ï¼Œå› ä¸ºalphaå’Œgammaæ˜¯è€¦åˆèµ·æ¥ä½œç”¨çš„ï¼Œï¼ˆå¯èƒ½æ£€æµ‹åœºæ™¯ä¸‹å›°éš¾çš„è´Ÿæ ·æœ¬ç›¸æ¯”äºæ­£æ ·æœ¬æ›´å°‘ï¼ŸèƒŒæ™¯å°±æ˜¯æ¯”å‰æ™¯å¥½å­¦ï¼Ÿä¸ç¡®å®šä¸ç¡®å®šã€‚ã€‚ã€‚ï¼‰</p></li></ul></li><li>self-adjusting L1 loss for box regression<ul><li>limit running paramsï¼š[0, 0.11]</li></ul></li><li>mask loss<ul><li>top-100 predicted boxes + ground truth boxes</li></ul></li></ul></li><li><p>inference</p><ul><li>box confidence threshold 0.05</li><li>nms threshold 0.4</li><li>use top-50 boxes for mask prediction</li></ul></li></ul></li></ol><h2 id="Retina-U-Net-Embarrassingly-Simple-Exploitation-of-Segmentation-Supervision-for-Medical-Object-Detection"><a href="#Retina-U-Net-Embarrassingly-Simple-Exploitation-of-Segmentation-Supervision-for-Medical-Object-Detection" class="headerlink" title="Retina U-Net: Embarrassingly Simple Exploitation of Segmentation Supervision for Medical Object Detection"></a>Retina U-Net: Embarrassingly Simple Exploitation of Segmentation Supervision for Medical Object Detection</h2><ol><li><p>åŠ¨æœº</p><ul><li>localization<ul><li>pixel-level predict</li><li>ad-hoc heuristics when mapping back to object-level scores</li></ul></li><li>semantic segmentation<ul><li>auxiliary task</li><li>overall one-stage </li><li>leveraging available supervision signals</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>monitoring pixel-wise predictions are clinically required </li><li>medical annotations is commonly performed in pixel- wise  </li><li>full semantic supervision <ul><li>fully exploiting the available semantic segmentation signal results in significant performance gains  </li></ul></li><li>one-stage<ul><li>explicit scale variance enforced by the resampling operation in two-stage detectors is not helpful in the medical domain </li></ul></li><li>two-stage methods<ul><li>predict proposal-based segmentations </li><li>mask loss is only evaluated on cropped proposalï¼šno context gradients</li><li>ROI-Alignï¼šnot suggested in medical image</li><li>depends on the results of region proposalï¼šserial vs parallel</li><li>gradients of the mask loss do not flow through the entire model</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>model</p><ul><li>backï¼š<ul><li>ResNet50 </li></ul></li><li>fpnï¼š<ul><li>shift p3-p6 to p2-p5</li><li>change sigmoid to softmax</li><li>3d head channelsï¼š64</li><li>anchor sizeï¼š$\{P_2: 4^2, P_3: 8^2,, P_4: 16^2,, P_5: 32^2\}$</li><li>3d z-scaleï¼š{1ï¼Œ2ï¼Œ4ï¼Œ8}ï¼Œè€ƒè™‘åˆ°zæ–¹å‘çš„low resolution</li></ul></li><li>segmentation supervision<ul><li>p0 &amp; p1</li><li>with skip connections </li><li>without detection heads</li><li>segmentation loss calculates on p0 logits</li><li>dice + ce</li></ul></li><li><p>h</p><p><img src="/2020/05/30/RetinaNet/retinaUnet.png" width="60%;"></p></li></ul></li><li><p>weighted box clustering</p><ul><li><p>patch crop</p></li><li><p>tiling strategies &amp; model ensembling causes multi predictions per location </p></li><li><p>nmsé€‰äº†ä¸€ç±»ä¸­scoreæœ€å¤§çš„boxï¼Œç„¶åæŠ‘åˆ¶æ‰€æœ‰ä¸å®ƒåŒç±»çš„IoUå¤§äºä¸€å®šé˜ˆå€¼çš„box</p></li><li><p>weighted boxä½œç”¨äºè¿™ä¸€ç±»æ‰€æœ‰çš„boxï¼Œè®¡ç®—ä¸€ä¸ªèåˆçš„ç»“æœ</p><ul><li>coordinates confidenceï¼š$o_c = \frac{\sum c_i s_i w_i}{\sum s_i w_i}$</li><li><p>score confidenceï¼š$o_s = \frac{\sum s_i w_i}{\sum w_i + n_{missing * \overline w}}$</p></li><li><p>$w_i$ï¼š$w=f a p$</p><ul><li>overlap factor fï¼šä¸highest scoring boxçš„overlap</li><li>area factor aï¼šhigher weights to larger boxesï¼Œç»éªŒ</li><li>patch center factor pï¼šç›¸å¯¹äºpatch centerçš„æ­£æ€åˆ†å¸ƒ</li></ul></li><li>score confidenceçš„åˆ†æ¯ä¸Šæœ‰ä¸€ä¸ªdown-weighté¡¹$n_{missing}$ï¼šåŸºäºprior knowledgeé¢„æœŸpredictionçš„æ€»æ•°å¾—åˆ°</li></ul></li><li><p>è®ºæ–‡ç»™çš„ä¾‹å­è®©æˆ‘æ„Ÿè§‰å¥½æ¯”nmsçš„ç‚¹</p><ul><li>ä¸€ä¸ªclusteré‡Œé¢ä¸€ç±»æœ€ç»ˆå°±ç•™ä¸‹ä¸€ä¸ªæ¡†ï¼šè§£å†³nmsä¸€ç±»å¤§æ¡†åŒ…å°æ¡†çš„æƒ…å†µ</li><li>è¿™ä¸ªlocationä¸Špredictionæ˜æ˜¾å°‘äºprior knowledgeçš„ç±»åˆ«confidenceä¼šè¢«æ˜¾è‘—æ‹‰ä½ï¼šè§£å†³ä¸€ä¸ªä½ç½®å‡ºç°å¤§æ¦‚ç‡å‡é˜³æ¡†çš„æƒ…å†µ</li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ç›®æ ‡æ£€æµ‹ï¼Œfocallossï¼Œå®ä¾‹åˆ†å‰²ï¼Œè‡ªé€‚åº”smoothL1 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DCGAN</title>
      <link href="/2020/05/27/DCGAN/"/>
      <url>/2020/05/27/DCGAN/</url>
      <content type="html"><![CDATA[<h2 id="UNSUPERVISED-REPRESENTATION-LEARNING-WITH-DEEP-CONVOLUTIONAL-GENERATIVE-ADVERSARIAL-NETWORKS"><a href="#UNSUPERVISED-REPRESENTATION-LEARNING-WITH-DEEP-CONVOLUTIONAL-GENERATIVE-ADVERSARIAL-NETWORKS" class="headerlink" title="UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS"></a>UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS</h2><ol><li><p>åŠ¨æœº</p><ul><li>unsupervised learning</li><li>learns a hierarchy of representations from object parts to scenes </li><li>used for novel tasks </li></ul></li><li><p>è®ºç‚¹</p><ul><li>GAN<ul><li>Learning reusable feature representations from large unlabeled datasets </li><li>generator and discriminator networks can be later used as feature extractors for supervised tasks </li><li>unstable to train </li></ul></li><li>we<ul><li>propose a set of constraints on the architectural topology making it stable to train</li><li>use the trained discriminators for image classification tasks </li><li>visualize the filters </li><li>show that the generators have interesting vector arithmetic properties  </li></ul></li><li>unsupervised representation learning <ul><li>clustering, hierarchical clustering </li><li>auto-encoders learn good feature representations </li></ul></li><li>generative image models <ul><li>samples often suffer from being blurry, being noisy and incomprehensible </li><li>further use for supervised tasks</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>architecture </p><ul><li>all convolutional netï¼šæ²¡æœ‰æ± åŒ–ï¼Œç”¨stride conv</li><li>eliminating fully connected layersï¼š<ul><li>generatorï¼šè¾“å…¥æ˜¯ä¸€ä¸ªå‘é‡ï¼Œreshapeä»¥åæ¥çš„å…¨æ˜¯å·ç§¯å±‚</li><li>discriminatorï¼šæœ€åä¸€å±‚å·ç§¯å‡ºæ¥ç›´æ¥flatten</li></ul></li><li>Batch Normalization <ul><li>generatorè¾“å‡ºå±‚ &amp; discriminatorè¾“å…¥å±‚ä¸åŠ </li><li>resulted in sample oscillation and model instability </li></ul></li><li><p>ReLU</p><ul><li>generatorè¾“å‡ºå±‚ç”¨Tanh </li><li>discriminatorç”¨leakyReLU</li></ul><p><img src="/2020/05/27/DCGAN/gen.png" width="60%;"></p></li></ul></li><li><p>train</p><ul><li>image preprocessï¼šrescale to [-1,1]</li><li>LeakyReLU(0.2)</li><li>lrï¼š2e-4</li><li>momentum term $\beta 1$ï¼š0.5, default 0.9</li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li>evaluate<ul><li>apply them as a feature extractor on supervised datasets</li><li>evaluate the performance of linear models on top of these features </li></ul></li><li>model<ul><li>use the discriminatorâ€™s convolutional features from all layers</li><li>maxpooling to 4x4 grids</li><li>flattened and concatenated to form a 28672 dimensional vector  </li><li>regularized linear L2-SVM </li><li>ç›¸æ¯”ä¹‹ä¸‹ï¼šthe discriminator has many less feature maps, but larger total feature vector size </li></ul></li><li>visualizing<ul><li>walking in the latent space <ul><li>åœ¨vector Zä¸Šå·®å€¼ï¼Œç”Ÿæˆå›¾åƒå¯ä»¥è§‚å¯Ÿåˆ°smooth transitions </li></ul></li><li>visualize the discriminator feature<ul><li>ç‰¹å¾å›¾å¯è§†åŒ–ï¼Œèƒ½è§‚å¯Ÿåˆ°åºŠç»“æ„</li></ul></li><li>manipulate the generator representation<ul><li>generator learns <strong>specific object representations</strong> for major scene components  </li><li>use logistic regression to find feature maps related with window, drop the spatial locations on feature-maps</li><li>most result forgets to draw windows in the bedrooms, replacing them with other objects</li></ul></li></ul></li><li>vector arithmetic <ul><li>averaging the Z vector for three examplars </li><li>semantically obeyed the arithmetic </li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>densenet</title>
      <link href="/2020/05/27/densenet/"/>
      <url>/2020/05/27/densenet/</url>
      <content type="html"><![CDATA[<ol><li><p>åŠ¨æœº</p><ul><li>embrace shorter connections</li><li>the feature-maps of all preceding layers are used as inputs</li><li>advantages<ul><li>alleviate vanishing-gradient</li><li>encourage feature reuse</li><li>reduce the number of parameters</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>Dense<ul><li>each layer obtains additional inputs from all preceding lay- ers and passes on its own feature-maps to all subsequent layers</li><li>feature reuse</li><li>combine features by concatenatingï¼šthe summation in ResNet may impede the information flow in the network</li></ul></li><li>information preservation<ul><li>id shortcut/additive identity transformations</li></ul></li><li><p>fewer params</p><ul><li>DenseNet layers are very narrow </li><li>add only a small set of feature-maps to the â€œcollective knowledgeâ€</li></ul></li><li><p>gradients flow</p><ul><li>each layer has direct access to the gradients from the loss function  </li><li>have regularizing effect </li></ul><p><img src="/2020/05/27/densenet/densenet.png" width="45%;"></p></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>architecture</p><p>  <img src="/2020/05/27/densenet/block.png" width="70%;"></p><ul><li>dense blocks<ul><li>concat</li><li>BN-ReLU-3x3 conv</li><li>$x_l = H_l([x_0, x_1, â€¦, x_{l-1}])$</li></ul></li><li>transition layers<ul><li>change the size of feature-maps</li><li>BN-1x1 conv-2x2 avg pooling</li></ul></li><li><p>growth rate k</p><ul><li>$H_l$ produces feature- maps</li><li>narrowï¼š<em>e.g.</em>, k = 12 </li><li>One can view the feature-maps as the global state of the network </li><li>The growth rate regulates <strong>how much new information</strong> each layer contributes to the global state</li></ul><p><img src="/2020/05/27/densenet/architectures.png" width="80%;"></p></li><li><p>bottleneck â€”- DenseNet-B </p><ul><li>in dense block stage</li><li>1x1 conv reduce dimension first</li><li>number of channelsï¼š4k </li></ul></li><li><p>compression â€”- DenseNet-C</p><ul><li>in transition stage</li><li>reduce the number of feature-maps </li><li>number of channelsï¼š$\theta k$</li></ul></li><li><p>structure configurations </p><ul><li>1st conv channelsï¼šç¬¬ä¸€å±‚å·ç§¯é€šé“æ•°</li><li>number of dense blocks</li><li>Lï¼šdense blocké‡Œé¢çš„layeræ•°</li><li>kï¼šgrowth rate</li><li>Bï¼šbottleneck 4k</li><li>Cï¼šcompression 0.5k</li></ul></li></ul></li></ul></li><li><p>è®¨è®º</p><ul><li><p>concat replace sumï¼š</p><ul><li>seemingly small modification lead to substantially different behaviors of the two network architectures </li><li>feature reuseï¼šfeature can be accessed anywhere</li><li>parameter efficientï¼šåŒæ ·å‚æ•°é‡ï¼Œtest accæ›´é«˜ï¼ŒåŒæ ·accï¼Œå‚æ•°é‡æ›´å°‘</li><li><p>deep supervisionï¼šclassifiers attached to every hidden layer</p></li><li><p>weight assign</p><ul><li>All layers spread their weights over multi inputs (include transition layers)</li><li>least weight are assigned to the transition layer, indicating that transition layers contain many redundant features, <strong>thus can be compressed</strong></li><li>overall there seems to be concentration towards final feature-maps, suggesting that more high-level features are produced late in the network</li></ul><p><img src="/2020/05/27/densenet/weight.png" width="50%;"></p></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>GANomaly</title>
      <link href="/2020/05/25/GANomaly/"/>
      <url>/2020/05/25/GANomaly/</url>
      <content type="html"><![CDATA[<h2 id="GANomaly-Semi-Supervised-Anomaly-Detection-via-Adversarial-Training"><a href="#GANomaly-Semi-Supervised-Anomaly-Detection-via-Adversarial-Training" class="headerlink" title="GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training"></a>GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training</h2><ol><li><p>åŠ¨æœº</p><ul><li>Anomaly detection <ul><li>highly biased towards one class (normal) </li><li>insufficient sample size of the other class (abnormal)</li></ul></li><li>semi-supervised learning <ul><li>detecting the unknown/unseen anomaly case  </li><li>trained on normal samples </li><li>tested on normal and abnormal samples </li></ul></li><li>encoder-decoder-encoder  <ul><li>minimizing the distance between the images </li><li>and the latent vectors  </li><li>a larger distance metric </li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>supervised approaches heavily depend on large, labeled datasets </li><li>Generative Adversarial Networks (GAN) have emerged as a leading methodology across both unsupervised and semi-supervised problems</li><li>reconstruction-based anomaly techniques<ul><li>Overall prior work strongly supports the hypothesis that the use of autoencoders and GAN </li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li>GAN<ul><li>unsupervised </li><li>to generate realistic images </li><li>compete<ul><li>generator tries to generate an image, decoder- alike network, map input to latent space</li><li>discriminator decides whether the generated image is a real or a fake, classical classification architecture, reading an input image, and determining its validity </li></ul></li></ul></li><li><p>Adversarial Auto-Encoders (AAE) </p><ul><li>encoder + decoder </li><li>reconstruction: maps the input to latent space and remaps back to input data space</li><li>train autoencoders with adversarial setting </li></ul></li><li><p>inverse mapping</p><ul><li>with the additional use of an encoder, a vanilla GAN network is capable of learning inverse mapping </li></ul></li><li><p>model</p><p>  <img src="/2020/05/25/GANomaly/GANomaly.png" width="50%;"></p><ul><li>learns both the normal data distribution and minimizes the output anomaly score </li><li>two encoder, one decoder, a discriminator</li><li>encoder<ul><li>convolutional layers followed by batch-norm and leaky ReLU() activation </li><li>compress to a vector z</li></ul></li><li>decoder<ul><li>convolutional transpose layers, ReLU() activation and batch-norm </li><li>a tanh layer at the end</li></ul></li><li>2nd encoder<ul><li>with the same architectural </li><li>but different parametrization</li></ul></li><li>discriminator <ul><li>DCGAN discriminator </li></ul></li><li>Adversarial Loss <ul><li>ä¸æ˜¯åŸºäºGANçš„traditional 0/1 ouput</li><li>è€Œæ˜¯é€‰äº†ä¸€ä¸ªä¸­é—´å±‚ï¼Œè®¡ç®—realï¼fake(reconstructed)çš„L2 distance</li></ul></li><li>Contextual Loss <ul><li>L1 yields less blurry results than L2</li><li>è®¡ç®—è¾“å…¥å›¾åƒå’Œé‡å»ºå›¾åƒçš„L1 distance</li></ul></li><li>Encoder Loss <ul><li>an additional encoder loss to minimize the distance of the bottleneck features </li><li>è®¡ç®—ä¸¤ä¸ªé«˜ç»´å‘é‡çš„L2 distance</li><li>åœ¨æµ‹è¯•çš„æ—¶å€™ç”¨å®ƒæ¥scoring the abnormality  </li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>resnets</title>
      <link href="/2020/05/23/resnets/"/>
      <url>/2020/05/23/resnets/</url>
      <content type="html"><![CDATA[<h2 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h2><ol><li><p>papers</p><p> [resnet] ResNet: Deep Residual Learning for Image Recognition </p><p> [resnext] ResNext: Aggregated Residual Transformations for Deep Neural Networks</p><p> [resnest] ResNeSt: Split-Attention Networks </p><p> [revisiting resnets] Revisiting ResNets: Improved Training and Scaling Strategies </p></li></ol><h2 id="ResNext-Aggregated-Residual-Transformations-for-Deep-Neural-Networks"><a href="#ResNext-Aggregated-Residual-Transformations-for-Deep-Neural-Networks" class="headerlink" title="ResNext: Aggregated Residual Transformations for Deep Neural Networks"></a>ResNext: Aggregated Residual Transformations for Deep Neural Networks</h2><ol><li><p>åŠ¨æœº</p><ul><li>new network architecture</li><li>new building blocks with the same topology</li><li>propose cardinality<ul><li>increasing cardinality is able to improve classification accuracy</li><li>is more effective than going deeper or wider</li></ul></li><li>classification task</li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>VGG &amp; ResNetsï¼š</p><ul><li>stacking building blocks of the same topology</li><li>deeper</li><li>reduces the free choices of hyper-parameters </li></ul></li><li><p>Inception models </p><ul><li>split-transform-merge strategy</li><li>splitï¼š1x1conv spliting into a few lower-dimensional embeddings </li><li>transformï¼ša set of specialized filters  </li><li><p>mergeï¼šconcat</p></li><li><p>approach the representational power of large and dense layers, but at a considerably lower computational complexity</p></li><li>modules are customized stage-by-stage</li></ul></li><li><p>our architecture </p><ul><li>adopts VGG/ResNetsâ€™ repeating layers</li><li>adopts Inceptionâ€˜s split-transform-merge strategy </li><li>aggregated by <strong>summation</strong></li><li><p>cardinalityï¼šthe size of the set of transformationsï¼ˆsplit pathæ•°ï¼‰</p><ul><li>å¤šäº†1x1 convçš„è®¡ç®—é‡</li><li>å°‘äº†3x3 convçš„è®¡ç®—é‡</li></ul><p><img src="/2020/05/23/resnets/rxblock.png" width="45%;"></p></li></ul></li></ul></li><li><p>è¦ç´ </p><ul><li>Multi-branch convolutional blocks</li><li>Grouped convolutionsï¼šé€šé“å¯¹é½ï¼Œç¨€ç–è¿æ¥</li><li>Compressing convolutional networks</li><li>Ensembling </li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>architecture</p><ul><li>a template module</li><li>if producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes) </li><li><p>when the spatial map is downsampled by a factor of 2, the width of the blocks is multiplied by a factor of 2</p></li><li><p>grouped convolutionsï¼šç¬¬ä¸€ä¸ª1x1å’Œ3x3convçš„widthè¦æ ¹æ®Cè¿›è¡Œsplit</p><p><img src="/2020/05/23/resnets/structure.png" width="40%;"></p></li><li><p>equivalent blocks</p><ul><li>BN after each conv</li><li>ReLU after each BN except the last of block</li><li>ReLU after add</li><li><p>r</p><p><img src="/2020/05/23/resnets/equiv.png" width="80%;"></p><p><img src="/2020/05/23/resnets/equiv2.png" width="50%;"></p></li></ul></li><li><p>Model Capacity </p><ul><li><p>improve accuracy when maintaining the model complexity and number of parameters </p></li><li><p>adjust the width of bottleneck, the according C to maintain capacityï¼šC=1çš„æ—¶å€™é€€åŒ–æˆResNet block</p><p>  <img src="/2020/05/23/resnets/bottled.png" width="45%;"></p></li></ul></li></ul></li></ul></li></ol><h2 id="ResNeSt-Split-Attention-Networks"><a href="#ResNeSt-Split-Attention-Networks" class="headerlink" title="ResNeSt: Split-Attention Networks"></a>ResNeSt: Split-Attention Networks</h2><ol><li><p>åŠ¨æœº</p><ul><li>propose a modular Split-Attention block </li><li>enables attention across feature-map groups</li><li>preserve the overall ResNet structure for downstream applications such as object detection and semantic segmentation  </li><li>prove improvement on detection &amp; segmentation tasks</li></ul></li><li><p>è®ºç‚¹</p><ul><li>ResNet<ul><li>simple and modular design </li><li>limited receptive-field size and lack of cross-channel interaction</li></ul></li><li>image classification networks have focused more on group or depth-wise convolution <ul><li>do not transfer well to other tasks </li><li>isolated representations cannot capture cross-channel relationships </li></ul></li><li>a versatile backbone <ul><li>improving performance across multiple tasks at the same time</li><li>a network with cross-channel representations is desirable </li><li>a Split-Attention block <ul><li>divides the feature-map into several groups (along the channel dimension)  </li><li>finer-grained subgroups or splits</li><li>weighted combination </li></ul></li></ul></li><li>featuremap attention mechanismï¼šNiNâ€™s 1x1 conv</li><li>Multi-pathï¼šGoogleNet </li><li>channel-attention mechanismï¼šSE-Net</li><li><p>ç»“æ„ä¸Šï¼Œå…¨å±€ä¸Šçœ‹ï¼Œæ¨¡ä»¿ResNextï¼Œå¼•å…¥cardinalityå’Œgroup convï¼Œå±€éƒ¨ä¸Šçœ‹ï¼Œæ¯ä¸ªgroupå†…éƒ¨ç»§ç»­åˆ†ç»„ï¼Œç„¶åæ¨¡ä»¿SK-Netï¼Œèåˆå¤šä¸ªåˆ†æ”¯çš„split-attentionï¼Œå¤§groupä¹‹é—´concatï¼Œè€Œä¸æ˜¯ResNextçš„addï¼Œå†ç»1x1 convè°ƒæ•´ç»´åº¦ï¼Œadd id path</p><p><img src="/2020/05/23/resnets/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/resnets/rSblock.png" width="60%;"></p></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>Split-Attention block </p><ul><li><p>enables feature-map attention across different feature-map groups</p></li><li><p>within a blockï¼šcontrolled by cardinality</p></li><li><p>within a cardinal groupï¼šintroduce a new radix hyperparameter R indicating the number of splits </p></li><li><p>split-attention</p><ul><li>å¤šä¸ªin-group branchçš„inputè¾“å…¥è¿›æ¥</li><li>fusionï¼šå…ˆåšelement-wise summation </li><li>channel-wise global contextual informationï¼šåšglobal average pooling </li><li>é™ç»´ï¼šDense-BN-ReLU</li><li>å„åˆ†æ”¯Dense(the attention weight function)ï¼šå­¦ä¹ å„è‡ªçš„é‡è¦æ€§æƒé‡</li><li>channel-wise soft attentionï¼šå¯¹å…¨éƒ¨çš„denseåšsoftmax</li><li>åŠ æƒï¼šåŸå§‹çš„å„åˆ†æ”¯inputä¸åŠ æƒçš„denseåšä¹˜æ³•</li><li>å’Œï¼šåŠ æƒçš„å„åˆ†æ”¯add</li><li><p>r=1ï¼šé€€åŒ–æˆSE-blockaverage pooling </p><p><img src="/2020/05/23/resnets/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/resnets/Split-Attention.png" width="35%;"></p></li></ul></li><li><p>shortcut connection</p><ul><li>for blocks with a strided convolution or combined convolution-with-pooling can be applied to the id</li></ul></li><li><p>concat</p></li><li><p>average pooling downsampling </p><ul><li>for dense prediction tasksï¼šit becomes essential to preserve spatial information </li><li>former work tend to use strided 3x3 conv</li><li>we use an average pooling layer with 3x3 kernel</li><li>2x2 average pooling applied to strided shortcut connection before 1x1 conv</li></ul></li></ul></li></ul></li></ol><h2 id="Revisiting-ResNets-Improved-Training-and-Scaling-Strategies"><a href="#Revisiting-ResNets-Improved-Training-and-Scaling-Strategies" class="headerlink" title="Revisiting ResNets: Improved Training and Scaling Strategies"></a>Revisiting ResNets: Improved Training and Scaling Strategies</h2><ol><li><p>åŠ¨æœº</p><ul><li><p>disentangle the three aspects</p><ul><li>model architecture</li><li>training methodology</li><li>scaling strategies</li></ul></li><li><p>improve ResNets to SOTA</p><ul><li>design a family of ResNet architectures, ResNet-RS </li><li>use improved training and scaling strategies </li><li>and combine minor architecture changes</li><li>åœ¨ImageNetä¸Šæ‰“è´¥efficientNet</li><li><p>åœ¨åŠç›‘ç£ä¸Šæ‰“è´¥efficientNet-noisystudent</p><p><img src="/2020/05/23/resnets/acc.png" width="45%;"></p></li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>ImageNetä¸Šæ¦œå¤§æ³•<ul><li>Architecture <ul><li>äººå·¥ç³»åˆ—ï¼šAlexNetï¼ŒVGGï¼ŒResNetï¼ŒInceptionï¼ŒResNeXt</li><li>NASç³»åˆ—ï¼šNasNet-Aï¼ŒAmoebaNet-Aï¼ŒEfficientNet</li></ul></li><li>Training and Regularization Methods<ul><li>regularization methods<ul><li>dropoutï¼Œlabel smoothingï¼Œstochastic depthï¼Œdropblockï¼Œdata augmentation</li><li>significantly improve generalization when training more epochs</li></ul></li><li>training <ul><li>learning rate schedules</li></ul></li></ul></li><li>Scaling Strategies<ul><li>model dimensionï¼šwidthï¼Œdepthï¼Œresolution</li><li>efficientNetæå‡ºçš„å‡è¡¡å¢é•¿ï¼Œåœ¨æœ¬æ–‡ä¸­shows sub-optimal for both resnet and efficientNet</li></ul></li><li>Additional Training Data<ul><li>pretraining on larger dataset</li><li>semi-supervised</li></ul></li></ul></li><li>the performance of a vision model <ul><li>architectureï¼šmost research focus on</li><li>training methods and scaling strategyï¼šless publicized but critical</li><li>unfairï¼šä½¿ç”¨modern training methodçš„æ–°æ¶æ„ä¸ä½¿ç”¨dated methodsçš„è€ç½‘ç»œç›´æ¥å¯¹æ¯”</li></ul></li><li>we focus on the impact of training methods and scaling strategies<ul><li>training methodsï¼š<ul><li>We survey the modern training and regularization techniques</li><li>å‘ç°å¼•å…¥å…¶ä»–æ­£åˆ™æ–¹æ³•çš„æ—¶å€™é™ä½ä¸€ç‚¹weight decayæœ‰å¥½å¤„</li></ul></li><li>scaling strategiesï¼š<ul><li>We offer new perspectives and practical advice on scaling</li><li>å¯èƒ½å‡ºç°è¿‡æ‹Ÿåˆçš„æ—¶å€™å°±åŠ depthï¼Œå¦åˆ™å…ˆåŠ å®½</li><li>resolutionæ…¢ç‚¹å¢é•¿ï¼Œmore slowly than prior works  </li><li>ä»accå›¾å¯ä»¥çœ‹åˆ°ï¼šæˆ‘ä»¬çš„scaling strategiesä¸ç½‘ç»œç»“æ„çš„lightweight changeæ­£äº¤ï¼Œæ˜¯additiveçš„</li></ul></li></ul></li><li>re-scaled ResNets, ResNet-RS<ul><li>ä»…improve training &amp; scaling strategyå°±èƒ½å¤§å¹…åº¦æ¶¨ç‚¹</li><li>combine minor architectural changesè¿›ä¸€æ­¥æ¶¨ç‚¹</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>architecture</p><ul><li>use ResNet with two widely used architecture changes</li><li>ResNet-D<ul><li>stemçš„7x7convæ¢æˆ3ä¸ª3x3conv</li><li>stemçš„maxpoolingå»æ‰ï¼Œæ¯ä¸ªstageçš„é¦–ä¸ª3x3convè´Ÿè´£stride2</li><li>residual pathä¸Šå‰ä¸¤ä¸ªå·ç§¯çš„strideäº’æ¢ï¼ˆåœ¨3x3ä¸Šä¸‹é‡‡æ ·ï¼‰</li><li>id pathä¸Šçš„1x1 s2convæ›¿æ¢æˆ2x2 s2çš„avg pooling+1x1conv</li></ul></li><li><p>SE in bottleneck</p><ul><li>use se-ratio of 0.25</li></ul><p><img src="/2020/05/23/resnets/ResNet-RS.png" width="75%;"></p></li></ul></li><li><p>training methods</p><ul><li>match the efficientNet setup<ul><li>train for 350 epochs</li><li>use cosine learning rate instead of exponential decay </li><li>RandAugment instead of AutoAugment </li><li>use Momentum optimizer instead of RMSProp </li></ul></li><li>regularization<ul><li>weight decay</li><li>label smoothing</li><li>dropout</li><li>stochastic depth</li></ul></li><li><p>data augmentation</p><ul><li>we use RandAugment</li><li>EfficientNet use AutoAugment which slightly outperforms RandAugment</li></ul><p><img src="/2020/05/23/resnets/training methods.png" width="75%;"></p></li><li><p>hyperï¼š</p><ul><li>droprate</li><li>increase the regularization as the model size increase to limit overfitting</li><li>label smoothing = 0.1</li><li>weight decay = 4e-5</li><li><p><img src="/2020/05/23/resnets/hyper.png" width="75%;"></p></li></ul></li></ul></li><li><p>improved training methods</p><ul><li><p>additive study</p><ul><li>æ€»ä½“ä¸Šçœ‹éƒ½æ˜¯additiveçš„</li><li>increase training epochsåœ¨æ·»åŠ regularization methodsçš„å‰æä¸‹æ‰ä¸hurtï¼Œå¦åˆ™ä¼šoverfitting</li><li><p>dropoutåœ¨ä¸é™ä½weight decayçš„æƒ…å†µä¸‹ä¼šhurt</p><p><img src="/2020/05/23/resnets/additive.png" width="45%;"></p></li></ul></li><li><p>weight decay</p><ul><li>å°‘é‡/æ²¡æœ‰regularization methodsçš„æƒ…å†µä¸‹ï¼šå¤§weight decayé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œ1e-4</li><li>å¤š/å¼ºregularization methodsçš„æƒ…å†µä¸‹ï¼šé€‚å½“å‡å°weight decayèƒ½æ¶¨ç‚¹ï¼Œ4e-5</li></ul></li></ul></li><li><p>improved scaling strategies</p><ul><li><p>search space</p><ul><li>width multiplierï¼š[0.25, 0.5, 1.0, 1.5, 2.0]</li><li>depthï¼š[26, 50, 101, 200, 300, 350, 400]</li><li>resolutionï¼š[128, 160, 224, 320, 448]</li><li>increase regularization as model size increase</li><li>observe 10/100/350 epoch regime</li></ul></li><li><p>we found that the best scaling strategies depends on training regime</p></li><li><p>strategy1ï¼šscale depth</p><ul><li>Depth scaling outperforms width scaling for longer epoch regimes </li><li>width scaling is preferable for shorter epoch regimes</li><li>scaling widthå¯èƒ½ä¼šå¼•èµ·overfittingï¼Œæœ‰æ—¶å€™ä¼šhurt performance</li><li>depth scalingå¼•å…¥çš„å‚æ•°é‡ä¹Ÿæ¯”widthå°</li></ul></li><li><p>strategy2ï¼šslow resolution scaling</p><ul><li>efficientNets/resNeSt lead to very large images</li><li><p>our experimentsï¼šå¤§å¯ä¸å¿…</p><p><img src="/2020/05/23/resnets/resolution.png" width="45%;"></p></li></ul></li></ul></li></ul></li><li><p>å®éªŒ</p></li></ol>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Receptive Field</title>
      <link href="/2020/05/18/Receptive-Field/"/>
      <url>/2020/05/18/Receptive-Field/</url>
      <content type="html"><![CDATA[<h2 id="ç»¼è¿°"><a href="#ç»¼è¿°" class="headerlink" title="ç»¼è¿°"></a>ç»¼è¿°</h2><ul><li><p>æ„Ÿå—é‡</p><p>  é™¤äº†å·ç§¯å’Œæ± åŒ–ï¼Œå…¶ä»–å±‚å¹¶ä¸å½±å“æ„Ÿå—é‡å¤§å°</p><p>  æ„Ÿå—é‡ä¸å·ç§¯æ ¸å°ºå¯¸kernel_sizeå’Œæ­¥é•¿strideæœ‰å…³</p><p>  é€’å½’è®¡ç®—ï¼š</p><script type="math/tex; mode=display">  N\_RF = kernel\_size + (cur\_RF-1)*stride</script><p>  å…¶ä¸­$cur_RF$æ˜¯å½“å‰å±‚ï¼ˆstart from 1ï¼‰ï¼Œ$kernel_size$ã€$stride$æ˜¯å½“å‰å±‚å‚æ•°ï¼Œ$N_RF$æ˜¯ä¸Šä¸€å±‚çš„æ„Ÿå—é‡ã€‚</p></li></ul><ul><li><p>æ„Ÿå—é‡è®¡ç®—å™¨</p><p>  <a href="https://fomoro.com/research/article/receptive-field-calculator" target="_blank" rel="noopener">https://fomoro.com/research/article/receptive-field-calculator</a></p></li></ul><h2 id="Understanding-the-Effective-Receptive-Field-in-Deep-Convolutional-Neural-Networks"><a href="#Understanding-the-Effective-Receptive-Field-in-Deep-Convolutional-Neural-Networks" class="headerlink" title="Understanding the Effective Receptive Field in Deep Convolutional Neural Networks"></a>Understanding the Effective Receptive Field in Deep Convolutional Neural Networks</h2><ol><li><p>åŠ¨æœº</p><ul><li><p>effective receptive field </p></li><li><p>the effect of nonlinear activations, dropout, sub-sampling and skip connections on it </p></li></ul></li><li><p>è®ºç‚¹</p><ul><li>it is critical for each output pixel to have a big receptive field, such that no important information is left out when making the prediction</li><li>deeper networkï¼šincrease the receptive field size linearly  </li><li>Sub-samplingï¼šincreases the receptive field size multiplicatively</li><li>it is easy to see that pixels at the center of a receptive field have a much larger impact on an outputï¼šå‰å‘ä¼ æ’­çš„æ—¶å€™ï¼Œä¸­é—´ä½ç½®çš„åƒç´ ç‚¹æœ‰æ›´å¤šæ¡pathé€šå‘output</li></ul></li><li><p>æ–¹æ³•çœ‹ä¸æ‡‚ç›´æ¥çœ‹ç»“è®º</p><ul><li><p><strong>dropout</strong> does not change the Gaussian ERF shape </p></li><li><p><strong>Subsampling</strong> and dilated convolutions turn out to be effective ways to increase receptive field size quickly </p></li><li><p><strong>Skip-connections</strong> make ERFs smaller </p></li><li><p>ERFs are Gaussian distributed</p><ul><li>uniformlyå’Œéšæœºåˆå§‹åŒ–éƒ½æ˜¯perfect Gaus- sian shapes </li><li><p>åŠ ä¸Šéçº¿æ€§æ¿€æ´»å‡½æ•°ä»¥åæ˜¯near Gaussian shapes </p><p><img src="/2020/05/18/Receptive-Field/gaussian.png" width="50%;"></p></li><li><p>with different nonlinearities </p><p><img src="/2020/05/18/Receptive-Field/activation.png" width="25%;"></p></li></ul></li><li><p>$\sqrt n$ absolute growth and $1/\sqrt n$ relative shrinkageï¼šRFæ˜¯éšç€layerçº¿æ€§å¢é•¿çš„ï¼ŒERFåœ¨logä¸Š0.56çš„æ–œç‡ï¼Œçº¦ç­‰äº$\sqrt n$ </p><p>  <img src="/2020/05/18/Receptive-Field/growth.png" width="50%;"></p></li><li><p>Subsampling &amp; dilated convolution increases receptive field</p><ul><li>The reference baseline is a convnet with 15 dense convolution layers </li><li>Subsamplingï¼šreplace 3 of the 15 convolutional layers with stride-2 convolution </li><li><p>dilatedï¼šreplace them with dilated convolution with factor 2,4 and 8ï¼Œrectangular ERF shape </p><p><img src="/2020/05/18/Receptive-Field/subsamp.png" width="25%;"></p></li></ul></li><li><p>evolves during training </p><ul><li>as the networks learns, the ERF gets bigger, and at the end of training is significantly larger than the initial ERF</li><li>classification<ul><li>32*32 cifar 10</li><li>theoretical receptive field of our network is actually 74 Ã— 74 </li></ul></li><li>segmentation<ul><li>CamVid dataset  </li><li>the theoretical receptive field of the top convolutional layer units is quite big at 505 Ã— 505</li></ul></li><li><p>å®é™…çš„ERFéƒ½å¾ˆå°ï¼Œéƒ½æ²¡åˆ°åŸå›¾å¤§å°</p><p><img src="/2020/05/18/Receptive-Field/evolve.png" width="50%;"></p></li></ul></li><li><p>increase the effective receptive field  </p><ul><li>New Initializationï¼š<ul><li>makes the weights at the center of the convolution kernel to have a smaller scale, and the weights on the outside to be larger</li><li>30% speed-up of training </li><li>å…¶ä»–æ•ˆæœä¸æ˜æ˜¾</li></ul></li><li>Architecturalchanges<ul><li>sparsely connect each unit to a larger area </li><li>dilated convolution or even not grid-like </li></ul></li></ul></li></ul></li></ol><p>g</p>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SqueezeNet</title>
      <link href="/2020/05/18/SqueezeNet/"/>
      <url>/2020/05/18/SqueezeNet/</url>
      <content type="html"><![CDATA[<h2 id="SQUEEZENET-ALEXNET-LEVEL-ACCURACY-WITH-50X-FEWER-PARAMETERS-AND-lt-0-5MB-MODEL-SIZE"><a href="#SQUEEZENET-ALEXNET-LEVEL-ACCURACY-WITH-50X-FEWER-PARAMETERS-AND-lt-0-5MB-MODEL-SIZE" class="headerlink" title="SQUEEZENET: ALEXNET-LEVEL ACCURACY WITH 50X FEWER PARAMETERS AND &lt;0.5MB MODEL SIZE"></a>SQUEEZENET: ALEXNET-LEVEL ACCURACY WITH 50X FEWER PARAMETERS AND &lt;0.5MB MODEL SIZE</h2><ol><li><p>åŠ¨æœº</p><ul><li>Smaller CNN</li><li>achieve AlexNet-level accuracy</li><li>model compression </li></ul></li><li><p>è®ºç‚¹</p><ul><li>model compression<ul><li>SVD</li><li>sparse matrix </li><li>quantization (to 8 bits or less)  </li></ul></li><li>CNN microarchitecture<ul><li>extensively 3x3 filters</li><li>1x1 filters  </li><li>higher level building blocks</li><li>bypass connections </li><li>automated designing approaches</li></ul></li><li>this paper eschew automated approaches  </li><li>propose and evaluate the SqueezeNet architecture with and without model compression</li><li>explore the impact of design choices </li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>architectural design strategy</p><ul><li>Replace 3x3 filters with 1x1 filters</li><li>Decrease the number of input channels to 3x3 filters ï¼ˆsqueezeï¼‰</li><li>Downsample late in the network so that convolution layers have large activation mapsï¼šlarge activation maps (due to delayed downsampling) can lead to higher classification accuracy </li></ul></li><li><p>the fire module</p><ul><li>squeezeï¼š1x1 convs</li><li>expandï¼šmix of 1x1 and 3x3 convs, same padding</li><li>relu</li><li><p>concatenate</p><p><img src="/2020/05/18/SqueezeNet/fire.png" width="50%"></p></li></ul></li><li><p>the SqueezeNet</p><ul><li>a standalone convolution layer (conv1)</li><li>followed by 8 Fire modules (fire2-9)</li><li>ending with a final conv layer (conv10)</li><li>stride2 max-pooling after layers conv1, fire4, fire8, and conv10</li><li>dropout with a ratio of 50% is applied after the fire9 module</li><li><p>GAP</p><p><img src="/2020/05/18/SqueezeNet/squeezenet.png" width="60%"></p></li></ul><p><img src="/2020/05/18/SqueezeNet/dimension.png" width="60%"></p></li></ul></li><li><p>understand the impact</p><ul><li><p>each Fire module has three dimensional hyperparameters, to simplifyï¼š</p><ul><li>define $base_e$ï¼šthe number of <em>expand</em> filters in the first Fire module </li><li>for layer iï¼š$e_i=base_e + (incr_e*[\frac{i}{freq}])$</li><li>expand ratio $pct_{3x3}$ï¼šthe percentage of 3x3 filters in expand layers</li><li>squeeze ratio $SR$ï¼šthe number of filters in the squeeze layerï¼the number of filters in the expnad layer</li><li>normal settingï¼š$base_e=128, incre_e=128, pct_{3x3}=0.5, freq=2, SR=0.125$</li></ul></li><li><p>SR</p><ul><li>increasing SR leads to higher accuracy and larger model size</li><li>Accuracy plateaus at 86.0% with SR=0.75 </li><li>further increasing provides no improvement  </li></ul></li><li><p>pct</p><ul><li>increasing pct leads to higher accuracy and larger model size</li><li>Accuracy plateaus at 85.6% with pct=50%</li><li>further increasing provides no improvement  </li></ul><p><img src="/2020/05/18/SqueezeNet/SR.png" width="60%"></p></li><li><p>bypass</p><ul><li>Vanilla </li><li>simple bypassï¼šwhen in &amp; out channels have the same dimensions</li><li>complex bypassï¼šincludes a 1x1 convolution layer </li><li><strong>alleviate the representational bottleneck introduced by squeeze layers</strong></li><li>both yielded accuracy improvements</li><li>simple bypass enabled higher accuracy </li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hausdorff Distance</title>
      <link href="/2020/05/14/Hausdorff-Distance/"/>
      <url>/2020/05/14/Hausdorff-Distance/</url>
      <content type="html"><![CDATA[<h2 id="Reducing-the-Hausdorff-Distance-in-Medical-Image-Segmentation-with-Convolutional-Neural-Networks"><a href="#Reducing-the-Hausdorff-Distance-in-Medical-Image-Segmentation-with-Convolutional-Neural-Networks" class="headerlink" title="Reducing the Hausdorff Distance in Medical Image Segmentation with Convolutional Neural Networks"></a>Reducing the Hausdorff Distance in Medical Image Segmentation with Convolutional Neural Networks</h2><ol><li><p>åŠ¨æœº</p><ul><li>novel loss function to reduce HD directly </li><li>propose three methods </li><li>2D&amp;3Dï¼Œultra &amp; MR &amp; CT</li><li>lead to approximately 18 âˆ’ 45% reduction in HD without degrading other segmentation performance criteria </li></ul></li><li><p>è®ºç‚¹</p><ul><li>HD is one of the most informative and useful criteria because it is an indicator of the largest segmentation error </li><li>current segmentation algorithms rarely aim at minimizing or reducing HD directly <ul><li>HD is determined <strong>solely</strong> by the largest error instead of the overall segmentation performance </li><li>HDâ€˜s sensitivity to noise and outliers  â€”&gt; modified version</li><li>the optimization diffculty</li></ul></li><li>thus we propose an â€œHD- inspiredâ€ loss function </li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>denotations</p><ul><li>probabilityï¼š$q$</li><li>binary maskï¼š$\bar p$ã€$\bar q$</li><li>boundaryï¼š$\delta p$ã€$\delta q$</li><li><p>single hdï¼š$hd(\bar p, \bar q)$ã€$hd(\bar q, \bar p)$</p><p><img src="/2020/05/14/Hausdorff-Distance/start.png" width="40%"></p></li></ul></li><li><p>based on distance transforms </p><ul><li><p>distance map $d_p$ï¼šdefine the distance map as the unsigned distance to the boundary $\delta p$</p><script type="math/tex; mode=display">  DT_X[i,j] = min_{[k,l]\in X}d([i,j], [k,l])</script><p>  è·ç¦»åœºå®šä¹‰ä¸ºï¼šæ¯ä¸ªç‚¹åˆ°ç›®æ ‡åŒºåŸŸ(X)çš„è·ç¦»çš„æœ€å°å€¼</p><p>  <img src="/2020/05/14/Hausdorff-Distance/dt.png" width="40%"></p></li><li><p>HD based on DTï¼š</p><script type="math/tex; mode=display">  hd_{DT}(\delta p, \delta q) = max((\bar p \triangle \bar q)\circ d_p)\\  \bar p \triangle \bar q = |\bar p - \bar q|</script><ul><li>finally haveï¼š<script type="math/tex; mode=display">  HD_{DT}(\delta p, \delta q) = max(hd_{DT}(\delta p, \delta q), hd_{DT}(\delta q, \delta p))</script></li></ul></li><li><p>modified loss version of HDï¼š</p><script type="math/tex; mode=display">  Loss_{DT}(q,p) = \frac{1}{|\Omega|}\sum_{\Omega}((p-q)^2\circ(d_p^{\alpha}+d_q^{\alpha}))</script><ul><li>penalizely focus on areas instead of single point</li><li>$\alpha$ determines how strongly we penalize larger errors</li><li>use possibility instead of thresholded value</li><li>use $(p-q)^2$ instead of $|p-q|$</li></ul></li><li><p>correlations</p><ul><li>$HD_{DT}$ï¼šPearson correlation coefficient above 0.99</li><li><p>$Loss_{DT}$ï¼šPearson correlation coefficient above 0.93</p><p><img src="/2020/05/14/Hausdorff-Distance/correlation.png" width="50%"></p></li></ul></li><li><p>drawback </p><ul><li><p><strong>high computational</strong> cost especially in 3D </p></li><li><p>$q$ changes along with training process thus $d_q$ changes while $d_p$ remains</p></li><li><p>modified one-sided HD (OS)ï¼š</p><script type="math/tex; mode=display">  Loss_{DT-OS}(q,p) = \frac{1}{|\Omega|}\sum_{\Omega}((p-q)^2\circ(d_p^{\alpha}))</script></li></ul></li></ul></li><li><p>HD using Morphological Operations</p><ul><li><p>morphological erosionï¼š</p><script type="math/tex; mode=display">  S \ominus B = \{z\in \Omega | B(z) \subseteq S\}</script><p>  è…èš€æ“ä½œå®šä¹‰ä¸ºï¼šåœ¨åŸå§‹äºŒå€¼åŒ–å›¾çš„å‰æ™¯åŒºåŸŸï¼Œä»¥æ¯ä¸ªåƒç´ ä¸ºä¸­å¿ƒç‚¹ï¼Œrun structure element block Bï¼Œå¦‚æœBå®Œå…¨åœ¨åŸå›¾å†…ï¼Œåˆ™å½“å‰ä¸­å¿ƒç‚¹åœ¨è…èš€åä¹Ÿæ˜¯å‰æ™¯ã€‚</p></li><li><p>HD based on erosionï¼š</p><script type="math/tex; mode=display">  HD_{ER}(\delta p, \delta q)=2r^*\\  where\ r^* = min_r \{(\bar p \triangle \bar q) \ominus B_r = \varnothing\}</script><ul><li>$HD_{ER}$ is a lower bound of the true value</li><li>can be computed more efficiently using convolutional operations </li></ul></li><li><p>modifid loss versionï¼š</p><script type="math/tex; mode=display">  Loss_{ER}(q,p) = \frac{1}{|\Omega|}\sum_k \sum_{\Omega}((p-q)^2 \ominus_k B)k^{\alpha}</script><ul><li>k successive erosions </li><li><strong>cross-shaped kernel</strong> whose elements sum to one followed by a <strong>soft thresholding</strong> at 0.50</li></ul></li><li><p>correlations</p><ul><li>$HD_{ER}$ï¼šPearson correlation coefficient above 0.91</li><li>$Loss_{ER}$ï¼šPearson correlation coefficient above 0.83</li></ul></li></ul></li><li><p>HD using circular-shaped convolutional kernel </p><ul><li><p>circular-shaped kernel</p><p>   <img src="/2020/05/14/Hausdorff-Distance/circle.png" width="40%"></p></li><li><p>HD based on circular-shaped kernelï¼š</p><script type="math/tex; mode=display">  hd_{CV}(\delta p, \delta q)=max(r_1, r_2)\\  where \ r_1=max_r (max_{\Omega}f_h(\bar p ^C * B_r)\circ(\bar q \backslash \bar p))\\  where \ r_2=max_r (max_{\Omega}f_h(\bar p * B_r)\circ(\bar p \backslash \bar q))\\</script><ul><li>$\bar p^C$ï¼šcomplement è¡¥é›†</li><li>$f_h$ï¼šhard thresholding setting all values below 1 to zero </li></ul></li><li><p>modified loss versionï¼š</p><script type="math/tex; mode=display">  Loss_{CV}(q,p)=\frac{1}{|\Omega|}\sum_{r\in R}r^{\alpha}\sum_{\Omega}[f_s(Br*\bar p^C)\circ f_{\bar q\backslash \bar p} + f_s(B_r * \bar p) \circ f_{\bar p \backslash \bar q}\\  +f_s(Br*\bar q^C)\circ f_{\bar p\backslash \bar q} + f_s(B_r * \bar q) \circ f_{\bar q \backslash \bar p}]</script><ul><li>soft thresholding</li><li><script type="math/tex; mode=display">f_{\bar p\backslash \bar q} = (p-q)^2*p</script></li></ul></li><li><p>correlations</p><ul><li>$HD_{CV}$ï¼šPearson correlation coefficient above 0.99</li><li>$Loss_{CV}$ï¼šPearson correlation coefficient above 0.88</li></ul></li><li><p>computationï¼š</p><ul><li>kernel size<ul><li>$HD_{ER}$ is computed using small fixed convolutional kernels (of size 3)</li><li>$Loss_{CV}$ require applying filters of increasing size(we use a maximum kernel radius of 18 pixels in 2D and 9 voxels in 3D)</li></ul></li><li>steps<ul><li>choose R based on the expected range of segmentation errors </li><li>set R = {3, 6, . . . 18} for 2D images and R = {3,6,9} for 3D </li></ul></li></ul></li></ul></li><li><p>training</p><ul><li>standard Unet</li><li>augment our HD-based loss term with a DSC loss term for more stable training</li><li>reweight both loss after every epoch</li></ul></li></ul></li></ol><p>d</p>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SE block</title>
      <link href="/2020/04/30/SE-block/"/>
      <url>/2020/04/30/SE-block/</url>
      <content type="html"><![CDATA[<h2 id="ç»¼è¿°"><a href="#ç»¼è¿°" class="headerlink" title="ç»¼è¿°"></a>ç»¼è¿°</h2><p>å›¾åƒç‰¹å¾çš„æå–èƒ½åŠ›æ˜¯CNNçš„æ ¸å¿ƒèƒ½åŠ›ï¼Œè€ŒSE blockå¯ä»¥èµ·åˆ°ä¸ºCNNæ ¡å‡†é‡‡æ ·çš„ä½œç”¨ã€‚</p><p>æ ¹æ®æ„Ÿå—é‡ç†è®ºï¼Œç‰¹å¾çŸ©é˜µä¸»è¦æ¥è‡ªäºæ ·æœ¬çš„ä¸­å¤®åŒºåŸŸï¼Œå¤„åœ¨è¾¹ç¼˜ä½ç½®çš„é…’ç“¶çš„å›¾åƒç‰¹å¾å¾ˆå¤§æ¦‚ç‡ä¼šè¢«poolingå±‚æŠ›å¼ƒæ‰ã€‚è€ŒSE blockçš„åŠ å…¥å°±å¯ä»¥é€šè¿‡æ¥è°ƒæ•´ç‰¹å¾çŸ©é˜µï¼Œå¢å¼ºé…’ç“¶ç‰¹å¾çš„æ¯”é‡ï¼Œæé«˜å®ƒçš„è¯†åˆ«æ¦‚ç‡ã€‚</p><p><img src="/2020/04/30/SE-block/bottle.png" width="25%"></p><ol><li>[SE-Net] Squeeze-and-Excitation Networks</li><li>[SC-SE] Concurrent Spatial and Channel â€˜Squeeze &amp; Excitationâ€™ in Fully Convolutional Networks </li><li>[CMPE-SE] Competitive Inner-Imaging Squeeze and Excitation for Residual Network</li></ol><h2 id="SENet-Squeeze-and-Excitation-Networks"><a href="#SENet-Squeeze-and-Excitation-Networks" class="headerlink" title="SENet: Squeeze-and-Excitation Networks"></a>SENet: Squeeze-and-Excitation Networks</h2><ol><li><p>åŠ¨æœº</p><ul><li>prior research has investigated the spatial component to achieve more powerful representations </li><li>we focus on the channel relationship instead</li><li>SE-blockï¼šadaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels</li><li>enhancing the representational power </li><li>in a computationally efficient manner</li></ul></li><li><p>è®ºç‚¹</p><ul><li>stronger networkï¼š<ul><li>deeper</li><li>NiN-like bocks</li></ul></li><li>cross-channel correlations in prior work<ul><li>mapped as new combinations of features through 1x1 conv</li><li>concentrated on the objective of reducing model and computational complexity</li></ul></li><li>In contrast, we found this mechanism <ul><li>can ease the learning process</li><li>and significantly enhance the representational power of the network</li></ul></li><li>Attention<ul><li>Attention can be interpreted as a means of biasing the allocation of available computational resources towards the most informative components<ul><li>Some works provide interesting studies into the combined use of spatial and channel attention </li></ul></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>SE-block</p><ul><li><p>The channel relationships modelled by convolution are inherently implicit and local </p></li><li><p>we would like to provide it with access to global information</p></li><li><p>squeezeï¼šusing global average pooling </p></li><li><p>excitationï¼šnonlinear &amp; non-mutually-exclusive using sigmoid</p><ul><li><p>bottleneckï¼ša dimensionality-reduction layer $W_1$ with reduction ratio $r$ and ReLU and a dimensionality-increasing layer $W_2$</p></li><li><p>$s = F_{ex}(z,W) = \sigma (W_2 \delta(W_1 z))$</p><p><img src="/2020/04/30/SE-block/seblock.png" width="70%"></p></li></ul></li><li><p>integration</p><ul><li>insert after the non-linearity following each convolution</li><li>inceptionï¼štake the transformation $F_{tr}$ to be an entire Inception module </li><li><p>residualï¼štake the transformation $F_{tr}$ to be the non-identity branch of a residual module</p><p><img src="/2020/04/30/SE-block/integrate.png" width="70%"></p></li></ul></li></ul></li><li><p>model and computational complexity</p><ul><li>ResNet50 vs. SE-ResNet50ï¼š0.26% relative increase GFLOPs approaching ResNet10â€™s accuracy </li><li>the additional parameters result solely from the two FC layers, among which  the final stage FC claims the majority due to being performed across the greatest number of channels</li><li>the costly final stage of SE blocks could be removed at only a small cost in performance </li></ul></li><li><p>ablations</p><ul><li>FC<ul><li>removing the biases of the FC layers  in the excitation facilitates the modelling of channel dependencies </li></ul></li><li>reduction ratio<ul><li>performance is robust to a range of reduction ratios </li><li>In practice, using an identical ratio throughout a network may not be optimal due to the distinct roles performed by different layers </li></ul></li><li>squeeze<ul><li>global average pooling vs. global max poolingï¼šaverage pooling slightly better </li></ul></li><li>excitation<ul><li>Sigmoid vs. ReLU vs. tanhï¼š<ul><li>tanhï¼šslightly worse</li><li>ReLUï¼šdramatically worse   </li></ul></li></ul></li><li>stages<ul><li>each stages brings benefits </li><li>combination make even better</li></ul></li><li>integration strategy<ul><li>fairly robust to their location, provided that they are applied prior to branch aggregation</li><li>inside the residual unitï¼šfewer channels, fewer parameters, comparable accuracy </li></ul></li></ul></li><li>primitive understanding <ul><li>squeeze<ul><li>the use of <strong>global information</strong> has a significant influence on the model performance</li></ul></li><li>excitation<ul><li>the distribution across different classes is very similar at the earlier layers  (general features)</li><li>the value of each channel becomes much more class-specific at greater depth</li><li>SE_5_2 exhibits an interesting tendency towards a saturated state in which most of the activations are close to one</li><li>SE_5_3 exhibits a similar pattern emerges over different classes, up to a modest change in scale</li><li>suggesting that SE_5_2 and SE_5_3 are less important than previous blocks in providing recalibration to the network (thus can be removed)</li></ul></li></ul></li></ul></li><li><p>APPENDIX</p><ul><li>åœ¨ImageNetä¸ŠSOTAçš„æ¨¡å‹æ˜¯SENet-154ï¼Œtop1-erræ˜¯18.68ï¼Œè¢«æ ‡è®°åœ¨äº†efficientNetè®ºæ–‡çš„æŠ˜çº¿å›¾ä¸Š<ul><li>SE-ResNeXt-152ï¼ˆ64x4dï¼‰<ul><li>input=(224,224)ï¼štop1-erræ˜¯18.68</li><li>input=320/299ï¼štop1-erræ˜¯17.28</li></ul></li><li>further difference<ul><li>each bottleneck building blockçš„ç¬¬ä¸€ä¸ª1x1 convsçš„é€šé“æ•°å‡åŠ</li><li>stemçš„ç¬¬ä¸€ä¸ª7x7convæ¢æˆäº†3ä¸ªè¿ç»­çš„3x3 conv</li><li>1x1çš„s2 convæ¢æˆäº†3x3çš„s2 conv</li><li>fcä¹‹å‰æ·»åŠ dropout layer</li><li>label smoothing</li><li>æœ€åå‡ ä¸ªtraining epochå°†BNå±‚çš„å‚æ•°å†»ä½ï¼Œä¿è¯è®­ç»ƒå’Œæµ‹è¯•çš„å‚æ•°ä¸€è‡´</li><li>64 GPUsï¼Œbatch size=2048ï¼ˆ32 per GPUï¼‰</li><li>initial lr=1.0</li></ul></li></ul></li></ul></li></ol><h2 id="SC-SE-Concurrent-Spatial-and-Channel-â€˜Squeeze-amp-Excitationâ€™-in-Fully-Convolutional-Networks"><a href="#SC-SE-Concurrent-Spatial-and-Channel-â€˜Squeeze-amp-Excitationâ€™-in-Fully-Convolutional-Networks" class="headerlink" title="SC-SE: Concurrent Spatial and Channel â€˜Squeeze &amp; Excitationâ€™ in Fully Convolutional Networks"></a>SC-SE: Concurrent Spatial and Channel â€˜Squeeze &amp; Excitationâ€™ in Fully Convolutional Networks</h2><ol><li><p>åŠ¨æœº</p><ul><li>image segmentation task ä¸Šé¢SE-Netæå‡ºæ¥ä¸»è¦æ˜¯é’ˆå¯¹åˆ†ç±»</li><li>three variants of SE modules <ul><li>squeezing spatially and exciting channel-wise (cSE)</li><li>squeezing channel-wise and exciting spatially (sSE)</li><li>concurrent spatial and channel squeeze &amp; excitation (scSE) </li></ul></li><li>integrate within three different state-of-the- art F-CNNs (DenseNet, SD-Net, U-Net) </li></ul></li><li><p>è®ºç‚¹</p><ul><li>F-CNNs have become the tool of choice for many image segmentation tasks </li><li>coreï¼šconvolutions that capturing <strong>local spatial pattern along all input channels</strong> jointly </li><li>SE block <strong>factors out the spatial dependency</strong> by global average pooling to learn a channel specific descriptor (later refered to as cSE /channel-SE)</li><li>while for image segmentation, we hypothesize that the pixel-wise spatial information is more informative </li><li>thus we propose sSE(spatial SE) and scSE(spatial and channel SE)</li><li><p>can be seamlessly integrated by placing after every encoder and decoder block</p><p><img src="/2020/04/30/SE-block/scse.png" width="60%"></p></li></ul></li><li><p>æ–¹æ³•</p><ul><li>cSE<ul><li>GAPï¼šembeds the global spatial information into a vector </li><li>FC-ReLU-FC-Sigmoidï¼šadaptively learns the importance  </li><li>recalibrate</li></ul></li><li>sSE<ul><li>1x1 convï¼šgenerating a projection tensor representing the linearly combined representation for all channels C for a spatial location (i,j)</li><li>Sigmoidï¼šrescale </li><li>recalibrate </li></ul></li><li>scSE<ul><li>by element-wise addition </li><li>encourages the network to learn more meaningful feature maps â€”â€”â€”- relevant both spatially and channel-wise</li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li><p>F-CNN architecturesï¼š</p><ul><li>4 encoder blocks, one bottleneck layer, 4 decoder blocks and a classification layer </li><li>class imbalanceï¼šmedian frequency balancing ce</li></ul></li><li><p>dice cmpï¼šscSE &gt; sSE &gt; cSE &gt; vanilla</p></li><li><p>å°åŒºåŸŸç±»åˆ«çš„åˆ†å‰²ï¼Œè§‚å¯Ÿåˆ°ä½¿ç”¨cSEå¯èƒ½ä¼šå·®äºvanillaï¼š might have got overlooked by only exciting the channels</p></li><li><p>å®šæ€§åˆ†æï¼š</p><ul><li>ä¸€äº›under segmentedçš„åœ°æ–¹ï¼ŒscSE improves with the inclusion </li><li><p>ä¸€äº›over segmentedçš„åœ°æ–¹ï¼ŒscSE rectified the result</p><p><img src="/2020/04/30/SE-block/qualitive.png" width="55%"></p></li></ul></li></ul></li></ol><h2 id="Competitive-Inner-Imaging-Squeeze-and-Excitation-for-Residual-Network"><a href="#Competitive-Inner-Imaging-Squeeze-and-Excitation-for-Residual-Network" class="headerlink" title="Competitive Inner-Imaging Squeeze and Excitation for Residual Network"></a>Competitive Inner-Imaging Squeeze and Excitation for Residual Network</h2><ol><li>åŠ¨æœº<ul><li>for residual network</li><li>the residual architecture has been proved to be diverse and redundant</li><li>model the competition between residual and identity mappings</li><li>make the identity flow to control the complement of the residual feature maps</li></ul></li><li><p>è®ºç‚¹</p><ul><li>For analysis of ResNet, with the increase in depth, the residual network exhibits a certain amount of redundancy </li><li><p>with the CMPE-SE mechanism, it makes residual mappings tend to provide more efficient supplementary for identity mappings</p><p><img src="/2020/04/30/SE-block/cmpese.png" width="55%"></p></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>ä¸»è¦æå‡ºäº†ä¸‰ç§å˜ä½“ï¼š</p><p><img src="/2020/04/30/SE-block/cmpeses.png" width="75%"></p></li><li><p>ç¬¬ä¸€ä¸ªå˜ä½“ï¼š</p><ul><li>ä¸¤ä¸ªåˆ†æ”¯idå’Œresåˆ†åˆ«GAPå‡ºä¸€ä¸ªvectorï¼Œç„¶åfc reduct by ratioï¼Œç„¶åconcatï¼Œç„¶åchannel back</li><li>Implicitly, we can believe that the winning of the identity channels in this competition results in less weights of the residual channels </li></ul></li><li><p>ç¬¬äºŒä¸ªå˜ä½“ï¼š</p><ul><li><p>ä¸¤ç§æ–¹æ¡ˆ</p><ul><li>2x1 convsï¼šå¯¹ä¸Šä¸‹ç›¸åº”ä½ç½®çš„å…ƒç´ æ±‚avg</li><li>1x1 convsï¼šå¯¹å…¨éƒ¨å…ƒç´ æ±‚avgï¼Œç„¶åflatten</li></ul><p><img src="/2020/04/30/SE-block/sec.png" width="45%"></p></li></ul></li><li><p>ç¬¬ä¸‰ä¸ªå˜ä½“ï¼š</p><ul><li><p>ä¸¤è¾¹çš„channel-wise vectorå èµ·æ¥ï¼Œç„¶åreshapeæˆçŸ©é˜µå½¢å¼ï¼Œç„¶å3x3 convï¼Œç„¶åflatten</p><p><img src="/2020/04/30/SE-block/trd.png" width="45%"></p></li></ul></li></ul></li></ol><p>æ¯”è¾ƒæ‰¯ï¼Œä¸æµªè´¹æ—¶é—´åˆ†æäº†ã€‚</p>]]></content>
      
      
        <tags>
            
            <tag> papers </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>cv2&amp;numpy&amp;tobeadded</title>
      <link href="/2020/04/19/cv2-numpy-tobeadded/"/>
      <url>/2020/04/19/cv2-numpy-tobeadded/</url>
      <content type="html"><![CDATA[<ol><li>çŸ©é˜µä¹˜æ³•<ul><li>np.dot(A,B)ï¼šçœŸæ­£çš„çŸ©é˜µä¹˜æ³•</li><li>np.multiply(A,B) &amp; npé‡è½½çš„*ï¼šelement-wise productï¼ŒçŸ©é˜µä¸­å¯¹åº”å…ƒç´ ç›¸ä¹˜</li><li>cvçš„A.dot(B) &amp; cvé‡è½½çš„*ï¼šçœŸæ­£çš„çŸ©é˜µä¹˜æ³•</li><li>cvçš„A.mul(B) ï¼šelement-wise productï¼ŒçŸ©é˜µä¸­å¯¹åº”å…ƒç´ ç›¸ä¹˜</li></ul></li></ol><ol><li><p>å›¾åƒæ—‹è½¬</p><p> é€šè¿‡ä»¿å°„çŸ©é˜µ<code>cv2.getRotationMatrix2D</code>å’Œä»¿å°„å˜æ¢å‡½æ•°<code>cv2.warpAffine</code>æ¥å®ç°</p><ul><li><p>srcï¼šè¾“å…¥å›¾åƒ</p></li><li><p>Mï¼šå˜æ¢çŸ©é˜µ</p></li><li><p>dsizeï¼šè¾“å‡ºå›¾åƒçš„å¤§å°ï¼ˆåŸºäºå›¾åƒ<strong>åŸç‚¹</strong>è£å‰ªï¼‰</p></li><li><p>flagsï¼šæ’å€¼æ–¹æ³•</p></li><li><p>borderModeï¼šè¾¹ç•Œåƒç´ æ¨¡å¼</p></li><li><p>borderValueï¼šè¾¹ç•Œå¡«å……å€¼ï¼Œé»˜è®¤ä¸º0</p><p>cv2.getRotationMatrix2D(center, angle, scale)ï¼šè¿”å›ä¸€ä¸ª2x3çš„å˜æ¢çŸ©é˜µ</p></li><li><p>centerï¼šæ—‹è½¬ä¸­å¿ƒ</p></li><li>angleï¼šæ—‹è½¬è§’åº¦ï¼Œ<strong>æ­£å€¼æ˜¯é€†æ—¶é’ˆæ—‹è½¬</strong></li><li><p>scaleï¼šç¼©æ”¾å› å­</p><p>cv2.warpAffine(src, M, dsize[, dst[, flags[, borderMode[, borderValue]]]]))ï¼šè¿”å›å˜æ¢åçš„å›¾åƒ</p></li><li><p>srcï¼šè¾“å…¥å›¾åƒ</p></li><li><p>Mï¼šå˜æ¢çŸ©é˜µ</p></li><li><p>dsizeï¼šè¾“å‡ºå›¾åƒçš„å¤§å°ï¼ˆåŸºäºå›¾åƒ<strong>åŸç‚¹</strong>è£å‰ªï¼‰</p></li><li><p>flagsï¼šæ’å€¼æ–¹æ³•</p></li><li><p>borderModeï¼šè¾¹ç•Œåƒç´ æ¨¡å¼</p></li><li><p>borderValueï¼šè¾¹ç•Œå¡«å……å€¼ï¼Œé»˜è®¤ä¸º0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rotate_img</span><span class="params">(angle, img, interpolation=cv2.INTER_LINEAR, points=[])</span>:</span></span><br><span class="line">    h, w = img.shape</span><br><span class="line">    rotataMat = cv2.getRotationMatrix2D((w/<span class="number">2</span>, h/<span class="number">2</span>), math.degrees(angle), <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># rotate_img1: è¾“å‡ºå›¾åƒå°ºå¯¸ä¸å˜ï¼Œè¶…å‡ºåŸå›¾åƒéƒ¨åˆ†è¢«cutæ‰</span></span><br><span class="line">    rotate_img1 = cv2.warpAffine(img, rotataMat, dsize=(w, h), flags=interpolation, borderMode=cv2.BORDER_CONSTANT, borderValue=<span class="number">0</span>)</span><br><span class="line">   <span class="comment"># rotate_img2: è¾“å‡ºå›¾åƒå°ºå¯¸å˜å¤§ï¼Œä¿ç•™è¶…å‡ºåŸå›¾åƒéƒ¨åˆ†ï¼Œæ–°çš„åæ ‡åŸç‚¹ä¿è¯æ—‹è½¬ä¸­å¿ƒä»æ—§ä½äºå›¾åƒä¸­å¿ƒ</span></span><br><span class="line">    new_h = int(w*math.fabs(math.sin(angle)) + h*math.fabs(math.cos(angle)))</span><br><span class="line">    new_w = int(h*math.fabs(math.sin(angle)) + w*math.fabs(math.cos(angle)))</span><br><span class="line">    rotataMat[<span class="number">0</span>, <span class="number">2</span>] += (new_w - w) / <span class="number">2</span></span><br><span class="line">    rotataMat[<span class="number">1</span>, <span class="number">2</span>] += (new_h - h) / <span class="number">2</span></span><br><span class="line">    rotate_img2 = cv2.warpAffine(img, rotataMat, dsize=(new_w, new_h), flags=interpolation, borderMode=cv2.BORDER_CONSTANT, borderValue=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># åæ ‡ç‚¹çš„å˜æ¢</span></span><br><span class="line">    rotated_points = []</span><br><span class="line">    <span class="keyword">for</span> point <span class="keyword">in</span> points:</span><br><span class="line">        point = rotataMat.dot([[point[<span class="number">0</span>]], [point[<span class="number">1</span>]], [<span class="number">1</span>]])</span><br><span class="line">        rotated_points.append((int(point[<span class="number">0</span>]), int(point[<span class="number">1</span>])))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> rotate_img2, rotated_points</span><br></pre></td></tr></table></figure><p>ä½¿ç”¨tipsï¼š</p></li><li><p>å¦‚æœä¸ä¿®æ”¹ä»¿å°„å˜æ¢çŸ©é˜µçš„å¹³ç§»å‚æ•°ï¼Œåæ ‡åŸç‚¹çš„ä½ç½®ä¸å‘ç”Ÿæ”¹å˜</p></li><li><p>dsizeæŒ‡å®šçš„è¾“å‡ºå›¾åƒæ˜¯ä»åŸç‚¹ä½ç½®å¼€å§‹è£å‰ª</p></li><li><p>åæ ‡ç‚¹çš„å˜æ¢æ»¡è¶³å…¬å¼ï¼š</p><script type="math/tex; mode=display">  dst(x,y) = src(M_{11}x+M_{12}y+M_{13}, M_{21}x+M_{22}y+M_{23})</script></li></ul></li></ol><ol><li><p>np.meshgrid(*xi,**kwargs)</p><p> è¿™ä¸ªå‡½æ•°ç¥ä»–å¦ˆå‘ï¼Œä½œç”¨æ˜¯Return coordinate matrices from coordinate vectors. Make N-D coordinate arrays for vectorized evaluations of N-D scalar/vector fields over N-D grids, given one-dimensional coordinate arrays x1, x2,â€¦, xn. ä½†æ˜¯å°è¯•ä¸€ä¸‹ä¼šå‘ç°ï¼š</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">0</span>,<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">y = np.arange(<span class="number">0</span>,<span class="number">20</span>,<span class="number">1</span>)</span><br><span class="line">z = np.arange(<span class="number">0</span>,<span class="number">30</span>,<span class="number">1</span>)</span><br><span class="line">x, y, z= np.meshgrid(x, y, z)</span><br><span class="line">print(x.shape)       <span class="comment"># (20, 10, 30)</span></span><br></pre></td></tr></table></figure><p> xyè½´åæ ‡æ˜¯åè¿‡æ¥çš„ï¼Œè¿™æ˜¯å› ä¸ºoptional argsé‡Œé¢æœ‰ä¸€ä¸ªindexingï¼š</p><p> <strong>indexing</strong> : {â€˜xyâ€™, â€˜ijâ€™}, Cartesian (â€˜xyâ€™, default) or matrix (â€˜ijâ€™) indexing of output.</p><p> æˆ‘ä»¬æƒ³è¦å¾—åˆ°çš„åæ ‡ç³»å’Œè¾“å…¥çš„è½´ä¸€ä¸€å¯¹åº”ï¼Œå¾—æŒ‡å®šå‚æ•°<code>indexing=&#39;ij&#39;</code></p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">0</span>,<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">y = np.arange(<span class="number">0</span>,<span class="number">20</span>,<span class="number">1</span>)</span><br><span class="line">z = np.arange(<span class="number">0</span>,<span class="number">30</span>,<span class="number">1</span>)</span><br><span class="line">x, y, z= np.meshgrid(x, y, z, indexing=<span class="string">'ij'</span>)</span><br><span class="line">print(x.shape)      <span class="comment"># (10, 20, 30)</span></span><br></pre></td></tr></table></figure><p> è¿˜æœ‰ä¸€ä¸ªå‚æ•°sparseï¼Œå› ä¸ºæ¯æ ¹è½´çš„åæ ‡éƒ½æ˜¯å¤åˆ¶çš„ï¼Œæ‰€ä»¥å¯ä»¥ç¨€ç–å­˜å‚¨ï¼Œæ­¤æ—¶å‡½æ•°è¿”å›å€¼å˜åŒ–ï¼š</p><p> <strong>sparse</strong> : bool, If True a sparse grid is returned in order to conserve memory. Default is False.</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">0</span>,<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">y = np.arange(<span class="number">0</span>,<span class="number">20</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">xx, yy = np.meshgrid(x, y)</span><br><span class="line">print(xx)        <span class="comment"># a 20x10 list</span></span><br><span class="line"></span><br><span class="line">xx, yy = np.meshgrid(x, y, sparse=<span class="keyword">True</span>)</span><br><span class="line">print(xx)        <span class="comment"># a 1*10 list</span></span><br><span class="line">print(yy)        <span class="comment"># a 20*1 list</span></span><br><span class="line"><span class="comment"># æ‰€ä»¥æ•´ä½“ä¸Šè¿˜æ˜¯ä¸ª20*10çš„çŸ©é˜µ</span></span><br></pre></td></tr></table></figure><p> äºŒç»´å¯è§†åŒ–ï¼š</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">z = xx**<span class="number">2</span> + yy**<span class="number">2</span>            <span class="comment"># xxå’Œyyæ—¢å¯ä»¥æ˜¯dense convervationä¹Ÿå¯ä»¥æ˜¯sparse convervation</span></span><br><span class="line">h = plt.contourf(x,y,z)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>np.tile(A,reps)</p><p> è¿™ä¸ªå‡½æ•°æŒºæœ‰ç”¨çš„ï¼ŒæŠŠæ•°ç»„æ²¿ç€æŒ‡å®šç»´åº¦å¤åˆ¶ï¼Œæ¯”stackã€concatå•¥çš„éƒ½ä¼˜é›…ï¼Œèƒ½è‡ªåŠ¨åˆ›å»ºæ–°çš„ç»´åº¦</p><ul><li>Aï¼šarray_like, The input array.</li><li>repsï¼šarray_like, The number of repetitions of <em>A</em> along each axis.</li></ul></li></ol><ol><li><p>np.reshape(a, newshape, order=â€™Câ€™)</p><p> è¿™ä¸ªå‡½æ•°è´¼å¸¸ç”¨ï¼Œä½†æ˜¯ä¸€èˆ¬ç”¨äºäºŒç»´çš„æ—¶å€™æ²¡è€ƒè™‘é‡ç»„é¡ºåºè¿™ä»¶äº‹</p><ul><li><p>order: {â€˜Câ€™, â€˜Fâ€™, â€˜Aâ€™}, optionalï¼Œç®€å•ç†è§£ï¼Œreshapeçš„é€šç”¨å®ç°æ–¹å¼æ˜¯å…ˆå°†çœŸä¸ªarrayæ‹‰ç›´ï¼Œç„¶åä¾æ¬¡å–æ•°æ®å¡«å…¥æŒ‡å®šç»´åº¦ï¼ŒCæ˜¯ä»æœ€é‡Œé¢çš„ç»´åº¦å¼€å§‹æ‹‰ç›´&amp;æ„é€ ï¼ŒFæ˜¯ä»æœ€å¤–é¢çš„ç»´åº¦å¼€å§‹æ‹‰ç›´&amp;æ„é€ ï¼ŒA for auto</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">6</span>)</span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># C-like index ordering</span></span><br><span class="line">np.reshape(a, (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fortran-like index ordering</span></span><br><span class="line">np.reshape(a, (<span class="number">2</span>, <span class="number">3</span>), order=<span class="string">'F'</span>)</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">4</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">1</span>, <span class="number">5</span>]])</span><br></pre></td></tr></table></figure></li><li><p>tfå’Œkerasé‡Œé¢ä¹Ÿæœ‰reshapeï¼Œæ˜¯æ²¡æœ‰orderå‚æ•°çš„ï¼Œé»˜è®¤æ˜¯â€™Câ€™</p></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>MobileNets</title>
      <link href="/2020/04/16/MobileNets/"/>
      <url>/2020/04/16/MobileNets/</url>
      <content type="html"><![CDATA[<h2 id="preview"><a href="#preview" class="headerlink" title="preview"></a>preview</h2><ol><li><p>åŠ¨æœº</p><ul><li>è®¡ç®—åŠ›æœ‰é™</li><li>æ¨¡å‹å‹ç¼©ï¼ä½¿ç”¨å°æ¨¡å‹</li></ul></li><li><p>æ·±åº¦å¯åˆ†ç¦»å·ç§¯ Depthwise Separable Convolution</p><ul><li>å°†æ ‡å‡†å·ç§¯æ‹†åˆ†ä¸ºä¸¤ä¸ªæ“ä½œï¼šæ·±åº¦å·ç§¯(depthwise convolution) å’Œé€ç‚¹å·ç§¯(pointwise convolution)</li><li>æ ‡å‡†å·ç§¯ï¼šå‚æ•°é‡k*k*input_channel*output_channel</li><li>æ·±åº¦å·ç§¯(depthwise convolution) ï¼šé’ˆå¯¹æ¯ä¸ªè¾“å…¥é€šé“é‡‡ç”¨ä¸åŒçš„å·ç§¯æ ¸ï¼Œå‚æ•°é‡k*k*input_channel</li><li>é€ç‚¹å·ç§¯(pointwise convolution)ï¼šå°±æ˜¯æ™®é€šçš„å·ç§¯ï¼Œåªä¸è¿‡å…¶é‡‡ç”¨1x1çš„å·ç§¯æ ¸ï¼Œå‚æ•°é‡1*1*input_channel*output_channel</li><li><p>with BN and ReLUï¼š</p><p><img src="/2020/04/16/MobileNets/deepwise block.png" width="40%"></p></li><li><p>DWæ²¡æœ‰æ”¹å˜é€šé“æ•°çš„èƒ½åŠ›ï¼Œå¦‚æœè¾“å…¥å±‚çš„é€šé“æ•°å¾ˆå°‘ï¼ŒDWä¹Ÿåªèƒ½åœ¨ä½ç»´ç©ºé—´æç‰¹å¾ï¼Œå› æ­¤V2æå‡ºå…ˆå¯¹åŸå§‹è¾“å…¥åšexpansionï¼Œç”¨ä¸€ä¸ªéçº¿æ€§PWå‡ç»´ï¼Œç„¶åDWï¼Œç„¶åå†ä½¿ç”¨ä¸€ä¸ªPWé™ç»´ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç¬¬äºŒä¸ªPWä¸ä½¿ç”¨éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œå› ä¸ºä½œè€…è®¤ä¸ºï¼Œreluä½œç”¨åœ¨ä½ç»´ç©ºé—´ä¸Šä¼šå¯¼è‡´ä¿¡æ¯æŸå¤±ã€‚</p></li></ul></li><li><p>è¿›ä¸€æ­¥ç¼©å‡è®¡ç®—é‡</p><ul><li>é€šé“æ•°ç¼©å‡ï¼šå®½åº¦å› å­ alpha</li><li>åˆ†è¾¨ç‡ç¼©å‡ï¼šåˆ†è¾¨ç‡å› å­rho</li></ul></li><li><p>papers</p><ul><li><p>[V1] MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applicationsï¼Œä¸»è¦è´¡çŒ®Depthwise Separable Convolution</p></li><li><p>[V2] MobileNetV2: Inverted Residuals and Linear Bottlenecksï¼Œä¸»è¦è´¡çŒ®inverted residual with linear bottleneck</p></li><li>[V3] Searching for MobileNetV3ï¼Œæ¨¡å‹ç»“æ„å‡çº§(inverted-res-block + SE-block)ï¼Œé€šè¿‡NASï¼Œè€Œéæ‰‹åŠ¨è®¾è®¡</li></ul></li></ol><h2 id="MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications"><a href="#MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications" class="headerlink" title="MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"></a>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</h2><ol><li>åŠ¨æœº<ul><li>efficient modelsï¼šuses depthwise separable convolutions and two simple global hyper-parameters</li><li>resource and accuracy tradeoffs</li><li>a class of network architectures that allows a model developer to specifically choose a small network that matches the resource restrictions (latency, size) for their application </li></ul></li><li><p>è®ºç‚¹ï¼š</p><ul><li>the general trend has been to make deeper and more complicated networks in order to achieve higher accuracy </li><li>not efficient on computationally limited platform</li><li>building small and efficient neural networksï¼šeither compressing pretrained networks or training small networks directly </li><li>Many papers on small networks focus only on size but do not consider speed</li><li>speed &amp; size ä¸å®Œå…¨å¯¹ç­‰</li><li>sizeï¼šdepthwise separable convolutions, bottleneck approaches, compressing pretrained networks, distillation </li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>depthwise separable convolutions </p><ul><li>a form of factorized convolutionsï¼ša standard conv splits into 2 layers</li><li>factorize the filtering and combination steps of standard conv</li><li>drastically reducing computation and model size to $\frac{1}{N} + \frac{1}{D_k^2}$</li><li>use both batchnorm and ReLU nonlinearities for both layers</li><li><p>MobileNet uses 3 Ã— 3 depthwise separable convolutions which bring between 8 to 9 times less computation  </p><p><img src="/2020/04/16/MobileNets/deepwise conv.png" width="30%"></p></li></ul></li><li><p>MobileNet</p><ul><li>the first layer is a full convolution, the rest depthwise separable convolutions  </li><li>down sampling is handled with strided convolution</li><li>all layers are followed by a BN and ReLU nonlinearity</li><li>a final average pooling reduces the spatial resolution to 1 before the fully connected layer. </li><li><p>the final fully connected layer has no nonlinearity and feeds into a softmax layer for classification</p><p><img src="/2020/04/16/MobileNets/mobileNet structure.png" width="45%"></p></li></ul></li><li><p>training so few parameters</p><ul><li>RMSprop </li><li>less regularization and data augmentation techniques because small models have less trouble with overfitting</li><li>it was important to put very little or no weight decay (l2 regularization) </li><li>do not use side heads or label smoothing or image distortions  </li></ul></li><li><p>Width Multiplier: Thinner Models </p><ul><li>thin a network uniformly at each layer </li><li>the input channels $M$ and output channels $N$ becomes $\alpha M$ and $\alpha N$</li><li>$\alpha=1$ï¼šbaseline MobileNet       $\alpha&lt;1$ï¼šreduced MobileNet</li><li>reduce the parameters roughly by $\alpha^2$</li></ul></li><li><p>Resolution Multiplier: Reduced Representation</p><ul><li>apply this to the input image</li><li>the input resolution of the network is typically 224, 192, 160 or 128</li><li>$\rho=1$ï¼šbaseline MobileNet       $\rho&lt;1$ï¼šreduced MobileNet</li><li><p>reduce the parameters roughly by $\rho^2$</p><p><img src="/2020/04/16/MobileNets/param reduce.png" width="45%"></p></li></ul></li></ul></li><li><p>ç»“è®º</p><ul><li>using depthwise separable convolutions compared to full convolutions only reduces accuracy by 1% on ImageNet but saving tremendously on mult-adds and parameters</li></ul><p><img src="/2020/04/16/MobileNets/cmp1.png" width="40%"></p><ul><li>at similar computation and number of parameters, thinner MobileNets is 3% better than making them shallower</li></ul><p><img src="/2020/04/16/MobileNets/cmp2.png" width="40%"></p><ul><li>trade-offs based on the two hyper-parameters</li></ul><p><img src="/2020/04/16/MobileNets/cmp3.png" width="40%"></p></li></ol><h2 id="MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks"><a href="#MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks" class="headerlink" title="MobileNetV2: Inverted Residuals and Linear Bottlenecks"></a>MobileNetV2: Inverted Residuals and Linear Bottlenecks</h2><ol><li><p>åŠ¨æœº</p><ul><li>a new mobile architecture <ul><li>based on an inverted residual structure</li><li>remove non-linearities in the narrow layers in order to maintain representational power</li></ul></li><li>prove on multiple tasks<ul><li>object detectionï¼šSSDLite</li><li>semantic segmentationï¼šMobile DeepLabv3</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>Depthwise Separable Convolutions </p><ul><li>replace a full convolutional opera- tor with a factorized version </li><li>depthwise convolution, it performs lightweight filtering per input channel</li><li>pointwise convolution, computing linear combinations of the input channels</li></ul></li><li><p>Linear Bottlenecks </p><ul><li>ReLU results in information loss in lower dimension space</li><li><strong>expansion ratio</strong>ï¼šif we have lots of channels, information might still be preserved in the other channels</li><li><p>linearï¼šbottleneckä¸Šé¢ä¸åŒ…å«éçº¿æ€§æ¿€æ´»å•å…ƒ</p><p><img src="/2020/04/16/MobileNets/expansion.png" width="30%"></p></li></ul></li><li><p>Inverted residuals </p><ul><li>bottlenecks actually contain all the necessary information </li><li>expansion layer acts merely as an implementation detail that accompanies a non-linear transformation</li></ul><p><img src="/2020/04/16/MobileNets/residual.png" width="30%"></p><ul><li>parameter countï¼š</li><li>basic building block is a bottleneck depth-separable convolution with residuals</li></ul></li></ul><p><img src="/2020/04/16/MobileNets/blockparam.png" width="40%"></p></li></ol><pre><code>* interpretation     * provides a natural separation between the input/output    * expansionï¼šcapacity    * layer transformationï¼šexpressiveness* MobileNetV2 model architecture    * initial filtersï¼š32    * ReLU6ï¼šuse ReLU6 as the non-linearity because of its robustness when used with low-precision computation      * use constant expansion rate between 5 and 10 except the 1stï¼šsmaller network inclines smaller and larger larger    &lt;img src=&quot;MobileNets/MobileNetV2.png&quot; width=&quot;40%&quot; /&gt;    * comparison with other architectures    &lt;img src=&quot;MobileNets/cmpV2.png&quot; width=&quot;40%&quot; /&gt;</code></pre><ol><li><p>å®éªŒ</p><ul><li><p>Object Detection  </p><ul><li>evaluate the performance as feature extractors</li><li>replace all the regular convolutions with separable convolutions <strong>in SSD prediction layers</strong>ï¼šbackboneæ²¡æœ‰æ”¹åŠ¨ï¼Œåªæ›¿æ¢å¤´éƒ¨çš„å·ç§¯ï¼Œé™ä½è®¡ç®—é‡</li><li>achieves competitive accuracy with significantly fewer parameters and smaller computational complexity </li></ul></li><li><p>Semantic Segmentation </p><ul><li>build DeepLabv3 heads on top of the second last feature map of MobileNetV2  </li><li>DeepLabv3 heads are computationally expensive and removing the ASPP module significantly reduces the MAdds </li></ul></li><li><p>ablation</p><ul><li>inverted residual connectionsï¼šshortcut connecting bottleneck perform better than shortcuts connecting the expanded layers åœ¨å°‘é€šé“çš„ç‰¹å¾ä¸Šè¿›è¡ŒçŸ­è¿æ¥</li><li><p>linear bottlenecksï¼šlinear bottlenecks improve performance, <strong>providing support that non-linearity destroys information in low-dimensional space</strong></p><p><img src="/2020/04/16/MobileNets/ablation.png" width="40%"></p></li></ul></li></ul></li></ol><h2 id="Searching-for-MobileNetV3"><a href="#Searching-for-MobileNetV3" class="headerlink" title="Searching for MobileNetV3"></a>Searching for MobileNetV3</h2><ol><li><p>åŠ¨æœº</p><ul><li>automated search algorithms and network design work together</li><li>classification &amp; detection &amp; segmentation</li><li>a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP)</li><li>new efficient versions of nonlinearities  </li></ul></li><li><p>è®ºç‚¹</p><ul><li>reducing<ul><li>the number of parameters </li><li>the number of operations (MAdds)  </li><li>inference latency</li></ul></li><li>related work<ul><li>SqueezeNetï¼š1x1 convolutions </li><li>MobileNetV1ï¼šseparable convolution  </li><li>MobileNetV2ï¼šinverted residuals </li><li>ShuffleNetï¼šgroup convolutions  </li><li>CondenseNetï¼šgroup convolutions  </li><li>ShiftNetï¼šshift operation </li><li>MnasNetï¼šMobileNetV2+SE-blockï¼Œattention modules are placed after the depthwise filters in the expansion </li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>base blocks </p><ul><li>combination of ideas from [MobileNetV1, MobileNetV2, MnasNet]</li><li>inverted-res-block + SE-block</li><li>swish nonlinearity  </li><li>hard sigmoid</li></ul><p><img src="/2020/04/16/MobileNets/IR+SE.png" width="40%"></p></li><li><p>Network Search </p><ul><li>use platform-aware NAS to search for the global network structures </li><li>use the NetAdapt algorithm to search per layer for the number of filters</li></ul></li><li><p>Network Improvements </p><ul><li><p>redesign the computionally-expensive layers at the beginning and the end of the network</p><ul><li>the last block of MobileNetV2â€™s inverted bottleneck structure </li><li>move this layer <strong>past</strong> the final average poolingï¼šç§»åŠ¨åˆ°GAPåé¢å»ï¼Œä½œç”¨åœ¨1x1çš„featuremapä¸Šinstead of 7x7ï¼Œæ›²çº¿æ•‘å›½</li></ul></li><li><p>a new nonlinearity, h-swish</p><ul><li><p>the initial set of filters are also expensiveï¼šusually start with 32 filters in a full 3x3 convolution to build initial filter banks for edge detection </p></li><li><p>reduce the number of filters to 16 and use the hard swish nonlinearity </p><script type="math/tex; mode=display">  swish\ [x]=x*\sigma(x)\\  h\_swish\ [x]=x\frac{ReLU6(x+3)}{6}</script><p><img src="/2020/04/16/MobileNets/swish.png" width="40%"></p></li><li><p>most of the benefits swish are realized by using them only in the deeper layersï¼šåªåœ¨ååŠæ®µç½‘ç»œä¸­ç”¨</p></li></ul></li><li><p>SE-block</p><ul><li>ratioï¼šall to fixed to be 1/4 of the number of channels in expansion layer</li></ul></li></ul></li><li><p>MobileNetV3 architecture</p><p> <img src="/2020/04/16/MobileNets/mobileNetV3.png" width="40%"></p></li></ul></li><li><p>å®éªŒ</p><ul><li><p>Detection </p><ul><li>use MobileNetV3 as replacement for the <strong>backbone feature extractor</strong> in SSDLiteï¼šæ”¹åšbackboneäº†</li><li>reduce the channel counts of C4&amp;C5â€™s blockï¼šå› ä¸ºMobileNetV3åŸæœ¬æ˜¯è¢«ç”¨æ¥è¾“å‡º1000ç±»çš„ï¼Œtransferåˆ°90ç±»çš„cocoæ•°æ®é›†ä¸Šæœ‰äº›redundant</li></ul></li><li><p>Segmentation </p><ul><li><p>as network backbone</p></li><li><p>compare two segmentation heads</p><ul><li>R-ASPPï¼šreduced design of the Atrous Spatial Pyramid Pooling module with only two branches </li><li><p>Lite R-ASPPï¼šç±»SE-blockçš„è®¾è®¡ï¼Œå¤§å·ç§¯æ ¸ï¼Œå¤§æ­¥é•¿</p><p><img src="/2020/04/16/MobileNets/LR-ASPP.png" width="40%"></p></li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> papers </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>verseg</title>
      <link href="/2020/04/15/verseg/"/>
      <url>/2020/04/15/verseg/</url>
      <content type="html"><![CDATA[<ol><li><p>challenge</p><p> Large Scale Vertebrae Segmentation Challenge</p><ul><li>task1:Vertebra Labellingï¼Œå…³é”®ç‚¹æ£€æµ‹</li><li>task2:Vertebra Segmentationï¼Œå¤šç±»åˆ«åˆ†å‰²</li></ul></li><li><p>data</p><p><img src="/2020/04/15/verseg/data.png" width="80%"></p><ol><li>variationï¼šæ•°æ®affineè½´ä¸ç»Ÿä¸€ï¼Œå°ºå¯¸ä¸ç»Ÿä¸€ï¼Œæ‰«æèŒƒå›´ä¸ç»Ÿä¸€ï¼ŒFOVåŒºåŸŸä¸ç»Ÿä¸€</li><li>niiçš„ä¸¤å¤§è§£æå·¥å…·ï¼šnibabelåº“load dataçš„xyzé¡ºåºä¸axcodeçš„é¡ºåºä¸€è‡´ï¼Œe.g.[â€˜Râ€™,â€™Aâ€™,â€™Sâ€™]çš„orientationä¼šå¾—åˆ°xyzçš„arrayï¼Œè€Œsitkçš„è¯»å–åˆšå¥½åè¿‡æ¥ï¼Œsitkçš„arrä¼šæ˜¯zyxã€‚æˆ‘ä»¬ä¹‹å‰åœ¨å°†dicomå†™å…¥niiæ—¶ï¼Œä¼šæŒ‡å®šä¸€ä¸ªä¸ä¸ºnp.eye(4)çš„affineï¼Œå°±æ˜¯ä¸ºäº†transposeè¿™ä¸‰ä¸ªè½´ã€‚</li></ol></li><li><p>model</p><ol><li><p>team paper \<vertebrae localization="" and="" segmentation="" with="" spatialconfiguration-net="" u-net\=""></vertebrae></p><ul><li><p>ä¸‰é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µï¼Œdue to large variation FOV of the datasetï¼Œç²—åˆ†å‰²å®šä½è„ŠæŸ±ä½ç½®ï¼Œç¬¬äºŒé˜¶æ®µï¼Œhigher resolutionå¤šç±»åˆ«å…³é”®ç‚¹å®šä½centerï¼Œè·å¾—each located vertebraï¼Œç¬¬ä¸‰é˜¶æ®µï¼ŒäºŒç±»åˆ†å‰²for each located vertebraã€‚</p></li><li><p>keywordsï¼š1. uniform voxel spacingï¼šä¸è¦éšæ„resizeï¼Œtodo: trilinear interpï¼›2. on-the-fly data augmentationï¼šusing SimpleITK</p></li><li><p>ç¬¬ä¸€é˜¶æ®µï¼šSpine Localization</p><ul><li>Unet</li><li><strong>regress</strong> the Gaussian <strong>heatmap</strong> of spinal centerline</li><li>L2-loss</li><li>uniform voxel spacing of 8mm</li><li><p>input shapeï¼š[64,64,128]ï¼Œpadï¼Ÿ</p><p><img src="/2020/04/15/verseg/stage1.png" width="50%"></p></li></ul></li><li><p>ç¬¬äºŒé˜¶æ®µï¼šVertebrae Localization</p><ul><li>SpatialConfiguration-Net</li><li>regress each located vertebraâ€˜s heatmap in individual channel</li><li>resamplingï¼šbi/tricubic interpolation</li><li>normï¼šmaxmin on the whole dataset</li><li>uniform voxel spacing of 2mm</li><li><p>input shapeï¼š[96,96,128]ï¼Œz-axis random cropï¼Œxy-plane use ROI from stage1</p><p><img src="/2020/04/15/verseg/stage2.png" width="50%"></p><p><img src="/2020/04/15/verseg/SCN.png" width="60%"></p></li></ul></li><li><p>ç¬¬ä¸‰é˜¶æ®µï¼šVertebrae Segmentation</p><ul><li>Unet</li><li>binary segment the mask of each vertebrae</li><li>sigmoid ce-loss</li><li>uniform voxel spacing of 1mm</li><li><p>input shapeï¼š[128,128,96]ï¼Œcrop origin image &amp; heatmap image based on centroids</p><p><img src="/2020/04/15/verseg/stage3.png" width="50%"></p></li></ul></li></ul></li><li><p>reference paper\<btrfly net:="" vertebrae="" labelling="" with="" energy-="" based="" adversarial="" learning="" of="" local="" spine="" prior="" \=""></btrfly></p><ul><li>æ ¸å¿ƒè´¡çŒ®ï¼š1.MIPï¼šcombines the information across reformationsï¼Œ3D to 2Dï¼Œ2. åŸºäºåˆ¤åˆ«å™¨çš„è®­ç»ƒæœºåˆ¶ï¼šencodes local spine structure as an anatomical priorï¼ŒåŠ å›ºæ¤å—é—´ç±»åˆ«&amp;ä½ç½®çš„spacial information</li></ul><p><img src="/2020/04/15/verseg/btr overview.png" width="60%"></p><ul><li>MIPï¼š<ul><li>localisation and identification rely on a large context </li><li>large receptive field</li><li>in full-body scans where spine is not spatially centred or is obstructed by the ribcage, such cases are handled with a pre-processing stage detecting the occluded spine  </li></ul></li><li><p>adversarial learningï¼š</p><ul><li>FCNç”¨äºåˆ†å‰²</li><li>AEç”¨äºè¯„ä¼°åˆ†å‰²çš„å¥½å</li><li>do not â€˜pre-trainâ€™ it (the AE)</li><li>lossï¼šan anatomically-inspired supervision instead of the usual binary adversarial supervision (vanilla GAN)</li></ul></li><li><p>å…ˆè¯´FCNâ€”â€”Btrfly Network</p><ul><li><p>å»ºæ¨¡æˆå›å½’é—®é¢˜ï¼Œæ¯ä¸ªå…³é”®ç‚¹å¯¹åº”ä¸€ä¸ªé€šé“çš„é«˜æ–¯heatmapï¼ŒèƒŒæ™¯channelä¸º$1-max_i (y_i)$</p></li><li><p>åŒè¾“å…¥åŒè¾“å‡ºï¼ˆsagittal &amp; coronalï¼‰</p></li><li><p>ä¸¤ä¸ªè§†è§’çš„feature mapåœ¨ç½‘ç»œæ·±å±‚åšäº†èåˆï¼Œto learn their inter-dependency</p></li><li><p>Batch- normalisation is used after every convolution layer, along with 20% dropout in the fused layers of Btrfly</p></li><li><p>lossï¼šl2 distance + weighted ce</p><script type="math/tex; mode=display">  L_{sag} = ||Y_{sag} - \hat{Y}_{sag}||^2 + \omega CE(softmax(Y_{sag}, softmax(\hat{Y}_{sag}))</script><p>  $\omega$ is the median frequency weighing map, boosting the learning of less frequent classes(ECB)</p><p><img src="/2020/04/15/verseg/btrfly.png" width="60%"></p></li></ul></li><li><p>å†è¯´åˆ¤åˆ«å™¨â€”â€”Energy-based adversary for encoding prior</p><ul><li><p>fully-convolutionalï¼šits predictions across voxels are independent of each other owing to the spatial invariance of convolutions</p></li><li><p>to impose the anatomical prior of the spineâ€™s shape onto the Btrfly net</p></li><li><p>look at $\hat{Y}_{sag}$ and $\hat{Y}_{cor}$ as a 3D volume and employ a 3D AE with a receptive field covering a part of the spine </p></li><li><p>$\hat{Y}_{sag}$ consists of Gaussiansï¼šless informative than an image, avoid using max-pooling by resorting to average pooling </p></li><li><p>employ spatially dilated convolution kernels</p></li><li><p>mission of AEï¼špredict the l2 distance of input and its reconstruction, it learns to discriminate by predicting a low E for real annotations, while G learns to generate annotations that would trick D </p><script type="math/tex; mode=display">  L = D(Y_x) + max(0, m-D(Y_g))\\  L_G = D(Y_g) + L_{fcn}</script><p><img src="/2020/04/15/verseg/btr gan.png" width="60%"></p></li></ul></li><li><p>inferenceï¼š</p><ul><li>The values below a threshold (T) are ignored in order to remove noisy predictions </li><li>ç”¨å¤–ç§¯ï¼Œ$\hat{Y}=\hat{Y}_{sag}\otimes\hat{Y}_{cor}$</li><li>æ¯ä¸ªchannelçš„æœ€å¤§å€¼ä½œä¸ºcentroids</li></ul></li><li><p>experiments</p><ul><li>ã€IMPORTANTã€‘10 MIPs are obtained from one 3D scan per view, each time randomly choosing half the slices of interest</li><li>å¯¹äºæ¯ä¸ªè§†è§’ï¼Œæ¯æ¬¡éšæœºæŠ½å–ä¸€åŠæ•°ç›®çš„sliceç”¨äºè®¡ç®—MIP</li></ul></li></ul></li></ol></li></ol><ul><li><p>similar local appearanceï¼š</p></li><li><p>strong spatial configurationï¼šå‡¡æ˜¯æ¶‰åŠåˆ°æ¤å—-wiseçš„ä¿¡æ¯ï¼Œä»å…¨å±€ä¿¡æ¯å…¥æ‰‹</p></li></ul>]]></content>
      
      
        <tags>
            
            <tag> challenge </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>GoogLeNetç³»åˆ—</title>
      <link href="/2020/04/13/GoogLeNet%E7%B3%BB%E5%88%97/"/>
      <url>/2020/04/13/GoogLeNet%E7%B3%BB%E5%88%97/</url>
      <content type="html"><![CDATA[<h2 id="ç»¼è¿°"><a href="#ç»¼è¿°" class="headerlink" title="ç»¼è¿°"></a>ç»¼è¿°</h2><ol><li><p>papers</p><ul><li><p>[V1] Going Deeper with Convolutions, 6.67% test error</p></li><li><p>[V2] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, 4.8% test error</p></li><li><p>[V3] Rethinking the Inception Architecture for Computer Vision, 3.5% test error</p></li><li><p>[V4] Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, 3.08% test error</p></li><li><p>[Xception] Xception: Deep Learning with Depthwise Separable Convolutions</p></li><li><p>[EfficientNet] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks </p></li><li><p>[EfficientDet] EfficientDet: Scalable and Efficient Object Detection </p></li><li><p>[EfficientNetV2] EfficientNetV2: Smaller Models and Faster Training </p></li></ul></li></ol><ol><li><p>å¤§ä½“æ€è·¯</p><ul><li>inception V1ï¼šæ‰“ç ´ä¼ ç»Ÿçš„conv blockï¼Œè®¾è®¡äº†<strong>Inception block</strong>ï¼Œå°†1*1ã€3*3ã€5*5çš„å·ç§¯ç»“æœconcatï¼Œå¢åŠ ç½‘ç»œå®½åº¦</li><li>inception V2ï¼šåŠ å…¥äº†<strong>BNå±‚</strong>ï¼Œå‡å°‘Internal Covariate Shiftï¼Œç”¨ä¸¤ä¸ª3*3æ›¿ä»£5*5ï¼Œé™ä½å‚æ•°é‡</li><li>inception V3ï¼šæå‡º<strong>åˆ†è§£Factorization</strong>ï¼Œ7*7æ”¹æˆ7*1å’Œ1*7ï¼Œå‚æ•°å‡å°‘åŠ é€Ÿè®¡ç®—ï¼Œå¢åŠ ç½‘ç»œæ·±åº¦å’Œéçº¿æ€§</li><li>inception V4ï¼šç»“åˆ<strong>Residual</strong> Connection</li><li>Xceptionï¼šé’ˆå¯¹inception V3çš„åˆ†è§£ç»“æ„çš„æ”¹è¿›ï¼Œä½¿ç”¨<strong>å¯åˆ†ç¦»å·ç§¯</strong></li><li>EfficientNetï¼šä¸»è¦ç ”ç©¶model scalingï¼Œé’ˆå¯¹ç½‘ç»œæ·±åº¦ã€å®½åº¦ã€å›¾åƒåˆ†è¾¨ç‡ï¼Œæœ‰æ•ˆåœ°æ‰©å±•CNN</li><li>EfficientDetï¼šå°†EfficientNetä»åˆ†ç±»ä»»åŠ¡æ‰©å±•åˆ°ç›®æ ‡æ£€æµ‹ä»»åŠ¡</li></ul></li></ol><h2 id="review"><a href="#review" class="headerlink" title="review"></a>review</h2><ol><li><p>review0122ï¼šconv-BNå±‚åˆå¹¶è¿ç®—</p><ul><li><p>referenceï¼š<a href="https://nenadmarkus.com/p/fusing-batchnorm-and-conv/" target="_blank" rel="noopener">https://nenadmarkus.com/p/fusing-batchnorm-and-conv/</a></p></li><li><p>freezed BNå¯ä»¥çœ‹æˆ1x1çš„å·ç§¯è¿ç®—</p></li><li><p>ä¸¤ä¸ªçº¿æ€§è¿ç®—æ˜¯å¯ä»¥åˆå¹¶çš„</p></li><li><p>given $W_{conv} \in R^{C<em>C_{prev}</em>k<em>k}$ï¼Œ$b_{conv} \in R^C $ï¼Œ$W_{bn}\in R^{C</em>C}$ï¼Œ$b_{bn}\in R^C$</p><script type="math/tex; mode=display">  F = W_{bn} * (W_{conv} * F_{prev} + b_{conv}) + b_{bn}</script></li></ul></li></ol><h2 id="V1-Going-deeper-with-convolutions"><a href="#V1-Going-deeper-with-convolutions" class="headerlink" title="V1: Going deeper with convolutions"></a>V1: Going deeper with convolutions</h2><ol><li><p>åŠ¨æœº</p><ul><li>improved utilization of the computing resources</li><li>increasing the depth and width of the network while keeping the computational budget</li></ul></li><li><p>è®ºç‚¹</p><ul><li>the recent trend has been to increase the number of layers and layer size, while using dropout to address the problem of overfitting </li><li>major bottleneckï¼šlarge networkï¼Œlarge number of paramsï¼Œlimited datasetï¼Œoverfitting</li><li>methods use filters of different sizes in order to handle multiple scales </li><li>NiN use  1x1 convolutional layers to easily integrate in the current CNN pipelines</li><li>we use 1x1 convs with a dual purpose of dimension reduction  </li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>Architectural</p><ul><li>1x1 conv+ReLU for compute reductions  </li><li><p>an alternative parallel pooling path since pooling operations have been essential for the success </p><p><img src="/2020/04/13/GoogLeNetç³»åˆ—/inceptionv1.png" width="80%"></p></li><li><p>overall architecture :</p><p><img src="/2020/04/13/GoogLeNetç³»åˆ—/googlenet.png" width="80%"></p></li><li><p>ç»†èŠ‚ï¼š</p><ul><li><p>rectified linear activation</p></li><li><p>mean subtraction</p></li><li><p>a move from fully connected layers to average pooling improves acc</p></li><li><p>the use of dropout remained essential  </p></li><li><p>adding auxiliary classifiers(on 4c&amp;4d) with a discount weight  </p><ul><li>5x5 avg pool, stride 3</li><li>1x1 conv+relu, 128 filters</li><li>1024 fc+relu</li><li>70% dropout</li><li><p>1000 fc+softmax</p><p><img src="/2020/04/13/GoogLeNetç³»åˆ—/auxilary.png" width="60%"></p></li></ul></li><li><p>asynchronous stochastic gradient descent with 0.9 momentum </p></li><li><p>fixed learning rate schedule (de- creasing the learning rate by 4% every 8 epochs </p></li><li><p>photometric distortions useful to combat overfitting </p></li><li><p>random interpolation methods for resizing </p></li></ul></li></ul></li></ul></li></ol><h2 id="V2-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift"><a href="#V2-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift" class="headerlink" title="V2: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"></a>V2: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</h2><ol><li><p>åŠ¨æœº</p><ul><li>use much higher learning rates </li><li>be less careful about initialization </li><li>also acts as a regularizer,  eliminating the need for Dropout</li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>SGDï¼šoptimizes the parameters $\theta$ of the network, so as to minimize the loss </p><script type="math/tex; mode=display">  \theta = argmin_{\theta} \frac{1}{N}\sum_{i=1}^N loss(x_i, \theta)</script><p>  æ¢¯åº¦æ›´æ–°ï¼š<script type="math/tex">\theta_{next-timestep} = \theta - \alpha \frac{\partial[\frac{1}{N}\sum_Nloss(\theta)]}{\partial \theta}</script>ï¼Œ$x_i$ is the full set</p><p>  batch approximationï¼šuse $\frac{1}{m} \sum_M \frac{\partial loss(\theta)}{\partial \theta}$ï¼Œ$x_i$ is the mini-batch set</p><ul><li>quality improves as the batch size increases </li><li>computation over a batch is much more efficient than m computations for individual examples</li><li>the learning rate and the initial values require careful tuning </li></ul></li><li><p>Internal covariate shift</p><ul><li>the input distribution of the layers changes</li><li>consider a gradient descent step aboveï¼Œ$x$çš„æ•°æ®åˆ†å¸ƒæ”¹å˜äº†ï¼Œ$\theta$å°±è¦ç›¸åº”åœ° readjust to compensate for the change in the distribution of x</li></ul></li><li><p>activation</p><ul><li>å¯¹äºç¥ç»å…ƒ$z = sigmoid(Wx+b)$ï¼Œå‰é¢å±‚çš„å‚æ•°å˜åŒ–ï¼Œå¾ˆå®¹æ˜“å¯¼è‡´å½“å‰ç¥ç»å…ƒçš„å“åº”å€¼ä¸åœ¨æœ‰æ•ˆæ´»åŠ¨åŒºé—´ï¼Œä»è€Œå¯¼è‡´è¿‡äº†å½“å‰æ¿€æ´»å‡½æ•°ä»¥åæ¢¯åº¦æ¶ˆå¤±ï¼Œslow down the convergence </li><li>In practice, using ReLU &amp; careful initialization  &amp; small learning rates</li><li>å¦‚æœæˆ‘ä»¬èƒ½ä½¿å¾—the distribution of nonlinearity inputs remains more stable as the network trainsï¼Œå°±ä¸ä¼šå‡ºç°ç¥ç»å…ƒé¥±å’Œçš„é—®é¢˜</li></ul></li><li><p>whitening</p><ul><li>å¯¹training setçš„é¢„å¤„ç†ï¼šlinearly transformed to have zero means and unit variances, and decorrelated</li><li>ä½¿å¾—è¾“å…¥æ•°æ®çš„åˆ†å¸ƒä¿æŒç¨³å®šï¼Œnormal distribution</li><li>åŒæ—¶å»é™¤äº†æ•°æ®é—´çš„ç›¸å…³æ€§</li></ul></li><li><p>batch normalization</p><ul><li>fixes the means and variances of layer inputs </li><li>reducing the dependence of gradients on the scale of the parameters or of their initial values</li><li>makes it possible to use saturating nonlinearities </li></ul></li></ul></li></ol><pre><code>    * full whitening of each layer is costly    * so we normalize each layer independently, full set--&gt; mini-batch    * standard normal distributionå¹¶ä¸æ˜¯æ¯ä¸ªç¥ç»å…ƒæ‰€éœ€çš„ï¼ˆå¦‚identity transformï¼‰ï¼šintroduce, for each activation $x(k)$ , a pair of parameters $\gamma(k)$, $\beta(k)$, which scale and shift the normalized value to maintain the representation ability of the neuron    â€‹        * for convolutional networks        * we add the BN transform immediately before the nonlinearity, $z = g(Wx+b)$ to $z = g(BN(Wx))$        * since we normalize $Wx+b$, the bias b can be ignored          * obey the convolutional propertyâ€”â€”different elements of the same feature map, at different locations, are normalized in the same way        * We learn a pair of parameters $\gamma(k)$ and $\beta(k)$ **per feature map**, rather than per activation    * properties        * back-propagation through a layer is unaffected by the scale of its parameters        * Moreover, larger weights lead to smaller gradients, thus stabilizing the parameter growth        * regularizes the modelï¼šå› ä¸ºç½‘ç»œä¸­mini-batchçš„æ•°æ®ä¹‹é—´æ˜¯æœ‰äº’ç›¸å½±å“çš„è€Œéindependentçš„</code></pre><ol><li><p>æ–¹æ³•</p><ul><li><p>batch normalization</p><ul><li>full whitening of each layer is costly</li><li>so we normalize each layer independently, full setâ€”&gt; mini-batch</li><li><p>standard normal distributionå¹¶ä¸æ˜¯æ¯ä¸ªç¥ç»å…ƒæ‰€éœ€çš„ï¼ˆå¦‚identity transformï¼‰ï¼šintroduce, for each activation $x(k)$ , a pair of parameters $\gamma(k)$, $\beta(k)$, which scale and shift the normalized value to maintain the representation ability of the neuron</p><p><img src="/2020/04/13/GoogLeNetç³»åˆ—/BN.png" width="40%"></p></li></ul></li><li><p>bpï¼š</p><p>  <img src="/2020/04/13/GoogLeNetç³»åˆ—/BNbp.png" width="40%"></p></li><li><p>inferenceé˜¶æ®µï¼š</p><ul><li>é¦–å…ˆä¸¤ä¸ªå¯å­¦ä¹ å‚æ•°$\gamma$å’Œ$\beta$æ˜¯å®šä¸‹æ¥çš„</li><li><p>è€Œå‡å€¼å’Œæ–¹å·®ä¸å†é€šè¿‡è¾“å…¥æ•°æ®æ¥è®¡ç®—ï¼Œè€Œæ˜¯è½½å…¥è®­ç»ƒè¿‡ç¨‹ä¸­ç»´æŠ¤çš„å‚æ•°ï¼ˆmoving averagesï¼‰</p><p><img src="/2020/04/13/GoogLeNetç³»åˆ—/BNinfer.png" width="40%"></p><p>â€‹    </p></li></ul></li><li><p>for convolutional networks</p><ul><li><p>we add the BN transform immediately before the nonlinearity, $z = g(Wx+b)$ to $z = g(BN(Wx))$</p></li><li><p>since we normalize $Wx+b$, the bias b can be ignored  </p></li><li>obey the convolutional propertyâ€”â€”different elements of the same feature map, at different locations, are normalized in the same way</li><li>We learn a pair of parameters $\gamma(k)$ and $\beta(k)$ <strong>per feature map</strong>, rather than per activation</li></ul></li><li><p>properties</p><ul><li>back-propagation through a layer is unaffected by the scale of its parameters</li><li>Moreover, larger weights lead to smaller gradients, thus stabilizing the parameter growth</li><li>regularizes the modelï¼šå› ä¸ºç½‘ç»œä¸­mini-batchçš„æ•°æ®ä¹‹é—´æ˜¯æœ‰äº’ç›¸å½±å“çš„è€Œéindependentçš„</li></ul></li></ul></li></ol><h2 id="V3-Rethinking-the-Inception-Architecture-for-Computer-Vision"><a href="#V3-Rethinking-the-Inception-Architecture-for-Computer-Vision" class="headerlink" title="V3: Rethinking the Inception Architecture for Computer Vision"></a>V3: Rethinking the Inception Architecture for Computer Vision</h2><ol><li><p>åŠ¨æœº</p><ul><li>go deeper and widerï¼š<ul><li>enough labeled data</li><li>computational efficiency</li><li>parameter count</li></ul></li><li>to scale up networks<ul><li>utilizing the added computation as efficiently</li><li>give general design principles and optimization ideas  <ul><li>factorized convolutions</li><li>aggressive regularization</li></ul></li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>GoogleNet does not provide a clear description about the contributing factors that lead to the various design </li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>General Design Principles </p><ul><li>Avoid representational bottlenecksï¼šç‰¹å¾å›¾å°ºå¯¸åº”è¯¥gently decreaseï¼Œresolutionçš„ä¸‹é™å¿…é¡»ä¼´éšç€channelæ•°çš„ä¸Šå‡ï¼Œ<a href="https://www.zybuluo.com/Team/note/1332528" target="_blank" rel="noopener">é¿å…ä½¿ç”¨max poolingå±‚è¿›è¡Œä¸‹é‡‡æ ·ï¼Œå› ä¸ºè¿™æ ·å¯¼è‡´ä¿¡æ¯æŸå¤±è¾ƒå¤§</a></li><li>Higher dimensional representations are easier to process locally within a network. Increasing the activa- tions per tile in a convolutional network allows for more disentangled features. The resulting networks will train fasterï¼šå‰åŠå¥æ‡‚äº†ï¼Œhigh-resoçš„ç‰¹å¾å›¾focusåœ¨å±€éƒ¨ä¿¡æ¯ï¼ŒååŠå¥ä¸æ‡‚ï¼Œæ ¹æ®ä¸Šä¸€ç¯‡paperï¼Œç”¨äº†batch normä»¥åï¼Œscale upç¥ç»å…ƒä¸å½±å“bpï¼ŒåŒæ—¶ä¼šlead to smaller gradientsï¼Œä¸ºå•¥èƒ½åŠ é€Ÿï¼Ÿ</li><li>Spatial aggregation can be done over lower dimensional embeddingsï¼šadjacent unitä¹‹é—´æœ‰strong correlationï¼Œæ‰€ä»¥å¯ä»¥reduce the dimension of the input representation before the spatial aggregationï¼Œä¸ä¼šæœ‰å¤ªå¤§çš„ä¿¡æ¯æŸå¤±ï¼Œå¹¶ä¸”promotes faster learning</li><li>The computational budget should therefore be distributed in a balanced way between the depth and width of the network.</li></ul></li><li><p>Factorizing Convolutions Filter Size </p><ul><li>into smaller convolutions <ul><li>å¤§filteréƒ½å¯ä»¥æ‹†è§£æˆå¤šä¸ª3x3</li><li>å•çº¯å»ç­‰ä»·çº¿æ€§åˆ†è§£å¯ä»¥ä¸ä½¿ç”¨éçº¿æ€§activationï¼Œä½†æ˜¯æˆ‘ä»¬ä½¿ç”¨äº†batch normï¼ˆincrease variatyï¼‰ï¼Œæ‰€ä»¥è§‚å¯Ÿåˆ°ä½¿ç”¨ReLUä»¥åæ‹Ÿåˆæ•ˆæœæ›´å¥½</li></ul></li><li>into Asymmetric Convolutions <ul><li>n*nçš„filteræ‹†è§£æˆ1*nå’Œn*1</li><li>this factorization does not work well on early layers, but gives very good results on medium grid-sizes (ranges between 12 and 20, using 1x7 and 7x1</li></ul></li></ul></li><li><p>Utility of Auxiliary Classifiers </p><ul><li>did not result in improved convergence early in the trainingï¼šè®­ç»ƒå¼€å§‹é˜¶æ®µæ²¡å•¥ç”¨ï¼Œå¿«æ”¶æ•›æ—¶å€™æœ‰ç‚¹ç‚¹accæå‡</li><li>removal of the lower auxiliary branch did not have any adverse effect on the final qualityï¼šæ‹¿æ‰å¯¹æœ€ç»ˆç»“æœæ²¡å½±å“</li><li>æ‰€ä»¥æœ€åˆçš„è®¾æƒ³ï¼ˆhelp evolving the low-level featuresï¼‰ æ˜¯é”™çš„ï¼Œä»…ä»…act as regularizerï¼Œauxiliary headé‡Œé¢åŠ ä¸Šbatch normä¼šä½¿å¾—æœ€ç»ˆç»“æœbetter</li></ul></li><li><p>Efficient Grid Size Reductionä¸‹é‡‡æ ·æ¨¡å—ä¸å†ä½¿ç”¨maxpooling</p><ul><li><p>dxdxk feature map expand to (d/2)x(d/2)x2kï¼š</p><ul><li>1x1x2k convï¼Œstride2 poolï¼škxdxdx2k computation</li><li>1x1x2k stride2 convï¼škx(d/2)x(d/2)x2k computationï¼Œè®¡ç®—é‡ä¸‹é™ï¼Œä½†æ˜¯è¿åprinciple1</li><li>parallel stride P and C blocksï¼škx(d/2)x(d/2)xk computationï¼Œç¬¦åˆprinciple1:reduces the grid-size while expands the filter banks</li></ul><p><img src="/2020/04/13/GoogLeNetç³»åˆ—/bottleneck1.png" width="40%">  <img src="/2020/04/13/GoogLeNetç³»åˆ—/bottleneck2.png" width="40%"></p></li></ul></li><li><p>Inception-v3</p><ul><li>å¼€å¤´çš„7x7convå·²ç»æ¢æˆäº†å¤šä¸ª3x3</li><li>ä¸­é—´å±‚featuremapé™ç»´åˆ°17x17çš„æ—¶å€™ï¼Œå¼€å§‹ç”¨Asymmetric Factorization block</li><li>åˆ°8x8çš„æ—¶å€™ï¼Œåšäº†expanding the filter bank outputs</li></ul><p><img src="/2020/04/13/GoogLeNetç³»åˆ—/v3.png" width="70%"></p><p><img src="/2020/04/13/GoogLeNetç³»åˆ—/inceptionv3.png" width="40%"></p></li><li><p>Label Smoothing ï¼ˆ<a href="https://zhuanlan.zhihu.com/p/116466239ï¼‰" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/116466239ï¼‰</a></p><ul><li><p>used the uniform distribution  $u(k)=1/K$</p><script type="math/tex; mode=display">  q(k) = (1-\epsilon)\delta(k) + \frac{\epsilon}{K}</script></li><li><p>å¯¹äºsoftmaxå…¬å¼ï¼š$p(k)=\frac{exp(y_k)}{\sum exp(y_i)}$ï¼Œè¿™ä¸ªlossè®­ç»ƒçš„ç»“æœå°±æ˜¯$y_k$æ— é™è¶‹è¿‘äº1ï¼Œå…¶ä»–$y_i$æ— é™è¶‹è¿‘äº0ï¼Œ</p></li><li><p>äº¤å‰ç†µlossï¼š$ce=\sum -y_{gt}log(y_k)$ï¼ŒåŠ äº†label smoothingä»¥åï¼Œlossä¸Šå¢åŠ äº†é˜´æ€§æ ·æœ¬çš„regularizationï¼Œæ­£è´Ÿæ ·æœ¬çš„æœ€ä¼˜è§£è¢«é™å®šåœ¨æœ‰é™å€¼ï¼Œé€šè¿‡æŠ‘åˆ¶æ­£è´Ÿæ ·æœ¬è¾“å‡ºå·®å€¼ï¼Œä½¿å¾—ç½‘ç»œæœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p></li></ul></li></ul></li></ol><h2 id="V4-Inception-v4-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning"><a href="#V4-Inception-v4-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning" class="headerlink" title="V4: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning"></a>V4: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</h2><ol><li><p>åŠ¨æœº</p><ul><li>residualï¼šwhether there are any benefit in combining the Inception architecture with residual connections</li><li>inceptionV4ï¼šsimplify the inception blocks</li></ul></li><li><p>è®ºç‚¹</p><ul><li>residual connections seems to improve the training speed greatlyï¼šä½†æ˜¯æ²¡æœ‰ä¹Ÿèƒ½è®­ç»ƒæ·±å±‚ç½‘ç»œ</li><li>made uniform choices for the Inception blocks for each grid size <ul><li>Inception-A for 35x35</li><li>Inception-B for 17x17</li><li>Inception-C for 8x8</li></ul></li><li>for residual versions  <ul><li>use cheaper Inception blocks for residual versionsï¼šç®€åŒ–moduleï¼Œå› ä¸ºidentityéƒ¨åˆ†ï¼ˆç›´æ¥ç›¸è¿çš„çº¿ï¼‰æœ¬èº«åŒ…å«ä¸°å¯Œçš„ç‰¹å¾ä¿¡æ¯</li><li>æ²¡æœ‰ä½¿ç”¨pooling</li><li>replace the filter concatenation stage of the Inception architecture with residual connectionsï¼šåŸæ¥blocké‡Œé¢çš„concatenationä¸»ä½“æ”¾åœ¨æ®‹å·®pathä¸­</li><li>Each Inception block is followed by filter-expansion layer (1 Ã— 1 convolution without activation) to match the depth of the input for additionï¼šç›¸åŠ ä¹‹å‰ä¿è¯channelæ•°ä¸€è‡´</li><li>used batch-normalization only on top of the traditional layers, but not on top of the summationsï¼šæµªè´¹å†…å­˜</li><li>number of filters exceeded 1000 causes instabilities </li><li>scaling down the residuals before adding by factors between 0.1 and 0.3ï¼šæ®‹å·®é€šé“å“åº”å€¼ä¸è¦å¤ªå¤§</li></ul></li></ul></li><li><p>blocks</p><ul><li><p>V4 ABCï¼š</p><p>  <img src="/2020/04/13/GoogLeNetç³»åˆ—/V4A.png" width="30%">  <img src="/2020/04/13/GoogLeNetç³»åˆ—/V4B.png" width="30%">    <img src="/2020/04/13/GoogLeNetç³»åˆ—/V4C.png" width="30%"></p></li><li><p>Res ABCï¼š</p><p>  <img src="/2020/04/13/GoogLeNetç³»åˆ—/ResA.png" width="25%">     <img src="/2020/04/13/GoogLeNetç³»åˆ—/ResB.png" width="25%">        <img src="/2020/04/13/GoogLeNetç³»åˆ—/ResC.png" width="25%"></p></li></ul></li></ol><h2 id="Xception-Deep-Learning-with-Depthwise-Separable-Convolutions"><a href="#Xception-Deep-Learning-with-Depthwise-Separable-Convolutions" class="headerlink" title="Xception: Deep Learning with Depthwise Separable Convolutions"></a>Xception: Deep Learning with Depthwise Separable Convolutions</h2><ol><li><p>åŠ¨æœº</p><ul><li>Inception modules have been replaced with depthwise separable convolutions</li><li>significantly outperforms Inception V3 on a larger dataset</li><li>due to more efficient use of model parameters</li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>early LeNet-style models </p><ul><li>simple stacks of convolutions for feature extraction and max-pooling operations for spatial sub-sampling </li><li>increasingly deeper</li></ul></li><li><p>complex blocks</p><ul><li>Inception modules inspired by NiN</li><li>be capable of learning richer repre- sentations with less parameters </li></ul></li><li><p>The Inception hypothesis</p><ul><li>a single convolution kernel is tasked with simultaneously mapping cross-channel correlations and spatial correlations</li><li>while Inception factors it into a series of operations that independently look at cross-channel correlations(1x1 convs) and at spatial correlations(3x3/5x5 convs)</li><li><p>suggesting that cross-channel correlations and spatial correlations are sufficiently decoupled that it is preferable not to map them jointly</p><p><img src="/2020/04/13/GoogLeNetç³»åˆ—/inception.png" width="45%"></p></li></ul></li><li><p>inception blockå…ˆç”¨1x1çš„convå°†åŸè¾“å‡ºæ˜ å°„åˆ°3-4ä¸ªlower spaceï¼ˆcross-channel correlationsï¼‰ï¼Œç„¶ååœ¨è¿™äº›å°çš„3d spacesä¸Šåšregular convï¼ˆmaps all correlations ï¼‰â€”â€”è¿›ä¸€æ­¥å‡è®¾ï¼Œå½»åº•è§£è€¦ï¼Œç¬¬äºŒæ­¥åªåšspatial correlations</p><p><img src="/2020/04/13/GoogLeNetç³»åˆ—/extreme.png" width="45%"></p></li><li><p>main differences between <strong>â€œextreme â€ Inception</strong> and <strong>depthwise separable convolution</strong></p><ul><li>order of the operationsï¼š1x1 first or latter</li><li>non-linearityï¼šdepthwise separable convolutions are usually implemented without non-linearitiesã€QUESTIONï¼šè¿™å’ŒMobileNeté‡Œé¢è¯´çš„ä¸ä¸€æ ·å•Šï¼ŒMé‡Œé¢çš„depthwiseä¹Ÿæ˜¯æ¯å±‚éƒ½å¸¦äº†BNå’ŒReLUçš„ã€‘</li></ul></li></ul></li><li><p>è¦ç´ </p><ul><li>Convolutional neural networks </li><li>The Inception family  </li><li>Depthwise separable convolutions</li><li>Residual connections</li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>architecture</p><ul><li>a linear stack of depthwise separable convolution layers with residual connections</li><li>all conv are followed by BN</li><li><p>kerasçš„separableConvå’ŒdepthwiseConvï¼šå‰è€…ç”±åè€…åŠ ä¸Šä¸€ä¸ªpointwiseConvç»„æˆï¼Œ<strong>æœ€åæœ‰activationï¼Œä¸­é—´æ²¡æœ‰</strong></p><p><img src="/2020/04/13/GoogLeNetç³»åˆ—/xception.png" width="80%;"></p></li><li><p>cmp</p><ul><li>Xception and Inception V3 have nearly the same number of parameters </li><li>marginally better  on ImageNet</li><li>much larger performance increasement on JFT </li><li>Residual connections are clearly essential in helping with convergence, both in terms of speed and final classification performance. </li><li><p><strong>Effect of intermediate activationï¼š</strong>the absence of any non-linearity leads to both faster convergence and better final performance </p><p><img src="/2020/04/13/GoogLeNetç³»åˆ—/intermediate.png" width="40%;"></p></li></ul></li></ul></li></ul></li></ol><h2 id="EfficientNet-Rethinking-Model-Scaling-for-Convolutional-Neural-Networks"><a href="#EfficientNet-Rethinking-Model-Scaling-for-Convolutional-Neural-Networks" class="headerlink" title="EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"></a>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</h2><ol><li><p>åŠ¨æœº</p><ul><li>common senseï¼šscaled up the network for better accuracy </li><li>we systematically study model scaling</li><li>and identify that carefully <strong>balancing network depth, width, and resolution</strong> can lead to better performance</li><li>propose a new scaling methodï¼šusing <strong>compound coefficient</strong> to uniformly scale all dimensions of depth/width/resolution <ul><li>on MobileNets and ResNet</li><li>a new baseline network family EfficientNets</li></ul></li><li>much better accuracy and efficiency </li></ul></li><li><p>è®ºç‚¹</p><ul><li>previous work scale up one of the three dimensions  <ul><li>depthï¼šmore layers</li><li>widthï¼šmore channels</li><li>image resolutionï¼šhigher resolution</li></ul></li><li><strong>arbitrary scaling</strong> requires tedious manual tuning and often yields sub-optimal accuracy and efficiency </li><li><p><strong>uniformly scaling</strong>ï¼šOur empirical study shows that it is critical to balance all dimensions of network width/depth/resolution, and surprisingly such balance can be achieved by simply scaling each of them with constant ratio. </p><p><img src="/2020/04/13/GoogLeNetç³»åˆ—/modelscaling.png" width="70%;"></p></li><li><p><strong>neural architecture searchï¼š</strong>becomes increasingly popular in designing efficient mobile-size ConvNets </p></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>problem formulation</p><ul><li><p>ConvNetsï¼š$N = \bigodot_{i=1â€¦s} F_i^{L_i}(X_{<h_i,w_i,c_i>})$, s for stage, L for repeat times, F for function</h_i,w_i,c_i></p></li><li><p>simplify the design problem </p><ul><li>fixing $F_i$</li><li>all layers must be scaled uniformly with constant ratio </li></ul></li><li><p>an optimization problemï¼šd for depth coefficients, w for width coefficients, r for resolution coefficients </p><script type="math/tex; mode=display">  max_{d,w,r} \ \ \ Accuracy(N(d,w,r))\\  s.t. \ \ \ N(d,w,r) = \bigodot_{i=1...s} F_i^{d*L_i}(X_{<r*H_i,r*W_i,w*C_i>})</script></li></ul></li><li><p>observation 1</p><ul><li>Scaling up any dimension of network (width, depth, or resolution) improves accuracy, but the accuracy gain diminishes for bigger models. å‡†ç¡®ç‡éƒ½ä¼šæå‡ï¼Œæœ€ç»ˆéƒ½ä¼šé¥±å’Œ</li><li>depthï¼šdeeper ConvNet can capture richer and more complex features </li><li>widthï¼šwider networks tend to be able to capture more fine-grained features and are easier to train ï¼ˆcommonly used for small size modelsï¼‰ä½†æ˜¯æ·±åº¦å’Œå®½åº¦æœ€å¥½åŒ¹é…ï¼Œä¸€å‘³åŠ å®½shallow networkä¼šè¾ƒéš¾æå–é«˜çº§ç‰¹å¾</li><li>resolutionï¼šhigher resolution input can potentially capture more fine-grained patterns   </li></ul><p><img src="/2020/04/13/GoogLeNetç³»åˆ—/dimensions.png" width="60%;"></p></li><li><p>observation 2</p><ul><li><p>compound scalingï¼šit is critical to balance all dimensions of network width, depth, and resolution</p></li><li><p>different scaling dimensions are not independent è¾“å…¥æ›´é«˜çš„resolutionï¼Œå°±éœ€è¦æ›´æ·±çš„ç½‘ç»œï¼Œä»¥è·å–æ›´å¤§çš„æ„Ÿå—é‡ï¼ŒåŒæ—¶è¿˜éœ€è¦æ›´å®½çš„ç½‘ç»œï¼Œä»¥æ•è·æ›´å¤šçš„ç»†ç²’åº¦ç‰¹å¾</p></li><li><p>compound coefficient $\phi$ï¼š</p><script type="math/tex; mode=display">  d = \alpha ^ \phi\\  w = \beta ^ \phi\\  r = \gamma ^ \phi\\  s.t. \alpha * \beta^2 * \gamma^2 \approx 2, \ \alpha\geq1,\ \beta\geq1,\ \gamma\geq1</script><ul><li>$\alpha, \beta, \gamma$ are constants determined by a small grid search, controling the assign among the 3 dimensions [d,w,r]</li><li>$\phi$ controls how many more resources are available for model scaling</li><li>the total FLOPS will approximately increase by $2^\phi$</li></ul></li></ul></li><li><p>efficientNet architecture </p><ul><li><p>having a good baseline network is also critical</p></li><li><p>thus we developed a new <strong>mobile-size baseline</strong> called EfficientNet by leveraging a <strong>multi-objective neural architecture search</strong> that <strong>optimizes both accuracy and FLOPS</strong> </p><p>  <img src="/2020/04/13/GoogLeNetç³»åˆ—/efficientB0.png" width="40%;"></p></li><li><p>compound scalingï¼šfix $\phi=1$ and <strong>grid search</strong> $\alpha, \beta, \gamma$, fix $\alpha, \beta, \gamma$ and use different $\phi$</p></li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li><p>on MobileNets and ResNets </p><ul><li>compared to other single-dimension scaling methods</li><li>compound scaling method improves the accuracy on all </li></ul></li><li><p>on EfficientNet </p><ul><li>model with compound scaling tends to focus on more relevant regions with more object details</li><li><p>while other models are either lack of object details or unable to capture all objects in the images</p><p><img src="/2020/04/13/GoogLeNetç³»åˆ—/activationMap.png" width="60%;"></p></li></ul></li></ul></li><li><p>implementing details</p><ul><li>RMSProp: decay=0.9, momentum(rho)=0.9ï¼Œtpuä¸Šä½¿ç”¨lars</li><li>BN: momentum=0.99</li><li>weight decay = 1e-5</li><li>lr: initial=0.256, decays by 0.97 every 2.4 epochs</li><li>SiLU activation</li><li>AutoAugment</li><li>Stochastic depth: survive_prob = 0.8</li><li>dropout rate: 0.2 to 0.5 for B0 to B7</li></ul></li></ol><h2 id="EfficientDet-Scalable-and-Efficient-Object-Detection"><a href="#EfficientDet-Scalable-and-Efficient-Object-Detection" class="headerlink" title="EfficientDet: Scalable and Efficient Object Detection"></a>EfficientDet: Scalable and Efficient Object Detection</h2><ol><li><p>åŠ¨æœº</p><ul><li>model efficiency</li><li>for object detectionï¼šbased on <strong>one-stage</strong> detector  </li><li>ç‰¹å¾èåˆï¼špropose a weighted bi-directional feature pyramid network (BiFPN)</li><li>ç½‘ç»œrescaleï¼šuniformly scales the resolution, depth, and width for all backbone</li><li>achieve better accuracy with much fewer parameters and FLOPs </li><li>also test on Pascal VOC 2012 semantic segmentation</li></ul></li><li><p>è®ºç‚¹</p><ul><li>previous work tends to achieve better efficiency by sacrificing accuracy</li><li>previous work fuse feature at different resolutions by simply summing up without distinction </li><li>EfficientNet <ul><li>backboneï¼šcombine EfficientNet backbones with our propose BiFPN  </li><li>scale upï¼šjointly scales up the resolution/depth/width for all <strong>backbone, feature network, box/class prediction network</strong></li></ul></li><li>Existing object detectors <ul><li>two-stageï¼šhave a region-of-interest proposal step  </li><li>one-stageï¼šhave not, use predefined anchors </li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>BiFPNï¼šefficient bidirectional cross-scale connec- tions and weighted feature fusion </p><ul><li><p>FPNï¼šlimitæ˜¯åªæœ‰top-bottomä¸€æ¡information flow</p></li><li><p>PANetï¼šåŠ ä¸Šäº†ä¸€æ¡bottom-up pathï¼Œbetter accuracyä½†æ˜¯more parameters and computations </p></li><li><p>NAS-FPNï¼šåŸºäºç½‘ç»œæœç´¢å‡ºçš„ç»“æ„ï¼Œirregular and difficult to interpret or modify </p></li><li><p>BiFPN</p><ul><li>remove those nodes that only have one input edgeï¼šåªæœ‰ä¸€æ¡è¾“å…¥çš„èŠ‚ç‚¹ï¼Œæ²¡åšåˆ°ä¿¡æ¯èåˆ</li><li>add an extra edge from the original input to output node if they are at the same levelï¼šfuse more features without adding much cost </li><li><p>repeat blocks</p><p><img src="/2020/04/13/GoogLeNetç³»åˆ—/fpn.png" width="80%;"></p></li></ul></li><li><p>Weighted Feature Fusion </p><ul><li>since different input features are at different resolutions, they usually contribute to the output feature unequally</li><li>learnable weight that can be a scalar (per-feature), a vector (per-channel), or a multi-dimensional tensor (per-pixel)</li><li><p>weight normalization  </p><ul><li>Softmax-basedï¼š$O=\sum_i \frac{e^{w_i}}{\sum_j e^{w_j}}*I_i$</li><li>Fast normalizedï¼š$O=\sum_i \frac{w_i}{\epsilon + \sum_j w_j}*I_i$ï¼ŒRelu is applied after each $w_i$ to keep non-negative</li></ul><script type="math/tex; mode=display">P_6^{td} = Conv(\frac{w_1P_6^{in}+w_2Resize(P_7^{in})}{\epsilon+w_1+w_2})\\P_6^{out} = Conv(\frac{w_1P_6^{in}+w_2P_6^{td}+w_3Resize(P_5^{out})}{\epsilon+w_1+w_2+w_3})</script></li></ul></li><li><p>EfficientDet </p><ul><li>ImageNet-pretrained Effi- cientNets as the backbone </li><li>BiFPN serves as the feature network</li><li><p>the fused features(level 3-7) are fed to a class and box network respectively </p><p><img src="/2020/04/13/GoogLeNetç³»åˆ—/efficientDet.png" width="70%;"></p></li><li><p>compound scaling </p><ul><li><p>backboneï¼šreuse the same width/depth scaling coefficients of EfficientNet-B0 to B6 </p></li><li><p>feature networkï¼š</p><ul><li>depth(layers)ï¼š$D=3+\phi$</li><li>width(channes)ï¼š$W=64 \cdot (1.35^{\phi}) $</li></ul></li><li><p>box/class prediction networkï¼š</p><ul><li>depthï¼š$D=3+[\phi/3]$</li><li>widthï¼šsame as FPN</li></ul></li><li><p>resolution</p><ul><li>use feature 3-7ï¼šmust be dividable by $2^7$</li><li>$R=512+128*\phi$</li></ul></li><li><p>EfficientDet-D0 ($\phi=0$) to D7 ($\phi=7$) </p><p>  <img src="/2020/04/13/GoogLeNetç³»åˆ—/scaling.png" width="40%;"></p></li></ul></li></ul></li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li>for object detection <ul><li>train<ul><li>Learning rate is linearly increased from 0 to 0.16 in the first training epoch and then annealed down </li><li>employ commonly-used focal loss </li><li>3x3 anchors</li></ul></li><li>compare<ul><li>low-accuracy regimeï¼šä½ç²¾åº¦ä¸‹ï¼ŒEfficientDet-D0å’ŒyoloV3å·®ä¸å¤š</li><li>ä¸­ç­‰ç²¾åº¦ï¼ŒEfficientDet-D1å’ŒMask-RCNNå·®ä¸å¤š</li><li>EfficientDet-D7 achieves a new state-of-the-art  </li></ul></li></ul></li><li>for semantic segmentation <ul><li>modify<ul><li>keep feature level {P2,P3,â€¦,P7} in BiFPN</li><li>but only use P2 for the final per-pixel classification </li><li>set the channel size to 128 for BiFPN and 256 for classification head </li><li>Both BiFPN and classification head are repeated by 3 times</li></ul></li><li>compare<ul><li>å’Œdeeplabv3æ¯”çš„ï¼ŒCOCOæ•°æ®é›†</li><li>better accuracy and fewer FLOPs </li></ul></li></ul></li><li>ablation study<ul><li>backbone improves accuracy v.s. resnet50</li><li>BiFPN improves accuracy v.s. FPN</li><li>BiFPN achieves similar accuracy as repeated FPN+PANet</li><li>BiFPN + weghting achieves the best accuracy</li><li>Normalizedï¼šsoftmaxå’Œfastç‰ˆæœ¬æ•ˆæœå·®ä¸å¤šï¼Œæ¯ä¸ªèŠ‚ç‚¹çš„weightåœ¨è®­ç»ƒå¼€å§‹è¿…é€Ÿå˜åŒ–ï¼ˆsuggesting different features contribute to the feature fusion unequallyï¼‰</li><li>Compound Scalingï¼šè¿™ä¸ªæ¯”å…¶ä»–åªæé«˜ä¸€ä¸ªæŒ‡æ ‡çš„æ•ˆæœå¥½å°±ä¸ç”¨è¯´äº†</li></ul></li></ul></li><li><p>è¶…å‚ï¼š</p><p> efficientNetå’ŒefficientDetçš„resolutionæ˜¯ä¸ä¸€æ ·çš„ï¼Œå› ä¸ºæ£€æµ‹è¿˜æœ‰neckå’Œheadï¼Œå±‚æ•°æ›´æ·±ï¼Œæ‰€ä»¥resolutionæ›´å¤§</p><p> <img src="/2020/04/13/GoogLeNetç³»åˆ—/hyper.png" width="60%;"></p></li></ol><h2 id="EfficientNetV2-Smaller-Models-and-Faster-Training"><a href="#EfficientNetV2-Smaller-Models-and-Faster-Training" class="headerlink" title="EfficientNetV2: Smaller Models and Faster Training"></a>EfficientNetV2: Smaller Models and Faster Training</h2><ol><li><p>åŠ¨æœº</p><ul><li>faster training speed and better parameter efficiency </li><li>use a new op: Fused-MBConv</li><li>propose progressive learningï¼šadaptively adjuts regularization &amp; image size</li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>review of EfficientNet</p><ul><li><p>large image size </p><ul><li>large memory usageï¼Œsmall batch sizeï¼Œlong training time</li><li>thus propose increasing image size gradually in V2</li></ul></li><li><p>extensive depthwise conv</p><ul><li>often cannot fully utilize modern accelerators</li><li><p>thus introduce Fused-MBConv in V2ï¼šWhen applied in early stage 1-3, Fused-MBConv can improve training speed with a small overhead on parameters and FLOPs </p><p><img src="/2020/04/13/GoogLeNetç³»åˆ—/Fused-MB.png" width="40%;"></p></li></ul></li><li><p>equally scaling up</p><ul><li>proved sub-optimal in nfnets</li><li>since the stages are not equally contributed to the efficiency &amp; accuracy </li><li>thus in V2<ul><li>use a non-uniform scaling strategyï¼šgradually add more layers to later stages(s5 &amp; s6)</li><li>restrict the max image size</li></ul></li></ul></li></ul></li><li><p>EfficientNet V2 Architecture </p><ul><li><p>basic ConvBlock</p><ul><li>use fused-MBConv in the early layers</li><li>use MBConv in the latter layers</li></ul></li><li><p>expansion ratios</p><ul><li>use smaller expansion ratios</li><li>å› ä¸ºåŒæ ·çš„é€šé“æ•°ï¼Œfused-MBæ¯”MBçš„å‚æ•°é‡å¤§</li></ul></li><li><p>kernel size</p><ul><li>å…¨å›¾3x3ï¼Œæ²¡æœ‰5x5äº†</li><li>add more layers to compensate the reduced receptive field </li></ul></li><li><p>last stride 1 stage</p><ul><li>effv1æ˜¯7ä¸ªstage</li><li><p>effv2æœ‰6ä¸ªstage</p><p><img src="/2020/04/13/GoogLeNetç³»åˆ—/efficientB0.png" width="40%;">   <img src="/2020/04/13/GoogLeNetç³»åˆ—/effv2.png" width="40%;"></p></li></ul></li><li><p>scaling policy</p><ul><li>compound scalingï¼šRã€Wã€Dä¸€èµ·scale</li><li>ä½†æ˜¯é™åˆ¶äº†æœ€å¤§inference image size=480ï¼ˆtrain=384ï¼‰</li><li>gradually add more layers to later stages (s5 &amp; s6)</li></ul></li></ul></li><li><p>progressive learning</p><ul><li><p>large models require stronger regularization </p></li><li><p>larger image size leads to more computations with larger capacityï¼Œthus also needs stronger regularization </p></li><li><p>training process</p><ul><li>in the early training epochs, we train the network with smaller images and weak regularization</li><li><p>gradually increase image size but also making learning more difficult by adding stronger regularization </p><p><img src="/2020/04/13/GoogLeNetç³»åˆ—/progressive.png" width="50%;"></p></li></ul></li><li><p>adaptive params</p><ul><li>image size</li><li>dropout rate</li><li>randAug magnitude</li><li>mixup alpha</li><li><p>ç»™å®šæœ€å¤§æœ€å°å€¼ï¼Œstage Nï¼Œä½¿ç”¨linear interpolation</p><p><img src="/2020/04/13/GoogLeNetç³»åˆ—/v2S.png" width="40%;"></p></li></ul></li></ul></li><li><p>train&amp;test details</p><ul><li>RMSProp optimizer with decay 0.9 and momentum 0.9 </li><li>batch norm momentum 0.99 </li><li>weight decay 1e-5 </li><li>trained for 350 epochs with total batch size 4096 </li><li>Learning rate is first warmed up from 0 to 0.256, and then decayed by 0.97 every 2.4 epochs </li><li>exponential moving average with 0.9999 decay rate </li><li><p>stochastic depth with 0.8 survival probability</p></li><li><p>4 stages (87 epochs per stage)ï¼šearly stage with weak regularization &amp; later stronger</p></li><li>maximum image size for training is about 20% smaller than inference &amp; no further finetuning</li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> papers </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>FCN</title>
      <link href="/2020/03/28/FCN/"/>
      <url>/2020/03/28/FCN/</url>
      <content type="html"><![CDATA[<h2 id="FCN-Fully-Convolutional-Networks-for-Semantic-Segmentation"><a href="#FCN-Fully-Convolutional-Networks-for-Semantic-Segmentation" class="headerlink" title="FCN: Fully Convolutional Networks for Semantic Segmentation"></a>FCN: Fully Convolutional Networks for Semantic Segmentation</h2><ol><li><p>åŠ¨æœº</p><ul><li>take input of arbitrary size</li><li>pixelwise prediction (semantic segmentation) </li><li>efficient inference and learning</li><li>end-to-end</li><li>with superwised-pretraining</li></ul></li><li><p>è®ºç‚¹</p><ul><li>fully connected layers brings heavy computation</li><li>patchwise/proposals training with less efficiency (ä¸ºäº†å¯¹ä¸€ä¸ªåƒç´ åˆ†ç±»ï¼Œè¦æ‰£å®ƒå‘¨å›´çš„patchï¼Œä¸€å¼ å›¾çš„å­˜å‚¨å®¹é‡ä¸Šå‡åˆ°k*kå€ï¼Œè€Œä¸”ç›¸é‚»patché‡å çš„éƒ¨åˆ†å¼•å…¥å¤§é‡é‡å¤è®¡ç®—ï¼ŒåŒæ—¶<strong>æ„Ÿå—é‡å¤ªå°</strong>ï¼Œæ²¡æ³•æœ‰æ•ˆåˆ©ç”¨å…¨å±€ä¿¡æ¯)</li><li>fully convolutional structure are used to get a feature extractor which yield a localized, fixed-length feature </li><li>Semantic segmentation faces an inherent tension between<strong> semantics</strong> and <strong>location</strong>: global information resolves what while local information resolves where. Deep feature hierarchies jointly encode location and semantics in a local-to-global pyramid. </li><li>other semantic works (RCNN) are not end-to-end</li></ul></li><li><p>è¦ç´ </p><ul><li>æŠŠå…¨è¿æ¥å±‚æ¢æˆ<strong>1*1å·ç§¯</strong>ï¼Œç”¨äºæå–ç‰¹å¾ï¼Œå½¢æˆçƒ­ç‚¹å›¾</li><li><strong>åå·ç§¯</strong>å°†å°å°ºå¯¸çš„çƒ­ç‚¹å›¾ä¸Šé‡‡æ ·åˆ°åŸå°ºå¯¸çš„è¯­ä¹‰åˆ†å‰²å›¾åƒ</li></ul><p><img src="/2020/03/28/FCN/fcn1.png" width="40%"></p><ul><li>a novel â€œskipâ€ architecture to combine deep, coarse, semantic information and shallow, fine, appearance information</li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>fully convolutional network</p><ul><li><strong>receptive fields</strong>:  Locations in higher layers correspond to the locations in the image they are path-connected to</li><li>typical recognition nets: <ul><li>fixed-input</li><li>patches</li><li>the fully connected layers can be viewed as convolutions with kernels that cover their entire input regions</li></ul></li><li>our structure:<ul><li>arbitrary-input</li><li>the computation is saved by computing the overlapping regions of those patches only once</li><li>output size corresponds to the input(H/16, W/16)<ul><li>heatmap: the (H/16 * W/16) high-dims feature-map corresponds to the 1000 classes</li></ul></li></ul></li></ul></li></ul><p><img src="/2020/03/28/FCN/fcn.png" width="50%"></p><ul><li><p><a href="https://blog.csdn.net/HMH2_YY/article/details/80935394" target="_blank" rel="noopener">coarse predictions to dense</a></p><ul><li>OverFeat introduced</li><li>å¯¹äºé«˜ç»´ç‰¹å¾å›¾ä¸Šä¸€ä¸ªå…ƒç´ ï¼Œå¯¹åº”äº†åŸå›¾æ„Ÿå—é‡ä¸€ç‰‡åŒºåŸŸï¼Œå°†reception fieldä¸­cä½å¡«ä¸Šè¿™ä¸ªå…ƒç´ çš„å€¼</li><li>ç§»åŠ¨åŸå›¾ï¼Œç›¸åº”çš„æ„Ÿå—é‡å¯¹åº”çš„å›¾ç‰‡ä¹Ÿå‘ç”Ÿäº†ç§»åŠ¨ï¼Œé«˜ç»´ç‰¹å¾å›¾çš„è¾“å‡ºå˜äº†ï¼Œcä½å˜äº†</li><li>ç§»åŠ¨èŒƒå›´stride*strideï¼Œå°±ä¼šå¾—åˆ°åŸå›¾å°ºå¯¸çš„è¾“å‡ºäº†</li></ul></li><li><p>upsampling</p><ul><li>simplest: bilinear interpolation</li><li><strong>in-network upsampling</strong>: backwards convolution (deconvolution) with an output stride of f</li><li>A stack of deconvolution layers and activation functions can even learn a <strong>nonlinear</strong> upsampling </li><li>factor: FCNé‡Œé¢inputsizeå’Œoutputsizeä¹‹é—´å­˜åœ¨çº¿æ€§å…³ç³»ï¼Œå°±æ˜¯æ‰€æœ‰å·ç§¯poolingå±‚çš„ç´¯ç§¯é‡‡æ ·æ­¥é•¿ä¹˜ç§¯</li><li>kernelsizeï¼š$2 * factor - factor \% 2$</li><li>strideï¼š$factor$</li><li>paddingï¼š$ceil((factor - 1) / 2.)$</li></ul><p><img src="/2020/03/28/FCN/deconv2.gif" width="30%"></p><p>è¿™å—çš„è®¡ç®—æœ‰ç‚¹ç»•ï¼Œ$stride=factor$æ¯”è¾ƒå¥½ç¡®å®šï¼Œè¿™æ˜¯å°†ç‰¹å¾å›¾æ¢å¤çš„åŸå›¾å°ºå¯¸è¦rescaleçš„å°ºåº¦ã€‚ç„¶ååœ¨è¾“å…¥çš„ç›¸é‚»å…ƒç´ ä¹‹é—´æ’å…¥s-1ä¸ª0å…ƒç´ ï¼ŒåŸå›¾å°ºå¯¸å˜ä¸º$(s-1)<em>(input_size-1)+input_size = s</em>input_size + (s-1)$ï¼Œä¸ºäº†å¾—åˆ°$output_size=s*input_size$è¾“å‡ºï¼Œå†è‡³å°‘$padding=[(s-1)/2]_{ceil}$ï¼Œç„¶åæ ¹æ®ï¼š</p><script type="math/tex; mode=display">(s-1) * (in-1) + in + 2p -k + 1 = out</script><p>æœ‰ï¼š</p><script type="math/tex; mode=display">2p-k+2 = s</script><p>åœ¨kerasé‡Œé¢å¯ä»¥è°ƒç”¨åº“å‡½æ•°Conv2DTransposeæ¥å®ç°ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">x = Input(shape=(<span class="number">64</span>,<span class="number">64</span>,<span class="number">16</span>))</span><br><span class="line">y = Conv2DTranspose(filters=<span class="number">16</span>, kernel_size=<span class="number">20</span>, strides=<span class="number">8</span>, padding=<span class="string">'same'</span>)(x)</span><br><span class="line">model = Model(x, y)</span><br><span class="line">model.summary()</span><br><span class="line"><span class="comment"># input: (None, 64, 64, 16)   output: (None, 512, 512, 16)   params: 102,416</span></span><br><span class="line"></span><br><span class="line">x = Input(shape=(<span class="number">32</span>,<span class="number">32</span>,<span class="number">16</span>))</span><br><span class="line">y = Conv2DTranspose(filters=<span class="number">16</span>, kernel_size=<span class="number">48</span>, strides=<span class="number">16</span>, padding=<span class="string">'same'</span>)(x)</span><br><span class="line"><span class="comment"># input: (None, 32, 32, 16)   output: (None, 512, 512, 16)   params: 589,840</span></span><br><span class="line"></span><br><span class="line">x = Input(shape=(<span class="number">16</span>,<span class="number">16</span>,<span class="number">16</span>))</span><br><span class="line">y = Conv2DTranspose(filters=<span class="number">16</span>, kernel_size=<span class="number">80</span>, strides=<span class="number">32</span>, padding=<span class="string">'same'</span>)(x)</span><br><span class="line"><span class="comment"># input: (None, 16, 16, 16)   output: (None, 512, 512, 16)   params: 1,638,416</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># å‚æ•°å‚è€ƒï¼šorig unetçš„totalå‚æ•°é‡ä¸º36,605,042</span></span><br><span class="line"><span class="comment"># å„çº§transposeçš„å‚æ•°é‡ä¸ºï¼š</span></span><br><span class="line"><span class="comment"># (None, 16, 16, 512)     4,194,816</span></span><br><span class="line"><span class="comment"># (None, 32, 32, 512)     4,194,816</span></span><br><span class="line"><span class="comment"># (None, 64, 64, 256)     1,048,832</span></span><br><span class="line"><span class="comment"># (None, 128, 128, 128)   262,272</span></span><br><span class="line"><span class="comment"># (None, 256, 256, 32)    16,416</span></span><br></pre></td></tr></table></figure><p>å¯ä»¥çœ‹åˆ°kernel_sizeå˜å¤§ï¼Œå¯¹å‚æ•°é‡çš„å½±å“æå¤§ã€‚ï¼ˆkernel_sizeè®¾ç½®çš„å°äº†ï¼Œåªèƒ½æå–åˆ°å•ä¸ªå…ƒç´ ï¼Œæˆ‘è§‰å¾—kernel_sizeè‡³å°‘è¦å¤§äºstrideï¼‰</p></li><li><p>Segmentation Architecture </p><ul><li>use pre-trained model</li><li>convert all fully connected layers to convolutions </li><li>append a 1*1 conv with channel dimension(including background)  to predict scores </li><li>followed by a deconvolution layer to upsample the coarse outputs to dense outputs</li></ul></li><li><p>skips</p><ul><li>the 32 pixel stride at the final prediction layer limits the scale of detail in the upsampled output</li><li>é€å±‚upsamplingï¼Œèåˆå‰å‡ å±‚çš„feature mapï¼Œelement-wise add</li></ul></li><li><p>finer layers: â€œAs they see fewer pixels, the finer scale predictions should need fewer layers.â€ è¿™æ˜¯é’ˆå¯¹å‰é¢çš„å·ç§¯ç½‘ç»œæ¥è¯´ï¼Œéšç€ç½‘ç»œåŠ æ·±ï¼Œç‰¹å¾å›¾ä¸Šçš„æ„Ÿå—é‡å˜å¤§ï¼Œå°±éœ€è¦æ›´å¤šçš„channelæ¥è®°å½•æ›´å¤šçš„ä½çº§ç‰¹å¾ç»„åˆ</p><ul><li>add a 1*1 conv on top of pool4 (zero-initialized)</li><li>adding a 2x upsampling layer on top of conv7 (We <strong>initialize this 2xupsampling to bilinear interpolation</strong>, but allow the parameters to be learned)</li><li><strong>sum</strong> the above two stride16 predictions (â€œMax fusion made learning difficult due to gradient switchingâ€)</li><li>16x upsampled back to the image </li><li><p>åšåˆ°ç¬¬ä¸‰è¡Œå†å¾€ä¸‹ï¼Œç»“æœåˆä¼šå˜å·®ï¼Œæ‰€ä»¥åšåˆ°è¿™é‡Œå°±åœä¸‹</p><p><img src="/2020/03/28/FCN/skip.png" width="90%"></p></li></ul></li></ul></li><li><p>æ€»ç»“</p><ul><li><p>åœ¨å‡é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œåˆ†é˜¶æ®µå¢å¤§æ¯”ä¸€æ­¥åˆ°ä½æ•ˆæœæ›´å¥½</p></li><li><p>åœ¨å‡é‡‡æ ·çš„æ¯ä¸ªé˜¶æ®µï¼Œä½¿ç”¨é™é‡‡æ ·å¯¹åº”å±‚çš„ç‰¹å¾è¿›è¡Œè¾…åŠ©</p></li><li><p>8å€ä¸Šé‡‡æ ·è™½ç„¶æ¯”32å€çš„æ•ˆæœå¥½äº†å¾ˆå¤šï¼Œä½†æ˜¯<strong>ç»“æœè¿˜æ˜¯æ¯”è¾ƒæ¨¡ç³Š</strong>å’Œå¹³æ»‘ï¼Œå¯¹å›¾åƒä¸­çš„ç»†èŠ‚ä¸æ•æ„Ÿï¼Œè®¸å¤šç ”ç©¶è€…é‡‡ç”¨MRFç®—æ³•æˆ–CRFç®—æ³•å¯¹FCNçš„è¾“å‡ºç»“æœåšè¿›ä¸€æ­¥ä¼˜åŒ–</p></li><li><p>x8ä¸ºå•¥å¥½äºx32ï¼š1. x32çš„ç‰¹å¾å›¾<strong>æ„Ÿå—é‡è¿‡å¤§</strong>ï¼Œå¯¹å°ç‰©ä½“ä¸æ•æ„Ÿ   2. x32çš„æ”¾å¤§æ¯”ä¾‹é€ æˆçš„å¤±çœŸæ›´å¤§</p></li><li><p><strong>unetçš„åŒºåˆ«</strong>ï¼š</p><ul><li><p>unetæ²¡ç”¨imagenetçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå› ä¸ºæ˜¯åŒ»å­¦å›¾åƒ</p></li><li><p>unetåœ¨è¿›è¡Œæµ…å±‚ç‰¹å¾èåˆçš„æ—¶å€™ç”¨äº†concatè€Œéelement-wise add</p></li><li><p>é€å±‚ä¸Šé‡‡æ ·ï¼Œx2 vs. x8/x32</p></li><li><p>orig unetæ²¡ç”¨padï¼Œè¾“å‡ºå°äºè¾“å…¥ï¼ŒFCNåˆ™pad+crop</p></li><li><p>æ•°æ®å¢å¼ºï¼ŒFCNæ²¡ç”¨è¿™äº›â€˜machineryâ€™ï¼ŒåŒ»å­¦å›¾åƒéœ€è¦å¼ºaugmentation</p></li><li><p>åŠ æƒloss</p></li></ul></li></ul></li></ol><p>â€‹    </p>]]></content>
      
      
        <tags>
            
            <tag> papers </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>cs231n-RNN-review</title>
      <link href="/2020/03/15/cs231n-RNN-review/"/>
      <url>/2020/03/15/cs231n-RNN-review/</url>
      <content type="html"><![CDATA[]]></content>
      
      
    </entry>
    
    <entry>
      <title>attentionç³»åˆ—</title>
      <link href="/2020/03/13/attention%E7%B3%BB%E5%88%97/"/>
      <url>/2020/03/13/attention%E7%B3%BB%E5%88%97/</url>
      <content type="html"><![CDATA[<h2 id="0-ç»¼è¿°"><a href="#0-ç»¼è¿°" class="headerlink" title="0. ç»¼è¿°"></a>0. ç»¼è¿°</h2><ol><li><p>attentionçš„æ–¹å¼åˆ†ä¸ºä¸¤ç§ï¼ˆ<a href="https://blog.csdn.net/yideqianfenzhiyi/article/details/79422857?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">Reference</a>ï¼‰</p><ul><li>å­¦ä¹ æƒé‡åˆ†å¸ƒ<ul><li>éƒ¨åˆ†åŠ æƒï¼ˆhard attentionï¼‰ï¼å…¨éƒ¨åŠ æƒï¼ˆsoft attentionï¼‰</li><li>åŸå›¾ä¸ŠåŠ æƒï¼ç‰¹å¾å›¾ä¸ŠåŠ æƒ</li><li>ç©ºé—´å°ºåº¦åŠ æƒï¼channelå°ºåº¦åŠ æƒï¼æ—¶é—´åŸŸåŠ æƒï¼æ··åˆåŸŸåŠ æƒ</li><li>CAMç³»åˆ—ã€SE-blockç³»åˆ—ï¼šèŠ±å¼åŠ æƒï¼Œå­¦ä¹ æƒé‡ï¼Œnon-localçš„æ¨¡å—ï¼Œä½œç”¨äºæŸä¸ªç»´åº¦</li></ul></li><li>ä»»åŠ¡åˆ†è§£<ul><li>è®¾è®¡ä¸åŒçš„ç½‘ç»œç»“æ„ï¼ˆæˆ–åˆ†æ”¯ï¼‰ä¸“æ³¨äºä¸åŒçš„å­ä»»åŠ¡ï¼Œ</li><li>é‡æ–°åˆ†é…ç½‘ç»œçš„å­¦ä¹ èƒ½åŠ›ï¼Œä»è€Œé™ä½åŸå§‹ä»»åŠ¡çš„éš¾åº¦ï¼Œä½¿ç½‘ç»œæ›´åŠ å®¹æ˜“è®­ç»ƒ</li><li>STNã€deformable convï¼šæ·»åŠ æ˜¾å¼çš„æ¨¡å—è´Ÿè´£å­¦ä¹ å½¢å˜/receptive fieldçš„å˜åŒ–ï¼Œlocalæ¨¡å—ï¼Œapply by pixel</li></ul></li><li>local / non-local<ul><li>localæ¨¡å—çš„ç»“æœæ˜¯pixel-specificçš„</li><li>non-localæ¨¡å—çš„ç»“æœæ˜¯å…¨å±€å…±åŒè®¡ç®—çš„çš„</li></ul></li></ul></li><li><p>åŸºäºæƒé‡çš„attentionï¼ˆ<a href="https://blog.csdn.net/bigbug_sec/article/details/89025318" target="_blank" rel="noopener">Reference</a>ï¼‰</p><ul><li>æ³¨æ„åŠ›æœºåˆ¶é€šå¸¸ç”±ä¸€ä¸ªè¿æ¥åœ¨åŸç¥ç»ç½‘ç»œä¹‹åçš„é¢å¤–çš„ç¥ç»ç½‘ç»œå®ç°</li><li>æ•´ä¸ªæ¨¡å‹ä»ç„¶æ˜¯ç«¯å¯¹ç«¯çš„ï¼Œå› æ­¤æ³¨æ„åŠ›æ¨¡å—èƒ½å¤Ÿå’ŒåŸæ¨¡å‹ä¸€èµ·åŒæ­¥è®­ç»ƒ</li><li>å¯¹äºsoft attentionï¼Œæ³¨æ„åŠ›æ¨¡å—å¯¹å…¶è¾“å…¥æ˜¯å¯å¾®çš„ï¼Œæ‰€ä»¥æ•´ä¸ªæ¨¡å‹ä»å¯ç”¨æ¢¯åº¦æ–¹æ³•æ¥ä¼˜åŒ–</li><li>è€Œhard attentionè¦ç¦»æ•£åœ°é€‰æ‹©å…¶è¾“å…¥çš„ä¸€éƒ¨åˆ†ï¼Œè¿™æ ·æ•´ä¸ªç³»ç»Ÿå¯¹äºè¾“å…¥ä¸å†æ˜¯å¯å¾®çš„</li></ul></li><li><p>papers</p><ul><li><p>[STN] <a href="https://amberzzzz.github.io/2021/02/03/transform-in-CNN/">Spatial Transformer Networks</a></p></li><li><p>[deformable conv] Deformable Convolutional Networks </p></li><li><p>[CBAM] CBAM: Convolutional Block Attention Module </p></li><li><p>[SE-Net] <a href="https://amberzzzz.github.io/2020/04/30/SE-block/">Squeeze-and-Excitation Networks</a></p></li><li><p>[SE-blockçš„ä¸€ç³»åˆ—å˜ä½“] SC-SEï¼ˆfor segmentationï¼‰ã€CMPE-SEï¼ˆå¤æ‚åˆæ²¡ç”¨ï¼‰</p></li><li><p>[SK-Net] Selective Kernel Networksï¼šæ˜¯attension moduleï¼Œä½†æ˜¯ä¸»è¦æ”¹è¿›ç‚¹åœ¨receptive fieldï¼Œtrickå¤§æ‚çƒ©</p></li><li><p>[GC-Net] GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond </p></li></ul></li></ol><h2 id="CBAM-Convolutional-Block-Attention-Module"><a href="#CBAM-Convolutional-Block-Attention-Module" class="headerlink" title="CBAM: Convolutional Block Attention Module"></a>CBAM: Convolutional Block Attention Module</h2><ol><li>åŠ¨æœº<ul><li>attention module </li><li>lightweight and general  </li><li>improvements in classification and detection </li></ul></li><li><p>è®ºç‚¹</p><ul><li>deeperï¼š can obtain richer representation </li><li>increased widthï¼šcan outperform an extremely deep network</li><li>cardinalityï¼šresults in stronger representation power than depth and width </li><li>attentionï¼šimproves the representation of interests <ul><li>humans exploit a sequence of partial glimpses and selectively focus on salient parts </li><li>Residual Attention Networkï¼šcomputes 3d attention map</li><li>we decompose the process that learns channel attention and spatial attention separately </li><li>SE-blockï¼šuse global average-pooled features</li><li>we suggest to use max-pooled features as well  </li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li>sequentially infers a <strong>1D channel attention map</strong> and a <strong>2D spatial attention map</strong>  </li><li><p>broadcast and element-wise multiplication </p><p><img src="/2020/03/13/attentionç³»åˆ—/CBAM.png" width="45%;"></p></li><li><p>Channel attention module</p><ul><li>focuses on â€˜whatâ€™ is meaningful  </li><li>squeeze the spatial dimension </li><li>use both average-pooled and max-pooled features simultaneously </li><li>both descriptors are then forwarded to a shared MLP to reduce dimension</li><li>ã€QUESTIONã€‘çœ‹è®ºæ–‡MLPæ˜¯çº¿æ€§çš„å—ï¼Œæ²¡å†™æ¿€æ´»å‡½æ•°</li><li>then use element-wise summation  </li><li>sigmoid function </li></ul></li><li>Spatial attention module <ul><li>focuses on â€˜whereâ€™ </li><li>apply average-pooling and max-pooling  along the channel axis and concatenate </li><li>7x7 conv</li><li>sigmoid function </li></ul></li><li><p>Arrangement of attention modules</p><ul><li>in a parallel or sequential manner </li><li>we found sequential better than parallel </li><li>we found channel-first order slightly better than the spatial-first</li></ul><p><img src="/2020/03/13/attentionç³»åˆ—/module.png" width="45%;"></p></li><li><p>integration</p><ul><li>apply CBAM on the convolution outputs in each block </li><li>in residual path</li><li><p>before the add operation</p><p><img src="/2020/03/13/attentionç³»åˆ—/integration.png" width="55%;"></p></li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li><p>Ablation studies </p><ul><li>Channel attentionï¼šä¸¤ä¸ªpooling pathéƒ½æœ‰æ•ˆï¼Œä¸€èµ·ç”¨æœ€å¥½</li><li>Spatial attentionï¼š1x1convç›´æ¥squeezeä¹Ÿè¡Œï¼Œavg+maxæ›´å¥½ï¼Œ7x7convç•¥å¥½äº3x3conv</li><li>arrangementï¼šå‰é¢è¯´äº†ï¼Œæ¯”SEçš„å•spacial squeezeå¥½ï¼Œchannelåœ¨å‰å¥½äºåœ¨åï¼Œä¸²è¡Œå¥½äºå¹¶è¡Œ</li></ul></li><li><p>Classification resultsï¼šoutperform baselines and SE</p></li><li>Network Visualization  <ul><li>cover the target object regions better </li><li>the target class scores also increase accordingly </li></ul></li><li>Object Detection results<ul><li>apply to detectorsï¼šright before every classifier </li><li>apply to backbone</li></ul></li></ul></li></ol><h2 id="SK-Net-Selective-Kernel-Networks"><a href="#SK-Net-Selective-Kernel-Networks" class="headerlink" title="SK-Net: Selective Kernel Networks"></a>SK-Net: Selective Kernel Networks</h2><ol><li><p>åŠ¨æœº</p><ul><li>ç”Ÿç‰©çš„ç¥ç»å…ƒçš„æ„Ÿå—é‡æ˜¯éšç€åˆºæ¿€å˜åŒ–è€Œå˜åŒ–çš„</li><li>propose a selective kernel unit<ul><li>adaptively adjust the RF</li><li>multiple branches with different kernel sizes</li><li>guided fusion</li><li>å¤§æ‚çƒ©ï¼šmulti-branch&amp;kernelï¼Œgroup convï¼Œdilated convï¼Œattention mechanism</li></ul></li><li>SKNet<ul><li>by stacking multiple SK units</li><li>åœ¨åˆ†ç±»ä»»åŠ¡ä¸ŠéªŒè¯</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>multi-scale aggregation<ul><li>inception blockå°±æœ‰äº†</li><li>but linear aggregation approach may be insufficient</li></ul></li><li>multi-branch network<ul><li>two-branchï¼šä»¥resnetä¸ºä»£è¡¨ï¼Œä¸»è¦æ˜¯ä¸ºäº†easier to train</li><li>multi-branchï¼šä»¥inceptionä¸ºä»£è¡¨ï¼Œä¸»è¦ä¸ºäº†å¾—åˆ°multifarious features </li></ul></li><li>grouped/depthwise/dilated conv<ul><li>grouped convï¼šreduce computationï¼Œæå‡ç²¾åº¦</li><li>depthwise convï¼šreduce computationï¼Œç‰ºç‰²ç²¾åº¦</li><li>dilated convï¼šenlarge RFï¼Œæ¯”dense large kernelèŠ‚çœå‚æ•°é‡</li></ul></li><li>attention mechanism<ul><li>åŠ æƒç³»åˆ—ï¼š<ul><li>SENet&amp;CBAMï¼š</li><li>ç›¸æ¯”ä¹‹ä¸‹SKNetå¤šäº†adaptive RF</li></ul></li><li>åŠ¨æ€å·ç§¯ç³»åˆ—ï¼š<ul><li>STNä¸å¥½è®­ç»ƒï¼Œè®­å¥½ä»¥åå˜æ¢å°±å®šæ­»äº†</li><li>deformable convèƒ½å¤Ÿåœ¨inferenceçš„æ—¶å€™ä¹ŸåŠ¨æ€çš„å˜åŒ–å˜æ¢ï¼Œä½†æ˜¯æ²¡æœ‰multi-scaleå’Œnonlinear aggregation</li></ul></li></ul></li><li>thus we propose SK convolution <ul><li>multi-kernelsï¼šå¤§sizeçš„conv kernelæ˜¯ç”¨äº†dilated conv</li><li>nonlinear aggregation</li><li>computationally lightweight </li><li>could successfully embedded into small models</li><li>workflow<ul><li>split</li><li>fuse</li><li>select</li></ul></li><li>main difference from inception<ul><li>less customized</li><li>adaptive selection instead of equally addition</li></ul></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>selective kernel convolution</p><ul><li><p>split</p><ul><li>multi-branch with different kernel size</li><li>grouped/depthwise conv + BN + ReLU</li><li>5x5 kernel can be further replaced with dilated conv</li></ul></li><li><p>fuse</p><ul><li>to learn the control of information flow from different branches</li><li>element-wise summation</li><li>global average pooling</li><li>fc-BN-ReLUï¼šreduce dimensionï¼Œat least 32</li></ul></li><li><p>select</p><ul><li><p>channel-wise weighting factor A &amp; B &amp; moreï¼šA+B + more = 1</p></li><li><p>fc-softmax</p><p>  <img src="/2020/03/13/attentionç³»åˆ—/softmax.png" width="50%;"></p></li><li><p>åœ¨2åˆ†æ”¯çš„æƒ…å†µä¸‹ï¼Œä¸€ä¸ªæƒé‡çŸ©é˜µAå°±å¤Ÿäº†ï¼ŒBæ˜¯å†—ä½™çš„ï¼Œå› ä¸ºå¯ä»¥é—´æ¥ç®—å‡ºæ¥</p></li><li><p>reweighting</p></li></ul><p><img src="/2020/03/13/attentionç³»åˆ—/sk.png" width="80%;"></p></li></ul></li><li><p>network</p><ul><li>start from resnext</li><li>repeated SK unitsï¼šç±»ä¼¼bottleneck<ul><li>1x1 conv</li><li>SK conv</li><li>1x1 conv</li><li>hyperparams<ul><li>number of branches M=2</li><li>group number G=32ï¼šcardinality of each path</li><li>reduction ratio r=16ï¼šfuse operatorä¸­dim-reductionçš„å‚æ•°</li></ul></li></ul></li><li><p>åµŒå…¥åˆ°è½»é‡çš„ç½‘ç»œç»“æ„</p><ul><li>MobileNet/shuffleNet</li><li>æŠŠå…¶ä¸­çš„3x3 depthwiseå·ç§¯æ›¿æ¢æˆSK conv</li></ul><p><img src="/2020/03/13/attentionç³»åˆ—/skNet.png" width="90%;"></p></li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li><p>æ¯”sortçš„resnetã€densenetã€resnextç²¾åº¦éƒ½è¦å¥½</p><p>  <img src="/2020/03/13/attentionç³»åˆ—/error.png" width="40%;"></p></li></ul></li></ol><h2 id="GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond"><a href="#GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond" class="headerlink" title="GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond"></a>GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</h2><ol><li><p>åŠ¨æœº</p><ul><li>Non-Local Network (NLNet)<ul><li>capture long-range dependencies</li><li>obtain query-specific global context</li><li>but we found global contexts are almost the same for different query positions</li></ul></li><li>we produce<ul><li>query-independent formulation</li><li>smiliar structure as SE-Net</li><li>aims at global context modeling</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>Capturing long-range dependency </p><ul><li><p>mainly by sdeeply stacking conv layersï¼šinefficient </p></li><li><p>non-local network</p><ul><li>via self-attention mechanism </li><li>computes the pairwise relations between the query position then aggregate</li><li><p>ä½†æ˜¯ä¸åŒä½ç½®queryå¾—åˆ°çš„attention mapåŸºæœ¬ä¸€è‡´</p><p><img src="/2020/03/13/attentionç³»åˆ—/NLNet.png" width="40%;"></p></li></ul></li><li><p>we simply the non-local block</p><ul><li>query-independent </li><li>maintain acc &amp; save computation</li></ul></li></ul></li><li><p>our proposed GC-block</p><ul><li>unifies both the NL block and the SE block</li><li>three steps<ul><li>global context modelingï¼š</li><li>feature transform moduleï¼šcapture channel-wise interdependency</li><li>fusion moduleï¼šmerge into the original features</li></ul></li></ul></li><li><p>å¤šç§ä»»åŠ¡ä¸Šå‡æœ‰æ¶¨ç‚¹</p><ul><li>ä½†éƒ½æ˜¯åœ¨è·Ÿresnet50å¯¹æ¯”</li></ul></li></ul></li><li><p>revisit NLNet</p><ul><li><p>non-local block</p><ul><li><p><img src="/2020/03/13/attentionç³»åˆ—/NL formulation.png" width="45%;"></p></li><li><p>$f(x_i, x_j)$ï¼š</p><ul><li>encodes the relationship between position i &amp; j</li><li>è®¡ç®—æ–¹å¼æœ‰Gaussianã€Embedded Gaussianã€Dot productã€Concat</li><li>different instantiations achieve comparable performance</li></ul></li><li><p>$C(x)$ï¼šnorm factor</p></li><li><p>$x_i + \sum^{N_p} F(x_j)$ï¼šaggregates a specific global feature on $x_i$</p></li><li><p>widely-used Embedded Gaussianï¼š</p><p>  <img src="/2020/03/13/attentionç³»åˆ—/NL block.png" width="50%;"></p></li><li><p>åµŒå…¥æ–¹å¼ï¼š</p><ul><li>Mask R-CNN with FPN and Res50 </li><li>only add one non-local block right before the last residual block of res4</li></ul></li><li><p>observations &amp; inspirations</p><ul><li>distances among inputs show that input features are discriminated</li><li>outputs &amp; attention maps are almost the sameï¼šglobal context after training is actually independent of query position</li><li>inspirations<ul><li>simplify the Non-local block</li><li>no need of query-specific</li></ul></li></ul></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>simplifying form of NL blockï¼šSNL</p><ul><li><p>æ±‚ä¸€ä¸ªcommonçš„global featureï¼Œshareç»™å…¨å›¾æ¯ä¸ªposition</p><p><img src="/2020/03/13/attentionç³»åˆ—/simplify1.png" width="45%;"></p></li><li><p>è¿›ä¸€æ­¥ç®€åŒ–ï¼šæŠŠ$x_j$çš„1x1 convæåˆ°å‰é¢ï¼ŒFLOPså¤§å¤§å‡å°‘ï¼Œå› ä¸ºfeature scaleä»HWå˜æˆäº†1x1</p><p>  <img src="/2020/03/13/attentionç³»åˆ—/simplify2.png" width="45%;"></p></li><li><p>the SNL block achieves comparable performance to the NL block with significantly lower FLOPs</p></li></ul></li><li><p>global context modeling</p><ul><li>SNLå¯ä»¥æŠ½è±¡æˆä¸‰éƒ¨åˆ†ï¼š<ul><li>global attention poolingï¼šé€šè¿‡$W_k$ &amp; softmaxè·å–attention weightsï¼Œç„¶åè¿›è¡Œglobal pooling</li><li>feature transformï¼š1x1 conv</li><li>feature aggregationï¼šbroadcast element-wise add</li></ul></li><li><p>SE-blockä¹Ÿå¯ä»¥åˆ†è§£æˆç±»ä¼¼çš„æŠ½è±¡</p><ul><li>global attention poolingï¼šç”¨äº†ç®€å•çš„global average pooling</li><li>feature transformï¼šç”¨äº†squeeze &amp; exciteçš„fc-relu-fc-sigmoid</li><li>feature aggregationï¼šbroadcast element-wise multiplication </li></ul><p><img src="/2020/03/13/attentionç³»åˆ—/gc-block.png" width="80%;"></p></li></ul></li><li><p>Global Context Block</p><ul><li>integrate the benefits of both<ul><li>SNL global attention poolingï¼šeffective modeling on long-range dependency</li><li>SE bottleneck transformï¼šlight computationï¼ˆåªè¦ratioå¤§äº2å°±ä¼šèŠ‚çœå‚æ•°é‡å’Œè®¡ç®—é‡ï¼‰</li></ul></li><li>ç‰¹åˆ«åœ°ï¼Œåœ¨SE transformçš„squeeze layerä¸Šï¼ŒåˆåŠ äº†BN<ul><li>ease optimization </li><li>benefit generalization </li></ul></li><li>fusionï¼šadd</li><li>åµŒå…¥æ–¹å¼ï¼š<ul><li>GC-ResNet50</li><li>add GC-block to all layers (c3+c4+c5) in resnet50 with se ratio of 16</li></ul></li></ul></li><li><p>relationship to SE-block</p><ul><li>é¦–å…ˆæ˜¯fusion method reflects different goals<ul><li>SEåŸºäºå…¨å±€ä¿¡æ¯rescales the channelsï¼Œé—´æ¥ä½¿ç”¨</li><li>GCç›´æ¥ä½¿ç”¨ï¼Œå°†long-range dependencyåŠ åœ¨æ¯ä¸ªpositionä¸Š</li></ul></li><li>å…¶æ¬¡æ˜¯norm layer<ul><li>ease optimization</li></ul></li><li>æœ€åæ˜¯global attention pooling<ul><li>SEçš„GAPæ˜¯a special case</li><li>weighting factors shows superior</li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> æ³¨æ„åŠ›æœºåˆ¶ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Deeplabç³»åˆ—</title>
      <link href="/2020/02/24/Deeplab%E7%B3%BB%E5%88%97/"/>
      <url>/2020/02/24/Deeplab%E7%B3%BB%E5%88%97/</url>
      <content type="html"><![CDATA[<h2 id="ç»¼è¿°"><a href="#ç»¼è¿°" class="headerlink" title="ç»¼è¿°"></a>ç»¼è¿°</h2><ol><li><p>papers</p><ul><li>deeplabV1: SEMANTIC IMAGE SEGMENTATION WITH DEEP CONVOLUTIONAL NETS AND FULLY CONNECTED CRFSï¼Œä¸»è¦è´¡çŒ®æå‡ºäº†ç©ºæ´å·ç§¯ï¼Œä½¿å¾—feature extractioné˜¶æ®µè¾“å‡ºçš„ç‰¹å¾å›¾ç»´æŒè¾ƒé«˜çš„resolution</li><li>deeplabV2: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFsï¼Œä¸»è¦è´¡çŒ®æ˜¯å¤šå°ºåº¦ASPPç»“æ„</li></ul></li></ol><ul><li>deeplabV3: Rethinking Atrous Convolution for Semantic Image Segmentationï¼Œæå‡ºäº†åŸºäºResNetçš„ä¸²è¡Œ&amp;å¹¶è¡Œä¸¤ç§ç»“æ„ï¼Œç»†èŠ‚ä¸Šæåˆ°äº†multi-gridï¼Œæ”¹è¿›äº†ASPPæ¨¡å—<ul><li>deeplabV3+: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation </li></ul></li><li>Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation </li></ul><ol><li><p>åˆ†å‰²ç»“æœæ¯”è¾ƒç²—ç³™çš„åŸå› </p><ul><li>æ± åŒ–ï¼šå°†å…¨å›¾æŠ½è±¡åŒ–ï¼Œé™ä½åˆ†è¾¨ç‡ï¼Œä¼šä¸¢å¤±ç»†èŠ‚ä¿¡æ¯ï¼Œå¹³ç§»ä¸å˜æ€§ï¼Œä½¿å¾—è¾¹ç•Œä¿¡æ¯ä¸æ¸…æ™°</li><li>æ²¡æœ‰åˆ©ç”¨æ ‡ç­¾ä¹‹é—´çš„æ¦‚ç‡å…³ç³»ï¼šCNNç¼ºå°‘å¯¹ç©ºé—´ã€è¾¹ç¼˜ä¿¡æ¯ç­‰çº¦æŸ</li></ul><p>å¯¹æ­¤ï¼ŒdeeplabV1å¼•å…¥äº†</p><ul><li><a href="https://www.jianshu.com/p/f743bd9041b3" target="_blank" rel="noopener">ç©ºæ´å·ç§¯</a>ï¼šVGGä¸­æå‡ºçš„å¤šä¸ªå°å·ç§¯æ ¸ä»£æ›¿å¤§å·ç§¯æ ¸çš„æ–¹æ³•ï¼Œåªèƒ½ä½¿æ„Ÿå—é‡çº¿æ€§å¢é•¿ï¼Œè€Œå¤šä¸ªç©ºæ´å·ç§¯ä¸²è”ï¼Œå¯ä»¥å®ç°æŒ‡æ•°å¢é•¿ã€‚</li><li>å…¨è¿æ¥æ¡ä»¶éšæœºåœºCRFï¼šä½œä¸ºstage2ï¼Œæé«˜æ¨¡å‹æ•è·ç»†èŠ‚çš„èƒ½åŠ›ï¼Œæå‡è¾¹ç•Œåˆ†å‰²ç²¾åº¦</li></ul></li><li><p>å¤§å°ç‰©ä½“åŒæ—¶åˆ†å‰²</p><p>deeplabV2å¼•å…¥</p><ul><li>å¤šå°ºåº¦ASPP(Atrous Spatial Pyramid Pooling)ï¼šå¹¶è¡Œçš„é‡‡ç”¨å¤šä¸ªé‡‡æ ·ç‡çš„ç©ºæ´å·ç§¯æå–ç‰¹å¾ï¼Œå†è¿›è¡Œç‰¹å¾èåˆ</li><li>backbone model changeï¼šVGG16æ”¹ä¸ºResNet</li><li>ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡</li></ul></li><li><p>è¿›ä¸€æ­¥æ”¹è¿›æ¨¡å‹æ¶æ„</p><p>deeplabV3å¼•å…¥</p><ul><li>ASPPåµŒå…¥ResNetåå‡ ä¸ªblock</li><li>å»æ‰äº†CRF</li></ul></li><li><p>ä½¿ç”¨åŸå§‹çš„Conv/poolæ“ä½œï¼Œå¾—åˆ°çš„low resolution score mapï¼Œpool strideä¼šä½¿å¾—è¿‡ç¨‹ä¸­ä¸¢å¼ƒä¸€éƒ¨åˆ†ä¿¡æ¯ï¼Œä¸Šé‡‡æ ·ä¼šå¾—åˆ°è¾ƒå¤§çš„å¤±çœŸå›¾åƒï¼Œä½¿ç”¨ç©ºæ´å·ç§¯ï¼Œä¿ç•™ç‰¹å¾å›¾ä¸Šçš„å…¨éƒ¨ä¿¡æ¯ï¼ŒåŒæ—¶keep resolutionï¼Œå‡å°‘äº†ä¿¡æ¯æŸå¤±</p></li><li><p>DeeplabV3çš„ASPPç›¸æ¯”è¾ƒäºV2ï¼Œå¢åŠ äº†ä¸€æ¡1x1 conv pathå’Œä¸€æ¡image pooling pathï¼ŒåŠ GAPè¿™æ¡pathæ˜¯å› ä¸ºï¼Œå®éªŒä¸­å‘ç°ï¼Œéšç€rateçš„å¢å¤§ï¼Œæœ‰æ•ˆçš„weightæ•°ç›®å¼€å§‹å‡å°‘ï¼ˆ<strong>éƒ¨åˆ†è¶…å‡ºè¾¹ç•Œæ— æ³•æœ‰æ•ˆæ•æ‰è¿œè·ç¦»ä¿¡æ¯</strong>ï¼‰ï¼Œå› æ­¤åˆ©ç”¨global average poolingæå–äº†image-levelçš„ç‰¹å¾å¹¶ä¸ASPPçš„ç‰¹å¾å¹¶åœ¨ä¸€èµ·ï¼Œæ¥è¡¥å……å› ä¸ºdilationä¸¢å¤±çš„ä¿¡æ¯</p><p>ç©ºæ´å·ç§¯çš„pathï¼ŒV2æ˜¯æ¯æ¡pathåˆ†åˆ«ç©ºæ´å·ç§¯ç„¶åæ¥ä¸¤ä¸ª1x1convï¼ˆæ²¡æœ‰BNï¼‰ï¼ŒV3æ˜¯ç©ºæ´å·ç§¯å’ŒBatchNormalizationç»„åˆ</p><p>fusionæ–¹å¼ï¼ŒV2æ˜¯sum fusionï¼ŒV3æ˜¯æ‰€æœ‰path concatç„¶å1x1 convï¼Œå¾—åˆ°æœ€ç»ˆscore map</p></li><li><p>DeeplabV3çš„ä¸²è¡Œç‰ˆæœ¬ï¼Œâ€œIn order to maintain original image size, convolutions are replaced with strous convolutions with rates that differ from each other with factor 2â€ï¼Œpptä¸Šè¯´åé¢å‡ ä¸ªblockå¤åˆ¶äº†block4ï¼Œæ¯ä¸ªblocké‡Œé¢ä¸‰å±‚convï¼Œå…¶ä¸­æœ€åä¸€å±‚conv stride2ï¼Œç„¶åä¸ºäº†maintain output sizeï¼Œç©ºæ´rate*2ï¼Œè¿™ä¸ªä¸å¤ªç†è§£ã€‚</p><p>multi-grid methodï¼šå¯¹æ¯ä¸ªblocké‡Œé¢çš„ä¸‰å±‚å·ç§¯é‡‡ç”¨ä¸åŒç©ºæ´ç‡ï¼Œunit rateï¼ˆe.g.(1,2,4)ï¼‰ * rate ï¼ˆe.g. 2ï¼‰</p></li></ol><h2 id="deeplabV1-SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CONVOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS"><a href="#deeplabV1-SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CONVOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS" class="headerlink" title="deeplabV1: SEMANTIC IMAGE SEGMENTATION WITH DEEP CONVOLUTIONAL NETS AND FULLY CONNECTED CRFS"></a>deeplabV1: SEMANTIC IMAGE SEGMENTATION WITH DEEP CONVOLUTIONAL NETS AND FULLY CONNECTED CRFS</h2><ol><li><p>åŠ¨æœº</p><ul><li>brings together methods from Deep Convolutional Neural Networks and probabilistic graphical models </li><li>poor localization property of deep networks</li><li>combine a fully connected Conditional Random Field (CRF) </li><li>be able to localize segment boundaries beyond previous accuracies</li><li>speed: atrous</li><li>accuracy: </li><li>simplicity: cascade modules</li></ul></li><li><p>è®ºç‚¹</p><ul><li>DCNN learns hierarchical abstractions of data, which is desirable for high-level vision tasks (classification)</li><li>but it hampers low-level tasks,  such as pose estimation and semantic segmentation, where we want precise localization, rather than <strong>abstraction of spatial details</strong></li><li>two technical hurdles in DCNNs when applying to image labeling tasks<ul><li>pooling, loss of resolution: we employ the â€˜atrousâ€™ (with holes) for efficient dense computation </li><li>spacial invariance: we use the fully connected pairwise CRF to capture fine edge details </li></ul></li><li><p>Our approach </p><ul><li>treats every pixel as a CRF node</li><li>exploits long-range dependencies</li><li>and uses CRF inference to directly optimize a DCNN-driven cost function </li></ul><p><img src="/2020/02/24/Deeplabç³»åˆ—/deeplabV1.png" width="65%"></p></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>structure</p><ul><li>fully convolutional VGG-16</li><li>keep the first 3 subsampling blocks for a target stride of 8 </li><li>use hole algorithm conv filters for the last two blocks</li><li>keep the pooling layers for the purpose of fine-tuingï¼Œchange strides from 2 to 1</li><li>for dense map(h/8), the first fully convolutional 7*7*4096 is computational, thus change to 4*4 / 3*3 convs</li><li>further computation decreasement: reduce the fc channels from 4096 to 1024</li></ul></li><li><p>train</p><ul><li>labelï¼šground truth subsampled by 8 </li><li>loss functionï¼šcross-entropy </li></ul></li><li><p>test</p><ul><li>x8ï¼šsimply bilinear interpolation </li><li>fcnï¼šstride32 forces them to use learned upsampling layers, significantly increasing the complexity and training time</li></ul></li><li><p>CRF</p><ul><li><p>short-rangeï¼šused to smooth noisy </p></li><li><p>fully connected modelï¼što recover detailed local structure rather than further smooth it   </p></li><li><p>energy function:</p><script type="math/tex; mode=display">  E(x) = \sum_{i}\theta_i(x_i) + \sum_{ij}\theta_{ij}(x_i, x_j)\\  \theta_i(x_i) = -logP(x_i)\\</script><p>  $P(x_i)$ is the bi-linear interpolated probability output of DCNN.</p><script type="math/tex; mode=display">  \theta_{ij}(x_i, x_j) = \mu(x_i, x_j)\sum_{m=1}^K \omega_m k^m (f_i,f_j)\\  \mu(x_i, x_j) = \begin{cases}  1& \text{if }x_i \neq x_j\\  0& \text{otherwise}  \end{cases}</script><p>  $k^m(f_i, f_j)$ is the Gaussian kernel depends on features (involving pixel positions &amp; pixel color intensities)</p></li></ul></li><li><p>multi-scale prediction</p><ul><li>to increase the boundary localization accuracy </li><li>we attach to the input image and the output of each of the first four max pooling layers a two-layer MLP (first layer: 128 3x3 convolutional filters, second layer: 128 1x1 convolutional filters)  </li><li>the feature maps above is <strong>concatenated</strong> to the main networkâ€™s last layer feature map </li><li>the new outputs is enhanced by 128*5=640 channels</li><li>we only adjust the newly added weights</li><li>introducing these extra direct connections from fine-resolution layers improves localization performance, <strong>yet the effect is not as dramatic as the one obtained with the fully-connected CRF</strong></li></ul></li></ul></li><li><p>ç©ºæ´å·ç§¯dilated convolution</p><p> <img src="/2020/02/24/Deeplabç³»åˆ—/holes.png" width="65%"></p><ul><li><p>ç©ºæ´å·ç§¯ç›¸æ¯”è¾ƒäºæ­£å¸¸å·ç§¯ï¼Œå¤šäº†ä¸€ä¸ª hyper-parameterâ€”â€”dilation rateï¼ŒæŒ‡çš„æ˜¯kernelçš„é—´éš”æ•°é‡(æ­£å¸¸çš„convolution dilatation rateæ˜¯1)</p></li><li><p>fcnï¼šå…ˆpoolingå†upsamplingï¼Œè¿‡ç¨‹ä¸­æœ‰ä¿¡æ¯æŸå¤±ï¼Œèƒ½ä¸èƒ½è®¾è®¡ä¸€ç§æ–°çš„æ“ä½œï¼Œä¸é€šè¿‡poolingä¹Ÿèƒ½<strong>æœ‰è¾ƒå¤§çš„æ„Ÿå—é‡</strong>çœ‹åˆ°æ›´å¤šçš„ä¿¡æ¯å‘¢ï¼Ÿ</p></li><li><p>å¦‚å›¾(b)çš„2-dilated convï¼Œkernel sizeåªæœ‰3x3ï¼Œä½†æ˜¯è¿™ä¸ªå·ç§¯çš„æ„Ÿå—é‡å·²ç»å¢å¤§åˆ°äº†7x7ï¼ˆå‡è®¾å‰ä¸€å±‚æ˜¯3x3çš„1-dilated convï¼‰</p></li><li><p>å¦‚å›¾(c)çš„4-dilated convï¼Œkernel sizeåªæœ‰3x3ï¼Œä½†æ˜¯è¿™ä¸ªå·ç§¯çš„æ„Ÿå—é‡å·²ç»å¢å¤§åˆ°äº†15x15ï¼ˆå‡è®¾å‰ä¸¤å±‚æ˜¯3x3çš„1-dilated convå’Œ3x3çš„2-dilated convï¼‰</p></li><li><p>è€Œä¼ ç»Ÿçš„ä¸‰ä¸ª3x3çš„1-dilated convå †å ï¼Œåªèƒ½è¾¾åˆ°7x7çš„æ„Ÿå—é‡</p></li><li><p>dilatedä½¿å¾—åœ¨ä¸åšpoolingæŸå¤±ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼ŒåŠ å¤§äº†æ„Ÿå—é‡ï¼Œè®©æ¯ä¸ªå·ç§¯è¾“å‡ºéƒ½åŒ…å«è¾ƒå¤§èŒƒå›´çš„ä¿¡æ¯</p></li><li><p><strong>The Gridding Effect</strong>ï¼šå¦‚ä¸‹å›¾ï¼Œå¤šæ¬¡å åŠ 3x3çš„2-dilated convï¼Œä¼šå‘ç°æˆ‘ä»¬å°†æ„¿è¾“å…¥ç¦»æ•£åŒ–äº†ã€‚å› æ­¤å åŠ å·ç§¯çš„ dilation rate ä¸èƒ½æœ‰å¤§äº1çš„å…¬çº¦æ•°ã€‚</p><p>  <img src="/2020/02/24/Deeplabç³»åˆ—/grid.png" width="60%"></p></li><li><p><strong>Long-ranged information</strong>ï¼šå¢å¤§dilation rateå¯¹å¤§ç‰©ä½“æœ‰æ•ˆæœï¼Œå¯¹å°ç‰©ä½“å¯èƒ½æœ‰å¼Šæ— åˆ©</p></li><li><p>HDC(Hybrid Dilated Convolution)è®¾è®¡ç»“æ„</p><ul><li><p>å åŠ å·ç§¯çš„ dilation rate ä¸èƒ½æœ‰å¤§äº1çš„å…¬çº¦æ•°ï¼Œå¦‚[2,4,6]</p></li><li><p>å°† dilation rate è®¾è®¡æˆé”¯é½¿çŠ¶ç»“æ„ï¼Œä¾‹å¦‚ [1, 2, 5, 1, 2, 5] å¾ªç¯ç»“æ„ï¼Œé”¯é½¿çŠ¶èƒ½å¤ŸåŒæ—¶æ»¡è¶³å°ç‰©ä½“å¤§ç‰©ä½“çš„åˆ†å‰²è¦æ±‚(å° dilation rate æ¥å…³å¿ƒè¿‘è·ç¦»ä¿¡æ¯ï¼Œå¤§ dilation rate æ¥å…³å¿ƒè¿œè·ç¦»ä¿¡æ¯)</p></li><li><p>æ»¡è¶³$M_i = max [M_{i+1}-2r_i, M_{i+1}-2(M_{i+1}-r_i), r_i]$ï¼Œ$M_i$æ˜¯ç¬¬iå±‚æœ€å¤§dilation rate</p></li><li><p>ä¸€ä¸ªå¯è¡Œæ–¹æ¡ˆ[1,2,5]ï¼š</p><p>  <img src="/2020/02/24/Deeplabç³»åˆ—/hdc.png" width="65%"></p></li></ul></li></ul></li></ol><h2 id="deeplabV2-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs"><a href="#deeplabV2-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs" class="headerlink" title="deeplabV2: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs"></a>deeplabV2: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</h2><ol><li><p>åŠ¨æœº</p><ul><li>atrous convolutionï¼šcontrol the resolution </li><li>atrous spatial pyramid pooling (ASPP) ï¼šmultiple sampling rates </li><li>fully connected Conditional Random Field (CRF) </li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>three challenges in the application of DCNNs to semantic image segmentation </p><ul><li>reduced feature resolutionï¼šmax-pooling and downsampling (â€˜stridingâ€™)  â€”&gt; atrous convolution</li><li>existence of objects at multiple scalesï¼šmulti input scale â€”&gt; ASPP</li><li>reduced localization accuracy due to DCNN invarianceï¼šskip-layers â€”&gt; CRF</li></ul><p><img src="/2020/02/24/Deeplabç³»åˆ—/deeplabV2.png" width="70%"></p></li><li><p>improvements compared to its first version </p><ul><li>better segment objects at multiple scales</li><li>ResNet replaces VGG16</li><li>a more comprehensive experimental evaluation on models &amp; dataset</li></ul></li><li>related works<ul><li>jointly learning of the DCNN and CRF to form an end-to-end trainable feed-forward network </li><li>while in our work still a 2 stage process</li><li>use a series of atrous convolutional layers with increasing rates to aggregate multiscale context </li><li>while in our structure using parallel instead of serial </li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>atrous convolution</p><ul><li>åœ¨ä¸‹é‡‡æ ·ä»¥åçš„ç‰¹å¾å›¾ä¸Šï¼Œè¿è¡Œæ™®é€šå·ç§¯ï¼Œç›¸å½“äºåœ¨åŸå›¾ä¸Šè¿è¡Œä¸Šé‡‡æ ·çš„filter<ul><li>1-Dç¤ºæ„å›¾ä¸Šå¯ä»¥çœ‹å‡ºï¼Œä¸¤è€…æ„Ÿå—é‡ç›¸åŒ</li><li>åŒæ—¶èƒ½ä¿æŒhigh resolution</li></ul></li><li><p>while both the number of filter parameters and the number of operations per position stay constant</p><p><img src="/2020/02/24/Deeplabç³»åˆ—/1d.png" width="40%">   <img src="/2020/02/24/Deeplabç³»åˆ—/2d.png" width="40%">  </p></li><li><p>æŠŠbackboneä¸­ä¸‹é‡‡æ ·çš„å±‚(pooling/conv)ä¸­çš„strideæ”¹æˆ1ï¼Œç„¶åå°†æ¥ä¸‹æ¥çš„convå±‚éƒ½æ”¹æˆ2-dilated convï¼šcould allow us to compute feature responses at the original image resolution</p></li><li>efficiency/accuracy trade-offï¼šusing atrous convolution to increase the resolution by a factor of 4</li><li>followed by fast bilinear interpolation by a factor of 8 to the original image resolution </li><li><p>Bilinear interpolation is sufficient in this setting because the class score maps are quite smooth <strong>unlike FCN</strong></p><p><img src="/2020/02/24/Deeplabç³»åˆ—/scoremap.png" width="40%"></p></li><li><p>Atrous convolution offers easily control of the field-of-view and finds the best trade-off between accurate localization (small field-of-view) and context assimilation (large field-of-view)ï¼šå¤§æ„Ÿå—é‡ï¼ŒæŠ½è±¡èåˆä¸Šä¸‹æ–‡ï¼Œå¤§æ„Ÿå—é‡ï¼Œlow-levelå±€éƒ¨ä¿¡æ¯å‡†ç¡®</p></li><li>å®ç°ï¼šï¼ˆ1ï¼‰æ ¹æ®å®šä¹‰ï¼Œç»™filterä¸Šé‡‡æ ·ï¼Œæ’0ï¼›ï¼ˆ2ï¼‰ç»™feature mapä¸‹é‡‡æ ·å¾—åˆ°k*kä¸ªreduced resolution mapsï¼Œç„¶årun orgin convï¼Œç»„åˆä½ç§»ç»“æœ</li></ul></li><li><p>ASPP</p><ul><li><p>multi input scaleï¼š</p><ul><li>run parallel DCNN branches that share the same parameters</li><li>fuse by taking at each position the maximum response across scales</li><li>computing</li></ul></li><li><p>spatial pyramid pooling</p><ul><li>run multiple parallel filters with different rates</li><li>multi-scale features are further processed in separate branchesï¼šfc7&amp;fc8</li><li><p>fuseï¼šsum fusion</p><p><img src="/2020/02/24/Deeplabç³»åˆ—/aspp.png" width="55%"></p></li></ul></li></ul></li><li><p>CRFï¼škeep the same as V1</p></li></ul></li></ol><h2 id="deeplabV3-Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation"><a href="#deeplabV3-Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation" class="headerlink" title="deeplabV3: Rethinking Atrous Convolution for Semantic Image Segmentation"></a>deeplabV3: Rethinking Atrous Convolution for Semantic Image Segmentation</h2><ol><li><p>åŠ¨æœº</p><ul><li>for segmenting objects at multiple scales<ul><li>employ atrous convolution in cascade or in parallel with multiple atrous rates</li><li>augment ASPP with image-level features encoding global context and further boost performance</li></ul></li><li>without DenseCRF</li></ul></li><li><p>è®ºç‚¹</p><ul><li>our proposed module consists of atrous convolution with various rates and <strong>batch normalization layers</strong></li><li>modules in cascade or in parallelï¼šwhen applying a 3*3  atrous convolution with an extremely large rate, it fails to capture long range information due to image boundary effects</li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>Atrous Convolution</p><p>  for each location $i$ on the output $y$ and a filter $w$,  an $r$-rate atrous convolution is applied over the input feature map $x$ï¼š</p><script type="math/tex; mode=display">  y[i] = \sum_k x[i+rk]w[k]</script></li><li><p>in cascade</p><ul><li>duplicate several copies of the last ResNet block (block4)</li><li><p>extra block5, block6, block7 as replicas of block4 </p></li><li><p>multi-rates</p><p><img src="/2020/02/24/Deeplabç³»åˆ—/cascade.png" width="70%"></p></li></ul></li><li><p>ASPP</p><ul><li><p>we include batch normalization within ASPP</p></li><li><p>as the sampling rate becomes larger, the number of valid filter weights becomes smaller (beyond boundary)</p></li><li><p>to incorporate global context informationï¼šwe adopt image-level features by GAP on the last feature map of the model </p><p>  GAP â€”&gt; 1*1*256 conv â€”&gt; BN â€”&gt; bilinearly upsample </p></li><li><p>fusion: concatenated + 1*1 conv</p><p><img src="/2020/02/24/Deeplabç³»åˆ—/aspp+gap.png" width="70%"></p></li><li><p>segï¼šfinal 1*1*n_classes conv</p></li></ul></li><li><p>training details</p><ul><li>large crop size required to make sure the large atrous rates effective </li><li>upsample the output: it is important to keep the groundtruths intact and instead upsample the final logits</li></ul></li></ul></li><li><p>ç»“è®º</p><ul><li><p>output stride=8 å¥½è¿‡16ï¼Œä½†æ˜¯è¿ç®—é€Ÿåº¦æ…¢äº†å‡ å€</p><p><img src="/2020/02/24/Deeplabç³»åˆ—/result.png" width="70%"></p></li></ul></li></ol><h2 id="deeplabV3-Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation"><a href="#deeplabV3-Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation" class="headerlink" title="deeplabV3+: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation"></a>deeplabV3+: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</h2><ol><li><p>åŠ¨æœº</p><ul><li>spatial pyramid pooling module captures rich contextual information  </li><li>encode-decoder structure captures sharp object boundaries </li><li>combine the above two methods</li><li>propose a simple yet effective decoder module </li><li>explore Xception backbone</li></ul></li><li><p>è®ºç‚¹</p><ul><li>even though rich semantic information is encoded through ASPP, detailed information related to object boundaries is missing due to striding operations </li><li>atrous convolution could alleviate but suffer the computational balance</li><li>while encoder-decoder models lend themselves to faster computation (since no features are dilated) in the encoder path and gradually recover sharp object boundaries in the decoder path </li><li><p>æ‰€è°“encoder-decoder structureï¼Œå°±æ˜¯é€šè¿‡encoderå’Œdecoderä¹‹é—´çš„çŸ­è¿æ¥æ¥å°†ä¸åŒå°ºåº¦çš„ç‰¹å¾é›†æˆèµ·æ¥ï¼Œå¢åŠ è¿™æ ·çš„shortcutï¼ŒåŒæ—¶å¢å¤§ç½‘ç»œçš„ä¸‹é‡‡æ ·ç‡ï¼ˆencoder pathä¸Šä¸ä½¿ç”¨ç©ºæ´å·ç§¯ï¼Œå› æ­¤ä¸ºäº†è¾¾åˆ°åŒæ ·çš„æ„Ÿå—é‡ï¼Œå¾—å¢åŠ poolingï¼Œç„¶åä¿ç•™æœ€åº•ç«¯çš„ASPP blockï¼‰ï¼Œæ—¢å‡å°‘äº†è®¡ç®—ï¼Œåˆenrichäº†local borderè¿™ç§ç»†èŠ‚ç‰¹å¾</p><p><img src="/2020/02/24/Deeplabç³»åˆ—/deeplabV3+.png" width="50%"></p></li><li><p>applying the atrous separable convolution to both the ASPP and decoder modulesï¼šæœ€ååˆå¼•å…¥å¯åˆ†ç¦»å·ç§¯ï¼Œè¿›ä¸€æ­¥æå‡è®¡ç®—æ•ˆç‡</p></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>atrous separable convolution </p><ul><li><p>significantly reduces the computation complexity while maintaining similar (or better) performance</p><p><img src="/2020/02/24/Deeplabç³»åˆ—/atrousDW.png" width="50%"></p></li></ul></li><li><p>DeepLabv3 as encoder</p><ul><li>output_stride=16/8ï¼šremove the striding of the last 1/2 blocks</li><li>atrous convolutionï¼šapply atrous convolution to the blocks without striding</li><li>ASPPï¼šrun 1x1 conv in the end to set the output channel to 256</li></ul></li><li><p>proposed decoder</p><ul><li>naive decoderï¼šbilinearly upsampled by 16 </li><li>proposedï¼šfirst bilinearly upsampled by 4, then concatenated with the corresponding low-level features</li><li>low-level featuresï¼š<ul><li>apply 1x1 conv on the low-level features to reduce the number of channels to avoid <strong>outweigh the importance</strong></li><li>the last feature map in res2x residual block before striding </li></ul></li><li>combined featuresï¼šapply 3x3 conv(2 layers, 256 channels) to obtain sharper segmentation results </li><li>more shortcutï¼šobserved no significant improvement</li></ul><p><img src="/2020/02/24/Deeplabç³»åˆ—/en-de.png" width="45%"></p></li><li><p>modified Xception backbone</p><ul><li>deeper</li><li>all the max pooling operations are replaced with depthwise separable convolutions with striding </li><li><p>DWconv-BN-ReLU-PWconv-BN-ReLU</p><p><img src="/2020/02/24/Deeplabç³»åˆ—/xception.png" width="50%"></p></li></ul></li></ul></li><li><p>å®éªŒ</p><ol><li><p>decoder effect on border</p><p> <img src="/2020/02/24/Deeplabç³»åˆ—/border.png" width="50%"></p></li></ol></li><li><p>f</p></li></ol><h2 id="Auto-DeepLab-Hierarchical-Neural-Architecture-Search-for-Semantic-Image-Segmentation"><a href="#Auto-DeepLab-Hierarchical-Neural-Architecture-Search-for-Semantic-Image-Segmentation" class="headerlink" title="Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation"></a>Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation</h2>]]></content>
      
      
        <tags>
            
            <tag> papers </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>RCNNç³»åˆ—</title>
      <link href="/2020/01/08/RCNN%E7%B3%BB%E5%88%97/"/>
      <url>/2020/01/08/RCNN%E7%B3%BB%E5%88%97/</url>
      <content type="html"><![CDATA[<h2 id="ç»¼è¿°"><a href="#ç»¼è¿°" class="headerlink" title="ç»¼è¿°"></a>ç»¼è¿°</h2><ol><li><p>papers</p><p> [R-CNN] R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation </p><p> [SPP] SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</p><p> [Fast R-CNN] Fast R-CNN: Fast Region-based Convolutional Network</p><p> [Faster R-CNN] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</p><p> [Mask R-CNN] Mask R-CNN  </p><p> [FPN] FPN: Feature Pyramid Networks for Object Detection</p><p> [Cascade R-CNN] Cascade R-CNN: Delving into High Quality Object Detection</p></li></ol><h2 id="R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation"><a href="#R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation" class="headerlink" title="R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation"></a>R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation</h2><ol><li><p>åŠ¨æœº</p><ul><li>localizing objects with a deep network and training a high-capacity model with only a small quantity of annotated detection data<ul><li>apply CNN to region proposals: R-CNN represents â€˜Regions with CNN featuresâ€™</li><li>supervised pre-training </li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>model as a regression problem: not fare well in practice  </li><li>build a sliding-window detector: have to maintain high spatial resolution</li><li><strong>what we do: </strong>our method gener- ates around 2000 category-independent region proposals for the input image, extracts a fixed-length feature vector from each proposal using a CNN, and then classifies each region with category-specific linear SVMs </li><li>conventional solution to training a large CNN is â€˜using unsupervised pre-training, followed by supervised fine-tuningâ€™</li><li><strong>what we do: </strong>â€˜supervised pre-training on a large auxiliary dataset (ILSVRC), followed by domain specific fine-tuning on a small dataset (PASCAL)â€™</li><li><strong>we also demonstrate: </strong>a simple bounding box regression method significantly reduces mislocalizations</li><li><strong>R-CNN operates on regions:</strong> it is natural to extend it to the task of semantic segmentation </li></ul></li><li><p>è¦ç´ </p><ul><li>category-independent region proposals </li><li>a large convolutional neural network that extracts a fixed-length feature vector from each region </li><li><p>a set of class-specific linear SVMs</p><p><img src="/2020/01/08/RCNNç³»åˆ—/R-CNN.png" width="40%;"></p></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>Region proposals: we use selective search</p></li><li><p>Feature extraction: we use Krizhevsky CNN, 227*227 RGB input, 5 convs, 2 fcs, 4096 output</p><ul><li>we first dilate the tight bounding box (padding=16)</li><li><p>then warp the bounding box to the required size (å„å‘å¼‚æ€§ç¼©æ”¾)</p><p><img src="/2020/01/08/RCNNç³»åˆ—/warp.png" width="40%;"></p></li></ul></li><li><p>Test-time detection:</p><ul><li>we score each extracted feature vector using the SVM trained for each class</li><li>we apply a greedy non-maximum suppression (for each class independently)  </li><li>å¯¹ç•™ä¸‹çš„è¿™äº›æ¡†è¿›è¡Œcannyè¾¹ç¼˜æ£€æµ‹ï¼Œå°±å¯ä»¥å¾—åˆ°bounding-box</li><li>(then B-BoxRegression)</li></ul></li><li><p>Supervised pre-training: pre-trained the CNN on a large auxiliary dataset (ILSVRC 2012) with <strong>image-level annotations</strong></p></li><li><p>Domain-specific fine-tuning: </p><ul><li>continue SGD training of the CNN using only warped region proposals from VOC </li><li>replace the 1000-way classification layer with a randomly initialized 21-way layer (20 VOC classes plus background)</li><li><strong>class label: all region proposals with â‰¥ 0.5 IoU overlap with a ground-truth box as positives, else negatives </strong></li><li>1/10th of the initial pre-training rate</li><li>uniformly sample 32 positive windows (over all classes) and 96 background windows to construct a mini-batch of size 128 </li></ul></li><li><p>Object category classifiers:</p><ul><li>considering a binary classifier for a specific class</li><li><strong>class label: take IoU overlap threshold &lt;0.3 as negatives, take only regions tightly enclosing the object as positives </strong> </li><li>take the ground-truth bounding boxes for each class as positives</li></ul></li><li><p><strong>unexplained:</strong></p><ul><li><p>the positive and negative examples are defined differently in CNN fine-tuning versus SVM training</p><p>  CNNå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œéœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œæ‰€ä»¥åœ¨CNNè®­ç»ƒé˜¶æ®µæˆ‘ä»¬å¯¹Bounding boxçš„ä½ç½®é™åˆ¶æ¡ä»¶é™åˆ¶çš„æ¯”è¾ƒæ¾(IOUåªè¦å¤§äº0.5éƒ½è¢«æ ‡æ³¨ä¸ºæ­£æ ·æœ¬)ï¼Œsvmé€‚ç”¨äºå°‘æ ·æœ¬è®­ç»ƒï¼Œæ‰€ä»¥å¯¹äºè®­ç»ƒæ ·æœ¬æ•°æ®çš„IOUè¦æ±‚æ¯”è¾ƒä¸¥æ ¼ï¼Œæˆ‘ä»¬åªæœ‰å½“bounding boxæŠŠæ•´ä¸ªç‰©ä½“éƒ½åŒ…å«è¿›å»äº†ï¼Œæˆ‘ä»¬æ‰æŠŠå®ƒæ ‡æ³¨ä¸ºç‰©ä½“ç±»åˆ«ã€‚</p></li><li><p>itâ€™s necessary to train detection classifiers rather than simply use outputs of the fine-tuned CNN</p><p>  ä¸Šä¸€ä¸ªå›ç­”å…¶å®åŒæ—¶ä¹Ÿè§£é‡Šäº†CNNçš„headå·²ç»æ˜¯ä¸€ä¸ªåˆ†ç±»å™¨äº†ï¼Œè¿˜è¦ç”¨SVMåˆ†ç±»ï¼šæŒ‰ç…§ä¸Šè¿°æ­£è´Ÿæ ·æœ¬å®šä¹‰ï¼ŒCNN softmaxçš„è¾“å‡ºæ¯”é‡‡ç”¨svmç²¾åº¦ä½ã€‚</p></li></ul></li></ul></li><li><p>åˆ†æ</p><ul><li><p>learned features:</p><ul><li>compute the unitsâ€™ activations on a large set of held-out region proposals  </li><li>sort from the highest to low</li><li>perform non-maximum suppression</li><li><p>display the top-scoring regions</p><p><img src="/2020/01/08/RCNNç³»åˆ—/activations.png" width="70%;"></p></li></ul></li><li><p>Ablation studies:</p><ul><li><strong>without fine-tuning:</strong> features from fc7 generalize worse than features from fc6, indicating that most of the CNNâ€™s representational power comes from its convolutional layers</li><li><strong>with fine-tuning: </strong>The boost from fine-tuning is much larger for fc6 and fc7 than for pool5, suggests that pool features learned from ImageNet are <strong>general</strong> and that most of the improvement is gained from learning <strong>domain-specific</strong> non-linear classifiers on top of them</li></ul></li><li><p>Detection error analysis:</p><ul><li>more of our errors result from poor localization rather than confusion </li><li>CNN features are much more discriminative than HOG </li><li>Loose localization likely results from our use of bottom-up region proposals and the positional invariance learned from pre-training the CNN for whole-image classification(ç²—æš´çš„IOUåˆ¤å®šå‰èƒŒæ™¯ï¼ŒäºŒå€¼åŒ–labelï¼Œæ— æ³•ä½“ç°å®šä½å¥½åå·®å¼‚)</li></ul></li><li><p>Bounding box regressionï¼š</p><ul><li>a linear regression model use the pool5 features for a selective search region proposal as input</li><li>è¾“å‡ºä¸ºxyæ–¹å‘çš„ç¼©æ”¾å’Œå¹³ç§»</li><li>è®­ç»ƒæ ·æœ¬ï¼šåˆ¤å®šä¸ºæœ¬ç±»çš„å€™é€‰æ¡†ä¸­å’ŒçœŸå€¼é‡å é¢ç§¯å¤§äº0.6çš„å€™é€‰æ¡†</li></ul></li><li><p>Semantic segmentationï¼š</p><ul><li>three strategies for computing features:<ul><li>â€˜<em>full</em> â€˜ ignores the regionâ€™s shape, two regions with different shape might have very similar bounding boxes(ä¿¡æ¯ä¸å……åˆ†)</li><li>â€˜<em>fg</em> â€˜ slightly outperforms <em>full</em>, indicating that the masked region shape provides a stronger signal</li><li>â€˜<em>full+fg</em> â€˜ achieves the best, indicating that the context provided by the <em>full</em> features is highly informative even given the <em>fg</em> features(å½¢çŠ¶å’Œcontextä¿¡æ¯éƒ½é‡è¦)</li></ul></li></ul></li></ul></li></ol><h2 id="SPP-net-Spatial-Pyramid-Pooling-in-Deep-Convolutional-Networks-for-Visual-Recognition"><a href="#SPP-net-Spatial-Pyramid-Pooling-in-Deep-Convolutional-Networks-for-Visual-Recognition" class="headerlink" title="SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition"></a>SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</h2><ol><li><p>åŠ¨æœºï¼š</p><ul><li>propose a new pooling strategy, â€œspatial pyramid poolingâ€</li><li>can generate a fixed-length representation regardless of image size/scale</li><li>also robust to object deformations </li></ul></li><li><p>è®ºç‚¹ï¼š</p><ul><li>existing CNNs require a fixed-size input<ul><li>reduce accuracy for sub-images of an arbitrary size/scale (need cropping/warping)</li><li>cropped region lost content, while warped content generates unwanted distortion</li><li>overlooks the issues involving scales </li></ul></li><li>convolutional layers do not require a fixed image size, whle the fully-connected layers need to have fixed- size/length input by their definition</li><li>by introducing the SPP layer<ul><li>between the last convolutional layer and the first fully-connected layer</li><li>pools the features and generates fixed- length outputs</li></ul></li><li>Spatial pyramid pooling <ul><li>partitions the image into divisions from finer to coarser levels, and aggregates local features in them</li><li>generates fixed- length output </li><li>uses multi-level spatial bins(robust to object deformations )</li><li>can run at variable scales </li><li>also allows varying sizes or scales <strong>during training</strong>: <ul><li>train the network with different input size at different epoch</li><li>increases scale-invariance </li><li>reduces over-fitting </li></ul></li><li>in object detection <ul><li>run the convolutional layers only <em>once</em> on the entire image </li><li>then extract features by SPP-net on the feature maps </li><li>speedup </li><li>accuracy </li></ul></li></ul></li></ul><ol><li><p>æ–¹æ³•ï¼š</p><ul><li><p>Convolutional Layers and Feature Maps</p><ul><li>the outputs of the convolutional layers are known as feature maps</li><li><p>feature maps involve not only the strength of the responses(the strength of activation), but also their spatial positions(the reception field)</p><p><img src="/2020/01/08/RCNNç³»åˆ—/feature maps.png" width="80%;"></p></li></ul></li><li><p>The Spatial Pyramid Pooling Layer</p><ul><li>it can maintain spatial information by pooling in local spatial bins</li><li>the spatial bins have sizes proportional to the image size(k-level: 1*1, 2*2, â€¦, k*k)</li><li>we can resize the input image to any scale, which is important for the accuracy  </li><li><p>the coarsest pyramid level has a single bin that covers the entire image, which is in fact a â€œglobal poolingâ€ operation </p><p><img src="/2020/01/08/RCNNç³»åˆ—/spp.png" width="40%;"></p></li><li><p>for a feature map of $aÃ—a$, with a pyramid level of $nÃ—n$ bins:</p><script type="math/tex; mode=display">  the\ window\ size:\ win = ceiling(a/n)\\  the\ stride:\ str = floor(a/n)</script></li></ul></li></ul></li></ol><ul><li><p>Training the Network</p><ul><li>Single-size training: fixed-size input (224Ã—224) cropped from images, cropping for data augmentation<ul><li>Multi-size training: rather than cropping, we resize the aforementioned 224Ã—224 region to 180Ã—180, then we train two fixed-size networks that share parameters by altenate epoch</li></ul></li></ul></li></ul></li><li><p>åˆ†æ</p><ul><li><strong>50 bins vs. 30 bins: </strong>the gain of multi-level pooling is not simply due to more parameters, it is because the multi-level pooling is robust to the variance in object deformations and spatial layout<ul><li><strong>multi-size vs. single-size:  </strong>multi results are more or less better than the single-size version</li><li><strong>full vs. crop: </strong>shows the importance of maintaining the complete content</li></ul></li></ul></li><li><p><strong>SPP-NET FOR OBJECT DETECTION</strong></p><ul><li><p>We extract the feature maps from the entire image only once </p></li><li><p>we apply the spatial pyramid pooling on each candidate window of the feature maps  </p></li><li><p>These representations are provided to the fully-connected layers of the network </p></li><li><p>SVM samples: We use the ground-truth windows to generate the positive samples, use the samples with IOU&lt;30% as the negative samples </p></li><li><p>multi-scale feature extraction: </p><ul><li>We resize the image at {480, 576, 688, 864, 1200}, and compute the feature maps of conv5 for each scale. <ul><li>we choose a single scale s âˆˆ S such that the scaled candidate window has a number of pixels closest to 224Ã—224. </li><li>And we use the corresponding feature map to compute the feature for this window</li><li>this is roughly equivalent to resizing the window to 224Ã—224 </li></ul></li></ul></li><li><p>fine-tuning:</p><ul><li>Since our features are pooled from the conv5 feature maps from windows of any sizes<ul><li>for simplicity we only fine-tune the fully-connected layers </li></ul></li></ul></li><li><p>Mapping a Window to Feature Maps**</p><ul><li><p>we project the corner point of a window onto a pixel in the feature maps, such that this corner point in the image domain is closest to the center of the receptive field of that feature map pixel.  </p><p><img src="/2020/01/08/RCNNç³»åˆ—/mapping.png" width="50%;"></p><p>â€‹    ç¡®å®šåŸå›¾ä¸Šçš„ä¸¤ä¸ªè§’ç‚¹ï¼ˆå·¦ä¸Šè§’å’Œå³ä¸‹è§’ï¼‰ï¼Œæ˜ å°„åˆ° feature mapä¸Šçš„ä¸¤ä¸ªå¯¹åº”ç‚¹ï¼Œä½¿å¾—æ˜ å°„ç‚¹$(x^{â€˜}, y^{â€˜})$åœ¨åŸå§‹å›¾ä¸Š<strong>æ„Ÿå—é‡ï¼ˆä¸Šå›¾ç»¿è‰²æ¡†ï¼‰çš„ä¸­å¿ƒç‚¹</strong>ä¸$(x,y)$å°½å¯èƒ½æ¥è¿‘ã€‚</p></li></ul></li></ul></li></ol><h2 id="Fast-R-CNN-Fast-Region-based-Convolutional-Network"><a href="#Fast-R-CNN-Fast-Region-based-Convolutional-Network" class="headerlink" title="Fast R-CNN: Fast Region-based Convolutional Network"></a>Fast R-CNN: Fast Region-based Convolutional Network</h2><ol><li><p>åŠ¨æœº</p><ul><li>improve training and testing speed</li><li>increase detection accuracy</li></ul></li><li><p>è®ºç‚¹</p><ul><li>current approaches train models in multi-stage pipelines that are slow and inelegant<ul><li>R-CNN &amp; SPPnet: CNN+SVM+bounding-box regression</li><li>disk storage: features are written to disk </li><li>SPPnet: can only fine-tuning the fc layers, limits the accuracy of very deep networks</li></ul></li><li>task complexity:<ul><li>numerous candidate proposals</li><li>rough localization proposals must be refined </li></ul></li><li>We propose:<ul><li>a single-stage training algorithm </li><li>multi-task: jointly learns to classify object proposals and refine their spatial locations </li></ul></li></ul></li><li><p>è¦ç´ </p><ul><li>input: an entire image and a set of object proposals </li><li>convs</li><li>a region of interest (RoI) pooling layer: extracts a fixed-length feature vector from the feature map</li><li>fcs that finally branch into two sibling output layers</li><li><p>multi-outputs:</p><ul><li>one produces softmax probability over K+1 classes</li><li>one outputs four bounding-box regression offsets per class</li></ul><p><img src="/2020/01/08/RCNNç³»åˆ—/fastRCNN.png" width="40%;"></p></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>RoI pooling</p><ul><li>an RoI is a rectangular window inside a conv feature map, which can be defined by (r, c, h, w)  </li><li>the RoI pooling layer converts the features inside any valid RoI into a small feature map with a fixed size H Ã— W </li><li>it is a special case of SPPnet when there is only one pyramid level (pooling window size = h/H * w/W)</li></ul></li><li><p>Initializing from pre-trained networks</p><ul><li>the last max pooling layer is replaced by a RoI pooling layer </li><li>the last fully connected layer and softmax is replaced by the wo sibling layers + respective head (softmax &amp; regressor)</li><li>modified to take two inputs</li></ul></li><li><p>Fine-tuning for detection</p><ul><li><p>why SPPnet is unable to update weights below the spatial pyramid pooling layer: </p><ul><li><p>åŸæ–‡æåˆ°feature vectoræ¥æºäºä¸åŒå°ºå¯¸çš„å›¾åƒâ€”â€”ä¸æ˜¯ä¸»è¦åŸå› </p></li><li><p>feature vectoråœ¨åŸå›¾ä¸Šçš„æ„Ÿå—é‡é€šå¸¸å¾ˆå¤§ï¼ˆæ¥è¿‘å…¨å›¾ï¼‰â€”â€”forward passçš„è®¡ç®—é‡å°±å¾ˆå¤§</p></li><li><p>ä¸åŒçš„å›¾ç‰‡forward passçš„è®¡ç®—ç»“æœä¸èƒ½å¤ç”¨ï¼ˆwhen each training sample (<em>i.e</em>. RoI) comes from a different image, which is exactly how R-CNN and SPPnet networks are trainedï¼‰</p></li></ul></li><li><p>We propose:</p><ul><li><p>takes advantage of feature sharing </p></li><li><p>mini-batches are sampled hierarchically: N images and R/N RoIs from each image</p></li><li><p>RoIs from the same image share computation and memory in the forward and backward passes </p></li><li><p>jointly optimize the two tasks</p><p>  each RoI is labeled with a ground-truth class $u$ and a ground-truth bounding-box regression target $v$ </p><p>  the network outputs are K+1 probability $p=(p_0,â€¦p_k)$ and K b-box regression offsets $t^k=(t_x^k, t_y^k, t_w^k,t_h^k)$</p><script type="math/tex; mode=display">  L(p, u, t^u, v) = L_{cls}(p,u) + \lambda[u>0]L_{loc}(t^u,v)\\</script><p>  $L_{cls}$:</p><script type="math/tex; mode=display">  L_{cls}(p,u) = -log p_u\\</script><p>  $L_{loc}$:</p><script type="math/tex; mode=display">  L_{loc}(t^u, v) = \sum_{i \in \{x,y,w,h\}}smooth_{L_1}(t^u_i - v_i)\\  smooth_{L_1}(x) =   \begin{cases}  0.5x^2\ \ \ \ \ \ \ \ \ \ \ if |x|<1\\  |x| - 0.5\ \ \ \ \ \ otherwise  \end{cases}</script><p>  ä½œè€…è¡¨ç¤ºè¿™ç§å½¢å¼å¯ä»¥å¢å¼ºæ¨¡å‹å¯¹å¼‚å¸¸æ•°æ®çš„é²æ£’æ€§</p><p>  <img src="/2020/01/08/RCNNç³»åˆ—/smoothL1.png" width="30%;"></p></li><li><p>class label: take $IoU\geq0.5$ as a foreground object, take negatives with $IoU \in [0.1,0.5)$</p><p>  The lower threshold of 0.1 appears to act as a heuristic for hard example mining </p></li><li></li></ul></li></ul></li><li><p>Truncated SVD for faster detection</p><ul><li>Large fully connected layers are easily accelerated by compressing them with truncated SVD <script type="math/tex; mode=display">  W \approx U \Sigma_t V^T</script></li></ul></li></ul></li></ol><ul><li><p>the single fully connected layer corresponding to W is replaced by two fully connected layers, <strong>without non-linearity</strong></p></li><li><p>The first layers uses the weight matrix $\Sigma_t V^T$(and no biases)</p></li><li><p>the second uses U (with the original biases)</p></li></ul><ol><li><p>åˆ†æ</p><ul><li>Fast R-CNN vs. SPPnet: even though Fast R-CNN uses single-scale training and testing, <strong>fine-tuning the conv layers</strong> provides a large improvement in mAP </li><li>Truncated SVD can reduce detection time by more than 30% with only a small (0.3 percent- age point) drop in mAP </li><li>deep vs. small networks: <ul><li>for very deep networks fine-tuning the conv layers is important </li><li>in the smaller networks (S and M) we find that conv1 is generic and task independent </li><li>all Fast R-CNN results in this paper using models L fine-tune layers conv3_1 and up</li><li>all experiments with models S and M fine-tune layers conv2 and up </li></ul></li><li>multi-task training vs. stage-wise: it has the potential to improve results because the tasks influence each other through a shared representation (the ConvNet) </li><li>single-scale vs. multi-scale: <ul><li>single-scale detection performs almost as well as multi-scale detection </li><li>deep ConvNets are adept at directly learning scale invariance </li><li>single-scale processing offers the best tradeoff be- tween speed and accuracy thus we choose single-scale</li></ul></li><li>softmax vs. SVM:<ul><li>â€œone-shotâ€ fine-tuning is sufficient compared to previous multi-stage training approaches</li><li>softmax introduces competition, while SVMs are one-vs-rest </li></ul></li></ul></li></ol><h2 id="Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks"><a href="#Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks" class="headerlink" title="Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"></a>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</h2><ol><li><p>åŠ¨æœº</p><ul><li>shares the convolutional features</li><li>merge the system using the concept of â€œattentionâ€ mechanisms </li><li>sharing convolutions across proposals â€”-&gt; across tasks</li><li>translation-Invariant &amp; scale/ratio-Invariant</li></ul></li><li><p>è®ºç‚¹</p><ul><li>proposals are now the test-time computational bottleneck in state-of-the-art detection systems</li><li>the region proposal methods are generally implemented on the CPU</li><li>we observe that the convolutional feature maps used by region-based detectors, like Fast R- CNN, can also be used for generating region proposals</li></ul></li><li><p>è¦ç´ </p><ul><li>RPN: On top of the convolutional features, we construct an RPN by adding a few additional convolutional layers that simultaneously regress region bounds and objectness scores at each location <strong>on a regular grid</strong></li><li>anchor: serves as references at multiple scales and aspect ratios </li><li><p>unify RPN and Fast R-CNN detector: we propose a training scheme that alternately fine-tuning the region proposal task and the object detection task</p><p><img src="/2020/01/08/RCNNç³»åˆ—/fasterRCNN.png" width="35%;"></p></li></ul></li><li><p>æ–¹æ³•</p><p> 4.1 Region Proposal Networks</p><p> <img src="/2020/01/08/RCNNç³»åˆ—/RPN.png" width="65%;"></p><ul><li><p>This architecture is naturally implemented with an nÃ—n convolutional layer followed by two sibling 1 Ã— 1 convolutional layers (for <em>reg</em> and <em>cls</em>, respectively)</p></li><li><p>conv: an n Ã— n sliding window </p></li><li><p>feature: 256-d for ZF(5 convs backbone) and 512-d for VGG(13 convs backbone)</p></li><li><p>two sibling fully-connected layers + respective output layer</p></li><li><p>anchors</p><ul><li>predict multiple region proposals: denoted as k</li><li>the reg head has 4k outputs, the cls head has 2k outputs</li><li>the k proposals are parameterized relative to <strong>k reference boxes</strong>â€”â€”â€”â€”the anchors</li><li>an anchor box is centered at the sliding window in question, and is associated with a scale and aspect ratio </li><li>for a convolutional feature map of a size W Ã— H , that is WHk anchors in total</li></ul></li><li><p>class label</p><ul><li>positives1: the anchors with the highest IoU with a ground-truth box </li><li>positives2: the anchors that has an IoU higher than 0.7 with any ground-truth box </li><li>negatives: non-positive anchors if their IoU is lower than 0.3 <strong>for all</strong> ground-truth boxes</li><li>the left: do not contribute </li><li>ignored: all cross-boundary anchors </li></ul></li><li><p>Loss function</p><ul><li><p>similar multi-task loss as fast-RCNN, with a normalization term</p></li><li><p>with $x,y,w,h$ denoting the boxâ€™s <strong>center coordinates</strong> and its width and height, the regression branch outputs $t_i$:</p><script type="math/tex; mode=display">  t_x = (x - x_a) / w_a\\  t_y = (y - y_a) / h_a\\  t_w = log(w/ w_a)\\  t_h = log(h/ h_a)</script></li></ul></li><li><p>mini-batch: sampled the positive and negative anchors from a single image with the ratio of 1:1 </p><p>4.2 the unified network </p></li><li><p>Alternating training</p><ul><li>ImageNet-pre-trained model, fine-tuning end-to-end for the region proposal task </li><li>ImageNet-pre-trained model, using the RPN proposals, fine-tuning end-to-end for the detection task </li><li>fixed detection network convs, fine-tuning the unique layers for region proposal </li><li>fixed detection network convs, fine-tuning the unique layers for detection</li></ul></li><li><p>Approximate joint training</p><ul><li>multi-task loss</li><li>approximate </li></ul><p>4.3 at training time</p></li><li><p>the total stride is 16 (input size / feature map size)</p></li><li>for a typical 1000 Ã— 600 image, there will be roughly 20000 (60*40*9) anchors in total </li><li><p>we ignore all cross-boundary anchors, there will be about 6000 anchors per image left for training</p><p>4.4 at testing time</p></li><li><p>we use NMS(iou_thresh=0.7), that leaves 2000 proposals per image</p></li><li>then we use the top-N ranked proposal regions for detection</li></ul></li><li><p>åˆ†æ</p></li></ol><h2 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h2><ol><li><p>åŠ¨æœº</p><ul><li><strong>instance segmentation: </strong><ul><li>detects objects while simultaneously generating instance mask</li><li>æ³¨æ„ä¸ä»…ä»…æ˜¯ç›®æ ‡æ£€æµ‹äº†</li></ul></li><li>easy to generalize to other tasks: <ul><li>instance segmentation</li><li>bounding-box object detection</li><li>person keypoint detection</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>challenging:<ul><li>requires the correct detection of objects</li><li>requires precisely segmentation of instances</li></ul></li><li><strong>a simple, flexible, and fast system can surpass all</strong><ul><li>adding a branch for predicting segmentation on Faster-RCNN</li><li>in parallel with the existing branch for classification and regression </li><li>the mask branch is a small FCN applied to each RoI</li></ul></li><li>Faster R- CNN was not designed for pixel-to-pixel alignment  </li><li><strong>we propose RoIAlign to preserve exact spatial locations</strong></li><li>FCNs usually perform per-pixel multi-class categorization, which couples segmentation and classification</li><li><p><strong>we predict a binary mask for each class independently, decouple mask(mask branch) and class(cls branch)</strong></p></li><li><p>other combining methods are multi-stage</p></li><li><strong>our method is based on parallel prediction</strong>  </li><li>FCIS also run the system in parallel but exhibits systematic errors on overlapping instances and creates spurious edges </li><li>segmentation-first strategies attempt to cut the pixels of the same category into different instances </li><li><strong>Mask R-CNN is based on an instance-first strategy</strong></li></ul></li><li><p>è¦ç´ </p><ul><li>a mask branch with $Km^2$-dims outputs for each RoI, m denotes the resolution, K denotes the number of classes</li><li>bce is key for good instance segmentation results:  $L_{mask} = [y&gt;0]\frac{1}{m^2}\sum bce_loss$</li><li>RoI features that are well aligned to the per-pixel input </li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>RoIAlign</p><ul><li>Quantizations in RoIPool: (1) RoI to feature map $[x/16]$; (2) feature map to spatial bins $[a/b]$; $[]$ denotes roundings</li><li>These quantizations introduce misalignments </li><li><p>We use bilinear interpolation to avoid quantization</p><ul><li>sample several points in the spatial bins</li><li>computes the value of each sampling point by bilinear interpolation from the nearby grid points on the feature map</li><li>aggregate the results of sampling points (using max or average) </li></ul><p><img src="/2020/01/08/RCNNç³»åˆ—/RoIAlign.png" width="50%;"></p></li></ul></li><li><p>Architecture </p><ul><li>backbone: using a ResNet-FPN backbone for feature extraction gives excellent gains in both accuracy and speed </li><li><p>head: use previous heads in ResNet/FPN(res5 contained in head/backbone)</p><p><img src="/2020/01/08/RCNNç³»åˆ—/mrcnnheads.png" width="45%;"></p></li></ul></li><li><p>Implementation Details </p><ul><li>positives: RoIs with IoU at least 0.5, otherwise negative</li><li>loss: dice loss defined only on positive RoIs</li><li>mini-batch: 2 images, N RoIs</li><li>at training time: parallel computation  for 3 branches</li><li>at test time: <ul><li>serial computation</li><li>proposals -&gt; box prediction -&gt; NMS -&gt; run mask branch on the highest scoring 100 detection boxes  </li><li>it speeds up inference and improves accuracy </li><li>the $28*28$ floating-number mask output is resized to the RoI size, and binarized at a threshold of 0.5</li></ul></li></ul></li></ul></li><li><p>åˆ†æ</p><ul><li>on overlapping instances: FCIS+++ exhibits systematic artifacts </li><li>architecture: it benefits from deeper networks (50 vs. 101) and advanced designs including FPN and ResNeXt</li><li>FCN vs. MLP for mask branch</li><li><strong>Human Pose Estimation</strong><ul><li>We model a keypointâ€™s location as a <strong>one-hot mask</strong>, and adopt Mask R-CNN to predict K masks, one for each of K keypoint types </li><li>the training target is a one-hot $m<em>m$ binary mask where only a </em>single* pixel is labeled as foreground </li><li>use the cross-entropy loss </li><li>We found that a relatively high resolution output ($56*56$ compared to masks) is required for keypoint-level localization accuracy</li></ul></li></ul></li></ol><h2 id="FPN-Feature-Pyramid-Networks-for-Object-Detection"><a href="#FPN-Feature-Pyramid-Networks-for-Object-Detection" class="headerlink" title="FPN: Feature Pyramid Networks for Object Detection"></a>FPN: Feature Pyramid Networks for Object Detection</h2><ol><li><p>åŠ¨æœº</p><ul><li>for object detection in multi-scale</li><li>struct feature pyramids with marginal extra cost</li><li>practical and accurate</li><li>leverage the pyramidal shape of a ConvNetâ€™s feature hierarchy while creating a feature pyramid that has strong semantics at all scales</li></ul></li><li><p>è®ºç‚¹</p><ul><li>single scale offers a good trade-off between accuracy and speed while multi-scale still performs better, especially for small objects</li><li>featurized image pyramids form the basis solution for multi-scale</li><li>ConvNets are proved robust to variance in scale and thus facilitate recognition from features computed on a single input scale </li><li>SSD uses the naturely feature hierarchy generated by ConvNet which introduces large semantic gaps caused by different depths<ul><li>high-level features are low-resolution but <strong>semantically strong</strong></li><li>low-level features are of lower-level semantics, but their activations are more <strong>accurately localized</strong> as subsampled fewer times</li></ul></li><li><p>thus we propose FPN:</p><ul><li>combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections </li><li>has rich semantics at all levels </li><li>built from a single scale</li><li>can be easily extended to mask proposals</li><li>can be trained end-to- end with all scales </li></ul><p><img src="/2020/01/08/RCNNç³»åˆ—/pyramids.png" width="40%;"></p></li><li><p>similar architectures make predictions only on a fine resolution  </p><p><img src="/2020/01/08/RCNNç³»åˆ—/similar structs.png" width="40%;"></p></li></ul></li><li><p>è¦ç´ </p><ul><li>takes a single-scale image of an arbitrary size as input </li><li>outputs proportionally sized feature maps at multiple levels</li><li>structure <ul><li>a bottom-up pathway:  the feed-forward computation of the backbone ConvNet</li><li>a top-down pathway and lateral connection: <ul><li>upsampling the spatially coarser, but semantically stronger, feature maps from higher pyramid levels</li><li>then enhance with features from the bottom-up pathway via lateral connections</li><li>a $3<em>3$ conv is appended on each merged map <em>*to reduce the aliasing effect of upsampling</em></em></li><li>shared classifiers/regressors among all levels, thus using fixed 256 channels convs</li><li>upsamling uses nearest neighbor interpolation</li><li>low-level features undergoes a $1*1$ conv to reduce channel dimensions </li><li>merge operation is a <strong>by element-wise addition</strong></li></ul></li></ul></li><li>adopt the method in RPN &amp; Fast-RCNN for demonstration</li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>RPN</p><ul><li>original design: <ul><li>backbone Convs -&gt; single-scale feature map -&gt; dense 3Ã—3 sliding windows -&gt; head($3<em>3$ convs + 2 sibling $1</em>1$ conv branches)</li><li>for regressor: multi-scale anchors(e.g. 3 scales 3 ratios -&gt; 9 anchors)</li></ul></li><li>new design: <ul><li>adapt FPN -&gt; multi-scale feature map -&gt; sharing heads</li><li>for regressor: set single-scale anchor for each level respectively (e.g. 5 level 3 ratios -&gt; 15 anchors)</li></ul></li><li>sharing heads:<ul><li>vs. not sharing: similar accuracy </li><li>indicates all levels of FPN share similar semantic levels (contrasted with naturally feature hierarchy of CNNs)</li></ul></li></ul></li><li><p>Fast  R-CNN</p><ul><li><p>original design: take the ROI feature map from the output of last conv layer</p></li><li><p>new design: take the specific level of ROI feature map based on ROI area</p><ul><li><p>with a $w*h$ ROI on the input image, $k_0$ refers to the target level on which an RoI with $wÃ—h=224^2$ should be mapped into </p><script type="math/tex; mode=display">k = [k_0 + log_2 (\sqrt{wh}/224)]</script></li><li><p>the smaller the ROI area, the lower the level k, the finer the resolution of the feature map</p></li></ul></li></ul></li></ul></li><li><p>åˆ†æ</p><ul><li>RPN<ul><li>use or not FPN: boost on small objects</li><li>use or not top-down pathway: semantic gaps</li><li>use or not lateral connection: locations </li><li>use or not multi-levels feature maps: <ul><li>using P2 alone leads to more anchors </li><li>more anchors are not sufficient to improve accuracy</li></ul></li></ul></li><li>Fast  R-CNN<ul><li>using P2 alone is <strong>marginally worse</strong> than that of using all pyramid levels </li><li>we argue that this is because <strong>RoI pooling is a warping-like operation</strong>, which is less sensitive to the regionâ€™s scales</li></ul></li><li><p>Faster R-CNN</p><ul><li>sharing features improves accuracy by a small margin</li><li>but reduces the testing time</li></ul></li><li><p><strong>Segmentation Proposals</strong></p><ul><li>use a fully convolutional setup for both training and inference </li><li><p>apply a small 5Ã—5 MLP to predict 14Ã—14 masks </p><p><img src="/2020/01/08/RCNNç³»åˆ—/FPNseg.png" width="40%;"></p></li></ul></li></ul></li></ol><h2 id="è¡ç”Ÿåº”ç”¨ï¼š"><a href="#è¡ç”Ÿåº”ç”¨ï¼š" class="headerlink" title="è¡ç”Ÿåº”ç”¨ï¼š"></a>è¡ç”Ÿåº”ç”¨ï¼š<lung nodules="" detection="" and="" segmentation="" using="" 3d="" mask-rcnn=""></lung></h2><ol><li>åŠ¨æœº<ul><li>3D volume detection and segmentation </li><li>ROI ï¼ full scan</li><li>LUNA16ï¼šlung nodules size evaluation </li></ul></li><li>è®ºç‚¹<ul><li>variety among nodules &amp; similarity among non-nodules</li></ul></li><li>æ–¹æ³•<ul><li>use overlapping sliding windows </li><li>use focal loss improve class result</li><li>use IOU loss improve mask result</li><li>use heavy augmentation</li></ul></li></ol><h2 id="Cascade-R-CNN-Delving-into-High-Quality-Object-Detection"><a href="#Cascade-R-CNN-Delving-into-High-Quality-Object-Detection" class="headerlink" title="Cascade R-CNN: Delving into High Quality Object Detection"></a>Cascade R-CNN: Delving into High Quality Object Detection</h2><ol><li><p>åŠ¨æœº</p><ul><li>an detector trained with low IoU threshold usually produces noisy detectionsï¼šä½è´¨é‡æ¡†issue</li><li>ä½†æ˜¯åˆä¸èƒ½ç®€å•åœ°æé«˜IoU threshold<ul><li>æ­£æ ·æœ¬ä¼šæ€¥å‰§å‡å°‘ï¼Œå¯¼è‡´è¿‡æ‹Ÿåˆ</li><li>inference-time mismatchï¼Œè®­ç»ƒé˜¶æ®µåªæœ‰é«˜è´¨é‡æ¡†ï¼Œä½†æ˜¯æµ‹è¯•é˜¶æ®µå•¥è´¨é‡æ¡†éƒ½æœ‰</li></ul></li><li>we propose Cascade R-CNN<ul><li>multi-stage object detection architecture</li><li>consists of a sequence of detectors trained with increasing IoU thresholds</li><li>trained stage by stage</li></ul></li><li>surpass all single-model on COCO</li></ul></li><li><p>è®ºç‚¹</p><ul><li>object detections two main tasks<ul><li>recognition problemï¼šforeground/backgroud &amp; object class</li><li>localization problemï¼šbounding box</li><li>loose requirement for positives<ul><li>an low IoU thresh(0.5) is required to define positives/negativesï¼šlooss</li><li>noisy bounding boxesï¼šclose false positives</li></ul></li></ul></li><li>quality<ul><li>å°†ä¸€ä¸ªæ¡†å’Œgtçš„IoUå®šä¹‰ä¸ºå®ƒçš„quality</li><li>å°†ä¸€ä¸ªdetectorè®­ç»ƒç”¨çš„IoU threshå®šä¹‰ä¸ºå®ƒçš„quality</li><li>detectorçš„qualityå’Œinput proposalsçš„qualityæ˜¯ç›¸å…³çš„ï¼ša single detector work on a specific quality level of hypotheses</li></ul></li><li>Cascade R-CNN<ul><li>multi-stage extension of R-CNN</li><li>sequentially more selective against close false positives</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>formulation</p><p> <img src="/2020/01/08/RCNNç³»åˆ—/formulation.png" width="80%;"></p><ul><li>first stageï¼š<ul><li>proposal network H0</li><li>applied to entire image</li></ul></li><li>second stage<ul><li>region-of-interest detection sub-network H1 (detection head)</li><li>run on proposals</li></ul></li><li>Cå’ŒBæ˜¯classification score &amp; bounding box regression</li><li>we focus on modeling the second stage</li></ul></li><li><p>bounding box regression é’ˆå¯¹å›å½’è´¨é‡</p><ul><li>an image patch $x$</li><li>a bounding box $b = (b_x, b_y, b_w, b_h)$</li><li>use a regressor $f(x,b)$ to fit the target $g$<ul><li>use L1 loss $L_{loc}(f(x_i,b_i), g_i)$</li><li>compute on ç›¸å¯¹é‡ &amp; std normalization</li><li>invariant to scale and location</li><li>results in minor adjustments on $b$ï¼šæ‰€ä»¥regression lossé€šå¸¸æ¯”cls losså°å¾—å¤š</li></ul></li><li>Iterative BBox<ul><li>a single regression step is not sufficient for accurate localization</li><li>æ‰€ä»¥å°±æäº†Nä¸ªä¸€æ ·çš„regression headsä¸²è”</li><li>ä½†è¿˜æ˜¯é‚£ä¸ªé—®é¢˜ï¼šä¸€ä¸ªregressoråªé’ˆå¯¹æŸä¸€ä¸ªquality levelçš„proposalsæ˜¯performance optimalçš„ï¼Œä½†æ˜¯æ¯ä¸ªiterationä»¥åæ¡†çš„distributionæ˜¯å‰§çƒˆå˜åŒ–çš„</li><li>æ‰€ä»¥è¿­ä»£ä¸¤æ¬¡ä»¥ä¸ŠåŸºæœ¬æ²¡æœ‰gainäº†</li></ul></li></ul></li><li><p>detection quality é’ˆå¯¹åˆ†ç±»è´¨é‡</p><ul><li>an image patch x</li><li>M foreground classes and 1 background</li><li>use a classifier $h(x)$ to learn the target class label among M+1<ul><li>use CE $L_{cls}(h(x_i),y_i)$</li><li>the class label is determined by IoU threshï¼šå¦‚æœimage patchå’Œgt boxçš„IoUå¤§äºé˜ˆå€¼ï¼Œé‚£ä¹ˆè¿™ä¸ªimage patchçš„class labelå°±æ˜¯gt boxçš„labelï¼Œå¦åˆ™æ˜¯èƒŒæ™¯</li><li>the IoU thresh defines the quality of a detector</li></ul></li><li>challenging<ul><li>å¦‚æœé˜ˆå€¼è°ƒé«˜äº†ï¼Œpositivesé‡Œé¢åŒ…å«æ›´å°‘çš„èƒŒæ™¯ï¼ˆé«˜è´¨é‡å‰æ™¯ï¼‰ï¼Œä½†æ˜¯æ ·æœ¬é‡å°‘</li><li>å¦‚æœé˜ˆå€¼ä½äº†ï¼Œå‰æ™¯æ ·æœ¬å¤šäº†ï¼Œä½†æ˜¯å†…å®¹æ›´åŠ diversifiedï¼Œæ›´éš¾reject close false positives</li><li>æ‰€ä»¥ä¸€ä¸ªåˆ†ç±»å™¨åœ¨ä¸åŒçš„IoUé˜ˆå€¼ä¸‹ï¼Œè¦é¢ä¸´ä¸åŒçš„é—®é¢˜ï¼Œåœ¨inferenceé˜¶æ®µï¼Œit is very difficult to perform uniformly well over all IoU levels</li></ul></li><li>Integral loss<ul><li>è®­ç»ƒå¥½å‡ ä¸ªåˆ†ç±»å™¨ï¼Œé’ˆå¯¹ä¸åŒçš„IoU levelï¼Œç„¶åinferenceé˜¶æ®µensemble</li><li>è¿˜æ˜¯æ²¡æœ‰è§£å†³é«˜IoUé˜ˆå€¼çš„é‚£ä¸ªåˆ†ç±»å™¨ä¼šå› ä¸ºæ ·æœ¬é‡å°‘è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œè€Œä¸”é«˜è´¨é‡åˆ†ç±»å™¨åœ¨infernceé˜¶æ®µè¿˜æ˜¯è¦å¤„ç†æ‰€æœ‰çš„ä½è´¨é‡æ¡†</li></ul></li></ul></li><li><p>Cascade R-CNN</p><ul><li>Cascaded Bounding Box Regression<ul><li>cascade specialized regressors</li><li>differs from Iterative BBox<ul><li>Iterative BBoxæ˜¯ä¸ªåå¤„ç†æ‰‹æ®µï¼Œä¸€ä¸ªregressoråœ¨0.5levelçš„boxesä¸Šé¢ä¼˜åŒ–ï¼Œç„¶ååœ¨inference proposalsä¸Šé¢åå¤è¿­ä»£</li><li>Cascade R-CNNæ˜¯ä¸ªresampling methodï¼Œå¤šä¸ªä¸åŒçš„regressorçº§è¿ï¼Œè®­ç»ƒæµ‹è¯•åŒæ“ä½œåŒåˆ†å¸ƒ</li></ul></li></ul></li><li><p>Cascaded Detection</p><ul><li>resamping manner<ul><li>keepä¸Šä¸€é˜¶æ®µçš„positives</li><li>åŒæ—¶ä¸¢æ‰ä¸€äº›outliers</li></ul></li><li>å®ç°å°±æ˜¯æ¯ä¸ªstageçš„IoU thresholdé€æ¸æé«˜</li><li><p>lossè¿˜æ˜¯æ‰€æœ‰proposalsçš„cls loss + å®šä¹‰ä¸ºå‰æ™¯proposalsçš„reg loss</p></li><li><p>outliers</p><p> <img src="/2020/01/08/RCNNç³»åˆ—/outlier.png" width="40%;"></p></li><li><p>proposal quality</p><p> <img src="/2020/01/08/RCNNç³»åˆ—/quality.png" width="50%;"></p></li></ul></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ç›®æ ‡æ£€æµ‹ï¼Œtwo-stage </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CNN Visualizationç³»åˆ—</title>
      <link href="/2020/01/03/CNN-Visualization%E7%B3%BB%E5%88%97/"/>
      <url>/2020/01/03/CNN-Visualization%E7%B3%BB%E5%88%97/</url>
      <content type="html"><![CDATA[<h3 id="1-Visualizing-and-Understanding-Convolutional-Networks"><a href="#1-Visualizing-and-Understanding-Convolutional-Networks" class="headerlink" title="1. Visualizing and Understanding Convolutional Networks"></a>1. Visualizing and Understanding Convolutional Networks</h3><ol><li><p>åŠ¨æœº</p><ul><li>give insight into the internal operation and behavior of the complex models</li><li>then one can design better models</li><li>reveal which parts of the scene in image are important for classification</li><li>explore the generalization ability of the model to other datasets </li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>most visualizing methods limited to the 1st layer where projections to pixel space are possible </p></li><li><p>Our approach propose a method that could projects high level feature maps to the pixel space</p></li></ul></li></ol><pre><code>* some methods give some insight into invariances basing on a simple quadratic approximation * Our approach, by contrast, provides a non-parametric view of invariance * some methods associate patches that responsible for strong activations at higher layers* In our approach they are not just crops of input images, but rather top-down projections that reveal structures  </code></pre><ol><li><p>æ–¹æ³•</p><p> 3.1 Deconvnet: use deconvnet to project the feature activations back to the input pixel space </p><ul><li>To examine a given convnet activation, we <strong>set all other activations in the layer to zero</strong> and pass the feature maps as input to the attached deconvnet layer</li><li>Then successively (i) unpool, (ii) rectify and (iii) filter to reconstruct the activity of the layer beneath until the input pixel space is reached</li><li>ã€Unpoolingã€‘using switches</li><li>ã€Rectificationã€‘the convnet uses relu to ensure always positive, same for back projection</li><li>ã€Filteringã€‘transposed conv</li><li><p>Due to unpooling, the reconstruction obtained from a single activation resembles a small piece of the original input image</p><p><img src="/2020/01/03/CNN-Visualizationç³»åˆ—/deconv.png" width="55%;"></p><p>3.2 CNN model</p><p><img src="/2020/01/03/CNN-Visualizationç³»åˆ—/CNN.png" width="70%;"></p><p>3.3 visualization among layers</p></li><li><p>for each layer, we take the top9 strongest activation across the validation data</p></li><li>calculate the back projection separately</li><li><p>alongside we provide the corresponding image patches</p><p>3.4 visualization during training</p></li><li><p>randomly choose several strongest activation of a given feature map</p></li><li><p>lower layers converge fast, higher layers conversely</p><p>3.5 visualizing the Feature Invariance</p></li><li><p>5 sample images being translated, rotated and scaled by varying degrees</p></li><li>Small transformations have a dramatic effect in the first layer of the model(c2 &amp; c3å¯¹æ¯”)</li><li><p>the network is stable to translations and scalings, but not invariant to rotation </p><p>3.6 architecture selection</p></li><li><p>old architecture(stride4, filterSize11)ï¼šThe first layer filters are a mix of extremely high and low frequency information, with little coverage of the mid frequencies.  The 2nd layer visualization shows aliasing artifacts caused by the large stride 4 used in the 1st layer convolutions. (è¿™ç‚¹å¯ä»¥å‚è€ƒä¹‹å‰vnetä¸­æåˆ°çš„ï¼Œdeconvå¯¼è‡´çš„æ£‹ç›˜æ ¼ä¼ªå½±ï¼Œå¤§strideä¼šæ›´æ˜æ˜¾)</p></li><li><p>smaller stride &amp; smaller filter(stride2, filterSize7)ï¼šmore coverage of mid frequencies, no aliasing, no dead feature</p><p>3.7</p></li><li><p>å¯¹äºç‰©ä½“çš„å…³é”®éƒ¨åˆ†é®æŒ¡ä¹‹åä¼šæå¤§çš„å½±å“åˆ†ç±»ç»“æœ</p></li><li>ç¬¬äºŒä¸ªå’Œç¬¬ä¸‰ä¸ªä¾‹å­ä¸­åˆ†åˆ«æ˜¯æ–‡å­—å’Œäººè„¸çš„å“åº”æ›´é«˜ï¼Œä½†æ˜¯å´ä¸æ˜¯å…³é”®éƒ¨åˆ†ã€‚</li></ul></li><li><p>ç†è§£</p><p> 4.1 æ€»çš„æ¥è¯´ï¼Œç½‘ç»œå­¦ä¹ åˆ°çš„ç‰¹å¾ï¼Œæ˜¯å…·æœ‰è¾¨åˆ«æ€§çš„ç‰¹å¾ï¼Œé€šè¿‡å¯è§†åŒ–å°±å¯ä»¥çœ‹åˆ°æˆ‘ä»¬æå–åˆ°çš„ç‰¹å¾å¿½è§†äº†èƒŒæ™¯ï¼Œè€Œæ˜¯æŠŠå…³é”®çš„ä¿¡æ¯ç»™æå–å‡ºæ¥äº†ã€‚ä»layer 1ã€layer 2å­¦ä¹ åˆ°çš„ç‰¹å¾åŸºæœ¬ä¸Šæ˜¯é¢œè‰²ã€è¾¹ç¼˜ç­‰ä½å±‚ç‰¹å¾ï¼›layer 3åˆ™å¼€å§‹ç¨å¾®å˜å¾—å¤æ‚ï¼Œå­¦ä¹ åˆ°çš„æ˜¯çº¹ç†ç‰¹å¾ï¼Œæ¯”å¦‚ä¸Šé¢çš„ä¸€äº›ç½‘æ ¼çº¹ç†ï¼›layer 4å­¦ä¹ åˆ°çš„åˆ™æ˜¯è¾ƒå¤šçš„ç±»åˆ«ä¿¡æ¯ï¼Œæ¯”å¦‚ç‹—å¤´ï¼›layer 5å¯¹åº”ç€æ›´å¼ºçš„ä¸å˜æ€§ï¼Œå¯ä»¥åŒ…å«ç‰©ä½“çš„æ•´ä½“ä¿¡æ¯ã€‚ã€‚</p><p> 4.2 åœ¨ç½‘ç»œè¿­ä»£çš„è¿‡ç¨‹ä¸­ï¼Œç‰¹å¾å›¾å‡ºç°äº†sudden jumpsã€‚ä½å±‚åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­åŸºæœ¬æ²¡å•¥å˜åŒ–ï¼Œæ¯”è¾ƒå®¹æ˜“æ”¶æ•›ï¼Œé«˜å±‚çš„ç‰¹å¾å­¦ä¹ åˆ™å˜åŒ–å¾ˆå¤§ã€‚è¿™è§£é‡Šäº†ä½å±‚ç½‘ç»œçš„ä»è®­ç»ƒå¼€å§‹ï¼ŒåŸºæœ¬ä¸Šæ²¡æœ‰å¤ªå¤§çš„å˜åŒ–ï¼Œå› ä¸ºæ¢¯åº¦å¼¥æ•£ã€‚é«˜å±‚ç½‘ç»œåˆšå¼€å§‹å‡ æ¬¡çš„è¿­ä»£ï¼Œå˜åŒ–ä¸æ˜¯å¾ˆå¤§ï¼Œä½†æ˜¯åˆ°äº†40~50çš„è¿­ä»£çš„æ—¶å€™ï¼Œå˜åŒ–å¾ˆå¤§ï¼Œå› æ­¤æˆ‘ä»¬ä»¥ååœ¨è®­ç»ƒç½‘ç»œçš„æ—¶å€™ï¼Œä¸è¦ç€æ€¥çœ‹ç»“æœï¼Œçœ‹ç»“æœéœ€è¦ä¿è¯ç½‘ç»œæ”¶æ•›ã€‚</p><p> 4.3 å›¾åƒçš„å¹³ç§»ã€ç¼©æ”¾ã€æ—‹è½¬ï¼Œå¯ä»¥çœ‹å‡ºç¬¬ä¸€å±‚ä¸­å¯¹äºå›¾åƒå˜åŒ–éå¸¸æ•æ„Ÿï¼Œç¬¬7å±‚å°±æ¥è¿‘äºçº¿æ€§å˜åŒ–ã€‚</p></li></ol><h3 id="2-Striving-for-Simplicity-The-All-Convolutional-Net"><a href="#2-Striving-for-Simplicity-The-All-Convolutional-Net" class="headerlink" title="2. Striving for Simplicity: The All Convolutional Net"></a>2. Striving for Simplicity: The All Convolutional Net</h3><ol><li><p>åŠ¨æœº</p><ul><li>traditional pipeline: alternating <strong>convolution</strong> and <strong>max-pooling</strong> layers followed by a small number of <strong>fully connected layers</strong></li><li>questioning the necessity of different components in the pipeline, <strong>max-pooling</strong> layer to be specified</li><li>to analyze the network we introduce a new variant of the â€œdeconvolution approachâ€ for visualizing features</li></ul></li><li><p>è®ºç‚¹</p><ul><li>two major improving directions based on traditional pipeline<ul><li>using more complex activation functions</li><li>building multiple conv modules</li></ul></li><li>we study the most simple architecture we could conceive<ul><li>a homogeneous network solely consisting of convolutional layers </li><li>without the need for complicated activation functions, any response normalization or max-pooling</li><li>reaches state of the art performance </li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>replace the pooling layers with standard convolutional layers with stride two</p><ul><li>the spatial dimensionality reduction performed by pooling makes covering larger parts of the input in higher layers possible </li><li>which is crucial for achieving good performance with CNNs </li></ul></li><li><p>make use of small convolutional layers </p><ul><li>greatly reduce the number of parameters in a network and thus serve as a form of regularization</li><li>if the topmost convolutional layer covers a portion of the image large enough to recognize its content then fully connected layers can also be replaced by simple 1-by-1 convolutions</li></ul></li><li><p>the overall architecture consists only of convolutional layers with rectified linear non-linearities and an averaging + softmax layer to produce predictions  </p><p>  <img src="/2020/01/03/CNN-Visualizationç³»åˆ—/basemodel.png" width="60%;"></p><p>  <img src="/2020/01/03/CNN-Visualizationç³»åˆ—/modifiedmodel.png" width="60%;"></p><ul><li>Strided-CNN-C: pooling is removed and the preceded conv stride is increase</li><li>ConvPool-CNN-C: a dense conv is placed, to show the effect of increasing parameters</li><li>All-CNN-C: max-pooling is replaced by conv</li><li>when pooling is replaced by an additional convolution layer with stride 2, performance stabilizes and even improves  </li><li>small 3 Ã— 3 convolutions stacked after each other seem to be enough to achieve the best performance </li></ul></li><li><p>guided backpropagation</p><ul><li>the paper above proposed â€˜deconvnetâ€™, which we observe that it does not always work well without max-pooling layers </li><li><strong>For higher layers</strong> of our network the method of Zeiler and Fergus fails to produce <strong>sharp, recognizable image structure</strong></li><li>Our architecture does not include max-pooling, thus we can â€™deconvolveâ€™ <strong>without switches</strong>, i.e. not conditioning on an input image</li><li><p>In order to obtain a reconstruction conditioned on an input image from our network without pooling layers we to combine the simple backward pass and the deconvnet</p><p><img src="/2020/01/03/CNN-Visualizationç³»åˆ—/backwardpass.png" width="70%;"></p></li><li><p>Interestingly, the very first layer of the network does not learn the usual Gabor filters, but higher layers do</p><p><img src="/2020/01/03/CNN-Visualizationç³»åˆ—/gabor.png" width="60%;"></p></li></ul></li></ul></li></ol><h3 id="3-Cam-Learning-Deep-Features-for-Discriminative-Localization"><a href="#3-Cam-Learning-Deep-Features-for-Discriminative-Localization" class="headerlink" title="3. Cam: Learning Deep Features for Discriminative Localization"></a>3. Cam: Learning Deep Features for Discriminative Localization</h3><ol><li><p>åŠ¨æœº</p><ul><li>we found that CNNs actually behave as object detectors despite no supervision on the location </li><li>this ability is lost when fully-connected layers are used for classification </li><li>we found that the advantages of global average pooling layers are beyond simply acting as a regularizer</li><li>it makes it easily to localize the discriminative image regions despite not being trained for them</li></ul></li><li><p>è®ºç‚¹</p><p> 2.1 Weakly-supervised object localization</p><ul><li>previous methods are not trained end-to-end and require multiple forward passes</li><li><p>Our approach is trained end-to-end and can localize objects in a single forward pass</p><p>2.2 Visualizing CNNs </p></li><li><p>previous methods only analyze the convolutional layers, ignoring the fully connected thereby painting an incomplete picture of the full story</p></li><li>we are able to understand our network from the beginning to the end</li></ul></li><li><p>æ–¹æ³•</p><p> 3.1 Class Activation Mapping</p><ul><li>A class activation map for a particular category indicates the discriminative image regions used by the network to identify that category  </li><li>the network architecture: convsâ€”-gapâ€”-fc+softmax</li><li>we can identify the importance of the image regions by projecting back the weights of the output layer on to the convolutional feature maps </li><li><p>by simply upsampling the class activation map to the size of the input image we can identify the image regions most relevant to the particular category</p><p><img src="/2020/01/03/CNN-Visualizationç³»åˆ—/cam.png" width="55%;"></p><p>3.2 Weakly-supervised Object Localization </p></li><li><p>our technique does not adversely impact the classification performance when learning to localize  </p></li><li>we found that the localization ability of the networks improved when the last convolutional layer before GAP had a higher spatial resolution, thus we removed several convolutional layers from the origin networks</li><li>overall we find that the classification performance is largely preserved for our GAP networks compared with the origin fc structure</li><li>our CAM approach significantly outperforms the backpropagation approach on generating bounding box </li><li><p>low mapping resolution prevents the network from obtaining accurate localizations</p><p>3.3 Visualizing Class-Specific Units </p></li><li><p>the convolutional units of various layers of CNNs act as visual concept detec- tors, identifying low-level concepts like textures or mate- rials, to high-level concepts like objects or scenes </p></li><li>Deeper into the network, the units become increasingly discriminative</li><li>given the fully-connected layers in many networks, it can be difficult to identify the importance of different units for identifying different categories </li></ul></li></ol><h3 id="4-Grad-CAM-Visual-Explanations-from-Deep-Networks-via-Gradient-based-Localization"><a href="#4-Grad-CAM-Visual-Explanations-from-Deep-Networks-via-Gradient-based-Localization" class="headerlink" title="4. Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization"></a>4. Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization</h3><h3 id="5-Grad-CAM-Improved-Visual-Explanations-for-Deep-Convolutional-Networks"><a href="#5-Grad-CAM-Improved-Visual-Explanations-for-Deep-Convolutional-Networks" class="headerlink" title="5. Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks"></a>5. Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks</h3><h2 id="6-ç»¼è¿°"><a href="#6-ç»¼è¿°" class="headerlink" title="6. ç»¼è¿°"></a>6. ç»¼è¿°</h2><ol><li><p>GAP</p><p> é¦–å…ˆå›é¡¾ä¸€ä¸‹GAPï¼ŒNiNä¸­æå‡ºäº†GAPï¼Œä¸»è¦ä¸ºäº†è§£å†³å…¨è¿æ¥å±‚å‚æ•°è¿‡å¤šï¼Œä¸æ˜“è®­ç»ƒä¸”å®¹æ˜“è¿‡æ‹Ÿåˆç­‰é—®é¢˜ã€‚</p><p> å¯¹å¤§å¤šæ•°åˆ†ç±»ä»»åŠ¡æ¥è¯´ä¸ä¼šå› ä¸ºåšäº†gapè®©ç‰¹å¾å˜å°‘è€Œè®©æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚å› ä¸ºGAPå±‚æ˜¯ä¸€ä¸ªéçº¿æ€§æ“ä½œå±‚ï¼Œè¿™Cä¸ªç‰¹å¾ç›¸å½“äºæ˜¯ä»kxkxCç»è¿‡éçº¿æ€§å˜åŒ–é€‰æ‹©å‡ºæ¥çš„å¼ºç‰¹å¾ã€‚</p></li><li><p>heatmap</p><p> step1. å›¾åƒç»è¿‡å·ç§¯ç½‘ç»œåæœ€åå¾—åˆ°çš„ç‰¹å¾å›¾ï¼Œåœ¨å…¨è¿æ¥å±‚åˆ†ç±»çš„æƒé‡ï¼ˆ$w_{k,n}$ï¼‰è‚¯å®šä¸åŒï¼Œ</p><p> step2. åˆ©ç”¨åå‘ä¼ æ’­æ±‚å‡ºæ¯å¼ ç‰¹å¾å›¾çš„æƒé‡ï¼Œ</p><p> step3. ç”¨æ¯å¼ ç‰¹å¾å›¾ä¹˜ä»¥æƒé‡å¾—åˆ°å¸¦æƒé‡çš„ç‰¹å¾å›¾ï¼Œåœ¨ç¬¬ä¸‰ç»´æ±‚å‡å€¼ï¼Œ<strong>reluæ¿€æ´»</strong>ï¼Œå½’ä¸€åŒ–å¤„ç†</p><ul><li>reluåªä¿ç•™wxå¤§äº0çš„å€¼â€”â€”æˆ‘ä»¬æ­£å“åº”æ˜¯å¯¹å½“å‰ç±»åˆ«æœ‰ç”¨çš„ç‰¹å¾ï¼Œè´Ÿå“åº”ä¼šæ‹‰ä½$\sum wx$ï¼Œå³ä¼šé™ä½å½“å‰ç±»åˆ«çš„ç½®ä¿¡åº¦</li><li><p>å¦‚æœæ²¡æœ‰reluï¼Œå®šä½å›¾è°±æ˜¾ç¤ºçš„ä¸ä»…ä»…æ˜¯æŸä¸€ç±»çš„ç‰¹å¾ã€‚è€Œæ˜¯æ‰€æœ‰ç±»åˆ«çš„ç‰¹å¾ã€‚</p><p>step4. å°†ç‰¹å¾å›¾resizeåˆ°åŸå›¾å°ºå¯¸ï¼Œä¾¿äºå åŠ æ˜¾ç¤º</p></li></ul></li><li><p>CAM</p><p> CAMè¦æ±‚å¿…é¡»ä½¿ç”¨GAPå±‚ï¼Œ</p><p> CAMé€‰æ‹©softmaxå±‚å€¼æœ€å¤§çš„èŠ‚ç‚¹åå‘ä¼ æ’­ï¼Œ<strong>æ±‚GAPå±‚çš„æ¢¯åº¦</strong>ä½œä¸ºç‰¹å¾å›¾çš„æƒé‡ï¼Œæ¯ä¸ªGAPçš„èŠ‚ç‚¹å¯¹åº”ä¸€å¼ ç‰¹å¾å›¾ã€‚</p></li><li><p>Grad-CAM</p><p> Grad-CAMä¸éœ€è¦é™åˆ¶æ¨¡å‹ç»“æ„ï¼Œ</p><p> Grad-CAMé€‰æ‹©softmaxå±‚å€¼æœ€å¤§çš„èŠ‚ç‚¹åå‘ä¼ æ’­ï¼Œå¯¹<strong>æœ€åä¸€å±‚å·ç§¯å±‚</strong>æ±‚æ¢¯åº¦ï¼Œç”¨æ¯å¼ ç‰¹å¾å›¾çš„æ¢¯åº¦çš„å‡å€¼ä½œä¸ºè¯¥ç‰¹å¾å›¾çš„æƒé‡ã€‚</p></li></ol>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>NiN: network in network</title>
      <link href="/2019/12/25/NiN-network-in-network/"/>
      <url>/2019/12/25/NiN-network-in-network/</url>
      <content type="html"><![CDATA[<h3 id="Network-In-Network"><a href="#Network-In-Network" class="headerlink" title="Network In Network"></a>Network In Network</h3><ol><li><p>åŠ¨æœº</p><ul><li>enhance model discriminability(è·å¾—æ›´å¥½çš„ç‰¹å¾æè¿°)ï¼špropose mlpconv</li><li>less prone to overfittingï¼špropose global average pooling</li></ul></li><li><p>è®ºç‚¹</p><p> <strong>comparison 1:</strong></p><ul><li>conventional CNN uses linear filter, which implicitly makes the assumption that the latent concepts are linearly separable. </li><li>traditional CNN is stacking [linear filters+nonlinear activation/linear+maxpooling+nonlinear]ï¼šè¿™é‡Œå¼•å‡ºäº†ä¸€ä¸ªæ¿€æ´»å‡½æ•°å’Œæ± åŒ–å±‚å…ˆåé¡ºåºçš„é—®é¢˜ï¼Œå¯¹äºavg_poollingï¼Œä¸¤ç§æ“ä½œå¾—åˆ°çš„ç»“æœæ˜¯ä¸ä¸€æ ·çš„ï¼Œå…ˆæ¥æ¿€æ´»å‡½æ•°ä¼šä¸¢å¤±éƒ¨åˆ†ä¿¡æ¯ï¼Œæ‰€ä»¥åº”è¯¥å…ˆæ± åŒ–å†æ¿€æ´»ï¼Œå¯¹äºMAX_poolingï¼Œä¸¤ç§æ“ä½œç»“æœä¸€æ ·ï¼Œä½†æ˜¯å…ˆæ± åŒ–ä¸‹é‡‡æ ·ï¼Œå¯ä»¥å‡å°‘æ¿€æ´»å‡½æ•°çš„è®¡ç®—é‡ï¼Œ<strong>æ€»ç»“å°±æ˜¯å…ˆæ± åŒ–å†æ¿€æ´»</strong>ã€‚ä½†æ˜¯å¥½å¤šç½‘ç»œå®é™…å®ç°ä¸Šéƒ½æ˜¯reluç´§è·Ÿç€convï¼Œåé¢æ¥poolingï¼Œè¿™æ ·æ¯”è¾ƒinterpretableâ€”â€”cross feature map pooling </li><li><p>mlpconv layer can be regarded as a highly nonlinear function(filter-fc-activation-fc-activation-fc-activationâ€¦)</p><p><img src="/2019/12/25/NiN-network-in-network/mlpconv.png" width="50%;"></p><p><strong>comparison 2:</strong></p></li><li><p>maxout network imposes the prior that instances of a latent concept lie within a convex set in the input spaceã€QUESTION HEREã€‘</p></li><li><p>mlpconv layer is a universal function approximator instead of a convex function approximator  </p><p><strong>comparison 3:</strong></p></li><li><p>fully connected layers are prone to overfitting and heavily depend on dropout regularization </p></li><li>global average pooling is more meaningful and interpretable, moreover it itself is a structural regularizerã€QUESTION HEREã€‘</li></ul></li><li><p>æ–¹æ³•</p><ul><li>use <strong>mlpconv layer</strong> to replace conventional GLM(linear filters)</li><li>use <strong>global average pooling</strong> to replace traditional fully connected layers</li><li>the overall structure is a stack of mlpconv layers, on top of which lie the global average pooling and the objective cost layer</li><li>Sub-sampling layers can be added in between the mlpconv as in CNN</li><li>dropout is applied on the outputs of all but the last mlpconv layers for regularization</li><li>another regularizer applied is weight decay </li></ul><p><img src="/2019/12/25/NiN-network-in-network/NIN.png" width="70%;"></p></li><li><p>ç»†èŠ‚</p><ul><li><p>preprocessingï¼šglobal contrast normalization and ZCA whitening </p></li><li><p>augmentationï¼štranslation and horizontal flipping</p></li><li><p>GAP for conventional CNNï¼šCNN+FC+DROPOUT &lt; CNN+GAP &lt; CNN+FC</p><ul><li>gap is effective as a regularizer</li><li>slightly worse than the dropout regularizer result for some reason</li></ul></li><li><p>confidence maps </p><ul><li>explicitly enforce feature maps in the last mlpconv layer of NIN to be confidence maps of the categories by means of global average poolingï¼šNiNå°†GAPçš„è¾“å‡ºç›´æ¥ä½œä¸ºoutput layerï¼Œå› æ­¤æ¯ä¸€ä¸ªç±»åˆ«å¯¹åº”çš„feature mapå¯ä»¥è¿‘ä¼¼è®¤ä¸ºæ˜¯ confidence mapã€‚</li><li>the strongest activations appear roughly at the same region of the object in the original imageï¼šç‰¹å¾å›¾ä¸Šé«˜å“åº”åŒºåŸŸåŸºæœ¬ä¸åŸå›¾ä¸Šç›®æ ‡åŒºåŸŸå¯¹åº”ã€‚</li><li>this motivates the possibility of performing object detection via NIN</li></ul></li><li><p>architectureï¼šå®é™…ä¸­å¤šå±‚æ„ŸçŸ¥å™¨ä½¿ç”¨1x1convæ¥å®ç°ï¼Œå¢åŠ çš„å¤šå±‚æ„ŸçŸ¥å™¨ç›¸å½“äºæ˜¯ä¸€ä¸ªå«å‚çš„æ± åŒ–å±‚ï¼Œé€šè¿‡å¯¹å¤šä¸ªç‰¹å¾å›¾è¿›è¡Œå«å‚æ± åŒ–ï¼Œå†ä¼ é€’åˆ°ä¸‹ä¸€å±‚ç»§ç»­å«å‚æ± åŒ–ï¼Œè¿™ç§çº§è”çš„<strong>è·¨é€šé“çš„å«å‚æ± åŒ–</strong>è®©ç½‘ç»œæœ‰äº†æ›´å¤æ‚çš„è¡¨å¾èƒ½åŠ›ã€‚</p><p>  <img src="/2019/12/25/NiN-network-in-network/architecture.png" width="55%;"></p></li></ul></li><li><p>æ€»ç»“</p><ol><li>mlpconvï¼šstronger local reception unit</li><li>gapï¼šregularizer &amp; bring confidence maps</li></ol></li></ol>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>unet &amp; vnet</title>
      <link href="/2019/12/05/unet-vnet/"/>
      <url>/2019/12/05/unet-vnet/</url>
      <content type="html"><![CDATA[<h2 id="U-NET-Convolutional-Networks-for-Biomedical-Image-Segmentation"><a href="#U-NET-Convolutional-Networks-for-Biomedical-Image-Segmentation" class="headerlink" title="U-NET: Convolutional Networks for Biomedical Image Segmentation"></a>U-NET: Convolutional Networks for Biomedical Image Segmentation</h2><ol><li><p>åŠ¨æœºï¼š</p><ul><li>train from very few images </li><li>outperforms more precisely on segmentation tasks</li><li>fast</li></ul></li><li><p>è¦ç´ ï¼š</p><ul><li>ç¼–ç ï¼ša contracting path to capture context</li><li>è§£ç ï¼ša symmetric expanding path that enables precise localization</li><li>å®ç°ï¼špooling operators &amp; upsampling operators</li></ul></li><li><p>è®ºç‚¹ï¼š</p><ul><li><p>when we talk about deep convolutional networksï¼š</p><ul><li>larger and deeper</li><li>millions of parameters </li><li>millions of training samples </li></ul></li><li><p>representative methodï¼šrun a sliding-window and predict a pixel label based on itsâ€˜ patch</p></li><li>drawbacksï¼š<ul><li>calculating redundancy of overlapping patches</li><li>big patchï¼šmore max-pooling layers that reduce the localization accuracy</li><li>small patchï¼šless involvement of context</li></ul></li><li>metioned but not further explainedï¼šcascade structure</li></ul></li><li><p>æ–¹æ³•ï¼š</p><ol><li><p>In order to localize, high resolution features from the contracting path are combined with the upsampled output. A successive convolution layer can then learn to assemble a more precise output based on this information. </p><p> ç†è§£ï¼šæ·±å±‚ç‰¹å¾å±‚æ„Ÿå—é‡è¾ƒå¤§ï¼Œå¸¦æœ‰å…¨å±€ä¿¡æ¯ï¼Œå°†å…¶ä¸Šé‡‡æ ·ç”¨äºæä¾›localization informationï¼Œè€Œæ¨ªå‘addè¿‡æ¥ç‰¹å¾å±‚å¸¦æœ‰å±€éƒ¨ç‰¹å¾ä¿¡æ¯ã€‚ä¸¤ä¸ª3*3çš„conv blockç”¨äºå°†ä¸¤ç±»ä¿¡æ¯æ•´åˆï¼Œè¾“å‡ºæ›´ç²¾ç¡®çš„è¡¨è¾¾ã€‚</p></li><li><p>In the upsampling part we have also a large number of feature channels, which allow the network to propagate context information to higher resolution layers.  </p><p> ç†è§£ï¼šåº”è¯¥æ˜¯å­—é¢æ„æ€å§ï¼Œä¸ºä¸Šé‡‡æ ·çš„å·ç§¯å±‚ä¿ç•™æ›´å¤šçš„ç‰¹å¾é€šé“ï¼Œå°±ç›¸å½“äºä¿ç•™äº†æ›´å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</p></li><li><p>we use excessive data augmentation.</p></li></ol></li><li><p>ç»†èŠ‚ï¼š</p><ol><li><p>contracting pathï¼š</p><ul><li>typical CNNï¼šblocks of [2 3*3 unpadded convs+ReLU+2*2 stride2 maxpooling]</li><li>At each downsampling step we double the number of feature channels</li></ul></li><li><p>expansive pathï¼š</p><ul><li>upsamplingï¼š<ul><li>2*2 up-conv that half the channels</li><li>concatenation the corresponding cropped feature map from the contracting path</li><li>2 [3x3 conv+ReLU]</li></ul></li><li>final layerï¼šuse a 1*1 conv to map the feature vectors to class vectors</li></ul><p><img src="/2019/12/05/unet-vnet/unet.png" width="60%"></p></li><li><p>trainï¼š</p><ul><li>prefer larger input size to larger batch size</li><li>sgd with 0.99 momentum so that the previously seen samples dominate the optimization</li></ul></li><li><p>lossï¼šsoftmax &amp; cross entropy </p></li><li><p>unbalanced weightï¼š</p><ul><li>pre-compute the weight map base on the frequency of pixels for a certain class </li><li>add the weight for a certain element to force the learning emphasisï¼še.g. the small separation borders </li><li>initializationï¼šGaussian distribution </li></ul></li><li><p>data augmentationï¼š</p><ul><li>deformations </li><li>â€œDrop-out layers at the end of the contracting path perform further implicit data augmentationâ€</li></ul></li><li><p>metricsï¼šâ€œwarping errorâ€, the â€œRand errorâ€ and the â€œpixel errorâ€  for EM segmentation challenge  and average IOU for ISBI cell tracking challenge </p></li><li><p>predictionï¼š</p><p>æŒ‰ç…§è®ºæ–‡çš„æ¨¡å‹ç»“æ„ï¼Œ<strong>è¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦æ˜¯ä¸ä¸€æ ·çš„</strong>â€”â€”åœ¨valid paddingçš„è¿‡ç¨‹ä¸­æœ‰è¾¹ç¼˜ä¿¡æ¯æŸå¤±ã€‚</p><p>é‚£ä¹ˆå¦‚æœæˆ‘ä»¬æƒ³è¦é¢„æµ‹é»„æ¡†å†…çš„åˆ†å‰²ç»“æœï¼Œéœ€è¦è¾“å…¥ä¸€å¼ æ›´å¤§çš„å›¾ï¼ˆè“æ¡†ï¼‰ä½œä¸ºè¾“å…¥ï¼Œåœ¨å›¾ç‰‡è¾¹ç¼˜çš„æ—¶å€™ï¼Œæˆ‘ä»¬é€šè¿‡é•œåƒçš„æ–¹å¼è¡¥å…¨ã€‚</p><p><img src="/2019/12/05/unet-vnet/predict.png" width="60%;"></p><p><strong>å› æœå…³ç³»ï¼š</strong></p><ul><li>é¦–å…ˆå› ä¸ºå†…å­˜é™åˆ¶ï¼Œè¾“å…¥çš„ä¸æ˜¯æ•´å¼ å›¾ï¼Œæ˜¯å›¾ç‰‡patchï¼Œ</li><li>ä¸ºäº†ä¿ç•™ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä½¿å¾—é¢„æµ‹æ›´å‡†ç¡®ï¼Œæˆ‘ä»¬ç»™å›¾ç‰‡patchæ·»åŠ ä¸€åœˆborderçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå®é™…æ„Ÿå…´è¶£çš„æ˜¯é»„æ¡†åŒºåŸŸï¼‰</li><li>åœ¨è®­ç»ƒæ—¶ï¼Œä¸ºäº†é¿å…é‡å å¼•å…¥çš„è®¡ç®—ï¼Œå·ç§¯å±‚ä½¿ç”¨äº†valid padding</li><li>å› æ­¤åœ¨ç½‘ç»œçš„è¾“å‡ºå±‚ï¼Œè¾“å‡ºå°ºå¯¸æ‰æ˜¯æˆ‘ä»¬çœŸæ­£å…³æ³¨çš„éƒ¨åˆ†</li><li>å¦‚æœè®­ç»ƒæ ·æœ¬å°ºå¯¸ä¸é‚£ä¹ˆhugeï¼Œå®Œå…¨å¯ä»¥å…¨å›¾è¾“å…¥ï¼Œç„¶åä½¿ç”¨same paddingï¼Œç›´æ¥é¢„æµ‹å…¨å›¾mask</li></ul></li></ol></li><li><p>æ€»ç»“ï¼š</p><ul><li>train from very few images â€”-&gt; data augmentation</li><li>fast â€”-&gt; full convolution layers</li><li>precise â€”-&gt; global?</li></ul></li></ol><h2 id="V-Net-Fully-Convolutional-Neural-Networks-for-Volumetric-Medical-Image-Segmentation"><a href="#V-Net-Fully-Convolutional-Neural-Networks-for-Volumetric-Medical-Image-Segmentation" class="headerlink" title="V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation"></a>V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation</h2><ol><li><p>åŠ¨æœº</p><ul><li>entire 3D volume</li><li>imbalance between the number of foreground and background voxelsï¼šdice coefficient</li><li>limited dataï¼šapply random non-linear transformations and histogram matching</li><li>fast and accurate</li></ul></li><li><p>è®ºç‚¹ï¼š</p><ul><li>early approaches based on patches<ul><li>local context</li><li>challenging modailities</li><li>efficiency issues</li></ul></li><li>fully convolutional networks<ul><li>2D so far</li></ul></li><li>imbalance issueï¼šthe anatomy of interest occupies only a very small region of the scan thus predictions are strongly biased towards the background.<ul><li>re-weighting</li><li>dice coefficient claims to be better that above</li></ul></li></ul></li><li><p>è¦ç´ ï¼š</p><ul><li>a compression path</li><li><p>a decompression path</p><p><img src="/2019/12/05/unet-vnet/vnet.png" width="55%"></p></li></ul></li><li><p>æ–¹æ³•ï¼š</p><ul><li><p>compressionï¼š</p><ul><li>add residualèƒ½å¤ŸåŠ é€Ÿæ”¶æ•›</li><li>resolution is reduced by [2*2*2 conv with stride 2]ç›¸æ¯”äºmaxpoolingèŠ‚çœäº†bpæ‰€éœ€switch mapçš„memoryæ¶ˆè€—</li><li>double the number of feature maps as we reduce their resolution</li><li>PReLU</li></ul></li><li><p>decompressionï¼š</p><ul><li>horizontal connectionsï¼š1) gather fine grained detail that would be otherwise lost in the compression path  2) improve the convergence time </li><li><p>residual convï¼šblocks of [5*5*5 conv with stride 1] æå–ç‰¹å¾ç»§ç»­å¢å¤§æ„Ÿå—é‡</p><p><img src="/2019/12/05/unet-vnet/receptive.png" width="50%"></p></li><li><p>up-convï¼šexpands the spatial support of the lower resolution feature maps </p><p><img src="/2019/12/05/unet-vnet/deconv.png" width="60%"></p></li><li><p>last layerï¼šrun [1*1*1conv with 2 channel+softmax] to obtain the voxelwise probabilistic segmentations of the foreground and background </p></li></ul></li><li><p>dice coefficientï¼š [0,1] which we aim to maximiseï¼Œassume $p_i$ã€$g_i$ belong to two <strong>binary volumes</strong></p><script type="math/tex; mode=display">  D = \frac{2\sum_i^N p_i g_i}{\sum_i^N p_i^2 + \sum_i^N g_i^2}</script></li><li><p>trainï¼š</p><ul><li>input fix size 128 Ã— 128 Ã— 64 voxels and a spatial resolution of 1 Ã— 1 Ã— 1.5 millimeters</li><li>each mini-batch contains 2 volumes</li><li>online augmentationï¼š<ul><li>randomly deformation</li><li>vary the intensity distributionï¼šéšæœºé€‰å–æ ·æœ¬çš„ç°åº¦åˆ†å¸ƒä½œä¸ºå½“å‰è®­ç»ƒæ ·æœ¬çš„ç°åº¦åˆ†å¸ƒ</li></ul></li><li>used a momentum of 0.99 and a initial learning rate of 0.0001 which decreases by one order of magnitude every 25K iterations</li></ul></li><li><p>metricsï¼š</p><ul><li>Dice coefficient</li><li>Hausdorff distance of the predicted delineation to the ground truth annotation</li><li>the score obtained on the challenge </li></ul></li></ul></li></ol><h2 id="dice-loss-amp-focal-loss"><a href="#dice-loss-amp-focal-loss" class="headerlink" title="dice loss &amp; focal loss"></a>dice loss &amp; focal loss</h2><ol><li><p>CE &amp; BCE</p><ul><li><p>CEï¼šcategorical_crossentropyï¼Œé’ˆå¯¹æ‰€æœ‰ç±»åˆ«è®¡ç®—ï¼Œç±»åˆ«é—´äº’æ–¥</p><script type="math/tex; mode=display">CE(x) = -\sum_{i=1}^{n\_class}y_i log f_i(x)</script><blockquote><p>$x$æ˜¯è¾“å…¥æ ·æœ¬ï¼Œ$y_i$æ˜¯ç¬¬$i$ä¸ªç±»åˆ«å¯¹åº”çš„çœŸå®æ ‡ç­¾ï¼Œ$f_i(x)$æ˜¯å¯¹åº”çš„æ¨¡å‹è¾“å‡ºå€¼ã€‚</p><p>å¯¹åˆ†ç±»é—®é¢˜ï¼Œ$y_i$æ˜¯one-hotï¼Œ$f_i(x)$æ˜¯ä¸ªä¸€ç»´å‘é‡ã€‚æœ€ç»ˆå¾—åˆ°ä¸€ä¸ªæ•°å€¼ã€‚</p></blockquote></li><li><p>BCEï¼šbinary_crossentropyï¼Œé’ˆå¯¹æ¯ä¸ªç±»åˆ«è®¡ç®—</p><script type="math/tex; mode=display">BCE(x)_i = - [y_i log f_i(x) + (1-y_i)log(1-f_i(x))]</script><blockquote><p>$i$æ˜¯ç±»åˆ«ç¼–å·ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªç»´åº¦ä¸º$n_class$çš„å‘é‡ã€‚</p><p>å†æ±‚ç±»å‡å€¼å¾—åˆ°ä¸€ä¸ªæ•°å€¼ä½œä¸ºå•ä¸ªæ ·æœ¬çš„lossã€‚</p></blockquote><script type="math/tex; mode=display">BCE(x) = \frac{\sum_{i=1}^{n\_class}BCE_i(x)}{n\_class}</script></li><li><p>batch lossï¼šå¯¹batchä¸­æ‰€æœ‰æ ·æœ¬çš„lossæ±‚å‡å€¼ã€‚</p></li><li><p>ä»å…¬å¼ä¸Šçœ‹ï¼ŒCEçš„è¾“å‡ºé€šå¸¸æ˜¯ç»è¿‡äº†softmaxï¼Œsoftmaxçš„æŸä¸€ä¸ªè¾“å‡ºå¢å¤§ï¼Œå¿…ç„¶å¯¼è‡´å…¶å®ƒç±»åˆ«çš„è¾“å‡ºå‡å°ï¼Œå› æ­¤åœ¨è®¡ç®—lossçš„æ—¶å€™å…³æ³¨æ­£ç¡®ç±»åˆ«çš„é¢„æµ‹å€¼æ˜¯å¦è¢«æ‹‰é«˜å³å¯ã€‚ä½¿ç”¨BCEçš„åœºæ™¯é€šå¸¸æ˜¯ä½¿ç”¨sigmoidï¼Œç±»åˆ«é—´ä¸ä¼šäº’ç›¸å‹åˆ¶ï¼Œå› æ­¤æ—¢è¦è€ƒè™‘æ‰€å±ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡å¤Ÿé«˜ï¼Œä¹Ÿè¦è€ƒè™‘ä¸æ‰€å±ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡è¶³å¤Ÿä½ï¼ˆè¿™ä¸€é¡¹åœ¨softmaxä¸­è¢«å®ç°äº†æ•…CEä¸éœ€è¦è¿™ä¸€é¡¹ï¼‰ã€‚</p></li><li>åœºæ™¯ï¼š<ul><li>äºŒåˆ†ç±»ï¼šåªæœ‰ä¸€ä¸ªè¾“å‡ºèŠ‚ç‚¹ï¼Œ$f(x) \in (0,1)$ï¼Œåº”è¯¥ä½¿ç”¨sigmoid+BCEä½œä¸ºæœ€åçš„è¾“å‡ºå±‚é…ç½®ã€‚</li><li>å•æ ‡ç­¾å¤šåˆ†ç±»ï¼šåº”è¯¥ä½¿ç”¨softmax+CEçš„æ–¹æ¡ˆï¼ŒBCEä¹ŸåŒæ ·é€‚ç”¨ã€‚</li><li>å¤šæ ‡ç­¾å¤šåˆ†ç±»ï¼šmulti-labelæ¯ä¸ªæ ‡ç­¾çš„è¾“å‡ºæ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œå› æ­¤å¸¸ç”¨é…ç½®æ˜¯sigmoid+BCEã€‚</li></ul></li><li>å¯¹åˆ†å‰²åœºæ™¯æ¥è¯´ï¼Œè¾“å‡ºçš„æ¯ä¸€ä¸ªchannelå¯¹åº”ä¸€ä¸ªç±»åˆ«çš„é¢„æµ‹mapï¼Œå¯ä»¥çœ‹æˆæ˜¯å¤šä¸ªchannelé—´çš„å•æ ‡ç­¾å¤šåˆ†ç±»ï¼ˆsoftmax+CEï¼‰ï¼Œä¹Ÿå¯ä»¥çœ‹æˆæ˜¯æ¯ä¸ªç‹¬ç«‹é€šé“ç±»åˆ«mapçš„äºŒåˆ†ç±»ï¼ˆsigmoid+BCEï¼‰ã€‚unetè®ºæ–‡ç”¨äº†weightedçš„softmax+CEã€‚vnetè®ºæ–‡ç”¨äº†dice_lossã€‚</li></ul></li><li><p>re-weighting(WCE)</p><p> åŸºäºCE&amp;BCEï¼Œç»™äº†æ ·æœ¬ä¸åŒçš„æƒé‡ã€‚</p><p> unetè®ºæ–‡ä¸­æåˆ°äº†åŸºäºpixel frequencyä¸ºä¸åŒçš„ç±»åˆ«åˆ›å»ºäº†weight mapã€‚</p><p> ä¸€ç§å®ç°ï¼šåŸºäºæ¯ä¸ªç±»åˆ«çš„weight mapï¼Œåœ¨å®ç°CEçš„æ—¶å€™æ”¹æˆåŠ æƒå¹³å‡å³å¯ã€‚</p><p> å¦ä¸€ç§å®ç°ï¼šåŸºäºæ¯ä¸ªæ ·æœ¬çš„weight mapï¼Œä½œä¸ºç½‘ç»œçš„é™„åŠ è¾“å…¥ï¼Œåœ¨å®ç°CEçš„æ—¶å€™ä¹˜åœ¨loss mapä¸Šã€‚</p></li><li><p>focal loss</p><p> æå‡ºæ˜¯åœ¨ç›®æ ‡æ£€æµ‹é¢†åŸŸï¼Œç”¨äºè§£å†³æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ä¸¥é‡å¤±è°ƒçš„é—®é¢˜ã€‚</p><p> ä¹Ÿæ˜¯ä¸€ç§åŠ æƒï¼Œä½†æ˜¯ç›¸æ¯”è¾ƒäºre-weightingï¼Œ<strong>å›°éš¾æ ·æœ¬çš„æƒé‡ç”±ç½‘ç»œè‡ªè¡Œæ¨æ–­å‡º</strong>ï¼Œé€šè¿‡æ·»åŠ $(\alpha)$å’Œ$(-)^\lambda$è¿™ä¸€åŠ æƒé¡¹ï¼š</p><script type="math/tex; mode=display"> focal\_loss(x)_i = -[\alpha y_i (1-p_i)^\lambda log (p_i)+(1-\alpha)(1-y_i)p_i^\lambda log(1-p_i)]</script><ul><li><p>å¯¹äºç±»åˆ«é—´ä¸å‡è¡¡çš„æƒ…å†µï¼ˆé€šå¸¸è´Ÿæ ·æœ¬è¿œè¿œå¤šäºæ­£æ ·æœ¬ï¼‰ï¼Œ$(\alpha)$é¡¹ç”¨äºå¹³è¡¡æ­£è´Ÿæ ·æœ¬æƒé‡ã€‚</p></li><li><p>å¯¹äºç±»å†…å›°éš¾æ ·æœ¬çš„æŒ–æ˜ï¼Œ$(-)^\lambda$é¡¹ç”¨äºè°ƒæ•´ç®€å•æ ·æœ¬å’Œå›°éš¾æ ·æœ¬çš„æƒé‡ï¼Œé¢„æµ‹æ¦‚ç‡æ›´æ¥è¿‘çœŸå®labelçš„æ ·æœ¬ï¼ˆç®€å•æ ·æœ¬ï¼‰çš„æƒé‡ä¼šè¡°å‡æ›´å¿«ï¼Œé¢„æµ‹æ¦‚ç‡æ¯”è¾ƒä¸å‡†ç¡®çš„æ ·æœ¬ï¼ˆè‹¦éš¾æ ·æœ¬ï¼‰çš„æƒé‡åˆ™æ›´é«˜äº›ã€‚</p><p>ç”±äºåˆ†å‰²ç½‘ç»œçš„è¾“å‡ºçš„å•é€šé“ï¼å¤šé€šé“çš„å›¾ç‰‡ï¼Œç›´æ¥ä½¿ç”¨focal lossä¼šå¯¼è‡´losså€¼å¾ˆå¤§ã€‚</p><p>â€‹    1. é€šå¸¸ä¸å…¶ä»–lossåŠ æƒç»„åˆä½¿ç”¨</p><p>â€‹    2. sumå¯ä»¥æ”¹æˆmean</p><p>â€‹    3.ä¸å»ºè®®åœ¨è®­ç»ƒåˆæœŸå°±åŠ å…¥ï¼Œå¯åœ¨è®­ç»ƒåæœŸç”¨äºä¼˜åŒ–æ¨¡å‹</p><p>â€‹    4. å…¬å¼ä¸­å«logè®¡ç®—ï¼Œå¯èƒ½å¯¼è‡´nanï¼Œè¦å¯¹logä¸­çš„å…ƒç´ clip</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">focal_loss</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    gamma = <span class="number">2.</span></span><br><span class="line">    alpha = <span class="number">0.25</span></span><br><span class="line">    <span class="comment"># score = alpha * y_true * K.pow(1 - y_pred, gamma) * K.log(y_pred) +            # this works when y_true==1</span></span><br><span class="line">    <span class="comment">#         (1 - alpha) * (1 - y_true) * K.pow(y_pred, gamma) * K.log(1 - y_pred)  # this works when y_true==0</span></span><br><span class="line">    pt_1 = tf.where(tf.equal(y_true, <span class="number">1</span>), y_pred, tf.ones_like(y_pred))</span><br><span class="line">    pt_0 = tf.where(tf.equal(y_true, <span class="number">0</span>), y_pred, tf.zeros_like(y_pred))</span><br><span class="line">    <span class="comment"># avoid nan</span></span><br><span class="line">    pt_1 = K.clip(pt_1, <span class="number">1e-3</span>, <span class="number">.999</span>)</span><br><span class="line">    pt_0 = K.clip(pt_0, <span class="number">1e-3</span>, <span class="number">.999</span>)</span><br><span class="line">    score = -K.sum(alpha * K.pow(<span class="number">1.</span> - pt_1, gamma) * K.log(pt_1)) -  \</span><br><span class="line">            K.sum((<span class="number">1</span> - alpha) * K.pow(pt_0, gamma) * K.log(<span class="number">1.</span> - pt_0))</span><br><span class="line">    <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure></li></ul></li><li><p>dice loss</p><p>diceå®šä¹‰ä¸¤ä¸ªmaskçš„ç›¸ä¼¼ç¨‹åº¦ï¼š</p><script type="math/tex; mode=display">dice = \frac{2 * A \bigcap B}{|A|+|B|} = \frac{2 * TP}{2*TP + FN + FP}</script><ul><li>åˆ†å­æ˜¯TPâ€”â€”åªå…³æ³¨å‰æ™¯</li><li><p>åˆ†æ¯å¯ä»¥æ˜¯$|A|$ï¼ˆé€ä¸ªå…ƒç´ ç›¸åŠ ï¼‰ï¼Œä¹Ÿå¯ä»¥æ˜¯å¹³æ–¹å½¢å¼$|A|^2$</p></li><li><p><strong>æ¢¯åº¦ï¼š</strong>â€œä½¿ç”¨dice lossæœ‰æ—¶ä¼šä¸å¯ä¿¡ï¼ŒåŸå› æ˜¯å¯¹äºsoftmaxæˆ–log losså…¶æ¢¯åº¦ç®€è¨€ä¹‹æ˜¯p-t ï¼Œtä¸ºç›®æ ‡å€¼ï¼Œpä¸ºé¢„æµ‹å€¼ã€‚è€Œdice loss ä¸º 2t2 / (p+t)2</p><p> å¦‚æœpï¼Œtè¿‡å°ä¼šå¯¼è‡´æ¢¯åº¦å˜åŒ–å‰§çƒˆï¼Œå¯¼è‡´è®­ç»ƒå›°éš¾ã€‚â€</p><p> ã€è¯¦ç»†è§£é‡Šä¸‹ã€‘äº¤å‰ç†µlossï¼š$L=-(1-|t-p|)log(1-|t-p|)$ï¼Œæ±‚å¯¼å¾—åˆ°$\frac{\partial L}{\partial p}=-log(1-|t-p|)$ï¼Œå…¶å®å°±å¯ä»¥ç®€åŒ–çœ‹ä½œ$t-p$ï¼Œå¾ˆæ˜¾ç„¶è¿™ä¸ªæ¢¯åº¦æ˜¯æœ‰ç•Œçš„ï¼Œå› æ­¤ä½¿ç”¨äº¤å‰ç†µlossçš„ä¼˜åŒ–è¿‡ç¨‹æ¯”è¾ƒç¨³å®šã€‚è€Œdice lossçš„ä¸¤ç§å½¢å¼ï¼ˆä¸å¹³æ–¹&amp;å¹³æ–¹ï¼‰ï¼š$L=\frac{2pt}{p+t}\ or\  L=\frac{2pt}{p^2+t^2}$ï¼Œæ±‚å¯¼ä»¥ååˆ†åˆ«æ˜¯$\frac{\partial L}{\partial p} = \frac{t^2+2pt}{(p+t)^2} \ or\ \frac{3tp^2+t^3}{(p^2+t^2)^2}$è®¡ç®—ç»“æœæ¯”è¾ƒå¤æ‚ï¼Œptéƒ½å¾ˆå°çš„æƒ…å†µä¸‹ï¼Œæ¢¯åº¦å€¼å¯èƒ½å¾ˆå¤§ï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œlossæ›²çº¿æ··ä¹±ã€‚</p></li></ul></li></ol><p>  vnetè®ºæ–‡ä¸­çš„å®šä¹‰åœ¨åˆ†æ¯ä¸Šç¨æœ‰ä¸åŒï¼ˆsee belowï¼‰ã€‚smoothingçš„å¥½å¤„ï¼š</p><ul><li>é¿å…åˆ†å­é™¤0</li><li><p>å‡å°‘è¿‡æ‹Ÿåˆ</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dice_coef</span><span class="params">(y_true, y_pred)</span>:</span> </span><br><span class="line">  smooth = <span class="number">1.</span></span><br><span class="line">    intersection = K.sum(y_true * y_pred, axis=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) </span><br><span class="line">    union = K.sum(y_true, axis=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) + K.sum(y_pred, axis=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) </span><br><span class="line">    <span class="keyword">return</span> K.mean( (<span class="number">2.</span> * intersection + smooth) / (union + smooth), axis=<span class="number">0</span>) </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dice_coef_loss</span><span class="params">(y_true, y_pred)</span>:</span> </span><br><span class="line">    <span class="number">1</span> - dice_coef(y_true, y_pred, smooth=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li></ul><ol><li><p>iou loss</p><p> dice lossè¡ç”Ÿï¼Œintersection over unionï¼š</p><script type="math/tex; mode=display"> iou = \frac{A \bigcap B}{A \bigcup B}</script><p> åˆ†æ¯ä¸Šæ¯”diceå°‘äº†ä¸€ä¸ªintersectionã€‚</p><ul><li>â€œIOU lossçš„ç¼ºç‚¹åŒDICE lossï¼Œè®­ç»ƒæ›²çº¿å¯èƒ½å¹¶ä¸å¯ä¿¡ï¼Œè®­ç»ƒçš„è¿‡ç¨‹ä¹Ÿå¯èƒ½å¹¶ä¸ç¨³å®šï¼Œæœ‰æ—¶ä¸å¦‚ä½¿ç”¨softmax lossç­‰çš„æ›²çº¿æœ‰ç›´è§‚æ€§ï¼Œé€šå¸¸è€Œè¨€softmax losså¾—åˆ°çš„lossä¸‹é™æ›²çº¿è¾ƒä¸ºå¹³æ»‘ã€‚â€</li></ul></li><li><p>boundary loss</p><p> dice losså’Œiou lossæ˜¯åŸºäº<strong>åŒºåŸŸé¢ç§¯åŒ¹é…åº¦</strong>å»å­¦ä¹ ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨<strong>è¾¹ç•ŒåŒ¹é…åº¦</strong>å»ç›‘ç£ç½‘ç»œçš„å­¦ä¹ ã€‚</p><p> åªå¯¹è¾¹ç•Œä¸Šçš„åƒç´ è¿›è¡Œè¯„ä¼°ï¼Œå’ŒGTçš„è¾¹ç•Œå»åˆåˆ™ä¸º0ï¼Œä¸å»åˆçš„ç‚¹ï¼Œæ ¹æ®å…¶è·ç¦»è¾¹ç•Œçš„è·ç¦»è¯„ä¼°å®ƒçš„Lossã€‚</p></li><li><p>Hausdorff distance</p><p> ç”¨äºåº¦é‡ä¸¤ä¸ªç‚¹é›†ä¹‹é—´çš„ç›¸ä¼¼ç¨‹åº¦ï¼Œdenote ç‚¹é›†$A\{a_1, a_2, â€¦, a_p\}$ï¼Œç‚¹é›†$B\{b_1, b_2, â€¦, b_p\}$ï¼š</p><script type="math/tex; mode=display"> HD(A, B) = max\{hd(A,B), hd(B,A)\}\\ hd(A,B) = max_{a \in A} min_{b in B} ||a-b||\\ hd(B,A) = max_{b \in B} min_{a in A} ||b-a||</script><p> å…¶ä¸­HD(A,B)æ˜¯Hausdorff distanceçš„åŸºæœ¬å½¢å¼ï¼Œç§°ä¸ºåŒå‘è·ç¦»</p><p> hd(A,B)æè¿°çš„æ˜¯å•å‘è·ç¦»ï¼Œé¦–å…ˆæ‰¾åˆ°ç‚¹é›†Aä¸­æ¯ä¸ªç‚¹åœ¨ç‚¹é›†Bä¸­è·ç¦»æœ€è¿‘çš„ç‚¹ä½œä¸ºåŒ¹é…ç‚¹ï¼Œç„¶åè®¡ç®—è¿™äº›a-b-pairçš„è·ç¦»çš„æœ€å¤§å€¼ã€‚</p><p> HD(A,B)å–å•å‘è·ç¦»ä¸­çš„æœ€å¤§å€¼ï¼Œæè¿°äº†ä¸¤ä¸ªç‚¹é›†åˆçš„æœ€å¤§ä¸åŒ¹é…ç¨‹åº¦ã€‚</p></li><li><p>mix loss</p><ul><li>BCE + dice lossï¼šåœ¨æ•°æ®è¾ƒä¸ºå¹³è¡¡çš„æƒ…å†µä¸‹æœ‰æ”¹å–„ä½œç”¨ï¼Œä½†æ˜¯åœ¨æ•°æ®æåº¦ä¸å‡è¡¡çš„æƒ…å†µä¸‹ï¼Œäº¤å‰ç†µæŸå¤±ä¼šåœ¨å‡ ä¸ªè®­ç»ƒä¹‹åè¿œå°äºDice æŸå¤±ï¼Œæ•ˆæœä¼šæŸå¤±ã€‚</li><li>focal loss + dice lossï¼šæ•°é‡çº§é—®é¢˜</li></ul></li><li><p>MSE</p><p> å…³é”®ç‚¹æ£€æµ‹æœ‰æ—¶å€™ä¹Ÿä¼šé‡‡ç”¨åˆ†å‰²æ¡†æ¶ï¼Œè¿™æ—¶å€™ground truthæ˜¯é«˜æ–¯mapï¼Œdiceæ˜¯é’ˆå¯¹äºŒå€¼åŒ–maskçš„ï¼Œè¿™æ—¶å€™è¿˜å¯ä»¥ç”¨MSEã€‚</p></li><li><p>ohnm</p><p>online hard negative mining å›°éš¾æ ·æœ¬æŒ–æ˜</p></li><li><p>Tversky loss</p><p>ä¸€ç§åŠ æƒçš„dice lossï¼Œdice lossä¼šå¹³ç­‰çš„æƒè¡¡FPï¼ˆç²¾åº¦ï¼Œå‡é˜³ï¼‰å’ŒFNï¼ˆå¬å›ï¼Œå‡é˜´ï¼‰ï¼Œä½†æ˜¯åŒ»å­¦å›¾åƒä¸­ç—…ç¶æ•°ç›®è¿œå°‘äºèƒŒæ™¯æ•°é‡ï¼Œå¾ˆå¯èƒ½å¯¼è‡´è®­ç»ƒç»“æœåå‘é«˜ç²¾åº¦ä½†æ˜¯ä½å¬å›ç‡ï¼ŒTversky lossæ§åˆ¶lossæ›´åå‘FNï¼š</p><script type="math/tex; mode=display">loss = 1-\frac{|PG|}{|PG|+\alpha|P\backslash G|+\beta|G\backslash P|}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tversky_loss</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    y_true_pos = K.flatten(y_true)</span><br><span class="line">    y_pred_pos = K.flatten(y_pred)</span><br><span class="line">    <span class="comment"># TP</span></span><br><span class="line">    true_pos = K.sum(y_true_pos * y_pred_pos)</span><br><span class="line">    <span class="comment"># FN</span></span><br><span class="line">    false_neg = K.sum(y_true_pos * (<span class="number">1</span>-y_pred_pos))</span><br><span class="line">    <span class="comment"># FP</span></span><br><span class="line">    false_pos = K.sum((<span class="number">1</span>-y_true_pos) * y_pred_pos)</span><br><span class="line">    alpha = <span class="number">0.7</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - (true_pos + K.epsilon())/(true_pos + alpha * false_neg + (<span class="number">1</span>-alpha) * false_pos + K.epsilon())</span><br></pre></td></tr></table></figure></li><li><p>Lovasz hinge &amp; Lovasz-Softmax loss</p><p>IOU lossè¡ç”Ÿï¼Œjaccard lossåªé€‚ç”¨äºç¦»æ•£æƒ…å†µï¼Œè€Œç½‘ç»œé¢„æµ‹æ˜¯è¿ç»­å€¼ï¼Œå¦‚æœä¸ä½¿ç”¨æŸä¸ªè¶…å‚å°†ç¥ç»å…ƒè¾“å‡ºäºŒå€¼åŒ–ï¼Œå°±ä¸å¯å¯¼ã€‚blabla</p><p>ä¸æ˜¯å¾ˆæ‡‚ç›´æ¥ç”¨å§ï¼š<a href="https://github.com/bermanmaxim/LovaszSoftmax" target="_blank" rel="noopener">https://github.com/bermanmaxim/LovaszSoftmax</a></p></li></ol><h2 id="ä¸€äº›è¡¥å……"><a href="#ä¸€äº›è¡¥å……" class="headerlink" title="ä¸€äº›è¡¥å……"></a>ä¸€äº›è¡¥å……</h2><ol><li><p>æ”¹è¿›ï¼š</p><ol><li>dropoutã€batch normalizationï¼šä»è®ºæ–‡ä¸Šçœ‹ï¼Œunetåªåœ¨æœ€æ·±å±‚å·ç§¯å±‚åé¢æ·»åŠ äº†dropout layerï¼ŒBNæœªè¡¨ï¼Œè€Œcommon senseç”¨æ¯ä¸€ä¸ªconvå±‚åé¢æ¥BNå±‚èƒ½å¤Ÿæ›¿æ¢æ‰dropoutå¹¶èƒ½è·å¾—æ€§èƒ½æå‡çš„ã€‚</li><li>UpSampling2Dã€Conv2DTransposeï¼šunetä½¿ç”¨äº†ä¸Šé‡‡æ ·ï¼Œvnetä½¿ç”¨äº†deconvï¼Œä½†æ˜¯â€œDeConv will produce image with checkerboard effect, which can be revised by upsample and convâ€(<a href="https://distill.pub/2016/deconv-checkerboard/" target="_blank" rel="noopener">Reference</a>)ã€‚</li><li>valid paddingã€same paddingï¼šunetè®ºæ–‡ä½¿ç”¨å›¾åƒpatchä½œä¸ºè¾“å…¥ï¼Œç‰¹å¾æå–æ—¶ä½¿ç”¨valid paddingï¼ŒæŸå¤±è¾¹ç¼˜ä¿¡æ¯ã€‚</li><li>network blocksï¼šunetç”¨çš„conv blockæ˜¯ä¸¤ä¸ªä¸€ç»„çš„3*3convï¼Œvnetç¨å¾®ä¸åŒä¸€ç‚¹ï¼Œå¯ä»¥å°è¯•çš„blockæœ‰ResNetï¼ResNextã€DenseNetã€DeepLabç­‰ã€‚</li><li>pretrained encoderï¼šfeature extraction pathä½¿ç”¨ä¸€äº›ç°æœ‰çš„backboneï¼Œå¯ä»¥åŠ è½½é¢„è®­ç»ƒæƒé‡(<a href="https://arxiv.org/abs/1801.05746" target="_blank" rel="noopener">Reference</a>)ï¼ŒåŠ é€Ÿè®­ç»ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚</li><li>åŠ å…¥SEæ¨¡å—(<a href="https://zhuanlan.zhihu.com/p/36890585" target="_blank" rel="noopener">Reference</a>)ï¼šå¯¹æ¯ä¸ªé€šé“çš„ç‰¹å¾åŠ æƒ</li><li>attention mechanismsï¼š</li><li>å¼•ç”¨nn-Unetä¸»è¦<strong>ç»“æ„æ”¹è¿›</strong>åˆé›†ï¼šâ€œJust to provide some prominent examples: variations of encoder-decoder style architectures with skip connections, first introduced by the U-Net [12], include the introduction of residual connections [9], dense connections [6], at- tention mechanisms [10], additional loss layers [5], feature recalibration [13], and others [11]. </li></ol></li><li><p>è¡ç”Ÿï¼š</p><ol><li><p>TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation </p></li><li><p>nnU-Net: Breaking the Spell on Successful Medical Image Segmentation</p></li></ol></li></ol><h2 id="TernausNet-U-Net-with-VGG11-Encoder-Pre-Trained-on-ImageNet-for-Image-Segmentation"><a href="#TernausNet-U-Net-with-VGG11-Encoder-Pre-Trained-on-ImageNet-for-Image-Segmentation" class="headerlink" title="TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation"></a>TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation</h2><ol><li><p>åŠ¨æœºï¼š</p><ul><li>neural network initialized with pre-trained weights usually shows better performance than those trained from scratch on a small dataset. </li><li>ä¿ç•™encoder-decoderçš„ç»“æ„ï¼ŒåŒæ—¶å……åˆ†åˆ©ç”¨è¿ç§»å­¦ä¹ çš„ä¼˜åŠ¿</li></ul></li><li><p>è®ºç‚¹ï¼š</p><ul><li>load pretrained weights</li><li>ç”¨huge datasetåšé¢„è®­ç»ƒ</li></ul></li><li><p>æ–¹æ³•ï¼š</p><ul><li>ç”¨vgg11æ›¿æ¢åŸå§‹çš„encoderï¼Œå¹¶load pre-trained weights on ImageNetï¼š</li><li>æœ€æ·±å±‚è¾“å…¥(maxpooling5)ï¼šuse a single conv of 512 channels that <strong>serves as a bottleneck central part</strong> of the network</li></ul><p><img src="/2019/12/05/unet-vnet/TernausNet.png" width="50%;">                      <img src="/2019/12/05/unet-vnet/vgg11.png" width="30%;"></p><ul><li><p>upsamplingæ¢æˆäº†convTranspose</p></li><li><p>loss functionï¼šIOU + BCEï¼š</p><script type="math/tex; mode=display"> L = BCE - log(IOU)</script></li><li><p>inferenceï¼šchoose a <strong>threshold 0.3</strong>, all pixel values below which are set to be zero</p></li></ul></li><li><p>ç»“è®ºï¼š</p><ol><li>converge faster</li><li>better IOU</li></ol></li></ol><h2 id="nnU-Net-Breaking-the-Spell-on-Successful-Medical-Image-Segmentation"><a href="#nnU-Net-Breaking-the-Spell-on-Successful-Medical-Image-Segmentation" class="headerlink" title="nnU-Net: Breaking the Spell on Successful Medical Image Segmentation"></a>nnU-Net: Breaking the Spell on Successful Medical Image Segmentation</h2><ol><li><p>åŠ¨æœº</p><ul><li>many proposed methods fail to generalize: å¯¹äºåˆ†å‰²ä»»åŠ¡ï¼Œä»unetå‡ºæ¥ä¹‹åçš„å‡ å¹´é‡Œï¼Œåœ¨ç½‘ç»œç»“æ„ä¸Šå·²ç»æ²¡æœ‰å¤šå°‘çš„çªç ´äº†ï¼Œç»“æ„ä¿®æ”¹è¶Šå¤šï¼Œåè€Œè¶Šå®¹æ˜“è¿‡æ‹Ÿåˆ</li><li>relies on just a simple U-Net architecture embedded in a robust training scheme</li><li>automate necessary adaptations such as preprocessing, the exact patch size, batch size, and inference settings based on the properties of a given dataset: æ›´å¤šçš„æå‡å…¶å®åœ¨äºç†è§£æ•°æ®ï¼Œé’ˆå¯¹æ•°æ®é‡‡ç”¨é€‚å½“çš„é¢„å¤„ç†å’Œè®­ç»ƒæ–¹æ³•å’ŒæŠ€å·§</li></ul></li><li><p>è®ºç‚¹</p><ul><li>the diversity and individual peculiarities of imaging datasets make it difficult to generalize </li><li>prominent modifications focus on architectural modifications, merely brushing over all the other hyperparameters</li><li>we propose: ä½¿ç”¨åŸºç¡€ç‰ˆunetï¼šnnUNetï¼ˆno-new-Netï¼‰<ul><li>a formalism for automatic adaptation to new datasets</li><li>automatically designs and executes a network training pipeline </li><li>without any manual fine-tuning</li></ul></li></ul></li><li><p>è¦ç´ </p><p>a segmentation task: $f_{\theta}(X) = \hat Y$,  in this paper we seek for a $g(X,Y)=\theta$.</p><p>First we distinguish two type of hyperparameters:</p><ul><li>static paramsï¼šin this case the network architecture and a robust training scheme </li><li>dynamic paramsï¼šthose that need to be changed in dependence of $X$ and $Y$</li></ul><p>Second we define gâ€”â€”a set of heuristics rules covering the entire process of the task:</p><ul><li>é¢„å¤„ç†ï¼šresamplingå’Œnormalization</li><li>è®­ç»ƒï¼šlossï¼Œoptimizerè®¾ç½®ã€æ•°æ®å¢å¹¿</li><li>æ¨ç†ï¼špatch-basedç­–ç•¥ã€test-time-augmentationsé›†æˆå’Œæ¨¡å‹é›†æˆç­‰</li><li>åå¤„ç†ï¼šå¢å¼ºå•è¿é€šåŸŸç­‰</li></ul></li><li><p>æ–¹æ³•</p><ol><li><p>Preprocessing</p><ul><li>Image Normalizationï¼š<ul><li>CTï¼š$normed_intensity = (intensity  - fg_mean) / fg_standard_deviation$,   $fg$ for $[0.05,0.95]$ foreground intensity</li><li>not CTï¼š$normed_intensity = (intensity  - mean) / standard_deviation $</li></ul></li><li>Voxel Spacingï¼š<ul><li>for each axis chooses the median as the target spacing</li><li>image resampled with third order spline interpolation</li><li>z-axis using nearest neighbor interpolation if â€˜anisotropic spacingâ€™ occurs</li><li>mask resampled with third order spline interpolation</li></ul></li></ul></li><li><p>Training Procedure </p><ul><li><p>Network Architectureï¼š</p><ul><li><p>3 <strong>independent</strong> modelï¼ša 2D U-Net, a 3D U-Net and a cascade of two 3D U-Net  </p><p>  <img src="/2019/12/05/unet-vnet/nnUnet.jpg" width="80%;"></p></li><li><p>padded convolutionsï¼što achieve identical output and input shapes </p></li><li><p><strong>instance normalization</strong>ï¼šâ€œBNé€‚ç”¨äºåˆ¤åˆ«æ¨¡å‹ï¼Œæ¯”å¦‚å›¾ç‰‡åˆ†ç±»æ¨¡å‹ã€‚å› ä¸ºBNæ³¨é‡å¯¹æ¯ä¸ªbatchè¿›è¡Œå½’ä¸€åŒ–ï¼Œä»è€Œä¿è¯æ•°æ®åˆ†å¸ƒçš„ä¸€è‡´æ€§ï¼Œè€Œåˆ¤åˆ«æ¨¡å‹çš„ç»“æœæ­£æ˜¯å–å†³äºæ•°æ®æ•´ä½“åˆ†å¸ƒã€‚ä½†æ˜¯BNå¯¹batchsizeçš„å¤§å°æ¯”è¾ƒæ•æ„Ÿï¼Œç”±äºæ¯æ¬¡è®¡ç®—å‡å€¼å’Œæ–¹å·®æ˜¯åœ¨ä¸€ä¸ªbatchä¸Šï¼Œæ‰€ä»¥å¦‚æœbatchsizeå¤ªå°ï¼Œåˆ™è®¡ç®—çš„å‡å€¼ã€æ–¹å·®ä¸è¶³ä»¥ä»£è¡¨æ•´ä¸ªæ•°æ®åˆ†å¸ƒï¼›INé€‚ç”¨äºç”Ÿæˆæ¨¡å‹ï¼Œæ¯”å¦‚å›¾ç‰‡é£æ ¼è¿ç§»ã€‚å› ä¸ºå›¾ç‰‡ç”Ÿæˆçš„ç»“æœä¸»è¦ä¾èµ–äºæŸä¸ªå›¾åƒå®ä¾‹ï¼Œæ‰€ä»¥å¯¹æ•´ä¸ªbatchå½’ä¸€åŒ–ä¸é€‚åˆå›¾åƒé£æ ¼åŒ–ï¼Œåœ¨é£æ ¼è¿ç§»ä¸­ä½¿ç”¨Instance Normalizationä¸ä»…å¯ä»¥åŠ é€Ÿæ¨¡å‹æ”¶æ•›ï¼Œå¹¶ä¸”å¯ä»¥ä¿æŒæ¯ä¸ªå›¾åƒå®ä¾‹ä¹‹é—´çš„ç‹¬ç«‹ã€‚â€</p></li><li><p>Leaky ReLUs</p></li></ul></li><li><p>Network Hyperparametersï¼š</p><ul><li>sets the batch size, patch size and number of pooling operations for each axis based on the memory consumption </li><li>large patch sizes are favored over large batch sizes </li><li>pooling along each axis is done until the voxel size=4</li><li>start num of filters=30, double after each pooling</li><li>If the selected patch size covers less than 25% of the voxels, train the 3D U-Net cascade on a downsampled version of the training data  to keep sufficient context </li></ul></li><li><p>Network Training:</p><ul><li>five-fold cross-validation </li><li>One epoch is defined as processing 250 batches </li><li>loss = dice loss + cross-entropy loss </li><li>Adam(lr=3e-4, decay=3e-5)</li><li>lrReduce: EMA(train_loss), 30 epoch, factor=0.2</li><li>earlyStop: earning rate drops below 10 6 or 1000 epochs are exceeded</li><li>data augmentation: elastic deformations, random scaling and random rotations as well as <strong>gamma augmentation</strong>($g(x,y)=f(x,y)^{gamma}$)</li><li>keep transformations in 2D-plane if â€˜anisotropic spacingâ€™ occurs</li></ul></li><li><p>Inference </p><ul><li>sliding window with half the patch size: this increases the weight of the predictions close to the center relative to the borders</li><li>ensemble:<ul><li>U-Net configurations (2D, 3D and cascade)</li><li>furthermore uses the five models (five-fold cross-validation)</li></ul></li><li></li></ul></li></ul></li></ol></li><li><p>Ablation studies</p><p><img src="/2019/12/05/unet-vnet/nn-Unet Ablation studies.png" width="70%;"></p></li></ol><h2 id="3D-U-Net-Learning-Dense-Volumetric-Segmentation-from-Sparse-Annotation"><a href="#3D-U-Net-Learning-Dense-Volumetric-Segmentation-from-Sparse-Annotation" class="headerlink" title="3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation"></a>3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation</h2><ol><li><p>åŠ¨æœº</p><ul><li>learns from sparsely/full annotated volumetric images (user annotates some slices)</li><li>provides a dense 3D segmentation </li></ul></li></ol><ol><li><p>è¦ç´ </p><ul><li>3D operations </li><li>avoid bottlenecks and use batch normalization for faster convergence</li><li>on-the-fly elastic deformation</li><li>train from scratch</li></ul></li><li><p>è®ºç‚¹</p><ul><li>neighboring slices show almost the same information </li><li>many biomedical applications generalizes reasonably well because medical images comprises repetitive structures  </li><li>thus we suggest dense-volume-segmentation-network that only requires some annotated 2D slices for training</li><li><p>scenarios</p><ul><li>manual annotated ä¸€éƒ¨åˆ†sliceï¼Œç„¶åè®­ç»ƒç½‘ç»œå®ç°dense seg</li><li>ç”¨ä¸€éƒ¨åˆ† sparsely annotatedçš„datasetä½œä¸ºtraining setï¼Œç„¶åè®­ç»ƒçš„ç½‘ç»œå®ç°åœ¨æ–°çš„æ•°æ®é›†ä¸Šdense seg</li></ul><p><img src="/2019/12/05/unet-vnet/scenarios.png" width="50%;"></p></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>Network Architecture </p><ul><li><p>compressionï¼š2*3x3x3 convs(+BN)+relu+2x2x2 maxpooling</p></li><li><p>decompressionï¼š2x2x2 upconv+2*3x3x3 convs+relu</p></li><li><p>headï¼š1x1x1 conv</p></li><li><p>concat shortcut connections</p></li><li><p>ã€QUESTIONã€‘avoid bottlenecks by doubling the number of channels already before max pooling</p><p>  ä¸ªäººç†è§£è¿™ä¸ªdouble channelæ˜¯åœ¨è·ŸåŸå§‹çš„unetç»“æ„å¯¹æ¯”ï¼ŒåŸå§‹unetæ¯ä¸ªstageçš„ä¸¤ä¸ªconvçš„filter numæ˜¯ä¸€æ ·çš„ï¼Œç„¶åè¿›è¡Œmax poolingä¼šæŸå¤±éƒ¨åˆ†ä¿¡æ¯ï¼Œä½†æ˜¯åˆ†å‰²ä»»åŠ¡æœ¬èº«æ˜¯ä¸ªdense predictionï¼Œæ‰€ä»¥å¢å¤§channelæ¥å‡å°‘ä¿¡æ¯æŸå¤±</p><p>  ä½†æ˜¯ä¸ç†è§£ä»€ä¹ˆå«â€œavoid bottlenecksâ€</p><p>  åŸæ–‡è¯´æ˜¯å‚è€ƒäº†ã€ŠRethinking the inception architecture for computer visionã€‹å¤§åé¼é¼çš„inception V3</p><p>  å¯èƒ½å¯¹åº”çš„æ˜¯â€œ1. Avoid representational bottlenecks, especially early in the network.â€ï¼Œä»è¾“å…¥åˆ°è¾“å‡ºï¼Œè¦é€æ¸å‡å°‘feature mapçš„å°ºå¯¸ï¼ŒåŒæ—¶è¦é€æ¸å¢åŠ feature mapçš„æ•°é‡ã€‚</p></li></ul><p><img src="/2019/12/05/unet-vnet/3Dunet.png" width="50%;"></p><ul><li>inputï¼š132x132x116 voxel tile </li><li>outputï¼š44x44x28</li><li>BNï¼šbefore each ReLU</li></ul></li></ul></li></ol><ul><li><p><strong>weighted softmax loss function</strong>ï¼šsetting the weights of unlabeled pixels to zero makes it possible to learn from only the labelled ones and, hence, to generalize to the whole volumeï¼ˆæ˜¯ä¸æ˜¯random set the loss zeros of some samplesæ€»èƒ½è®©ç½‘ç»œæ›´å¥½çš„generalizeï¼Ÿï¼‰</p><ul><li><p>Data</p><ul><li>manually annotated some orthogonal xy, xz, and yz slices </li><li>annotation slices were sampled uniformly</li></ul></li><li><p>ran on down-sampled versions of the original resolution by factor of two </p></li><li><p>labelsï¼š0: â€œinside the tubuleâ€; 1: â€œtubuleâ€; 2: â€œbackgroundâ€, and 3: â€œunlabeledâ€.  </p></li><li><p>Training</p><ul><li>rotation, scaling and gray value augmentation</li></ul></li><li>a smooth dense deformationï¼šrandom vector, normal distribution, B-spline interpolation <ul><li>weighted cross-entropy lossï¼šincrease weights  â€œinside the tubuleâ€, reduce weights â€œbackgroundâ€, set zero â€œunlabeledâ€</li></ul></li></ul></li></ul><h2 id="2-5D-UNet-Automatic-Segmentation-of-Vestibular-Schwannoma-from-T2-Weighted-MRI-by-Deep-Spatial-Attention-with-Hardness-Weighted-Loss"><a href="#2-5D-UNet-Automatic-Segmentation-of-Vestibular-Schwannoma-from-T2-Weighted-MRI-by-Deep-Spatial-Attention-with-Hardness-Weighted-Loss" class="headerlink" title="2.5D-UNet: Automatic Segmentation of Vestibular Schwannoma from T2-Weighted MRI by Deep Spatial Attention with Hardness-Weighted Loss"></a>2.5D-UNet: Automatic Segmentation of Vestibular Schwannoma from T2-Weighted MRI by Deep Spatial Attention with Hardness-Weighted Loss</h2><ol><li><p>ä¸“ä¸šæœ¯è¯­</p><ul><li>Vestibular Schwannoma(VS) tumorsï¼šå‰åº­ç¥ç»é˜ç˜¤</li><li>through-plane resolutionï¼šå±‚åš</li><li>isotropic resolutionï¼šå„å‘åŒæ€§</li><li>anisotropic resolutionsï¼šå„å‘å¼‚æ€§</li></ul></li><li><p>åŠ¨æœº</p><ul><li><p>tumorçš„ç²¾ç¡®è‡ªåŠ¨åˆ†å‰²</p></li><li><p>challenge</p><ul><li>low contrastï¼šhardness-weighted Dice loss functio </li><li>small target regionï¼šattention module </li><li>low through-plane resolutionï¼š2.5D</li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li>segment small structures from large image contexts<ul><li>coarse-to-fine </li><li>attention map </li><li>Dice loss </li><li>our method<ul><li>end-to-end supervision on the learning of attention map </li><li>voxel-level hardness- weighted Dice loss function </li></ul></li></ul></li><li>CNN<ul><li>2D CNNs ignore inter-slice correlation </li><li>3D CNNs most applied to images with isotropic resolution requiring upsampling</li><li>to balance the physical receptive field (in terms of mm rather than voxels)ï¼šmemory rise</li><li>our method<ul><li>high in-plane resolution &amp; low through-plane resolution </li><li>2.5D CNN combining 2D and 3D convolutions </li><li>use inter-slice features </li><li>more efficient than 3D CNNs</li></ul></li></ul></li><li>æ•°æ®<ul><li>T2-weighted MR images of 245 patients with VS tumor</li><li>high in-plane resolution around 0.4 mmÃ—0.4 mmï¼Œ512x512</li><li>slice thickness and inter-slice spacing 1.5 mmï¼Œslice number 19 to 118</li><li>cropped cube sizeï¼š100 mmÃ—50 mmÃ—50 mm </li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>architecture</p><ul><li>five levelsï¼šL1ã€L2 use 2Dï¼ŒL3ã€L4ã€L5 use 3D</li><li>After the first two max-pooling layers that downsample the feature maps only in 2D, the feature maps in L3 and the followings have a near- isotropic 3D resolution.  </li><li>start channelsï¼š16</li><li>conv blockï¼šconv-BN-pReLU</li><li><p>add a spatial attention module to each level of the decoder </p><p><img src="/2019/12/05/unet-vnet/2.5D UNet.png" width="70%;"></p></li></ul></li><li><p>spatial attention module </p><ul><li>A spatial attention map can be seen as a single-channel image of attention coefficient </li><li>inputï¼šfeature map with channel $N_l$</li><li>conv1+ReLUï¼š channel $N_l/2$</li><li>conv2+Sigmoidï¼šchannel 1ï¼Œoutputs the attention map</li><li>multiplied the feature map with the attention map</li><li>a residual connection</li><li>explicit supervision <ul><li>multi-scale attention loss </li><li>$L_{attention} = \frac{1}{L} \sum_{L} l(A_l, G_l^f)$</li><li>$A_l$æ˜¯æ¯ä¸€å±‚çš„attention mapï¼Œ$G_l^f$æ˜¯æ¯ä¸€å±‚æ˜¯å‰æ™¯ground truth average-poolåˆ°å½“å‰resolutionçš„mask</li></ul></li></ul></li><li><p>Voxel-Level Hardness-Weighted Dice Loss</p><ul><li><p>automatic hard voxel weightingï¼š$w_i = \lambda * abs(p_i - g_i) + (1-\lambda)$</p></li><li><p>$\lambda \in [0,1]$ï¼Œcontrols the degree of hard voxel weighting</p></li><li><p>hardness-weighted Dice loss (HDL) ï¼š</p><script type="math/tex; mode=display">  l(P,G) = 1.0 - \frac{1}{C}\sum_{C} \frac{2\sum_i w_i p_i g_i + \epsilon}{\sum_i w_i (p_i + g_i) + \epsilon}</script></li><li><p>total lossï¼š</p><script type="math/tex; mode=display">  L = \frac{1}{L} \sum_{L} l(A_l, G_l^f) + l(P,G)</script></li></ul></li></ul></li></ol><h2 id="Combining-analysis-of-multi-parametric-MR-images-into-a-convolutional-neural-network-Precise-target-delineation-for-vestibular-schwannoma-treatment-planning"><a href="#Combining-analysis-of-multi-parametric-MR-images-into-a-convolutional-neural-network-Precise-target-delineation-for-vestibular-schwannoma-treatment-planning" class="headerlink" title="Combining analysis of multi-parametric MR images into a convolutional neural network: Precise target delineation for vestibular schwannoma treatment planning"></a>Combining analysis of multi-parametric MR images into a convolutional neural network: Precise target delineation for vestibular schwannoma treatment planning</h2><p>åªæœ‰æ‘˜è¦å’Œä¸€å¹…å›¾</p><p><img src="/2019/12/05/unet-vnet/two-pathway-unet.jpg" width="80%;"></p><ul><li>multi-parametric MR imagesï¼šT1Wã€T2Wã€T1C</li><li>two-pathway U-Net model<ul><li>kernel 3 Ã— 3 Ã— 1 and 1 Ã— 1 Ã— 3 respectively</li><li>to extract the in-plane and through-plane features of the anisotropic MR images</li></ul></li><li>ç»“è®º<ul><li>The proposed two-pathway U-Net model outperformed the single-pathway U-Net model when segmenting VS using anisotropic MR images.</li><li>multi-inputsï¼ˆT1ã€T2ï¼‰outperforms single-inputs</li></ul></li></ul>]]></content>
      
      
        <tags>
            
            <tag> paper, è¯­ä¹‰åˆ†å‰² </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>yoloç³»åˆ—</title>
      <link href="/2019/11/28/yolo%E7%B3%BB%E5%88%97/"/>
      <url>/2019/11/28/yolo%E7%B3%BB%E5%88%97/</url>
      <content type="html"><![CDATA[<h2 id="ç»¼è¿°"><a href="#ç»¼è¿°" class="headerlink" title="ç»¼è¿°"></a>ç»¼è¿°</h2><ol><li>[yolov1] Yolov1: You Only Look Once: Unified, Real-Time Object Detection</li><li>[yolov2] Yolov2: YOLO9000: Better, Faster, Stronger</li><li>[yolov3] Yolov3: An Incremental Improvement </li><li>[yolov4] YOLOv4: Optimal Speed and Accuracy of Object Detection </li><li>[poly-yolo] POLY-YOLO: HIGHER SPEED, MORE PRECISE DETECTION AND INSTANCE SEGMENTATION FOR YOLOV3 </li><li>[scaled-yolov4] Scaled-YOLOv4: Scaling Cross Stage Partial Network </li></ol><h2 id="0-review"><a href="#0-review" class="headerlink" title="0. review"></a>0. review</h2><ol><li><p>review0121ï¼šå…³äºyolo loss</p><p> ä¹‹å‰çœ‹kerasç‰ˆçš„yolo lossï¼ŒåŒ…å«åˆ†ç±»çš„bceï¼Œå›å½’çš„l2/mseï¼Œä»¥åŠconfidenceçš„å›å½’lossï¼Œå…¶ä¸­conf lossè¢«å»ºæ¨¡æˆå•çº¯çš„0-1åˆ†ç±»é—®é¢˜ï¼Œç”¨bceæ¥å®ç°ã€‚</p><p> äº‹å®ä¸ŠåŸç‰ˆçš„yolo lossä¸­ï¼Œobjectnessæ˜¯iouï¼ˆpredå’Œgtçš„iouï¼‰ï¼Œä»æ„ä¹‰ä¸Šï¼Œä¸ä»…æŒ‡ç¤ºå½“å‰æ ¼å­æœ‰æ— ç›®æ ‡ï¼Œè¿˜å¯¹å½“å‰çš„box predictionåšäº†è¯„ä¼°</p><ul><li>å›ä¼ æ¢¯åº¦</li><li><p>ä¸å›ä¼ æ¢¯åº¦</p><p>iouæ˜¯é€šè¿‡xywhè®¡ç®—çš„ï¼Œscaled_yolov4ä¸­æŠŠè¿™ä¸ªæ¢¯åº¦æˆªæ–­ï¼Œåªä½œä¸ºä¸€ä¸ªå€¼ï¼Œå¯¹confidenceè¿›è¡Œæ¢¯åº¦å›ä¼ ï¼Œ</p><p>æ¢¯åº¦ä¸æˆªæ–­ä¹Ÿæ²¡æœ‰é—®é¢˜ï¼Œç›¸å½“äºå¯¹xywhå†å›ä¼ ä¸€ä¸ªiouçš„loss</p></li></ul></li></ol><h2 id="1-Yolov1-You-Only-Look-Once-Unified-Real-Time-Object-Detection"><a href="#1-Yolov1-You-Only-Look-Once-Unified-Real-Time-Object-Detection" class="headerlink" title="1. Yolov1: You Only Look Once: Unified, Real-Time Object Detection"></a>1. Yolov1: You Only Look Once: Unified, Real-Time Object Detection</h2><ol><li>åŠ¨æœº:<ul><li>end-to-end: 2 stages â€”-&gt; 1 stage</li><li>real-time</li></ul></li><li><p>è®ºç‚¹ï¼š</p><ul><li>past methods:  complex pipelines, hard to optimize(trained separately) <ul><li>DPM use a sliding window and a classifier to evaluate an object at various locations </li><li>R-CNN use region proposal and run classifier on the proposed boxes,  then post-processing </li></ul></li><li>in this paper:  you only look once at an image <ul><li>rebuild the framework as a <strong>single</strong> regression problem:  single stands for <strong>you donâ€™t have to run classifiers on each patch</strong></li><li><strong>straight</strong> from image pixels to bounding box coordinates and class probabilities:  straight stands for <strong>you obtain the bounding box and the classification results side by side, comparing to the previous serial pipeline </strong></li></ul></li></ul></li><li><p>advantagesï¼š</p><ul><li>fast &amp; twice the <strong>mean average precision</strong> of other real-time systems</li><li>CNN sees the entire image thus encodes contextual information </li><li>generalize better</li></ul></li><li><p>disadvantage:</p><ul><li>accuracy: â€œ it struggles to precisely localize some objects, especially small onesâ€</li></ul></li><li><p>ç»†èŠ‚ï¼š</p><ul><li><p>gridï¼š</p><p>Our system divides the input image into an S Ã— S grid. </p><p>If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object. </p><p><img src="/2019/11/28/yoloç³»åˆ—/grid.png" width="60%;"></p></li><li><p>predictionï¼š</p><p> Each grid cell predicts B bounding boxes,  confidence scores <strong>for these boxes</strong> , and C conditional class probabilities <strong>for each grid</strong></p><p> that is an <script type="math/tex">S*S*(B*5+C)</script> tensor</p><ul><li>We normalize the bounding box width and height by the image width and height so that they fall between 0 and 1.</li><li>We parametrize the bounding box x and y coordinates to be offsets of a particular grid cell so they are also bounded between 0 and 1. </li></ul></li><li><p>at test timeï¼š</p><p> We obtain the <strong>class-specific confidence</strong> for individual box by multiply the class probability and box confidenceï¼š</p><script type="math/tex; mode=display"> Pr(Class_i | Object) * Pr(Object)* IOU^{truth}_{pred} = Pr(Class_i)* IOU^{truth}_{pred}</script></li><li><p>networkï¼š</p><p>the convolutional layers extract features from the image </p><p>while the fully connected layers predict the probabilities and coordinates</p><p><img src="/2019/11/28/yoloç³»åˆ—/network.png" width="60%"></p></li><li><p>trainingï¼š</p><p> activationï¼šuse a linear activation function for the final layer and leaky rectified linear activation all the other layers</p><p> optimizationï¼šuse sum-squared error, however it does not perfectly align with the goal of maximizing average precision</p><p> â€‹    * weights equally the localization error and classification errorï¼š$\lambda_{coord}$</p><p> â€‹    * weights equally the grid cells containing and not-containing objectsï¼š$\lambda_{noobj}$</p><p> â€‹    * weights equally the large boxes and small boxesï¼šsquare roots the h&amp;w insteand of the straight h&amp;w</p><p> lossï¼špick the box predictor has the highest current IOU with the ground truth per grid cell</p><p> avoid overfittingï¼šdropout &amp; data augmentation</p><p> â€‹    * use dropout after the first connected layer,</p><p> â€‹    * introduce random scaling and translations of up to 20% of the original image size for data augmentation</p><p> â€‹    * randomly adjust the exposure and saturation of the image by up to a factor of 1.5 in the HSV color space for data augmentation</p></li><li><p>inferenceï¼š</p><p> multiple detectionsï¼šsome objects locates near the border of multiple cells and <strong>can be well localized by multiple cells</strong>. <strong>Non-maximal suppression</strong> is proved critical, adding 2- 3% in mAP. </p></li></ul></li><li><p>Limitationsï¼š</p><ul><li>strong spatial constraintsï¼šdecided by the settings of bounding boxes</li><li><p>softmax classificationï¼šcan only have one class for each grid </p><blockquote><p>â€œThis spatial constraint lim- its the number of nearby objects that our model can pre- dict. Our model struggles with small objects that appear in groups, such as flocks of birds. â€œ</p><p>â€œ It struggles to generalize to objects in new or unusual aspect ratios or configurations. â€œ</p></blockquote></li><li><p>coarse bounding box predictionï¼šthe architecture has multiple downsampling layers</p></li><li><p>the loss function treats errors the same in small bounding boxes versus large bounding boxesï¼š</p><blockquote><p>The same error has much greater effect on a small boxâ€™s IOU than a big box.</p><p>â€œOur main source of error is incorrect localizations. â€œ</p></blockquote></li></ul></li><li><p>Comparisonï¼š</p><ul><li>mAP among <strong>real-time detectors</strong> and <strong>Less Than Real-Time detectors</strong>ï¼šless mAP than fast-rcnn but much faster</li><li>error analysis between yolo and fast-rcnnï¼šgreater localization error and less background false-positive</li><li>combination analysisï¼š[fast-rcnn+yolo] defeats [fast-rcnn+fast-rcnn] since YOLO makes different kinds of mistakes  with fast-rcnn</li><li>generalizabilityï¼šRCNN degrades more because the Selective Search is tuned for natural images, change of dataset makes the proposals get worse. YOLO degrades less because it models the size and shape of objects, change of dataset varies less at object level but more at pixel level.</li></ul></li></ol><h2 id="2-Yolov2-YOLO9000-Better-Faster-Stronger"><a href="#2-Yolov2-YOLO9000-Better-Faster-Stronger" class="headerlink" title="2. Yolov2: YOLO9000: Better, Faster, Stronger"></a>2. Yolov2: YOLO9000: Better, Faster, Stronger</h2><ol><li>åŠ¨æœºï¼š<ul><li>run at varying sizesï¼šoffering an easy tradeoff between speed and accuracy</li><li>recognize a wide variety of objects ï¼šjointly train on object detection and classification, so that the model can predict objects that arenâ€™t labelled in detection data</li><li>better performance but still fast</li></ul></li><li><p>è®ºç‚¹ï¼š</p><ul><li>Current object detection datasets are limited compared to classification datasets  <ul><li>leverage the classification data to expand the scope of current detection system</li><li>joint training algorithm making the object detectors working on both detection and classification data</li></ul></li><li><strong>Better performance</strong> often hinges on larger networks or ensembling multiple models. However we want a more accurate detector that is <strong>still fast</strong></li><li>YOLOv1â€™s shortcomings<ul><li>more localization errors</li><li>low recall</li></ul></li></ul></li><li><p>è¦ç´ ï¼š</p><ol><li><p>better</p><p> <img src="/2019/11/28/yoloç³»åˆ—/better.png" width="70%;"></p></li><li><p>faster</p><ul><li>backbone</li></ul></li><li><p>stronger</p><ul><li><p>uses labeled detection images to learn to precisely localize objects </p></li><li><p>uses classification images to increase its vocabulary and robustness</p></li></ul></li></ol></li><li><p>æ–¹æ³•ï¼š</p><ol><li><p>betterï¼š</p><ol><li><p>batch normalizationï¼šconvergence &amp; regularization</p><blockquote><p>add batch normalization on all of the convolutional layers </p><p>remove dropout from the model </p></blockquote></li><li><p>high resolution classifierï¼špretrain a hi-res classifier </p><blockquote><p>first fine tune the classification network at the full 448 Ã— 448 resolution for 10 epochs on ImageNet</p><p>then fine tune the resulting network on detection </p></blockquote></li><li><p>convolutional with anchor boxesï¼š</p><p>YOLOv1é€šè¿‡ç½‘ç»œæœ€åçš„<strong>å…¨è¿æ¥å±‚</strong>ï¼Œç›´æ¥é¢„æµ‹æ¯ä¸ªgridä¸Šbounding boxçš„åæ ‡</p><p>è€ŒRPNåŸºäºå…ˆéªŒæ¡†ï¼Œä½¿ç”¨æœ€åä¸€å±‚<strong>å·ç§¯å±‚</strong>ï¼Œåœ¨ç‰¹å¾å›¾çš„å„ä½ç½®é¢„æµ‹bounding boxçš„offsetå’Œconfidence</p><blockquote><p> â€œPredicting offsets instead of coordinates simplifies the problem and makes it easier for the network to learnâ€</p></blockquote><p>YOLOv2å»æ‰äº†å…¨è¿æ¥å±‚ï¼Œä¹Ÿä½¿ç”¨anchor boxæ¥å›å½’bounding box</p><blockquote><p>eliminate one pooling layer to make the network output have higher resolution</p><p>shrink the network input to 416<em>416 to obtain an odd number so that there is a <em>*single center cell</em></em> in the feature map</p><p>predict class and objectness for every anchor box(offset prediction) instead of nothing(direct location&amp;scale prediction)</p></blockquote></li><li><p>dimension clusteringï¼š</p><p>what we want are priors that lead to good IOU scores, thus comes the distance metricï¼š</p><script type="math/tex; mode=display">d(box, centroid) = 1 - IOU(box, centroid)</script></li><li><p>direct location predictionï¼š</p><p>YOLOv1 encounter model instability issue for predicting the (x, y) locations for the box</p><p>RPN also takes a long time to stabilize  by predicting a (tx, ty) and obtain the (x, y) center coordinates indirectly because this formulation is unconstrained so any anchor box can end up at any point in the imageï¼š</p><script type="math/tex; mode=display">x = t_x * w_a - x_a\\y = t_y * h_a - y_a</script><blockquote><p>å­¦ä¹ RPNï¼šå›å½’ä¸€ä¸ªç›¸å¯¹é‡ï¼Œæ¯”ç›²çŒœå›å½’ä¸€ä¸ªç»å¯¹locationï¼ˆYOLOv1ï¼‰æ›´å¥½å­¦ä¹ </p><p>å­¦ä¹ YOLOv1ï¼šåŸºäºcellçš„é¢„æµ‹ï¼Œå°†bounding boxé™å®šåœ¨æœ‰é™åŒºåŸŸï¼Œä¸æ˜¯å…¨å›¾é£ï¼ˆRPNï¼‰</p></blockquote><p>YOLOv2å¯¹æ¯ä¸ªcellï¼ŒåŸºäº5ä¸ªprior anchor sizeï¼Œé¢„æµ‹5ä¸ªbounding boxï¼Œæ¯ä¸ªbounding boxå…·æœ‰5ç»´ï¼š</p><script type="math/tex; mode=display">b_x = \sigma(t_x) + c_x\\b_y = \sigma(t_y) + c_y\\b_w = p_w e^{t_w}\\b_h = p_h e^{t_h}\\Pr(object)*IOU(b,object) = \sigma(t_o)</script><ul><li>$t_x\ \&amp;\ t_y$ç”¨äºå›å½’bounding boxçš„ä½ç½®ï¼Œé€šè¿‡sigmoidæ¿€æ´»å‡½æ•°è¢«é™å®šåœ¨0-1ï¼Œé€šè¿‡ä¸Šå¼èƒ½å¤Ÿé—´æ¥å¾—åˆ°bounding boxçš„å½’ä¸€åŒ–ä½ç½®ï¼ˆç›¸å¯¹åŸå›¾ï¼‰</li><li>$t_w\ \&amp;\ t_h$ç”¨äºå›å½’bounding boxçš„å°ºåº¦ï¼Œè¾“å‡ºåº”è¯¥ä¸æ˜¯0-1é™å®šï¼Œ$p_w\ \&amp;\ p_h$æ˜¯å…ˆéªŒæ¡†çš„å½’ä¸€åŒ–å°ºåº¦ï¼Œé€šè¿‡ä¸Šå¼èƒ½å¤Ÿé—´æ¥å¾—åˆ°bounding boxçš„å½’ä¸€åŒ–å°ºåº¦ï¼ˆç›¸å¯¹åŸå›¾ï¼‰</li><li>$t_o$ç”¨äºå›å½’objectnessï¼Œé€šè¿‡sigmoidé™å®šåœ¨0-1ä¹‹é—´ï¼Œå› ä¸º$Pr(object)\ \&amp;\ IOU(b,object)$éƒ½æ˜¯0-1ä¹‹é—´çš„å€¼ï¼ŒIOUé€šè¿‡å‰é¢å››ä¸ªå€¼èƒ½å¤Ÿæ±‚è§£ï¼Œè¿›è€Œå¯ä»¥è§£è€¦objectness</li></ul><p><img src="/2019/11/28/yoloç³»åˆ—/regression.png" width="40%;"></p></li><li><p>fine-grained featuresï¼š</p><p>motiveï¼šå°ç‰©ä½“çš„æ£€æµ‹ä¾èµ–æ›´åŠ ç»†ç²’åº¦çš„ç‰¹å¾</p><p>cascadeï¼šFaster R-CNN and SSD both run their proposal networks at various feature maps in the network to get a range of resolutions</p><p>ã€QUESTIONã€‘YOLOv2 simply adds a passthrough layer from an earlier layer at 26 Ã— 26 resolutionï¼š</p><blockquote><p>latter featuremap â€”-&gt; upsampling</p><p>concatenate with early featuremap</p><p>the detector runs on top of this expanded feature map </p></blockquote><p>predicts a $N<em>N</em>(3*(4+1+80))$ tensor for each scale</p></li><li><p>multi-scale trainingï¼š</p><p>æ¨¡å‹æœ¬èº«ä¸é™å®šè¾“å…¥å°ºå¯¸ï¼šmodel only uses convolutional and pooling layers thus it can be resized on the fly </p></li></ol><ul><li>forces the network to learn to predict well across a variety of input dimensions <ul><li>the same network can predict detections at different resolutions</li></ul></li></ul><ol><li><p>lossï¼š<strong>cited from the latter yolov3 paper</strong></p><ul><li>use sum of squared error loss for box coordinate(x,y,w,h)ï¼šthen the gradient is $y_{true} - y_{pred}$</li></ul></li></ol><ul><li>use logistic regression for objectness scoreï¼šwhich should be 1 if the bounding box prior overlaps a ground truth object by more than any other bounding box prior<ul><li>if a bounding box prior is not assigned to a ground truth object it incurs no loss(coordinate&amp;objectness)</li></ul></li><li>use binary cross-entropy loss for multilabel classification</li></ul></li></ol></li><li><p>fasterï¼š</p><ol><li><p>darknet-19ï¼š</p><p> YOLOv1ä¸­è®¨è®ºè¿‡æ¢VGG-16å’ŒYOLOv1ä½¿ç”¨çš„backboneå¯¹æ¯”ï¼Œå‰è€…æœ‰mapæå‡ï¼Œä½†æ˜¯è€—æ—¶ã€‚</p><p> YOLOv2çš„æ–°backboneï¼Œå‚æ•°æ›´å°‘ï¼Œè€Œä¸”ç›¸å¯¹äºVGG16åœ¨ImageNetä¸Šç²¾åº¦æ›´é«˜ã€‚</p><p> <img src="/2019/11/28/yoloç³»åˆ—/darknet19.png" width="40%;"></p></li><li><p>training for classificationï¼š</p><pre><code> * first train on ImageNet using 224*224 * then fine-tuning on 448*448</code></pre><ol><li><p>training for detectionï¼š</p><ul><li><p>remove the last convolutional layer</p></li><li><p>add on three 3 Ã— 3 convolutional layers with 1024 filters each followed by a final 1Ã—1 convolutional layer with the number of outputs we need for detection </p></li><li>add a passthrough from the final 3 Ã— 3 Ã— 512 layer to the second to last convolutional layer so that our model can use fine grain features. </li></ul></li></ol></li><li><p>strongerï¼š</p><p> jointly trainingï¼šä»¥åå†å¡«å‘</p><ul><li>æ„é€ æ ‡ç­¾æ ‘</li><li>classification sampleç”¨cls lossï¼Œdetection sampleç”¨detect loss</li><li>é¢„æµ‹æ­£ç¡®çš„classification sampleç»™ä¸€ä¸ª.3 IOUçš„å‡è®¾å€¼ç”¨äºè®¡ç®—objectness loss</li></ul></li></ol></li></ol><h2 id="3-Yolov3-An-Incremental-Improvement"><a href="#3-Yolov3-An-Incremental-Improvement" class="headerlink" title="3. Yolov3: An Incremental Improvement"></a>3. Yolov3: An Incremental Improvement</h2><ol><li><p>åŠ¨æœºï¼š</p><p> nothing like super interesting, just a bunch of small changes that make it better</p></li><li><p>æ–¹æ³•ï¼š</p><ol><li><p>bounding box predictionï¼š</p><p> use anchor boxes and predicts offsets for each bounding box</p><p> use sum of squared error loss for training</p><p> predicts the objectness score for each bounding box using logistic regression</p><p> one ground truth coresponds to one best box and one loss</p></li><li><p>class predictionï¼š</p><p> use binary cross-entropy loss for multilabel classification</p></li><li><p><strong>ã€NEWã€‘prediction across scalesï¼š</strong></p><p> the detectorï¼ša few more convolutional layers following the feature map, the last of which predicts a 3-d(for 3 priors) tensor encoding bounding box, objectness, and class predictions</p><p> expanded feature mapï¼šupsampling the deeper feature map by 2X and concatenating with the former features</p><blockquote><p>â€œWith the new multi-scale predictions, YOLOv3 has better perfomance on small objects and comparatively worse performance on medium and larger size objects â€œ</p></blockquote></li><li><p><strong>ã€NEWã€‘feature extractorï¼š</strong></p><p> darknet-53 !</p><p> <img src="/2019/11/28/yoloç³»åˆ—/darknet53.png" width="40%;"></p></li><li><p>trainingï¼šcommon skills</p></li></ol></li></ol><h2 id="4-ä¸€äº›è¡¥å……"><a href="#4-ä¸€äº›è¡¥å……" class="headerlink" title="4. ä¸€äº›è¡¥å……"></a>4. ä¸€äº›è¡¥å……</h2><ol><li><p>metricsï¼šmAP</p><p> æœ€æ—©ç”±PASCAL VOCæå‡ºï¼Œè¾“å‡ºç»“æœæ˜¯ä¸€ä¸ªranked listï¼Œæ¯ä¸€é¡¹åŒ…å«æ¡†ã€confidenceã€classï¼Œ</p><p> yolov3æåˆ°äº†ä¸€ä¸ªâ€œCOCOs weird average mean AP metric â€</p><ul><li><p>IoUï¼šé¢„æµ‹æ¡†ä¸ground truthçš„äº¤å¹¶æ¯”ï¼Œä¹Ÿè¢«ç§°ä¸ºJaccardæŒ‡æ•°ï¼Œæˆ‘ä»¬é€šå¸¸ç”¨å…¶æ¥åˆ¤æ–­æ¯ä¸ªæ£€æµ‹çš„æ­£ç¡®æ€§ã€‚PASCAL VOCæ•°æ®é›†ç”¨0.5ä¸ºé˜ˆå€¼æ¥åˆ¤å®šé¢„æµ‹æ¡†æ˜¯True Positiveè¿˜æ˜¯False Positiveï¼ŒCOCOæ•°æ®é›†åˆ™å»ºè®®å¯¹ä¸åŒçš„IoUé˜ˆå€¼è¿›è¡Œè®¡ç®—ã€‚</p><p><img src="/2019/11/28/yoloç³»åˆ—/iou.png" width="20%;"></p></li><li><p>ç½®ä¿¡åº¦ï¼šé€šè¿‡æ”¹å˜ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œæˆ‘ä»¬å¯ä»¥æ”¹å˜ä¸€ä¸ªé¢„æµ‹æ¡†æ˜¯Positiveè¿˜æ˜¯ Negativeã€‚</p></li><li><p>precision &amp; recallï¼šprecision = TP ï¼(TP + FP)ï¼Œrecall = TPï¼(TP + FN)ã€‚å›¾ç‰‡ä¸­æˆ‘ä»¬æ²¡æœ‰é¢„æµ‹åˆ°çš„æ¯ä¸ªéƒ¨åˆ†éƒ½æ˜¯Negativeï¼Œå› æ­¤è®¡ç®—True Negativesæ¯”è¾ƒéš¾åŠã€‚ä½†æ˜¯æˆ‘ä»¬åªéœ€è¦è®¡ç®—False Negativesï¼Œå³æˆ‘ä»¬æ¨¡å‹æ‰€æ¼æ£€çš„ç‰©ä½“ã€‚</p><p>  <img src="/2019/11/28/yoloç³»åˆ—/pr.png" width="30%;"></p></li><li><p>APï¼šä¸åŒçš„ç½®ä¿¡åº¦ä¸‹ä¼šå¾—åˆ°ä¸åŒçš„precision-recallã€‚ä¸ºäº†å¾—åˆ°precision-recallæ›²çº¿ï¼Œé¦–å…ˆå¯¹æ¨¡å‹é¢„æµ‹ç»“æœè¿›è¡Œæ’åºï¼ŒæŒ‰ç…§å„ä¸ªé¢„æµ‹å€¼ç½®ä¿¡åº¦é™åºæ’åˆ—ã€‚ç»™å®šä¸åŒçš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œå°±æœ‰ä¸åŒçš„ranked outputï¼ŒRecallå’ŒPrecisionä»…åœ¨é«˜äºè¯¥rankå€¼çš„é¢„æµ‹ç»“æœä¸­è®¡ç®—ã€‚è¿™é‡Œå…±é€‰æ‹©11ä¸ªä¸åŒçš„recallï¼ˆ[0, 0.1, â€¦, 0.9, 1.0]ï¼‰ï¼Œé‚£ä¹ˆAPå°±å®šä¹‰ä¸ºåœ¨è¿™11ä¸ªrecallä¸‹precisionçš„å¹³å‡å€¼ï¼Œå…¶å¯ä»¥è¡¨å¾æ•´ä¸ªprecision-recallæ›²çº¿ï¼ˆæ›²çº¿ä¸‹é¢ç§¯ï¼‰ã€‚ç»™å®šrecallä¸‹çš„precisionè®¡ç®—ï¼Œæ˜¯é€šè¿‡ä¸€ç§æ’å€¼çš„æ–¹å¼ï¼š</p><script type="math/tex; mode=display">  AP = \frac{1}{11}\sum_{r\in\{0,0.1,...,1.0\}}p_{interp}(r) \\  p_{interp}(r) = max_{\tilde r: \tilde r > r} p(\tilde r)</script></li><li><p>mAPï¼šæ­¤åº¦é‡æŒ‡æ ‡åœ¨<strong>ä¿¡æ¯æ£€ç´¢</strong>å’Œ<strong>ç›®æ ‡æ£€æµ‹</strong>é¢†åŸŸæœ‰ä¸åŒçš„è®¡ç®—æ–¹å¼ã€‚å¯¹äºç›®æ ‡æ£€æµ‹ï¼Œå¯¹äºå„ä¸ªç±»åˆ«ï¼Œåˆ†åˆ«æŒ‰ç…§ä¸Šè¿°æ–¹å¼è®¡ç®—APï¼Œå–æ‰€æœ‰ç±»åˆ«çš„APå¹³å‡å€¼å°±æ˜¯mAPã€‚</p><p>  <img src="/2019/11/28/yoloç³»åˆ—/map.png" width="40%;"></p></li></ul></li><li><p>evalï¼š</p><ol><li>yolo_headè¾“å‡ºï¼šbox_xyæ˜¯boxçš„ä¸­å¿ƒåæ ‡ï¼Œ(0~1)ç›¸å¯¹å€¼ï¼›box_whæ˜¯boxçš„å®½é«˜ï¼Œ(0~1)ç›¸å¯¹å€¼ï¼›box_confidenceæ˜¯æ¡†ä¸­ç‰©ä½“ç½®ä¿¡åº¦ï¼›box_class_probsæ˜¯ç±»åˆ«ç½®ä¿¡åº¦ï¼›</li><li>yolo_correct_boxeså‡½æ•°ï¼šèƒ½å¤Ÿå°†boxä¸­å¿ƒçš„ç›¸å¯¹ä¿¡æ¯è½¬æ¢æˆ[y_min,x_min,y_max,x_max]çš„ç»å¯¹å€¼</li><li>yolo_boxes_and_scoreså‡½æ•°ï¼šè¾“å‡ºç½‘ç»œé¢„æµ‹çš„æ‰€æœ‰box</li><li>yolo_evalå‡½æ•°ï¼šåŸºäºscore_thresholdã€max_boxesä¸¤é¡¹è¿‡æ»¤ï¼Œç±»å†…NMSï¼Œå¾—åˆ°æœ€ç»ˆè¾“å‡º</li></ol></li></ol><h2 id="4-YOLOv4-Optimal-Speed-and-Accuracy-of-Object-Detection"><a href="#4-YOLOv4-Optimal-Speed-and-Accuracy-of-Object-Detection" class="headerlink" title="4. YOLOv4: Optimal Speed and Accuracy of Object Detection"></a>4. YOLOv4: Optimal Speed and Accuracy of Object Detection</h2><ol><li><p>åŠ¨æœº</p><ul><li>Practical testing the tricks of improving CNN</li><li>some features<ul><li>work for certain problems/dataset exclusively</li><li>applicable to the majority of models, tasks, and datasets</li><li>only increase the training cost  [bag-of-freebies]</li><li>only increase the inference cost by a small amount but can significantly improve the accuracy  [bag-of-specials]</li></ul></li><li>Optimal Speed and Accuracy </li></ul></li><li><p>è®ºç‚¹</p><ul><li>headï¼š<ul><li>predict classes and bounding boxes </li><li>one-stage head<ul><li>YOLO, SSD, RetinaNet </li><li>anchor-freeï¼šCenterNet, CornerNet, FCOS  </li></ul></li><li>two-stage head<ul><li>R-CNN series </li><li>anchor-freeï¼šRepPoints </li></ul></li></ul></li><li>neckï¼š<ul><li>collect feature maps from different stages </li><li>FPN, PAN, BiFPN, NAS-FPN </li></ul></li><li><p>backboneï¼š</p><ul><li>pre-trained on ImageNet</li><li>VGG, ResNet, ResNeXt, DenseNet</li></ul><p><img src="/2019/11/28/yoloç³»åˆ—/detector.png" width="70%;"></p></li><li><p>Bag of freebies</p><ul><li>data augmentation <ul><li>pixel-wise adjustments<ul><li>photometric distortionsï¼šbrightness, contrast, hue, saturation, and noise </li><li>geometric distortionsï¼šrandom scaling, cropping, flipping, and rotating </li></ul></li><li>object-wise<ul><li>cutï¼š<ul><li>to imageï¼šCutOut </li><li>to featuremapsï¼šDropOut, DropConnect, DropBlock </li></ul></li><li>addï¼šMixUp, CutMix, GAN </li></ul></li></ul></li><li>data imbalance for classification<ul><li>two-stageï¼šhard example mining </li><li>one-stageï¼šfocal loss, soft label  </li></ul></li><li>bounding box regression <ul><li>MSE-regressionï¼štreat [x,y,w,h] as independent variables</li><li>IoU lossï¼šconsider the integrity &amp; scale invariant </li></ul></li></ul></li><li>Bag of specials <ul><li>enlarging receptive fieldï¼šimproved SPP, ASPP, RFB</li><li>introducing attention mechanism <ul><li>channel-wise attentionï¼šSE, increase the inference time by about 10%</li><li>point-wise attentionï¼šSpatial Attention Module (SAM), does not affect the speed of inference</li></ul></li><li>strengthening feature integration<ul><li>channel-wise levelï¼šSFAM</li><li>point-wise levelï¼šASFF </li><li>scale-wise levelï¼šBiFPN </li></ul></li><li>activation functionï¼šA good activation function can make the gradient more efficiently propagated</li><li>post-processingï¼šå„ç§NMS</li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>choose a backbone â€”- CSPDarknet53 </p><ul><li>Higher input network size (resolution) â€“ for detecting multiple small-sized objects </li><li>More conv layers â€“ for a higher receptive field to cover the increased size of input network </li><li>More parameters â€“ for greater capacity of a model to detect multiple objects of different sizes in a single image </li></ul></li><li><p>add the SPP block over the CSPDarknet53 </p><ul><li>significantly increases the receptive field </li><li>separates out the most significant context features  </li><li>causes almost no re- duction of the network operation speed</li></ul></li><li><p>use PANet as the method of parameter aggregation </p><ul><li>Modified PAN</li><li><p>replace shortcut connection of PAN to concatenation </p><p><img src="/2019/11/28/yoloç³»åˆ—/pan.png" width="30%;"></p></li></ul></li><li><p>use YOLOv3 (anchor based) head </p></li><li><p>Mosaic data augmentation</p><ul><li>mixes 4 training images </li><li>allows detection of objects outside their normal context</li><li><p>reduces the need for a large mini-batch size </p><p><img src="/2019/11/28/yoloç³»åˆ—/mosaic.png" width="40%;"></p></li></ul></li><li><p>Self-Adversarial Training (SAT) data augmentation</p><ul><li>1st stage alters images</li><li>2nd stage train on the modified images</li></ul></li><li><p>CmBNï¼ša CBN modified version </p><p> <img src="/2019/11/28/yoloç³»åˆ—/cmbn.png" width="40%;"></p></li><li><p>modified SAMï¼šfrom spatial-wise attention to point- wise attention</p><ul><li>è¿™é‡Œçš„SAM for ã€ŠAn Empirical Study of Spatial <em>Attention</em> Mechanisms in Deep Networksã€‹ ï¼Œç©ºé—´æ³¨æ„åŠ›æœºåˆ¶</li><li><p>è¿˜æœ‰ä¸€ç¯‡SAMæ˜¯ã€ŠSharpness-Aware Minimization for Efficiently Improving Generalizationã€‹ï¼Œgoogleçš„é”åº¦æ„ŸçŸ¥æœ€å°åŒ–ï¼Œç”¨æ¥æå‡æ¨¡å‹æ³›åŒ–æ€§èƒ½ï¼Œæ³¨æ„åŒºåˆ†</p><p><img src="/2019/11/28/yoloç³»åˆ—/sam.png" width="35%;"></p></li></ul></li></ul></li><li><p>å®éªŒ</p><ul><li><p>Influence of different features on Classifier training </p><ul><li><p>Bluringå’ŒSwishæ²¡æœ‰æå‡</p><p><img src="/2019/11/28/yoloç³»åˆ—/cls.png" width="35%;"></p></li></ul></li><li><p>Influence of different features on Detector training </p><ul><li>IoU threshold, CmBN, Cosine annealing sheduler, CIOUæœ‰æå‡</li></ul></li></ul></li></ol><h2 id="POLY-YOLO-HIGHER-SPEED-MORE-PRECISE-DETECTION-AND-INSTANCE-SEGMENTATION-FOR-YOLOV3"><a href="#POLY-YOLO-HIGHER-SPEED-MORE-PRECISE-DETECTION-AND-INSTANCE-SEGMENTATION-FOR-YOLOV3" class="headerlink" title="POLY-YOLO: HIGHER SPEED, MORE PRECISE DETECTION AND INSTANCE SEGMENTATION FOR YOLOV3"></a>POLY-YOLO: HIGHER SPEED, MORE PRECISE DETECTION AND INSTANCE SEGMENTATION FOR YOLOV3</h2><ol><li><p>åŠ¨æœº</p><ul><li><p>yoloV3â€™s weakness</p><ul><li>rewritten labels </li><li>inefficient distribution of anchors </li></ul></li><li><p>light backboneï¼š</p><ul><li>stairstep upsampling </li></ul></li><li>single scale output </li><li>to extend instance segmentation <ul><li>detect size-independent polygons defined on a polar grid </li><li>real-time processing </li></ul></li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>yolov3</p><ul><li>real-time</li><li>low precision cmp with RetinaNet, EfficientDet <ul><li>low precision of the detection of big boxes </li><li>rewriting of labels by each-other due to the coarse resolution </li></ul></li></ul></li><li><p>this paper solutionï¼š</p><ul><li>è§£å†³yoloç²¾åº¦é—®é¢˜ï¼špropose a brand-new feature decoder with a single ouput tensor that goes to head with higher resolution</li><li>å¤šå°ºåº¦ç‰¹å¾èåˆï¼šutilize stairstep upscaling</li><li>å®ä¾‹åˆ†å‰²ï¼šbounding polygon within a poly grid</li></ul></li><li><p>instance segmentation</p><ul><li>two-stageï¼šmask-rcnn</li><li>one-stageï¼š<ul><li>top-downï¼šsegmenting this object within a bounding box</li><li>bottom-upï¼šstart with clustering pixels</li><li>direct methodsï¼šæ—¢ä¸éœ€è¦bounding boxä¹Ÿä¸éœ€è¦clustered pixelsï¼ŒPolarMask </li></ul></li></ul></li><li><p>cmp with PolarMask </p><ul><li>size-independentï¼šå°ºåº¦ï¼Œå¤§å°ç›®æ ‡éƒ½èƒ½æ£€æµ‹</li><li>dynamic number of verticesï¼šå¤šè¾¹å½¢å®šç‚¹å¯å˜</li></ul></li><li><p>yolov3 issues</p><ul><li><p>rewriting of labelsï¼š</p><ul><li>ä¸¤ä¸ªç›®æ ‡å¦‚æœè½åœ¨åŒä¸€ä¸ªæ ¼å­é‡Œï¼Œåœ¨ä¸€ä¸ªå°ºåº¦ä¸Šground truth labelåªä¼šä¿ç•™ä¸€ä¸ªbox</li><li><p>å¯¹è¶Šå°çš„ç‰¹å¾å›¾ï¼Œgridè¶Šå¤§ï¼Œè¿™ä¸ªé—®é¢˜è¶Šä¸¥é‡</p><p><img src="/2019/11/28/yoloç³»åˆ—/rewriting.png" width="90%;"></p></li></ul></li><li><p>imbalanced distribution of anchors across output scales </p><ul><li>anchorå¦‚æœé€‰çš„ä¸åˆç†ï¼Œä¼šå¯¼è‡´ç‰¹å¾å›¾å°ºåº¦å’Œanchorå°ºåº¦ä¸åŒ¹é…</li><li>most of the boxes will be captured by the middle output layer and the two other layers will be underused</li><li>å¦‚ä¸Šé¢è½¦çš„caseï¼Œå¤§å¤šæ•°è½¦çš„æ¡†å¾ˆå°ï¼Œèšç±»å‡ºçš„ç»™level0å’Œlevel1çš„anchor shapeè¿˜æ˜¯å¾ˆå°ï¼Œä½†æ˜¯level0æ˜¯ç¨€ç–grid<ul><li>ä¸€æ–¹é¢ï¼Œgrid shapeå’Œanchor shapeä¸åŒ¹é…</li><li>ä¸€æ–¹é¢ï¼Œlabel rewritené—®é¢˜ä¼šå‡çº§</li></ul></li><li>åè¿‡æ¥ï¼Œå¦‚æœdense gridä¸Šé¢„æµ‹å¤§ç›®æ ‡ï¼Œä¼šå—åˆ°æ„Ÿå—é‡çš„åˆ¶çº¦</li><li>ä¸€ç§è§£å†³æ–¹æ¡ˆæ˜¯åŸºäºæ„Ÿå—é‡é¦–å…ˆå¯¹gt boxåˆ†æˆä¸‰ç»„ï¼Œç„¶ååˆ†åˆ«èšç±»ï¼Œç„¶å9é€‰1</li></ul></li><li><p>yolov3åŸæ–‡ï¼šYOLOv3 has relatively high $AP_{small}$ performance. However, it has comparatively worse performance on medium and larger size objects. More investigation is needed to get to the bottom of this.</p><ul><li>å°ç›®æ ‡performanceæ›´å¥½ï¼Œå¤§ç›®æ ‡worseï¼Œä¸»è¦æ˜¯å°±æ˜¯å› ä¸ºcoarse gridä¸Šå­˜åœ¨label rewritené—®é¢˜ï¼Œå­˜åœ¨éƒ¨åˆ†gt boxè¢«æŠ‘åˆ¶æ‰äº†ã€‚</li></ul></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>architecture </p><p>  <img src="/2019/11/28/yoloç³»åˆ—/poly-yolo.png" width="80%;"></p><ul><li><p>single output  </p></li><li><p>higher resolutionï¼šstride4</p></li><li><p>handle all the anchors at once </p></li><li><p>cross-scale fusion</p><ul><li>hypercolumn techniqueï¼šadd operation</li><li><p>stairstep interpolationï¼šx2 x2 â€¦</p><p><img src="/2019/11/28/yoloç³»åˆ—/hypercolumn.png" width="80%;"></p></li></ul></li><li><p>SE-blocks</p></li><li>reduced the number of convolutional filters to 75%  in the feature extraction phase </li></ul></li><li><p>bounding polygons </p><ul><li>extend the box tupleï¼š$b_i=\{b_i^{x^1},b_i^{y^1},b_i^{x^2},b_i^{y^2},V_i\}$</li><li>The center of a bounding box is used as the origin </li><li>polygon tupleï¼š$v_{i,j}=\{\alpha_{i,j},\beta_{i,j},\gamma_{i,j}\}$</li><li>polar coordinateï¼šdistance &amp; oriented angleï¼Œç›¸å¯¹è·ç¦»ï¼ˆç›¸å¯¹anchor boxçš„å¯¹è§’çº¿ï¼‰ï¼Œç›¸å¯¹è§’åº¦ï¼ˆnormåˆ°[0,1]ï¼‰</li><li><p>polar cellï¼šä¸€å®šè§’åº¦çš„æ‰‡å½¢åŒºåŸŸ å†…ï¼Œå¦‚æœsectorå†…æ²¡æœ‰å®šç‚¹ï¼Œconf=0</p><p><img src="/2019/11/28/yoloç³»åˆ—/polar.png" width="80%;"></p></li><li><p>general shapeï¼š</p><ul><li>ä¸åŒå°ºåº¦ï¼Œå½¢çŠ¶ç›¸åŒçš„objectï¼Œåœ¨polar coordä¸‹è¡¨ç¤ºæ˜¯ä¸€æ ·çš„</li><li>distance*anchor boxçš„å¯¹è§’çº¿ï¼Œè½¬æ¢æˆç»å¯¹å°ºåº¦</li><li>bounding boxçš„ä¸¤ä¸ªå¯¹è§’é¢„æµ‹ï¼Œè´Ÿè´£å°ºåº¦ä¼°è®¡ï¼Œpolygonåªè´Ÿè´£é¢„æµ‹å½¢çŠ¶</li><li>sharing values should make the learning easier</li></ul></li></ul></li><li><p>mix loss</p><ul><li>outputï¼ša*(4+1+3*n_vmax)</li><li>box center lossï¼šbce</li><li>box wh lossï¼šl2 loss</li><li>conf lossï¼šbce with ignore mask</li><li>cls lossï¼šbce</li><li>polygon lossï¼š$\gamma<em>(log(\frac{\alpha}{anchor^d})-\hat a)^2 + \gamma</em>bce(\beta,\hat{beta})+bce(\gamma, \hat \gamma)$</li><li>auxiliary task learningï¼š<ul><li>ä»»åŠ¡é—´ç›¸äº’boost</li><li>converge faster </li></ul></li></ul></li></ul></li></ol><h2 id="Scaled-YOLOv4-Scaling-Cross-Stage-Partial-Network"><a href="#Scaled-YOLOv4-Scaling-Cross-Stage-Partial-Network" class="headerlink" title="Scaled-YOLOv4: Scaling Cross Stage Partial Network"></a>Scaled-YOLOv4: Scaling Cross Stage Partial Network</h2><ol><li><p>åŠ¨æœº</p><ul><li>model scaling method</li><li>redesign yolov4 and propose yolov4-CSP</li><li>develop scaled yolov4<ul><li>yolov4-tiny</li><li>yolov4-large</li></ul></li><li>æ²¡ä»€ä¹ˆæŠ€æœ¯ç»†èŠ‚ï¼Œå°±æ˜¯ç½‘ç»œç»“æ„å¤§æ›´æ–°</li></ul></li><li><p>è®ºç‚¹</p><ul><li><p>common technique changes depth &amp; width of the backbone</p></li><li><p>recently there are NAS</p></li><li><p>model scaling</p><ul><li><p>input sizeã€widthã€depthå¯¹ç½‘ç»œè®¡ç®—é‡å‘ˆç°square, linear, and square increase</p><p><img src="/2019/11/28/yoloç³»åˆ—/flop.png" width="50%;"></p></li><li><p>æ”¹æˆCSPç‰ˆæœ¬ä»¥åï¼Œèƒ½å¤Ÿå‡å°‘å‚æ•°é‡ã€è®¡ç®—é‡ï¼Œæé«˜accï¼Œç¼©çŸ­inference time</p><p><img src="/2019/11/28/yoloç³»åˆ—/CSPflop.png" width="50%;"></p></li><li><p>æ£€æµ‹çš„å‡†ç¡®æ€§é«˜åº¦ä¾èµ–reception fieldï¼ŒRFéšç€depthçº¿æ€§å¢é•¿ï¼Œéšç€strideå€æ•°å¢é•¿ï¼Œæ‰€ä»¥ä¸€èˆ¬å…ˆç»„åˆè°ƒèŠ‚input sizeå’Œstageï¼Œç„¶åå†æ ¹æ®ç®—åŠ›è°ƒæ•´depthå’Œwidth</p><p>  <img src="/2019/11/28/yoloç³»åˆ—/reception.png" width="50%;"></p></li></ul></li></ul></li><li><p>æ–¹æ³•</p><ul><li><p>backboneï¼šCSPDarknet53 </p></li><li><p>neckï¼šCSP-PANï¼Œå‡å°‘40%è®¡ç®—é‡ï¼ŒSPP</p><p>  <img src="/2019/11/28/yoloç³»åˆ—/CSPpan.png" width="50%;"></p></li><li><p>yoloV4-tiny</p><p>  <img src="/2019/11/28/yoloç³»åˆ—/tiny.png" width="40%;"></p></li><li><p>yoloV4-largeï¼šP456</p><p>  <img src="/2019/11/28/yoloç³»åˆ—/large.png" width="80%;"></p></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ç›®æ ‡æ£€æµ‹ï¼Œone-stage </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>triplet-center-lossè®ºæ–‡</title>
      <link href="/2019/11/13/triplet-center-loss%E8%AE%BA%E6%96%87/"/>
      <url>/2019/11/13/triplet-center-loss%E8%AE%BA%E6%96%87/</url>
      <content type="html"><![CDATA[<h2 id="0-before-reading"><a href="#0-before-reading" class="headerlink" title="0. before reading"></a>0. before reading</h2><p>ç»“åˆï¼š</p><ul><li><p>triplet lossï¼šè€ƒè™‘ç±»é—´å…³ç³»ï¼Œä½†è®¡ç®—å¤æ‚åº¦é«˜ï¼Œå›°éš¾æ ·æœ¬éš¾æŒ–æ˜</p></li><li><p>center lossï¼šè€ƒè™‘ç±»å†…å…³ç³»</p></li><li><p>TCLï¼šåŒæ—¶å¢åŠ ç±»å†…æ•°æ®çš„ç´§å®åº¦ï¼ˆcompactnessï¼‰å’Œç±»é—´çš„åˆ†ç¦»åº¦ï¼ˆseparabilityï¼‰</p><p>  ä¸‰å…ƒç»„åªè€ƒè™‘æ ·æœ¬ã€æ‰€å±ç±»ä¸­å¿ƒã€æœ€è¿‘é‚»ç±»çš„ä¸­å¿ƒã€‚é¿å…äº†å»ºç«‹tripletsçš„å¤æ‚åº¦å’Œmining hard samplesçš„éš¾åº¦ã€‚</p></li></ul><h2 id="titleï¼šTriplet-Center-Loss-for-Multi-View-3D-Object-Retrieval"><a href="#titleï¼šTriplet-Center-Loss-for-Multi-View-3D-Object-Retrieval" class="headerlink" title="titleï¼šTriplet-Center Loss for Multi-View 3D Object Retrieval"></a>titleï¼šTriplet-Center Loss for Multi-View 3D Object Retrieval</h2><ol><li><p>åŠ¨æœºï¼š<em>deep metric learning</em> </p><ul><li><p>the learned features using softmax loss are not discriminative enough in nature </p></li><li><p>although samples of the two classes are separated by the decision boundary elaborately, there exists significant intra-class variations</p></li><li><p><strong>QUESTION1</strong>ï¼šso what? how does this affect the current task? åŠ¨æœºæè¿°ä¸å……åˆ†ã€‚</p></li><li><p><strong>QUESTION2</strong>ï¼šåœ¨äºŒç»´å¹³é¢ä¸Šoverlapä¸ä»£è¡¨åœ¨é«˜ç»´ç©ºé—´ä¸­overlapï¼Œè¿™ç§illustrationç©¶ç«Ÿæ˜¯å¦æœ‰æ„ä¹‰ã€‚</p></li><li><p><strong>ANSWER for above</strong>ï¼šé«˜ç»´ç©ºé—´å¯åˆ†ï¼ŒæŠ•å½±åˆ°äºŒç»´å¹³é¢ä¸ä¸€å®šå¯åˆ†ï¼Œä½†æ˜¯åè¿‡æ¥ï¼ŒäºŒç»´å¹³é¢ä¸Šé«˜åº¦å¯åˆ†ï¼Œæ˜ å°„ä¼šé«˜ç»´ç©ºé—´æ•°æ®ä»æ—§æ˜¯é«˜åº¦å¯åˆ†çš„ã€‚åªèƒ½è¯´ï¼Œåè€…èƒ½å¤Ÿç¡®ä¿ä¸åŒç±»åˆ«æ•°æ®ç¦»æ•£æ€§æ›´å¥½ï¼Œä¸èƒ½è¯´æ˜å‰è€…æ•°æ®ç¦»æ•£æ€§ä¸å¥½ï¼ˆå¦‚æœå®šä¹‰äº†é«˜ç»´è·ç¦»ï¼Œä¹Ÿå¯ä»¥è¯´æ˜ï¼‰ã€‚</p><p> <img src="/2019/11/13/triplet-center-lossè®ºæ–‡/distance.png" width="80%"></p></li></ul></li><li><p>åº”ç”¨åœºæ™¯ï¼š<em>3D object retrieval</em> </p></li><li><p>è¦ç´ ï¼š</p><ul><li>learns a center for each class </li><li>requires that the distances between samples and centers from the same class are smaller than those from different classes, in this way the samples are pulled closer to the corresponding center and meanwhile pushed away from the  different centers</li><li>both the inter-class separability  and the intra-class variations are considered</li></ul></li><li><p>è®ºç‚¹ï¼š</p><ul><li><p>Compared with triplet loss, TCL avoids the complex construction of triplets and hard sample mining mechanism. </p></li><li><p>Compared with center loss, TCL not only considers to reduce the intra-class variations.</p></li><li><p><strong>QUESTION</strong>ï¼šwhat about the comparison with [softmax loss + center loss]?</p></li><li><p><strong>ANSWER for above</strong>ï¼šcenter-loss is actually representing for the joint loss [softmax loss + center loss]. </p><p>  â€˜â€™Since the class centers are updated at each iteration based on a mini-batch instead of the whole dataset, which can be very unstable, it has to be under the joint supervision of softmax loss during training. â€˜â€™</p></li></ul></li><li><p>æœ¬æ–‡åšæ³•ï¼š</p><ul><li>the proposed TCL is used as the supervision loss</li><li>the softmax loss could be also combined in as an addition</li></ul></li><li><p>ç»†èŠ‚ï¼š</p><ul><li><p>TCLï¼š</p><script type="math/tex; mode=display">  L_{tc} = \sum_{i=1}^Mmax(D(f_i, c_{y^i}) + m - min_{j\neq y^i}D(f_i, c_j), 0)</script><p>  å‰åŠéƒ¨åˆ†æ˜¯center-lossï¼Œç±»å†…æ¬§å‡ é‡Œå¾—è·ç¦»ï¼ŒååŠéƒ¨åˆ†æ˜¯æ¯ä¸ªæ ·æœ¬å’Œä¸å…¶æœ€è¿‘çš„negative centerä¹‹é—´çš„è·ç¦»ã€‚</p></li><li><p>â€˜Unlike center loss, TCL can be used independently from softmax loss. <strong>However</strong>â€¦ â€˜</p><p>  ä½œè€…è§£é‡Šè¯´ï¼Œå› ä¸ºcenter layeræ˜¯éšæœºåˆå§‹åŒ–å‡ºæ¥çš„ï¼Œè€Œä¸”æ˜¯batch updatingï¼Œå› æ­¤å¼€å§‹é˜¶æ®µä¼šæ¯”è¾ƒtrickyï¼Œâ€™while softmax loss could serve as a good guider for seeking better class centers â€˜</p></li><li><p>è°ƒå‚ä¸­æåˆ°â€™m is fixed to 5â€™ï¼Œè¯´æ˜æœ¬æ–‡å¯¹feature vectoræ²¡æœ‰åšnormalizationï¼ˆç›¸æ¯”ä¹‹ä¸‹facenetåšäº†å½’ä¸€åŒ–ï¼Œé™å®šæ‰€æœ‰embeddingåˆ†å¸ƒåœ¨é«˜ç»´çƒé¢ä¸Šï¼‰ã€‚</p></li><li><p>è¡¡é‡æŒ‡æ ‡ï¼šAUCå’ŒMAPï¼Œè¿™æ˜¯ä¸€ä¸ªretrievalä»»åŠ¡ï¼Œæœ€ç»ˆéœ€è¦çš„æ˜¯embeddingï¼Œç»™å®šQueryï¼Œå¬å›top matchesã€‚</p></li></ul></li><li><p>reviewsï¼š</p><ul><li>ä¸ªäººç†è§£ï¼š<ol><li>softmaxåˆ†ç±»å™¨æ—¨åœ¨æ•°æ®å¯åˆ†ï¼Œå¯¹äºåˆ†ç±»è¾¹ç•Œã€feature vectorçš„ç©ºé—´æ„ä¹‰ä¸å­˜åœ¨ä¸€ä¸ªå…·è±¡çš„æè¿°ã€‚deep metric learningèƒ½å¤Ÿå¼•å…¥è¿™ç§å…·è±¡çš„ã€å›¾åƒå­¦çš„æ„ä¹‰ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šï¼Œæ¢è®¨distanceã€centeræ‰æœ‰æ„ä¹‰ã€‚</li><li>å°±å°é—­ç±»æ•°æ®ï¼ˆç±»åˆ«æœ‰é™ä¸”å·²çŸ¥ï¼‰åˆ†ç±»æ¥è®²ï¼Œåˆ†ç±»è¾¹ç•Œæœ‰æ— å›¾åƒå­¦æè¿°å…¶å®æ„ä¹‰ä¸å¤§ã€‚å·²çŸ¥çš„æ•°æ®åˆ†å¸ƒå°½å¯èƒ½discriminativeçš„ä¸»è¦æ„ä¹‰æ˜¯é’ˆå¯¹æœªçŸ¥ç±»åˆ«ï¼Œæˆ‘ä»¬å¸Œæœ›ç»™åˆ°æ¨¡å‹ä¸€ä¸ª<strong>æœªçŸ¥æ•°æ®</strong>æ—¶ï¼Œå®ƒèƒ½å¤Ÿæ£€æµ‹å‡ºæ¥ï¼Œè€Œä¸æ˜¯åˆ’å…¥æŸä¸ªå·²çŸ¥ç±»ï¼ˆsoftmaxï¼‰ã€‚</li><li>TCLçš„æœ€å¤§è´¡çŒ®åº”è¯¥æ˜¯æƒ³åˆ°ç”¨centeræ›¿ä»£æ ·æœ¬æ¥è¿›è¡Œmetric judgementï¼Œæ”¹å–„triplet-losså¤æ‚è®¡ç®—é‡è¿™ä¸€é—®é¢˜ï¼Œåè€…å®é™…è®­èµ·æ¥å¤ªéš¾äº†ï¼Œæ²¡æœ‰æ„Ÿæƒ…çš„GPUåå™¬æœºå™¨ã€‚</li></ol></li><li>XXXï¼š</li><li></li></ul></li></ol><p>èƒ½å¤Ÿå¼•å…¥è¿™ç§å…·è±¡çš„ã€å›¾åƒå­¦çš„æ„ä¹‰ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æ¢è®¨distanceã€centeræ‰æœ‰æ„ä¹‰ã€‚</p><p>â€‹        </p>]]></content>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>dicomReader</title>
      <link href="/2019/11/11/dicomReader/"/>
      <url>/2019/11/11/dicomReader/</url>
      <content type="html"><![CDATA[<ol><li><p>read a dcm file</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> SimpleITK <span class="keyword">as</span> sitk</span><br><span class="line"></span><br><span class="line">image = sitk.ReadImage(dcm_file)</span><br><span class="line">image_arr = sitk.GetArrayFromImage(image)</span><br></pre></td></tr></table></figure></li><li><p>read a dcm series</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">series_IDs = sitk.ImageSeriesReader.GetGDCMSeriesIDs(series_path)</span><br><span class="line"></span><br><span class="line">nb_series = len(series_IDs)</span><br><span class="line">print(nb_series)</span><br><span class="line"></span><br><span class="line"><span class="comment"># é»˜è®¤è·å–ç¬¬ä¸€ä¸ªåºåˆ—çš„æ‰€æœ‰åˆ‡ç‰‡è·¯å¾„</span></span><br><span class="line">dicom_names = sitk.ImageSeriesReader.GetGDCMSeriesFileNames(file_path)</span><br><span class="line">series_reader = sitk.ImageSeriesReader()</span><br><span class="line">series_reader.SetFileNames(dicom_names)</span><br><span class="line">image3D = series_reader.Execute()</span><br></pre></td></tr></table></figure></li><li><p>read a dcm case</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">series_IDs = sitk.ImageSeriesReader.GetGDCMSeriesIDs(case_path)</span><br><span class="line"><span class="keyword">for</span> series_id <span class="keyword">in</span> series_IDs:</span><br><span class="line">    dicom_names = sitk.ImageSeriesReader.GetGDCMSeriesFileNames(case_path, series_id)</span><br><span class="line">    series_reader = sitk.ImageSeriesReader()</span><br><span class="line">    series_reader.SetFileNames(dicom_names)</span><br><span class="line">    image3D = series_reader.Execute()</span><br></pre></td></tr></table></figure></li><li><p>read tag</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># é¦–å…ˆå¾—åˆ°imageå¯¹è±¡</span></span><br><span class="line">Image_type = image.GetMetaData(<span class="string">"0008|0008"</span>) <span class="keyword">if</span> image.HasMetaData(<span class="string">"0008|0008"</span>) <span class="keyword">else</span> <span class="string">'Nan'</span></span><br></pre></td></tr></table></figure></li><li><p>å‘ç°ä¸€ç§åºåˆ—ï¼Œæ¯å¼ å›¾çš„å°ºå¯¸ä¸åŒï¼Œè¿™æ ·æ‰§è¡Œseries_readerçš„æ—¶å€™ä¼šæŠ¥é”™ï¼Œå› ä¸ºseries_readerä¼šä¾ç…§ç¬¬ä¸€å±‚çš„å›¾åƒå°ºå¯¸ç”³è¯·ç©ºé—´ï¼Œæ‰€ä»¥è¦ä¹ˆå¼‚å¸¸è¦ä¹ˆé€å¼ è¯»ã€‚</p><p> reference: <a href="http://itk-users.7.n7.nabble.com/ITK-users-Reader-InvalidRequestedRegionError-td38608.html" target="_blank" rel="noopener">http://itk-users.7.n7.nabble.com/ITK-users-Reader-InvalidRequestedRegionError-td38608.html</a></p></li></ol>]]></content>
      
      
        <tags>
            
            <tag> SimpleITK </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>c++ tricks in engineering</title>
      <link href="/2019/11/06/c-tricks-in-engineering/"/>
      <url>/2019/11/06/c-tricks-in-engineering/</url>
      <content type="html"><![CDATA[<ol><li><p>æ•°ç»„ä¼ å‚</p><p> å·¥ç¨‹åŒ–è¢«å‘äº†å¥½å¤šå›ï¼</p><p> C/C++ ä¼ é€’æ•°ç»„ï¼Œè™½ç„¶ä¼ é€’çš„æ˜¯é¦–åœ°å€åœ°å€ï¼Œä½†æ˜¯<strong>å‚æ•°åˆ°äº†å‡½æ•°å†…å°±æˆäº†æ™®é€šæŒ‡é’ˆ</strong>ã€‚</p><p> æ‰€ä»¥è¯•å›¾åœ¨è°ƒç”¨å‡½æ•°ä¸­æ±‚å–æ‰€ä¼ é€’æ•°ç»„çš„é•¿åº¦æ˜¯è¡Œä¸é€šçš„ã€‚</p></li><li><p>vectorä¼ å‚</p><p> ä¼ å€¼â€”&gt;æ‹·è´æ„é€ ï¼Œä¼ å¼•ç”¨ï¼æŒ‡é’ˆâ€”&gt;ä¸å‘ç”Ÿæ‹·è´æ„é€ ã€‚</p><p> å®é™…å·¥ç¨‹åŒ–ä¸­é‡åˆ°çš„é—®é¢˜æ˜¯ï¼Œæ„å»ºäº†ä¸€ä¸ªvector\<cv::mat\> imgså¯¹è±¡ï¼Œä¼ å…¥å‡½æ•°ä»¥åï¼Œåœ¨å‡½æ•°å†…éƒ¨åˆ›å»ºç©ºé—´cv::Mat imgï¼Œç„¶åå°†img pushè¿›vectorã€‚åœ¨å‡½æ•°å¤–è¯»å–è¯¥vectorçš„æ—¶å€™å‘ç°å…¶å†…éƒ¨æ²¡å€¼ã€‚</cv::mat\></p><p> <strong>è¦ç‚¹ï¼š1. è¦ä¼ å¼•ç”¨ï¼Œ2. push cloneï¼šimgs.push_back(img)</strong></p><p> å¦å¤–ï¼Œvectorå¯ä»¥ä½œä¸ºå‡½æ•°è¿”å›å€¼ã€‚</p></li><li></li></ol>]]></content>
      
      
        <tags>
            
            <tag> c++ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>å›¾åƒç®—æ³•ç»¼è¿°</title>
      <link href="/2019/10/31/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0/"/>
      <url>/2019/10/31/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0/</url>
      <content type="html"><![CDATA[<ol><li><p>ç±»åˆ«</p><ul><li>æŒ‰ç…§ä»»åŠ¡ç±»å‹ï¼šåº¦é‡å­¦ä¹ ï¼ˆmetric learningï¼‰å’Œæè¿°å­å­¦ä¹ ï¼ˆimage descriptor learningï¼‰</li><li>æŒ‰ç…§ç½‘ç»œç»“æ„ï¼špairwiseçš„siameseç»“æ„ã€tripletçš„three branchç»“æ„ã€ä»¥åŠå¼•å…¥å°ºåº¦ä¿¡æ¯çš„central-surroundç»“æ„</li><li>æŒ‰ç…§ç½‘ç»œè¾“å‡ºï¼šç‰¹å¾å‘é‡ï¼ˆfeature embeddingï¼‰å’Œå•ä¸ªæ¦‚ç‡å€¼ï¼ˆpairwise similarityï¼‰</li><li>æŒ‰ç…§æŸå¤±å‡½æ•°ï¼šå¯¹æ¯”æŸå¤±å‡½æ•°ã€äº¤å‰ç†µæŸå¤±å‡½æ•°ã€triplet lossã€hinge lossç­‰ï¼Œæ­¤å¤–æŸå¤±å‡½æ•°å¯ä»¥å¸¦æœ‰éšå¼çš„<strong>å›°éš¾æ ·æœ¬æŒ–æ˜</strong>ï¼Œä¾‹å¦‚pn-netä¸­çš„softpnç­‰ï¼Œä¹Ÿå¯ä»¥æ˜¯æ˜¾ç¤ºçš„å›°éš¾æŒ–æ˜ã€‚</li></ul></li><li><p>Plainç½‘ç»œ</p><p> ä¸»è¦æ˜¯è¯´AlexNetï¼VGG-Netï¼Œåè€…æ›´å¸¸ç”¨ä¸€äº›ã€‚</p><p> Plainç½‘ç»œçš„è®¾è®¡ä¸»è¦éµå¾ªä»¥ä¸‹å‡ ä¸ªå‡†åˆ™ï¼š</p><p> ï¼ˆ1ï¼‰è¾“å‡ºç‰¹å¾å›¾å°ºå¯¸ç›¸åŒçš„å±‚ä½¿ç”¨ç›¸åŒæ•°é‡çš„æ»¤æ³¢å™¨ã€‚</p><p> ï¼ˆ2ï¼‰å¦‚æœç‰¹å¾å›¾å°ºå¯¸å‡åŠï¼Œé‚£ä¹ˆæ»¤æ³¢å™¨æ•°é‡å°±åŠ å€ï¼Œä»è€Œä¿è¯æ¯å±‚çš„æ—¶é—´å¤æ‚åº¦ç›¸åŒï¼ˆè¿™æ˜¯ä¸ºå•¥ï¼Ÿï¼Ÿï¼‰ã€‚</p></li><li><p>åè¯</p><ul><li><p>æ„Ÿå—é‡ï¼šå·ç§¯ç¥ç»ç½‘ç»œæ¯ä¸€å±‚è¾“å‡ºçš„ç‰¹å¾å›¾ä¸Šçš„åƒç´ ç‚¹åœ¨<strong>åŸå§‹å›¾åƒ</strong>ä¸Šæ˜ å°„åŒºåŸŸçš„å¤§å°ã€‚é€šä¿—çš„è¯´ï¼Œå°±æ˜¯è¾“å…¥å›¾åƒå¯¹è¿™ä¸€å±‚è¾“å‡ºçš„ç¥ç»å…ƒçš„å½±å“æœ‰å¤šå¤§ã€‚</p><p>  æ„Ÿå—é‡è®¡ç®—ï¼šç”±å½“å‰å±‚å‘å‰æ¨ï¼Œéœ€è¦çš„å‚æ•°æ˜¯kernel sizeå’Œstrideã€‚</p><script type="math/tex; mode=display">  N\_RF = kernel\_size + (cur\_RF-1)*stride</script><p>  å…¶ä¸­$cur_RF$æ˜¯å½“å‰å±‚ï¼ˆstart from 1ï¼‰ï¼Œ$N_RF$ã€$kernel_size$ã€$stride$æ˜¯ä¸Šä¸€å±‚å‚æ•°ã€‚</p></li><li><p>æœ‰æ•ˆæ„Ÿå—é‡ï¼šå¹¶ä¸æ˜¯æ„Ÿå—é‡å†…æ‰€æœ‰åƒç´ å¯¹è¾“å‡ºå‘é‡çš„è´¡çŒ®ç›¸åŒï¼Œåœ¨å¾ˆå¤šæƒ…å†µä¸‹æ„Ÿå—é‡åŒºåŸŸå†…åƒç´ çš„å½±å“åˆ†å¸ƒæ˜¯é«˜æ–¯ï¼Œæœ‰æ•ˆæ„Ÿå—é‡ä»…å ç†è®ºæ„Ÿå—é‡çš„ä¸€éƒ¨åˆ†ï¼Œä¸”é«˜æ–¯åˆ†å¸ƒä»ä¸­å¿ƒåˆ°è¾¹ç¼˜å¿«é€Ÿè¡°å‡ã€‚</p></li><li><p>æ„Ÿå—é‡å¤§å°ï¼š</p><ul><li>å°æ„Ÿå—é‡ï¼šlocalï¼Œä½ç½®ä¿¡æ¯æ›´å‡†ç¡®</li></ul></li><li>å¤§æ„Ÿå—é‡ï¼šglobalï¼Œè¯­ä¹‰ä¿¡æ¯æ›´ä¸°å¯Œ</li></ul></li></ol><ul><li><p>inception moduleï¼šä¸‹å›¾ä¸ºå…¶ä¸­ä¸€ç§ã€‚</p><p>æ„ä¹‰ï¼šå¢åŠ ç½‘ç»œæ·±åº¦å’Œå®½åº¦çš„åŒæ—¶ï¼Œå‡å°‘å‚æ•°ã€‚ç»“æ„ä¸­åµŒå…¥äº†å¤šå°ºåº¦ä¿¡æ¯ï¼Œé›†æˆäº†å¤šç§ä¸åŒæ„Ÿå—é‡ä¸Šçš„ç‰¹å¾ã€‚</p><p><img src="/2019/10/31/å›¾åƒç®—æ³•ç»¼è¿°/inception.png" width="50%;"></p></li><li><p>building blockï¼šå·¦è¾¹è¿™ç§ï¼Œçº¢è‰²æ¡†æ¡†é‡Œé¢æ˜¯ä¸€ä¸ªblockã€‚</p><p>å‡ ä¸ªç›¸åŒçš„building blockå †å ä¸ºä¸€å±‚convã€‚åœ¨ç¬¬ä¸€ä¸ªbuilding Blockå—ä¸­ï¼Œè¾“å‡ºç‰¹å¾å›¾çš„å°ºå¯¸ä¸‹é™ä¸€åŠï¼ˆç¬¬ä¸€ä¸ªå·ç§¯stride=2ï¼‰ï¼Œå‰©ä½™çš„building Blockå—è¾“å…¥è¾“å‡ºå°ºå¯¸æ˜¯ä¸€æ ·çš„ã€‚</p></li><li><p>bottleneckï¼šå³è¾¹è¿™ç§ï¼Œè“è‰²æ¡†æ¡†blockã€‚å­—é¢æ„æ€ï¼Œç“¶é¢ˆï¼Œå½¢å®¹è¾“å…¥è¾“å‡ºç»´åº¦å·®è·è¾ƒå¤§ã€‚</p><p>ç¬¬ä¸€ä¸ª1*1è´Ÿè´£é™ä½ç»´åº¦ï¼Œç¬¬äºŒä¸ª1*1è´Ÿè´£æ¢å¤ç»´åº¦ï¼Œ3*3å±‚å°±å¤„åœ¨ä¸€ä¸ªè¾“å…¥ï¼è¾“å‡ºç»´åº¦è¾ƒå°çš„ç“¶é¢ˆã€‚</p><pre><code>å·¦å³ä¸¤ç§ç»“æ„æ—¶é—´å¤æ‚åº¦ç›¸ä¼¼ã€‚&lt;img src=&quot;å›¾åƒç®—æ³•ç»¼è¿°/block.png&quot; width=&quot;30%;&quot; /&gt;&lt;img src=&quot;å›¾åƒç®—æ³•ç»¼è¿°/ImageNet.png&quot; width=&quot;110%;&quot; /&gt;</code></pre><ul><li><p>top-1å’Œtop-5ï¼štop-1å°±æ˜¯é¢„æµ‹æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«ï¼Œtop-5åˆ™å–æœ€åé¢„æµ‹æ¦‚ç‡çš„å‰äº”ä¸ªï¼Œåªè¦å…¶ä¸­åŒ…å«æ­£ç¡®ç±»åˆ«åˆ™è®¤ä¸ºé¢„æµ‹æ­£ç¡®ã€‚</p><p>ä½¿ç”¨top-5ä¸»è¦æ˜¯å› ä¸ºImageNetä¸­å¾ˆå¤šå›¾ç‰‡ä¸­å…¶å®æ˜¯åŒ…å«å¤šä¸ªç‰©ä½“çš„ã€‚    </p></li><li><p>accuracyã€error rateã€F1-scoreã€sensitivityã€specificityã€precisionã€recall</p><ul><li>accuracyï¼šæ€»ä½“å‡†ç¡®ç‡</li><li>precisionï¼šä»ç»“æœè§’åº¦ï¼Œå•ä¸€ç±»åˆ«å‡†ç¡®ç‡</li><li>recallï¼šä»è¾“å…¥è§’åº¦ï¼Œé¢„æµ‹ç±»åˆ«çœŸå®ä¸º1çš„å‡†ç¡®ç‡</li><li>P-Ræ›²çº¿ï¼šé€‰ç”¨ä¸åŒé˜ˆå€¼ï¼Œprecision-recallå›´æˆçš„æ›²çº¿</li><li>APï¼šå¹³å‡ç²¾åº¦ï¼ŒP-Ræ›²çº¿å›´ä½çš„é¢ç§¯</li><li>F1-scoreï¼šå¯¹äºæŸä¸ªåˆ†ç±»ï¼Œç»¼åˆäº†Precisionå’ŒRecallçš„ä¸€ä¸ªåˆ¤æ–­æŒ‡æ ‡ï¼Œå› ä¸ºé€‰ç”¨ä¸åŒé˜ˆå€¼ï¼Œprecision-recallä¼šéšä¹‹å˜åŒ–ï¼ŒF1-scoreç”¨äºé€‰å‡ºæœ€ä½³é˜ˆå€¼ã€‚</li><li>sensitivityï¼š=recall</li><li>specificityï¼šé¢„æµ‹ç±»åˆ«çœŸå®ä¸º0çš„å‡†ç¡®ç‡</li></ul><p>referenceï¼š<a href="https://zhuanlan.zhihu.com/p/33273532" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/33273532</a></p></li><li><p>trade-offï¼š</p></li><li><p>FLOPSï¼šæ¯ç§’æµ®ç‚¹è¿ç®—æ¬¡æ•°æ˜¯æ¯ç§’æ‰€æ‰§è¡Œçš„æµ®ç‚¹è¿ç®—æ¬¡æ•°çš„ç®€ç§°ï¼Œè¢«ç”¨æ¥ä¼°ç®—ç”µè„‘æ•ˆèƒ½ã€‚</p></li><li><p>ROCã€AUCã€MAPï¼š</p><ul><li>ROCï¼šTPRå’ŒFPRå›´æˆçš„æ›²çº¿</li><li>AUCï¼šROCå›´ä½çš„é¢ç§¯</li><li>mAPï¼šæ‰€æœ‰ç±»åˆ«APçš„å¹³å‡å€¼</li></ul></li><li><p>æ¢¯åº¦å¼¥æ•£ï¼š</p></li><li><p>â€œåº•å±‚å…ˆæ”¶æ•›ã€é«˜å±‚å†æ”¶æ•›â€ï¼š</p></li><li><p>ç‰¹å¾å›¾ï¼šå·ç§¯å±‚é€šè¿‡çº¿æ€§æ»¤æ³¢å™¨è¿›è¡Œçº¿æ€§å·ç§¯è¿ç®—ï¼Œç„¶åå†æ¥ä¸ªéçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œæœ€ç»ˆç”Ÿæˆç‰¹å¾å›¾ã€‚</p></li><li><p>TTA test time augmentationï¼šæµ‹è¯•æ—¶å¢å¼ºï¼Œä¸ºåŸå§‹å›¾åƒé€ å‡ºå¤šä¸ªä¸åŒç‰ˆæœ¬ï¼ŒåŒ…æ‹¬ä¸åŒåŒºåŸŸè£å‰ªå’Œæ›´æ”¹ç¼©æ”¾ç¨‹åº¦ç­‰ï¼Œå¹¶å°†å®ƒä»¬è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼›ç„¶åå¯¹å¤šä¸ªç‰ˆæœ¬è¿›è¡Œè®¡ç®—å¾—åˆ°å¹³å‡è¾“å‡ºï¼Œä½œä¸ºå›¾åƒçš„æœ€ç»ˆè¾“å‡ºåˆ†æ•°ã€‚</p></li><li><p>pooling mode: </p><ul><li>full modeï¼šä»filterå’Œimageåˆšå¼€å§‹ç›¸äº¤å¼€å§‹å·ç§¯</li><li>same modeï¼šå½“filterçš„ä¸­å¿ƒå’Œimageçš„è§’é‡åˆæ—¶å¼€å§‹å·ç§¯ï¼Œå¦‚æœstride=1ï¼Œé‚£ä¹ˆè¾“å…¥è¾“å‡ºå°ºå¯¸ç›¸åŒ</li><li>valid modeï¼šå½“filterå®Œå…¨åœ¨imageé‡Œé¢æ—¶å¼€å§‹å·ç§¯</li></ul><p><img src="/2019/10/31/å›¾åƒç®—æ³•ç»¼è¿°/full.png" width="20%;"> <img src="/2019/10/31/å›¾åƒç®—æ³•ç»¼è¿°/same.png" width="20%;"> <img src="/2019/10/31/å›¾åƒç®—æ³•ç»¼è¿°/valid.png" width="20%;"></p></li><li><p><a href="https://zhangting2020.github.io/2018/05/30/Transform-Invariance/" target="_blank" rel="noopener">ç©ºé—´ä¸å˜æ€§</a>ï¼š</p><ul><li>å¹³ç§»ä¸å˜æ€§ï¼šä¸ç®¡è¾“å…¥å¦‚ä½•å¹³ç§»ï¼Œç³»ç»Ÿäº§ç”Ÿå®Œå…¨ç›¸åŒçš„å“åº”ï¼Œ<strong>æ¯”å¦‚å›¾åƒåˆ†ç±»ä»»åŠ¡</strong>ï¼Œå›¾åƒä¸­çš„ç›®æ ‡ä¸ç®¡è¢«ç§»åŠ¨åˆ°å›¾ç‰‡çš„å“ªä¸ªä½ç½®ï¼Œå¾—åˆ°çš„ç»“æœï¼ˆæ ‡ç­¾ï¼‰åº”è¯¥æ˜¯ç›¸åŒçš„</li><li><strong>å¹³ç§»åŒå˜æ€§ï¼ˆtranslation equivarianceï¼‰ï¼š</strong>ç³»ç»Ÿåœ¨ä¸åŒä½ç½®çš„å·¥ä½œåŸç†ç›¸åŒï¼Œä½†å®ƒçš„å“åº”éšç€ç›®æ ‡ä½ç½®çš„å˜åŒ–è€Œå˜åŒ–ï¼Œ<strong>æ¯”å¦‚å®ä¾‹åˆ†å‰²ä»»åŠ¡</strong>ï¼Œç›®æ ‡å¦‚æœè¢«å¹³ç§»äº†ï¼Œé‚£ä¹ˆè¾“å‡ºçš„å®ä¾‹æ©ç ä¹Ÿç›¸åº”å˜åŒ–</li><li>å±€éƒ¨è¿æ¥ï¼šæ¯ä¸ªç¥ç»å…ƒæ²¡æœ‰å¿…è¦å¯¹å…¨å±€å›¾åƒè¿›è¡Œæ„ŸçŸ¥ï¼Œåªéœ€è¦å¯¹å±€éƒ¨è¿›è¡Œæ„ŸçŸ¥ï¼Œç„¶ååœ¨æ›´é«˜å±‚å°†å±€éƒ¨çš„ä¿¡æ¯ç»¼åˆèµ·æ¥å°±å¾—åˆ°äº†å…¨å±€çš„ä¿¡æ¯</li><li>æƒå€¼å…±äº«ï¼šå¯¹äºè¿™ä¸ªå›¾åƒä¸Šçš„æ‰€æœ‰ä½ç½®ï¼Œæˆ‘ä»¬éƒ½èƒ½ä½¿ç”¨åŒæ ·çš„å­¦ä¹ ç‰¹å¾</li><li>æ± åŒ–ï¼š<a href="https://www.zhihu.com/question/36980971" target="_blank" rel="noopener">é€šè¿‡æ¶ˆé™¤éæå¤§å€¼ï¼Œé™ä½äº†ä¸Šå±‚çš„è®¡ç®—å¤æ‚åº¦</a>ã€‚æœ€å¤§æ± åŒ–è¿”å›æ„Ÿå—é‡ä¸­çš„æœ€å¤§å€¼ï¼Œå¦‚æœæœ€å¤§å€¼è¢«ç§»åŠ¨äº†ï¼Œä½†æ˜¯ä»ç„¶åœ¨è¿™ä¸ªæ„Ÿå—é‡ä¸­ï¼Œé‚£ä¹ˆæ± åŒ–å±‚ä¹Ÿä»ç„¶ä¼šè¾“å‡ºç›¸åŒçš„æœ€å¤§å€¼ã€‚</li><li>å·ç§¯å’Œæ± åŒ–è¿™ä¸¤ç§æ“ä½œ<strong>å…±åŒ</strong>æä¾›äº†<strong>ä¸€äº›</strong>å¹³ç§»ä¸å˜æ€§ï¼Œå³ä½¿å›¾åƒè¢«å¹³ç§»ï¼Œå·ç§¯ä¿è¯ä»ç„¶èƒ½æ£€æµ‹åˆ°å®ƒçš„ç‰¹å¾ï¼Œæ± åŒ–åˆ™å°½å¯èƒ½åœ°ä¿æŒä¸€è‡´çš„è¡¨è¾¾ã€‚</li><li>åŒç†ï¼Œæ‰€è°“çš„CNNçš„å°ºåº¦ã€æ—‹è½¬ä¸å˜æ€§ï¼Œä¹Ÿæ˜¯ç”±äºpoolingæ“ä½œï¼Œå¼•å…¥çš„å¾®å°å½¢å˜çš„é²æ£’æ€§ã€‚</li></ul></li><li><p>æ¨¡å‹å¤§å°ä¸å‚æ•°é‡ï¼šfloat32æ˜¯4ä¸ªå­—èŠ‚ï¼Œå› æ­¤æ¨¡å‹å¤§å°å­—èŠ‚æ•°=å‚æ•°é‡Ã—4</p></li></ul></li></ul><ol><li><p>è®­ç»ƒæŠ€å·§</p><ul><li><p>è¿ç§»å­¦ä¹ ï¼šå½“æ•°æ®é›†å¤ªå°ï¼Œæ— æ³•ç”¨æ¥è®­ç»ƒä¸€ä¸ªè¶³å¤Ÿå¥½çš„ç¥ç»ç½‘ç»œï¼Œå¯ä»¥é€‰æ‹©fine-tuneä¸€äº›é¢„è®­ç»ƒç½‘ç»œã€‚ä½¿ç”¨æ—¶ä¿®æ”¹æœ€åå‡ å±‚ï¼Œé™ä½å­¦ä¹ ç‡ã€‚</p><p>  kerasä¸­ä¸€äº›é¢„è®­ç»ƒæƒé‡ä¸‹è½½åœ°å€ï¼š<a href="https://github.com/fchollet/deep-learning-models/releases/" target="_blank" rel="noopener">https://github.com/fchollet/deep-learning-models/releases/</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/24825503" target="_blank" rel="noopener">K-foldäº¤å‰éªŒè¯</a>ï¼š</p><ol><li><p>æˆ‘ä»¬ä¸èƒ½å°†å…¨éƒ¨æ•°æ®é›†ç”¨äºè®­ç»ƒâ€”â€”è¿™æ ·å°±æ²¡æœ‰æ•°æ®æ¥æµ‹è¯•æ¨¡å‹æ€§èƒ½äº†</p></li><li><p>å°†æ•°æ®é›†åˆ†å‰²ä¸ºtraining set å’Œ test setï¼Œè¡¡é‡ç»“æœå–å†³äºæ•°æ®é›†åˆ’åˆ†ï¼Œtraining setå’Œå…¨é›†ä¹‹é—´å­˜åœ¨biasï¼Œä¸åŒtestä¸‹ç»“æœvarietyå¾ˆå¤§</p></li><li><p>äº¤å‰éªŒè¯Cross-Validationï¼š</p><ul><li>æç«¯æƒ…å†µLOOCVï¼šå…¨é›†Nï¼Œæ¯æ¬¡å–ä¸€ä¸ªåštestï¼Œå…¶ä»–åštrainï¼Œé‡å¤Næ¬¡ï¼Œå¾—åˆ°Nä¸ªæ¨¡å‹ï¼Œå¹¶è®¡ç®—Nä¸ªteståšå¹³å‡</li><li>K-foldï¼šå…¨é›†åˆ‡åˆ†æˆkä»½ï¼Œæ¯æ¬¡å–ä¸€ä¸ªåštestï¼Œå…¶ä»–åštrainï¼Œé‡å¤kæ¬¡ï½</li><li>å®éªŒæ˜¾ç¤ºLOOCVå’Œ10-foldCVçš„ç»“æœå¾ˆç›¸è¿‘ï¼Œåè€…è®¡ç®—æˆæœ¬æ˜æ˜¾å‡å°</li><li>Bias-Variance Trade-Offï¼šKè¶Šå¤§ï¼Œtrain setè¶Šæ¥è¿‘å…¨é›†ï¼Œbiasè¶Šå°ï¼Œä½†æ˜¯æ¯ä¸ªtrain setä¹‹é—´ç›¸å…³æ€§è¶Šå¤§ï¼Œ<strong>è€Œè¿™ç§å¤§ç›¸å…³æ€§ä¼šå¯¼è‡´æœ€ç»ˆçš„test errorå…·æœ‰æ›´å¤§çš„Variance</strong></li></ul></li></ol></li></ul></li></ol><ol><li><p>åˆ†å‰²</p><ul><li><p>å®ä¾‹åˆ†å‰²&amp;è¯­ä¹‰åˆ†å‰²</p><ul><li>instance segmentationï¼šæ ‡è®°å®ä¾‹å’Œè¯­ä¹‰, ä¸ä»…è¦åˆ†å‰²å‡º<code>äºº</code>è¿™ä¸ªç±», è€Œä¸”è¦åˆ†å‰²å‡º<code>è¿™ä¸ªäººæ˜¯è°</code>, ä¹Ÿå°±æ˜¯å…·ä½“çš„å®ä¾‹</li><li><p>semantic segmentationï¼šåªæ ‡è®°è¯­ä¹‰, ä¹Ÿå°±æ˜¯è¯´åªåˆ†å‰²å‡º<code>äºº</code>è¿™ä¸ªç±»æ¥</p><p><img src="/2019/10/31/å›¾åƒç®—æ³•ç»¼è¿°/segmentation.png" width="60%;"></p></li></ul></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Segmentation</title>
      <link href="/2019/08/22/Segmentation/"/>
      <url>/2019/08/22/Segmentation/</url>
      <content type="html"><![CDATA[<p>idea: </p><ol><li>CTå›¾ä¸€èˆ¬æ˜¯å•é€šé“ç°åº¦å›¾åƒï¼Œå‡å¦‚æˆ‘å°†128å¼ CTå›¾å †å åœ¨ä¸€èµ·ï¼ˆå³128é€šé“çš„å›¾åƒï¼‰ï¼Œç„¶åç”¨2Då·ç§¯ï¼ˆä¼šè€ƒè™‘é€šé“æ•°128ï¼‰ï¼Œè¿™æ ·å’Œç›´æ¥ç”¨3Då·ç§¯ä¼šæœ‰ç»“æœä¸Šçš„å·®åˆ«å—ï¼Ÿ</li><li>3dç½‘ç»œå¯ä»¥ç»“åˆå›¾åƒå±‚é—´ä¿¡æ¯ï¼Œèƒ½å¤Ÿä¿è¯éš”å±‚å›¾åƒMaskä¹‹é—´çš„ä¸€ä¸ªå˜åŒ–è¿ç»­æ€§ï¼Œæ•ˆæœä¼šæ¯”2då¥½ã€‚å±‚é—´è·å¤§çš„å›¾åƒï¼Œåœ¨é¢„å¤„ç†ä¸­ä¼šæœ‰æ’å€¼ã€‚</li><li>3dç½‘ç»œå› ä¸ºæ˜¾å­˜çš„é™åˆ¶ï¼Œä¸€ç§å¤„ç†æ–¹å¼æ˜¯è£æˆ3d patchä½œä¸ºè¾“å…¥ï¼Œå¯¼è‡´å…¶æ„Ÿå—é‡æœ‰é™ï¼Œé€šå¸¸åªèƒ½ä¸“æ³¨äºç»†èŠ‚å’Œå±€éƒ¨ç‰¹å¾ï¼Œé€‚åˆä½œä¸ºç¬¬äºŒçº§ç½‘ç»œç”¨äºå¯¹ç»†èŠ‚åšç²¾ä¼˜åŒ–ã€‚ä¸€ç§å¤„ç†æ–¹å¼æ˜¯é™é‡‡æ ·ï¼Œåˆ†å‰²ç²¾åº¦ä¸‹é™ã€‚</li><li>2.5Dç½‘ç»œã€‚</li></ol>]]></content>
      
      
    </entry>
    
    <entry>
      <title>keras note</title>
      <link href="/2019/08/14/keras-note/"/>
      <url>/2019/08/14/keras-note/</url>
      <content type="html"><![CDATA[<h3 id="1-keras-Lambdaè‡ªå®šä¹‰å±‚"><a href="#1-keras-Lambdaè‡ªå®šä¹‰å±‚" class="headerlink" title="1. keras Lambdaè‡ªå®šä¹‰å±‚"></a>1. keras Lambdaè‡ªå®šä¹‰å±‚</h3><p>å®˜æ–¹æ–‡æ¡£ï¼šå°†ä»»æ„è¡¨è¾¾å¼(function)å°è£…ä¸º Layer å¯¹è±¡ã€‚<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Lambda(function, output_shape=<span class="keyword">None</span>, mask=<span class="keyword">None</span>, arguments=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure></p><ul><li>function: éœ€è¦å°è£…çš„å‡½æ•°ã€‚ å°†è¾“å…¥å¼ é‡ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°ã€‚</li><li>output_shape: é¢„æœŸçš„å‡½æ•°è¾“å‡ºå°ºå¯¸ã€‚(ä½¿ç”¨ TensorFlow æ—¶ï¼Œå¯è‡ªåŠ¨æ¨ç†å¾—åˆ°)</li><li>arguments: å¯é€‰çš„éœ€è¦ä¼ é€’ç»™å‡½æ•°çš„å…³é”®å­—å‚æ•°ã€‚<strong>ä»¥å­—å…¸å½¢å¼ä¼ å…¥ã€‚</strong></li></ul><p>å‡ ä¸ªæ —å­ï¼š</p><p>1.1 æœ€ç®€ï¼šä½¿ç”¨åŒ¿åå‡½æ•°<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.add(Lambda(<span class="keyword">lambda</span> x: x ** <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">x = Lambda(<span class="keyword">lambda</span> image: K.image.resize_images(image, (target_size, target_size)))(inpt)</span><br></pre></td></tr></table></figure></p><p>å…¶ä¸­ï¼Œ<strong>lambda</strong>æ˜¯pythonçš„åŒ¿åå‡½æ•°ï¼Œåé¢çš„[xx: xxxx]ç”¨æ¥æè¿°å‡½æ•°çš„è¡¨è¾¾å½¢å¼ï¼Œ<br><strong>lambda xx: xxxx</strong>æ•´ä½“ä½œä¸º<strong>Lambda</strong>å‡½æ•°çš„functionå‚æ•°ã€‚</p><p>1.2 ä¸­çº§ï¼šé€šè¿‡å­—å…¸ä¼ å‚ï¼Œå°è£…è‡ªå®šä¹‰å‡½æ•°ï¼Œå®ç°æ•°æ®åˆ‡ç‰‡<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Lambda, Dense, Activation, Reshape, concatenate</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> plot_model</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">slice</span><span class="params">(x, index)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x[:, :, index]</span><br><span class="line"></span><br><span class="line">a = Input(shape=(<span class="number">4</span>,<span class="number">2</span>))</span><br><span class="line">x1 = Lambda(slice,output_shape=(<span class="number">4</span>,<span class="number">1</span>),arguments=&#123;<span class="string">'index'</span>:<span class="number">0</span>&#125;)(a)</span><br><span class="line">x2 = Lambda(slice,output_shape=(<span class="number">4</span>,<span class="number">1</span>),arguments=&#123;<span class="string">'index'</span>:<span class="number">1</span>&#125;)(a)</span><br><span class="line">x1 = Reshape((<span class="number">4</span>,<span class="number">1</span>,<span class="number">1</span>))(x1)</span><br><span class="line">x2 = Reshape((<span class="number">4</span>,<span class="number">1</span>,<span class="number">1</span>))(x2)</span><br><span class="line">output = concatenate([x1,x2])</span><br><span class="line">model = Model(a, output)</span><br><span class="line">plot_model(model, to_file=<span class="string">'model.png'</span>, show_shapes=<span class="keyword">True</span>, show_layer_names=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure></p><p>æ¨¡å‹ç»“æ„å›¾å¦‚ä¸‹ï¼š</p><p><img src="/2019/08/14/keras-note/Users/amber/Documents/workspace/blog/gitBlog/source/_posts/keras-note/slice.png" alt="slice img" style="zoom:67%;"></p><p>1.3 é«˜çº§ï¼šè‡ªå®šä¹‰æŸå¤±å‡½æ•°<br>    step 1. æŠŠy_trueå®šä¹‰ä¸ºä¸€ä¸ªè¾“å…¥<br>    step 2. æŠŠlosså†™æˆä¸€ä¸ªå±‚ï¼Œä½œä¸ºç½‘ç»œçš„æœ€ç»ˆè¾“å‡º<br>    step 3. åœ¨compileçš„æ—¶å€™ï¼Œå°†lossè®¾ç½®ä¸ºy_pred</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yolov3 train.py create_model:</span></span><br><span class="line">model_loss = Lambda(yolo_loss, output_shape=(<span class="number">1</span>,), name=<span class="string">'yolo_loss'</span>, </span><br><span class="line">                    arguments=&#123;<span class="string">'anchors'</span>: anchors, <span class="string">'num_classes'</span>: num_classes, <span class="string">'ignore_thresh'</span>: <span class="number">0.5</span>&#125;)(</span><br><span class="line">                    [*model_body.output, *y_true])</span><br><span class="line">model = Model([model_body.input, *y_true], model_loss)</span><br><span class="line">model.compile(optimizer=Adam(lr=<span class="number">1e-3</span>), loss=&#123;<span class="string">'yolo_loss'</span>: <span class="keyword">lambda</span> y_true, y_pred: y_pred&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># yolov3 model.py yolo_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">yolo_loss</span><span class="params">(args, anchors, num_classes, ignore_thresh=<span class="number">.5</span>, print_loss=False)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="2-keras-è‡ªå®šä¹‰loss"><a href="#2-keras-è‡ªå®šä¹‰loss" class="headerlink" title="2. keras è‡ªå®šä¹‰loss"></a>2. keras è‡ªå®šä¹‰loss</h3><p>è¡¥å……1.3: ä¹Ÿå¯ä»¥ä¸å®šä¹‰ä¸ºç½‘ç»œå±‚ï¼Œç›´æ¥è°ƒç”¨è‡ªå®šä¹‰losså‡½æ•°<br>å‚æ•°ï¼š</p><ul><li>y_true: çœŸå®æ ‡ç­¾ï¼ŒTheano/Tensorflow å¼ é‡ã€‚</li><li>y_pred: é¢„æµ‹å€¼ã€‚å’Œ y_true ç›¸åŒå°ºå¯¸çš„ Theano/TensorFlow å¼ é‡ã€‚<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mycrossentropy</span><span class="params">(y_true, y_pred, e=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span>-e)*K.categorical_crossentropy(y_pred,y_true) + e*K.categorical_crossentropy(y_pred, K.ones_like(y_pred)/num_classes)</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,loss=mycrossentropy, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure></li></ul><p>å¸¦å‚æ•°çš„è‡ªå®šä¹‰lossï¼š</p><p>æœ‰æ—¶å€™æˆ‘ä»¬è®¡ç®—lossçš„æ—¶å€™ä¸åªè¦ç”¨åˆ°y_trueå’Œy_predï¼Œè¿˜æƒ³å¼•å…¥ä¸€äº›å‚æ•°ï¼Œä½†æ˜¯kerasé™å®šæ„é€ losså‡½æ•°æ—¶åªèƒ½æ¥æ”¶(y_true, y_pred)ä¸¤ä¸ªå‚æ•°ï¼Œå¦‚ä½•ä¼˜é›…çš„ä¼ å…¥å‚æ•°ï¼Ÿ</p><p>ä¼˜é›…çš„è§£å†³æ–¹æ¡ˆå¦‚ä¸‹ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># build model</span></span><br><span class="line">model = my_model()</span><br><span class="line"><span class="comment"># define loss func</span></span><br><span class="line">model_loss = dice_loss(smooth=<span class="number">1e-5</span>, thresh=<span class="number">0.5</span>)</span><br><span class="line">model.compile(loss=model_loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®ç°</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dice_loss</span><span class="params">(smooth, thresh)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">dice</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>-dice_coef(y_true, y_pred, smooth, thresh)</span><br><span class="line">  <span class="keyword">return</span> dice</span><br></pre></td></tr></table></figure><h3 id="3-keras-è‡ªå®šä¹‰metrics"><a href="#3-keras-è‡ªå®šä¹‰metrics" class="headerlink" title="3. keras è‡ªå®šä¹‰metrics"></a>3. keras è‡ªå®šä¹‰metrics</h3><p>model.compileé‡Œé¢é™¤äº†lossè¿˜æœ‰ä¸€ä¸ªmetricsï¼Œç”¨äºæ¨¡å‹æ€§èƒ½è¯„ä¼°<br>å‚æ•°ï¼š</p><ul><li>y_true: çœŸå®æ ‡ç­¾ï¼ŒTheano/Tensorflow å¼ é‡ã€‚</li><li>y_pred: é¢„æµ‹å€¼ã€‚å’Œ y_true ç›¸åŒå°ºå¯¸çš„ Theano/TensorFlow å¼ é‡ã€‚<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">precision</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    <span class="comment"># Calculates the precision</span></span><br><span class="line">    true_positives = K.sum(K.round(K.clip(y_true * y_pred, <span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">    predicted_positives = K.sum(K.round(K.clip(y_pred, <span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">    precision = true_positives / (predicted_positives + K.epsilon())</span><br><span class="line">    <span class="keyword">return</span> precision</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recall</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    <span class="comment"># Calculates the recall</span></span><br><span class="line">    true_positives = K.sum(K.round(K.clip(y_true * y_pred, <span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">    possible_positives = K.sum(K.round(K.clip(y_true, <span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">    recall = true_positives / (possible_positives + K.epsilon())</span><br><span class="line">    <span class="keyword">return</span> recall</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,loss=mycrossentropy, metrics=[<span class="string">'accuracy'</span>, recall, precision])</span><br></pre></td></tr></table></figure></li></ul><h3 id="4-keras-è‡ªå®šä¹‰Layer"><a href="#4-keras-è‡ªå®šä¹‰Layer" class="headerlink" title="4. keras è‡ªå®šä¹‰Layer"></a>4. keras è‡ªå®šä¹‰Layer</h3><p>æºä»£ç ï¼š<a href="https://github.com/keras-team/keras/blob/master/keras/engine/base_layer.py" target="_blank" rel="noopener">https://github.com/keras-team/keras/blob/master/keras/engine/base_layer.py</a></p><p>è‡ªå®šä¹‰layerç»§æ‰¿kerasçš„Layerç±»ï¼Œéœ€è¦å®ç°ä¸‰ä¸ªæ–¹æ³•ï¼š</p><ul><li><code>build(input_shape)</code>ï¼šå®šä¹‰æƒé‡ï¼Œè°ƒç”¨add_weightæ¥åˆ›å»ºå±‚çš„æƒé‡çŸ©é˜µï¼Œå…¶ä¸­æœ‰å‚æ•°trainableå£°æ˜è¯¥å‚æ•°çš„æƒé‡æ˜¯å¦ä¸ºå¯è®­ç»ƒæƒé‡ï¼Œè‹¥trainable==Trueï¼Œä¼šæ‰§è¡Œself._trainable_weights.append(weight)å°†è¯¥æƒé‡åŠ å…¥åˆ°å¯è®­ç»ƒæƒé‡çš„lstä¸­ã€‚</li><li><code>call(x)</code>ï¼šç¼–å†™å±‚é€»è¾‘</li><li><code>compute_output_shape(input_shape)</code>ï¼šå®šä¹‰å¼ é‡å½¢çŠ¶çš„å˜åŒ–é€»è¾‘</li><li>get_configï¼šè¿”å›ä¸€ä¸ªå­—å…¸ï¼Œè·å–å½“å‰å±‚çš„å‚æ•°ä¿¡æ¯</li></ul><p>çœ‹äº†kerasä¸€äº›å±‚çš„å®ç°ï¼Œkerasä¸­å±‚ï¼ˆå¦‚convã€depthwise convï¼‰çš„callå‡½æ•°åŸºæœ¬éƒ½æ˜¯é€šè¿‡è°ƒç”¨tf.backendä¸­çš„æ–¹æ³•æ¥å®ç°</p><p>4.1 æ —å­ï¼šCenterLossLayer</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CenterLossLayer</span><span class="params">(Layer)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, alpha=<span class="number">0.5</span>, **kwargs)</span>:</span>     <span class="comment"># alphaï¼šcenter updateçš„å­¦ä¹ ç‡</span></span><br><span class="line">        super().__init__(**kwargs)</span><br><span class="line">        self.alpha = alpha</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">     <span class="comment"># add_weightï¼šä¸ºè¯¥å±‚åˆ›å»ºä¸€ä¸ªå¯è®­ç»ƒï¼ä¸å¯è®­ç»ƒçš„æƒé‡</span></span><br><span class="line">        self.centers = self.add_weight(name=<span class="string">'centers'</span>,</span><br><span class="line">                                       shape=(<span class="number">10</span>, <span class="number">2</span>),</span><br><span class="line">                                       initializer=<span class="string">'uniform'</span>,</span><br><span class="line">                                       trainable=<span class="keyword">False</span>)</span><br><span class="line">        <span class="comment"># ä¸€å®šè¦åœ¨æœ€åè°ƒç”¨å®ƒ</span></span><br><span class="line">        super().build(input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x, mask=None)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># x[0] is Nx2, x[1] is Nx10 onehot, self.centers is 10x2</span></span><br><span class="line">        delta_centers = K.dot(K.transpose(x[<span class="number">1</span>]), (K.dot(x[<span class="number">1</span>], self.centers) - x[<span class="number">0</span>]))  <span class="comment"># 10x2</span></span><br><span class="line">        center_counts = K.sum(K.transpose(x[<span class="number">1</span>]), axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>) + <span class="number">1</span>  <span class="comment"># 10x1</span></span><br><span class="line">        delta_centers /= center_counts</span><br><span class="line">        new_centers = self.centers - self.alpha * delta_centers</span><br><span class="line">        <span class="comment"># add_updateï¼šæ›´æ–°å±‚å†…å‚æ•°(buildä¸­å®šä¹‰çš„å‚æ•°)çš„æ–¹æ³•</span></span><br><span class="line">        self.add_update((self.centers, new_centers), x)</span><br><span class="line">        self.result = x[<span class="number">0</span>] - K.dot(x[<span class="number">1</span>], self.centers)</span><br><span class="line">        self.result = K.sum(self.result ** <span class="number">2</span>, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>) <span class="comment">#/ K.dot(x[1], center_counts)</span></span><br><span class="line">        <span class="keyword">return</span> self.result <span class="comment"># Nx1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_output_shape</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> K.int_shape(self.result)</span><br></pre></td></tr></table></figure><p>æœ‰ä¸€äº›è‡ªå®šä¹‰å±‚ï¼Œæœ‰æ—¶å€™ä¼šä¸å®ç°compute_output_shapeå’Œget_config</p><ul><li>åœ¨callæ–¹æ³•ä¸­ï¼Œè¾“å‡ºtensorå¦‚æœå‘ç”Ÿäº†shapeçš„å˜åŒ–ï¼Œkeras layeræ˜¯ä¸ä¼šè‡ªåŠ¨æ¨å¯¼å‡ºè¾“å‡ºshapeçš„ï¼Œæ‰€ä»¥è¦æ˜¾ç¤ºçš„è‡ªå®šä¹‰compute_output_shape</li><li>ä¸ç®¡å®šä¸å®šä¹‰get_configæ–¹æ³•ï¼Œéƒ½å¯ä»¥ä½¿ç”¨load_weightsæ–¹æ³•åŠ è½½ä¿å­˜çš„æƒé‡</li><li>ä½†æ˜¯å¦‚æœè¦ä½¿ç”¨load_modelæ–¹æ³•è½½å…¥åŒ…å«è‡ªå®šä¹‰å±‚çš„modelï¼Œå¿…é¡»è¦æ˜¾ç¤ºè‡ªå®šä¹‰get_configæ–¹æ³•ï¼Œå¦åˆ™keras æ— æ³•è·çŸ¥ Linear çš„é…ç½®å‚æ•°ï¼<ul><li>åœ¨ <code>__init__</code> çš„æœ€ååŠ ä¸Š <code>**kwargs</code> å‚æ•°ï¼Œå¹¶ç”¨ <code>**kwargs</code> å‚æ•°åˆå§‹åŒ–çˆ¶ç±»ã€‚</li><li>å®ç°ä¸Šè¿°çš„ <code>get_config</code> æ–¹æ³•ï¼Œè¿”å›è‡ªå®šä¹‰çš„å‚æ•°é…ç½®å’Œé»˜è®¤çš„å‚æ•°é…ç½®</li></ul></li></ul><p>4.2 è¡¥å……1.3 &amp; 2: è‡ªå®šä¹‰æŸå¤±å‡½æ•°é™¤äº†å¯ä»¥ç”¨Lambdaå±‚ï¼Œä¹Ÿå¯ä»¥å®šä¹‰Layerå±‚ï¼Œè¿™æ˜¯ä¸ªæ²¡æœ‰æƒé‡çš„è‡ªå®šä¹‰Layerã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># å®˜æ–¹ç¤ºä¾‹ï¼šCustom loss layer</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomVariationalLayer</span><span class="params">(Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        self.is_placeholder = <span class="keyword">True</span></span><br><span class="line">        super(CustomVariationalLayer, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">vae_loss</span><span class="params">(self, x, x_decoded_mean)</span>:</span></span><br><span class="line">        xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)<span class="comment">#Square Loss</span></span><br><span class="line">        kl_loss = - <span class="number">0.5</span> * K.sum(<span class="number">1</span> + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=<span class="number">-1</span>)<span class="comment"># KL-Divergence Loss</span></span><br><span class="line">        <span class="keyword">return</span> K.mean(xent_loss + kl_loss)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        x = inputs[<span class="number">0</span>]</span><br><span class="line">        x_decoded_mean = inputs[<span class="number">1</span>]</span><br><span class="line">        loss = self.vae_loss(x, x_decoded_mean)</span><br><span class="line">        self.add_loss(loss, inputs=inputs)</span><br><span class="line">        <span class="comment"># We won't actually use the output.</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>4.3 è¡¥å……</p><p>callæ–¹æ³•çš„å®Œæ•´å‚æ•°ï¼šcall(self, inputs, <em>args, *</em>kwargs)</p><ul><li>å…¶ä¸­inputså°±æ˜¯å±‚è¾“å…¥ï¼Œtensor/tensors</li><li>é™¤æ­¤ä¹‹å¤–è¿˜æœ‰ä¸¤ä¸ªreserved keyword argumentsï¼štraining&amp;maskï¼Œä¸€ä¸ªç”¨äºbn/dropoutè¿™ç§train/testè®¡ç®—æœ‰åŒºåˆ«çš„flagï¼Œä¸€ä¸ªç”¨äºRNNlayersçº¦æŸæ—¶åºç›¸å…³å…³ç³»</li><li><em>argså’Œ*</em>kwargsæ˜¯é¢„ç•™ä¸ºäº†ä»¥åæ‰©å±•æ›´å¤šè¾“å…¥å‚æ•°çš„</li></ul><h3 id="5-keras-Generator"><a href="#5-keras-Generator" class="headerlink" title="5. keras Generator"></a>5. keras Generator</h3><p>æœ¬è´¨ä¸Šå°±æ˜¯pythonçš„ç”Ÿæˆå™¨ï¼Œæ¯æ¬¡è¿”å›<strong>ä¸€ä¸ªbatch</strong>çš„æ ·æœ¬åŠæ ‡ç­¾<br>è‡ªå®šä¹‰generatorçš„æ—¶å€™è¦å†™æˆæ­»å¾ªç¯ï¼ˆwhile trueï¼‰ï¼Œå› ä¸ºmodel.fit_generator()åœ¨ä½¿ç”¨åœ¨ä¸ªå‡½æ•°çš„æ—¶å€™ï¼Œå¹¶ä¸ä¼šåœ¨æ¯ä¸€ä¸ªepochä¹‹åé‡æ–°è°ƒç”¨ï¼Œé‚£ä¹ˆå¦‚æœè¿™æ—¶å€™generatorè‡ªå·±ç»“æŸäº†å°±ä¼šæœ‰é—®é¢˜ã€‚<br>æ —å­æ˜¯æˆ‘ä¸ºmixupå†™çš„generatorï¼š<br>æ²¡æœ‰æ˜¾ç¤ºçš„while Trueæ˜¯å› ä¸ºåˆ›å»ºkerasè‡ªå¸¦çš„generatorçš„æ—¶å€™å·²ç»æ˜¯æ­»å¾ªç¯äº†ï¼ˆforæ°¸ä¸è·³å‡ºï¼‰<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Datagen_mixup</span><span class="params">(data_path, img_size, batch_size, is_train=True, mix_prop=<span class="number">0.8</span>, alpha=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> is_train:</span><br><span class="line">        datagen = ImageDataGenerator()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        datagen = ImageDataGenerator()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># using kerasåº“å‡½æ•°</span></span><br><span class="line">    generator = datagen.flow_from_directory(data_path, target_size=(img_size, img_size),</span><br><span class="line">                                            batch_size=batch_size,</span><br><span class="line">                                            color_mode=<span class="string">"grayscale"</span>,</span><br><span class="line">                                            shuffle=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> generator:     <span class="comment"># a batch of &lt;img, label&gt;</span></span><br><span class="line">        <span class="keyword">if</span> alpha &gt; <span class="number">0</span>:</span><br><span class="line">            lam = np.random.beta(alpha, alpha)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            lam = <span class="number">1</span></span><br><span class="line">        idx = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">0</span>])]</span><br><span class="line">        random.shuffle(idx)</span><br><span class="line">        mixed_x = lam*x + (<span class="number">1</span>-lam)*x[idx]</span><br><span class="line">        mixed_y = lam*y + (<span class="number">1</span>-lam)*y[idx]</span><br><span class="line"></span><br><span class="line">        n_origin = int(batch_size * mix_prop)</span><br><span class="line">        gen_x = np.vstack(x[:n_origin], mixed_x[:(batch_size-n_origin)])</span><br><span class="line">        gen_y = np.vstack(y[:n_origin], mixed_y[:(batch_size-n_origin)])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> gen_x, gen_y</span><br></pre></td></tr></table></figure></p><p>ã€å¤šè¿›ç¨‹ã€‘fit_generatorä¸­æœ‰ä¸€ä¸ªå‚æ•°use_multiprocessingï¼Œé»˜è®¤è®¾ç½®ä¸ºfalseï¼Œå› ä¸ºâ€˜using a generator with use_multiprocessing=True and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequenceâ€™ classâ€™</p><p>å¦‚æœè®¾ç½®å¤šè¿›ç¨‹use_multiprocessingï¼Œä»£ç ä¼šæŠŠä½ çš„æ•°æ®å¤åˆ¶å‡ ä»½ï¼Œåˆ†ç»™ä¸åŒçš„workersè¿›è¡Œè¾“å…¥ï¼Œè¿™æ˜¾ç„¶ä¸æ˜¯æˆ‘ä»¬å¸Œæœ›çš„ï¼Œæˆ‘ä»¬å¸Œæœ›ä¸€ä»½æ•°æ®ç›´æ¥å¹³å‡åˆ†ç»™å¤šä¸ªworkerså¸®å¿™è¾“å…¥ï¼Œè¿™æ ·æ‰æ˜¯æœ€å¿«çš„ã€‚è€ŒSequenceæ•°æ®ç±»èƒ½å®Œç¾è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</p><p><strong>keras.utils.Sequence()</strong>ï¼š</p><ul><li>æ¯ä¸€ä¸ª <code>Sequence</code> å¿…é¡»å®ç° <code>__getitem__</code> å’Œ <code>__len__</code> æ–¹æ³•</li><li><code>__getitem__</code> æ–¹æ³•åº”è¯¥èŒƒå›´ä¸€ä¸ªå®Œæ•´çš„æ‰¹æ¬¡</li><li><strong>å¦‚æœä½ æƒ³åœ¨è¿­ä»£ä¹‹é—´ä¿®æ”¹ä½ çš„æ•°æ®é›†ï¼Œä½ å¯ä»¥å®ç° <code>on_epoch_end</code></strong>ï¼ˆä¼šåœ¨æ¯ä¸ªè¿­ä»£ä¹‹é—´è¢«éšå¼è°ƒç”¨ï¼‰\</li><li>githubä¸Šæœ‰issueåæ˜ on_epoch_endä¸ä¼šæ²¡è°ƒç”¨ï¼Œè§£å†³æ–¹æ¡ˆï¼šåœ¨__len__æ–¹æ³•ä¸­æ˜¾ç¤ºè‡ªè¡Œè°ƒç”¨</li></ul><p>ç›´æ¥çœ‹æ —å­ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.applications <span class="keyword">import</span> ResNet50</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataGenerator</span><span class="params">(keras.utils.Sequence)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data, batch_size=<span class="number">1</span>, shuffle=True)</span>:</span></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.data = data</span><br><span class="line">        self.indexes = np.arange(len(self.data))</span><br><span class="line">        self.shuffle = shuffle</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># è®¡ç®—æ¯ä¸€ä¸ªepochçš„è¿­ä»£æ¬¡æ•°</span></span><br><span class="line">        <span class="keyword">return</span> math.ceil(len(self.data) / float(self.batch_size))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="comment"># ç”Ÿæˆæ¯ä¸ªbatchæ•°æ®</span></span><br><span class="line">        batch_indices = self.indexes[index*self.batch_size:(index+<span class="number">1</span>)*self.batch_size]</span><br><span class="line">        batch_data = [self.data[k] <span class="keyword">for</span> k <span class="keyword">in</span> batch_indices]</span><br><span class="line"></span><br><span class="line">        x_batch, y_batch = self.data_generation(batch_data)</span><br><span class="line">        <span class="keyword">return</span> x_batch, y_batch</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.shuffle == <span class="keyword">True</span>:</span><br><span class="line">            np.random.shuffle(self.indexes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">data_generation</span><span class="params">(self, batch_data)</span>:</span></span><br><span class="line">        images = []</span><br><span class="line">        labels = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ç”Ÿæˆæ•°æ®</span></span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(batch_data):</span><br><span class="line">            image = cv2.imread(data, <span class="number">0</span>)</span><br><span class="line">            image = cv2.resize(image, dsize=(<span class="number">64</span>,<span class="number">64</span>), interpolation=cv2.INTER_LINEAR)</span><br><span class="line">            <span class="keyword">if</span> np.max(image)&gt;<span class="number">1</span>:</span><br><span class="line">                image = image / <span class="number">255.</span></span><br><span class="line">            image = np.expand_dims(image, axis=<span class="number">-1</span>)</span><br><span class="line">            images.append(image)</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'d0'</span> <span class="keyword">in</span> data:</span><br><span class="line">                labels.append([<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                labels.append([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> np.array(images), np.array(labels)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">  <span class="comment"># data</span></span><br><span class="line">    data_dir = <span class="string">"/Users/amber/dataset/mnist"</span></span><br><span class="line">    data_lst = []</span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(data_dir+<span class="string">"/d0"</span>)[:]:</span><br><span class="line">        data_lst.append(os.path.join(data_dir, <span class="string">"d0"</span>, file))</span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(data_dir+<span class="string">"/d1"</span>)[:]:</span><br><span class="line">        data_lst.append(os.path.join(data_dir, <span class="string">"d1"</span>, file))</span><br><span class="line">    training_generator = DataGenerator(data_lst, batch_size=<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># model</span></span><br><span class="line">    model = ResNet50(input_shape=(<span class="number">64</span>,<span class="number">64</span>,<span class="number">1</span>),weights=<span class="keyword">None</span>, classes=<span class="number">2</span>)</span><br><span class="line">    model.compile(optimizer=SGD(<span class="number">1e-3</span>), loss=<span class="string">'categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">    model.fit_generator(training_generator, epochs=<span class="number">50</span>,max_queue_size=<span class="number">200</span>,workers=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>ç»éªŒå€¼ï¼š</p><ul><li>workersï¼š2/3</li><li>max_queue_sizeï¼šé»˜è®¤10ï¼Œå…·ä½“åŸºäºGPUå¤„äºç©ºé—²çŠ¶æ€é€‚é‡è°ƒèŠ‚</li></ul><p>ã€é™„åŠ ã€‘å®éªŒä¸­è¿˜å‘ç°ä¸€ä¸ªé—®é¢˜ï¼Œæœ€å¼€å§‹å®šä¹‰äº†ä¸€ä¸ªsequential modelï¼Œç„¶ååœ¨è°ƒç”¨fit_generatorä¸€ç›´æŠ¥é”™ï¼šmodel not compileï¼Œä½†æ˜¯æ˜¾ç„¶modelæ˜¯compileè¿‡äº†çš„ï¼Œç½‘ä¸ŠæŸ¥åˆ°çš„è§£é‡Šï¼šâ€˜Sequential model works with model.fit but not with model.fit_generatorâ€™</p><h3 id="6-å¤šGPU"><a href="#6-å¤šGPU" class="headerlink" title="6. å¤šGPU"></a>6. å¤šGPU</h3><p>å¤šGPUè¿è¡Œåˆ†ä¸ºä¸¤ç§æƒ…å†µï¼š</p><pre><code>* æ•°æ®å¹¶è¡Œ* è®¾å¤‡å¹¶è¡Œ</code></pre><p>6.1 æ•°æ®å¹¶è¡Œ</p><p>æ•°æ®å¹¶è¡Œå°†ç›®æ ‡æ¨¡å‹åœ¨å¤šä¸ªGPUä¸Šå„å¤åˆ¶ä¸€ä»½ï¼Œä½¿ç”¨æ¯ä¸ªå¤åˆ¶å“å¤„ç†æ•°æ®é›†çš„ä¸åŒéƒ¨åˆ†ã€‚</p><p>ä¸€ä¸ªæ —å­ï¼šå†™tripleNetæ¨¡å‹æ—¶ï¼Œå–äº†batch=4ï¼Œæ€»å…±15ç±»ï¼Œé‚£ä¹ˆä¸‰å…ƒç»„æ€»å…±æœ‰$(4/2)^2*15=60$ä¸ªï¼Œè®­ç»ƒç”¨äº†224çš„å›¾åƒï¼Œå•å¼ GPUå†…å­˜ä¼šæº¢å‡ºï¼Œå› æ­¤éœ€è¦å•æœºå¤šå¡æ•°æ®å¹¶è¡Œã€‚</p><p>â€‹    step1. åœ¨æ¨¡å‹å®šä¹‰ä¸­ï¼Œç”¨multi_gpu_modelå°ä¸€å±‚ï¼Œ<strong>éœ€è¦åœ¨model.compileä¹‹å‰</strong>ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.util <span class="keyword">import</span> multi_gpu_model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triple_model</span><span class="params">(input_shape=<span class="params">(<span class="number">512</span>,<span class="number">512</span>,<span class="number">1</span>)</span>, n_classes=<span class="number">10</span>, multi_gpu=False)</span>:</span></span><br><span class="line">  anchor_input = Input(shape=input_shape)</span><br><span class="line">  positive_input = Input(shape=input_shape)</span><br><span class="line">  negative_input = Input(shape=input_shape)</span><br><span class="line">  </span><br><span class="line">  sharedCNN = base_model(input_shape)</span><br><span class="line">  encoded_anchor = sharedCNN(anchor_input)</span><br><span class="line">  encoded_positive = sharedCNN(positive_input)</span><br><span class="line">  encoded_negative = sharedCNN(negative_input)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># class branch</span></span><br><span class="line">  x = Dense(n_classses, activation=<span class="string">'softmax'</span>)(encoded_anchor)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># distance branch</span></span><br><span class="line">  encoded_anchor = Activation(<span class="string">'sigmoid'</span>)(encoded_anchor)</span><br><span class="line">  encoded_positive = Activation(<span class="string">'sigmoid'</span>)(encoded_positive)</span><br><span class="line">  encoded_negative = Activation(<span class="string">'sigmoid'</span>)(encoded_negative)</span><br><span class="line">  merged = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=<span class="number">-1</span>, name=<span class="string">'tripleLossLayer'</span>)</span><br><span class="line">  </span><br><span class="line">  model = Model(inputs=[anchor_input,positive_input,negative_input], outputs=[x, merged])</span><br><span class="line">  <span class="keyword">if</span> multi_gpu:</span><br><span class="line">    model = multi_gpu_model(model, GPU_COUNT)</span><br><span class="line">  </span><br><span class="line">  model.compile(optimizer=SGD, loss=[cls_loss, triplet_loss], metrics=[cls_acc])</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>â€‹    step2. åœ¨å®šä¹‰checkpointæ—¶ï¼Œè¦ç”¨ParallelModelCheckpointå°ä¸€å±‚ï¼Œåˆå§‹åŒ–å‚æ•°çš„modelè¦ä¼ åŸå§‹æ¨¡å‹ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> ModelCheckpoint</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ParallelModelCheckpoint</span><span class="params">(ModelCheckpoint)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,single_model,multi_model, filepath, monitor=<span class="string">'val_loss'</span>, verbose=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 save_best_only=False, save_weights_only=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 mode=<span class="string">'auto'</span>, period=<span class="number">1</span>)</span>:</span></span><br><span class="line">        self.single_model = single_model</span><br><span class="line">        self.multi_model = multi_model</span><br><span class="line">        super(ParallelModelCheckpoint,self).__init__(filepath, monitor, verbose,save_best_only, save_weights_only,mode, period)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_model</span><span class="params">(self, model)</span>:</span></span><br><span class="line">        self.single_model.optimizer = self.multi_model.optimizer</span><br><span class="line">        super(ParallelModelCheckpoint,self).set_model(self.single_model)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span><span class="params">(self, epoch, logs=None)</span>:</span></span><br><span class="line">        <span class="comment"># save optimizer weights</span></span><br><span class="line">        self.single_model.optimizer = self.multi_model.optimizer</span><br><span class="line">        super(ParallelModelCheckpoint, self).on_epoch_end(epoch, logs)</span><br><span class="line"></span><br><span class="line">model = triple_model(multi_gpu=<span class="keyword">True</span>)</span><br><span class="line">single_model = triple_model(multi_gpu=<span class="keyword">False</span>)</span><br><span class="line">filepath = <span class="string">"./tripleNet_&#123;epoch:02d&#125;_val_loss_&#123;val_loss:.3f&#125;.h5"</span></span><br><span class="line">check_point = ParallelModelCheckpoint(single_model, filepath)</span><br></pre></td></tr></table></figure><p>â€‹    step3. åœ¨ä¿å­˜æƒé‡æ—¶ï¼Œé€šè¿‡cpuæ¨¡å‹æ¥ä¿å­˜ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># å®ä¾‹åŒ–åŸºç¡€æ¨¡å‹ï¼Œè¿™æ ·å®šä¹‰æ¨¡å‹æƒé‡ä¼šå­˜å‚¨åœ¨CPUå†…å­˜ä¸­</span></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">model = Resnet50(input_shape=(<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>), classes=<span class="number">4</span>, weights=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">parallel_model = multi_gpu_model(model, GPU_COUNT)</span><br><span class="line">parallel_model.fit(x,y, epochs=<span class="number">20</span>, batch_size=<span class="number">32</span>)</span><br><span class="line">model.save(<span class="string">'model.h5'</span>)</span><br></pre></td></tr></table></figure><p>â€‹    ã€attentionã€‘åŒç†ï¼Œåœ¨loadæƒé‡æ—¶ï¼Œä¹Ÿæ˜¯loadå•æ¨¡å‹çš„æƒé‡ï¼Œå†è°ƒç”¨multi_gpu_modelå°†æ¨¡å‹å¤åˆ¶åˆ°å¤šä¸ªGPUä¸Šï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = Model(inputs=[anchor_input,positive_input,negative_input], outputs=[x, merged])</span><br><span class="line"><span class="keyword">if</span> multi_gpu:</span><br><span class="line"><span class="keyword">if</span> os.path.exists(weight_pt):</span><br><span class="line">model.load_weights(weight_pt)</span><br><span class="line">  model = multi_gpu_model(model, GPU_COUNT)</span><br></pre></td></tr></table></figure><p>ã€ATTENTIONã€‘å®éªŒä¸­å‘ç°ä¸€ä¸ªé—®é¢˜ï¼šåœ¨æœ‰äº›caseä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†è‡ªå®šä¹‰lossä½œä¸ºç½‘ç»œçš„è¾“å‡ºï¼Œ<strong>æ­¤æ—¶ç½‘ç»œçš„è¾“å‡ºæ˜¯ä¸ªæ ‡é‡</strong>ï¼Œä½†æ˜¯åœ¨è°ƒç”¨multi_gpu_modelè¿™ä¸ªæ–¹æ³•æ—¶ï¼Œå…·ä½“å®ç°åœ¨multi_gpu_utils.pyä¸­ï¼Œæœ€åä¸€ä¸ªæ­¥éª¤è¦mergeå‡ ä¸ªdeviceçš„è¾“å‡ºï¼Œé€šè¿‡axis=0çš„concatå®ç°ï¼Œç½‘ç»œè¾“å‡ºæ˜¯æ ‡é‡çš„è¯å°±ä¼šæŠ¥é”™â€”â€”list assignment index out of rangeã€‚</p><p>å°è¯•çš„è§£å†³æ–¹æ¡ˆæ˜¯æ”¹æˆç›¸åŠ ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Merge outputs under expected scope.</span></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span> <span class="keyword">if</span> cpu_merge <span class="keyword">else</span> <span class="string">'/gpu:%d'</span> % target_gpu_ids[<span class="number">0</span>]):</span><br><span class="line">    merged = []</span><br><span class="line">    <span class="keyword">for</span> name, outputs <span class="keyword">in</span> zip(output_names, all_outputs):</span><br><span class="line">        merged.append(Lambda(<span class="keyword">lambda</span> x: K.sum(x))(outputs)) </span><br><span class="line">        <span class="comment"># merged.append(concatenate(outputs, axis=0, name=name))</span></span><br><span class="line">    <span class="keyword">return</span> Model(model.inputs, merged)</span><br></pre></td></tr></table></figure><p>ã€ATTENTION++ã€‘ç½‘ç»œçš„è¾“å‡ºä¸èƒ½æ˜¯æ ‡é‡ï¼ï¼æ°¸è¿œä¼šéšè—ä¿ç•™ä¸€ä¸ªbatch dimï¼Œä¹‹å‰æ˜¯å†™é”™äº†ï¼ï¼</p><ul><li>model lossæ˜¯ä¸€ä¸ªæ ‡é‡</li><li>ä½œä¸ºè¾“å‡ºå±‚çš„lossæ˜¯ä¿ç•™batch dimçš„ï¼ï¼</li></ul><p>6.2 è®¾å¤‡å¹¶è¡Œ</p><p>è®¾å¤‡å¹¶è¡Œé€‚ç”¨äº<strong>å¤šåˆ†æ”¯ç»“æ„</strong>ï¼Œä¸€ä¸ªåˆ†æ”¯ç”¨ä¸€ä¸ªGPUã€‚é€šè¿‡ä½¿ç”¨TensorFlow device scopeså®ç°ã€‚</p><p>æ —å­ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Model where a shared LSTM is used to encode two different sequences in parallel</span></span><br><span class="line">input_a = keras.Input(shape=(<span class="number">140</span>, <span class="number">256</span>))</span><br><span class="line">input_b = keras.Input(shape=(<span class="number">140</span>, <span class="number">256</span>))</span><br><span class="line"></span><br><span class="line">shared_lstm = keras.layers.LSTM(<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Process the first sequence on one GPU</span></span><br><span class="line"><span class="keyword">with</span> tf.device_scope(<span class="string">'/gpu:0'</span>):</span><br><span class="line">    encoded_a = shared_lstm(tweet_a)</span><br><span class="line"><span class="comment"># Process the next sequence on another GPU</span></span><br><span class="line"><span class="keyword">with</span> tf.device_scope(<span class="string">'/gpu:1'</span>):</span><br><span class="line">    encoded_b = shared_lstm(tweet_b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Concatenate results on CPU</span></span><br><span class="line"><span class="keyword">with</span> tf.device_scope(<span class="string">'/cpu:0'</span>):</span><br><span class="line">    merged_vector = keras.layers.concatenate([encoded_a, encoded_b],axis=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h3 id="7-åº“å‡½æ•°è®²è§£"><a href="#7-åº“å‡½æ•°è®²è§£" class="headerlink" title="7. åº“å‡½æ•°è®²è§£"></a>7. åº“å‡½æ•°è®²è§£</h3><p>7.1 BatchNormalization(axis=-1)</p><p>ç”¨äºåœ¨æ¯ä¸ªbatchä¸Šå°†å‰ä¸€å±‚çš„æ¿€æ´»å€¼é‡æ–°è§„èŒƒåŒ–ï¼Œå³ä½¿å¾—å…¶è¾“å‡ºæ•°æ®çš„å‡å€¼æ¥è¿‘0ï¼Œå…¶æ ‡å‡†å·®æ¥è¿‘1ã€‚</p><p>å¸¸ç”¨å‚æ•°axisï¼šæŒ‡å®šè¦è§„èŒƒåŒ–çš„è½´ï¼Œé€šå¸¸ä¸ºç‰¹å¾è½´ï¼Œå¦‚åœ¨â€œchannels_firstâ€çš„data formatä¸‹ï¼Œaxis=1ï¼Œåä¹‹axis=-1ã€‚</p><p>7.2 LSTM</p><p>å‚æ•°ï¼š</p><ul><li>unitsï¼šè¾“å‡ºç»´åº¦ï¼ˆæœ€åä¸€ç»´ï¼‰ï¼Œæ ‡å‡†è¾“å…¥NxTxDï¼ŒN for batchï¼ŒT for time-stepï¼ŒD for vector-dimensionã€‚</li><li>activationï¼šæ¿€æ´»å‡½æ•°</li><li>recurrent_activationï¼šç”¨äºå¾ªç¯æ—¶é—´æ­¥çš„æ¿€æ´»å‡½æ•°</li><li>dropoutï¼šåœ¨ 0 å’Œ 1 ä¹‹é—´çš„æµ®ç‚¹æ•°ã€‚ å•å…ƒçš„ä¸¢å¼ƒæ¯”ä¾‹ï¼Œç”¨äºè¾“å…¥çš„çº¿æ€§è½¬æ¢</li><li>recurrent_dropoutï¼šåœ¨ 0 å’Œ 1 ä¹‹é—´çš„æµ®ç‚¹æ•°ã€‚ å•å…ƒçš„ä¸¢å¼ƒæ¯”ä¾‹ï¼Œç”¨äºå¾ªç¯å±‚çŠ¶æ€çš„çº¿æ€§è½¬æ¢</li><li><strong>return_sequences: </strong>å¸ƒå°”å€¼ï¼Œé»˜è®¤Falseã€‚æ˜¯è¿”å›è¾“å‡ºåºåˆ—ä¸­çš„æœ€åä¸€ä¸ªè¾“å‡ºï¼Œè¿˜æ˜¯å…¨éƒ¨åºåˆ—çš„è¾“å‡ºã€‚å³many-to-oneè¿˜æ˜¯many-to-manyï¼Œç®€å•æ¥è®²ï¼Œå½“æˆ‘ä»¬éœ€è¦æ—¶åºè¾“å‡ºï¼ˆmany-to-manyï¼‰çš„æ—¶å€™ï¼Œå°±set Trueã€‚</li><li><strong>return_state</strong>: å¸ƒå°”å€¼ï¼Œé»˜è®¤Falseã€‚é™¤äº†è¾“å‡ºä¹‹å¤–æ˜¯å¦è¿”å›<strong>æœ€åä¸€ä¸ª</strong>çŠ¶æ€ï¼ˆcellå€¼ï¼‰</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># return_sequences</span></span><br><span class="line">inputs1 = Input(tensor=(<span class="number">1</span>ï¼Œ<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">lstm1 = LSTM(<span class="number">1</span>, return_sequences=<span class="keyword">True</span>)(inputs1)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">è¾“å‡ºç»“æœä¸º</span></span><br><span class="line"><span class="string">[[[-0.02243521]</span></span><br><span class="line"><span class="string">[-0.06210149]</span></span><br><span class="line"><span class="string">[-0.11457888]]]</span></span><br><span class="line"><span class="string">è¡¨ç¤ºæ¯ä¸ªtime-stepï¼ŒLSTM cellçš„è¾“å‡º</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># return_state</span></span><br><span class="line">lstm1, state_h, state_c = LSTM(<span class="number">1</span>, return_state=<span class="keyword">True</span>)(inputs1)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">è¾“å‡ºç»“æœä¸º</span></span><br><span class="line"><span class="string">[array([[ 0.10951342]], dtype=float32),</span></span><br><span class="line"><span class="string"> array([[ 0.10951342]], dtype=float32),</span></span><br><span class="line"><span class="string"> array([[ 0.24143776]], dtype=float32)]</span></span><br><span class="line"><span class="string"> listä¸­ä¾æ¬¡ä¸ºç½‘ç»œè¾“å‡ºï¼Œæœ€åä¸€ä¸ªtime-stepçš„LSTM cellçš„è¾“å‡ºå€¼å’Œcellå€¼</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><p>7.2.5 TimeDistributed</p><p>é¡ºä¾¿å†è¯´ä¸‹TimeDistributedï¼Œå½“æˆ‘ä»¬ä½¿ç”¨many-to-manyæ¨¡å‹ï¼Œæœ€åä¸€å±‚LSTMçš„è¾“å‡ºç»´åº¦ä¸ºkï¼Œè€Œæˆ‘ä»¬æƒ³è¦çš„æœ€ç»ˆè¾“å‡ºç»´åº¦ä¸ºnï¼Œé‚£ä¹ˆå°±éœ€è¦å¼•å…¥Denseå±‚ï¼Œå¯¹äºæ—¶åºæ¨¡å‹ï¼Œæˆ‘ä»¬è¦å¯¹æ¯ä¸€ä¸ªtime-stepå¼•å…¥denseå±‚ï¼Œè¿™å®è´¨ä¸Šæ˜¯å¤šä¸ªDenseæ“ä½œï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥ç”¨TimeDistributedæ¥åŒ…è£¹Denseå±‚æ¥å®ç°ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(LSTM(<span class="number">3</span>, input_shape=(length, <span class="number">1</span>), return_sequences=<span class="keyword">True</span>))</span><br><span class="line">model.add(TimeDistributed(Dense(<span class="number">1</span>)))</span><br></pre></td></tr></table></figure><p>å®˜æ–¹æ–‡æ¡£ï¼šè¿™ä¸ªå°è£…å™¨å°†ä¸€ä¸ªå±‚åº”ç”¨äºè¾“å…¥çš„æ¯ä¸ªæ—¶é—´ç‰‡ã€‚</p><ul><li>å½“è¯¥å±‚ä½œä¸ºç¬¬ä¸€å±‚æ—¶ï¼Œåº”æ˜¾å¼è¯´æ˜input_shape</li><li>TimeDistributedå¯ä»¥åº”ç”¨äºä»»æ„å±‚ï¼Œå¦‚Conv3Dï¼š</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ä¾‹å¦‚æˆ‘çš„crnn model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crnn</span><span class="params">(input_shape, cnn, n_classes=<span class="number">24</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">    inpt = Input(input_shape)</span><br><span class="line"></span><br><span class="line">    x = TimeDistributed(cnn, input_shape=input_shape)(inpt)</span><br><span class="line">    x = LSTM(<span class="number">128</span>, return_sequences=<span class="keyword">True</span>)(x)</span><br><span class="line">    x = LSTM(<span class="number">256</span>, return_sequences=<span class="keyword">True</span>)(x)</span><br><span class="line"></span><br><span class="line">    x = TimeDistributed(Dense(n_classes))(x)</span><br><span class="line"></span><br><span class="line">    model = Model(inpt, x)</span><br><span class="line">    model.summary()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"> </span><br><span class="line">crnn_model = crnn((<span class="number">24</span>,<span class="number">128</span>,<span class="number">128</span>,<span class="number">128</span>,<span class="number">2</span>), cnn_model)</span><br></pre></td></tr></table></figure><p>7.3 Embedding</p><p>ç”¨äºå°†ç¨€ç–ç¼–ç æ˜ å°„ä¸ºå›ºå®šå°ºå¯¸çš„å¯†é›†è¡¨ç¤ºã€‚</p><p>è¾“å…¥å½¢å¦‚ï¼ˆsamplesï¼Œsequence_lengthï¼‰çš„2Då¼ é‡ï¼Œè¾“å‡ºå½¢å¦‚(samples, sequence_length, output_dim)çš„3Då¼ é‡ã€‚</p><p>å‚æ•°ï¼š</p><ul><li>input_dimï¼šå­—å…¸é•¿åº¦ï¼Œå³è¾“å…¥æ•°æ®æœ€å¤§ä¸‹æ ‡+1</li><li>output_dimï¼š</li><li>input_lengthï¼š</li></ul><p>æ —å­ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># centerloss branch</span></span><br><span class="line">lambda_c = <span class="number">1</span></span><br><span class="line">input_ = Input(shape=(<span class="number">1</span>,))</span><br><span class="line">centers = Embedding(<span class="number">10</span>,<span class="number">2</span>)(input_)    <span class="comment"># (None, 1, 2)</span></span><br><span class="line"><span class="comment"># è¿™é‡Œçš„è¾“å…¥æ˜¯0-9çš„æšä¸¾ï¼ˆdim=10ï¼‰ï¼Œç„¶åæ˜ å°„æˆä¸€ä¸ªç°‡å¿ƒ</span></span><br><span class="line">intra_loss = Lambda(<span class="keyword">lambda</span> x:K.sum(K.square(x[<span class="number">0</span>]-x[<span class="number">1</span>][:,<span class="number">0</span>]),<span class="number">1</span>,keepdims=<span class="keyword">True</span>))([out1,centers])</span><br><span class="line">model_center_loss = Model([inputs,input_],[out2,intra_loss])</span><br><span class="line">model_center_loss.compile(optimizer=<span class="string">"sgd"</span>,</span><br><span class="line">                          loss=[<span class="string">"categorical_crossentropy"</span>,<span class="keyword">lambda</span> y_true,y_pred:y_pred],</span><br><span class="line">                          loss_weights=[<span class="number">1</span>,lambda_c/<span class="number">2.</span>],</span><br><span class="line">                          metrics=[<span class="string">"acc"</span>])</span><br><span class="line">model_center_loss.summary()</span><br></pre></td></tr></table></figure><p>7.4 plot_model</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from keras.utils import plot_model</span><br><span class="line">plot_model(model, to_file=&apos;model.png&apos;, show_shapes=False, show_layer_names=True)</span><br></pre></td></tr></table></figure><p>7.5 K.function</p><p>è·å–æ¨¡å‹æŸå±‚çš„è¾“å‡ºï¼Œä¸€ç§æ–¹æ³•æ˜¯åˆ›å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹ï¼Œä½¿å®ƒçš„è¾“å‡ºæ˜¯ç›®æ ‡å±‚ï¼Œç„¶åè°ƒç”¨predictã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = ...    <span class="comment"># the original model</span></span><br><span class="line"></span><br><span class="line">new_model = Model(input=model.input,</span><br><span class="line">                  output=model.get_layer(<span class="string">'my_layer'</span>).output)</span><br><span class="line"></span><br><span class="line">intermediate_output = new_model.predict(input_dataï¼‰</span><br></pre></td></tr></table></figure><p>ä¹Ÿå¯ä»¥åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥å®ç°ï¼škeras.backend.function(inputs, outputs, updates=<strong>None</strong>)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># è¿™æ˜¯å†™center-lossæ—¶å†™çš„æ —å­ï¼š</span></span><br><span class="line">func = K.function(inputs=[model.input[<span class="number">0</span>]],          </span><br><span class="line">                  outputs=[model.get_layer(<span class="string">'out1'</span>).output]) </span><br><span class="line"><span class="comment"># model.input[0]: one input of the multi-input model</span></span><br><span class="line"></span><br><span class="line">test_features = func([x_test])[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>7.6 K.gradients(y,x)</p><p>æ±‚yå…³äºxçš„å¯¼æ•°ï¼Œyå’Œxå¯ä»¥æ˜¯å¼ é‡ï¼å¼ é‡åˆ—è¡¨ã€‚è¿”å›å¼ é‡åˆ—è¡¨ï¼Œåˆ—è¡¨é•¿åº¦åŒxåˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­å…ƒç´ shapeåŒxåˆ—è¡¨ä¸­å…ƒç´ ã€‚</p><p>å¯¹äº$y=[y_1, y_2], x=[x_1, x_2, x_3]$ï¼Œæœ‰è¿”å›å€¼$[grad_1, grad_2, grad_3]$ï¼ŒçœŸå®çš„è®¡ç®—è¿‡ç¨‹ä¸ºï¼š</p><script type="math/tex; mode=display">grad_1 = \frac{\partial y_1}{\partial x_1} + \frac{\partial y_2}{\partial x_1} \\grad_2 = \frac{\partial y_1}{\partial x_2} + \frac{\partial y_2}{\partial x_2} \\grad_3 = \frac{\partial y_1}{\partial x_3} + \frac{\partial y_2}{\partial x_3}</script><p>7.7  ModelCheckpointã€ReduceLROnPlateauã€EarlyStoppingã€LearningRateSchedulerã€Tensorboard</p><ul><li><p>æ¨¡å‹æ£€æŸ¥ç‚¹ModelCheckpoint</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">ModelCheckpoint(filepath, monitor=<span class="string">'val_loss'</span>, verbose=<span class="number">0</span>, save_best_only=<span class="keyword">False</span>, save_weights_only=<span class="keyword">False</span>, mode=<span class="string">'auto'</span>, period=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><ul><li>filepathå¯ä»¥ç”± <code>epoch</code> çš„å€¼å’Œ <code>logs</code> çš„é”®æ¥å¡«å……ï¼Œå¦‚weights.{epoch:02d}-{val_loss:.2f}.hdf5ã€‚</li><li>moniterï¼šè¢«ç›‘æµ‹çš„æ•°æ®</li><li>modeï¼šåœ¨ <code>auto</code> æ¨¡å¼ä¸­ï¼Œæ–¹å‘ä¼šè‡ªåŠ¨ä»è¢«ç›‘æµ‹çš„æ•°æ®çš„<strong>åå­—</strong>(ä¸é è°±ğŸ¤·â€â™€ï¸)ä¸­åˆ¤æ–­å‡ºæ¥ã€‚</li></ul></li><li><p>å­¦ä¹ ç‡è¡°å‡ReduceLROnPlateau</p><p>  å­¦ä¹ ç‡çš„æ–¹æ¡ˆç›¸å¯¹ç®€å•ï¼Œè¦ä¹ˆåœ¨éªŒè¯é›†çš„æŸå¤±æˆ–å‡†ç¡®ç‡å¼€å§‹ç¨³å®šæ—¶è°ƒä½å­¦ä¹ ç‡ï¼Œè¦ä¹ˆåœ¨å›ºå®šé—´éš”ä¸Šè°ƒä½å­¦ä¹ ç‡ã€‚</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ReduceLROnPlateau(monitor=<span class="string">'val_loss'</span>, factor=<span class="number">0.1</span>, patience=<span class="number">10</span>, verbose=<span class="number">0</span>, mode=<span class="string">'auto'</span>, min_delta=<span class="number">0.0001</span>, cooldown=<span class="number">0</span>, min_lr=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>  å½“å­¦ä¹ åœæ­¢æ—¶ï¼Œæ¨¡å‹æ€»æ˜¯ä¼šå—ç›Šäºé™ä½ 2-10 å€çš„å­¦ä¹ é€Ÿç‡ã€‚</p><ul><li>moniterï¼šè¢«ç›‘æµ‹çš„æ•°æ®</li><li>factorï¼šæ–°çš„å­¦ä¹ é€Ÿç‡ = å­¦ä¹ é€Ÿç‡ * factor</li><li>patienceï¼šè¢«ç›‘æµ‹æ•°æ®æ²¡æœ‰è¿›æ­¥çš„è®­ç»ƒè½®æ•°ï¼Œåœ¨è¿™ä¹‹åè®­ç»ƒé€Ÿç‡ä¼šè¢«é™ä½ã€‚</li></ul></li><li><p>æ›´å¤æ‚çš„å­¦ä¹ ç‡å˜åŒ–æ¨¡å¼å®šä¹‰LearningRateScheduler</p><p>  å‰ææ˜¯åªéœ€è¦ç”¨åˆ°é»˜è®¤å‚æ•°æ˜¯epoch</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># é¦–å…ˆå®šä¹‰ä¸€ä¸ªå˜åŒ–æ¨¡å¼</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">warmup_scheduler</span><span class="params">(epoch, mode=<span class="string">'power_decay'</span>)</span>:</span></span><br><span class="line">lr_base = <span class="number">1e-5</span></span><br><span class="line">lr_stable = <span class="number">1e-4</span></span><br><span class="line"></span><br><span class="line">lr = lr_base * math.pow(<span class="number">10</span>, epoch)</span><br><span class="line"><span class="keyword">if</span> lr&gt;lr_stable:</span><br><span class="line"><span class="keyword">return</span> lr_stable</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> lr</span><br><span class="line">  </span><br><span class="line"><span class="comment"># ç„¶åè°ƒç”¨LearningRateScheduleræ–¹æ³•wrapperè¿™ä¸ªscheduler</span></span><br><span class="line">scheduler = LearningRateScheduler(warmup_scheduler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨ä½¿ç”¨çš„æ—¶å€™æ”¾åœ¨callbacksçš„listé‡Œé¢ï¼Œåœ¨æ¯ä¸ªepochç»“æŸè§¦å‘</span></span><br><span class="line">callbacks = [checkpoint, reduce_lr, scheduler, early_stopping]</span><br></pre></td></tr></table></figure></li><li><p>æ›´æ›´å¤æ‚çš„å­¦ä¹ ç‡å˜åŒ–æ¨¡å¼å®šä¹‰å¯ä»¥ç›´æ¥ç»§æ‰¿Callbackï¼š<a href="https://kexue.fm/archives/5765" target="_blank" rel="noopener">https://kexue.fm/archives/5765</a></p><ul><li>å½“æˆ‘ä»¬éœ€è¦ä¼ å…¥æ›´ä¸°å¯Œçš„è‡ªå®šä¹‰å‚æ•°/éœ€è¦è¿›è¡Œby stepçš„å‚æ•°æ›´æ–°ç­‰ï¼Œå¯ä»¥ç›´æ¥ç»§æ‰¿Callbackï¼Œè¿›è¡Œæ›´è‡ªç”±çš„è‡ªå®šä¹‰</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ä»¥ä½™å¼¦é€€ç«ç®—æ³•ä¸ºä¾‹ï¼š</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CosineAnnealingScheduler</span><span class="params">(Callback)</span>:</span></span><br><span class="line">    <span class="string">"""Cosine annealing scheduler.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, epochs, scale=<span class="number">1.6</span>, shift=<span class="number">0</span>, verbose=<span class="number">0</span>)</span>:</span></span><br><span class="line">        super(CosineAnnealingScheduler, self).__init__()</span><br><span class="line">        self.epochs = epochs</span><br><span class="line">        self.scale = scale</span><br><span class="line">        self.shift = shift</span><br><span class="line">        self.verbose = verbose</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_begin</span><span class="params">(self, epoch, logs=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> epoch&lt;=<span class="number">6</span>:</span><br><span class="line">            <span class="comment"># linearly increase from 0 to 1.6 in first 5 epochs</span></span><br><span class="line">            lr = <span class="number">1.6</span> / <span class="number">5</span> * (epoch+<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># cosine annealing</span></span><br><span class="line">lr = self.shift + self.scale * (<span class="number">1</span> + math.cos(math.pi * (epoch+<span class="number">1</span><span class="number">-5</span>) / self.epochs)) / <span class="number">2</span></span><br><span class="line">        K.set_value(self.model.optimizer.lr, lr)</span><br><span class="line">        <span class="keyword">if</span> self.verbose &gt; <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'\nEpoch %05d: CosineAnnealingScheduler setting learning rate to %s.'</span> % (epoch+<span class="number">1</span>, lr))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span><span class="params">(self, epoch, logs=None)</span>:</span></span><br><span class="line">        logs = logs <span class="keyword">or</span> &#123;&#125;</span><br><span class="line">        logs[<span class="string">'lr'</span>] = K.get_value(self.model.optimizer.lr)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="comment"># è°ƒç”¨</span></span><br><span class="line">lrscheduler = CosineAnnealingScheduler(epochs=<span class="number">2</span>, verbose=<span class="number">1</span>)</span><br><span class="line">callbacks = [checkpoint, lrscheduler]</span><br><span class="line">model.fit(...,</span><br><span class="line">         callbacks=callbacks)</span><br></pre></td></tr></table></figure></li><li><p>æœ‰ä¸€äº›è®¡ç®—æŒ‡æ ‡ï¼Œä¸å¥½å†™æˆå¼ é‡å½¢å¼ï¼Œä¹Ÿå¯ä»¥æ”¾åœ¨Callbackå™¨é‡Œé¢ï¼Œæƒ³å’‹å†™å°±å’‹å†™</p><p>  è¯´ç™½äº†å°±æ˜¯on_epoch_endé‡Œé¢çš„æ•°æ®æ˜¯arrayï¼Œè€Œä¸æ˜¯tensorï¼Œæ¯”è¾ƒå¥½å†™</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> Callback</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰Callbackå™¨ï¼Œè®¡ç®—éªŒè¯é›†çš„accï¼Œå¹¶ä¿å­˜æœ€ä¼˜æ¨¡å‹</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Evaluate</span><span class="params">(Callback)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.accs = []</span><br><span class="line">        self.highest = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span><span class="params">(self, epoch, logs=None)</span>:</span></span><br><span class="line">      <span class="comment">###### è‡ªç”±å‘æŒ¥åŒºåŸŸ</span></span><br><span class="line">        pred = model.predict(x_test)</span><br><span class="line">        acc = np.mean(pred.argmax(axis=<span class="number">1</span>) == y_test)</span><br><span class="line">        <span class="comment">########</span></span><br><span class="line">        self.accs.append(acc)</span><br><span class="line">        <span class="keyword">if</span> acc &gt;= self.highest: <span class="comment"># ä¿å­˜æœ€ä¼˜æ¨¡å‹æƒé‡</span></span><br><span class="line">            self.highest = acc</span><br><span class="line">            model.save_weights(<span class="string">'best_model.weights'</span>)</span><br><span class="line"></span><br><span class="line">        print(<span class="string">'acc: %s, highest: %s'</span> % (acc, self.highest))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">evaluator = Evaluate()</span><br><span class="line">model.fit(x_train,</span><br><span class="line">          y_train,</span><br><span class="line">          epochs=<span class="number">10</span>,</span><br><span class="line">          callbacks=[evaluator])</span><br></pre></td></tr></table></figure></li><li><p>Callbackç±»å…±æ”¯æŒå…­ç§åœ¨ä¸åŒé˜¶æ®µçš„æ‰§è¡Œå‡½æ•°ï¼š</p><ul><li>on_epoch_beginï¼šwarmup</li><li>on_epoch_endï¼šmetrics</li><li>on_batch_begin</li><li>on_batch_end</li><li>on_train_begin</li><li>on_train_end</li></ul></li><li><p>æå‰åœæ­¢è®­ç»ƒEarlyStopping</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EarlyStopping(monitor=<span class="string">'val_loss'</span>, min_delta=<span class="number">0</span>, patience=<span class="number">0</span>, verbose=<span class="number">0</span>, mode=<span class="string">'auto'</span>, baseline=<span class="keyword">None</span>, restore_best_weights=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><ul><li>moniterï¼šè¢«ç›‘æµ‹çš„æ•°æ®</li><li>patienceï¼šè¢«ç›‘æµ‹æ•°æ®æ²¡æœ‰è¿›æ­¥çš„è®­ç»ƒè½®æ•°ï¼Œåœ¨è¿™ä¹‹åè®­ç»ƒé€Ÿç‡ä¼šè¢«é™ä½ã€‚</li><li>min_deltaï¼šåœ¨è¢«ç›‘æµ‹çš„æ•°æ®ä¸­è¢«è®¤ä¸ºæ˜¯æå‡çš„æœ€å°å˜åŒ–ï¼Œå°äº min_delta çš„ç»å¯¹å˜åŒ–ä¼šè¢«è®¤ä¸ºæ²¡æœ‰æå‡ã€‚</li><li>baseline: è¦ç›‘æ§çš„æ•°é‡çš„åŸºå‡†å€¼ã€‚</li></ul></li><li><p>ä»¥ä¸Šè¿™å››ä¸ªéƒ½æ˜¯ç»§æ‰¿è‡ªkeras.callbacks()</p></li><li><p>å¯è§†åŒ–å·¥å…·TensorBoard</p><p>  è¿™ä¸ªå›è°ƒå‡½æ•°ä¸º Tensorboard ç¼–å†™ä¸€ä¸ªæ—¥å¿—ï¼Œ è¿™æ ·ä½ å¯ä»¥å¯è§†åŒ–æµ‹è¯•å’Œè®­ç»ƒçš„æ ‡å‡†è¯„ä¼°çš„åŠ¨æ€å›¾åƒï¼Œ ä¹Ÿå¯ä»¥å¯è§†åŒ–æ¨¡å‹ä¸­ä¸åŒå±‚çš„æ¿€æ´»å€¼ç›´æ–¹å›¾ã€‚</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TensorBoard(log_dir=<span class="string">'./logs'</span>, histogram_freq=<span class="number">0</span>, batch_size=<span class="number">32</span>, write_graph=<span class="keyword">True</span>, write_grads=<span class="keyword">False</span>, write_images=<span class="keyword">False</span>, embeddings_freq=<span class="number">0</span>, embeddings_layer_names=<span class="keyword">None</span>, embeddings_metadata=<span class="keyword">None</span>, embeddings_data=<span class="keyword">None</span>, update_freq=<span class="string">'epoch'</span>)</span><br></pre></td></tr></table></figure><p>  å®é™…ä½¿ç”¨æ—¶å…³æ³¨ç¬¬ä¸€ä¸ªå‚æ•°log_dirå°±å¥½ï¼ŒæŸ¥çœ‹æ—¶é€šè¿‡å‘½ä»¤è¡Œå¯åŠ¨ï¼š</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=/full_path_to_your_logs</span><br></pre></td></tr></table></figure></li></ul><p><strong>è¿™å‡ ä¸ªå›è°ƒå‡½æ•°ï¼Œé€šé€šåœ¨è®­ç»ƒæ—¶ï¼ˆmodel.fit / fit_generatorï¼‰æ”¾åœ¨callbackså…³é”®å­—é‡Œé¢ã€‚</strong></p><p>7.8 åå·ç§¯ Conv2DTranspose</p><p>ä¸‰ä¸ªæ ¸å¿ƒçš„å‚æ•°filtesã€kernel_sizeã€stridesã€padding=â€™validâ€™</p><ul><li>filtesï¼šè¾“å‡ºé€šé“æ•°</li><li>stridesï¼šæ­¥é•¿</li><li>kernel_sizeï¼šä¸€èˆ¬éœ€è¦é€šè¿‡ä¸Šé¢ä¸¤é¡¹è®¡ç®—å¾—åˆ°</li></ul><p>åå·ç§¯è¿ç®—å’Œæ­£å‘å·ç§¯è¿ç®—ä¿æŒä¸€è‡´ï¼Œå³ï¼š</p><script type="math/tex; mode=display">(output\_shape - kernel\_size) / stride + 1 = input\_shape</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fcn example: current feature map x (,32,32,32), input_shape (512,512,2), output_shape (,512,512,1)</span></span><br><span class="line">strides = <span class="number">2</span></span><br><span class="line">kernel_size = input_shape[<span class="number">0</span>] - (x.get_shape().as_list()[<span class="number">1</span>] - <span class="number">1</span>)*strides</span><br><span class="line">y = Conv2DTranspose(<span class="number">1</span>, kernel_size, padding=<span class="string">'valid'</span>, strides=strides)</span><br></pre></td></tr></table></figure><p>7.9 K.shape &amp; K.int_shape &amp; tensor._keras_shape</p><ul><li>tensor._keras_shapeç­‰ä»·äºK.int_shapeï¼šå¼ é‡çš„shapeï¼Œè¿”å›å€¼æ˜¯ä¸ªtuple</li><li>K.shapeï¼šè¿”å›å€¼æ˜¯ä¸ªtensorï¼Œtensoræ˜¯ä¸ªä¸€ç»´å‘é‡ï¼Œå…¶ä¸­æ¯ä¸€ä¸ªå…ƒç´ å¯ä»¥ç”¨[i]æ¥è®¿é—®ï¼Œæ˜¯ä¸ªæ ‡é‡tensor</li></ul><p>ä¸¤ä¸ªæ–¹æ³•çš„ä¸»è¦åŒºåˆ«æ˜¯ï¼šå‰è€…è¿”å›å€¼æ˜¯ä¸ªå¸¸é‡ï¼Œåªèƒ½è¡¨å¾è¯­å¥æ‰§è¡Œæ—¶åˆ»ï¼ˆå¦‚æ„å»ºå›¾ï¼‰tensorçš„çŠ¶æ€ï¼Œåè€…è¿”å›å€¼æ˜¯ä¸ªå˜é‡ï¼Œwrapperçš„æ–¹æ³•å¯ä»¥çœ‹æˆä¸€ä¸ªèŠ‚ç‚¹ï¼Œåœ¨graphçš„ä½œç”¨åŸŸå†…å§‹ç»ˆæœ‰æ•ˆï¼Œåœ¨æ„å»ºå›¾çš„æ—¶å€™å¯ä»¥æ˜¯Noneï¼Œåœ¨å®é™…æµå…¥æ•°æ®æµçš„æ—¶å€™ä¼ å€¼å°±è¡Œï¼Œå¦‚batch_sizeï¼ï¼ï¼</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input</span><br><span class="line"></span><br><span class="line">x = Input((<span class="number">22</span>,<span class="number">22</span>,<span class="number">1</span>))</span><br><span class="line">print(K.shape(x))</span><br><span class="line"><span class="comment"># Tensor("Shape:0", shape=(4,), dtype=int32)</span></span><br><span class="line">print(K.shape(x)[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># Tensor("strided_slice:0", shape=(), dtype=int32)</span></span><br><span class="line"></span><br><span class="line">print(K.int_shape(x))</span><br><span class="line"><span class="comment"># (None, 22, 22, 1)</span></span><br></pre></td></tr></table></figure><p>7.10 binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0)</p><ul><li>from_logitsï¼šlogitsè¡¨ç¤ºç½‘ç»œçš„ç›´æ¥è¾“å‡ºâ€”â€”æ²¡ç»è¿‡sigmoidæˆ–è€…softmaxçš„æ¦‚ç‡åŒ–ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è®¤ä¸ºy_predæ˜¯å·²ç»å¤„ç†è¿‡çš„æ¦‚ç‡åˆ†å¸ƒ</li><li></li></ul><h3 id="8-è¡ç”Ÿï¼šä¸€äº›tfå‡½æ•°"><a href="#8-è¡ç”Ÿï¼šä¸€äº›tfå‡½æ•°" class="headerlink" title="8. è¡ç”Ÿï¼šä¸€äº›tfå‡½æ•°"></a>8. è¡ç”Ÿï¼šä¸€äº›tfå‡½æ•°</h3><p>8.1 tf.where(condition, x=None, y=None,name=None)</p><p>ä¸¤ç§ç”¨æ³•ï¼š</p><ul><li>å¦‚æœxï¼Œyä¸ºç©ºï¼Œè¿”å›å€¼æ˜¯æ»¡è¶³conditionå…ƒç´ çš„<strong>ç´¢å¼•</strong>ï¼Œæ¯ä¸ªç´¢å¼•å ä¸€è¡Œã€‚</li><li>å¦‚æœxï¼Œyä¸ä¸ºç©ºï¼Œé‚£ä¹ˆconditionã€xã€y å’Œè¿”å›å€¼ç›¸åŒç»´åº¦ï¼Œconditionä¸ºTrueçš„ä½ç½®æ›¿æ¢xä¸­å¯¹åº”å…ƒç´ ï¼Œconditionä¸ºFalseçš„ä½ç½®æ›¿æ¢yä¸­å¯¹åº”å…ƒç´ ã€‚</li></ul><p>å…³äºç´¢å¼•indicesï¼š</p><ul><li><p>conditionçš„shapeçš„dimï¼Œå°±æ˜¯æ¯ä¸€è¡Œç´¢å¼•vectorçš„shapeï¼Œä¾‹ï¼š</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">condition1 = np.array([[<span class="keyword">True</span>,<span class="keyword">False</span>,<span class="keyword">False</span>],[<span class="keyword">False</span>,<span class="keyword">True</span>,<span class="keyword">True</span>]])</span><br><span class="line">print(condition1.shape)    <span class="comment"># (2,3)</span></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(tf.where(condition1)))</span><br><span class="line"><span class="comment"># [[0 0]</span></span><br><span class="line"><span class="comment"># [1 1]</span></span><br><span class="line"><span class="comment"># [1 2]]</span></span><br><span class="line"><span class="comment"># conditionæ˜¯2x3çš„arrï¼Œä¹Ÿå°±æ˜¯dim=2ï¼Œé‚£ä¹ˆç´¢å¼•vectorçš„shapeå°±æ˜¯2ï¼Œçºµè½´çš„shapeæ˜¯æ»¡è¶³condçš„æ•°é‡</span></span><br></pre></td></tr></table></figure></li><li><p>ç´¢å¼•é€šå¸¸ä¸tf.gatherå’Œtf.gather_ndæ­é…ä½¿ç”¨ï¼š</p><ul><li>tf.gather(params,indices,axis=0.name=None)ï¼štf.gatheråªèƒ½æ¥å—1-Dçš„ç´¢å¼•ï¼Œaxisç”¨æ¥æŒ‡å®šè½´ï¼Œä¸€ä¸ªç´¢å¼•å–å›å¯¹åº”ç»´åº¦çš„ä¸€ä¸ªå‘é‡</li><li>tf.gather_nd(params,indices)ï¼štf.gather_ndå¯ä»¥æ¥å—å¤šç»´çš„ç´¢å¼•ï¼Œå¦‚æœç´¢å¼•çš„dimå°äºparamsçš„dimï¼Œåˆ™ä»axis=0å¼€å§‹ç´¢å¼•ï¼Œåé¢çš„å–å…¨éƒ¨ã€‚</li></ul></li></ul><p>8.2 tf.Print()</p><p>ç›¸å½“äºä¸€ä¸ªèŠ‚ç‚¹ï¼Œå®šä¹‰äº†æ•°æ®çš„æµå…¥å’Œæµå‡ºã€‚</p><p>ä¸€ä¸ªerrorï¼šåœ¨æ¨¡å‹å®šä¹‰ä¸­ï¼Œç›´æ¥è°ƒç”¨ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">intra_distance = tf.Print(intra_distance,</span><br><span class="line">                          [intra_distance],</span><br><span class="line">                          message=<span class="string">'Debug info: '</span>,</span><br><span class="line">                          summarize=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>ä¼šæŠ¥é”™ï¼šAttributeError: â€˜Tensorâ€™ object has no attribute â€˜_keras_historyâ€™</p><p>å‚è€ƒï¼š<a href="https://stackoverflow.com/questions/56096399/creating-model-throws-attributeerror-tensor-object-has-no-attribute-keras" target="_blank" rel="noopener">https://stackoverflow.com/questions/56096399/creating-model-throws-attributeerror-tensor-object-has-no-attribute-keras</a></p><blockquote><p>You cannot use backend functions directly in Keras tensors, every operation in these tensors must be a layer. You need to wrap each custom operation in a Lambda layer and provide the appropriate inputs to the layer.</p></blockquote><p>ä¹‹å‰ä¸€ç›´æ²¡æ³¨æ„åˆ°è¿™ä¸ªé—®é¢˜ï¼Œå‡¡æ˜¯è°ƒç”¨äº†tf.XXXçš„operationï¼Œéƒ½è¦wrapperåœ¨Lambdaå±‚é‡Œã€‚</p><p>æ”¹å†™ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># wrapper function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">debug</span><span class="params">(args)</span>:</span></span><br><span class="line">    intra_distance, min_inter_distance = args</span><br><span class="line">    intra_distance = tf.Print(intra_distance,</span><br><span class="line">                              [intra_distance],</span><br><span class="line">                              message=<span class="string">'Debug info: '</span>,</span><br><span class="line">                              summarize=<span class="number">10</span>)</span><br><span class="line">    min_inter_distance = tf.Print(min_inter_distance,</span><br><span class="line">                                  [min_inter_distance],</span><br><span class="line">                                  message=<span class="string">'Debug info: '</span>,</span><br><span class="line">                                  summarize=<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> [intra_distance, min_inter_distance]</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨¡å‹å†…</span></span><br><span class="line">intra_distance, min_inter_distance = Lambda(debug)([intra_distance, min_inter_distance])</span><br></pre></td></tr></table></figure><p>ã€å¤¹å¸¦ç§è´§ã€‘tf.PrintåŒæ—¶ä¹Ÿå¯ä»¥æ‰“å°wrapper functionå†…çš„ä¸­é—´å˜é‡ï¼Œéƒ½æ”¾åœ¨åˆ—è¡¨é‡Œé¢å°±å¯ä»¥äº†ã€‚</p><p>8.3 tf.while_loop(cond, body, init_value)</p><p>tensorflowä¸­å®ç°å¾ªç¯çš„è¯­å¥</p><ul><li>ç»ˆæ­¢æ¡ä»¶condï¼šæ˜¯ä¸€ä¸ªå‡½æ•°</li><li>å¾ªç¯ä½“bodyï¼šæ˜¯ä¸€ä¸ªå‡½æ•°</li><li>init_valueï¼šæ˜¯ä¸€ä¸ªlistï¼Œä¿å­˜å¾ªç¯ç›¸å…³å‚æ•°</li></ul><ol><li>condã€bodyçš„å‚æ•°æ˜¯è¦ä¸init_valueåˆ—è¡¨ä¸­å˜é‡ä¸€ä¸€å¯¹åº”çš„</li><li>bodyè¿”å›å€¼çš„æ ¼å¼è¦ä¸init_valueå˜é‡ä¸€è‡´ï¼ˆtensorå½¢çŠ¶ä¿æŒä¸å˜ï¼‰</li><li>è‹¥éè¦å˜æ€ä¹ˆåŠï¼ˆæœ‰æ—¶å€™æˆ‘ä»¬å¸Œæœ›åœ¨while_loopçš„è¿‡ç¨‹ä¸­ï¼Œç»´æŠ¤ä¸€ä¸ªlistï¼‰ï¼ŸåŠ¨æ€æ•°ç»„TensorArrayï¼é«˜çº§å‚æ•°shape_invariants</li></ol><p>8.3.1 åŠ¨æ€æ•°ç»„</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># å®šä¹‰</span></span><br><span class="line">b_boxes = tf.TensorArray(K.dtype(boxes), size=<span class="number">1</span>, dynamic_size=<span class="keyword">True</span>, clear_after_read=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å†™å…¥æŒ‡å®šä½ç½®</span></span><br><span class="line">b_boxes = b_boxes.write(b, boxes_)</span><br></pre></td></tr></table></figure><p>â€‹    tensor arrayå˜é‡ä¸­ä¸€ä¸ªä½ç½®åªèƒ½å†™å…¥ä¸€æ¬¡</p><p>8.3.2 shape_invariants</p><p>â€‹    <a href="https://stackoverflow.com/questions/41233462/tensorflow-while-loop-dealing-with-lists" target="_blank" rel="noopener">reference stackoverflow</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">i = tf.constant(<span class="number">0</span>)</span><br><span class="line">l = tf.Variable([])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">body</span><span class="params">(i, l)</span>:</span>                                               </span><br><span class="line">    temp = tf.gather(array,i)</span><br><span class="line">    l = tf.concat([l, [temp]], <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> i+<span class="number">1</span>, l</span><br><span class="line"></span><br><span class="line">index, list_vals = tf.while_loop(cond, body, [i, l],</span><br><span class="line">                                 shape_invariants=[i.get_shape(), tf.TensorShape([<span class="keyword">None</span>])])</span><br></pre></td></tr></table></figure><p>â€‹    åœ¨while_loopä¸­æ˜¾ç¤ºåœ°æŒ‡å®šå‚æ•°çš„shapeï¼Œä¸Šé¢çš„ä¾‹å­ç”¨äº†tf.TensorShape([None])ä»¤å…¶è‡ªåŠ¨æ¨æ–­ï¼Œè€Œä¸æ˜¯å›ºå®šæ£€æŸ¥ï¼Œå› æ­¤å¯ä»¥è§£å†³å˜åŒ–é•¿åº¦åˆ—è¡¨ã€‚</p><p>ä¸€ä¸ªå®Œæ•´çš„æ —å­ï¼šç¬¬ä¸€æ¬¡è§while_loopï¼Œåœ¨yolo_lossé‡Œé¢</p><ol><li>åŸºäºbatchç»´åº¦åšéå†</li><li>loopç»“æŸåå°†åŠ¨æ€æ•°æ®stackèµ·æ¥ï¼Œé‡è·batch dim</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Find ignore mask, iterate over each of batch.</span></span><br><span class="line"><span class="comment"># extract the elements on the mask which has iou &lt; ignore_thresh</span></span><br><span class="line">ignore_mask = tf.TensorArray(K.dtype(y_true[<span class="number">0</span>]), size=<span class="number">1</span>, dynamic_size=<span class="keyword">True</span>)  <span class="comment"># åŠ¨æ€sizeæ•°ç»„</span></span><br><span class="line">object_mask_bool = K.cast(object_mask, <span class="string">'bool'</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loop_body</span><span class="params">(b, ignore_mask)</span>:</span></span><br><span class="line">    true_box = tf.boolean_mask(y_true[l][b,...,<span class="number">0</span>:<span class="number">4</span>], object_mask_bool[b,...,<span class="number">0</span>])   <span class="comment"># (H,W,3,5)</span></span><br><span class="line">    iou = box_iou(pred_box[b], true_box)     <span class="comment"># (H,W,3,1)</span></span><br><span class="line">    best_iou = K.max(iou, axis=<span class="number">-1</span>)</span><br><span class="line">    ignore_mask = ignore_mask.write(b, K.cast(best_iou&lt;ignore_thresh, K.dtype(true_box)))</span><br><span class="line">    <span class="keyword">return</span> b+<span class="number">1</span>, ignore_mask</span><br><span class="line">_, ignore_mask = K.control_flow_ops.while_loop(<span class="keyword">lambda</span> b,*args: b&lt;m, loop_body, [<span class="number">0</span>, ignore_mask])</span><br><span class="line">ignore_mask = ignore_mask.stack()</span><br><span class="line">ignore_mask = K.expand_dims(ignore_mask, <span class="number">-1</span>)     <span class="comment"># ï¼ˆN,H,W,3,1ï¼‰</span></span><br></pre></td></tr></table></figure><p>8.4 tf.image.non_max_suppression()</p><p>éæœ€å¤§å€¼æŠ‘åˆ¶ï¼šè´ªå©ªç®—æ³•ï¼ŒæŒ‰scoresç”±å¤§åˆ°å°æ’åºï¼Œé€‰å®šç¬¬ä¸€ä¸ªï¼Œä¾æ¬¡å¯¹ä¹‹åçš„æ¡†æ±‚iouï¼Œåˆ é™¤é‚£äº›å’Œé€‰å®šæ¡†iouå¤§äºé˜ˆå€¼çš„boxã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># è¿”å›æ˜¯è¢«é€‰ä¸­è¾¹æ¡†åœ¨å‚æ•°boxesä¸­çš„ä¸‹æ ‡ä½ç½®</span></span><br><span class="line">selected_indices=tf.image.non_max_suppression(boxes, scores, max_output_size, iou_threshold=<span class="number">0.5</span>, name=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ ¹æ®indicesè·å–è¾¹æ¡†</span></span><br><span class="line">selected_boxes=tf.gather(boxes,selected_indices)</span><br></pre></td></tr></table></figure><ul><li>boxesï¼š2-Dçš„floatç±»å‹çš„ï¼Œå¤§å°ä¸º[num_boxes,4]çš„å¼ é‡</li><li>scoresï¼š1-Dçš„floatç±»å‹çš„ï¼Œå¤§å°ä¸º[num_boxes]ï¼Œå¯¹åº”çš„æ¯ä¸€ä¸ªboxçš„ä¸€ä¸ªscore</li><li>max_output_sizeï¼šæ ‡é‡æ•´æ•°Tensorï¼Œè¾“å‡ºæ¡†çš„æœ€å¤§æ•°é‡</li><li>iou_thresholdï¼šæµ®ç‚¹æ•°ï¼ŒIOUé˜ˆå€¼</li><li>selected_indicesï¼š1-Dçš„æ•´æ•°å¼ é‡ï¼Œå¤§å°ä¸º[M]ï¼Œç•™ä¸‹æ¥çš„è¾¹æ¡†ä¸‹æ ‡ï¼ŒMå°äºç­‰äºmax_output_size</li></ul><p>ã€æ‹“å±•ã€‘è¿˜æœ‰Multi-class version of NMSâ€”â€”tf.multiclass_non_max_suppression()</p><p>8.5 é™åˆ¶GPUç”¨é‡</p><ul><li><p>linuxä¸‹æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µï¼Œ1ç§’åˆ·æ–°ä¸€æ¬¡ï¼š</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">watch -n 1 nvidia-smi</span><br></pre></td></tr></table></figure></li><li><p>æŒ‡å®šæ˜¾å¡å·</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"2"</span></span><br></pre></td></tr></table></figure></li><li><p>é™åˆ¶GPUç”¨é‡</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line"></span><br><span class="line"><span class="comment"># è®¾ç½®ç™¾åˆ†æ¯”</span></span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.per_process_gpu_memory_fraction = <span class="number">0.3</span></span><br><span class="line">session = tf.Session(config=config)</span><br><span class="line">K.set_session(session)</span><br><span class="line"></span><br><span class="line"><span class="comment"># è®¾ç½®åŠ¨æ€ç”³è¯·</span></span><br><span class="line">config = tf.ConfigProto()  </span><br><span class="line">config.gpu_options.allow_growth=<span class="keyword">True</span>   <span class="comment">#ä¸å…¨éƒ¨å æ»¡æ˜¾å­˜, æŒ‰éœ€åˆ†é…</span></span><br><span class="line">session = tf.Session(config=config)</span><br><span class="line">K.set_session(session)</span><br></pre></td></tr></table></figure></li></ul><p>8.6 tf.boolean_mask()</p><p>tf.boolean_mask(tensor,mask,name=â€™boolean_maskâ€™,axis=None)</p><p>å…¶ä¸­ï¼Œtensoræ˜¯Nç»´åº¦çš„ï¼Œmaskæ˜¯Kç»´åº¦çš„ï¼Œ$K \leq N$</p><p>axisè¡¨ç¤ºmaskçš„èµ·å§‹ç»´åº¦ï¼Œè¢«maskçš„ç»´åº¦åªä¿ç•™maskä¸ºTrueçš„æ•°æ®ï¼ŒåŒæ—¶è¿™éƒ¨åˆ†æ•°æ®flattenæˆä¸€ç»´ï¼Œæœ€ç»ˆtensorçš„ç»´åº¦æ˜¯N-K+1</p><p>æ —å­ï¼šyolov3é‡Œé¢ï¼ŒæŠŠç‰¹å¾å›¾ä¸Šæœ‰objectçš„gridæå–å‡ºæ¥ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># y_trues: [b,h,w,a,4]</span></span><br><span class="line"><span class="comment"># conf_gt: [b,h,w,a,1]</span></span><br><span class="line">true_box = tf.boolean_mask(y_trues[i][b,...,<span class="number">0</span>:<span class="number">4</span>], conf_gt[b,...,<span class="number">0</span>])</span><br></pre></td></tr></table></figure><h2 id="9-kerasè‡ªå®šä¹‰ä¼˜åŒ–å™¨optimizer"><a href="#9-kerasè‡ªå®šä¹‰ä¼˜åŒ–å™¨optimizer" class="headerlink" title="9. kerasè‡ªå®šä¹‰ä¼˜åŒ–å™¨optimizer"></a>9. kerasè‡ªå®šä¹‰ä¼˜åŒ–å™¨optimizer</h2><p>9.1 å…³äºæ¢¯åº¦çš„ä¼˜åŒ–å™¨å…¬å…±å‚æ•°ï¼Œç”¨äºæ¢¯åº¦è£å‰ª</p><ul><li>clipnormï¼šå¯¹æ‰€æœ‰æ¢¯åº¦è¿›è¡Œdownscaleï¼Œä½¿å¾—æ¢¯åº¦vectorä¸­l2èŒƒæ•°æœ€å¤§ä¸º1ï¼ˆg * 1 / max(1, l2_norm)ï¼‰</li><li>clipvalueï¼šå¯¹ç»å¯¹å€¼è¿›è¡Œä¸Šä¸‹é™æˆªæ–­</li></ul><p>9.2 kerasçš„Optimizierå¯¹è±¡</p><ul><li>kerasçš„å®˜æ–¹ä»£ç æœ‰optimizier_v1å’Œoptimizier_v2ä¸¤ç‰ˆï¼Œåˆ†åˆ«é¢å‘tf1å’Œtf2ï¼Œv1çš„çœ‹èµ·æ¥ç®€æ´ä¸€äº›</li><li><a href="https://stackoverflow.com/questions/56806419/keras-how-to-reset-optimizer-state/56807007#56807007" target="_blank" rel="noopener">self.updates &amp; self.weights</a><ul><li>self.updatesï¼šstores the variables that will be updated with every batch that is processed by the model in training<ul><li>ç”¨æ¥ä¿å­˜ä¸æ¨¡å‹è®­ç»ƒç›¸å…³çš„å‚æ•°ï¼ˆiterationsã€paramsã€momentsã€accumulatorsï¼Œetcï¼‰</li><li>symbolic graph variableï¼Œé€šè¿‡K.update_addæ–¹æ³•è¯´æ˜å›¾çš„operation</li></ul></li><li>self.weightsï¼šthe functions that save and load optimizers will save and load this property<ul><li>ç”¨æ¥ä¿å­˜ä¸ä¼˜åŒ–å™¨ç›¸å…³çš„å‚æ•°</li><li>model.save()æ–¹æ³•ä¸­æ¶‰åŠinclude_optimizer=Falseï¼Œå†³å®šä¼˜åŒ–å™¨çš„ä¿å­˜å’Œé‡è½½</li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Optimizer</span><span class="params">(object)</span>:</span></span><br><span class="line"><span class="comment"># - æŠ½è±¡ç±»ï¼Œæ‰€æœ‰çœŸå®çš„ä¼˜åŒ–å™¨ç»§æ‰¿è‡ªOptimizerå¯¹è±¡</span></span><br><span class="line"><span class="comment"># - æä¾›ä¸¤ä¸ªç”¨äºæ¢¯åº¦æˆªæ–­çš„å…¬å…±å‚æ•°</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">    allowed_kwargs = &#123;<span class="string">'clipnorm'</span>, <span class="string">'clipvalue'</span>&#125;</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> kwargs:</span><br><span class="line">      <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> allowed_kwargs:</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">'Unexpected keyword argument passed to optimizer: '</span> + str(k))</span><br><span class="line">      <span class="comment"># checks that clipnorm &gt;= 0 and clipvalue &gt;= 0</span></span><br><span class="line">      <span class="keyword">if</span> kwargs[k] &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Expected &#123;&#125; &gt;= 0, received: &#123;&#125;'</span>.format(k, kwargs[k]))</span><br><span class="line">    self.__dict__.update(kwargs)</span><br><span class="line">    self.updates = []   <span class="comment"># è®¡ç®—æ›´æ–°çš„å‚æ•°</span></span><br><span class="line">    self.weights = []   <span class="comment"># ä¼˜åŒ–å™¨å¸¦æ¥çš„æƒé‡ï¼Œåœ¨get_updatesä»¥åæ‰æœ‰å…ƒç´ ï¼Œåœ¨ä¿å­˜æ¨¡å‹æ—¶ä¼šè¢«ä¿å­˜</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Set this to False, indicating `apply_gradients` does not take the</span></span><br><span class="line">  <span class="comment"># `experimental_aggregate_gradients` argument.</span></span><br><span class="line">  _HAS_AGGREGATE_GRAD = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_create_all_weights</span><span class="params">(self, params)</span>:</span></span><br><span class="line">    <span class="comment"># å£°æ˜é™¤äº†gradsä»¥å¤–ç”¨äºæ¢¯åº¦æ›´æ–°çš„å‚æ•°ï¼Œåˆ›å»ºå†…å­˜ç©ºé—´ï¼Œåœ¨get_updatesæ–¹æ³•ä¸­ä½¿ç”¨</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_updates</span><span class="params">(self, loss, params)</span>:</span></span><br><span class="line">    <span class="comment"># å®šä¹‰æ¢¯åº¦æ›´æ–°çš„è®¡ç®—æ–¹æ³•, æ›´æ–°self.updates</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">   </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># configé‡Œé¢æ˜¯ä¼˜åŒ–å™¨ç›¸å…³çš„å‚æ•°ï¼Œé»˜è®¤åªæœ‰ä¸¤ä¸ªæ¢¯åº¦æˆªæ–­çš„å‚æ•°ï¼Œéœ€è¦æ ¹æ®å®é™…ä¼˜åŒ–å™¨æ·»åŠ ï¼ˆlrã€decay ...ï¼‰</span></span><br><span class="line">    config = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> hasattr(self, <span class="string">'clipnorm'</span>):</span><br><span class="line">      config[<span class="string">'clipnorm'</span>] = self.clipnorm</span><br><span class="line">    <span class="keyword">if</span> hasattr(self, <span class="string">'clipvalue'</span>):</span><br><span class="line">      config[<span class="string">'clipvalue'</span>] = self.clipvalue</span><br><span class="line">    <span class="keyword">return</span> config</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_gradients</span><span class="params">(self, loss, params)</span>:</span></span><br><span class="line">    <span class="comment"># è®¡ç®—æ¢¯åº¦å€¼ï¼Œå¹¶åœ¨æœ‰å¿…è¦æ—¶è¿›è¡Œæ¢¯åº¦æˆªæ–­</span></span><br><span class="line">    grads = K.gradients(loss, params)</span><br><span class="line">    <span class="keyword">if</span> any(g <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">for</span> g <span class="keyword">in</span> grads):</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">'An operation has `None` for gradient. '</span></span><br><span class="line">                       <span class="string">'Please make sure that all of your ops have a '</span></span><br><span class="line">                       <span class="string">'gradient defined (i.e. are differentiable). '</span></span><br><span class="line">                       <span class="string">'Common ops without gradient: '</span></span><br><span class="line">                       <span class="string">'K.argmax, K.round, K.eval.'</span>)</span><br><span class="line">    <span class="keyword">if</span> hasattr(self, <span class="string">'clipnorm'</span>):</span><br><span class="line">      grads = [tf.clip_by_norm(g, self.clipnorm) <span class="keyword">for</span> g <span class="keyword">in</span> grads]</span><br><span class="line">    <span class="keyword">if</span> hasattr(self, <span class="string">'clipvalue'</span>):</span><br><span class="line">      grads = [</span><br><span class="line">          tf.clip_by_value(g, -self.clipvalue, self.clipvalue)</span><br><span class="line">          <span class="keyword">for</span> g <span class="keyword">in</span> grads</span><br><span class="line">      ]</span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">set_weights</span><span class="params">(self, weights)</span>:</span></span><br><span class="line"><span class="comment"># ç»™optimizerçš„weightsç”¨ä¸€ç³»åˆ—np arrayèµ‹å€¼</span></span><br><span class="line">    <span class="comment"># æ²¡çœ‹åˆ°æœ‰è°ƒç”¨ï¼Œçœç•¥codeï¼š K.batch_set_value()</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># è·å–weightsçš„np arrayå€¼</span></span><br><span class="line">    <span class="comment"># æ²¡çœ‹åˆ°æœ‰è°ƒç”¨ï¼Œçœç•¥codeï¼š K.batch_get_value()</span></span><br><span class="line"></span><br><span class="line"><span class="meta">  @classmethod</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">from_config</span><span class="params">(cls, config)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> cls(**config)</span><br></pre></td></tr></table></figure><p>9.3 å®ä¾‹åŒ–ä¸€ä¸ªä¼˜åŒ–å™¨</p><ul><li>based on keras.Optimizerå¯¹è±¡</li><li>ä¸»è¦éœ€è¦é‡å†™get_updateså’Œget_configæ–¹æ³•<ul><li>get_updatesç”¨æ¥å®šä¹‰æ¢¯åº¦æ›´æ–°çš„è®¡ç®—æ–¹æ³•</li><li>get_configç”¨æ¥å®šä¹‰å®ä¾‹ç”¨åˆ°çš„å‚æ•°</li></ul></li><li>ä»¥SoftSGDä¸ºä¾‹ï¼š<ul><li>æ¯éš”ä¸€å®šçš„batchæ‰æ›´æ–°ä¸€æ¬¡å‚æ•°ï¼Œä¸æ›´æ–°æ¢¯åº¦çš„stepæ¢¯åº¦ä¸æ¸…ç©ºï¼Œæ‰§è¡Œç´¯åŠ ï¼Œä»è€Œå®ç°batchsizeçš„å˜ç›¸æ‰©å¤§</li><li>å»ºè®®æ­é…é—´éš”æ›´æ–°å‚æ•°çš„BNå±‚æ¥ä½¿ç”¨ï¼Œå¦åˆ™BNè¿˜æ˜¯åŸºäºå°batchsizeæ¥æ›´æ–°å‡å€¼å’Œæ–¹å·®</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SoftSGD</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line">    <span class="comment"># [new arg] steps_per_update: how many batch to update gradient</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, lr=<span class="number">0.01</span>, momentum=<span class="number">0.</span>, decay=<span class="number">0.</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 nesterov=False, steps_per_update=<span class="number">2</span>, **kwargs)</span>:</span></span><br><span class="line">        super(SoftSGD, self).__init__(**kwargs)</span><br><span class="line">        <span class="keyword">with</span> K.name_scope(self.__class__.__name__):</span><br><span class="line">            self.iterations = K.variable(<span class="number">0</span>, dtype=<span class="string">'int64'</span>, name=<span class="string">'iterations'</span>)</span><br><span class="line">            self.lr = K.variable(lr, name=<span class="string">'lr'</span>)</span><br><span class="line">            self.steps_per_update = steps_per_update  <span class="comment"># å¤šå°‘batchæ‰æ›´æ–°ä¸€æ¬¡</span></span><br><span class="line">            self.momentum = K.variable(momentum, name=<span class="string">'momentum'</span>)</span><br><span class="line">            self.decay = K.variable(decay, name=<span class="string">'decay'</span>)</span><br><span class="line">        self.initial_decay = decay</span><br><span class="line">        self.nesterov = nesterov</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_updates</span><span class="params">(self, loss, params)</span>:</span></span><br><span class="line">        <span class="comment"># learning rate decay</span></span><br><span class="line">        lr = self.lr</span><br><span class="line">        <span class="keyword">if</span> self.initial_decay &gt; <span class="number">0</span>:</span><br><span class="line">            lr = lr * (<span class="number">1.</span> / (<span class="number">1.</span> + self.decay * K.cast(self.iterations, K.dtype(self.decay))))</span><br><span class="line"> </span><br><span class="line">        shapes = [K.int_shape(p) <span class="keyword">for</span> p <span class="keyword">in</span> params]</span><br><span class="line">        sum_grads = [K.zeros(shape) <span class="keyword">for</span> shape <span class="keyword">in</span> shapes]  <span class="comment"># å¹³å‡æ¢¯åº¦ï¼Œç”¨æ¥æ¢¯åº¦ä¸‹é™</span></span><br><span class="line">        grads = self.get_gradients(loss, params)  <span class="comment"># å½“å‰batchæ¢¯åº¦</span></span><br><span class="line">        self.updates = [K.update_add(self.iterations, <span class="number">1</span>)]</span><br><span class="line">        self.weights = [self.iterations] + sum_grads</span><br><span class="line">        <span class="keyword">for</span> p, g, sg <span class="keyword">in</span> zip(params, grads, sum_grads):</span><br><span class="line">            <span class="comment"># momentum æ¢¯åº¦ä¸‹é™</span></span><br><span class="line">            v = self.momentum * sg / float(self.steps_per_update) - lr * g  <span class="comment"># velocity</span></span><br><span class="line">            <span class="keyword">if</span> self.nesterov:</span><br><span class="line">                new_p = p + self.momentum * v - lr * sg / float(self.steps_per_update)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                new_p = p + v</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># å¦‚æœæœ‰çº¦æŸï¼Œå¯¹å‚æ•°åŠ ä¸Šçº¦æŸ</span></span><br><span class="line">            <span class="keyword">if</span> getattr(p, <span class="string">'constraint'</span>, <span class="keyword">None</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                new_p = p.constraint(new_p)</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># æ»¡è¶³æ¡ä»¶æ‰æ›´æ–°å‚æ•°</span></span><br><span class="line">            cond = K.equal(self.iterations % self.steps_per_update, <span class="number">0</span>)</span><br><span class="line">            self.updates.append(K.switch(cond, K.update(p, new_p), p))</span><br><span class="line">            self.updates.append(K.switch(cond, K.update(sg, g), K.update(sg, sg + g)))</span><br><span class="line">        <span class="keyword">return</span> self.updates</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self)</span>:</span></span><br><span class="line">        config = &#123;<span class="string">'lr'</span>: float(K.get_value(self.lr)),</span><br><span class="line">                  <span class="string">'steps_per_update'</span>: self.steps_per_update,</span><br><span class="line">                  <span class="string">'momentum'</span>: float(K.get_value(self.momentum)),</span><br><span class="line">                  <span class="string">'decay'</span>: float(K.get_value(self.decay)),</span><br><span class="line">                  <span class="string">'nesterov'</span>: self.nesterov</span><br><span class="line">                  &#125;</span><br><span class="line">        base_config = super(SoftSGD, self).get_config()</span><br><span class="line">        <span class="keyword">return</span> dict(list(base_config.items()) + list(config.items()))</span><br></pre></td></tr></table></figure><h2 id="10-kerasè‡ªå®šä¹‰æ¿€æ´»å‡½æ•°activation"><a href="#10-kerasè‡ªå®šä¹‰æ¿€æ´»å‡½æ•°activation" class="headerlink" title="10. kerasè‡ªå®šä¹‰æ¿€æ´»å‡½æ•°activation"></a>10. kerasè‡ªå®šä¹‰æ¿€æ´»å‡½æ•°activation</h2><p>10.1 å®šä¹‰æ¿€æ´»å‡½æ•°</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gelu</span><span class="params">(x)</span>:</span></span><br><span class="line">    cdf = <span class="number">0.5</span> * (<span class="number">1.0</span> + tf.erf(x / tf.sqrt(<span class="number">2.0</span>)))</span><br><span class="line">    <span class="keyword">return</span> x*cdf</span><br></pre></td></tr></table></figure><p>10.2 ä½¿ç”¨è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°</p><ul><li><p>ä½¿ç”¨Activationæ–¹æ³•</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = Activation(gelu)(x)</span><br></pre></td></tr></table></figure></li><li><p>ä¸èƒ½æ•´åˆè¿›å¸¦æœ‰activationå‚æ•°çš„å±‚ï¼ˆå¦‚Conv2Dï¼‰ï¼Œå› ä¸ºConvåŸºç±»çš„get_config()æ–¹æ³•ä»keras.activationsé‡Œé¢è¯»å–ç›¸åº”çš„æ¿€æ´»å‡½æ•°ï¼Œå…¶ä¸­å¸¦å‚æ•°çš„æ¿€æ´»å‡½æ•°å¦‚PReLUï¼ˆAdvanced activationsï¼‰ã€ä»¥åŠè‡ªå®šä¹‰çš„æ¿€æ´»å‡½æ•°éƒ½ä¸åœ¨è¿™ä¸ªå­—å…¸ä¸­ï¼Œå¦åˆ™ä¼šæŠ¥é”™ï¼š</p><p>  AttributeError: â€˜Activationâ€™ object has no attribute â€˜<strong>name</strong>â€˜</p></li></ul><p>10.3 checkpoint issue</p><p>ç½‘ä¸Šè¿˜æœ‰å¦ä¸€ç§å†™æ³•ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Activation</span><br><span class="line"><span class="keyword">from</span> keras.utils.generic_utils <span class="keyword">import</span> get_custom_objects</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gelu</span><span class="params">(x)</span>:</span></span><br><span class="line">    cdf = <span class="number">0.5</span> * (<span class="number">1.0</span> + tf.erf(x / tf.sqrt(<span class="number">2.0</span>)))</span><br><span class="line">    <span class="keyword">return</span> x*cdf</span><br><span class="line">  </span><br><span class="line">get_custom_objects().update(&#123;<span class="string">'gelu'</span>: Activation(gelu)&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># åé¢å¯ä»¥é€šè¿‡åå­—è°ƒç”¨æ¿€æ´»å‡½æ•°</span></span><br><span class="line">x = Activation(<span class="string">'gelu'</span>)(x)</span><br></pre></td></tr></table></figure><p>è¿™ç§å†™æ³•åœ¨ä½¿ç”¨ModelCheckpointsæ–¹æ³•ä¿å­˜æƒé‡æ—¶ä¼šæŠ¥é”™ï¼š</p><p>AttributeError: â€˜Activationâ€™ object has no attribute â€˜<strong>name</strong>â€˜</p><p>çœ‹logå‘ç°å½“ä½¿ç”¨åå­—ä»£è¡¨æ¿€æ´»å±‚çš„æ—¶å€™ï¼Œåœ¨ä¿å­˜æ¨¡å‹çš„æ—¶å€™ï¼Œåˆä¼šæœ‰ä¸€ä¸ªget_config()å‡½æ•°ä»keras.activationsä¸­æŸ¥è¡¨</p><h2 id="11-kerasè‡ªå®šä¹‰æ­£åˆ™åŒ–å™¨regularizers"><a href="#11-kerasè‡ªå®šä¹‰æ­£åˆ™åŒ–å™¨regularizers" class="headerlink" title="11. kerasè‡ªå®šä¹‰æ­£åˆ™åŒ–å™¨regularizers"></a>11. kerasè‡ªå®šä¹‰æ­£åˆ™åŒ–å™¨regularizers</h2><p>11.1 ä½¿ç”¨å°è£…å¥½çš„regularizers</p><ul><li><p>kerasçš„æ­£åˆ™åŒ–å™¨æ²¡æœ‰globalçš„ä¸€é”®æ·»åŠ æ–¹æ³•ï¼Œè¦layer-wiseä¸ºæ¯ä¸€å±‚æ·»åŠ </p></li><li><p>kerasçš„å±‚share 3 commonå‚æ•°æ¥å£ï¼š</p><ul><li>kernel_regularizer</li><li>bias_regularizer</li><li>activity_regularizer</li></ul></li><li><p>å¯é€‰ç”¨çš„æ­£åˆ™åŒ–å™¨</p><ul><li>keras.regularizers.l1(0.01) </li><li>keras.regularizers.l2(0.01) </li><li>keras.regularizers.l1_l2(l1=0.01, l2=0.01)</li></ul></li><li><p>ä½¿ç”¨</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">layer = tf.keras.layers.Dense(<span class="number">5</span>, kernel_initializer=<span class="string">'ones'</span>,</span><br><span class="line">                              kernel_regularizer=tf.keras.regularizers.l1(<span class="number">0.01</span>),</span><br><span class="line">                              activity_regularizer=tf.keras.regularizers.l2(<span class="number">0.01</span>))</span><br><span class="line">tensor = tf.ones(shape=(<span class="number">5</span>, <span class="number">5</span>)) * <span class="number">2.0</span></span><br><span class="line">out = layer(tensor)</span><br><span class="line"><span class="comment"># The kernel regularization term is 0.25</span></span><br><span class="line"><span class="comment"># The activity regularization term (after dividing by the batch size) is 5</span></span><br><span class="line">print(tf.math.reduce_sum(layer.losses))  <span class="comment"># 5.25 (= 5 + 0.25)</span></span><br></pre></td></tr></table></figure></li></ul><p>11.2 custom regularizer</p><p>ä¸€èˆ¬ä¸ä¼šè‡ªå®šä¹‰è¿™ä¸ªä¸œè¥¿ï¼Œç¡¬è¦customçš„è¯ï¼Œä¸¤ç§æ–¹å¼</p><ul><li>ç®€å•ç‰ˆï¼Œæ¥å£å‚æ•°æ˜¯weight_matrixï¼Œæ— é¢å¤–å‚æ•°ï¼Œå±‚ç›´æ¥è°ƒç”¨</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_regularizer</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1e-3</span> * tf.reduce_sum(tf.square(x))</span><br><span class="line"></span><br><span class="line">layer = tf.keras.layers.Dense(<span class="number">5</span>, kernel_initializer=<span class="string">'ones'</span>, kernel_regularizer=my_regularizer)</span><br></pre></td></tr></table></figure><ul><li>å­ç±»ç»§æ‰¿ç‰ˆï¼Œå¯ä»¥åŠ é¢å¤–å‚æ•°ï¼Œéœ€è¦è¡¥å……get_configæ–¹æ³•ï¼Œæ”¯æŒè¯»å†™æƒé‡æ—¶çš„ä¸²è¡ŒåŒ–</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyRegularizer</span><span class="params">(regularizers.Regularizer)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, strength)</span>:</span></span><br><span class="line">        self.strength = strength</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.strength * tf.reduce_sum(tf.square(x))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'strength'</span>: self.strength&#125;</span><br><span class="line"></span><br><span class="line">layer = tf.keras.layers.Dense(<span class="number">5</span>, kernel_initializer=<span class="string">'ones'</span>, kernel_regularizer=MyRegularizer(<span class="number">0.01</span>))</span><br></pre></td></tr></table></figure><p>11.3 å¼ºè¡Œglobal</p><ul><li>æ¯å±‚åŠ èµ·æ¥å¤ªçƒ¦äº†ï¼Œæ‰¹é‡åŠ çš„å®è´¨ä¹Ÿæ˜¯é€å±‚åŠ ï¼Œåªä¸è¿‡å†™æˆå¾ªç¯</li><li>æ ¸å¿ƒæ˜¯layerçš„add_lossæ–¹æ³•</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model = keras.applications.ResNet50(include_top=<span class="keyword">True</span>, weights=<span class="string">'imagenet'</span>)</span><br><span class="line">alpha = <span class="number">0.00002</span>  <span class="comment"># weight decay coefficient</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> model.layers:</span><br><span class="line">    <span class="keyword">if</span> isinstance(layer, keras.layers.Conv2D) <span class="keyword">or</span> isinstance(layer, keras.layers.Dense):</span><br><span class="line">        layer.add_loss(<span class="keyword">lambda</span>: keras.regularizers.l2(alpha)(layer.kernel))</span><br><span class="line">    <span class="keyword">if</span> hasattr(layer, <span class="string">'bias_regularizer'</span>) <span class="keyword">and</span> layer.use_bias:</span><br><span class="line">        layer.add_loss(<span class="keyword">lambda</span>: keras.regularizers.l2(alpha)(layer.bias))</span><br></pre></td></tr></table></figure><h2 id="12-kerasæŸ¥çœ‹æ¢¯åº¦-amp-æƒé‡"><a href="#12-kerasæŸ¥çœ‹æ¢¯åº¦-amp-æƒé‡" class="headerlink" title="12. kerasæŸ¥çœ‹æ¢¯åº¦&amp;æƒé‡"></a>12. kerasæŸ¥çœ‹æ¢¯åº¦&amp;æƒé‡</h2><p>12.1 easiest way</p><ul><li>æŸ¥çœ‹æ¢¯åº¦æœ€ç®€å•çš„æ–¹æ³•ï¼šé€šè¿‡K.gradientsæ–¹æ³•å®šä¹‰ä¸€ä¸ªæ±‚æ¢¯åº¦çš„funcï¼Œç„¶åç»™å®šè¾“å…¥ï¼Œå¾—åˆ°æ¢¯åº¦ï¼ˆCAMå°±æ˜¯è¿™ä¹ˆå¹²çš„ï¼‰</li><li>æŸ¥çœ‹æƒé‡æœ€ç®€å•çš„æ–¹æ³•ï¼šå­˜åœ¨h5æ–‡ä»¶ï¼Œç„¶åèŠ±å¼h5pyè§£æ</li></ul><p>12.2 dig deeper</p><ul><li>ä¸€ä¸ªæ€è·¯ï¼šå°†æ¢¯åº¦ä¿å­˜åœ¨optimizerçš„self.weightsä¸­ï¼Œå¹¶åœ¨model.saveå¾—åˆ°çš„æ¨¡å‹ä¸­è§£æ</li></ul><h2 id="13-keraså®ç°æƒé‡æ»‘åŠ¨å¹³å‡"><a href="#13-keraså®ç°æƒé‡æ»‘åŠ¨å¹³å‡" class="headerlink" title="13. keraså®ç°æƒé‡æ»‘åŠ¨å¹³å‡"></a>13. keraså®ç°æƒé‡æ»‘åŠ¨å¹³å‡</h2><p>13.1 why EMA on weights</p><ul><li><p>[reference1][<a href="https://www.jiqizhixin.com/articles/2019-05-07-18]ï¼šæƒé‡æ»‘åŠ¨å¹³å‡æ˜¯æä¾›è®­ç»ƒç¨³å®šæ€§çš„æœ‰æ•ˆæ–¹æ³•ï¼Œè¦ä¹ˆåœ¨ä¼˜åŒ–å™¨é‡Œé¢å®ç°ï¼Œè¦ä¹ˆå¤–åµŒåœ¨è®­ç»ƒä»£ç é‡Œ" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2019-05-07-18]ï¼šæƒé‡æ»‘åŠ¨å¹³å‡æ˜¯æä¾›è®­ç»ƒç¨³å®šæ€§çš„æœ‰æ•ˆæ–¹æ³•ï¼Œè¦ä¹ˆåœ¨ä¼˜åŒ–å™¨é‡Œé¢å®ç°ï¼Œè¦ä¹ˆå¤–åµŒåœ¨è®­ç»ƒä»£ç é‡Œ</a></p></li><li><p>[reference2][<a href="https://cloud.tencent.com/developer/article/1636781]ï¼šè¿™é‡Œé¢ä¸¾çš„ä¾‹å­å¾ˆæ¸…æ™°äº†ï¼Œå°±æ˜¯ä¸ºäº†æƒé‡æ¯ä¸ªstepå‰åå˜åŒ–ä¸å¤§" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1636781]ï¼šè¿™é‡Œé¢ä¸¾çš„ä¾‹å­å¾ˆæ¸…æ™°äº†ï¼Œå°±æ˜¯ä¸ºäº†æƒé‡æ¯ä¸ªstepå‰åå˜åŒ–ä¸å¤§</a></p></li></ul><p>æƒé‡EMAçš„è®¡ç®—æ–¹å¼æœ‰ç‚¹ç±»ä¼¼äºBNçš„running mean&amp;varï¼š</p><ul><li>åœ¨è®­ç»ƒé˜¶æ®µï¼šå®ƒä¸æ”¹å˜æ¯ä¸ªtraining stepçš„ä¼˜åŒ–æ–¹å‘ï¼Œè€Œæ˜¯ä»initial weightså¼€å§‹ï¼Œå¦å¤–ç»´æŠ¤ä¸€ç»„shadow weightsï¼Œç”¨æ¯æ¬¡çš„updating weightsæ¥è¿›è¡Œæ»‘åŠ¨æ›´æ–°</li><li>åœ¨inferenceé˜¶æ®µï¼Œæˆ‘ä»¬è¦ç”¨shadow weightsæ¥æ›¿æ¢å½“å‰æƒé‡æ–‡ä»¶ä¿å­˜çš„weightsï¼ˆcurrent stepä¸‹è®¡ç®—çš„æ–°æƒé‡ï¼‰</li><li>å¦‚æœè¦ç»§ç»­è®­ç»ƒï¼Œè¦å°†æ›¿æ¢çš„æƒé‡åœ¨æ¢å›æ¥ï¼Œå› ä¸ºã€EMAä¸å½±å“æ¨¡å‹çš„ä¼˜åŒ–è½¨è¿¹ã€‘</li></ul><p>13.2 who uses EMA</p><ul><li>å¾ˆå¤šGANçš„è®ºæ–‡éƒ½ç”¨äº†EMAï¼Œ</li><li>è¿˜æœ‰NLPé˜…è¯»ç†è§£æ¨¡å‹QANetï¼Œ</li><li>è¿˜æœ‰Googleçš„efficientNetã€resnet_rs</li></ul><p>13.3 how to implement outside</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExponentialMovingAverage</span>:</span></span><br><span class="line">    <span class="string">"""å¯¹æ¨¡å‹æƒé‡è¿›è¡ŒæŒ‡æ•°æ»‘åŠ¨å¹³å‡ã€‚</span></span><br><span class="line"><span class="string">    ç”¨æ³•ï¼šåœ¨model.compileä¹‹åã€ç¬¬ä¸€æ¬¡è®­ç»ƒä¹‹å‰ä½¿ç”¨ï¼›</span></span><br><span class="line"><span class="string">    å…ˆåˆå§‹åŒ–å¯¹è±¡ï¼Œç„¶åæ‰§è¡Œinjectæ–¹æ³•ã€‚</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model, momentum=<span class="number">0.9999</span>)</span>:</span></span><br><span class="line">        self.momentum = momentum</span><br><span class="line">        self.model = model</span><br><span class="line">        self.ema_weights = [K.zeros(K.shape(w)) <span class="keyword">for</span> w <span class="keyword">in</span> model.weights]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inject</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""æ·»åŠ æ›´æ–°ç®—å­åˆ°model.metrics_updatesã€‚</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.initialize()</span><br><span class="line">        <span class="keyword">for</span> w1, w2 <span class="keyword">in</span> zip(self.ema_weights, self.model.weights):</span><br><span class="line">            op = K.moving_average_update(w1, w2, self.momentum)</span><br><span class="line">            self.model.metrics_updates.append(op)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""ema_weightsåˆå§‹åŒ–è·ŸåŸæ¨¡å‹åˆå§‹åŒ–ä¸€è‡´ã€‚</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.old_weights = K.batch_get_value(self.model.weights)</span><br><span class="line">        K.batch_set_value(zip(self.ema_weights, self.old_weights))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply_ema_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""å¤‡ä»½åŸæ¨¡å‹æƒé‡ï¼Œç„¶åå°†å¹³å‡æƒé‡åº”ç”¨åˆ°æ¨¡å‹ä¸Šå»ã€‚</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.old_weights = K.batch_get_value(self.model.weights)</span><br><span class="line">        ema_weights = K.batch_get_value(self.ema_weights)</span><br><span class="line">        K.batch_set_value(zip(self.model.weights, ema_weights))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_old_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""æ¢å¤æ¨¡å‹åˆ°æ—§æƒé‡ã€‚</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        K.batch_set_value(zip(self.model.weights, self.old_weights))</span><br></pre></td></tr></table></figure><ul><li>then train</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">EMAer = ExponentialMovingAverage(model) <span class="comment"># åœ¨æ¨¡å‹compileä¹‹åæ‰§è¡Œ</span></span><br><span class="line">EMAer.inject() <span class="comment"># åœ¨æ¨¡å‹compileä¹‹åæ‰§è¡Œ</span></span><br><span class="line"></span><br><span class="line">model.fit(x_train, y_train) <span class="comment"># è®­ç»ƒæ¨¡å‹</span></span><br></pre></td></tr></table></figure><ul><li>then inference</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MAer.apply_ema_weights() <span class="comment"># å°†EMAçš„æƒé‡åº”ç”¨åˆ°æ¨¡å‹ä¸­</span></span><br><span class="line">model.predict(x_test) <span class="comment"># è¿›è¡Œé¢„æµ‹ã€éªŒè¯ã€ä¿å­˜ç­‰æ“ä½œ</span></span><br><span class="line"></span><br><span class="line">EMAer.reset_old_weights() <span class="comment"># ç»§ç»­è®­ç»ƒä¹‹å‰ï¼Œè¦æ¢å¤æ¨¡å‹æ—§æƒé‡ã€‚è¿˜æ˜¯é‚£å¥è¯ï¼ŒEMAä¸å½±å“æ¨¡å‹çš„ä¼˜åŒ–è½¨è¿¹ã€‚</span></span><br><span class="line">model.fit(x_train, y_train) <span class="comment"># ç»§ç»­è®­ç»ƒ</span></span><br></pre></td></tr></table></figure><h2 id="14-kerasçš„Modelç±»ç»§æ‰¿"><a href="#14-kerasçš„Modelç±»ç»§æ‰¿" class="headerlink" title="14. kerasçš„Modelç±»ç»§æ‰¿"></a>14. kerasçš„Modelç±»ç»§æ‰¿</h2><p>14.1 å®šä¹‰æ¨¡å‹çš„æ–¹å¼</p><ul><li>Sequentialï¼šæœ€ç®€å•ï¼Œä½†æ˜¯ä¸èƒ½è¡¨ç¤ºå¤æ‚æ‹“æ‰‘ç»“æ„</li><li>å‡½æ•°å¼ APIï¼šå’ŒSequentialç”¨æ³•åŸºæœ¬ä¸€è‡´ï¼Œè¾“å…¥å¼ é‡å’Œè¾“å‡ºå¼ é‡ç”¨äºå®šä¹‰ tf.keras.Modelå®ä¾‹</li><li>æ¨¡å‹å­ç±»åŒ–ï¼šå¼•å…¥äº Keras 2.2.0</li><li>kerasæºä»£ç å®šä¹‰åœ¨ï¼š<a href="https://github.com/keras-team/keras/blob/master/keras/engine/training.py" target="_blank" rel="noopener">https://github.com/keras-team/keras/blob/master/keras/engine/training.py</a></li></ul><p>14.2 æ¨¡å‹å­ç±»åŒ–overview</p><ul><li>æ—¢å¯ä»¥ç”¨æ¥å®šä¹‰ä¸€ä¸ªmodelï¼Œä¹Ÿå¯ä»¥ç”¨æ¥å®šä¹‰ä¸€ä¸ªå¤æ‚çš„ç½‘ç»œå±‚ï¼Œä¸ºå®ç°å¤æ‚æ¨¡å‹æä¾›æ›´å¤§çš„çµæ´»æ€§</li><li>æœ‰ç‚¹ç±»ä¼¼äºtorchçš„è¯­æ³•<ul><li>ç½‘ç»œå±‚å®šä¹‰åœ¨ <code>__init__(self, ...)</code> ä¸­ï¼šè·Ÿtorchè¯­æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºå±‚ä¸èƒ½å¤ç”¨ï¼ŒtorchåŒä¸€ä¸ªå±‚åœ¨forwardä¸­æ¯è°ƒç”¨ä¸€æ¬¡èƒ½å¤Ÿåˆ›å»ºä¸€ä¸ªå®ä¾‹ï¼Œkerasæ¯ä¸ªå±‚åº”è¯¥æ˜¯åœ¨initä¸­å£°æ˜å¹¶åˆ›å»ºï¼Œæ‰€ä»¥ä¸èƒ½å¤ç”¨</li><li>å‰å‘ä¼ æ’­åœ¨ <code>call(self, inputs)</code> ä¸­ï¼Œè¿™é‡Œé¢ä¹Ÿå¯ä»¥æ·»åŠ loss</li><li>compute_output_shapeè®¡ç®—æ¨¡å‹è¾“å‡ºçš„å½¢çŠ¶</li></ul></li><li>å’Œkerasè‡ªå®šä¹‰å±‚çš„è¯­æ³•ä¹Ÿå¾ˆç›¸ä¼¼<ul><li>build(input_shape)ï¼šä¸»è¦åŒºåˆ«å°±åœ¨äºbuildï¼Œå› ä¸ºè‡ªå®šä¹‰å±‚æœ‰buildï¼Œæ˜¾å¼å£°æ˜äº†æ•°æ®æµçš„shapeï¼Œèƒ½å¤Ÿæ„é€ å‡ºé™æ€å›¾</li><li>call(x)ï¼š</li><li>compute_output_shape(input_shape)ï¼š</li></ul></li><li>ã€ä»¥ä¸‹æ–¹æ³•å’Œå±æ€§ä¸é€‚ç”¨äºç±»ç»§æ‰¿æ¨¡å‹ã€‘ï¼Œæ‰€ä»¥è¿˜æ˜¯æ¨èä¼˜å…ˆä½¿ç”¨å‡½æ•°å¼ API<ul><li>model.inputs &amp; model.outputs</li><li>model.to_yaml() &amp; model.to_json()</li><li>model.get_config() &amp; model.save()ï¼šï¼ï¼ï¼åªèƒ½save_weightsï¼ï¼ï¼</li></ul></li></ul><p>14.3 æ —å­</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleMLP</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes=<span class="number">10</span>)</span>:</span></span><br><span class="line">        super(SimpleMLP, self).__init__(name=<span class="string">'mlp'</span>)</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.dense1 = keras.layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.dense2 = keras.layers.Dense(num_classes, activation=<span class="string">'softmax'</span>)</span><br><span class="line">        self.dp = keras.layers.Dropout(<span class="number">0.5</span>)</span><br><span class="line">        self.bn = keras.layers.BatchNormalization(axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None, mask=None)</span>:</span></span><br><span class="line">        x = self.dense1(inputs)</span><br><span class="line">        x = self.dp(x)</span><br><span class="line">        x = self.bn(x, training=training)</span><br><span class="line">        <span class="keyword">return</span> self.dense2(x)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_output_shape</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        batch, dim = input_shape</span><br><span class="line">        <span class="keyword">return</span> (batch, self.num_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = SimpleMLP()</span><br><span class="line">model.compile(<span class="string">'adam'</span>, loss=<span class="string">'categorical_crossentropy'</span>)</span><br><span class="line">x = np.random.uniform(<span class="number">0</span>,<span class="number">1</span>,(<span class="number">32</span>,<span class="number">100</span>))</span><br><span class="line">y = np.random.randint(<span class="number">0</span>, <span class="number">2</span>, (<span class="number">32</span>,<span class="number">10</span>))</span><br><span class="line">model.fit(x, y)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><ul><li>å¯ä»¥çœ‹åˆ°ï¼Œç±»ç»§æ‰¿æ¨¡å‹æ˜¯æ²¡æœ‰æŒ‡æ˜input_shapeçš„ï¼Œæ‰€ä»¥ä¹Ÿå°±ä¸å­˜åœ¨é™æ€å›¾ï¼Œè¦åœ¨æœ‰çœŸæ­£æ•°æ®æµä»¥åï¼Œmodelæ‰è¢«buildï¼Œæ‰èƒ½å¤Ÿè°ƒç”¨summayæ–¹æ³•ï¼ŒæŸ¥çœ‹å›¾ç»“æ„</li><li>ç¬¬äºŒä¸ªæ˜¯ï¼Œcallæ–¹æ³•çš„é»˜è®¤å‚æ•°ï¼šdef call(self, inputs, training=None, mask=None)ï¼Œ<ul><li>å­ç±»ç»§æ‰¿æ¨¡å‹ä¸æ”¯æŒæ˜¾å¼çš„å¤šè¾“å…¥å®šä¹‰ï¼Œæ‰€æœ‰çš„è¾“å…¥æ„æˆinputs</li><li>éœ€è¦æ‰‹å·¥ç®¡ç†trainingå‚æ•°ï¼Œbn/dropoutç­‰åœ¨train/inference modeä¸‹è®¡ç®—ä¸ä¸€æ ·çš„æƒ…å†µï¼Œè¦æ˜¾å¼ä¼ å…¥trainingå‚æ•°</li><li>maskåœ¨æ„å»ºAttentionæœºåˆ¶æˆ–è€…åºåˆ—æ¨¡å‹æ—¶ä¼šä½¿ç”¨åˆ°ï¼Œå¦‚æœprevious layerç”Ÿæˆäº†æ©ç ï¼ˆembeddingçš„mask_zeroå‚æ•°ä¸ºTrueï¼‰ï¼Œå‰ä¸¤ç§æ„å»ºæ¨¡å‹çš„æ–¹æ³•ä¸­ï¼Œmaskä¼šè‡ªåŠ¨ä¼ å…¥å½“å‰å±‚çš„callæ–¹æ³•ä¸­</li></ul></li></ul><h2 id="15-low-level-training-amp-evaluation-loops"><a href="#15-low-level-training-amp-evaluation-loops" class="headerlink" title="15. low-level training &amp; evaluation loops"></a>15. low-level training &amp; evaluation loops</h2><p>15.1 kerasçš„Modelç±»æä¾›äº†build-inçš„train/evalæ–¹æ³•</p><pre><code>* fit()* evaluate()* predict()* reference: https://keras.io/api/models/model_training_apis/* reference: https://keras.io/guides/training_with_built_in_methods/</code></pre><p>15.2 å¦‚æœä½ æƒ³ä¿®æ”¹æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œä½†ä»æ—§é€šè¿‡fit()æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼ŒModelç±»ä¸­æä¾›äº†train_step()å¯ä»¥ç»§æ‰¿å’Œé‡è½½</p><ul><li><p>reference: <a href="https://keras.io/guides/customizing_what_happens_in_fit/" target="_blank" rel="noopener">https://keras.io/guides/customizing_what_happens_in_fit/</a></p></li><li><p>Modelç±»ä¸­æœ‰ä¸€ä¸ªtrain_step()æ–¹æ³•ï¼Œfitæ¯ä¸ªbatchçš„æ—¶å€™éƒ½ä¼šè°ƒç”¨ä¸€æ¬¡</p></li><li><p>åœ¨é‡å†™è¿™ä¸ªtrain_step()æ–¹æ³•æ—¶</p><ul><li>ä¼ å…¥å‚æ•°dataï¼šå–å†³äºfit()æ–¹æ³•ä¼ å…¥çš„å‚æ•°å½¢å¼ï¼Œtuple(x,y) / tf.data.Dataset</li><li>forward passï¼šself(model)</li><li>è®¡ç®—lossï¼šself.compiled_loss</li><li>è®¡ç®—æ¢¯åº¦ï¼štf.GradientTape()</li><li>æ›´æ–°æƒé‡ï¼šself.optimizer</li><li>æ›´æ–°metricsï¼šself.compiled_metrics</li><li>è¿”å›å€¼a dictionary mapping metric names</li></ul></li><li><p>æ —å­ğŸŒ°</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomModel</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="comment"># Unpack the data</span></span><br><span class="line">        x, y = data</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            y_pred = self(x, training=<span class="keyword">True</span>)  <span class="comment"># Forward pass</span></span><br><span class="line">            <span class="comment"># Compute the loss value</span></span><br><span class="line">            <span class="comment"># (the loss function is configured in `compile()`)</span></span><br><span class="line">            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute gradients</span></span><br><span class="line">        trainable_vars = self.trainable_variables</span><br><span class="line">        gradients = tape.gradient(loss, trainable_vars)</span><br><span class="line">        <span class="comment"># Update weights</span></span><br><span class="line">        self.optimizer.apply_gradients(zip(gradients, trainable_vars))</span><br><span class="line">        <span class="comment"># Update metrics (includes the metric that tracks the loss)</span></span><br><span class="line">        self.compiled_metrics.update_state(y, y_pred)</span><br><span class="line">        <span class="comment"># Return a dict mapping metric names to current value</span></span><br><span class="line">        <span class="keyword">return</span> &#123;m.name: m.result() <span class="keyword">for</span> m <span class="keyword">in</span> self.metrics&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct and compile an instance of CustomModel</span></span><br><span class="line">inputs = keras.Input(shape=(<span class="number">32</span>,))</span><br><span class="line">outputs = keras.layers.Dense(<span class="number">1</span>)(inputs)</span><br><span class="line">model = CustomModel(inputs, outputs)</span><br><span class="line">model.compile(optimizer=<span class="string">"adam"</span>, loss=<span class="string">"mse"</span>, metrics=[<span class="string">"mae"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Just use `fit` as usual</span></span><br><span class="line">x = np.random.random((<span class="number">1000</span>, <span class="number">32</span>))</span><br><span class="line">y = np.random.random((<span class="number">1000</span>, <span class="number">1</span>))</span><br><span class="line">model.fit(x, y, epochs=<span class="number">3</span>)</span><br></pre></td></tr></table></figure><pre><code>  * æ³¨æ„åˆ°è¿™é‡Œé¢æˆ‘ä»¬è°ƒç”¨äº†self.compiled_losså’Œself.compiled_metricsï¼Œè¿™å°±æ˜¯åœ¨è°ƒç”¨compile()æ–¹æ³•çš„æ—¶å€™ä¼ å…¥çš„losså’Œmetricså‚æ•°</code></pre></li><li><p>get lower</p><ul><li>losså’Œmetricsä¹Ÿå¯ä»¥ä¸ä¼ ï¼Œç›´æ¥åœ¨CustomModelé‡Œé¢å£°æ˜å’Œå®šä¹‰</li><li>å£°æ˜ï¼šé‡è½½metrics()æ–¹æ³•ï¼Œåˆ›å»ºmetric instancesï¼Œç”¨äºè®¡ç®—losså’Œmetricsï¼ŒæŠŠä»–ä»¬æ”¾åœ¨è¿™é‡Œæ¨¡å‹ä¼šåœ¨fit()/evaluate()æ–¹æ³•çš„æ¯ä¸ªepochèµ·å§‹é˜¶æ®µè°ƒç”¨reset_states()æ–¹æ³•ï¼Œç¡®ä¿losså’Œmetricsçš„stateséƒ½æ˜¯per epochçš„ï¼Œè€Œä¸æ˜¯avg from the beginning</li><li><p>æ›´æ–°ï¼šè°ƒç”¨update_state()æ–¹æ³•æ›´æ–°ä»–ä»¬çš„çŠ¶æ€å‚æ•°ï¼Œè°ƒç”¨result()æ–¹æ³•æ‹¿åˆ°ä»–ä»¬çš„current value</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">loss_tracker = keras.metrics.Mean(name=<span class="string">"loss"</span>)</span><br><span class="line">mae_metric = keras.metrics.MeanAbsoluteError(name=<span class="string">"mae"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomModel</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        x, y = data</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            y_pred = self(x, training=<span class="keyword">True</span>)  <span class="comment"># Forward pass</span></span><br><span class="line">            <span class="comment"># Compute our own loss</span></span><br><span class="line">            loss = keras.losses.mean_squared_error(y, y_pred)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute gradients</span></span><br><span class="line">        trainable_vars = self.trainable_variables</span><br><span class="line">        gradients = tape.gradient(loss, trainable_vars)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update weights</span></span><br><span class="line">        self.optimizer.apply_gradients(zip(gradients, trainable_vars))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute our own metrics</span></span><br><span class="line">        loss_tracker.update_state(loss)</span><br><span class="line">        mae_metric.update_state(y, y_pred)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">"loss"</span>: loss_tracker.result(), <span class="string">"mae"</span>: mae_metric.result()&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">metrics</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># We list our `Metric` objects here so that `reset_states()` can be called automatically per epoch</span></span><br><span class="line">        <span class="keyword">return</span> [loss_tracker, mae_metric]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct an instance of CustomModel</span></span><br><span class="line">inputs = keras.Input(shape=(<span class="number">32</span>,))</span><br><span class="line">outputs = keras.layers.Dense(<span class="number">1</span>)(inputs)</span><br><span class="line">model = CustomModel(inputs, outputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We don't passs a loss or metrics here.</span></span><br><span class="line">model.compile(optimizer=<span class="string">"adam"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Just use `fit` as usual -- you can use callbacks, etc.</span></span><br><span class="line">x = np.random.random((<span class="number">1000</span>, <span class="number">32</span>))</span><br><span class="line">y = np.random.random((<span class="number">1000</span>, <span class="number">1</span>))</span><br><span class="line">model.fit(x, y, epochs=<span class="number">5</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>ç›¸å¯¹åº”åœ°ï¼Œä¹Ÿå¯ä»¥å®šåˆ¶model.evaluate()çš„è®¡ç®—è¿‡ç¨‹â€”â€”override test_step()æ–¹æ³•</p><ul><li>ä¼ å…¥å‚æ•°dataï¼šå–å†³äºfit()æ–¹æ³•ä¼ å…¥çš„å‚æ•°å½¢å¼ï¼Œtuple(x,y) / tf.data.Dataset</li><li>forward passï¼šself(model)</li><li>è®¡ç®—lossï¼šself.compiled_loss</li><li>è®¡ç®—metricsï¼šself.compiled_metrics</li><li><p>è¿”å›å€¼a dictionary mapping metric names</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomModel</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_step</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="comment"># Unpack the data</span></span><br><span class="line">        x, y = data</span><br><span class="line">        <span class="comment"># Compute predictions</span></span><br><span class="line">        y_pred = self(x, training=<span class="keyword">False</span>)</span><br><span class="line">        <span class="comment"># Updates the metrics tracking the loss</span></span><br><span class="line">        self.compiled_loss(y, y_pred, regularization_losses=self.losses)</span><br><span class="line">        <span class="comment"># Update the metrics.</span></span><br><span class="line">        self.compiled_metrics.update_state(y, y_pred)</span><br><span class="line">        <span class="comment"># Return a dict mapping metric names to current value.</span></span><br><span class="line">        <span class="comment"># Note that it will include the loss (tracked in self.metrics).</span></span><br><span class="line">        <span class="keyword">return</span> &#123;m.name: m.result() <span class="keyword">for</span> m <span class="keyword">in</span> self.metrics&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct an instance of CustomModel</span></span><br><span class="line">inputs = keras.Input(shape=(<span class="number">32</span>,))</span><br><span class="line">outputs = keras.layers.Dense(<span class="number">1</span>)(inputs)</span><br><span class="line">model = CustomModel(inputs, outputs)</span><br><span class="line">model.compile(loss=<span class="string">"mse"</span>, metrics=[<span class="string">"mae"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate with our custom test_step</span></span><br><span class="line">x = np.random.random((<span class="number">1000</span>, <span class="number">32</span>))</span><br><span class="line">y = np.random.random((<span class="number">1000</span>, <span class="number">1</span>))</span><br><span class="line">model.evaluate(x, y)</span><br></pre></td></tr></table></figure></li></ul></li></ul><p>15.3 å®ç°å®Œæ•´çš„train loops</p><ul><li><p>reference: <a href="https://keras.io/guides/writing_a_training_loop_from_scratch/" target="_blank" rel="noopener">https://keras.io/guides/writing_a_training_loop_from_scratch/</a></p><ul><li><p>a train loop</p><ul><li>a for loopï¼šiter for each epoch<ul><li>a for loopï¼šiter over the dataset<pre><code> * open a `GradientTape()` scopeï¼štensorflowçš„æ¢¯åº¦APIï¼Œç”¨äºç»™å®šlossè®¡ç®—æ¢¯åº¦ * Inside this scopeï¼šforward passï¼Œcompute loss * Outside the scopeï¼šretrieve the gradients  * use optimizer to update the gradientsï¼š`optimizer.apply_gradients`ï¼Œä½¿ç”¨è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦æ¥æ›´æ–°å¯¹åº”çš„variable</code></pre></li></ul></li></ul></li></ul></li><li><p>æ —å­ğŸŒ°</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model</span></span><br><span class="line">model = keras.Model(inputs=inputs, outputs=outputs)</span><br><span class="line">optimizer = keras.optimizers.SGD(learning_rate=<span class="number">1e-3</span>)</span><br><span class="line">loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># data</span></span><br><span class="line">train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))</span><br><span class="line">train_dataset = train_dataset.shuffle(buffer_size=<span class="number">1024</span>).batch(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># iter epochs</span></span><br><span class="line">epochs = <span class="number">2</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    print(<span class="string">"\nStart of epoch %d"</span> % (epoch,))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># iter batches</span></span><br><span class="line">    <span class="keyword">for</span> step, (x_batch_train, y_batch_train) <span class="keyword">in</span> enumerate(train_dataset):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Open a GradientTape to record the operations run</span></span><br><span class="line">        <span class="comment"># during the forward pass, which enables auto-differentiation.</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Run the forward pass of the layer.</span></span><br><span class="line">            logits = model(x_batch_train, training=<span class="keyword">True</span>)  <span class="comment"># Logits for this minibatch</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute the loss value for this minibatch.</span></span><br><span class="line">            loss_value = loss_fn(y_batch_train, logits)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># automatically retrieve the gradients of the trainable variables with respect to the loss</span></span><br><span class="line">        grads = tape.gradient(loss_value, model.trainable_weights)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run one step of gradient descent by updating the value of the variables to minimize the loss.</span></span><br><span class="line">        optimizer.apply_gradients(zip(grads, model.trainable_weights))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Log every 200 batches.</span></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">            print(</span><br><span class="line">                <span class="string">"Training loss (for one batch) at step %d: %.4f"</span></span><br><span class="line">                % (step, float(loss_value))</span><br><span class="line">            )</span><br><span class="line">            print(<span class="string">"Seen so far: %s samples"</span> % ((step + <span class="number">1</span>) * <span class="number">64</span>))</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
        <tags>
            
            <tag> keras </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>opencvåº“å‡½æ•°</title>
      <link href="/2019/08/11/opencv%E5%BA%93%E5%87%BD%E6%95%B0/"/>
      <url>/2019/08/11/opencv%E5%BA%93%E5%87%BD%E6%95%B0/</url>
      <content type="html"><![CDATA[<h3 id="1-imshow"><a href="#1-imshow" class="headerlink" title="1. imshow"></a>1. imshow</h3><p>æœ‰æ—¶å€™imshowçš„å›¾ç‰‡ä¼šæ˜¾ç¤ºçš„å’ŒåŸå›¾ä¸ä¸€æ ·ï¼Œè¦æŸ¥çœ‹readè¿›æ¥çš„æ•°æ®æ ¼å¼ï¼Œimshowä¼šæ ¹æ®è¯»å…¥çš„æ•°æ®æ ¼å¼è‡ªåŠ¨è¿›è¡Œå½’ä¸€åŒ–ï¼Œæ˜ å°„åˆ°0-255ã€‚</p><ul><li>å¦‚æœimageæ˜¯é»˜è®¤çš„8-bit unsignedï¼ˆ0-255ï¼‰ï¼Œä¸åšå¤„ç†ã€‚</li><li>å¦‚æœimageæ˜¯16-bit unsignedï¼ˆ0-65535ï¼‰æˆ–è€…32-bit integerï¼ˆï¼Ÿï¼Ÿè´¼å¤§ï¼‰ï¼Œåƒç´ å€¼é™¤ä»¥256ï¼Œ[0,255*256]å½’ä¸€åŒ–åˆ°[0ï¼Œ255]ã€‚</li><li>å¦‚æœimageæ˜¯32-bit floatï¼Œåƒç´ å€¼ä¹˜ä»¥255ï¼Œ[0,1]å½’ä¸€åŒ–åˆ°[0ï¼Œ255]ã€‚</li></ul><h3 id="2-imwrite"><a href="#2-imwrite" class="headerlink" title="2. imwrite"></a>2. imwrite</h3><p>é€šå¸¸imwriteæŠŠæ‰€æœ‰æ•°æ®éƒ½å¼ºåˆ¶è½¬æ¢æˆucharï¼ˆ0-255ï¼‰ã€‚</p>]]></content>
      
      
    </entry>
    
    <entry>
      <title>beautifulsoup saving file </title>
      <link href="/2019/06/11/beautifulsoup-saving-file/"/>
      <url>/2019/06/11/beautifulsoup-saving-file/</url>
      <content type="html"><![CDATA[<p>æœ€è¿‘ç”¨bs4å¤„ç†xmlæ–‡ä»¶ï¼Œé‡åˆ°äº†ä¸€ä¸ªåœ¨çˆ¬è™«æ—¶å€™ä»æœªæ€è€ƒè¿‡çš„é—®é¢˜â€”â€”</p><p><strong>ä¿®æ­£ä»xmlæ–‡ä»¶ä¸­è§£æå‡ºçš„æ–‡ä»¶æ ‘ï¼Œå¹¶å°†changesä¿å­˜åˆ°åŸæ¥çš„xmlæ–‡ä»¶ä¸­ã€‚</strong></p><p>æˆ‘ä¸€ç›´åœ¨beautifulsoupçš„æ‰‹å†Œä¸­å»å¯»æ‰¾åº“å‡½æ•°ï¼Œå®é™…åªéœ€è¦ç®€å•çš„æ–‡ä»¶è¯»å†™æ“ä½œï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(open(<span class="string">'test.xml'</span>), <span class="string">'xml'</span>)</span><br><span class="line">add = BeautifulSoup(<span class="string">"&lt;a&gt;Foo&lt;/a&gt;"</span>, <span class="string">'xml'</span>)</span><br><span class="line">soup.orderlist.append(add)</span><br><span class="line">print(soup.prettify())</span><br><span class="line">f = open(<span class="string">'test.xml'</span>, <span class="string">'w'</span>)</span><br><span class="line">f.write(str(soup))</span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure><p>é™„ä¸€ä¸ªç®€å•xmlæ–‡ä»¶ç”¨æ¥å®éªŒï¼š</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="utf-8"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">orderlist</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">order</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">customer</span>&gt;</span>å§“å1<span class="tag">&lt;/<span class="name">customer</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">phone</span>&gt;</span>ç”µè¯1<span class="tag">&lt;/<span class="name">phone</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">address</span>&gt;</span>åœ°å€1<span class="tag">&lt;/<span class="name">address</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">count</span>&gt;</span>ç‚¹é¤æ¬¡æ•°1<span class="tag">&lt;/<span class="name">count</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">order</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">order</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">customer</span>&gt;</span>å§“å2<span class="tag">&lt;/<span class="name">customer</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">phone</span>&gt;</span>ç”µè¯2<span class="tag">&lt;/<span class="name">phone</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">address</span>&gt;</span>åœ°å€2<span class="tag">&lt;/<span class="name">address</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">count</span>&gt;</span>ç‚¹é¤æ¬¡æ•°2<span class="tag">&lt;/<span class="name">count</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">order</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ssh visualization</title>
      <link href="/2018/12/29/ssh-visualization/"/>
      <url>/2018/12/29/ssh-visualization/</url>
      <content type="html"><![CDATA[<ol><li><p>ssh boocax@192.168.1.100</p><p>å¯†ç ï¼šrobot123</p></li><li><p>echo $ROS_MASTER_URI</p><p>æŸ¥çœ‹ç«¯å£å·11311</p></li><li><p>å°è½¦ç«¯ï¼š</p><p>export ROS_MASTER_URI=<a href="http://192.168.1.100:11311" target="_blank" rel="noopener">http://192.168.1.100:11311</a></p><p>export ROS_IP=192.168.1.100</p></li><li><p>è™šæ‹Ÿæœºç«¯ï¼š</p><p>export ROS_MASTER_URI=<a href="http://192.168.1.100:11311" target="_blank" rel="noopener">http://192.168.1.100:11311</a></p><p>export ROS_IP=172.16.128.142</p></li></ol><h4 id=""><a href="#" class="headerlink" title=" "></a> </h4><h6 id="-1"><a href="#-1" class="headerlink" title="#"></a>#</h6><h4 id="-2"><a href="#-2" class="headerlink" title=" "></a> </h4><ol><li><p>navè¿œç¨‹å¼€å¯ä¸‰ä¸ªç»ˆç«¯ï¼ˆä»£ç é‡æ„ä»¥å‰ï¼‰ï¼š</p><p>move_base: roslaunch teleop_twist_joy real_nav.launch </p><p>mapserver: rosrun map_server map_server catkin_ws2/src/patrol/map/p1.yaml </p><p>amcl: roslaunch patrol real_loc.launch </p></li><li><p>æœ¬åœ°å¯è§†åŒ–ï¼šrvizï¼rqt_graph / rosservice call /rostopic pub</p><ul><li><p>å…¨å±€å®šä½ï¼š</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rosservice call /global_localization "&#123;&#125;"</span><br></pre></td></tr></table></figure></li><li><p>è®¾ç½®å¯¼èˆªç›®æ ‡ç‚¹ï¼š</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// ç›¸å¯¹base_linkåæ ‡ç³»</span><br><span class="line">rostopic pub -1 /navigation_simple/goal geometry_msgs/PoseStamped '&#123; header: &#123; frame_id: "base_link" &#125;, pose: &#123; position: &#123; x: 0.5, y: 0.0, z: 0 &#125;, orientation: &#123; x: 0, y: 0, z: 0, w: 1 &#125; &#125; &#125;'</span><br><span class="line"></span><br><span class="line">// ç›¸å¯¹mapåæ ‡ç³»</span><br><span class="line">rostopic pub -1 /navigation_simple/goal geometry_msgs/PoseStamped '&#123; header: &#123; frame_id: "map" &#125;, pose: &#123; position: &#123; x: 5, y: 0.0, z: 0 &#125;, orientation: &#123; x: 0, y: 0, z: 0, w: 1 &#125; &#125; &#125;'</span><br></pre></td></tr></table></figure><p>æ³¨æ„<strong>-1</strong>ï¼Œå¦åˆ™å¾ªç¯å‘å¸ƒã€‚</p></li></ul></li></ol><h4 id="-3"><a href="#-3" class="headerlink" title=" "></a> </h4><h6 id="-4"><a href="#-4" class="headerlink" title="#"></a>#</h6><h4 id="-5"><a href="#-5" class="headerlink" title=" "></a> </h4><p>å¾€å›å¤‡ä»½ï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r boocax@192.168.1.100:/home/boocax/catkin_ws2019 bkp/</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> ROS </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Occupancy Grid Map</title>
      <link href="/2018/12/04/Occupancy-Grid-Map/"/>
      <url>/2018/12/04/Occupancy-Grid-Map/</url>
      <content type="html"><![CDATA[<p>to be completedâ€¦</p><ul><li>Inverse Sensor Model</li><li>Incremental Updating</li></ul>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>object tracking</title>
      <link href="/2018/12/03/object-tracking/"/>
      <url>/2018/12/03/object-tracking/</url>
      <content type="html"><![CDATA[<ol><li><p>èŒƒå›´é™å®šï¼šè¿‡æ»¤æ‰è¾ƒè¿œèŒƒå›´çš„ç‚¹äº‘æ•°æ®</p></li><li><p>èšç±»ï¼šK-Meansï¼æ¬§å¼èšç±»ï¼Œå› ä¸ºå‰è€…éœ€è¦è®¾å®šKï¼Œæ•…ä½¿ç”¨åè€…ã€‚</p><script type="math/tex; mode=display">D(p_i, p_{i+1}) = \sqrt{r_i^2 + r_{i+1}^2 - 2r_ir_{i+1}cos(\varphi_{i+1} - \varphi_i)}</script><p>å¦‚æœè¿ç»­æ‰«æç‚¹ä¹‹é—´çš„è·ç¦»å°äºä¸€ä¸ªé˜ˆå€¼$D_t$ï¼Œé‚£ä¹ˆè¿™ä¸¤ä¸ªç‚¹è¢«è®¤ä¸ºå±äºåŒä¸€ä¸ªå¯¹è±¡ã€‚è¿™ä¸ªé˜ˆå€¼æ˜¯æ ¹æ®å½“å‰å‚è€ƒç‚¹çš„è·ç¦»åŠ¨æ€è°ƒæ•´çš„ã€‚</p><script type="math/tex; mode=display">D_t = D_0 + a*r_i*sin(\Delta \varphi)</script></li><li><p>è¿åŠ¨ç›®æ ‡ç‰¹å¾æå–ï¼šï¼ˆä¸­å¿ƒåæ ‡ï¼Œé•¿ï¼å®½ï¼åŠå¾„ï¼Œåå°„å¼ºåº¦ï¼‰</p></li><li><p>ç”±ä¸Šä¸€æ—¶åˆ»çš„ä½ç½®é€Ÿåº¦è®¾ç½®ROIï¼š</p><ul><li>åŸºäºå±€éƒ¨åŒ¹é…ï¼šé€šè¿‡ç›¸ä¼¼åº¦è®¡ç®—é€‰å–å“åº”å€¼æœ€é«˜çš„ç›®æ ‡</li><li>åŸºäºåˆ†ç±»å™¨ï¼šåŠ¨æ€ç›®æ ‡å·²çŸ¥ï¼ˆäººè…¿ï¼‰ï¼Œé‡‡é›†æ­£è´Ÿæ ·æœ¬ï¼Œæ„é€ åˆ†ç±»å™¨ï¼Œ</li></ul></li><li><p>å¡å°”æ›¼æ»¤æ³¢ï¼š</p></li></ol>]]></content>
      
      
        <tags>
            
            <tag> extensions for slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>matplotlibçš„colormap</title>
      <link href="/2018/11/29/matplotlib%E7%9A%84colormap/"/>
      <url>/2018/11/29/matplotlib%E7%9A%84colormap/</url>
      <content type="html"><![CDATA[<p>ç”¨pltçš„imshowç”»å›¾ï¼Œæ€»æ˜¯æ‰¾ä¸åˆ°å¿ƒä»ªçš„colorbarï¼Œå¯ä»¥è‡ªå®šä¹‰ï¼š</p><ol><li><p>åœ¨åŸæœ‰cmapåŸºç¡€ä¸Šè‡ªå®šä¹‰ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">colorbar = plt.get_cmap(<span class="string">'Greys'</span>)(range(<span class="number">180</span>))</span><br><span class="line">cm = LinearSegmentedColormap.from_list(name=<span class="string">"grey_cm"</span>, colors=colorbar)</span><br><span class="line">plt.register_cmap(cmap=cm)</span><br><span class="line"></span><br><span class="line">plt.imshow(map2d.data, cmap=<span class="string">'grey_cm'</span>)</span><br></pre></td></tr></table></figure></li><li><p>defineä¸€ä¸ªæ–°çš„cmapï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">colormap</span><span class="params">()</span>:</span></span><br><span class="line">    colors = [<span class="string">'#FFFFFF'</span>, <span class="string">'#9ff113'</span>, <span class="string">'#5fbb44'</span>, <span class="string">'#f5f329'</span>, <span class="string">'#e50b32'</span>]</span><br><span class="line">    <span class="keyword">return</span> colors.ListedColormap(colors, <span class="string">'my_cmap'</span>)</span><br><span class="line"></span><br><span class="line">my_cmap = colormap()</span><br><span class="line">plt.imshow(map2d.data, cmap=my_cmap)</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>mpv-video-cutter</title>
      <link href="/2018/11/26/mpv-video-cutter/"/>
      <url>/2018/11/26/mpv-video-cutter/</url>
      <content type="html"><![CDATA[<p>mpvçš„å°æ’ä»¶ï¼Œèƒ½å¤Ÿä¸€é”®ï¼ˆä¸‰é”®ï¼‰å‰ªè¾‘ã€‚</p><p>å·¥ç¨‹åœ°å€ï¼š<a href="https://github.com/rushmj/mpv-video-cutter" target="_blank" rel="noopener">https://github.com/rushmj/mpv-video-cutter</a></p><p>step1ï¼šæŠŠc_concat.shå’Œcutter.luaä¸¤ä¸ªæ–‡ä»¶å¤åˆ¶åˆ°~/.config/mpv/scripts/ç›®å½•ä¸‹ã€‚</p><p>step2ï¼š<strong>ç»™c_concat.shè„šæœ¬æ·»åŠ æ‰§è¡Œæƒé™</strong>ã€‚</p><p>step3ï¼šç”¨å‘½ä»¤è¡Œæ‰“å¼€æ–‡ä»¶ï¼Œc-c-oåœ¨åŸç›®å½•ä¸‹ç”Ÿæˆå‰ªè¾‘æ–‡ä»¶ã€‚</p>]]></content>
      
      
        <tags>
            
            <tag> tool </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>problems with ROS</title>
      <link href="/2018/11/22/problems-with-ROS/"/>
      <url>/2018/11/22/problems-with-ROS/</url>
      <content type="html"><![CDATA[<ol><li><p><code>[WARN] Detected jump back in time of 5.51266s. Clearing TF buffer.</code></p><p>æ‰‹åŠ¨å»ºå›¾çš„æ—¶å€™ï¼Œæ—¶ä¸æ—¶çš„å°±è·³å‡ºæ¥è¿™ä¸ªï¼Œç„¶åå°è½¦è·³å˜åˆ°åˆå§‹ä½ç½®ï¼Œè€Œä¸”è¿˜æ˜¯æ ¹æ®TF bufferå›æº¯å›å»çš„ï¼ŒçœŸé«˜çº§ã€‚ã€‚ã€‚</p><p>æ’æŸ¥åŸå› å‘ç°ç«Ÿç„¶æ˜¯å¿˜è®°è¿è¡Œroscoreäº†ï¼Œmmpã€‚</p></li><li><p><code>[rosrun] Couldn&#39;t find executable named patrol.py below /home/carrol/catkin_ws/src/patrol</code></p><p>åŸå› å¦‚æç¤ºï¼Œpythonæ˜¯è„šæœ¬æ‰§è¡Œï¼Œè¦æ·»åŠ å¯æ‰§è¡Œæƒé™ã€‚</p></li><li><p><code>error: â€˜arrayâ€™ is not a member of â€˜stdâ€™</code></p><p>ç¼–è¯‘å¯¼èˆªåŒ…æ—¶åå¤å‡ºç°è¿™ä¸ªé”™è¯¯ï¼Œå› ä¸ºcmakeç‰ˆæœ¬æ¯”è¾ƒä½ï¼ˆ2.8ï¼‰ï¼Œä¸ä¼šè‡ªåŠ¨æ‰¾c++11ï¼Œè§£å†³åŠæ³•åœ¨å¯¹åº”packageçš„cmakeæ–‡ä»¶ä¸­æ·»åŠ c++å£°æ˜ï¼š<code>add_definitions(-std=c++11)</code></p></li><li><p>åŒæ ·çš„é”™è¯¯catkin_makeæ—¶é‡å¤å‡ºç°ï¼Œæˆ‘è¿˜ä»¥ä¸ºé—®é¢˜æ²¡è§£å†³ï¼š</p><p>åˆ é™¤buildæ–‡ä»¶å¤¹ä¸­å¯¹åº”åŒ…ï¼Œå†è¿›è¡Œcatkin_makeã€‚å¦‚æœåˆ é™¤äº†æŸä¸ªåŒ…ï¼Œè¿˜è¦åˆ é™¤develæ–‡ä»¶å¤¹å†ç¼–è¯‘ã€‚</p></li><li><p>cmake warning conflicts with Anacondaï¼š</p><p>ç¼–è¯‘åˆ°æœ€åä¼šå¡æ­»ï¼Œé”™è¯¯å…·ä½“å•¥æ„æ€æˆ‘ä¹Ÿæ²¡å¼„æ˜ç™½ï¼Œç²—æš´è§£å†³äº†ï¼Œå°†ç³»ç»Ÿç¯å¢ƒå˜é‡é‡Œé¢çš„anaconda pathæš‚æ—¶å±è”½ï¼Œé¦–å…ˆæŸ¥çœ‹ç¯å¢ƒå˜é‡ï¼š<code>echo $PATH</code>ï¼Œç„¶åè¿”å›ç»“æœï¼š</p><p>/home/[username]/anaconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games</p><p>ç„¶ååœ¨å½“å‰å‘½ä»¤è¡Œæ‰§è¡Œï¼š<code>export PATH=&quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games&quot;</code></p></li><li><p><code>c++: internal compiler error: Killed (program cc1plus)</code></p><p>è™šæ‹Ÿæœºå†…å­˜ä¸è¶³ã€‚</p></li><li><p><code>undefined error with CONSOLE_BRIDGE_logError/CONSOLE_BRIDGE_logWarn</code></p><p>å®‰è£…å¹¶ç¼–è¯‘<code>console_bridge</code>åŒ…ï¼Œ<strong>æ³¨æ„build instructionsï¼š</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git clone git://github.com/ros/console_bridge.git</span><br><span class="line">cd console_bridge</span><br><span class="line">cmake .</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure></li><li><p><code>there are no arguments to â€˜logDebugâ€™ that depend on a template parameter, so a declaration of â€˜logDebugâ€™ must be available [-fpermissive]</code></p><p>å‚è€ƒï¼ˆ<a href="https://talk.apolloauto.io/t/topic/77" target="_blank" rel="noopener">Reference</a>ï¼‰ï¼Œè¿˜æ˜¯ä¸Šé¢çš„é—®é¢˜ï¼Œ    <code>console_bridge</code>çš„APIå˜äº†ï¼Œå°†<code>logDebug</code>æ”¹æˆ<code>CONSOLE_BRIDGE_logDebug</code>å°±è¡Œäº†ã€‚</p></li><li><p>running environmentç›¸å…³åŒ…çš„ç¼ºå¤±å’Œå®‰è£…ï¼š</p><p>åœ¨<a href="https://packages.ubuntu.com/xenial/libconsole-bridge-dev" target="_blank" rel="noopener">å®˜ç½‘</a>æŸ¥æ‰¾ç›¸å…³åŒ…å’Œä¾èµ–ï¼Œç„¶åæ‰§è¡Œï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># install</span><br><span class="line">sudo dpkg -i è½¯ä»¶åŒ…å.deb</span><br><span class="line"></span><br><span class="line"># uninstall</span><br><span class="line">sudo apt-get remove è½¯ä»¶åŒ…åç§°</span><br></pre></td></tr></table></figure></li><li></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ROS </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>amcl</title>
      <link href="/2018/11/16/amcl/"/>
      <url>/2018/11/16/amcl/</url>
      <content type="html"><![CDATA[<h4 id="æ»¤æ³¢ï¼š"><a href="#æ»¤æ³¢ï¼š" class="headerlink" title="æ»¤æ³¢ï¼š"></a>æ»¤æ³¢ï¼š</h4><p>æœºå™¨äººä»å·²çŸ¥ç‚¹$x_0$å¼€å§‹è¿åŠ¨ï¼Œé‡Œç¨‹è®¡è¯¯å·®é€æ¸ç´¯ç§¯ï¼Œä½ç½®ä¸ç¡®å®šæ€§å°†è¶Šæ¥è¶Šå¤§ï¼ˆ$x_1, x_2$ï¼‰ã€‚å› æ­¤éœ€è¦å€ŸåŠ©å¤–éƒ¨ç¯å¢ƒä¿¡æ¯å¯¹è‡ªå·±è¿›è¡Œå®šä½ï¼Œäºæ˜¯å¼•å…¥æµ‹é‡å€¼$d$ï¼Œè®¡ç®—å‡ºå½“å‰ä½ç½®$x_2^{â€˜}$ï¼Œå†ç»“åˆé¢„æµ‹å€¼$x_2$ï¼Œå¾—åˆ°ä¸€ä¸ªçŸ«æ­£ä½ç½®$x_2^{â€˜â€™}$ï¼Œä½¿å…¶<strong>ä¸ç¡®å®šæ€§é™åˆ°æœ€å°</strong>ã€‚</p><p><img src="/2018/11/16/amcl/filter.png" alt=""></p><h4 id="è´å¶æ–¯æ»¤æ³¢ï¼š-p-x-z-frac-p-z-x-p-x-p-z"><a href="#è´å¶æ–¯æ»¤æ³¢ï¼š-p-x-z-frac-p-z-x-p-x-p-z" class="headerlink" title="è´å¶æ–¯æ»¤æ³¢ï¼š$p(x|z) = \frac{p(z|x)p(x)}{p(z)}$"></a>è´å¶æ–¯æ»¤æ³¢ï¼š$p(x|z) = \frac{p(z|x)p(x)}{p(z)}$</h4><p>å…ˆéªŒï¼š$p(x_t|u_t, x_{t-1})$ï¼Œé€šè¿‡é¢„æµ‹æ–¹ç¨‹å¾—åˆ°</p><p>ä¼¼ç„¶ï¼š$p(z_t| x_t)$ï¼Œé€šè¿‡æµ‹é‡æ–¹ç¨‹å¾—åˆ°</p><p>åéªŒï¼š$p(x_t|z_t)$ï¼Œé€šè¿‡è´å¶æ–¯æ–¹ç¨‹å¾—åˆ°</p><p>å¯¹äºä¸€èˆ¬çš„éçº¿æ€§ã€éé«˜æ–¯ç³»ç»Ÿï¼Œå¾ˆéš¾é€šè¿‡ä¸Šè¿°æ–¹æ³•å¾—åˆ°åéªŒæ¦‚ç‡çš„è§£æè§£ã€‚</p><h4 id="è’™ç‰¹å¡æ´›é‡‡æ ·ï¼š"><a href="#è’™ç‰¹å¡æ´›é‡‡æ ·ï¼š" class="headerlink" title="è’™ç‰¹å¡æ´›é‡‡æ ·ï¼š"></a>è’™ç‰¹å¡æ´›é‡‡æ ·ï¼š</h4><p>å‡è®¾èƒ½ä»ä¸€ä¸ªç›®æ ‡åˆ†å¸ƒ$p(x)$è·å¾—ä¸€ç³»åˆ—æ ·æœ¬$x_1, x2, â€¦, x_N$ï¼Œé‚£ä¹ˆå°±èƒ½åˆ©ç”¨è¿™äº›æ ·æœ¬å»ä¼°è®¡è¿™ä¸ªåˆ†å¸ƒçš„æŸäº›å‡½æ•°çš„æœŸæœ›å€¼ã€‚</p><script type="math/tex; mode=display">E(f(x)) = \int_a^{b}f(x)p(x)dx \approx\frac{f(x_1) + f(x_2) + ... + f(x_N)}{N}</script><p>è’™ç‰¹å¡æ´›é‡‡æ ·çš„æ ¸å¿ƒæ€æƒ³å°±æ˜¯ç”¨å‡å€¼æ¥ä»£æ›¿ç§¯åˆ†ã€‚</p><p>å‡è®¾å¯ä»¥ä»åéªŒæ¦‚ç‡ä¸­é‡‡æ ·åˆ°Nä¸ªæ ·æœ¬ï¼Œé‚£ä¹ˆåéªŒæ¦‚ç‡å¯ä»¥è¡¨ç¤ºä¸ºï¼š</p><script type="math/tex; mode=display">\hat p(x_t|z_{1:t}) = \frac{1}{N} \sum_{i=1}^{N}\delta(x_n - x_n^{i}) \approx p(x_t|z_{1:t})</script><h4 id="ç²’å­æ»¤æ³¢ï¼š"><a href="#ç²’å­æ»¤æ³¢ï¼š" class="headerlink" title="ç²’å­æ»¤æ³¢ï¼š"></a>ç²’å­æ»¤æ³¢ï¼š</h4><script type="math/tex; mode=display">\begin{align}E(f(x)) & \approx \int f(x_n) \hat p(x_t|z_{1:t}) dx \nonumber\\& = \frac{1}{N}\sum_N f(x_n) \delta(x_n - x_n^{i})\nonumber\\& = \frac{1}{N}\sum_N f(x_n^{i}) \nonumber\end{align}</script><p>ç”¨é‡‡æ ·ç²’å­ï¼ˆæœä»åéªŒæ¦‚ç‡ï¼‰çš„çŠ¶æ€å€¼ç›´æ¥å¹³å‡ä½œä¸ºæœŸæœ›å€¼ï¼Œè¿™å°±æ˜¯ç²’å­æ»¤æ³¢ã€‚</p><h4 id="MCLï¼šè’™ç‰¹å¡æ´›å®šä½ï¼ç²’å­æ»¤æ³¢å®šä½"><a href="#MCLï¼šè’™ç‰¹å¡æ´›å®šä½ï¼ç²’å­æ»¤æ³¢å®šä½" class="headerlink" title="MCLï¼šè’™ç‰¹å¡æ´›å®šä½ï¼ç²’å­æ»¤æ³¢å®šä½"></a><a href="https://en.wikipedia.org/wiki/Monte_Carlo_localization" target="_blank" rel="noopener">MCL</a>ï¼šè’™ç‰¹å¡æ´›å®šä½ï¼ç²’å­æ»¤æ³¢å®šä½</h4><ol><li><p>Randomly generate a bunch of particles</p></li><li><p>Predict next state of the particles</p></li><li><p>Update the weighting of the particles based on the measurement. </p></li><li><p>Resampleï¼šDiscard highly improbable particle and replace them with copies of the more probable particles. </p><blockquote><p>This leads to a new particle set with uniform importance weights, but with an increased number of particles near the three likely places. </p></blockquote></li><li><p>Compute the weighted mean and covariance of the set of particles to get a state estimate.</p></li></ol><p><strong>æƒå€¼é€€åŒ–ï¼š</strong>å¦‚æœä»»ç”±ç²’å­æƒå€¼å¢é•¿ï¼Œåªæœ‰å°‘æ•°ç²’å­çš„æƒå€¼è¾ƒå¤§ï¼Œå…¶ä½™ç²’å­çš„æƒå€¼å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œå˜æˆæ— æ•ˆç²’å­ï¼Œå› æ­¤éœ€è¦å¼•å…¥<strong>é‡é‡‡æ ·</strong>ã€‚é‡‡ç”¨$N_{eff}$è¡¡é‡ç²’å­æƒå€¼çš„é€€åŒ–ç¨‹åº¦ã€‚</p><script type="math/tex; mode=display">N_{eff} \approx \hat{N_{eff}} = \frac{1}{\sum_N (w_k^{i})^2}</script><p><strong>ç²’å­å¤šæ ·æ€§ï¼š</strong>é€šå¸¸æˆ‘ä»¬ä¼šèˆå¼ƒæƒå€¼è¾ƒå°çš„ç²’å­ï¼Œä»£ä¹‹ä»¥æƒå€¼è¾ƒå¤§çš„ç²’å­ã€‚è¿™æ ·ä¼šå¯¼è‡´æƒå€¼å°çš„ç²’å­é€æ¸ç»ç§ï¼Œç²’å­ç¾¤å¤šæ ·æ€§å‡å¼±ï¼Œä»è€Œä¸è¶³ä»¥è¿‘ä¼¼è¡¨å¾åéªŒå¯†åº¦ã€‚</p><h4 id="é‡è¦æ€§é‡‡æ ·"><a href="#é‡è¦æ€§é‡‡æ ·" class="headerlink" title="é‡è¦æ€§é‡‡æ ·"></a>é‡è¦æ€§é‡‡æ ·</h4><p>å®é™…ä¸ŠåéªŒæ¦‚ç‡å¹¶ä¸çŸ¥é“ï¼Œè°ˆä½•é‡‡æ ·ï¼ˆ$x_n^i$ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥ä»ä¸€ä¸ªå·²çŸ¥çš„åˆ†å¸ƒ$q(x|z)$é‡Œæ¥é‡‡æ ·ï¼Œé—´æ¥å¾—åˆ°æ»¤æ³¢å€¼ã€‚</p><script type="math/tex; mode=display">\begin{align}E(f(x_k))&  = \int f(x) \frac{p(x|z)}{q(x|z)} q(x|z) dx \nonumber\\& = \frac{E_{q(x|z)}W_k(x_k)f(x_k)}{E_{q(x|z)}W_k(x_k)}\nonumber\\& \approx \frac{\frac{1}{N} \sum_{i=1}^N W_k({x_k^{i})f(x_k^{i})}}{\frac{1}{N} \sum_{i=1}^N W_k({x_k^{i}})}\nonumber\\& = \sum_N \hat W_k(x_k^i)f(x_k^i) \nonumber\end{align}</script><p>ç›¸æ¯”è¾ƒäºåŸå§‹çš„å‡å€¼è¡¨ç¤ºï¼Œå˜æˆäº†åŠ æƒå¹³å‡å€¼ã€‚ä¸åŒç²’å­æ‹¥æœ‰äº†ä¸åŒçš„æƒé‡ã€‚</p><script type="math/tex; mode=display">\hat W_k(x_k^i) = \frac{W_k(x_k^i)}{\sum_N W_k(x_k^i)}\\W_k (x_k) \propto \frac{p(x_k|z_{1:k})}{q(x_k|z_{1:k})}</script><p>å·²çŸ¥çš„$q$åˆ†å¸ƒå«åš<strong>é‡è¦æ€§æ¦‚ç‡å¯†åº¦å‡½æ•°</strong>ã€‚</p><h4 id="é€’æ¨ç®—æ³•ï¼šåºè´¯é‡è¦æ€§é‡‡æ ·"><a href="#é€’æ¨ç®—æ³•ï¼šåºè´¯é‡è¦æ€§é‡‡æ ·" class="headerlink" title="é€’æ¨ç®—æ³•ï¼šåºè´¯é‡è¦æ€§é‡‡æ ·"></a>é€’æ¨ç®—æ³•ï¼šåºè´¯é‡è¦æ€§é‡‡æ ·</h4><script type="math/tex; mode=display">\{x_k^i, w_k^i\} = SIS(\{x_{k-1}, w_{k-1}\})_{i=1}^N, y_k)</script><p>é¦–å…ˆå‡è®¾é‡è¦æ€§åˆ†å¸ƒ$q(x|z)$æ»¡è¶³ï¼š</p><script type="math/tex; mode=display">q(x_k | x_{0:k-1}, y_{1:k}) = q(x_k|x_{k-1}, y_k)</script><p>å³åªå’Œå‰ä¸€æ—¶åˆ»çš„çŠ¶æ€$x_{k-1}$å’Œæµ‹é‡$y_k$æœ‰å…³ã€‚äºæ˜¯æœ‰ï¼š</p><script type="math/tex; mode=display">w_k^i \approx w_{k-1}^i \frac{p(y_k|x_k^i)p(x_k^i|x_{k-1}^i)}{q(x_k^i|x_{k-1}^i, y_k)}</script><p>ä¼ªä»£ç ï¼š</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">For i=<span class="number">1</span>:N</span><br><span class="line">(<span class="number">1</span>)é‡‡æ ·ï¼šå¼<span class="number">1</span></span><br><span class="line">(<span class="number">2</span>)æƒå€¼æ›´æ–°ï¼šå¼<span class="number">2</span></span><br><span class="line">End For</span><br><span class="line">æƒå€¼å½’ä¸€åŒ–</span><br><span class="line">åŠ æƒå¹³å‡å¾—åˆ°ç²’å­æ»¤æ³¢å€¼ï¼Œä¹Ÿå°±æ˜¯å½“å‰çŠ¶æ€çš„ä¼°è®¡å€¼</span><br><span class="line">é‡é‡‡æ ·</span><br></pre></td></tr></table></figure><h4 id="é‡é‡‡æ ·"><a href="#é‡é‡‡æ ·" class="headerlink" title="é‡é‡‡æ ·"></a>é‡é‡‡æ ·</h4><p>æ—¢ç„¶æƒé‡å°çš„é‚£äº›ç²’å­ä¸èµ·ä½œç”¨äº†ï¼Œé‚£å°±ä¸è¦äº†ã€‚ä¸ºäº†ä¿æŒç²’å­æ•°ç›®ä¸å˜ï¼Œå°±è¦è¡¥å……æ–°ç²’å­ï¼Œæœ€ç®€å•çš„åŠæ³•å°±æ˜¯å¤åˆ¶æƒé‡å¤§çš„ç²’å­ã€‚ç”¨$x_k^i$è¡¨ç¤ºkæ—¶åˆ»çš„ç²’å­ï¼Œ$x_k^j$è¡¨ç¤ºé‡é‡‡æ ·ä»¥åçš„ç²’å­ï¼Œé‚£ä¹ˆï¼š</p><script type="math/tex; mode=display">\tilde p (x_k|y_{1:k}) = \sum_N \frac{1}{N}\delta(x_k - x_k^j) = \sum_N \frac{n_i}{N}\delta(x_k - x_i^j)</script><p>æ€»çš„æ¥è¯´ï¼Œæ–°ç²’å­æŒ‰ç…§æƒé‡æ¯”ä¾‹æ¥è¡¥å……ï¼Œç®—æ³•æµç¨‹ä¸ºï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">è®¡ç®—æ¦‚ç‡ç´¯ç§¯å’Œwcum(N)</span><br><span class="line">ç”¨[<span class="number">0</span>,<span class="number">1</span>]ä¹‹é—´çš„å‡åŒ€åˆ†å¸ƒéšæœºé‡‡æ ·Nä¸ªå€¼u(N)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="number">1</span>:N:</span><br><span class="line">k = <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> u(i)&lt;wcum(k):</span><br><span class="line">k += <span class="number">1</span></span><br><span class="line">end <span class="keyword">while</span></span><br><span class="line">resample(i) = k</span><br></pre></td></tr></table></figure><p><img src="/2018/11/16/amcl/resample.png" alt=""></p><h4 id="SIRæ»¤æ³¢å™¨ï¼ˆSampling-Importance-Resampling-Filter-ï¼‰"><a href="#SIRæ»¤æ³¢å™¨ï¼ˆSampling-Importance-Resampling-Filter-ï¼‰" class="headerlink" title="SIRæ»¤æ³¢å™¨ï¼ˆSampling Importance Resampling Filter ï¼‰"></a>SIRæ»¤æ³¢å™¨ï¼ˆSampling Importance Resampling Filter ï¼‰</h4><p>é€‰å–ç‰¹å®šçš„é‡è¦æ€§æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼š</p><script type="math/tex; mode=display">q(x_k^i|x_{k-1}^i, y_k) = p(x_k^i|x_{k-1}^i)</script><p>äºæ˜¯æƒé‡æ›´æ–°å…¬å¼å¯ä»¥ç®€åŒ–ï¼š</p><script type="math/tex; mode=display">w_k^i \propto w_{k-1}^i \frac{p(z|x)p(x|x_{k-1})}{q(x|x_{k-1})}</script><p>ç”±äºé‡é‡‡æ ·ä»¥åï¼Œç²’å­åˆ†å¸ƒæ›´æ–°ï¼Œæƒå€¼ç»Ÿä¸€ä¸º$\frac{1}{N}$ï¼Œäºæ˜¯æƒé‡æ›´æ–°å…¬å¼è¿›ä¸€æ­¥ç®€åŒ–ï¼š</p><script type="math/tex; mode=display">w_k^i \propto p(z_k|x_k^i)</script><p>æ ¹æ®æµ‹é‡æ–¹ç¨‹å¯çŸ¥ï¼Œä¸Šé¢è¿™ä¸ªæ¦‚ç‡å°±æ˜¯ä»¥çœŸå®æµ‹é‡å€¼ä¸ºå‡å€¼ï¼Œä»¥å™ªå£°æ–¹å·®ä¸ºæ–¹å·®çš„é«˜æ–¯åˆ†å¸ƒã€‚</p><p>æ­¤ç®—æ³•ä¸­çš„é‡‡æ ·ï¼Œå¹¶æ²¡æœ‰åŠ å…¥æµ‹é‡$z_k$ï¼Œåªå‡­å…ˆéªŒçŸ¥è¯†$p(x_k|x_{k-1})$ï¼Œè™½ç„¶ç®€å•æ˜“ç”¨ï¼Œä½†æ˜¯å­˜åœ¨æ•ˆç‡ä¸é«˜å’Œå¯¹å¥‡å¼‚ç‚¹(outliers)æ•æ„Ÿçš„é—®é¢˜ã€‚ </p><h4 id="AMCL"><a href="#AMCL" class="headerlink" title="AMCL"></a>AMCL</h4><p>MCLç®—æ³•èƒ½å¤Ÿç”¨äºå…¨å±€å®šä½ï¼Œä½†æ˜¯æ— æ³•ä»<strong>æœºå™¨äººç»‘æ¶</strong>æˆ–<strong>å…¨å±€å®šä½å¤±è´¥</strong>ä¸­æ¢å¤è¿‡æ¥ï¼Œå› ä¸ºéšç€ä½ç½®è¢«è·å–ï¼Œå…¶ä»–åœ°æ–¹çš„ä¸æ­£ç¡®ç²’å­ä¼šé€æ¸æ¶ˆå¤±ã€‚ç¨³å®šçŠ¶æ€ä¸‹ï¼Œç²’å­åªâ€œç”Ÿå­˜â€åœ¨ä¸€ä¸ªå•ä¸€çš„å§¿æ€é™„è¿‘ï¼Œå¦‚æœè¿™ä¸ªå§¿æ€æ°å¥½ä¸æ­£ç¡®ï¼ˆåœ¨é‡é‡‡æ ·æ­¥éª¤ä¸­å¯èƒ½æ„å¤–çš„ä¸¢å¼ƒæ‰€æœ‰æ­£ç¡®ä½å§¿é™„è¿‘çš„ç²’å­ï¼‰ï¼Œç®—æ³•å°±æ— æ³•æ¢å¤ã€‚</p><p>AMCLå°±æ˜¯ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼šç»“åˆäº†è‡ªé€‚åº”ï¼ˆAugmented_MCLï¼‰å’Œåº“å°”è´å…‹-è±ä¸å‹’æ•£åº¦é‡‡æ ·ï¼ˆKLD_Sampling_MCLï¼‰</p><ul><li>Augmented_MCLï¼šåœ¨æœºå™¨äººé­åˆ°ç»‘æ¶çš„æ—¶å€™ï¼Œå®ƒä¼šåœ¨å‘ç°ç²’å­ä»¬çš„å¹³å‡åˆ†æ•°çªç„¶é™ä½äº†ï¼Œè¿™æ„å‘³ç€æ­£ç¡®çš„ç²’å­åœ¨æŸæ¬¡è¿­ä»£ä¸­è¢«æŠ›å¼ƒäº†ï¼Œæ­¤æ—¶ä¼šéšæœºçš„å…¨å±€æ³¨å…¥ç²’å­ï¼ˆinjection of random particlesï¼‰ã€‚</li><li>KLD_Sampling_MCLï¼šåŠ¨æ€è°ƒæ•´ç²’å­æ•°ï¼Œå½“æœºå™¨äººå®šä½å·®ä¸å¤šå¾—åˆ°äº†çš„æ—¶å€™ï¼Œç²’å­éƒ½é›†ä¸­åœ¨ä¸€å—äº†ï¼Œå°±æ²¡å¿…è¦ç»´æŒè¿™ä¹ˆå¤šçš„ç²’å­äº†â€”â€”åœ¨æ …æ ¼åœ°å›¾ä¸­ï¼Œçœ‹ç²’å­å äº†å¤šå°‘æ …æ ¼ã€‚å å¾—å¤šï¼Œè¯´æ˜ç²’å­å¾ˆåˆ†æ•£ï¼Œåœ¨æ¯æ¬¡è¿­ä»£é‡é‡‡æ ·çš„æ—¶å€™ï¼Œå…è®¸ç²’å­æ•°é‡çš„ä¸Šé™é«˜ä¸€äº›ã€‚å å¾—å°‘ï¼Œè¯´æ˜ç²’å­éƒ½å·²ç»é›†ä¸­äº†ï¼Œé‚£å°±å°†ä¸Šé™è®¾ä½ã€‚</li></ul><p><img src="/2018/11/16/amcl/mcl&amp;amcl.png" alt="mcl&amp;amcl"></p><p>ç®—æ³•æµç¨‹ä¸Šçœ‹ï¼Œaugmented_MCLç®—æ³•æœ€æ˜¾è‘—çš„åŒºåˆ«å°±æ˜¯å¼•å…¥äº†å››ä¸ªå‚æ•°ç”¨äºå¤±æ•ˆæ¢å¤ï¼š</p><ul><li>$w_{slow}$ï¼šé•¿æœŸä¼¼ç„¶å¹³å‡ä¼°è®¡</li><li>$w_{fast}$ï¼šçŸ­æœŸä¼¼ç„¶å¹³å‡ä¼°è®¡</li><li>$\alpha_{slow}$ï¼šé•¿æœŸæŒ‡æ•°æ»¤æ³¢å™¨è¡°å‡ç‡</li><li>$\alpha_{fast}$ï¼šçŸ­æœŸæŒ‡æ•°æ»¤æ³¢å™¨è¡°å‡ç‡</li></ul><p>å¤±æ•ˆæ¢å¤çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šæµ‹é‡ä¼¼ç„¶çš„ä¸€ä¸ªçªç„¶è¡°å‡ï¼ˆçŸ­æœŸä¼¼ç„¶åŠ£äºé•¿æœŸä¼¼ç„¶ï¼‰è±¡å¾ç€ç²’å­è´¨é‡çš„ä¸‹é™ï¼Œè¿™å°†å¼•èµ·éšæœºé‡‡æ ·æ•°ç›®çš„å¢åŠ ã€‚</p><p>$w_{avg}$è®¡ç®—äº†ç²’å­çš„å¹³å‡æƒé‡ï¼Œå½“ç²’å­è´¨é‡ä¸‹é™æ—¶ï¼Œå¹³å‡æƒé‡éšä¹‹ä¸‹é™ï¼Œ$w_{slow}ã€w_{fast}$ä¹Ÿä¼šéšä¹‹ä¸‹é™ï¼Œä½†æ˜¯æ˜¾ç„¶$w_{fast}$ä¸‹é™çš„é€Ÿåº¦è¦å¿«äº$w_{slow}$â€”â€”è¿™ç”±è¡°å‡ç‡å†³å®šï¼Œå› æ­¤éšæœºæ¦‚ç‡$p = 1 - \frac{w_{fast}}{w_{slow}}$ä¼šå¢å¤§ï¼Œéšæœºç²’å­æ•°ç›®å¢åŠ ã€‚è€Œå½“ç²’å­è´¨é‡æé«˜æ—¶ï¼Œç²’å­çŸ­æœŸæƒé‡è¦å¥½äºé•¿æœŸï¼Œéšæœºæ¦‚ç‡å°äº0ï¼Œä¸ç”Ÿæˆéšæœºç²’å­ã€‚</p><p><img src="/2018/11/16/amcl/é‡å®šä½.png" alt="é‡å®šä½"></p><h4 id="ROS-amclå‚æ•°è§£æ"><a href="#ROS-amclå‚æ•°è§£æ" class="headerlink" title="ROS amclå‚æ•°è§£æ"></a>ROS amcl<a href="https://www.cnblogs.com/dyan1024/p/7825988.html" target="_blank" rel="noopener">å‚æ•°è§£æ</a></h4><p>Augmented_MCLï¼š</p><ul><li><code>&lt;param name=&quot;recovery_alpha_slow&quot; value=&quot;0.0&quot;/&gt;</code>ï¼šé»˜è®¤0ï¼ˆMCLï¼‰ï¼Œæˆ‘çš„0.001ã€‚</li><li><code>&lt;param name=&quot;recovery_alpha_fast&quot; value=&quot;0.0&quot;/&gt;</code>ï¼šé»˜è®¤0ï¼ˆMCLï¼‰ï¼Œæˆ‘çš„0.8ã€‚</li><li>åœ¨rvizé‡Œé€šè¿‡2D Pose EstimateæŒ‰é’®ç§»åŠ¨æœºå™¨äººæ¥è§¦å‘ï¼Œæœºå™¨äººä½ç½®çªå˜åè¦è¿‡ä¸€ä¼šå„¿æ‰æ³¨å…¥éšæœºç²’å­ï¼Œå› ä¸ºæ¦‚ç‡æ˜¯æ¸å˜çš„ã€‚</li></ul><p>KLDï¼š</p><ul><li><code>&lt;param name=&quot;kld_z&quot; value=&quot;0.99&quot;/&gt;</code>ï¼š KLDé‡‡æ ·ä»¥æ¦‚ç‡$1-\deltaï¼ˆkld_zï¼‰$ç¡®å®šæ ·æœ¬æ•°ã€‚</li><li><code>&lt;param name=&quot;kld_err&quot; value=&quot;0.05&quot;/&gt;</code>ï¼š çœŸå®çš„åéªŒä¸åŸºäºé‡‡æ ·çš„è¿‘ä¼¼ä¹‹é—´çš„è¯¯å·®ã€‚</li></ul><p>åŠ¨æ€éšœç¢ç‰©ï¼šç¯å¢ƒä¸­çš„åŠ¨æ€ç‰©ä½“æ€»æ˜¯ä¼šè·å¾—æ¯”é™æ€éšœç¢ç‰©æ›´çŸ­çš„è¯»æ•°ï¼Œå› æ­¤å¯ä»¥æ ¹æ®è¿™æ ·çš„ä¸å¯¹ç§°æ€§å»é™¤å¼‚å¸¸å€¼ã€‚</p><ul><li>é™æ€éšœç¢ç‰©åº”è¯¥æœä»ç¨³å®šçš„é«˜æ–¯åˆ†å¸ƒï¼Œä»¥è·ç¦»ä¼ æ„Ÿå™¨çš„çœŸå®è·ç¦»ä¸ºå‡å€¼ã€‚</li><li>æ‰«æåˆ°åŠ¨æ€ç›®æ ‡çš„beamåˆ™æœä»è¡°å‡åˆ†å¸ƒï¼Œ$-\eta e ^{-\lambda z}$ã€‚</li><li><code>laser_model_type</code>ï¼šä½¿ç”¨beam modelæ—¶ä¼šç”¨åˆ°å››ä¸ª<strong>æ··åˆæƒé‡</strong>å‚æ•°z_hitï¼Œz_shortï¼Œz_maxå’Œz_randï¼Œä½¿ç”¨likelihood_field modelæ—¶ä½¿ç”¨ä¸¤ä¸ªz_hitå’Œz_randã€‚<ul><li><code>laser_z_hit</code>ï¼šdefault=0.95ï¼Œä»¥çœŸå®å€¼ä¸ºå‡å€¼çš„å™ªå£°é«˜æ–¯åˆ†å¸ƒ</li><li><code>laser_z_rand</code>ï¼šdefualt=0.05ï¼Œéšæœºæµ‹é‡æƒé‡ï¼Œå‡åŒ€åˆ†å¸ƒ</li><li><code>laser_z_short</code>ï¼šdefault=0.1ï¼Œæ„å¤–å¯¹è±¡æƒé‡ï¼Œè¡°å‡åˆ†å¸ƒ</li><li><code>laser_z_max</code>ï¼šdefault=0.05ï¼Œæµ‹é‡å¤±è´¥æƒé‡ï¼Œ0/1åˆ†å¸ƒ</li></ul></li></ul><p>åˆå§‹ä½å§¿ï¼š</p><ul><li><p>å¯ä»¥åœ¨rvizé‡Œé€šè¿‡2D Pose EstimateæŒ‰é’®è®¾å®šï¼ˆrvizä¼šå‘å¸ƒinitialPoseè¯é¢˜ï¼‰ã€‚</p></li><li><p>æˆ–è€…å†™åœ¨launchæ–‡ä»¶ä¸­ï¼š</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">"initial_pose_x"</span>            <span class="attr">value</span>=<span class="string">"0.0"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">"initial_pose_y"</span>            <span class="attr">value</span>=<span class="string">"0.0"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">"initial_pose_a"</span>            <span class="attr">value</span>=<span class="string">"0.0"</span>/&gt;</span>      </span><br><span class="line"><span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">"initial_cov_xx"</span>            <span class="attr">value</span>=<span class="string">"0.25"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">"initial_cov_yy"</span>            <span class="attr">value</span>=<span class="string">"0.25"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">"initial_cov_aa"</span>            <span class="attr">value</span>=<span class="string">"(pi/12)*(pi/12)"</span>/&gt;</span></span><br></pre></td></tr></table></figure></li><li><p><a href="http://wiki.ros.org/Robots/TIAGo/Tutorials/Navigation/Localization" target="_blank" rel="noopener">è°ƒç”¨å…¨å±€å®šä½æœåŠ¡</a>ï¼š</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rosservice call /global_localization <span class="string">"&#123;&#125;"</span></span><br></pre></td></tr></table></figure><p>ä½å§¿éšæœºåˆå§‹åŒ–ï¼Œç²’å­æ´’æ»¡åœ°å›¾ï¼š</p><p><img src="/2018/11/16/amcl/global.png" alt=""></p></li></ul><p>transform_toleranceï¼š</p><ul><li>é»˜è®¤æ˜¯0.1secondsï¼Œå®˜æ–¹å®šä¹‰æ˜¯Time with which to post-date the transform that is published, to indicate that this transform is valid into the future. tfå˜æ¢å‘å¸ƒæ¨è¿Ÿçš„æ—¶é—´ï¼Œæ„æ€æ˜¯tfå˜æ¢åœ¨æœªæ¥è¿™æ®µæ—¶é—´å†…æ˜¯å¯ç”¨çš„ã€‚</li><li>ã€å­˜ç–‘ã€‘æˆ‘ä¸ªäººç†è§£tfçš„æ›´æ–°é¢‘ç‡åº”è¯¥è¶Šå¿«è¶Šå‡†ç¡®ï¼Œlaunchæ–‡ä»¶ä¸­æœ€å¼€å§‹è®¾å®šä¸º0.5ï¼Œä½†æ˜¯å®é™…ä¸Šæœºå™¨äººç§»åŠ¨é€Ÿåº¦è°ƒå¿«æ—¶ï¼Œä¼šæŠ¥é”™<code>Costmap2DROS transform timeout...Could not get robot pose, cancelling reconfiguration</code>ï¼Œç„¶åæˆ‘è°ƒæ•´ä¸º1.5å°±ä¸æŠ¥é”™äº†ã€‚</li><li>ç›®å‰è®¾å®šä¸º1.0ï¼Œä»¿çœŸé‡Œè§‚æµ‹ä¸å‡ºå·®å¼‚ã€‚</li></ul>]]></content>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>recovery behavior</title>
      <link href="/2018/11/16/recovery-behavior/"/>
      <url>/2018/11/16/recovery-behavior/</url>
      <content type="html"><![CDATA[<p>navigation stackçš„move_baseåŒ…ä¸­ä¸€ä¸ªæ’ä»¶ã€‚<a href="https://amberzzzz.github.io/2018/11/15/dynamic-window-approach/">DWA</a>çš„é€Ÿåº¦ç©ºé—´ä¸­å¦‚æœæ²¡æœ‰å¯è¡Œçš„é‡‡æ ·ç‚¹ï¼Œé‚£ä¹ˆæœºå™¨äººget stuckï¼Œè§¦å‘recoveryè¡Œä¸ºã€‚</p><p>recoveryè¡Œä¸ºçš„å®è´¨æ˜¯clear out spaceâ€”â€”è¯•å›¾ææ¸…æ¥šè‡ªå·±çš„å¤„å¢ƒï¼š</p><ol><li>é¦–å…ˆæœºå™¨äººä¼šæ¸…æ‰«åœ°å›¾â€”â€”conservative reset</li><li>ç„¶ååŸåœ°æ—‹è½¬360åº¦ï¼Œåˆ·æ–°å¤„å¢ƒâ€”â€”clearing rotation</li><li>å¦‚æœè¿˜æ˜¯å¯¼èˆªå¤±è´¥ï¼Œæœºå™¨äººä¼šæ›´åŠ æ¿€è¿›çš„æ¸…æ‰«åœ°å›¾â€”â€”aggressive reset</li><li>ç„¶ååŸåœ°æ—‹è½¬360åº¦ï¼Œåˆ·æ–°å¤„å¢ƒâ€”â€”clearing rotation</li><li>å¦‚æœä»æ—§å¤±è´¥â€”â€”mission impossible</li></ol><p><img src="/2018/11/16/recovery-behavior/recovery_behaviors.png" alt=""></p><p>æºä»£ç åœ¨move_base.cppé‡Œé¢ï¼Œç»§æ‰¿äº†nav_coreçš„æ¥å£ï¼Œè®¾ç½®åœ¨move_base_params.yamlé…ç½®æ–‡ä»¶ä¸­ã€‚</p><ul><li>nav_coreçš„recovery_behavior.hå°è£…äº†RecoveryBehaviorç±»ã€‚</li><li>move_baseä¸­åˆ›å»ºäº†åä¸ºâ€clear_costmap_recovery/ClearCostmapRecoveryâ€ã€â€rotate_recovery/RotateRecoveryâ€ã€â€clear_costmap_recovery/ClearCostmapRecoveryâ€çš„é»˜è®¤å¯¹è±¡ã€‚</li><li>move_baseçš„ä¸»ç¨‹åºæ˜¯ä¸€ä¸ªçŠ¶æ€æœºï¼Œcase CLEARINGå°±è°ƒç”¨RecoveryBehaviorçš„runBehavior()ã€‚</li></ul>]]></content>
      
      
        <tags>
            
            <tag> ROS </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>dynamic window approach</title>
      <link href="/2018/11/15/dynamic-window-approach/"/>
      <url>/2018/11/15/dynamic-window-approach/</url>
      <content type="html"><![CDATA[<p><strong>åŠ¨æ€çª—å£ï¼š</strong></p><p>çª—å£æ¡†çš„æ˜¯é€Ÿåº¦ç©ºé—´çš„é‡‡æ ·ç‚¹$(v_t, w_t)$ï¼Œä¸€å¯¹$(v_t, w_t)$å°±ä»£è¡¨ä¸€æ®µè½¨è¿¹ï¼Œè½¨è¿¹é€šè¿‡æœºå™¨äººåº•ç›˜çš„è¿åŠ¨å­¦å»ºæ¨¡å¾—åˆ°ã€‚</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// base_local_plannerçš„simple_trajectory_generator.cpp</span></span><br><span class="line"><span class="comment">// ç›´çº¿æ¨¡å‹</span></span><br><span class="line"><span class="keyword">double</span> dt = (current_time - last_time).toSec();</span><br><span class="line"><span class="keyword">double</span> delta_x = (vx * <span class="built_in">cos</span>(theta) - vy * <span class="built_in">sin</span>(theta)) * dt;</span><br><span class="line"><span class="keyword">double</span> delta_y = (vx * <span class="built_in">sin</span>(theta) + vy * <span class="built_in">cos</span>(theta)) * dt;</span><br><span class="line"><span class="keyword">double</span> delta_th = vth * dt;</span><br><span class="line"></span><br><span class="line">x += delta_x;</span><br><span class="line">y += delta_y;</span><br><span class="line">theta += delta_th;</span><br></pre></td></tr></table></figure><p><strong>çª—å£çš„é€‰æ‹©ï¼š</strong></p><ol><li><p>é€Ÿåº¦é™åˆ¶</p><script type="math/tex; mode=display">(V, W) = \{v \in[v_{min}, v_{max}], w \in [w_{min}, w_{max}] \}</script></li><li><p>åŠ é€Ÿåº¦é™åˆ¶</p><script type="math/tex; mode=display">(V, W) = \left\{\begin{array}& v \in[v_c - \dot{v}*\Delta t, v_c + \dot{v}*\Delta t], \\w \in [w_c - \dot{w}*\Delta t, w_c + \dot{w}*\Delta t] \end{array}\right\}</script></li><li><p>éšœç¢ç‰©åˆ¶åŠ¨é™åˆ¶</p><script type="math/tex; mode=display">(V, W) = \left\{v \leq \sqrt{2*dist(v,w)*\dot{v}}, w \leq \sqrt{2*dist(v,w)*\dot{w}}\right\}</script><p>$dist(v,w)$è¡¨ç¤ºé‡‡æ ·ç‚¹$(v, w)$å¯¹åº”è½¨è¿¹ä¸Šç¦»éšœç¢ç‰©æœ€è¿‘çš„è·ç¦»ã€‚</p></li></ol><p>ç¡®å®šçª—å£åè¿›è¡Œé‡‡æ ·ï¼Œå¯ä»¥å¾—åˆ°ä¸€ç³»åˆ—è½¨è¿¹ï¼š</p><p><img src="/2018/11/15/dynamic-window-approach/è½¨è¿¹.png" alt=""></p><p><strong>è½¨è¿¹çš„é€‰æ‹©ï¼š</strong></p><p>åŸå§‹è®ºæ–‡ä¸­é‡‡ç”¨è¯„ä»·å‡½æ•°ï¼š</p><script type="math/tex; mode=display">G(v,w) = \sigma [\alpha * heading(v, w) + \beta * dist(v,w) + \gamma * velocity(v,w)]</script><ol><li>æ–¹ä½è§’è¯„ä»·å‡½æ•°ï¼šé‡‡ç”¨å½“å‰é‡‡æ ·ç‚¹è®¾å®šä¸‹ï¼Œè¾¾åˆ°æ¨¡æ‹Ÿè½¨è¿¹æœ«ç«¯æ—¶æœºå™¨äººçš„æœå‘è§’ä¸ç›®æ ‡ç‚¹æœå‘è§’çš„å·®è·ã€‚</li><li>ç©ºéš™è¯„ä»·ï¼šå½“å‰æ¨¡æ‹Ÿè½¨è¿¹ä¸Šä¸æœ€è¿‘éšœç¢ç‰©ä¹‹é—´çš„è·ç¦»ã€‚</li><li>é€Ÿåº¦è¯„ä»·ï¼šé‡‡æ ·ç‚¹é€Ÿåº¦ä¸æœ€å¤§é€Ÿåº¦çš„å·®è·ã€‚</li></ol><p>ä¸Šè¿°è¯„ä»·å‡½æ•°è¦è¿›è¡Œ<strong>å½’ä¸€åŒ–</strong>ã€‚</p><p>ç®—æ³•æµç¨‹ï¼š</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">BEGIN <span class="title">DWA</span><span class="params">(robotPose,robotGoal,robotModel)</span></span></span><br><span class="line"><span class="function">    laserscan </span>= readScanner()</span><br><span class="line">    allowable_v = generateWindow(robotV, robotModel)</span><br><span class="line">    allowable_w  = generateWindow(robotW, robotModel)</span><br><span class="line"><span class="keyword">for</span> each v in allowable_v</span><br><span class="line">        <span class="keyword">for</span> each w in allowable_w</span><br><span class="line">            dist = find_dist(v,w,laserscan,robotModel)</span><br><span class="line">            breakDist = calculateBreakingDistance(v)</span><br><span class="line">            <span class="keyword">if</span> (dist &gt; breakDist)  <span class="comment">//can stop in time</span></span><br><span class="line">                heading = hDiff(robotPose,goalPose, v,w) </span><br><span class="line">                clearance = (dist-breakDist)/(dmax - breakDist) </span><br><span class="line">                cost = costFunction(heading,clearance, <span class="built_in">abs</span>(desired_v - v))</span><br><span class="line">                <span class="keyword">if</span> (cost &gt; optimal)</span><br><span class="line">                    best_v = v</span><br><span class="line">                    best_w = w</span><br><span class="line">                    optimal = cost</span><br><span class="line"><span class="built_in">set</span> robot trajectory to best_v, best_w</span><br><span class="line">END</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>A-star algorithm</title>
      <link href="/2018/11/14/A-star-algorithm/"/>
      <url>/2018/11/14/A-star-algorithm/</url>
      <content type="html"><![CDATA[<p>æœ‰ç›®çš„æ€§åœ°å¯»æ‰¾æœ€ä½³è·¯å¾„ï¼Œé¦–å…ˆå®šä¹‰ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œè¡¨ç¤ºèŠ‚ç‚¹æ¶ˆè€—ï¼š</p><script type="math/tex; mode=display">f = g+h</script><p>$g$è¡¨ç¤ºèµ·ç‚¹åˆ°å½“å‰èŠ‚ç‚¹çš„å·²çŸ¥æ¶ˆè€—</p><p>$h$è¡¨ç¤ºå¯¹å½“å‰èŠ‚ç‚¹åˆ°ç»ˆç‚¹æ¶ˆè€—çš„çŒœæµ‹ï¼Œ<strong>ä¼°ä»·å‡½æ•°</strong>æœ‰å¤šç§å½¢å¼â€”â€”å¯å‘å¼æ¢ç´¢çš„æ ¸å¿ƒ</p><p>ç®—æ³•æµç¨‹ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">åˆå§‹åŒ–openList</span><br><span class="line">åˆå§‹åŒ–closeList</span><br><span class="line">å°†èµ·ç‚¹æ”¾å…¥openList</span><br><span class="line"><span class="keyword">while</span> openListéç©ºï¼š</span><br><span class="line">æ‰¾åˆ°å¼€å¯åˆ—è¡¨ä¸Šfæœ€å°çš„èŠ‚ç‚¹ï¼Œè®°ä¸ºq</span><br><span class="line">    æ‰¾åˆ°qå‘¨å›´çš„å­èŠ‚ç‚¹pï¼Œè®°å…¶çˆ¶èŠ‚ç‚¹ä¸ºq</span><br><span class="line">    <span class="keyword">for</span> æ¯ä¸€ä¸ªå­èŠ‚ç‚¹pï¼š</span><br><span class="line">    <span class="keyword">if</span> pæ˜¯ç»ˆç‚¹ï¼š</span><br><span class="line">        ç®—æ³•ç»ˆæ­¢</span><br><span class="line">            </span><br><span class="line">        p.g = q.g + qp</span><br><span class="line">        p.h = h(p, terminate)</span><br><span class="line">        p.f = p.g + p.h</span><br><span class="line">        <span class="keyword">if</span> på·²ç»åœ¨å¼€å¯åˆ—è¡¨ä¸­ä¸”ä¿å­˜çš„få€¼å°äºå½“å‰è®¡ç®—å€¼||påœ¨å…³é—­åˆ—è¡¨ä¸­ï¼š</span><br><span class="line">        è·³è¿‡è¯¥èŠ‚ç‚¹</span><br><span class="line"><span class="keyword">else</span>ï¼š</span><br><span class="line">        <span class="keyword">if</span> på·²ç»åœ¨å¼€å¯åˆ—è¡¨ä¸­ï¼š</span><br><span class="line">        ä¿®æ”¹è¯¥èŠ‚ç‚¹çš„ä¿¡æ¯ï¼ˆçˆ¶èŠ‚ç‚¹ã€fghï¼‰</span><br><span class="line"><span class="keyword">else</span>ï¼š</span><br><span class="line">            å°†è¯¥èŠ‚ç‚¹åŠ å…¥openList</span><br><span class="line">å°†qæ”¾å…¥closeList</span><br></pre></td></tr></table></figure><p>ç®—æ³•æ€§èƒ½åœ¨ç»†èŠ‚ä¸Šçš„ä¼˜åŒ–ï¼š<a href="http://theory.stanford.edu/~amitp/GameProgramming/" target="_blank" rel="noopener">http://theory.stanford.edu/~amitp/GameProgramming/</a></p><p><strong>åºè¨€ï¼šè·¯å¾„æœç´¢ç®—æ³•çš„å‰ä¸–ä»Šç”Ÿ</strong></p><p><strong>Dijkstraç®—æ³•ï¼š</strong>ä»åˆå§‹èŠ‚ç‚¹å¼€å§‹å‘å¤–æ‰©å±•ï¼Œç›´åˆ°åˆ°è¾¾ç›®æ ‡èŠ‚ç‚¹ã€‚ç®—æ³•ä¿è¯èƒ½æ‰¾åˆ°ä»åˆå§‹ç‚¹åˆ°ç›®æ ‡ç‚¹çš„æœ€çŸ­è·¯å¾„ã€‚</p><p><img src="/2018/11/14/A-star-algorithm/Dç®—æ³•.png" alt=""></p><p><strong>æœ€ä½³ä¼˜å…ˆæœç´¢BFSç®—æ³•ï¼š</strong>ç®—æ³•èƒ½å¤Ÿè¯„ä¼°ä»»æ„èŠ‚ç‚¹åˆ°ç›®æ ‡ç‚¹çš„ä»£ä»·ï¼Œå¹¶ä¼˜å…ˆé€‰æ‹©ç¦»ç›®æ ‡æœ€è¿‘çš„ç»“ç‚¹ã€‚å¯å‘å¼ç®—æ³•ï¼Œæ¯”Dijkstraç®—æ³•è¿è¡Œå¿«å¾—å¤šï¼Œä½†æ˜¯ä¸èƒ½ä¿è¯è·¯å¾„æœ€çŸ­ã€‚</p><p><img src="/2018/11/14/A-star-algorithm/BFSç®—æ³•.png" alt=""></p><p>å¦‚ä¸‹é¢è¿™ç§æƒ…å†µï¼š</p><p><img src="/2018/11/14/A-star-algorithm/D1.png" alt="D1">     <img src="/2018/11/14/A-star-algorithm/B1.png" alt="B1"></p><p>å› ä¸ºBFSæ˜¯åŸºäºè´ªå¿ƒç­–ç•¥çš„ï¼Œå®ƒåªå…³æ³¨åˆ°å°½å¯èƒ½å‘ç€ç›®æ ‡ç‚¹ç§»åŠ¨ï¼Œè€Œä¸è€ƒè™‘å·²èŠ±è´¹çš„ä»£ä»·ã€‚Dijkstraç®—æ³•åˆ™æ­£ç›¸åï¼Œå®ƒä¼šç¡®ä¿æ¯ä¸€æ­¥éƒ½æ˜¯æœ€ä¼˜çš„ï¼Œä½†æ˜¯ä¸ºæ­¤è¦éå†å‘¨å›´å…¨éƒ¨çš„èŠ‚ç‚¹ã€‚</p><p><strong>A*ç®—æ³•ï¼š</strong>å°†ä¸¤ç§è·¯å¾„æœç´¢ç®—æ³•çš„æ€æƒ³ç»“åˆèµ·æ¥ï¼Œè€ƒè™‘ä¸¤ä¸ªæç«¯åŠå…¶ä¸­é—´çš„æƒ…å†µï¼š</p><ul><li><p>å¦‚æœ$h(n)$æ˜¯0ï¼Œåªæœ‰$g(n)$èµ·ä½œç”¨ï¼Œé‚£ä¹ˆç®—æ³•æ¼”å˜ä¸ºDijkstraç®—æ³•ã€‚</p></li><li><p>å¦‚æœ$h(n)$èƒ½å¤Ÿå§‹ç»ˆæ»¡è¶³â€œæ¯”å½“å‰èŠ‚ç‚¹ç§»åŠ¨åˆ°ç›®æ ‡èŠ‚ç‚¹çš„å®é™…ä»£ä»·å°â€ï¼Œé‚£ä¹ˆç®—æ³•ä¿è¯èƒ½å¤Ÿæ‰¾åˆ°æœ€çŸ­è·¯å¾„ã€‚ï¼ˆ$h(n)$è¶Šå°ï¼Œç®—æ³•æ‰©å±•çš„èŠ‚ç‚¹æ•°å°±è¶Šå¤šï¼‰</p></li><li>å¦‚æœ$h(n)$èƒ½å¤Ÿç²¾ç¡®ç­‰äºâ€œå½“å‰èŠ‚ç‚¹ç§»åŠ¨åˆ°ç›®æ ‡èŠ‚ç‚¹çš„å®é™…ä»£ä»·â€ï¼Œé‚£ä¹ˆç®—æ³•å°†ä¼šä»…ä»…æ‰©å±•æœ€ä¼˜è·¯å¾„ã€‚è€Œä¸æ‰©å±•å…¶ä»–èŠ‚ç‚¹ï¼Œç®—æ³•è¿è¡Œéå¸¸å¿«ã€‚</li><li>å¦‚æœ$h(n)$æœ‰æ—¶ä¼šâ€œæ¯”å½“å‰èŠ‚ç‚¹ç§»åŠ¨åˆ°ç›®æ ‡èŠ‚ç‚¹çš„å®é™…ä»£ä»·å¤§â€ï¼Œé‚£ä¹ˆæ­¤æ—¶ç®—æ³•ä¸èƒ½ä¿è¯æœ€çŸ­è·¯å¾„äº†ã€‚</li><li>å¦‚æœ$g(n)$æ¯”$h(n)$å°çš„å¤šï¼Œåªæœ‰$h(n)$èµ·ä½œç”¨ï¼Œé‚£ä¹ˆç®—æ³•æ¼”å˜ä¸ºBFSç®—æ³•ã€‚</li></ul><p><strong>ä¼°ä»·å‡½æ•°Heuristic function $h(a, b)$</strong> </p><p>ä¼°ä»·å‡½æ•°çš„é€‰æ‹©å¯ä»¥followä»¥ä¸‹çš„instructionsï¼š</p><ol><li><p>square grid that allows 4 directionsï¼šuse Manhattan distance (L1)</p><script type="math/tex; mode=display">dx = abs(a.x - b.x)\\dy = abs(a.y - b.y)\\dis = D*(dx+dy)</script></li><li><p>square grid that allows 8 directionsï¼šuse Diagonal distance (Lâˆ)</p><script type="math/tex; mode=display">dx = abs(a.x - b.x)\\dy = abs(a.y - b.y)\\dis = D*(dx+dy) + (D2 - 2*D)*min(dx, dy)</script><p>å½“$D = D2 =1$æ—¶ï¼Œ$dis = dx + dy -min(dx, dy) = max(dx, dy)$ï¼Œè¿™ä¸ªè·ç¦»ç§°ä¸ºåˆ‡æ¯”é›ªå¤«è·ç¦»ã€‚</p><p>å½“$D = 1, D2 =\sqrt 2$æ—¶ï¼Œè¿™ä¸ªè·ç¦»ç§°ä¸º<em>octile distance</em>ã€‚</p></li><li><p>square grid that allows any direcitonsï¼šuse Euclidean distance (L2)</p><script type="math/tex; mode=display">dx = abs(a.x - b.x)\\dy = abs(a.y - b.y)\\dis = D*\sqrt{dx*dx + dy*dy}</script><blockquote><p>If A* is finding paths on the grid but you are allowing movement not on the grid, you may want to consider other representations of the map</p></blockquote><p>â€‹    æ¬§å‡ é‡Œå¾—è·ç¦»å¹¶ä¸é€‚ç”¨äºæ …æ ¼åœ°å›¾ï¼Œå› ä¸ºè¿™ä¼šå¯¼è‡´ä»£ä»·å‡½æ•°gå’Œä¼°ä»·å‡½æ•°çš„ä¸åŒ¹é…ï¼ˆä»£ä»·å‡½æ•°å¹¶ä¸æ˜¯è¿ç»­çš„ï¼‰ã€‚</p><p>ç”±äºæ¬§å‡ é‡Œå¾—è·ç¦»å¼•å…¥äº†å¼€æ ¹å·è®¡ç®—ï¼Œä¸€äº›ç®—æ³•ä¼šç›´æ¥ç”¨$dis = D<em>(dx</em>dx + dy<em>dy)$æ¥ä»£æ›¿ï¼Œ<em>*ä¸å»ºè®®ï¼</em></em>ï¼Œä¼šå¼•å…¥å°ºåº¦é—®é¢˜ï¼Œ$f = g + h$ï¼Œå…¶ä¸­ä»£ä»·å‡½æ•°ä¼šé€æ¸å¢é•¿ï¼Œä¼°ä»·å‡½æ•°åˆ™é€æ¸å‡å°ï¼Œå¹³æ–¹ä¼šå¯¼è‡´ä¸¤ä¸ªå‡½æ•°çš„å˜åŒ–é€Ÿç‡ä¸matchï¼Œä½¿å¾—ä¼°ä»·å‡½æ•°çš„æƒé‡è¿‡å¤§ï¼Œå¯¼è‡´BFSã€‚</p></li></ol><p>codeï¼š</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">public <span class="function"><span class="keyword">function</span> <span class="title">manhattanHeuristic</span>(<span class="params">a:Object, b:Object</span>):<span class="title">Number</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> graph.distance(a, b) + simpleCost(a, b) - <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public <span class="function"><span class="keyword">function</span> <span class="title">simpleCost</span>(<span class="params">a:Object, b:Object</span>):<span class="title">Number</span> </span>&#123;</span><br><span class="line"><span class="keyword">var</span> c:<span class="built_in">Number</span> = costs[graph.nodeToString(b)];</span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">isNaN</span>(c)) &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> c;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// simpleCosté™å®šä¸ºå°äºç­‰äº1çš„æ•°</span></span><br></pre></td></tr></table></figure><p>æ­¤æ—¶$h(a,b) = dis(a,b)+c-1 \leq h^{*}(a,b)$ï¼Œæ­¤æ—¶èƒ½å¤Ÿæ‰¾åˆ°æœ€ä¼˜è§£ã€‚</p>]]></content>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>branch and bound</title>
      <link href="/2018/11/13/branch-and-bound/"/>
      <url>/2018/11/13/branch-and-bound/</url>
      <content type="html"><![CDATA[<p>ä¸€ä¸ªæ —å­ï¼šæ•´æ•°è§„åˆ’é—®é¢˜æ¬²æ±‚$max\ z = 5x_1 + 8x_2$</p><script type="math/tex; mode=display">\left\{\begin{align} &   x_1 + x_2 \leq 6 \nonumber\\  &   5x_1 + 9x_2 \leq 45 \nonumber\\  & x_1\geq0, x_2\geq0 \nonumber\\  & x_1,x_2ä¸ºæ•´æ•° \nonumber\end{align}   \right.</script><p>æ ¹æ®æ–¹ç¨‹ç»„å¯ä»¥ç»˜åˆ¶ä¸‹å›¾ï¼š</p><p><img src="/2018/11/13/branch-and-bound/è§„åˆ’.png" alt=""></p><p>äºæ˜¯å¯ä»¥å¾—åˆ°å®æ•°ç©ºé—´ä¸Šçš„æœ€ä¼˜è§£ï¼š$x_1 = 2.25, x_2 = 3.75, z_0 = 41.25$ã€‚â€”â€”<strong>æ¾å¼›é—®é¢˜</strong></p><p>ç”±äºå­˜åœ¨æ•´æ•°é™å®šæ¡ä»¶ï¼š</p><ol><li>æœ€ä¼˜è§£$0 \leq z^{*} \leq 41$ï¼Œä¸”å¿…ä¸ºæ•´æ•°</li><li>x_2çš„æœ€ä¼˜è§£ä¸åœ¨3å’Œ4ä¹‹é—´ï¼Œå› ä¸ºé™å®šä¸ºæ•´æ•°</li></ol><p>ä¸€ã€åˆ†æ”¯</p><p>äºæ˜¯é—®é¢˜å¯ä»¥æ‹†åˆ†ä¸ºï¼š$max\ z = 5x_1 + 8x_2$</p><script type="math/tex; mode=display">p1\left\{\begin{align} &   x_1 + x_2 \leq 6 \nonumber\\ &   5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\& x_2\leq 3 \nonumber\end{align}   \right.\\</script><script type="math/tex; mode=display">p2\left\{\begin{align} &   x_1 + x_2 \leq 6 \nonumber\\ &   5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\& x_2\geq 4 \nonumber\end{align}   \right.\\</script><p>é—®é¢˜æ‹†åˆ†çš„å®è´¨æ˜¯å°†$x_2$åœ¨3å’Œ4ä¹‹é—´çš„å°æ•°éƒ¨åˆ†åˆ’æ‰äº†ï¼Œå°†å¯è¡ŒåŸŸæ‹†åˆ†æˆ$x_2 \leq 3$ å’Œ$x_3 \geq 4$ï¼Œä½†æ˜¯æ²¡æœ‰æ’é™¤ä»»ä½•æ•´æ•°å¯è¡Œè§£ã€‚â€”â€”<strong>åˆ†æ”¯</strong></p><p>äºŒã€å®šç•Œ</p><p>å­é—®é¢˜$p1$çš„æœ€ä¼˜è§£ä¸ºï¼š$x_1 = 3, x_2=3, z^{*}=39$</p><p>å­é—®é¢˜$p2$çš„æœ€ä¼˜è§£ä¸ºï¼š$x_1 = 1.8, x_2=4, z^{*}=41$</p><p>ä¹Ÿå°±æ˜¯è¯´ï¼Œå­é—®é¢˜$p1$çš„æ•´ä¸ªå‚æ•°ç©ºé—´ä¸Šï¼Œèƒ½å¤Ÿå–å¾—çš„æœ€ä¼˜è§£ä¸º39ï¼Œå­é—®é¢˜$p2$ä¸Šåˆ™ä¸º41ï¼Œæ˜¾ç„¶æœ€ä¼˜è§£åº”è¯¥ä½äºå­é—®é¢˜$p2$æ‰€åœ¨çš„å‚æ•°ç©ºé—´ä¸­ï¼Œä¸”$39\leq z^{<em>} \leq41$ã€‚â€”â€”<em>*å®šç•Œ</em></em></p><p>ä¸‰ã€è¿­ä»£</p><p>å¯¹$p2$å‚æ•°ç©ºé—´å†åˆ†æ”¯ï¼Œå‚æ•°$x_1$å¯ä»¥æ‹†åˆ†ä¸º$x_1 \leq 1$å’Œ$x_1 \geq 2$ï¼š</p><script type="math/tex; mode=display">p3\left\{\begin{align} &   x_1 + x_2 \leq 6 \nonumber\\ &   5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\ & x_1\leq 1 \nonumber\\& x_2\geq 4 \nonumber\end{align}   \right.\\</script><script type="math/tex; mode=display">p4\left\{\begin{align} &   x_1 + x_2 \leq 6 \nonumber\\ &   5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\ & x_1\geq 2 \nonumber \\& x_2\geq 4 \nonumber\end{align}   \right.\\</script><p>å››ã€æ€»ç»“</p><p>åˆ†æ”¯å®šç•Œç®—æ³•çš„æ€»ä½“æµç¨‹å¦‚ä¸‹ï¼š</p><ol><li>å…ˆæ±‚è§£ç›¸åº”çš„æ¾å¼›é—®é¢˜ï¼Œå¾—åˆ°æœ€ä¼˜è§£ï¼Œæ£€æŸ¥å…¶æ˜¯å¦ç¬¦åˆåŸé—®é¢˜çº¦æŸï¼Œè‹¥ç¬¦åˆåˆ™ä¸ºæœ€ä¼˜è§£ï¼Œå¦åˆ™è¿›è¡Œä¸‹ä¸€æ­¥ã€‚</li><li>å®šç•Œï¼Œå–å„åˆ†æ”¯ä¸­ç›®æ ‡å‡½æ•°æœ€å¤§çš„ä½œä¸ºä¸Šç•Œ$U_z$ï¼Œå–å…¶ä½™åˆ†æ”¯ä¸­ç›®æ ‡å‡½æ•°ä¸­æœ€å¤§çš„ä½œä¸ºä¸‹ç•Œ$L_z$ã€‚$L_z \leq z^{*} \leq U_z$ã€‚</li><li>åˆ†æ”¯ï¼Œå¦åˆ™é€‰æ‹©ä¸€ä¸ªä¸ç¬¦åˆåŸé—®é¢˜æ¡ä»¶çš„å˜é‡ï¼Œæ„å»ºå­é—®é¢˜ã€‚</li><li>å¯¹å„åˆ†æ”¯ï¼Œæœ‰åºåœ°ï¼Œè¿›è¡Œæ­¥éª¤1ã€‚</li></ol><p>åœ¨æ±‚è§£å¯¹åº”çš„æ¾å¼›é—®é¢˜æ—¶ï¼Œé€šå¸¸ä¼šæœ‰ä»¥ä¸‹å‡ ç§æƒ…å†µï¼š</p><ol><li>æ¾å¼›é—®é¢˜æ²¡æœ‰å¯è¡Œè§£ï¼Œé‚£ä¹ˆåŸé—®é¢˜ä¹Ÿæ²¡æœ‰å¯è¡Œè§£ã€‚</li><li>æ¾å¼›é—®é¢˜çš„æœ€ä¼˜è§£ä¹Ÿæ»¡è¶³åŸé—®é¢˜çº¦æŸï¼Œé‚£ä¹ˆè¯¥è§£ä¹Ÿæ˜¯åŸé—®é¢˜çš„æœ€ä¼˜è§£ï¼Œç®—æ³•ç»ˆæ­¢ã€‚</li><li>æ¾å¼›é—®é¢˜çš„æœ€ä¼˜è§£å°äºç°æœ‰ä¸‹ç•Œï¼Œé‚£ä¹ˆåº”è¯¥å¯¹è¯¥å­é—®é¢˜è¿›è¡Œå‰ªæã€‚</li></ol><p>äº”ã€DFS</p><p>å¯¹ä¸€é¢—æœç´¢æ ‘ï¼Œä¸ç”¨è®¡ç®—æ¯ä¸€å±‚å…¨éƒ¨èŠ‚ç‚¹çš„scoreï¼ˆBFSï¼‰ï¼Œæˆ‘ä»¬ä¼šç»´æŠ¤ä¸€ä¸ªä¼˜å…ˆé˜Ÿåˆ—ï¼Œå…¶ä¸­æŒ‰ç…§scoreçš„å¤§å°å­˜æ”¾èŠ‚ç‚¹ï¼Œç„¶åé€‰æ‹©scoreæœ€å¤§çš„èŠ‚ç‚¹ï¼ˆthe most promising childï¼‰è¿›è¡Œåˆ†æ”¯ã€‚</p>]]></content>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>trifles with arduino</title>
      <link href="/2018/10/30/trifles-with-arduino/"/>
      <url>/2018/10/30/trifles-with-arduino/</url>
      <content type="html"><![CDATA[<p>ç³»ç»Ÿæ€»ä½“çš„é€šä¿¡æ¶æ„å¦‚ä¸‹ï¼š</p><p><img src="/2018/10/30/trifles-with-arduino/é€šä¿¡.png" alt=""></p><p>åº•ç›˜é©±åŠ¨æ¿Arduinoè´Ÿè´£æ¥æ”¶ä¸Šå±‚çš„è¿åŠ¨æ§åˆ¶æŒ‡ä»¤ï¼Œå¹¶é©±åŠ¨ç”µæœºï¼Œä¸¤å—æ¿å­é€šè¿‡ä¸²å£è¿›è¡Œé€šä¿¡ã€‚</p><p>ROSæä¾›äº†ä¸€ä¸ª<a href="https://github.com/hbrobotics/ros_arduino_bridge" target="_blank" rel="noopener">ros_arduino_bridge</a>åŠŸèƒ½åŒ…é›†ï¼Œå®ƒåŒ…æ‹¬äº†Arduinoåº“ï¼ˆROSArduinoBridgeï¼‰ä»¥åŠä¸€ç³»åˆ—ç”¨æ¥æ§åˆ¶Arduino-based robotçš„ROSåŠŸèƒ½åŒ…ï¼Œ<strong>è¿™ä¸ªåŠŸèƒ½åŒ…å¯ä»¥å®ç°è¯»å–Twistæ§åˆ¶ä¿¡æ¯ï¼Œä»¥åŠå‘å¸ƒé‡Œç¨‹è®¡ä¿¡æ¯ç­‰ä»»åŠ¡</strong>ï¼Œå°è£…äº†Raspberry Piå’ŒArduinoä¹‹é—´çš„åº•å±‚é€šä¿¡ã€‚</p><p>Arduinoåº“ï¼ˆROSArduinoBridgeï¼‰ä½äºros_arduino_firmware/src/libraries/è·¯å¾„ä¸‹ï¼Œé‡Œé¢æ˜¯ä¸€äº›arduinoè„šæœ¬å’Œå¤´æ–‡ä»¶ï¼Œå°†è¿™ä¸ªæ–‡ä»¶å¤¹å¤åˆ¶åˆ°æˆ‘ä»¬Arduino IDEçš„SKETCHBOOK_PATHä¸‹ï¼Œç„¶ååœ¨Arduino IDEä¸­å°±å¯ä»¥ç›´æ¥æ‰“å¼€è¿™ä¸ªsketché¡¹ç›®ã€‚</p><p>ROSArduinoBridgeæ–‡ä»¶ä¸‹æ˜¯ä¸€äº›é…ç½®é€‰é¡¹ï¼Œå¦å¤–commands.hæ–‡ä»¶ä¸­ç»™å‡ºäº†ä¸€äº›å¯ç”¨çš„ä¸²å£æ§åˆ¶æŒ‡ä»¤ï¼Œå¦‚ç”µæœºæ§åˆ¶æŒ‡ä»¤ï¼š</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">m <span class="number">20</span> <span class="number">20</span>   <span class="comment">// move the robot forward at 20 encoder ticks per second</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>skid-steer drive</title>
      <link href="/2018/10/29/skid-steer-drive/"/>
      <url>/2018/10/29/skid-steer-drive/</url>
      <content type="html"><![CDATA[<p>å®éªŒä¸­ä½¿ç”¨äº†ä¸¤ç§ç±»å‹çš„åº•ç›˜ï¼ŒåŸºäºå·®é€Ÿé©±åŠ¨çš„2WDåº•ç›˜å’ŒåŸºäºæ»‘åŠ¨è½¬å‘çš„4WDåº•ç›˜ã€‚ä¸¤ç§é©±åŠ¨æ–¹å¼åŸç†ç›¸ä¼¼ï¼Œä¹Ÿæœ‰å…¶æ˜¾è‘—çš„åŒºåˆ«ã€‚</p><p><strong>ç›¸åŒç‚¹ï¼š</strong></p><p>ä¸¤ç§åº•ç›˜éƒ½æ²¡æœ‰æ˜¾ç¤ºçš„è½¬åŠ¨æœºåˆ¶ï¼Œé‡‡ç”¨å·®é€Ÿé©±åŠ¨çš„æ–¹å¼é€šè¿‡ä»¥ä¸åŒçš„æ–¹å‘æˆ–é€Ÿåº¦é©±åŠ¨ä¸¤è¾¹è½®å­æ¥å®ç°æ–¹å‘æ§åˆ¶ã€‚</p><p>å·®é€Ÿé©±åŠ¨çš„è¿åŠ¨å½¢å¼é€šå¸¸æœ‰ä¸€ä¸‹å‡ ç§ç±»å‹ï¼š</p><ul><li>ç¬¬ä¸€ç§æ˜¯åŸåœ°æ—‹è½¬ï¼Œå·¦å³è½®çš„é€Ÿåº¦å¤§å°ç›¸ç­‰ï¼Œæ–¹å‘ç›¸åï¼Œè¿™æ ·ç›¸å½“äºç»•ç€åº•ç›˜çš„å½¢å¿ƒåŸåœ°æ‰“è½¬ã€‚</li><li>ç¬¬äºŒç§æ˜¯æ²¿ç€æŸä¸ªæ–¹å‘ç›´çº¿è¡Œèµ°ï¼Œæ­¤æ—¶å·¦å³è½®é€Ÿåº¦ç›¸åŒã€‚</li><li>ç¬¬ä¸‰ç§æ˜¯æ²¿ç€æŸæ¡æ›²çº¿å‰è¡Œæˆ–åé€€ï¼Œæ­¤æ—¶å·¦å³è½®é€Ÿåº¦æ–¹å‘ç›¸åŒï¼Œå¤§å°ä¸åŒã€‚</li><li>ç¬¬å››ç§æ˜¯æ—‹è½¬è½¬å¼¯ï¼Œæ­¤æ—¶å·¦å³è½®é€Ÿåº¦æ–¹å‘ç›¸åã€‚</li></ul><p>ä¸¤ç§æœºæ„å…±åŒçš„ä¼˜åŠ¿æ˜¯ï¼šæ²¡æœ‰æ˜¾ç¤ºçš„è½¬å‘æœºæ„ï¼Œæå¤§åœ°ç®€åŒ–äº†è¿åŠ¨å­¦æ¨¡å‹ã€‚</p><p>è€Œä¸¤ç§æœºæ„å…±åŒçš„ç¼ºç‚¹æ˜¯ï¼šç”±äºä¸¤ä¾§çš„è½®å­æ˜¯ç”±ç‹¬ç«‹ç”µæœºåˆ†åˆ«é©±åŠ¨çš„ï¼Œç›´çº¿è¿åŠ¨è¦æ±‚ä¸¤ä¾§çš„è½®å­ä»¥ç›¸åŒé€Ÿåº¦è½¬åŠ¨ï¼Œè¿™å°†å¾ˆéš¾å®Œæˆã€‚</p><p><strong>ä¸åŒç‚¹ï¼š</strong></p><p>å·®é€Ÿé©±åŠ¨åº•ç›˜é€šå¸¸æ˜¯ç”±ä¸€ä¸ªä¸¤è½®ç³»ç»Ÿï¼Œæ¯ä¸ªè½®å­éƒ½å¸¦æœ‰ç‹¬ç«‹çš„æ‰§è¡Œæœºæ„ï¼ˆç›´æµç”µæœºï¼‰ï¼Œä»¥åŠä¸€ä¸ªæ— é©±åŠ¨è½®ï¼ˆå¯ä»¥æ˜¯è„šè½®æˆ–è€…ä¸‡å‘æ»šç ï¼‰ç»„æˆï¼Œ<strong>æœºå™¨äººçš„è¿åŠ¨çŸ¢é‡æ˜¯æ¯ä¸ªç‹¬ç«‹è½¦è½®è¿åŠ¨çš„æ€»å’Œ</strong>ã€‚</p><p><img src="/2018/10/29/skid-steer-drive/Diffdrv.jpg" alt="Diffdrv"></p><p>æ»‘åŠ¨è½¬å‘åº•ç›˜é€šå¸¸è¢«ç”¨åœ¨å±¥å¸¦è½¦ä¸Šï¼Œæ¯”å¦‚å¦å…‹å’Œæ¨åœŸæœºï¼Œä¹Ÿè¢«ç”¨äºæŸäº›å››è½®å…­è½®æœºæ„ä¸Šï¼Œç›¸æ¯”è¾ƒäºä¸¤è½®å·®é€Ÿåº•ç›˜ï¼Œæ»‘åŠ¨è½¬å‘çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼š</p><ul><li>ä¼˜åŠ¿ï¼šæ»‘åŠ¨è½¬å‘ä½¿ç”¨äº†ä¸¤ä¸ªé¢å¤–çš„é©±åŠ¨è½®ä»£æ›¿äº†å·®é€Ÿé©±åŠ¨çš„è„šè½®ï¼Œå¢å¤§äº†ç‰µå¼•åŠ›ã€‚</li><li>åŠ£åŠ¿ï¼šå¼•å…¥äº†æ»‘åŠ¨ï¼Œåœ¨å¯¹é‡Œç¨‹è®¡è¦æ±‚é«˜çš„åœºæ™¯ä¸­ï¼Œæ»‘åŠ¨æ˜¯ä¸€ä¸ªè‡´å‘½çš„ç¼ºé™·ï¼Œå› ä¸ºè¿™ä¼šå¯¹ç¼–ç å™¨é€ æˆè´Ÿé¢å½±å“ï¼Œæ»‘åŠ¨çš„è½®å­ä¸ä¼šè·Ÿè¸ªæœºå™¨äººçš„ç¡®åˆ‡è¿åŠ¨ã€‚</li></ul><p><img src="/2018/10/29/skid-steer-drive/4WD.png" alt="4WD"></p><p><strong>è¿åŠ¨å­¦åˆ†æï¼š</strong></p><p>å¯¹äºå·®é€Ÿé©±åŠ¨æœºæ„ï¼Œç§»åŠ¨æœºå™¨äººèˆªå‘è§’å˜åŒ–äº†å¤šå°‘è§’åº¦ï¼Œå®ƒå°±ç»•å…¶è¿åŠ¨è½¨è¿¹çš„åœ†å¿ƒæ—‹è½¬äº†å¤šå°‘è§’åº¦ã€‚è¿™å¥è¯å¾ˆå¥½éªŒè¯ï¼Œæˆ‘ä»¬è®©æœºå™¨äººåšåœ†å‘¨è¿åŠ¨ï¼Œä»èµ·ç‚¹å‡ºå‘ç»•åœ†å¿ƒä¸€åœˆå›åˆ°èµ·ç‚¹å¤„ï¼Œåœ¨è¿™è¿‡ç¨‹ä¸­æœºå™¨äººç´¯è®¡çš„èˆªå‘è§’ä¸º360åº¦ï¼ŒåŒæ—¶å®ƒä¹Ÿç¡®å®ç»•è½¨è¿¹åœ†å¿ƒè¿åŠ¨äº†360åº¦ã€‚</p><p>æœºå™¨äººçš„é€Ÿåº¦æ˜¯æŒ‡ä¸¤ä¸ªç›¸é‚»çš„æ§åˆ¶æ—¶åˆ»ä¹‹é—´çš„é€Ÿåº¦ï¼Œå› æ­¤å°è½¦çš„è¡Œé©¶è½¨è¿¹å¯ä»¥åˆ†è§£ä¸ºè¿ç»­çš„åœ†å¼§ç‰‡æ®µï¼Œå¯¹äºæ¯ä¸€æ®µåœ†å¼§ï¼Œæ ¹æ®<strong>é˜¿å…‹æ›¼è½¬å‘å‡ ä½•åŸç†</strong>ï¼Œåœ¨å°è½¦è½¬å‘æ—¶ï¼Œä¸ºä¿è¯è¡Œé©¶ç¨³å®šæ€§ï¼Œä¸¤ä¾§è½®èƒéƒ½è¿‘ä¼¼å›´ç»•ä¸€ä¸ªä¸­å¿ƒç‚¹æ—‹è½¬ã€‚å³æ•´ä¸ªå°è½¦åº•ç›˜éƒ½å›´ç»•ä¸€ä¸ªä¸­å¿ƒç‚¹æ—‹è½¬ï¼Œå·²çŸ¥å°è½¦ä¸­å¿ƒçš„çº¿é€Ÿåº¦ï¼ˆä¸Šå±‚ç®—æ³•ç»™å®šï¼‰ï¼Œæ­¤æ—¶å°è½¦åº•ç›˜çš„è¿åŠ¨å­¦æ¨¡å‹å¦‚ä¸‹å›¾ï¼š</p><p><img src="/2018/10/29/skid-steer-drive/åº•ç›˜.png" alt=""></p><p>å‚æ•°è¯´æ˜ï¼š</p><blockquote><p>$\alpha_1$æ˜¯å°è½¦å‰å·¦è½®å’Œåå·¦è½®çš„è½¬è§’ã€‚</p><p>$\alpha_2$æ˜¯å°è½¦å‰å³è½®å’Œåå³è½®çš„è½¬è§’ã€‚</p><p>$2L$æ˜¯å·¦å³è½®è·ç¦»ã€‚</p><p>$2K$æ˜¯å‰åè½®è·ç¦»ã€‚</p><p>$w$æ˜¯å°è½¦è½¬è½´çš„è§’é€Ÿåº¦ã€‚</p><p>$v$æ˜¯å°è½¦å‡ ä½•ä¸­å¿ƒçš„çº¿é€Ÿåº¦ã€‚</p><p>$v1, v2, v3, v4$æ˜¯å››ä¸ªè½¦è½®çš„é€Ÿåº¦ã€‚</p><p>$i$æ˜¯ç”µæœºçš„å‡é€Ÿæ¯”ã€‚</p><p>$r$æ˜¯è½¦è½®åŠå¾„ã€‚</p></blockquote><p>é¦–å…ˆå¯ä»¥å¾—åˆ°å„è½¦è½®é€Ÿåº¦å’Œè§’é€Ÿåº¦çš„å…³ç³»ï¼š</p><script type="math/tex; mode=display">V_1 = w * R_1 = w * \frac{K}{sin\alpha_1}\\V_2 = w * R_2 = w * \frac{K}{sin\alpha_2}\\V_3 = V_1 = w * \frac{K}{sin\alpha_1}\\V_4 = V_2 = w * \frac{K}{sin\alpha_2}\\</script><p>å…¶ä¸­è½¦è½®æ²¿ç€è½¬åŠ¨æ–¹å‘ï¼ˆ$y$æ–¹å‘ï¼‰çš„é€Ÿåº¦ç”±ç”µæœºæä¾›ï¼Œåˆ‡å‘é€Ÿåº¦ç”±åœ°é¢æ‘©æ“¦æä¾›ï¼Œè½¦è½®æ²¿ç€$y$æ–¹å‘çš„é€Ÿåº¦ä¸ºï¼š</p><script type="math/tex; mode=display">R =\frac{v}{w}\\V_{1y} = V_1 * cos\alpha_1 = w * \frac{K}{tan \alpha_1} = w(R-L)\\V_{2y} = V_2 * cos\alpha_2 = w * \frac{K}{tan \alpha_2} = w(R+L)\\V_{3y} = V_{1y} =  w(R-L)\\V_{4y} = V_{2y} = w(R+L)\\</script><p>é‚£ä¹ˆç”µæœºçš„è§’é€Ÿåº¦ä¸ºï¼š</p><script type="math/tex; mode=display">w_n= \frac{V_{ny}*i}{r}, n = 1,2,3,4\\</script><p>ç›¸åº”ç”µæœºçš„è½¬é€Ÿï¼ˆby rpmï¼‰ä¸ºï¼š</p><script type="math/tex; mode=display">n  = \frac{w_n*60}{2\pi}</script><p>æ•´ç†æˆçŸ©é˜µè¡¨è¾¾å¼ä¸ºï¼š</p><script type="math/tex; mode=display">\begin{bmatrix}w_1\\w_2\\w_3\\w_4\end{bmatrix}=\begin{bmatrix}1 & - L\\1 & L\\1 & - L\\1 & L\end{bmatrix}\begin{bmatrix}v\\w\end{bmatrix}</script><p>è¯¥è¡¨è¾¾å¼åæ˜ äº†æœºå™¨äººå…³é”®ç‚¹é€Ÿåº¦ä¸ä¸»åŠ¨è½®è½¬é€Ÿä¹‹é—´çš„å…³ç³»ã€‚ç»™å®šå°è½¦åº•ç›˜ç”µæœºè½¬é€Ÿå°±å¯ä»¥æ±‚å‡ºæœºå™¨äººå…³é”®ç‚¹çš„é€Ÿåº¦ï¼Œå¹¶ç”±æ­¤å¾—åˆ°æœºå™¨äººä¸Šä»»æ„ä¸€ç‚¹çš„é€Ÿåº¦ï¼ˆå¦‚æ¿€å…‰é›·è¾¾çš„å®‰è£…ä½ç½®çš„é€Ÿåº¦ï¼‰ï¼Œä¸Šå±‚ç®—æ³•ç»™å‡ºçš„å…³é”®ç‚¹é€Ÿåº¦æ§åˆ¶ä¿¡å·ä¹Ÿå¯ä»¥ç”±æ­¤è½¬åŒ–æˆç”µæœºçš„æ§åˆ¶é‡ã€‚</p><p><strong>é‡Œç¨‹è®¡æ¨¡å‹ ï¼ æœºå™¨äººå®šä½æ–¹æ³•</strong></p><p><strong>åæ ‡å˜æ¢æ¨¡å‹ï¼š</strong></p><p><img src="/2018/10/29/skid-steer-drive/ç¼–ç å™¨.png" alt="ç¼–ç å™¨"></p><p>åœ¨ä¸€ä¸ªè¾ƒçŸ­çš„æ—¶é—´é—´éš”$\Delta t$å†…ï¼Œå‡å®šæœºå™¨äººå·¦å³è½®çš„ç§»åŠ¨è·ç¦»åˆ†åˆ«æ˜¯$\Delta l$å’Œ$\Delta r$ï¼Œé‚£ä¹ˆåœ¨<strong>æœºå™¨äººåæ ‡ç³»ä¸‹</strong>ï¼šæœºå™¨äººä¸­å¿ƒæ²¿ç€æœºå™¨äººåæ ‡ç³»çš„$x$è½´æ–¹å‘å‰è¿›çš„è·ç¦»ä¸º$\Delta u = (\Delta l + \Delta r)/2$ï¼Œ$y$è½´æ–¹å‘å‰è¿›çš„è·ç¦»ä¸º$\Delta v = 0$ï¼Œè½¬è¿‡çš„è§’åº¦ä¸º$\Delta \varphi = (\Delta l - \Delta r)/b$ã€‚æœºå™¨äººåæ ‡ç³»åˆ°ä¸–ç•Œåæ ‡ç³»çš„æ—‹è½¬å˜æ¢çŸ©é˜µä¸º$R(\phi)$ã€‚</p><p>é‚£ä¹ˆè½¬æ¢åˆ°<strong>ä¸–ç•Œåæ ‡ç³»ä¸‹</strong>æœºå™¨äººçš„è¿åŠ¨å¢é‡ä¸ºï¼š</p><script type="math/tex; mode=display">\begin{bmatrix}\Delta x\\\Delta y\\\Delta \phi\end{bmatrix} = \begin{bmatrix}R(\phi) & 0\\0 & 1\end{bmatrix}\begin{bmatrix}\Delta u\\\Delta v\\\Delta \varphi\end{bmatrix} = \begin{bmatrix}cos\phi & sin\phi & 0\\-sin\phi & cos\phi & 0\\0 & 0 & 1\end{bmatrix}\begin{bmatrix}(\Delta l + \Delta r)/2\\0\\(\Delta l - \Delta r)/b\end{bmatrix}</script><p>ä¸–ç•Œåæ ‡ç³»ä¸‹æœºå™¨äººä½å§¿æ›´æ–°ä¸ºï¼š</p><script type="math/tex; mode=display">\begin{bmatrix}x_t\\y_t\\\phi_t\end{bmatrix} =\begin{bmatrix}cos\phi_{t-1}(\Delta l + \Delta r)/2\\sin\phi_{t-1}(\Delta l + \Delta r)/2\\(\Delta l - \Delta r)/b\end{bmatrix} +\begin{bmatrix}x_{t-1}\\y_{t-1}\\\phi_{t-1}\end{bmatrix}</script><p>beside fromæµ‹é‡è¯¯å·®ï¼Œåˆ©ç”¨åæ ‡å˜æ¢æ¨¡å‹å»æ¨ç®—é‡Œç¨‹è®¡ä¿¡æ¯æ˜¯å¼•å…¥äº†<strong>æ¨¡å‹è¯¯å·®</strong>çš„â€”â€”åœ¨æ—¶é—´é—´éš”$\Delta t$å†…ï¼Œä¸ºäº†ç®€åŒ–è®¡ç®—ï¼Œæœºå™¨äººåæ ‡ç³»ç›¸å¯¹ä¸–ç•Œåæ ‡ç³»çš„æ—‹è½¬å˜æ¢çŸ©é˜µè¢«å‡å®šä¸ºèµ·å§‹å€¼$R(\phi)$ã€‚åœ¨è½¬å‘è¿åŠ¨æ¯”è¾ƒå¤šçš„æƒ…å†µä¸‹ï¼Œé‡Œç¨‹è®¡ä¿¡æ¯ä¼šè¿…é€Ÿæ¶åŒ–ã€‚</p><p><strong>åœ†å¼§æ¨¡å‹ï¼š</strong></p><p>å°†æå°æ—¶é—´é—´éš”å†…å°è½¦è¿åŠ¨çš„è½¨è¿¹çœ‹ä½œæ˜¯ä¸€æ®µåœ†å¼§ï¼Œé‚£ä¹ˆå°±å¯ä»¥ç¡®å®šè¯¥æ—¶åˆ»çš„è½¬åŠ¨ä¸­å¿ƒ$C_{t-1}$ï¼ŒåŠå†…ä¾§è½®çš„è½¬åŠ¨åŠå¾„ä¸º$R_{t-1}$ï¼Œæ ¹æ®å‡ ä½•å…³ç³»ï¼š</p><p><img src="/2018/10/29/skid-steer-drive/åœ†å¼§æ¨¡å‹.png" alt="åœ†å¼§æ¨¡å‹"></p><script type="math/tex; mode=display">\left\{ \begin{align}&\Delta l = (b + R)\Delta \varphi\\&\Delta r = R  \Delta \varphi\end{align}\right.</script><p>è§£å¾—ï¼š</p><script type="math/tex; mode=display">\left\{ \begin{align}&R = \frac{b\Delta r}{\Delta l - \Delta r}\\&\Delta \varphi = \frac{\Delta l - \Delta r }{b}\end{align}\right.</script><p>ç”±ä¸‰è§’ç›¸ä¼¼å¾—ï¼š</p><script type="math/tex; mode=display">\frac{Rsin(\Delta \varphi/2)}{D/2} = \frac{R}{R + b/2}</script><p>è§£å¾—å¼¦$D$çš„é•¿åº¦ä¸ºï¼š</p><script type="math/tex; mode=display">D = [b(\Delta l + \Delta r)/(\Delta l-\Delta r)]sin[(\Delta l - \Delta r)/2b]</script><p>å¼¦$D$ä¸ä¸–ç•Œåæ ‡ç³»$x$è½´æ­£å‘çš„å¤¹è§’ä¸º$\theta = \phi - \Delta \varphi/2$ï¼Œé‚£ä¹ˆæœºå™¨äººåœ¨ä¸–ç•Œåæ ‡ç³»ä¸‹çš„ä½å§¿å¢é‡ä¸ºï¼š</p><script type="math/tex; mode=display">\left\{\begin{align}&\Delta x_{t-1} = D_{t-1} cos\theta_{t-1}\\&\Delta y_{t-1} = D_{t-1} sin\theta_{t-1}\\&\Delta \varphi = (\Delta l - \Delta r )/b\end{align}\right.</script><p>ä½¿ç”¨åœ†å¼§æ¨¡å‹å¯¹é‡Œç¨‹è®¡å¢é‡è¿›è¡Œæ¨ç®—ï¼Œå®Œå…¨ä¾ç…§å‡ ä½•å…³ç³»æ¥è®¡ç®—ï¼Œè®¡ç®—è¿‡ç¨‹ä¸­æ²¡æœ‰è¿‘ä¼¼ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ§åˆ¶è¯¯å·®ç´¯ç§¯ã€‚</p><p><strong>æ¦‚ç‡æ¨¡å‹ï¼š</strong></p><p>ä¸æ˜¯å•çº¯çš„åŸºäºé‡Œç¨‹è®¡çš„ä¼°è®¡ï¼Œè€Œæ˜¯ç»“åˆå…¶ä»–ä¼ æ„Ÿå™¨çš„æµ‹é‡å€¼å¯¹é‡Œç¨‹è®¡è¿›è¡ŒçŸ«æ­£ï¼Œè¯¦è§<a href="https://amberzzzz.github.io/2018/11/16/amcl/">æ»¤æ³¢ç®—æ³•</a>ã€‚</p><p><strong>scan matchæ¨¡å‹ï¼š</strong></p><p>åŒæ ·ä¹Ÿä¸æ˜¯å•çº¯çš„åŸºäºé‡Œç¨‹è®¡çš„ä¼°è®¡ï¼Œè¯¦è§<a href="https://amberzzzz.github.io/2018/06/05/karto-key-concepts/">karto scanMatch</a>ã€‚ä¸æ»¤æ³¢çš„åŒºåˆ«åœ¨äºï¼Œè¿”å›çš„ä¸æ˜¯æ¦‚ç‡åˆ†å¸ƒï¼Œè€Œæ˜¯ä¸€ä¸ªä»£è¡¨æœ€ä½³ä¼°è®¡çš„å€¼ã€‚</p>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>scan matcher</title>
      <link href="/2018/10/26/scan-matcher/"/>
      <url>/2018/10/26/scan-matcher/</url>
      <content type="html"><![CDATA[<p>ç›®å‰å¼€æºç®—æ³•ä¸­é‡‡å–çš„scanMatchingæ–¹æ³•ä¸»è¦æ˜¯ä»¥ä¸‹å››ç§ï¼š</p><ol><li>Gmappingï¼šICPï¼ˆsimple Gradient Descentï¼‰</li><li>Hectorï¼šGuass-Newtonï¼ˆmulti-resolution mapï¼‰</li><li>kartoï¼šReal-time CSMï¼ˆmulti-resolution + ä¸‰ç»´çª—å£éå†å¯»ä¼˜ï¼‰</li><li>cartographerï¼šFast CSMï¼ˆmulti-resolution + branch and boundï¼‰</li></ol><p>scanMatcherä¸»è¦æ¶‰åŠä¸¤ä¸ªè¯„ä»·å‡½æ•°ï¼Œä¸€ä¸ªscoreç”¨äºä¼˜åŒ–è°ƒæ•´ç²’å­poseä½œä¸ºå‚è€ƒï¼Œä¸€ä¸ªlikelihoodAndScoreç”¨äºæ›´æ–°ç²’å­æƒé‡ï¼š</p><script type="math/tex; mode=display">s(x, z, m) = \sum_i s(x, z^i, m)\\s(x, z^i, m) = e^{d^2/ \sigma}</script><p>ç²’å­æƒé‡æ ¹æ®åœ°å›¾çš„åŒ¹é…åº¦æ›´æ–°ï¼š</p><script type="math/tex; mode=display">laser\ frame \to map\ frame: \hat{z}^i = x \oplus z^i\\map\ cell: (x,\ y)^T\\d^2 = (\hat{z}^i - (x,y)^T)^T (\hat{z}^i - (x,y)^T)</script>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>navigation stack</title>
      <link href="/2018/10/23/navigation-stack/"/>
      <url>/2018/10/23/navigation-stack/</url>
      <content type="html"><![CDATA[<h4 id="part0"><a href="#part0" class="headerlink" title="part0"></a>part0</h4><p>ç°å®ä¸­æƒ³è¦ç§»åŠ¨å¹³å°ç§»åŠ¨åˆ°æŒ‡å®šåœ°ç‚¹ï¼Œæœºå™¨äººè¿åŠ¨æ§åˆ¶ç³»ç»Ÿæ¶æ„åŒ…æ‹¬äº†å¦‚ä¸‹å‡ ä¸ªå±‚æ¬¡ï¼š</p><p><img src="/2018/10/23/navigation-stack/navi.png" alt=""></p><p>æœ€åº•å±‚æ˜¯æœºå™¨äººçš„åº•ç›˜æ§åˆ¶éƒ¨åˆ†ï¼Œé©±åŠ¨å™¨æ¥æ”¶çš„æ˜¯æœºå™¨äººçš„æœŸæœ›é€Ÿåº¦ï¼ˆTwistï¼‰ï¼Œå°†é€Ÿåº¦è§£ç®—ä¸ºå·¦å³è½®çš„æœŸæœ›é€Ÿåº¦ï¼Œå¹¶æ ¹æ®æœŸæœ›é€Ÿåº¦å¯¹å·¦å³è½®åˆ†åˆ«è¿›è¡ŒPIDé©±æ§é€Ÿï¼Œè¾“å‡ºç”µæœºçš„è½¬é€Ÿã€‚</p><blockquote><p>è¿™éƒ¨åˆ†ROSç¤¾åŒºå·²ç»æœ‰é’ˆå¯¹Arduinoå°è£…å¥½çš„Packageâ€”â€”rosserial_arduinoã€‚</p></blockquote><p>ä¸­é—´å±‚æ˜¯é€šä¿¡å±‚ï¼Œç”µè„‘ç«¯å‘å¸ƒé€Ÿåº¦æŒ‡ä»¤ç»™å¹³å°ï¼ŒåŒæ—¶æ¥æ”¶å¹³å°å‘å¸ƒçš„å½“å‰é€Ÿåº¦ï¼Œç„¶åå‘å¸ƒ/odom topicï¼Œè®©å…¶ä»–èŠ‚ç‚¹è®¢é˜…ã€‚</p><p>æœ€ä¸Šå±‚æ˜¯å†³ç­–å±‚ï¼Œä¹Ÿå°±æ˜¯å¯¼èˆªè§„åˆ’å±‚ï¼Œgoalã€localizationï¼ˆmatching&amp;é‡Œç¨‹è®¡ï¼‰ã€path plannerä»¥åŠæœ€ç»ˆè¾“å‡ºé€Ÿåº¦æŒ‡ä»¤ï¼Œè¿™ä¸€éƒ¨åˆ†éƒ½åœ¨navigation stacké‡Œé¢ã€‚</p><h4 id="part1-packages"><a href="#part1-packages" class="headerlink" title="part1 packages"></a>part1 packages</h4><p>navigation stackæ˜¯ROSæä¾›çš„å¯¼èˆªæ–¹æ¡ˆï¼Œå†…éƒ¨é›†æˆäº†å¾ˆå¤šä¸ªpackageï¼Œæ¨¡å—ä¹‹é—´å®Œå…¨è§£è€¦ï¼Œå¯ä»¥ä¸ªæ€§åŒ–é€‰æ‹©æä¾›çš„æ–¹æ³•ã€‚</p><p><img src="/2018/10/23/navigation-stack/navigation%20stack.png" alt=""></p><p>å®˜ç½‘ä¸Šç»™å‡ºäº†å¯¼èˆªæ ˆå®è§‚çš„ç»“æ„æè¿°ï¼š</p><p><img src="/2018/10/23/navigation-stack/module.png" alt=""></p><p>move_baseä¸­ä¸»è¦åŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼Œglobal_planã€local_planä»¥åŠrecovery behaviorã€‚å¯¹åº”çš„æ’ä»¶æœ‰ï¼š</p><ul><li>global_planï¼šglobal_plannerï¼ˆå®ç°äº†dijkstraå’ŒA*ç®—æ³•ï¼‰ï¼Œcarrot_plannerï¼Œnavfn</li><li>local_planï¼šbase_local_plannerï¼ˆå®ç°äº†Trajectory Rolloutå’ŒDWAç®—æ³•ï¼‰ï¼Œdwa_local_planner</li><li>recoveryï¼šclear_costmap_recoveryï¼Œmove_slow_and_clearï¼Œrotate_recovery</li></ul><p>nav_coreæ˜¯ä¸€ä¸ªæ¥å£æ’ä»¶ï¼ŒåŒ…å«äº†ä»¥ä¸Šæ’ä»¶åŸºç±»çš„å¤´æ–‡ä»¶ï¼Œmove_baseä¸­çš„æ–¹æ³•éƒ½æ˜¯åœ¨å…¶è§„åˆ™ä¸Šæ‰©å±•çš„ã€‚</p><p>ä¸¤ä¸ªç°è‰²çš„æ’ä»¶map_serverå’Œamclè¡¨ç¤ºå¯é€‰å¯ä¸é€‰ï¼š</p><ul><li>å¯ä»¥ä½¿ç”¨meta packageæä¾›çš„map_serverèŠ‚ç‚¹æ¥è¿›è¡Œ<strong>ä»£ä»·åœ°å›¾</strong>ç®¡ç†ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–èŠ‚ç‚¹ï¼ˆä¾‹å¦‚ç›´æ¥ä½¿ç”¨gmappingçš„è¾“å‡ºï¼‰ã€‚</li><li>å¯ä»¥ä½¿ç”¨meta packageæä¾›çš„amclèŠ‚ç‚¹æ¥è¿›è¡Œè‡ªå®šä½ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–ç®—æ³•åŒ…ï¼ˆä¾‹å¦‚ROSé‡Œé¢è¿˜æœ‰ä¸€ä¸ªrobot_pose_ekfèŠ‚ç‚¹ï¼‰ã€‚</li></ul><p>costmap_2då°†ä¸åŒä¼ æ„Ÿå™¨çš„è¾“å…¥å¤„ç†æˆç»Ÿä¸€çš„æ …æ ¼åœ°å›¾æ ¼å¼ã€‚ä»¥å±‚çš„æ¦‚å¿µæ¥ç»„ç»‡å›¾å±‚ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®éœ€è¦è‡ªå·±é…ç½®ï¼ˆé€šè¿‡Social Costmap Layerã€Range Sensor Layerç­‰å¼€æºæ’ä»¶ï¼‰ï¼Œé»˜è®¤çš„å±‚æœ‰ï¼š</p><ul><li>static_layerï¼šé™æ€åœ°å›¾å±‚ï¼Œï¼ˆé€šè¿‡è®¢é˜…map_serverçš„/mapä¸»é¢˜ï¼‰æ¥ç”Ÿæˆã€‚</li><li>obstacle_layerï¼šéšœç¢åœ°å›¾å±‚ï¼Œæ ¹æ®åŠ¨æ€çš„ä¼ æ„Ÿå™¨ä¿¡æ¯æ¥ç”Ÿæˆã€‚</li><li>inflation_layerï¼šè†¨èƒ€å±‚ï¼Œå°†å‰ä¸¤ä¸ªå›¾å±‚çš„ä¿¡æ¯ç»¼åˆè¿›è¡Œç¼“å†²åŒºæ‰©å±•ã€‚</li></ul><p>voxel_gridæ˜¯ä¸‰ç»´ä»£ä»·åœ°å›¾ã€‚</p><p>fake_localizationç”¨æ¥åšå®šä½ä»¿çœŸï¼Œå†…å«/base_pose_ground_truthè¯é¢˜ã€‚</p><h4 id="part2-params"><a href="#part2-params" class="headerlink" title="part2 params"></a>part2 params</h4><ol><li>move_base_params.yamlï¼š<ul><li>planner_frequencyï¼š<strong>å…¨å±€è§„åˆ’</strong>çš„æ‰§è¡Œé¢‘ç‡ï¼Œå¦‚æœè®¾ç½®ä¸º0.0åˆ™å…¨å±€è§„åˆ’å™¨ä»…åœ¨æ¥å—åˆ°æ–°ç›®æ ‡ç‚¹æˆ–è€…å±€éƒ¨è§„åˆ’å™¨æŠ¥å‘Šè·¯å¾„å µå¡æ—¶æ‰ä¼šé‡æ–°æ‰§è¡Œã€‚</li></ul></li><li>global_planner_params.yamlï¼š<ul><li>default_toleranceï¼šå½“è®¾ç½®çš„ç›®çš„åœ°è¢«å æ®æ—¶ï¼Œä»¥è¯¥å‚æ•°ä¸ºåŠå¾„çš„èŒƒå›´å†…é€‰å–æœ€è¿‘çš„ç‚¹ä½œä¸ºæ–°ç›®æ ‡ç‚¹ã€‚</li></ul></li><li>dwa_local_planner_params.yamlï¼š<ul><li>latch_xy_goal_toleranceï¼šå¦‚æœè®¾ç½®ä¸ºtrueï¼Œè¾¾åˆ°xy_goal_toleranceä»¥å†…æœºå™¨äººå°±ä¼šåŸåœ°æ—‹è½¬ï¼Œå³ä½¿ä¼šè½¬å‡ºå®¹é”™åœˆå¤–ã€‚</li><li>sim_granularityï¼šé—´éš”å°ºå¯¸ï¼Œè½¨è¿¹ä¸Šé‡‡æ ·ç‚¹æ­¥é•¿ã€‚</li><li>scaling_speedï¼šå¯åŠ¨æœºå™¨äººåº•ç›˜çš„é€Ÿåº¦ã€‚</li></ul></li><li><p>global_costmap_params.yamlï¼š</p><ul><li>raytrace_rangeï¼šå®æ—¶æ¸…é™¤ä»£ä»·åœ°å›¾ä¸Šéšœç¢ç‰©çš„æœ€å¤§èŒƒå›´ï¼Œæ¸…é™¤çš„æ˜¯obstacle_layerçš„æ•°æ®ã€‚</li></ul></li></ol><h4 id="part3-topics"><a href="#part3-topics" class="headerlink" title="part3 topics"></a>part3 topics</h4><ol><li><p>move_base &amp; move_base_simpleï¼š</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ros::<span class="function">NodeHandle <span class="title">action_nh</span><span class="params">(<span class="string">"move_base"</span>)</span></span>;</span><br><span class="line">action_goal_pub_ = action_nh.advertise&lt;move_base_msgs::MoveBaseActionGoal&gt;(<span class="string">"goal"</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//we'll provide a mechanism for some people to send goals as PoseStamped messages over a topic</span></span><br><span class="line"><span class="comment">//they won't get any useful information back about its status, but this is useful for tools</span></span><br><span class="line"><span class="comment">//like nav_view and rviz</span></span><br><span class="line">ros::<span class="function">NodeHandle <span class="title">simple_nh</span><span class="params">(<span class="string">"move_base_simple"</span>)</span></span>;</span><br><span class="line">goal_sub_ = simple_nh.subscribe&lt;geometry_msgs::PoseStamped&gt;(<span class="string">"goal"</span>, <span class="number">1</span>, boost::bind(&amp;MoveBase::goalCB, <span class="keyword">this</span>, _1));</span><br></pre></td></tr></table></figure><p>ä¹‹å‰æŸ¥çœ‹èŠ‚ç‚¹å›¾çš„æ—¶å€™å‘ç°è¿™ä¸¤ä¸ªèŠ‚ç‚¹éƒ½æä¾›goalï¼Œä¸€ç›´æ²¡æƒ³é€šä¸¤è€…çš„å…³ç³»ï¼Œå‘ç°ä»£ç æ³¨é‡Šé‡Œé¢æœ‰ï¼Œmove_baseç»§æ‰¿äº†actionlibï¼Œæœ‰çŠ¶æ€åé¦ˆï¼ˆ<a href="http://wiki.ros.org/move_base" target="_blank" rel="noopener">è¯¦è§wiki 1.1.2 ActionAPI</a>ï¼‰ï¼Œmove_base_simpleå°±æ˜¯ä¸€ä¸ªpublisherï¼ˆtopicå¯ä»¥æ¥è‡ªrvizï¼cmd lineï¼‰ã€‚</p><p><img src="/2018/10/23/navigation-stack/rostopic.png" alt=""></p><p>/result è®°å½•äº†Goal reached</p><p>/feedback è®°å½•äº†æ¯ä¸ªæ—¶åˆ»æœºå™¨äººçš„ä½å§¿</p><p>/status è®°å½•äº†ä»»åŠ¡è¿›ç¨‹ï¼ˆgoal acceptedã€failedã€abortingï¼‰</p><p>/cancel æ²¡echoå‡ºä¿¡æ¯ï¼Œåº”è¯¥ä¸ä¸Šå±‚å¯¹æ¥</p><p>ã€å®šç‚¹å·¡èˆªã€‘å¦å¤–ï¼Œå®šç‚¹å·¡èˆªçš„æ—¶å€™å°†global_pathçš„bufferè®¾ç½®ä¸ºnå°±å¯ä»¥æ˜¾ç¤ºå¤šæ¡è·¯å¾„äº†ã€‚</p></li><li><p>DWAPlannerçš„global_plan &amp; local_planï¼š</p><p>local_planå°±æ˜¯DWAç®—æ³•æ¯ä¸ªæ—¶åˆ»è®¡ç®—çš„æœ€ä¼˜é¢„æœŸè·¯å¾„ã€‚global_planæ˜¯æ•´ä¸ªå±€éƒ¨ä»£ä»·åœ°å›¾ä¸Šçš„è·¯å¾„ï¼Œå®ƒæ˜¯å…¨å±€è·¯å¾„çš„cropï¼Œå› ä¸ºå±€éƒ¨åŠ¨æ€ç¯å¢ƒä¸ä¼šå½±å“å…¨å±€è·¯å¾„ï¼Œæˆ‘ä»¬åªç ”ç©¶è½åœ¨localmapä»¥å†…è¿™ä¸€æ®µè·¯å¾„æ˜¯å¦éœ€è¦çŸ«æ­£ã€‚</p><p><img src="/2018/10/23/navigation-stack/path.png" alt=""></p></li></ol>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Autolabor</title>
      <link href="/2018/10/15/autolabor/"/>
      <url>/2018/10/15/autolabor/</url>
      <content type="html"><![CDATA[<p>ä»Šå¤©å‘ç°äº†ä¸€ä¸ªæ”¯æŒäºŒæ¬¡å¼€å‘çš„å¼€æºæ–¹æ¡ˆï¼Œè¯´ç™½äº†å°±æ˜¯æŠŠkartoã€acmlã€navigation stackç­‰å‡ ä¸ªROSå¼€æºåŒ…æ•´åˆçš„æ¯”è¾ƒæ¼‚äº®ï¼Œä»£ç ç»“æ„å€¼å¾—å€Ÿé‰´ã€‚</p><ol><li><p>æ‰§è¡Œkeyboard_controlä¹‹å‰è¦å…ˆæ‰§è¡Œè„šæœ¬ï¼Œæ·»åŠ é”®ç›˜ã€‚</p><blockquote><p>è€ASåŒ…é‡Œé¢æ²¡æœ‰keyboard controlï¼Œå¯ä»¥æ‰§è¡Œteleop_twist_keyboardåŒ…ã€‚autolabor_fakeæ˜¯è™šæ‹Ÿå°è½¦çš„driverï¼Œå»ºæ¨¡äº†ç”µæœºã€odomç›¸å…³ä¿¡æ¯ï¼Œè®¢é˜…cmd_velä¿¡æ¯ã€‚æ§åˆ¶çœŸå®å°è½¦æ—¶è¿™ä¸ªèŠ‚ç‚¹è¦æ›¿æ¢ã€‚</p></blockquote></li><li><p>baseæ˜¯å¯¹æœºå™¨äººåº•ç›˜çš„ä»¿çœŸï¼Œlaunchæ–‡ä»¶çš„é»˜è®¤Fixed Frameæ˜¯base_linkï¼Œæƒ³è¦æ§åˆ¶å°è½¦è¿åŠ¨å¯ä»¥å°†Frameåˆ‡æˆreal_mapæˆ–odomã€‚</p><blockquote><p>todolist:</p><p>å‘½ä»¤è¡Œé‡Œé¢æ§åˆ¶é‡çš„æ˜¾ç¤ºä¸å¤ªå¥½çœ‹ï¼Œå¯ä»¥å°è¯•åœ¨æºæ–‡ä»¶é‡Œé¢ä¼˜åŒ–ï¼Œæ·»åŠ äº¤äº’æç¤ºã€‚</p></blockquote><p><img src="/2018/10/15/autolabor/keyboard.png" alt=""></p></li><li><p>stageæ˜¯å¯¹åœºæ™¯çš„ä»¿çœŸï¼Œåœºæ™¯ç”±map_serverè¯»å–ï¼Œlaunchä»¥åå°±èƒ½æŸ¥çœ‹å½“å‰åœºæ™¯åœ°å›¾ã€‚</p><blockquote><p>rostopicé‡Œé¢æœ‰ä¸€ä¸ªinitialposeä¿¡æ¯ï¼Œç”±rvizå‘å¸ƒã€‚rostopic listé‡Œé¢å¥½å¤štopicéƒ½æ˜¯rvizå‘å¸ƒçš„ï¼Œåœ¨displaysæ ç›®é‡Œé¢å–æ¶ˆå‹¾é€‰å°±ä¸ä¼šå‘å¸ƒäº†ã€‚</p></blockquote><p><img src="/2018/10/15/autolabor/initial.png" alt=""></p></li><li><p>objectæ˜¯å¯¹éšœç¢ç‰©çš„ä»¿çœŸï¼Œè°ƒç”¨stageï¼Œæ·»åŠ interactivemarkerï¼Œ<strong>ç„¶åé€‰æ‹©Interactå·¥å…·</strong>ï¼Œç†è®ºä¸Šåœ°å›¾ä¸Šåº”è¯¥å‡ºç°éšœç¢ç‰©ï¼Œä½†æ˜¯æˆ‘æ²¡æ‰¾åˆ°ã€‚ã€‚ã€‚çŠ¶æ€æ˜¾ç¤ºwaiting for tf infoã€‚</p><p><img src="/2018/10/15/autolabor/marker.png" alt=""></p><p><img src="/2018/10/15/autolabor/param.png" alt=""></p><p><strong>ä¿®æ­£</strong>ï¼šæ·»åŠ çš„markerè¦åœ¨topicé‡Œé¢é€‰æ‹©ï¼Œä¸è¦åœ¨typeæ ä¸‹ã€‚ç„¶ååœ°å›¾ä¸Šæ”¾å¥½éšœç¢ç‰©ä»¥å<strong>è¦å³é”®apply</strong>ã€‚</p><p><img src="/2018/10/15/autolabor/obstacle.png" alt=""></p></li><li><p>lidaræ˜¯å¯¹é›·è¾¾ç‚¹äº‘çš„ä»¿çœŸï¼Œlaunchä¸­ç»™äº†ä¸€ä¸ªlidarå’Œmapçš„é™æ€tfï¼Œå®é™…ä½¿ç”¨ä¸­åº”è¯¥ç»™lidarå’Œbase_linkçš„ã€‚</p><blockquote><p>todolist:</p><p>mapå’Œreal_mapç»™çš„æœ‰ç‚¹æ··ä¹±ï¼Œæ˜å¤©ä¼šç»Ÿä¸€ä¸€ä¸‹ã€‚</p></blockquote></li><li><p>è€ASåŒ…create_mapä»¿çœŸè¿‡ç¨‹ä¸­ï¼Œç”±äºåœºæ™¯æä¾›mapserverå’Œslamç®—æ³•åŒæ—¶publishäº†/mapè¿™ä¸ªtopicï¼Œè¦è¿›è¡ŒåŒºåˆ†ï¼Œåœ¨launchæ–‡ä»¶é‡Œé¢å¯¹å…¶ä¸­ä¸€ä¸ªè¿›è¡Œremapï¼š</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">node</span> <span class="attr">pkg</span>=<span class="string">"map_server"</span> <span class="attr">type</span>=<span class="string">"map_server"</span> <span class="attr">name</span>=<span class="string">"map_server"</span> <span class="attr">args</span>=<span class="string">"$(find simulation_launch)/map/MG_map.yaml"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">remap</span> <span class="attr">from</span>=<span class="string">"/map"</span> <span class="attr">to</span>=<span class="string">"real_map"</span>  /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="/2018/10/15/autolabor/remap.png" alt=""></p><p>è¿™æ—¶rostopicé‡Œé¢å°±ä¼šå‡ºç°real_mapè¿™ä¸ªè¯é¢˜ï¼Œä¸¤ä¸ªåœ°å›¾èƒ½å¤ŸåŒæ—¶æ˜¾ç¤ºã€‚</p><p><img src="/2018/10/15/autolabor/real_map.png" alt="real_map"></p></li></ol><h4 id="ä»£ç è§£æ"><a href="#ä»£ç è§£æ" class="headerlink" title="ä»£ç è§£æ"></a>ä»£ç è§£æ</h4><p><strong>é¦–å…ˆæ˜¯simulationåŒ…ï¼š</strong></p><ol><li><p>autolabor_descriptionæ²¡å•¥å¥½è¯´çš„ï¼Œurdfæ–‡ä»¶é‡Œé¢å®šä¹‰äº†ä¸€ä¸ªrobotï¼Œæ•´ä¸ªæœºå™¨äººè¢«æ¸²æŸ“æˆäº†ä¸€ä¸ªbase_linkï¼Œæ²¡æœ‰å­èŠ‚ç‚¹ï¼Œæ‡’ã€‚</p></li><li><p>autolabor_fakeåŒ…æ˜¯åº•ç›˜é©±åŠ¨ï¼Œæä¾›äº†ä¸€ä¸ª<code>autolabor_fake_node</code>èŠ‚ç‚¹ï¼Œå…¶è®¢é˜…ç±»å‹ä¸º<code>geometry_msgs/Twist</code>çš„è¯é¢˜<code>cmd_vel</code>ï¼Œä¿¡æ¯æ¥æºå¯ä»¥æ˜¯joystickï¼keyboardï¼ˆtele_op_xxxï¼‰ï¼cmd lineï¼ˆrostopic pub xxxï¼‰ã€‚å…¶å‘å¸ƒç±»å‹ä¸º<code>nav_msgs/Odometry</code>çš„è¯é¢˜<code>odom</code>ã€‚åŒæ—¶è¯¥èŠ‚ç‚¹è¿˜ä¼šå°†odom frameåˆ°base_link frameçš„transformä¿¡æ¯æä¾›ç»™tf nodeï¼Œç”¨æ¥tf_broadcastã€‚</p><p><img src="/2018/10/15/autolabor/rosgraph.png" alt="rosgraph"></p></li><li><p>lidar_simulationåŒ…è‡ªèº«æä¾›äº†ä¸¤ä¸ªèŠ‚ç‚¹<code>lidar_simulation</code>å’Œ<code>obstacle_simulation</code>ã€‚</p><p>3.1 <code>LidarSimulation::getPose</code>è¿™ä¸ªå‡½æ•°ä¸­æœ‰ä¸€æ®µä»£ç å¼€å§‹æ¯”è¾ƒå›°æƒ‘ï¼š</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ROS_INFO("roll and pitch and yaw and esp :%lf %lf %lf %lf", roll, pitch, yaw, esp);  //esp=0.000001,r&amp;p=0.000000</span></span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">pow</span>(roll,<span class="number">2</span>) + <span class="built_in">pow</span>(pitch,<span class="number">2</span>) &gt; esp)&#123;</span><br><span class="line">start_angle = yaw + max_angle_;</span><br><span class="line">reverse = <span class="number">-1.0</span>;</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;  <span class="comment">// default situation: </span></span><br><span class="line">start_angle = yaw + min_angle_;</span><br><span class="line">reverse = <span class="number">1.0</span>;</span><br></pre></td></tr></table></figure><p>é€šå¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éƒ½ä½¿ç”¨å³æ‰‹åæ ‡ç³»ï¼ŒäºŒç»´å¹³é¢ä¸‹ï¼Œglobal_frame_åˆ°lidar_frame_çš„åæ ‡å˜æ¢transformæ¬§æ‹‰è§’å½¢å¼ä¸‹çš„rå’Œpè§’åº”è¯¥å§‹ç»ˆæ˜¯$0.0$ï¼Œyawä»£è¡¨äº†æ¿€å…‰é›·è¾¾$x$è½´çš„å˜æ¢ï¼ŒåŠ ä¸Š<code>min_angle_</code>å°±åˆ‡æ¢æˆäº†æ¿€å…‰å…‰æŸçš„åˆå§‹å‘å°„è§’åº¦<code>start_angle</code>ã€‚å¦‚æœåæ ‡ç³»å®šä¹‰åäº†ï¼Œr&amp;på°±åº”è¯¥æœ‰å€¼ï¼Œè¿™æ—¶å› ä¸ºåæ ‡è½´å®šä¹‰åè¿‡æ¥äº†ï¼Œæ¿€å…‰å…‰æŸçš„åˆå§‹å‘å°„è§’åº¦å°±å˜æˆäº†ä»æ­£æ–¹å‘ä¸Šçš„<code>max_angle_</code>å¼€å§‹çš„ã€‚</p><p>3.2 <code>LidarSimulation::updateMap</code>è¿™ä¸ªå‡½æ•°å€¼å¾—æ³¨æ„ï¼Œè¿™æ˜¯ä¸€ä¸ªservice clientï¼Œç”¨æ¥è°ƒç”¨åœ°å›¾æ›´æ–°ï¼Œåœ¨å½“å‰åŠŸèƒ½åŒ…çš„é»˜è®¤launchæ–‡ä»¶ä¸­ï¼ŒåªåŠ è½½äº†ä¸€æ¬¡åœ°å›¾ï¼Œæ²¡æœ‰ä½“ç°å‡ºå®ƒçš„ä½œç”¨ã€‚å½“æ‰§è¡Œå»ºå›¾ä»»åŠ¡æ—¶ï¼Œå› ä¸ºmap frameå’Œodom frameä¼šä¸æ–­è¿›è¡ŒçŸ«æ­£ï¼Œå»ºå›¾åŒ…å°±ä¼šcallè¿™ä¸ªrequestæ¥å®æ—¶æ›´æ–°åœ°å›¾ã€‚</p><p><img src="/2018/10/15/autolabor/mapupdate.png" alt="mapupdate"></p><p>3.3 è¯¥åŠŸèƒ½åŒ…ä¸‹è¿˜è‡ªå®šä¹‰äº†ä¸€ä¸ªobstacle serviceï¼Œæä¾›<code>obstacle_simulation</code>èŠ‚ç‚¹æ¥æ›´æ–°éšœç¢ç‰©ä¿¡æ¯ã€‚è¿™é‡Œçš„éšœç¢ç‰©æ˜¯æŒ‡æ‰‹åŠ¨æ·»åŠ çš„éšœç¢ç‰©ï¼ˆinteractiveMarkerï¼‰ï¼Œlaunchæ–‡ä»¶ä¸­å¯ä»¥å®šä¹‰å…¶å½¢çŠ¶é¡¶ç‚¹ã€‚</p><p>â€‹    å…¶ä¸­çš„<code>ObstacleSimulation::pnpoly</code>å‡½æ•°ç”¨æ¥åˆ¤æ–­æŸç‚¹æ˜¯å¦è½åœ¨å¤šè¾¹å½¢å†…ï¼Œä¹‹å‰<strong>åˆ·ç®—æ³•</strong>æ—¶æœ‰è€ƒè™‘è¿‡è¿™ä¸ªé—®é¢˜ï¼Œè¿™é‡Œç»™å‡ºçš„è§£æ³•ä¸çŸ¥é“æ˜¯ä¸æ˜¯æœ€ä¼˜çš„ï¼Œjust for recordï¼š</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bool</span> ObstacleSimulation::pnpoly(geometry_msgs::Polygon&amp; footprint, <span class="keyword">float</span>&amp; x, <span class="keyword">float</span>&amp; y)&#123;</span><br><span class="line">  <span class="keyword">int</span> i,j;</span><br><span class="line">  <span class="keyword">bool</span> c = <span class="literal">false</span>;</span><br><span class="line">  <span class="keyword">for</span> (i=<span class="number">0</span>, j=footprint.points.size()<span class="number">-1</span>; i&lt;footprint.points.size(); j = i++)&#123;</span><br><span class="line">    <span class="keyword">if</span> ( ( (footprint.points.at(i).y &gt; y) != (footprint.points.at(j).y &gt; y) ) &amp;&amp;</span><br><span class="line">         (x &lt; (footprint.points.at(j).x-footprint.points.at(i).x) * (y-footprint.points.at(i).y) / (footprint.points.at(j).y - footprint.points.at(i).y) + footprint.points.at(i).x) )&#123;</span><br><span class="line">      c = !c;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> c;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>è¿™é‡Œé¢çš„forï¼Œå¾ªç¯æ¡ä»¶éå†çš„æ˜¯å¤šè¾¹å½¢çš„æ¯ä¸€æ¡è¾¹ï¼Œå¦‚$(3,0), (0,1), (1,2), (2, 3)$è¿™æ ·ã€‚åˆ¤å®šçš„æ˜¯ç»™å®šç‚¹æ˜¯å¦è½åœ¨ç»™å®šè¾¹çš„<strong>å†…ä¾§</strong>ï¼Œè¿™é‡Œæ‰€è°“çš„å†…ä¾§æ˜¯ä»¥ç»™å®šè¾¹çš„èµ·å§‹èŠ‚ç‚¹ä¸ºåŸç‚¹ï¼Œç»™å®šçº¿æ®µçš„é¡ºæ—¶é’ˆæ–¹å‘ã€‚</p><p>3.4 obstacleçš„å…·ä½“æ“ä½œå®šä¹‰åœ¨static_mapä¸­ï¼Œè¿™é‡Œé¢å‡ºç°äº†ä¸–ç•Œåæ ‡ç³»Worldï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ …æ ¼åœ°å›¾çš„åŸç‚¹åœ¨åœ°å›¾çš„ä¸€è§’ï¼Œæ …æ ¼çš„ä½ç½®ç”¨æ•´å‹æ¥è¡¨ç¤ºï¼Œè€Œä¸–ç•Œåæ ‡ç³»ä¸­æ …æ ¼çš„ä½ç½®ç”±å…¶ä¸­å¿ƒæ¥è¡¨ç¤ºï¼Œä¸¤è€…ç›¸å·®$0.5$ä¸ªresolutionã€‚lidar_simulationé‡Œé¢åˆ›å»ºäº†ä¸€ä¸ªstatic_mapå¯¹è±¡<code>map_</code>ï¼Œä»¥åŠå›è°ƒå‡½æ•°<code>LidarSimulation::obstacleHandleServer</code>ã€‚</p><p>3.5 lidar_simulationåŠŸèƒ½åŒ…ä¸­çš„è¿™ä¸¤ä¸ªèŠ‚ç‚¹ï¼š<code>lidar_simulation</code>æ˜¯mapçº§çš„ï¼Œ<code>obstacle_simulation</code>æ˜¯obstacleçº§çš„ã€‚</p></li></ol><p><strong>æ¥ä¸‹æ¥çœ‹simulation_launchåŒ…ï¼š</strong></p><p>è¿™ä¸ªåŒ…é‡Œé¢æ²¡æœ‰æºä»£ç ï¼Œåªæä¾›äº†å‡ ä¸ªlaunchæ–‡ä»¶ï¼Œç”¨æ¥ä»¿çœŸå‡ ç§ä¸åŒçš„æƒ…å†µï¼š</p><ul><li><code>sim_move_simulation.launch</code>å°±æ˜¯ç®€å•çš„åº•ç›˜æ§åˆ¶ï¼Œæ§åˆ¶å°è½¦åœ¨ç»™å®šåœ°å›¾ä¸Šè¿åŠ¨ï¼ŒåŒæ—¶å¯è§†åŒ–é›·è¾¾ç‚¹äº‘ä¿¡æ¯ã€‚</li><li><code>create_map_simulation.launch</code>ç”¨æ¥å»ºå›¾ï¼Œåœ¨åº•ç›˜æ§åˆ¶çš„åŸºç¡€ä¸Šï¼Œå¯åŠ¨äº†<strong>å»ºå›¾åŠŸèƒ½åŒ…</strong>ã€‚å‘å¸ƒé»˜è®¤åå­—ä¸º/mapçš„topicï¼Œå‘½ä»¤è¡Œæ‰§è¡Œmap_saverä¿å­˜ã€‚</li><li><code>move_base_simulation.launch</code>ç”¨æ¥å¯¼èˆªï¼Œåœ¨åº•ç›˜æ§åˆ¶çš„åŸºç¡€ä¸Šï¼Œå¯åŠ¨äº†<strong>å¯¼èˆªå¥—ä»¶acml&amp;move_base</strong>ï¼Œè¿™æ—¶å°è½¦çš„åº•ç›˜æ§åˆ¶èŠ‚ç‚¹<code>autolabor_fake_node</code>è®¢é˜…çš„<code>cmd_vel</code>ä¿¡æ¯ä¸å†æ¥è‡ªteleop_keyboardï¼Œè€Œæ˜¯æ¥è‡ªmove_baseçš„è§„åˆ’ç»“æœã€‚</li></ul><p><strong>æœ€åæ˜¯move_base_simè¿™ä¸ªåŠŸèƒ½åŒ…ï¼š</strong></p><p>æ˜¯ç”¨ä½œ<strong>çœŸå®åº•ç›˜</strong>æ§åˆ¶çš„ï¼ˆç›®æµ‹å°±æ˜¯å¯¹ROSå¼€æºçš„move_baseåŒ…çš„äºŒæ¬¡å°è£…ï¼Œè²Œä¼¼åˆ äº†ä¸€äº›ä¸ç”¨çš„æ’ä»¶ï¼‰ï¼Œå…ˆskipï¼Œæ¥ä¸‹æ¥æˆ‘ä¼šç›´æ¥è§£æROSå¯¼èˆªå¥—ä»¶ã€‚</p>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>sublimeæ— æ³•å®‰è£…æ’ä»¶</title>
      <link href="/2018/10/12/sublime%E6%97%A0%E6%B3%95%E5%AE%89%E8%A3%85%E6%8F%92%E4%BB%B6/"/>
      <url>/2018/10/12/sublime%E6%97%A0%E6%B3%95%E5%AE%89%E8%A3%85%E6%8F%92%E4%BB%B6/</url>
      <content type="html"><![CDATA[<p>Package Controlçš„é…ç½®æ–‡ä»¶ä¸­æ·»åŠ ï¼š</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">"downloader_precedence":</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"linux"</span>:</span><br><span class="line">[</span><br><span class="line"><span class="string">"curl"</span>,</span><br><span class="line"><span class="string">"urllib"</span>,</span><br><span class="line"><span class="string">"wget"</span></span><br><span class="line">],</span><br><span class="line"><span class="attr">"osx"</span>:</span><br><span class="line">[</span><br><span class="line"><span class="string">"curl"</span>,</span><br><span class="line"><span class="string">"urllib"</span></span><br><span class="line">],</span><br><span class="line"><span class="attr">"windows"</span>:</span><br><span class="line">[</span><br><span class="line"><span class="string">"wininet"</span></span><br><span class="line">]</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><p>OSXå’Œubuntuä¸‹äº²æµ‹å‡æœ‰æ•ˆã€‚</p>]]></content>
      
      
        <tags>
            
            <tag> tool </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>VMwareæ‰©å®¹</title>
      <link href="/2018/10/10/VMware%E6%89%A9%E5%AE%B9/"/>
      <url>/2018/10/10/VMware%E6%89%A9%E5%AE%B9/</url>
      <content type="html"><![CDATA[<p>ä¹‹å‰åˆ›å»ºè™šæ‹Ÿæœºæ—¶ä¸çŸ¥é“éœ€è¦å¤šå¤§ç©ºé—´ï¼Œç»™äº†20Gï¼Œè£…äº†ROSç›¸å…³å¥—ä»¶ä¹‹åç£ç›˜ä½¿ç”¨ç‡åˆ°äº†90%ã€‚</p><p>ç„¶ååœ¨ç¡¬ç›˜è®¾ç½®é‡Œé¢è°ƒæ•´åˆ°äº†60Gï¼Œæˆ‘ä»¥ä¸ºè¿™æ ·å°±å¯ä»¥äº†ã€‚ã€‚ã€‚ã€‚</p><p><img src="/2018/10/10/VMwareæ‰©å®¹/ç¡¬ç›˜.png" alt=""></p><p>too naiveã€‚ã€‚ã€‚è¿˜æ˜¯è¦æ‰‹åŠ¨é…ç½®ä¸€ä¸‹ï¼Œéœ€è¦å®‰è£…å·¥å…·Gpartedï¼š</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install gparted</span><br></pre></td></tr></table></figure><p>ç„¶åå…ˆæŠ¹æ‰extendedå’Œswapä¸¤ä¸ªåˆ†åŒºï¼Œç„¶åå°±å¯ä»¥resizeä¸»åˆ†åŒºäº†ï¼Œç„¶ååœ¨é‡æ–°åˆ›å»ºé‚£ä¸¤ä¸ªåˆ†åŒºå°±å¥½äº†ã€‚</p><p><img src="/2018/10/10/VMwareæ‰©å®¹/gparted.png" alt=""></p><p><strong>Attentionï¼š</strong>ç£ç›˜åªèƒ½æ‰©å±•ï¼Œä¸èƒ½å˜å°ï¼Œå› æ­¤å»ºè®®é€æ¸æ‰©å±•ã€‚</p>]]></content>
      
      
        <tags>
            
            <tag> tool </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CSM, Correlative Scan Matching</title>
      <link href="/2018/10/08/CSM/"/>
      <url>/2018/10/08/CSM/</url>
      <content type="html"><![CDATA[<p>SLAMå‰ç«¯ä¸»è¦è§£å†³ä¸¤ä¸ªé—®é¢˜ï¼Œä¸€æ˜¯ä»ä¸åŒæ—¶åˆ»çš„ä¼ æ„Ÿå™¨è¾“å…¥ä¸­è¯†åˆ«å‡ºåŒä¸€ä¸ªåœ°å›¾ç‰¹å¾ï¼ŒäºŒæ˜¯è®¡ç®—æ¯ä¸ªå½“å‰æ—¶åˆ»æœºå™¨äººç›¸å¯¹è¯¥ç‰¹å¾çš„ä½å§¿ã€‚</p><p>vSLAMèƒ½å¤Ÿè½»æ¾è§£å†³å‰è€…ï¼ŒLidarSLAMè§£å†³åè€…æ— å‹åŠ›ã€‚æœ¬æ–‡è®¨è®ºæ¿€å…‰SLAMå‰ç«¯é—®é¢˜1â€”â€”ç‰¹å¾ç‚¹åŒ¹é…ã€‚ç›®å‰ä¸¤å¤§å¸¸ç”¨æ€è·¯ï¼šscan2scanâ€”â€”ICPï¼Œscan2mapâ€”â€”CSMã€‚</p><p>basicçš„CSMç®—æ³•æ€è·¯å¦‚ä¸‹ï¼šä»å‰ä¸€å¸§æœºå™¨äººä½å§¿å¼€å§‹ï¼Œå¯»æ‰¾æœ€ä¼˜åˆšæ€§å˜æ¢ï¼Œä½¿é›·è¾¾ç‚¹åœ¨æ …æ ¼åœ°å›¾ä¸­çš„ä½ç½®å°½é‡å¯¹åº”äºå æ®åº¦ä¸º1çš„æ …æ ¼ã€‚</p><p>ä¸ºäº†ä¿æŒå®šä½ä¿¡æ¯åŸæœ‰çš„æ•°ä½ç²¾åº¦ï¼Œä½¿ç”¨åŒçº¿æ€§æ’å€¼æ–¹æ³•æ¥è·å–é›·è¾¾ç‚¹çš„åœ°å›¾å€¼(å æ®åº¦)ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å…¶æ‰€åœ¨æ …æ ¼çš„åœ°å›¾å€¼æ¥ç›´æ¥å¯¹åº”ï¼š</p><p><img src="/2018/10/08/CSM/æ’å€¼.png" alt="æ’å€¼"></p><p>$Pm$æ˜¯é›·è¾¾ç‚¹ï¼Œ$P_{00}, P_{01}, P_{10}, P_{11}$æ˜¯é›·è¾¾ç‚¹é‚»è¿‘çš„å››ä¸ª<strong>æ …æ ¼ä¸­å¿ƒ</strong>ç‚¹ã€‚äºæ˜¯å¾—åˆ°åœ°å›¾å€¼ï¼š</p><script type="math/tex; mode=display">\begin{align}M(P_m)&  \approx \frac{y-y_0}{y_1-y_0}M(I_0) +\frac{y_1-y}{y_1-y_0}M(I_1)\\& =\frac{y-y_0}{y_1-y_0} \left ( \frac{x-x_0}{x_1-x_0}M(P_{11}) + \frac{x_1-x}{x_1-x_0}M(P_{01})\right)\\& +\frac{y_1-y}{y_1-y_0}\left ( \frac{x-x_0}{x_1-x_0}M(P_{10}) + \frac{x_1-x}{x_1-x_0}M(P_{00})\right)\end{align}</script><p>é›·è¾¾ç‚¹æ‰€åœ¨ä½ç½®çš„åœ°å›¾å€¼å˜åŒ–æ¢¯åº¦ï¼š</p><script type="math/tex; mode=display">\triangledown M(P_m) = \left [\frac{\delta M(P_m)}{\delta x}, \frac{\delta M(P_m)}{\delta y} \right]\\\frac{\delta M(P_m)}{\delta x} =\frac{y-y_0}{(y_1-y_0)(x_1-x_0)}(M(P_{11})- M(P_{01}))  +\frac{y_1-y}{(y_1-y_0)(x_1-x_0)}(M(P_{10})- M(P_{00}))\\\frac{\delta M(P_m)}{\delta y} =\frac{x-x_0}{(y_1-y_0)(x_1-x_0)}(M(P_{11})- M(P_{10}))  +\frac{x_1-x}{(y_1-y_0)(x_1-x_0)}(M(P_{01})- M(P_{00}))\\</script><p>è®°å½“å‰æ—¶åˆ»çš„å·²æœ‰åœ°å›¾$M$ï¼Œå½“å‰å¸§å…±è¾“å…¥$n$ä¸ªé›·è¾¾ç‚¹$S_1, â€¦, S_n$ï¼Œå…¶å¯¹åº”ä½ç½®çš„å æ®åº¦ä¸º$M(S_k)$ï¼Œæœ€ä¼˜å˜æ¢å®šä¹‰ä¸º$\xi = (\Delta x, \Delta y, \psi)^T$ï¼Œåˆ™æœ€ä¼˜é—®é¢˜çš„æœ€å°äºŒä¹˜æè¿°ä¸ºï¼š</p><script type="math/tex; mode=display">\xi = argmin_{\xi} \sum_{k=1}^n[1-M(S_k(\xi))]^2</script><p>scan2mapçš„é²æ£’æ€§æ›´å¼ºï¼Œä½†æ˜¯å®æ—¶æ€§ä¸Šæ‰“äº†æŠ˜æ‰£ã€‚å¯¹æ­¤ä¸»è¦æœ‰ä¸¤ä¸ªæ”¹è¿›æªæ–½ï¼šä¸€æ˜¯å±€éƒ¨æœç´¢ï¼ŒäºŒæ˜¯åˆ†è¾¨ç‡é‡‘å­—å¡”ã€‚</p><p>ä¸€ã€å±€éƒ¨æœç´¢</p><p>å®é™…è®¡ç®—ä¸­ä¼š<strong>é€‰å®šä¸€ä¸ªæœç´¢åŒºé—´</strong>ï¼Œé€šå¸¸åªåœ¨ä¸Šä¸€æ—¶åˆ»åœ°å›¾ä½ç½®çš„é™„è¿‘ï¼Œå¯¹å…¶ä¸­åŒ…å«çš„å…¨éƒ¨å¯èƒ½ä½å§¿è¿›è¡Œè¯„åˆ†ã€‚</p><p>ä¸Šå¼ï¼ˆæœ€å°äºŒä¹˜è¡¨è¾¾å¼ï¼‰åªåŒ…å«äº†é›·è¾¾ç«¯ç‚¹ï¼Œè€ƒè™‘åˆ°ä¼ æ„Ÿå™¨çš„å™ªç‚¹å½±å“ï¼Œ<strong>å±€éƒ¨æå€¼</strong>å½±å“å¤§ã€‚</p><ul><li>åŸºäºæ¨¡ç‰ˆçš„åŒ¹é…ï¼šé›·è¾¾ç«¯ç‚¹åŠå°„çº¿æ‰€åœ¨æ …æ ¼æ„æˆçš„å¤šè¾¹å½¢åŒºåŸŸï¼Œä»¥æ­¤ä½œä¸ºå±€éƒ¨åœ°å›¾ï¼Œè¿›è¡Œmap2mapåŒ¹é…ã€‚å‡å°‘å±€éƒ¨æå€¼çš„å½±å“ï¼Œæé«˜è®¡ç®—ä»£ä»·ï¼ŒåŒæ—¶è€ƒè™‘åˆ°åŠ¨æ€ç›®æ ‡ï¼Œä¼šå¼•å…¥æ–°çš„å±€éƒ¨æå€¼ã€‚</li></ul><p><img src="/2018/10/08/CSM/æ¨¡ç‰ˆ.png" alt="æ¨¡ç‰ˆ"></p><ul><li><p>LMç®—æ³•ï¼šè¿­ä»£çš„æ–¹å¼æ±‚è§£æœ€å°äºŒä¹˜çš„æœ€ä¼˜è§£ã€‚</p></li><li><p>åˆ†æ”¯ç•Œå®šç®—æ³•ï¼šåŸºäºå¹¿åº¦ä¼˜å…ˆæœç´¢çš„ç®—æ³•ï¼Œé€šè¿‡å¯¹è§£ç©ºé—´æœç´¢æ ‘çš„åˆ†æ”¯è¿›è¡Œæ‰©å±•å’Œå‰ªæï¼Œä¸æ–­è°ƒæ•´æœç´¢æ–¹å‘ï¼ŒåŠ å¿«æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£çš„é€Ÿåº¦ã€‚ç•Œå®šæ ¸å¿ƒï¼šè‹¥å½“å‰åˆ†æ”¯çš„ä¸‹ç•Œ$C_{branch}$å°äºè§£ç©ºé—´ä¸Šç•Œ$C_{HB}$ï¼Œåˆ™è¿›è¡Œæ‹“å±•ï¼Œå¦åˆ™è¿›è¡Œè£å‰ªï¼Œç›´è‡³åˆ°è¾¾å¶å­ç»“ç‚¹ï¼Œå³æ‰¾åˆ°æœ€å°ä»£ä»·è§£ã€‚</p></li></ul><p>äºŒã€å¤šåˆ†è¾¨ç‡é‡‘å­—å¡”</p><p>ä¸¤å¸§é›·è¾¾ç‚¹äº‘çš„ç›¸ä¼¼åŒºåŸŸå¹¶ä¸ä¼šå½±å“åŒ¹é…çš„æœ€ç»ˆç»“æœï¼Œä½†ä¼šå‚ä¸è®¡ç®—ï¼Œå¯¼è‡´æœç´¢æ•ˆç‡é™ä½ï¼Œéœ€è¦æ›´å¤šçš„è¿­ä»£æ¬¡æ•°è¾¾åˆ°æ”¶æ•›ã€‚</p><p>å½“åœ°å›¾åˆ†è¾¨ç‡è¾ƒä½æ—¶ï¼Œéƒ¨åˆ†åœ°å›¾ä¿¡æ¯ä¼šè¢«å¿½ç•¥ï¼Œè¿™ç§é«˜ã€ä½åˆ†è¾¨ç‡ä¸‹çš„å·®å¼‚ï¼Œæœ‰åŠ©äºå¯¹åœ°å›¾ä¸­çš„ç›¸ä¼¼åœºæ™¯è¿›è¡ŒåŒºåˆ†ã€‚å®é™…ä½¿ç”¨ä¸­ï¼Œé¦–å…ˆå°†åˆå§‹ä½å§¿å¯¹åº”çš„é›·è¾¾ç‚¹äº‘ä¸æœ€ä¸Šå±‚ï¼ˆç²—åˆ†è¾¨ç‡ï¼‰çš„åœ°å›¾è¿›è¡ŒåŒ¹é…ï¼Œè®¡ç®—å‡ºå½“å‰åˆ†è¾¨ç‡ä¸‹çš„ä½å§¿ï¼Œå¹¶ä½œä¸ºåˆå§‹å€¼è¿›å…¥æ¬¡ä¸€çº§åœ°å›¾è¿›è¡ŒåŒ¹é…ï¼Œä»¥æ­¤ç±»æ¨ã€‚</p>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>leetcode95-äºŒå‰æœç´¢æ ‘</title>
      <link href="/2018/08/16/leetcode95-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/"/>
      <url>/2018/08/16/leetcode95-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/</url>
      <content type="html"><![CDATA[<p>96å’Œ95éƒ½æ˜¯äºŒå‰æœç´¢æ ‘ï¼Œå…ˆåšçš„96ï¼Œæ±‚æ ‘çš„ç»“æ„æœ‰å‡ ç§ï¼Œæ²¡æ³¨æ„ç»“ç‚¹å¤§å°å…³ç³»ï¼Œç”¨<strong>åŠ¨æ€è§„åˆ’</strong>æ¥åšï¼Œ$dp[i] = dp[0]*dp[i-1] + â€¦ + dp[i-1]*dp[0]$ã€‚æ³¨æ„é€’å½’è°ƒç”¨çš„æ—¶é—´å¤æ‚åº¦ï¼Œ<strong>è‡ªåº•å‘ä¸Š</strong>æ¥ç®—ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">numTrees</span><span class="params">(self, n)</span>:</span></span><br><span class="line">res = [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">if</span> n &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> res[n]</span><br><span class="line">    </span><br><span class="line">    res += [<span class="number">0</span>]*(n<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, n+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</span><br><span class="line">            res[i] += res[j]*res[i<span class="number">-1</span>-j]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> res[n]</span><br></pre></td></tr></table></figure><p>95è¦åˆ—å‡ºç»“æ„äº†ï¼Œæ‰å‘ç°<strong>ä»€ä¹ˆæ˜¯äºŒå‰æœç´¢æ ‘æ¥ç€</strong>â€”â€”å·¦å­æ ‘çš„ç»“ç‚¹å€¼å‡å°äºæ ¹èŠ‚ç‚¹ï¼Œå³å­æ ‘çš„ç»“ç‚¹å€¼å‡å¤§äºæ ¹èŠ‚ç‚¹ã€‚æŒ‰ç…§æƒ¯ä¾‹ï¼Œæ±‚æ•°é‡ç”¨DPï¼Œæ±‚æšä¸¾åˆ™ç”¨DFSï¼š</p><p>éå†æ¯ä¸€ä¸ªæ•°å­—$i$ä½œä¸ºæ ¹èŠ‚ç‚¹ï¼Œé‚£ä¹ˆ$[1, 2, â€¦, i-1]$æ„æˆå…¶å·¦å­æ ‘ï¼Œ$[i+1, i+2, â€¦, n]$æ„æˆå…¶å³å­æ ‘ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateTrees</span><span class="params">(self, n)</span>:</span></span><br><span class="line"><span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line">    <span class="keyword">return</span> self.dfs(<span class="number">1</span>,n)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(self, b, e)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> b &gt; e:</span><br><span class="line">        <span class="keyword">return</span> [<span class="keyword">None</span>]</span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(b, e+<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># set as the root node</span></span><br><span class="line">        leftTrees = self.dfs(b, i<span class="number">-1</span>)</span><br><span class="line">        rightTrees = self.dfs(i+<span class="number">1</span>, e)</span><br><span class="line">        <span class="keyword">for</span> left <span class="keyword">in</span> leftTrees:</span><br><span class="line">            <span class="keyword">for</span> right <span class="keyword">in</span> rightTrees:</span><br><span class="line">                root = TreeNode(i)</span><br><span class="line">                root.left = left</span><br><span class="line">                root.right = right</span><br><span class="line">                res.append(root)</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>çŠ¶æ€ä¼°è®¡</title>
      <link href="/2018/06/30/%E7%8A%B6%E6%80%81%E4%BC%B0%E8%AE%A1/"/>
      <url>/2018/06/30/%E7%8A%B6%E6%80%81%E4%BC%B0%E8%AE%A1/</url>
      <content type="html"><![CDATA[<h4 id="1-æ¦‚è¿°"><a href="#1-æ¦‚è¿°" class="headerlink" title="1. æ¦‚è¿°"></a>1. æ¦‚è¿°</h4><ul><li>ç»å…¸çš„SLAMæ¨¡å‹ç”±ä¸€ä¸ªè¿åŠ¨æ–¹ç¨‹å’Œä¸€ä¸ªè§‚æµ‹æ–¹ç¨‹æ„æˆï¼š<script type="math/tex; mode=display">\left\{\begin{split}& x_k = f(x_{k-1}, u_k) + w_k\\& z_{k,j} = h(y_i, x_k) + v_{kj}\end{split}\right.</script>å…¶ä¸­ï¼Œ$x_k$è¡¨ç¤ºç›¸æœºçš„ä½å§¿ï¼Œå¯ä»¥ç”¨å˜æ¢çŸ©é˜µæˆ–æä»£æ•°æ¥è¡¨ç¤ºï¼Œ$y_i$è¡¨ç¤ºè·¯æ ‡ï¼Œä¹Ÿå°±æ˜¯å›¾åƒä¸­çš„ç‰¹å¾ç‚¹ï¼Œ$w_k$å’Œ$v_{kj}$è¡¨ç¤ºå™ªå£°é¡¹ï¼Œå‡è®¾æ»¡è¶³é›¶å‡å€¼çš„é«˜æ–¯åˆ†å¸ƒã€‚</li></ul><ul><li>æˆ‘ä»¬å¸Œæœ›é€šè¿‡å¸¦å™ªå£°çš„è§‚æµ‹æ•°æ®$z$å’Œè¾“å…¥æ•°æ®$u$æ¨æ–­ä½å§¿$x$å’Œåœ°å›¾$y$çš„æ¦‚ç‡åˆ†å¸ƒã€‚ä¸»è¦é‡‡ç”¨ä¸¤å¤§ç±»æ–¹æ³•ï¼Œä¸€ç±»æ˜¯æ»¤æ³¢æ–¹æ³•ï¼šåŸºäºå½“å‰çŠ¶æ€æ¥ä¼°è®¡ä¸‹ä¸€çŠ¶æ€ï¼Œå¿½ç•¥å†å²ï¼›ä¸€ç±»æ˜¯éçº¿æ€§ä¼˜åŒ–æ–¹æ³•ï¼Œä½¿ç”¨æ‰€æœ‰æ—¶åˆ»çš„æ•°æ®ä¼°è®¡æ–°çŠ¶æ€çš„æœ€ä¼˜åˆ†å¸ƒã€‚ <ul><li>æ»¤æ³¢æ–¹æ³•ä¸»è¦åˆ†ä¸ºæ‰©å±•å¡å°”æ›¼æ»¤æ³¢å’Œç²’å­æ»¤æ³¢ä¸¤å¤§ç±»ã€‚</li><li>éçº¿æ€§ä¼˜åŒ–æ ¹æ®å®ç°ç»†èŠ‚çš„ä¸åŒä¸»è¦åˆ†ä¸ºæ»‘åŠ¨çª—å£æ³•å’ŒPose Graphæ³•ã€‚</li></ul></li></ul><h4 id="2-éçº¿æ€§ä¼˜åŒ–"><a href="#2-éçº¿æ€§ä¼˜åŒ–" class="headerlink" title="2. éçº¿æ€§ä¼˜åŒ–"></a>2. éçº¿æ€§ä¼˜åŒ–</h4><ul><li><p>éçº¿æ€§ä¼˜åŒ–åŸºäºå†å²ï¼ŒåŒæ—¶ä¹Ÿä½œç”¨äºå†å²ï¼Œå› æ­¤æŠŠæ‰€æœ‰å¾…ä¼°è®¡çš„å˜é‡æ”¾åœ¨ä¸€ä¸ªçŠ¶æ€å˜é‡ä¸­ï¼š</p><script type="math/tex; mode=display">x = \{x_1, ..., x_N, y_1, ..., y_M\}</script><p>åœ¨å·²çŸ¥è§‚æµ‹æ•°æ®$z$å’Œè¾“å…¥æ•°æ®$u$çš„æ¡ä»¶ä¸‹ï¼Œå¯¹æœºå™¨äººçš„çŠ¶æ€ä¼°è®¡ï¼š</p><script type="math/tex; mode=display">P(x | z,u)</script><p>å…ˆå¿½ç•¥æµ‹é‡è¿åŠ¨çš„ä¼ æ„Ÿå™¨ï¼Œä»…è€ƒè™‘æµ‹é‡æ–¹ç¨‹ï¼Œæ ¹æ®<strong>è´å¶æ–¯æ³•åˆ™</strong>ï¼š</p><script type="math/tex; mode=display">P(x|z) = \frac{P(z|x)P(x)}{P(z)} \varpropto P(z|x)P(x)</script><ul><li>å…ˆéªŒæ¦‚ç‡$P(x)$ï¼šå…ˆéªŒçš„æ¦‚å¿µæœ€å¥½ç†è§£ï¼Œå°±æ˜¯ä¸€ä¸ªäº‹ä»¶çš„æ¦‚ç‡åˆ†å¸ƒã€‚</li><li>ä¼¼ç„¶æ¦‚ç‡$P(z|x)$ï¼šå·²çŸ¥äº‹ä»¶çš„æ¦‚ç‡åˆ†å¸ƒï¼Œäº‹ä»¶ä¸­æŸçŠ¶æ€çš„æ¦‚ç‡ã€‚</li><li>åéªŒæ¦‚ç‡$P(x|z)$ï¼šåœ¨ç»™å®šæ•°æ®æ¡ä»¶ä¸‹ï¼Œä¸ç¡®å®šæ€§çš„æ¡ä»¶åˆ†å¸ƒã€‚</li><li>$Posterior  \varpropto Likelihood * Prior$</li></ul><p>æ±‚è§£åéªŒåˆ†å¸ƒæ¯”è¾ƒå›°éš¾ï¼Œä½†æ˜¯æ±‚ä¸€ä¸ªçŠ¶æ€æœ€ä¼˜ä¼°è®¡ï¼ˆä½¿å¾—åéªŒæ¦‚ç‡æœ€å¤§åŒ–ï¼‰æ˜¯å¯è¡Œçš„ï¼š</p><script type="math/tex; mode=display">x^*_{MAP} = argmaxP(x|z) = argmaxP(z|x)P(x) = argmaxP(z|x)</script><p>å› ä¸ºå…ˆéªŒæ¦‚ç‡ä¸çŸ¥é“ï¼Œæ‰€ä»¥é—®é¢˜ç›´æ¥è½¬æˆä¸ºæ±‚è§£æå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œé—®é¢˜ä¸­çš„æœªçŸ¥æ•°æ˜¯$x$ï¼Œç›´è§‚æ„ä¹‰å°±æ˜¯ï¼šå¯»æ‰¾ä¸€ä¸ªæœ€ä¼˜çš„çŠ¶æ€åˆ†å¸ƒï¼Œä½¿å…¶æœ€å¯èƒ½äº§ç”Ÿå½“å‰è§‚æµ‹åˆ°çš„æ•°æ®ã€‚</p></li><li><p>å‡è®¾äº†å™ªå£°é¡¹$v_{kj} \thicksim N(0, Q_{k,j})$ï¼Œæ‰€ä»¥æå¤§ä¼¼ç„¶æ¦‚ç‡ä¹Ÿæœä»ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼š</p><script type="math/tex; mode=display">P(z_{kj}|x_k, y_j) = N(h (y_j, x_i), Q)</script><p>æ±‚é«˜æ–¯åˆ†å¸ƒçš„æœ€å€¼é€šå¸¸å–è´Ÿå¯¹æ•°å¤„ç†ï¼Œæœ€å¤§åŒ–å˜æˆæ±‚æœ€å°åŒ–ï¼š</p><script type="math/tex; mode=display">P(x) = \frac{1}{\sqrt{(2\pi)^Ndet(\Sigma)}} exp\bigg(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\bigg)\\-ln(P(x) ) = \frac{1}{2}ln\big((2\pi)^Ndet(\Sigma)\big) + \frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)</script><p>å¯¹æ•°ç¬¬ä¸€é¡¹ä¸$x$æ— å…³ï¼Œç¬¬äºŒé¡¹ç­‰ä»·äºå™ªå£°çš„å¹³æ–¹é¡¹ã€‚å› æ­¤å¯ä»¥å¾—åˆ°ä¼˜åŒ–é—®é¢˜çš„ç›®æ ‡å‡½æ•°ï¼š</p><script type="math/tex; mode=display">e_{k} = x_k - f(x_{k-1}, u_k)\\e_{kj} = z_{kj} - h(x_k, y_j)\\J(x) = \Sigma e_k^TR^{-1}_ke_{k} + \Sigma e_{kj}^TQ^{-1}_ke_{kj}\\x* = argmin_x J(x)</script><p>ä»¥ä¸Šçš„æœ€å°äºŒä¹˜é—®é¢˜å¯ä»¥é‡‡ç”¨å„ç§å„æ ·çš„æ¢¯åº¦ä¸‹é™æ³•æ±‚è§£æœ€ä¼˜è§£ï¼ˆå‚è€ƒ<a href="https://amberzzzz.github.io/2018/05/02/graph-based-optimization/">å›¾ä¼˜åŒ–</a>ï¼‰ã€‚</p></li><li><p>Bundle Ajustment</p><p>ä¼˜åŒ–é—®é¢˜æœ€ç»ˆå¯ä»¥è¡¨ç¤ºæˆ$H\Delta x = g$çš„å½¢å¼ï¼Œå…¶å¯¹è§’çº¿ä¸Šçš„ä¸¤ä¸ªçŸ©é˜µä¸ºç¨€ç–çŸ©é˜µï¼Œä¸”å³ä¸‹è§’çš„çŸ©é˜µç»´åº¦å¾€å¾€è¿œå¤§äºå·¦ä¸Šè§’ï¼ˆå› ä¸ºç‰¹å¾ç‚¹çš„æ•°ç›®è¿œå¤§äºä½å§¿èŠ‚ç‚¹ï¼‰ï¼š</p><script type="math/tex; mode=display">H = \begin{bmatrix}B &E\\E^T & C\end{bmatrix}</script><p>ä¸€ä¸ªæœ‰æ•ˆçš„æ±‚è§£æ–¹å¼ç§°ä¸º<strong>Schuræ¶ˆå…ƒ</strong>ï¼Œä¹Ÿå«<strong>è¾¹ç¼˜åŒ–Marginalization</strong>ï¼Œä¸»è¦æ€è·¯å¦‚ä¸‹ï¼šé¦–å…ˆæ±‚è§£$C$çŸ©é˜µçš„é€†çŸ©é˜µï¼Œç„¶åå¯¹$H$çŸ©é˜µè¿›è¡Œæ¶ˆå…ƒï¼Œç›®æ ‡æ˜¯æ¶ˆå»å…¶å³ä¸Šè§’çš„$E$çŸ©é˜µï¼Œè¿™æ ·å°±èƒ½å¤Ÿå…ˆç‹¬ç«‹æ±‚è§£ç›¸æœºå‚æ•°$\Delta x_c$ï¼Œå†åˆ©ç”¨æ±‚å¾—çš„è§£æ¥æ±‚landmarkså‚æ•°$\Delta x_p$ï¼š</p><script type="math/tex; mode=display">\begin{bmatrix}I &-EC^{-1}\\0 & I\end{bmatrix}\begin{bmatrix}B &E\\E^T & C\end{bmatrix}\begin{bmatrix}\Delta x_c \\\Delta x_p\end{bmatrix} = \begin{bmatrix}I &-EC^{-1}\\0 & I\end{bmatrix}\begin{bmatrix}v \\w\end{bmatrix} \\\begin{bmatrix}B - EC^{-1}E^T & 0\\E^T & C\end{bmatrix}\begin{bmatrix}\Delta x_c \\\Delta x_p\end{bmatrix} = \begin{bmatrix}v  - EC^{-1}w\\w\end{bmatrix}</script><p>å› æ­¤å¯ä»¥è§£å¾—$\Delta x_c$ï¼š</p><script type="math/tex; mode=display">\begin{bmatrix}B - EC^{-1}E^T\end{bmatrix} \Delta x_c = v - EC^{-1}w</script><ul><li>è¿™ä¸ªçŸ©é˜µç§°ä¸ºæ¶ˆå…ƒä¹‹åçš„$S$çŸ©é˜µï¼Œå®ƒçš„ç»´åº¦ä¸ç›¸æœºå‚æ•°çš„ç»´åº¦ä¸€è‡´</li><li>$S$çŸ©é˜µçš„æ„ä¹‰æ˜¯ä¸¤ä¸ªç›¸æœºå˜é‡ä¹‹é—´æ˜¯å¦å­˜åœ¨ç€å…±åŒè§‚æµ‹ç‚¹</li><li>$S$çŸ©é˜µçš„ç¨€ç–æ€§ç”±å®é™…æ•°æ®æƒ…å†µå†³å®šï¼Œå› æ­¤åªèƒ½é€šè¿‡æ™®é€šçš„çŸ©é˜µåˆ†è§£çš„æ–¹å¼æ¥æ±‚è§£</li></ul></li><li><p>æ ¸å‡½æ•°</p><p>å½“è¯¯å·®å¾ˆå¤§æ—¶ï¼ŒäºŒèŒƒæ•°å¢é•¿çš„å¾ˆå¿«ï¼Œä¸ºäº†é˜²æ­¢å…¶è¿‡å¤§æ©ç›–æ‰å…¶ä»–çš„è¾¹ï¼Œå¯ä»¥å°†å…¶æ›¿æ¢æˆå¢é•¿æ²¡é‚£ä¹ˆå¿«çš„å‡½æ•°ï¼Œä½¿å¾—æ•´ä¸ªä¼˜åŒ–ç»“æœæ›´ä¸ºç¨³å¥ï¼Œå› æ­¤åˆå«é²æ£’æ ¸å‡½æ•°ï¼Œå¸¸ç”¨çš„æ ¸æœ‰Huberæ ¸ã€Cauchyæ ¸ã€Tukeyæ ¸ç­‰ï¼ŒHuberæ ¸çš„å®šä¹‰å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">H(e) = \left\{\begin{split}   & \frac{1}{2} e ^2, \ \ \ |e| \leq \delta\\  &  \delta(|e| -  \frac{1}{2}\delta), \ \ else\end{split}   \right.</script></li></ul><h4 id="3-å¡å°”æ›¼æ»¤æ³¢"><a href="#3-å¡å°”æ›¼æ»¤æ³¢" class="headerlink" title="3. å¡å°”æ›¼æ»¤æ³¢"></a>3. å¡å°”æ›¼æ»¤æ³¢</h4><p>æ»¤æ³¢æ€è·¯åŸºäºä¸€ä¸ªé‡è¦çš„<strong>å‡è®¾ï¼šä¸€é˜¶é©¬å°”å¯å¤«æ€§â€”â€”kæ—¶åˆ»çŠ¶æ€åªä¸k-1æ—¶åˆ»çŠ¶æ€æœ‰å…³</strong>ï¼Œæ•´ç†æˆä¸¤ä¸ªè¦ç´ å¦‚ä¸‹ï¼š</p><ul><li>$x_{k-1}$ contains the whole history</li><li>$x_k = f(x_{k-1}, u_k, z_k)$</li></ul><p>åœ¨è¿™é‡Œæˆ‘ä»¬åªéœ€è¦ç»´æŠ¤ä¸€ä¸ªçŠ¶æ€é‡$x_k$ï¼Œå¹¶å¯¹å®ƒä¸æ–­è¿›è¡Œè¿­ä»£æ›´æ–°ï¼Œ<strong>moreoverï¼Œå¦‚æœçŠ¶æ€é‡æœä»é«˜æ–¯åˆ†å¸ƒï¼Œæˆ‘ä»¬åªéœ€è¦ç»´æŠ¤çŠ¶æ€é‡çš„å‡å€¼å’Œæ–¹å·®å³å¯ï¼ˆè¿›ä¸€æ­¥ç®€åŒ–ï¼‰</strong>ã€‚</p><p>é¦–å…ˆè€ƒè™‘ä¸€ä¸ª<strong>çº¿æ€§ç³»ç»Ÿ</strong>ï¼š</p><script type="math/tex; mode=display">\left\{\begin{split}& x_k = A_k x_{k-1}+u_k + w_k\\& z_k = C_k x_k + v_k\end{split}\right.\\w_k \thicksim N(0, R), v_k \thicksim N(0, Q)</script><p>å¡å°”æ›¼æ»¤æ³¢å™¨çš„ç¬¬ä¸€æ­¥<strong>é¢„æµ‹</strong>ï¼Œé€šè¿‡è¿åŠ¨æ–¹ç¨‹ç¡®å®š$x_k$çš„å…ˆéªŒåˆ†å¸ƒï¼Œæ³¨æ„ç”¨ä¸åŒçš„ä¸Šæ ‡åŒºåˆ†ä¸åŒçš„æ¦‚ç‡åˆ†å¸ƒï¼šå°–å¸½å­$\hat x_k$è¡¨ç¤ºåéªŒï¼Œå¹³å¸½å­$\bar x_k$è¡¨ç¤ºå…ˆéªŒï¼š</p><script type="math/tex; mode=display">P(\bar x_k) =  N(\bar x_k, \bar P_k)\\\bar x_k = A_k \hat x_{k-1} + u_k\\\bar P_k = A_k\hat P_{k-1}A_k^T+R</script><p>ç¬¬äºŒæ­¥ä¸º<strong>è§‚æµ‹</strong>ï¼Œé€šè¿‡åˆ†æå®é™…è§‚æµ‹å€¼ï¼Œè®¡ç®—<strong>åœ¨æŸçŠ¶æ€ä¸‹åº”è¯¥äº§ç”Ÿæ€æ ·çš„åˆ†å¸ƒ</strong>ï¼š</p><script type="math/tex; mode=display">P(z_k|x_k) = N(C_kx_k, Q)</script><p>ç¬¬ä¸‰æ­¥ä¸º<strong>æ›´æ–°</strong>ï¼Œæ ¹æ®ç¬¬ä¸€èŠ‚ä¸­çš„<strong>è´å¶æ–¯æ³•åˆ™</strong>ï¼Œå¾—åˆ°$x_k$çš„åéªŒåˆ†å¸ƒï¼š</p><script type="math/tex; mode=display">(\hat x_k, \hat P_k) = N(C_kx_k, Q)N(\bar x_k, \bar P_k)\\K = \bar P_k C_k^T(C_k\bar P_k C_k^T + Q_k)^{-1}\\\hat x_k = \bar x_k + K(z_k-C_k\bar x_k)\\\hat P_k = (I - KC_k)\bar P_k</script><p>æ•´ä½“çš„æµç¨‹å›¾å¦‚ä¸‹ï¼š</p><p><img src="/2018/06/30/çŠ¶æ€ä¼°è®¡/KF.jpeg" alt=""></p><p>å…·ä½“è¿‡ç¨‹æœ¬èŠ‚ä¸­ä¸åšå±•å¼€ï¼Œè¯¦æƒ…å¯ä»¥å‚è€ƒ<a href="">å¡å°”æ›¼æ»¤æ³¢</a>ã€‚é«˜æ–¯åˆ†å¸ƒç»è¿‡çº¿æ€§å˜æ¢ä»ç„¶æœä»é«˜æ–¯åˆ†å¸ƒï¼Œå› æ­¤æ•´ä¸ªè¿‡ç¨‹æ²¡æœ‰å‘ç”Ÿä»»ä½•çš„è¿‘ä¼¼ï¼Œå› æ­¤å¯ä»¥è¯´å¡å°”æ›¼æ»¤æ³¢å™¨æ„æˆäº†çº¿æ€§ç³»ç»Ÿçš„æœ€ä¼˜æ— åä¼°è®¡ã€‚</p><p>ä¸‹é¢è€ƒè™‘<strong>éçº¿æ€§ç³»ç»Ÿ</strong>ï¼š</p><p>SLAMä¸­ä¸ç®¡æ˜¯ä¸‰ç»´è¿˜æ˜¯å¹³é¢åˆšä½“è¿åŠ¨ï¼Œå› ä¸ºéƒ½å¼•å…¥äº†æ—‹è½¬ï¼Œå› æ­¤å…¶è¿åŠ¨æ–¹ç¨‹å’Œè§‚æµ‹æ–¹ç¨‹éƒ½æ˜¯éçº¿æ€§å‡½æ•°ã€‚ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œç»è¿‡éçº¿æ€§å˜æ¢ï¼Œé€šå¸¸å°±ä¸å†æœä»é«˜æ–¯åˆ†å¸ƒï¼Œå› æ­¤<strong>å¯¹äºéçº¿æ€§ç³»ç»Ÿï¼Œå¿…é¡»é‡‡å–ä¸€å®šçš„è¿‘ä¼¼ï¼Œå°†ä¸€ä¸ªéé«˜æ–¯åˆ†å¸ƒè¿‘ä¼¼æˆé«˜æ–¯åˆ†å¸ƒ</strong>ã€‚</p><p>é€šå¸¸çš„åšæ³•æ˜¯ï¼Œå°†kæ—¶åˆ»çš„è¿åŠ¨æ–¹ç¨‹å’Œè§‚æµ‹æ–¹ç¨‹åœ¨$\hat x_{k-1}$ï¼Œ$\hat P_{k-1}$å¤„åšä¸€é˜¶æ³°å‹’å±•å¼€ï¼Œå¾—åˆ°ä¸¤ä¸ªé›…å¯æ¯”çŸ©é˜µï¼š</p><script type="math/tex; mode=display">F = \frac{\partial f}{\partial x_{k-1}}\bigg|_{\hat x_{k-1}}\\H = \frac{\partial h}{\partial x_k}\bigg|_{\hat x_k}</script><p>ä¸­é—´é‡å¡å°”æ›¼å¢ç›Š$K_k$ï¼š</p><script type="math/tex; mode=display">\bar P_k = F\hat P_{k-1}F^T + R_k\\K_k = \bar P_k H^T(H \bar P_k H^T + Q_k)^{-1}</script><p>åéªŒæ¦‚ç‡ï¼š</p><script type="math/tex; mode=display">\hat x_k = \bar x_k + K_k(z_k - h(\bar x_k))\\\hat P_k = (I - K_k H)\bar P_k</script><p> å¯¹äºSLAMè¿™ç§éçº¿æ€§çš„æƒ…å†µï¼ŒEKFç»™å‡ºçš„æ˜¯å•æ¬¡çº¿æ€§è¿‘ä¼¼ä¸‹çš„æœ€å¤§åéªŒä¼°è®¡ï¼ˆMAPï¼‰ã€‚</p><h4 id="4-EKF-VS-Graph-Optimization"><a href="#4-EKF-VS-Graph-Optimization" class="headerlink" title="4. EKF VS Graph-Optimization"></a>4. EKF VS Graph-Optimization</h4><ol><li>é©¬å°”å¯å¤«æ€§æŠ›å¼ƒäº†æ›´ä¹…ä¹‹å‰çš„çŠ¶æ€ï¼Œä¼˜åŒ–æ–¹æ³•åˆ™è¿ç”¨äº†æ›´å¤šçš„ä¿¡æ¯ã€‚</li><li>éçº¿æ€§è¯¯å·®ï¼šä¸¤ç§æ–¹æ³•éƒ½ä½¿ç”¨äº†çº¿æ€§åŒ–è¿‘ä¼¼ï¼ŒEKFåªåœ¨$x_{k-1}$å¤„åšäº†ä¸€æ¬¡çº¿æ€§åŒ–ï¼Œå›¾ä¼˜åŒ–æ³•åˆ™åœ¨æ¯ä¸€æ¬¡è¿­ä»£æ›´æ–°æ—¶éƒ½å¯¹æ–°çš„çŠ¶æ€ç‚¹åšæ³°å‹’å±•å¼€ï¼Œå…¶çº¿æ€§åŒ–çš„æ¨¡å‹æ›´æ¥è¿‘åŸå§‹éçº¿æ€§æ¨¡å‹ã€‚</li><li>å­˜å‚¨ï¼šEKFç»´æŠ¤çš„æ˜¯çŠ¶æ€çš„å‡å€¼å’Œæ–¹å·®ï¼Œå­˜å‚¨é‡ä¸çŠ¶æ€ç»´åº¦æˆå¹³æ–¹å¢é•¿ï¼Œå›¾ä¼˜åŒ–å­˜å‚¨çš„æ˜¯æ¯ä¸ªçŠ¶æ€ç‚¹çš„ä½å§¿ï¼Œå­˜å‚¨çº¿æ€§å¢é•¿ã€‚</li></ol>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ä¼˜åŒ–åº“ï¼šCeres &amp; g2o</title>
      <link href="/2018/06/28/%E4%BC%98%E5%8C%96%E5%BA%93%EF%BC%9Ag2o-Ceres/"/>
      <url>/2018/06/28/%E4%BC%98%E5%8C%96%E5%BA%93%EF%BC%9Ag2o-Ceres/</url>
      <content type="html"><![CDATA[<h3 id="Ceres"><a href="#Ceres" class="headerlink" title="Ceres"></a>Ceres</h3><p>ä½¿ç”¨Ceresæ±‚è§£éçº¿æ€§ä¼˜åŒ–é—®é¢˜ï¼Œä¸»è¦åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼š</p><ul><li><p>ç¬¬ä¸€éƒ¨åˆ†ï¼šæ„å»ºä»£ä»·å‡½æ•°<code>Cost_Functor</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// å®šä¹‰ä¸€ä¸ªå®ä¾‹åŒ–æ—¶æ‰çŸ¥é“çš„ç±»å‹T</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">// è¿ç®—ç¬¦()çš„é‡è½½ï¼Œç”¨æ¥å¾—åˆ°æ®‹å·®fi</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> T* <span class="keyword">const</span> x, T* residual)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">     residual[<span class="number">0</span>] = T(<span class="number">10.0</span>) - x[<span class="number">0</span>];</span><br><span class="line">     <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></li><li><p>ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºæœ€å°äºŒä¹˜é—®é¢˜<code>problem</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Problem problem;</span><br><span class="line"><span class="comment">// ä½¿ç”¨è‡ªåŠ¨æ±‚å¯¼ï¼Œç¬¬ä¸€ä¸ª1æ˜¯è¾“å‡ºç»´åº¦ï¼ˆæ®‹å·®é¡¹ï¼‰ï¼Œç¬¬äºŒä¸ª1æ˜¯è¾“å…¥ç»´åº¦ï¼ˆä¼˜åŒ–é¡¹ï¼‰</span></span><br><span class="line">CostFunction* cost_function = <span class="keyword">new</span> AutoDiffCostFunction&lt;CostFunctor, <span class="number">1</span>, <span class="number">1</span>&gt;(<span class="keyword">new</span> CostFunctor);</span><br><span class="line"><span class="comment">// æ·»åŠ è¯¯å·®é¡¹ï¼ŒNULLè¡¨ç¤ºä¸ä½¿ç”¨æ ¸å‡½æ•°ï¼Œxæ˜¯ä¼˜åŒ–é¡¹</span></span><br><span class="line">problem.AddResidualBlock(cost_function, <span class="literal">NULL</span>, &amp;x);</span><br></pre></td></tr></table></figure></li><li><p>ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ±‚è§£å™¨å‚æ•°é…ç½®<code>Solver</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Solver::Options options;</span><br><span class="line">options.linear_solver_type = ceres::DENSE_QR; <span class="comment">//é…ç½®å¢é‡æ–¹ç¨‹çš„è§£æ³•ï¼Œç¨ å¯†çš„QRåˆ†è§£</span></span><br><span class="line">options.minimizer_progress_to_stdout = <span class="literal">true</span>;<span class="comment">//è¾“å‡ºåˆ°cout</span></span><br><span class="line">Solver::Summary summary;<span class="comment">//ä¼˜åŒ–ä¿¡æ¯</span></span><br><span class="line">Solve(options, &amp;problem, &amp;summary);<span class="comment">//æ±‚è§£</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; summary.BriefReport() &lt;&lt; <span class="string">"\n"</span>;<span class="comment">//è¾“å‡ºä¼˜åŒ–çš„ç®€è¦ä¿¡æ¯</span></span><br></pre></td></tr></table></figure></li></ul><p><strong>ä½¿ç”¨æ ¸å‡½æ•°</strong>ï¼šæ•°æ®ä¸­å¾€å¾€å­˜åœ¨<strong>ç¦»ç¾¤ç‚¹</strong>ï¼Œç¦»ç¾¤ç‚¹ä¼šå¯¹å¯»ä¼˜ç»“æœé€ æˆå½±å“ï¼Œè¿™æ—¶å¯ä»¥ä½¿ç”¨ä¸€äº›æŸå¤±æ ¸å‡½æ•°æ¥å¯¹ç¦»ç¾¤ç‚¹çš„å½±å“åŠ ä»¥æ¶ˆé™¤ï¼ŒCeresåº“ä¸­æä¾›çš„æ ¸å‡½æ•°ä¸»è¦æœ‰ï¼šTrivialLoss ã€HuberLossã€ SoftLOneLoss ã€ CauchyLossã€‚ </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ä½¿ç”¨æ ¸å‡½æ•°</span></span><br><span class="line">problem.AddResidualBlock(cost_function, <span class="keyword">new</span> CauchyLoss(<span class="number">0.5</span>, &amp;x);</span><br></pre></td></tr></table></figure><p><img src="/2018/06/28/ä¼˜åŒ–åº“ï¼šg2o-Ceres/without kernel.png" alt="without kernel"></p><p><img src="/2018/06/28/ä¼˜åŒ–åº“ï¼šg2o-Ceres/with kernel.png" alt="with kernel"></p><h3 id="g2o"><a href="#g2o" class="headerlink" title="g2o"></a>g2o</h3><p>ç”¨g2oä¼˜åŒ–åº“æ¥è¿›è¡Œä¼˜åŒ–çš„æ­¥éª¤å¦‚ä¸‹ï¼š</p><ol><li><p>å®šä¹‰èŠ‚ç‚¹å’Œè¾¹çš„ç±»å‹ï¼Œé€šå¸¸åœ¨é»˜è®¤çš„åŸºç¡€ç±»å‹ä¸Šåšä¿®æ”¹</p><p>å®šä¹‰é¡¶ç‚¹ï¼Œé¡¶ç‚¹çš„åŸºç±»ä¸º<code>g2o::BaseVertex&lt;ä¼˜åŒ–å˜é‡ç»´åº¦ï¼Œæ•°æ®ç±»å‹&gt;</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CurveFittingVertex</span>:</span> <span class="keyword">public</span> g2o::BaseVertex&lt;<span class="number">3</span>, Eigen::Vector3d&gt;</span><br></pre></td></tr></table></figure><p>é¡¶ç‚¹çš„æ›´æ–°å‡½æ•°<code>oplusImpl</code>ï¼šå®šä¹‰å¢é‡åŠ æ³•ï¼Œå› ä¸ºä¼˜åŒ–å˜é‡å’Œå¢é‡ä¹‹é—´å¹¶ä¸ä¸€å®šæ˜¯çº¿æ€§å åŠ çš„å…³ç³»ï¼Œå¦‚ä½å§¿å˜æ¢ã€‚</p><p>å®šä¹‰è¾¹ï¼Œ æœ¬ä¾‹ä¸­çš„è¾¹ä¸ºä¸€å…ƒè¾¹ï¼ŒåŸºç±»ä¸º<code>g2o::BaseUnaryEdge&lt;è§‚æµ‹å€¼ç»´åº¦ï¼Œæ•°æ®ç±»å‹ï¼Œè¿æ¥é¡¶ç‚¹ç±»å‹&gt;</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CurveFittingEdge</span>:</span> <span class="keyword">public</span>  g2o::BaseUnaryEdge&lt;<span class="number">1</span>, <span class="keyword">double</span> , CurveFittingVertex&gt;</span><br></pre></td></tr></table></figure><p>è¯¯å·®é¡¹è®¡ç®—å‡½æ•°<code>computeError</code>ï¼šè®¡ç®—<strong>é¢„æµ‹å€¼</strong>å’Œ<strong>è§‚æµ‹å€¼</strong>çš„è¯¯å·®ã€‚ä¼°è®¡å€¼æ˜¯åŸºäºå½“å‰å¯¹ä¼˜åŒ–å˜é‡çš„estimateè®¡ç®—å‡ºçš„ï¼Œè§‚æµ‹å€¼æ˜¯ç›´æ¥è·å–çš„ï¼Œå¦‚æœ¬ä¾‹ä¸­çš„yå€¼ã€‚</p></li></ol><ol><li><p>æ„å»ºå›¾æ¨¡å‹</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// vertex</span></span><br><span class="line">g2o::VertexSE3Expmap* pose = <span class="keyword">new</span> g2o::VertexSE3Expmap(); <span class="comment">// camera pose</span></span><br><span class="line">pose-&gt;setId( index );</span><br><span class="line">pose-&gt;setEstimate( expression );</span><br><span class="line">optimizer.addVertex ( pose );</span><br><span class="line"></span><br><span class="line"><span class="comment">// edge</span></span><br><span class="line">g2o::EdgeProjectXYZ2UV* edge = <span class="keyword">new</span> g2o::EdgeProjectXYZ2UV();</span><br><span class="line">edge-&gt;setId ( index );</span><br><span class="line">edge-&gt;setVertex ( <span class="number">0</span>, point );</span><br><span class="line">edge-&gt;setVertex ( <span class="number">1</span>, pose );</span><br><span class="line">edge-&gt;setMeasurement ( Eigen::Vector2d ( p.x, p.y ) );     <span class="comment">// å¯¼å…¥è§‚æµ‹å€¼</span></span><br><span class="line">edge-&gt;setParameterId ( <span class="number">0</span>,<span class="number">0</span> );</span><br><span class="line">edge-&gt;setInformation ( Eigen::Matrix2d::Identity() );      <span class="comment">// è®¾ç½®ä¿¡æ¯çŸ©é˜µ</span></span><br><span class="line">optimizer.addEdge ( edge );</span><br></pre></td></tr></table></figure><p>ä¿¡æ¯çŸ©é˜µ<code>edge-&gt;setInformation(ä¿¡æ¯çŸ©é˜µ)</code>ï¼šå› ä¸ºæœ€ç»ˆçš„ä¼˜åŒ–å‡½æ•°æ˜¯$\sum e_i^T \Sigma^{-1}e_i$ï¼Œæ˜¯è¯¯å·®é¡¹å’Œä¿¡æ¯çŸ©é˜µä¹˜ç§¯çš„å½¢å¼ã€‚</p></li><li><p>ä¼˜åŒ–å™¨é…ç½®</p><ul><li>çŸ©é˜µå—Block</li><li>ä¼˜åŒ–ç®—æ³•solver</li><li>å›¾æ¨¡å‹optimizer</li></ul></li><li><p>æ‰§è¡Œä¼˜åŒ–</p></li></ol>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ç›´æ¥æ³•</title>
      <link href="/2018/06/16/%E7%9B%B4%E6%8E%A5%E6%B3%95/"/>
      <url>/2018/06/16/%E7%9B%B4%E6%8E%A5%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>å‰é¢è¯´å®Œäº† <a href="https://amberzzzz.github.io/2018/06/07/PnP-Perspective-n-Point/">PnP</a>ï¼Œè¶çƒ­æ‰“é“æ›´æ–°ç›´æ¥æ³•ï¼Œå› ä¸ºä¸¤è€…çš„æ€è·¯åŸºæœ¬ä¸€è‡´ï¼Œä¸»è¦çš„å·®åˆ«åœ¨äºPnPä¸­åˆ©ç”¨çš„æ˜¯<strong>ç‰¹å¾ç‚¹çš„é‡æŠ•å½±è¯¯å·®</strong>â€”â€”åŒ¹é…ç‚¹åœ¨queryå¸§åƒç´ å¹³é¢ä¸Šçš„å®é™…ä½ç½®å’Œä¼°è®¡ä½ç½®çš„è¯¯å·®ï¼Œç›´æ¥æ³•ä¸æå–ç‰¹å¾ç‚¹ï¼Œè€Œæ˜¯é‡‡ç”¨<strong>åƒç´ äº®åº¦è¯¯å·®</strong>ã€‚</p><h3 id="1-ç›´æ¥æ³•çš„æ¨å¯¼"><a href="#1-ç›´æ¥æ³•çš„æ¨å¯¼" class="headerlink" title="1. ç›´æ¥æ³•çš„æ¨å¯¼"></a>1. ç›´æ¥æ³•çš„æ¨å¯¼</h3><p>ä»¥ç¬¬ä¸€ä¸ªç›¸æœºä¸ºå‚è€ƒç³»ï¼Œç¬¬äºŒä¸ªç›¸æœºçš„è¿åŠ¨å‚æ•°ä¸º$R, t, \xi$ï¼Œå¯¹æŸä¸ªç©ºé—´ç‚¹$P$ï¼š</p><script type="math/tex; mode=display">p_1 = \begin{bmatrix}u_1\\v_1\\1\end{bmatrix} = \frac{1}{Z_1}KP\\p_2 = \begin{bmatrix}u_2\\v_2\\1\end{bmatrix} = \frac{1}{Z_2}K(RP+t) = \frac{1}{Z_2}Kexp(\xi^{\wedge})P</script><p>ä¸¤ä¸ªåƒç´ ç‚¹çš„äº®åº¦è¯¯å·®ï¼š</p><script type="math/tex; mode=display">e = I_1(p_1) - I_2(p_2)</script><p>ç›®æ ‡å‡½æ•°ï¼š</p><script type="math/tex; mode=display">min_{\xi} J(\xi) = \sum_{i=1}^N||e_i^Te_i||^2_2</script><p>è¯¯å·®å‡½æ•°å…³äºä¼˜åŒ–å˜é‡çš„å¯¼æ•°ï¼š</p><script type="math/tex; mode=display">\begin{split}e(\xi + \delta \xi)&  = I_1\big(\frac{1}{Z_1}KP\big) - I_2\big(\frac{1}{Z_2}K(exp(\delta \xi^{\wedge})exp(\xi^{\wedge})P\big)\\& \approx I_1\big(\frac{1}{Z_1}KP\big) - I_2\big(\frac{1}{Z_2}K(1+\delta \xi^{\wedge})exp(\xi^{\wedge})P\big)\\& = I_1\big(\frac{1}{Z_1}KP\big) -  I_2\big(\frac{1}{Z_2}Kexp(\xi^{\wedge})P\big) - I_2\big(\frac{1}{Z_2}K\delta \xi^{\wedge} exp(\xi^{\wedge})P \big)\\& = e(\xi) -  I_2\big(\frac{1}{Z_2}K\delta \xi^{\wedge} exp(\xi^{\wedge})P \big)\end{split}</script><p>ä¸Šé¢çš„æ‰°åŠ¨ç›¸å…³é¡¹ä¸­ï¼Œè®°ï¼š</p><script type="math/tex; mode=display">q =  \delta \xi^{\wedge} exp(\xi^{\wedge})P\\u =  \frac{1}{Z_2}Kq\\</script><p>è¯¯å·®å‡½æ•°çº¿æ€§åŒ–ï¼š</p><script type="math/tex; mode=display">e(\xi + \delta \xi) = e(\xi) - I_2(u)\\\therefore \frac{e(\xi + \delta \xi)}{\partial \delta \xi} = \frac{-I_2(u)}{\partial \delta \xi}\\e(\xi + \delta \xi) =e(\xi) - (\frac{\partial I_2}{\partial u}\frac{\partial u}{\partial q}\frac{\partial q}{\partial \delta \xi})\delta \xi</script><p>$q$è¡¨ç¤ºæ‰°åŠ¨åˆ†é‡åœ¨ç¬¬äºŒç›¸æœºåæ ‡ç³»ä¸‹çš„åæ ‡ï¼ˆå›é¡¾å…³äº$R$çš„å¾®åˆ†æ–¹ç¨‹ï¼š$\dot R(t) = \phi_0^{\wedge}R(t)$ï¼‰ï¼Œå› æ­¤$u$çš„æ„ä¹‰ä¸ºåƒç´ åæ ‡ï¼Œ$\frac{\partial I_2}{\partial u}$çš„ç‰©ç†æ„ä¹‰ä¸ºåƒç´ æ¢¯åº¦ï¼Œ$\frac{\partial u}{\partial q}$çš„ç‰©ç†æ„ä¹‰ä¸ºåƒç´ åæ ‡å…³äºä¸‰ç»´ç‚¹çš„å¯¼æ•°ï¼ˆå‚è€ƒé’ˆå­”ç›¸æœºæ¨¡å‹ï¼‰ï¼Œ$\frac{\partial q}{\partial \delta \xi}$çš„ç‰©ç†æ„ä¹‰ä¸ºä¸‰ç»´ç‚¹å…³äºæ‰°åŠ¨çš„å¯¼æ•°ï¼ˆå‚è€ƒæä»£æ•°ï¼‰ã€‚</p><h3 id="2-ç›´æ¥æ³•åˆ†ç±»"><a href="#2-ç›´æ¥æ³•åˆ†ç±»" class="headerlink" title="2. ç›´æ¥æ³•åˆ†ç±»"></a>2. ç›´æ¥æ³•åˆ†ç±»</h3><p>æ ¹æ®Pçš„æ¥æºï¼Œç›´æ¥æ³•åˆ†ä¸ºä¸‰ç±»ï¼š</p><ol><li>Pæ¥è‡ªäºç¨€ç–å…³é”®ç‚¹â€”â€”ç¨€ç–ç›´æ¥æ³•</li><li>Pæ¥è‡ªéƒ¨åˆ†åƒç´ ï¼Œåªä½¿ç”¨å¸¦æœ‰æ¢¯åº¦çš„åƒç´ ç‚¹â€”â€”åŠç¨ å¯†ç›´æ¥æ³•</li><li>Pä¸ºæ‰€æœ‰åƒç´ â€”â€”ç¨ å¯†ç›´æ¥æ³•</li></ol><h3 id="3-ä»£ç å®ç°"><a href="#3-ä»£ç å®ç°" class="headerlink" title="3. ä»£ç å®ç°"></a>3. ä»£ç å®ç°</h3><p>ä¸»è¦å…³æ³¨Edgeç±»é‡Œé¢é‡å®šä¹‰çš„å¢é‡æ›´æ–°å‡½æ•°linearizeOplus()é‡Œé¢JacobiançŸ©é˜µçš„å†™æ³•ã€‚</p><script type="math/tex; mode=display">\begin{split}J & = -\frac{\partial I_2}{\partial u}\frac{\partial u}{\partial q}\frac{\partial q}{\partial \delta \xi}\\& = -\frac{\partial I_2}{\partial u}\frac{\partial u}{\partial \delta \xi}\\\end{split}</script><ul><li>å‰ä¸€é¡¹æ˜¯$u$å¤„çš„åƒç´ æ¢¯åº¦ï¼Œä½¿ç”¨æ•°å€¼å¯¼æ•°ï¼š</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// jacobian from I to u (1*2)</span></span><br><span class="line">Eigen::Matrix&lt;<span class="keyword">double</span>, <span class="number">1</span>, <span class="number">2</span>&gt; jacobian_pixel_uv;</span><br><span class="line">jacobian_pixel_uv ( <span class="number">0</span>,<span class="number">0</span> ) = ( getPixelValue ( u+<span class="number">1</span>,v )-getPixelValue ( u<span class="number">-1</span>,v ) ) /<span class="number">2</span>;</span><br><span class="line">jacobian_pixel_uv ( <span class="number">0</span>,<span class="number">1</span> ) = ( getPixelValue ( u,v+<span class="number">1</span> )-getPixelValue ( u,v<span class="number">-1</span> ) ) /<span class="number">2</span>;</span><br></pre></td></tr></table></figure><ul><li><p>getPixelValueè¿™ä¸ªå‡½æ•°æ¶‰åŠåˆ°ä¸€ä¸ª<strong>åŒçº¿æ€§æ’å€¼</strong>ï¼Œå› ä¸ºä¸Šé¢çš„äºŒç»´åæ ‡uvæ˜¯é€šè¿‡ç›¸æœºæŠ•å½±å˜æ¢å¾—åˆ°çš„ï¼Œæ˜¯æµ®ç‚¹å½¢å¼ï¼Œè€Œåƒç´ å€¼æ˜¯ç¦»æ•£çš„æ•´æ•°å€¼ï¼Œä¸ºäº†æ›´ç²¾ç»†åœ°è¡¨ç¤ºåƒç´ äº®åº¦ï¼Œè¦å¯¹å›¾åƒè¿›è¡Œè¿›è¡Œæ’å€¼ã€‚</p><ul><li>çº¿æ€§æ’å€¼ï¼šå·²çŸ¥æ•°æ®$(x_0, y_0)$å’Œ$(x_1, y_1)$ï¼Œè¦è®¡ç®—$[x_0, x_1]$åŒºé—´å†…ä»»ä¸€$x$å¯¹åº”çš„$y$å€¼ï¼š</li></ul><script type="math/tex; mode=display">\frac{y - y_0}{x-x_0} = \frac{y_1-y_0}{x_1-x_0}\\\therefore y = \frac{x_1 -x}{x_1 - x_0}y_0 + \frac{x-x_0}{x_1-x_0}y_1</script><ul><li><p>åŒçº¿æ€§æ’å€¼ï¼šæœ¬è´¨ä¸Šå°±æ˜¯åœ¨ä¸¤ä¸ªæ–¹å‘ä¸Šåšçº¿æ€§æ’å€¼ï¼š</p><p><img src="/2018/06/16/ç›´æ¥æ³•/interpolation.png" alt="interpolate"></p><p>é¦–å…ˆæ˜¯xæ–¹å‘ï¼š</p><script type="math/tex; mode=display">f(R_1) = \frac{x_2 - x}{x_2-x_1}f(Q_{11}) + \frac{x - x_1}{x_2-x_1}f(Q_{21}), \ where\ R_1 = (x, y_1)\\f(R_2) = \frac{x_2 - x}{x_2-x_1}f(Q_{12}) + \frac{x - x_1}{x_2-x_1}f(Q_{22}),  \ where\ R_2 = (x, y_2)</script><p>ç„¶åyæ–¹å‘ï¼š</p><script type="math/tex; mode=display">f(P) = \frac{y_2 - y}{y_2 - y_1}f(R_1) + \frac{y-y_1}{y_2-y_1}f(R_2)</script><p>ç»¼åˆèµ·æ¥å°±æ˜¯ï¼š </p><script type="math/tex; mode=display">f(x,y) = \frac{f(Q_{11})}{(x_2-x_1)(y_2-y_1)}(x_2-x)(y_2-y) +  \frac{f(Q_{21})}{(x_2-x_1)(y_2-y_1)}(x-x_1)(y_2-y) \\+ \frac{f(Q_{12})}{(x_2-x_1)(y_2-y_1)}(x_2-x)(y-y_1) +  \frac{f(Q_{22})}{(x_2-x_1)(y_2-y_1)}(x-x_1)(y-y_1)</script></li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">float</span> <span class="title">getPixelValue</span> <span class="params">( <span class="keyword">float</span> x, <span class="keyword">float</span> y )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    uchar* data = &amp; image_-&gt;data[ <span class="keyword">int</span> ( y ) * image_-&gt;step + <span class="keyword">int</span> ( x ) ];</span><br><span class="line">    <span class="keyword">float</span> xx = x - <span class="built_in">floor</span> ( x );</span><br><span class="line">    <span class="keyword">float</span> yy = y - <span class="built_in">floor</span> ( y );</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">float</span> (</span><br><span class="line">            ( <span class="number">1</span>-xx ) * ( <span class="number">1</span>-yy ) * data[<span class="number">0</span>] +</span><br><span class="line">            xx* ( <span class="number">1</span>-yy ) * data[<span class="number">1</span>] +</span><br><span class="line">            ( <span class="number">1</span>-xx ) *yy*data[ image_-&gt;step ] +</span><br><span class="line">            xx*yy*data[image_-&gt;step+<span class="number">1</span>]</span><br><span class="line">    );</span><br></pre></td></tr></table></figure><ul><li>åä¸¤é¡¹éƒ½æ˜¯ä¸ç›¸æœºå‚æ•°å’Œä¸‰ç»´ç‚¹åæ ‡æœ‰å…³ï¼Œå¯ä»¥åˆå¹¶ï¼ŒåŒæ—¶æ³¨æ„g2oä¸­å¯¹SE3çš„å®šä¹‰å¹³ç§»å’Œæ—‹è½¬å’Œæœ¬æ–‡è®¾å®šæ˜¯åè¿‡æ¥çš„ã€‚</li></ul><script type="math/tex; mode=display">\xi = \begin{bmatrix}\rho\\\phi\end{bmatrix}\\\frac{\partial u}{\partial \delta \xi}=\begin{bmatrix}\frac{f_x}{Z} & 0 & -\frac{f_xX}{Z^2} & |& -\frac{f_xXY}{Z^2} & f_x + \frac{f_xX^2}{Z^2} & -\frac{f_xY}{Z}\\0 & \frac{f_y}{Z}  & -\frac{f_yY}{Z^2}& | & - f_y - \frac{f_xY^2}{Z^2}  & \frac{f_yXY}{Z^2} &  \frac{f_yX}{Z}\\\end{bmatrix}</script><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// jacobian from u to xi (2*6)</span></span><br><span class="line">Eigen::Matrix&lt;<span class="keyword">double</span>, <span class="number">2</span>, <span class="number">6</span>&gt; jacobian_uv_ksai;</span><br><span class="line"><span class="comment">// xi = [\phi, \pho]</span></span><br><span class="line">jacobian_uv_ksai ( <span class="number">0</span>,<span class="number">0</span> ) = - x*y*invz_2 *fx_;</span><br><span class="line">jacobian_uv_ksai ( <span class="number">0</span>,<span class="number">1</span> ) = ( <span class="number">1</span>+ ( x*x*invz_2 ) ) *fx_;</span><br><span class="line">jacobian_uv_ksai ( <span class="number">0</span>,<span class="number">2</span> ) = - y*invz *fx_;</span><br><span class="line">jacobian_uv_ksai ( <span class="number">0</span>,<span class="number">3</span> ) = invz *fx_;</span><br><span class="line">jacobian_uv_ksai ( <span class="number">0</span>,<span class="number">4</span> ) = <span class="number">0</span>;</span><br><span class="line">jacobian_uv_ksai ( <span class="number">0</span>,<span class="number">5</span> ) = -x*invz_2 *fx_;</span><br><span class="line"></span><br><span class="line">jacobian_uv_ksai ( <span class="number">1</span>,<span class="number">0</span> ) = - ( <span class="number">1</span>+y*y*invz_2 ) *fy_;</span><br><span class="line">jacobian_uv_ksai ( <span class="number">1</span>,<span class="number">1</span> ) = x*y*invz_2 *fy_;</span><br><span class="line">jacobian_uv_ksai ( <span class="number">1</span>,<span class="number">2</span> ) = x*invz *fy_;</span><br><span class="line">jacobian_uv_ksai ( <span class="number">1</span>,<span class="number">3</span> ) = <span class="number">0</span>;</span><br><span class="line">jacobian_uv_ksai ( <span class="number">1</span>,<span class="number">4</span> ) = invz *fy_;</span><br><span class="line">jacobian_uv_ksai ( <span class="number">1</span>,<span class="number">5</span> ) = -y*invz_2 *fy_;</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>OpenCVå®šåˆ¶æºç ç¼–è¯‘</title>
      <link href="/2018/06/12/OpenCV%E5%AE%9A%E5%88%B6%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"/>
      <url>/2018/06/12/OpenCV%E5%AE%9A%E5%88%B6%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/</url>
      <content type="html"><![CDATA[<h3 id="1-cmakeé€‰é¡¹"><a href="#1-cmakeé€‰é¡¹" class="headerlink" title="1. cmakeé€‰é¡¹"></a>1. cmakeé€‰é¡¹</h3><ul><li>æµ‹è¯•å•å…ƒå¯ä»¥å…³æ‰ï¼šBUILD_DOCSï¼ŒBUILD_EXAMPLESï¼ŒBUILD_XXX_TESTSï¼ŒBUILD_opencv_ts(ä¸€äº›å•å…ƒæµ‹è¯•ä»£ç )ï¼ŒBUILD_PACKAGE (CPACK_BINARY_XXXï¼ŒCPACK_SOURCE_XXX)ï¼ŒINSTALL_XXX</li><li>å‡å°‘å¼•å…¥ä½“ç§¯ï¼šæ‰“å¼€ä¸–ç•Œæ¨¡å—å¼€å…³BUILD_opencv_world(æš‚æ—¶æ²¡å¼€ï¼Œå› ä¸ºç¼–è¯‘ä¹‹åå‘ç°æ‰¾ä¸åˆ°è¦å¼•ç”¨çš„å¤´æ–‡ä»¶äº†)ï¼Œæ‰“å¼€BUILD_SHARED_LIBS</li><li>å…³æ‰éŸ³è§†é¢‘å¤„ç†ç›¸å…³æ¨¡å—ï¼šBUILD_opencv_videoï¼ŒBUILD_opencv_videoioï¼ŒBUILD_opencv_videostabï¼ŒWITH_1394ï¼ŒWITH_GSTREAMER_XXX</li><li>å…³é—­GPUç›¸å…³æ¨¡å—ï¼šWITH_OPENCLï¼ŒWITH_CUDA</li><li>æ‰“å¼€TBBæ¨¡å—ï¼šéšå¼çš„å¹¶è¡Œè®¡ç®—ç¨‹åºï¼Œåº•å±‚ä¾èµ–äºæ“ä½œç³»ç»Ÿçš„å¤šçº¿ç¨‹åº“ï¼ŒBUILD_TBB</li><li>æ‰“å¼€vizæ¨¡å—ï¼šWITH_VTKï¼ŒBUILD_opencv_viz</li><li>æš‚æ—¶æ²¡å¼€å¯Javaç›¸å…³æ¨¡å—ï¼šantï¼Œå°±æ²¡brewè¿‡è¿™ä¸ªåŒ…</li></ul><p>ä»¥ä¸Šreference from <a href="https://blog.csdn.net/rrrfff/article/details/76796261" target="_blank" rel="noopener">åšå®¢1</a>ï¼Œ<a href="http://tianchunbinghe.blog.163.com/blog/static/7001201151592834161/" target="_blank" rel="noopener">åšå®¢2</a></p><h3 id="2-extra-modules"><a href="#2-extra-modules" class="headerlink" title="2. extra modules"></a>2. extra modules</h3><blockquote><p>with opencv3.0, SURF/SIFT and some other things have been moved to a seperate <a href="https://github.com/itseez/opencv_contrib/" target="_blank" rel="noopener">opencv_contrib repo</a>.</p></blockquote><p>ä¸€éƒ¨åˆ†æ¨¡å—è¢«ç‹¬ç«‹åˆ°äº†opencv_contribè¿™ä¸ªåŒ…ï¼Œé¦–å…ˆcloneåˆ°æœ¬åœ°ï¼Œç„¶ååœ¨cmakeé€‰é¡¹é‡Œé¢æ‰¾åˆ°OPENCV_EXTRA_MODULES_PATHï¼Œå¡«å¥½ã€‚</p><h3 id="2-å…¶ä»–è¯´æ˜"><a href="#2-å…¶ä»–è¯´æ˜" class="headerlink" title="2. å…¶ä»–è¯´æ˜"></a>2. å…¶ä»–è¯´æ˜</h3><p>å¦å¤–ä¹‹å‰brew installçš„opencvåŒ…ä¸€å®šè¦å¸è½½æ‰ï¼Œä¸è¦ä¹±linkï¼Œå¦åˆ™INCLUDEå’ŒLIBSçš„è·¯å¾„éƒ½ä¼šå‡ºé—®é¢˜ï¼Œæ‰‹åŠ¨ä¿®æ”¹cmakeæ–‡ä»¶ä¸è¦å¤ªé…¸çˆ½ã€‚</p>]]></content>
      
      
        <tags>
            
            <tag> opencv </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>leetcode 81 æœç´¢æ—‹è½¬æ•°ç»„2</title>
      <link href="/2018/06/10/leetcode-81-%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%842/"/>
      <url>/2018/06/10/leetcode-81-%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%842/</url>
      <content type="html"><![CDATA[<p>å®ådissè¿™é“é¢˜ï¼Œä¸€çœ‹æ˜¯æ—‹è½¬æ’åºæ•°ç»„ç›´æ¥åšäº†ï¼Œç„¶åæ‰å‘ç°æµ‹è¯•æ ·ä¾‹é‡Œé¢å‡ºç°äº†å·¦å³è¾¹ç•Œé‡åˆçš„æƒ…å†µï¼Œç„¶åä»”ç»†å†å®¡é¢˜æ‰å‘ç°è¿™è¡Œå°å­—ï¼š</p><p><img src="/2018/06/10/leetcode-81-æœç´¢æ—‹è½¬æ•°ç»„2/å±å¹•å¿«ç…§ 2018-06-10 14.59.19.png" alt=""></p><p>ä¸åŒ…å«é‡å¤å…ƒç´ çš„æƒ…å†µä¸‹ä»£ç å®ç°å¦‚ä¸‹ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">search</span><span class="params">(self, nums, target)</span>:</span></span><br><span class="line"></span><br><span class="line">        size = len(nums)</span><br><span class="line">        start = <span class="number">0</span></span><br><span class="line">        end = size - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> start &lt;= end:</span><br><span class="line"></span><br><span class="line">            mid = (start + end) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> nums[mid] == target:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> nums[mid] &lt;= nums[end]:</span><br><span class="line">                <span class="keyword">if</span> target &lt; nums[mid] <span class="keyword">or</span> target &gt; nums[end]:</span><br><span class="line">                    end = mid - <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    start = mid + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> target &gt; nums[mid] <span class="keyword">or</span> target &lt; nums[start]:</span><br><span class="line">                    start = mid + <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    end = mid - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br></pre></td></tr></table></figure><p>å› ä¸ºæ•°ç»„ä¸­ç°åœ¨å­˜åœ¨é‡å¤çš„å…ƒç´ ï¼Œå› æ­¤æœ‰ä¸€ä¸ªç‰¹æ®Šçš„æƒ…å†µï¼šå·¦å³è¾¹ç•Œå€¼ç›¸åŒï¼Œå¹¶ä¸”nums[mid]å€¼ä¸è¾¹ç•Œå€¼ä¹Ÿç›¸åŒï¼Œè¿™æ—¶nums[mid]å¯èƒ½ä½äºä¸¤æ®µæ•°ç»„çš„ä»»æ„ä¸€è¾¹ã€‚å› æ­¤è¦ç‹¬ç«‹è®¨è®ºä¸€ä¸‹è¿™ä¸ªæƒ…å†µï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">search</span><span class="params">(self, nums, target)</span>:</span></span><br><span class="line"></span><br><span class="line">        size = len(nums)</span><br><span class="line">        start = <span class="number">0</span></span><br><span class="line">        end = size - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> start &lt;= end:</span><br><span class="line"></span><br><span class="line">            mid = (start + end) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> nums[mid] == target:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> nums[mid] &lt; nums[end]:</span><br><span class="line">            <span class="keyword">if</span> target &lt; nums[mid] <span class="keyword">or</span> target &gt; nums[end]:</span><br><span class="line">                end = mid - <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                start = mid + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">elif</span> nums[mid] &gt; nums[end]:</span><br><span class="line">                <span class="keyword">if</span> target &gt; nums[mid] <span class="keyword">or</span> target &lt; nums[start]:</span><br><span class="line">                    start = mid + <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    end = mid - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># nums[mid] = nums[end]çš„æƒ…å†µ</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">            end -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br></pre></td></tr></table></figure><p>æµ‹è¯•ç”¨æ—¶50msï¼Œå› ä¸ºè¾¹ç•Œé‡å¤çš„å¾ªç¯æ²¡æœ‰æœ‰æ•ˆåœ°äºŒåˆ†æ•°ç»„ï¼Œä½†æ˜¯æ€è·¯è´¼ç®€å•å•Šã€‚</p>]]></content>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PnP, Perspective-n-Point</title>
      <link href="/2018/06/07/PnP-Perspective-n-Point/"/>
      <url>/2018/06/07/PnP-Perspective-n-Point/</url>
      <content type="html"><![CDATA[<h3 id="1-æ¦‚è¿°"><a href="#1-æ¦‚è¿°" class="headerlink" title="1. æ¦‚è¿°"></a>1. æ¦‚è¿°</h3><blockquote><p>PnPæ˜¯æ±‚è§£3Dåˆ°2Dç‚¹å¯¹è¿åŠ¨çš„æ–¹æ³•ï¼Œå®ƒæè¿°äº†å½“çŸ¥é“nä¸ª3Dç©ºé—´ç‚¹åŠå…¶æŠ•å½±ä½ç½®æ—¶ï¼Œå¦‚ä½•ä¼°è®¡ç›¸æœºçš„ä½å§¿å˜æ¢ã€‚æœ€å°‘åªéœ€è¦3ä¸ªç‚¹å¯¹å°±å¯ä»¥ä¼°è®¡ç›¸æœºçš„è¿åŠ¨ã€‚</p></blockquote><p>è¯¥æ–¹æ³•ä½¿ç”¨çš„æ¡ä»¶æ˜¯ï¼Œå‚è€ƒç‚¹ä¸ºä¸–ç•Œåæ ‡ç³»ä¸‹çš„ç‰¹å¾ç‚¹ï¼Œå…¶ç©ºé—´ä½ç½®å·²çŸ¥ï¼Œå¹¶ä¸”çŸ¥é“queryå¸§ä¸­å¯¹åº”å‚è€ƒç‚¹çš„åƒç´ åæ ‡ã€‚</p><p>PnPé—®é¢˜çš„æ±‚è§£æ–¹æ³•æœ‰å¾ˆå¤šç§ï¼š</p><ul><li>ç›´æ¥çº¿æ€§å˜æ¢</li><li>P3P</li><li>éçº¿æ€§ä¼˜åŒ–æ–¹æ³•â€”â€”Bundle Ajustment</li></ul><p>æœ¬èŠ‚ç€çœ¼äºBAæ±‚è§£æ–¹æ³•ï¼Œå…¶ä»–æ–¹æ³•æš‚æ—¶ä¸åšå±•å¼€ã€‚</p><h3 id="2-Bundle-Ajustment"><a href="#2-Bundle-Ajustment" class="headerlink" title="2. Bundle Ajustment"></a>2. Bundle Ajustment</h3><p>åˆ©ç”¨ä¼˜åŒ–æ±‚è§£çš„æ€è·¯æ˜¯ï¼š<strong>æœ€å°åŒ–é‡æŠ•å½±è¯¯å·®</strong>â€”â€”æœŸæœ›è®¡ç®—queryå¸§çš„ç›¸æœºä½å§¿$R, t$ï¼Œå®ƒçš„æä»£æ•°ä¸º$\xi$ï¼Œç©ºé—´ç‰¹å¾ç‚¹çš„åæ ‡ä¸º$P_i = [X_i, Y_i, Z_i]^T$ï¼Œå…¶åœ¨queryå¸§ä¸Šçš„åƒç´ åæ ‡ä¸º$u_i = [u_i, v_i]^T$ï¼Œé‚£ä¹ˆç†è®ºä¸Šï¼š</p><script type="math/tex; mode=display">s_i u_i = K exp(\xi^{\wedge})P_i</script><p>æ„å»ºæˆæœ€å°äºŒä¹˜é—®é¢˜å°±æ˜¯ï¼šå¯»æ‰¾æœ€ä¼˜çš„ç›¸æœºä½å§¿$\xi$ï¼Œä½¿å¾—è¯¯å·®æœ€å°åŒ–ï¼š</p><script type="math/tex; mode=display">\xi^* = argmin_{\xi} \frac{1}{2}\sum_{i=1}^{n}||u_i -\frac{1}{s_i}Kexp(\xi^{\wedge})P_i||^2_2</script><p>åœ¨ä½¿ç”¨ä¼˜åŒ–åº“æ¥æ±‚è§£ä¹‹å‰ï¼Œè¿˜æœ‰ä¸€ä¸ªé—®é¢˜â€”â€”æ¯ä¸ªè¯¯å·®é¡¹$e_i = u_i -\frac{1}{s_i}Kexp(\xi^{\wedge})P_i$çš„å¯¼æ•°$J_i$ã€‚</p><blockquote><p>å›å¿†<a href="https://amberzzzz.github.io/2018/05/02/graph-based-optimization/">å›¾ä¼˜åŒ–</a>ä¸­è®²è¿‡çš„ï¼Œä¼˜åŒ–é—®é¢˜æœ€ç»ˆè½¬åŒ–æˆä¸ºçŸ©é˜µçš„çº¿æ€§æ±‚è§£$H\Delta x = g$ï¼Œå…¶ä¸­çŸ©é˜µ$H$æ˜¯ç”±å•ä¸ªè¯¯å·®é¡¹ä¸€é˜¶å±•å¼€$e(x+\Delta x) = e(x) + J\Delta x$ä¸­çš„é›…å¯æ¯”çŸ©é˜µ$J_i$ æ„æˆçš„ç¨€ç–å¯¹ç§°é˜µã€‚</p></blockquote><p>è¯¯å·®é¡¹æ˜¯ä¸€ä¸ªäºŒç»´å‘é‡ï¼ˆåƒç´ åæ ‡å·®ï¼‰ï¼Œä¼˜åŒ–å˜é‡æ˜¯ä¸€ä¸ªå…­ç»´å‘é‡ï¼ˆç©ºé—´ä½å§¿æä»£æ•°ï¼‰ï¼Œå› æ­¤$J$æ˜¯ä¸€ä¸ª2*6çš„çŸ©é˜µã€‚</p><script type="math/tex; mode=display">\frac{\partial e}{\partial \delta \xi} = \lim_{\delta \xi \rightarrow0} \frac{\partial e}{\partial P^{'}}\frac{\partial P^{'}}{\partial \delta \xi}</script><p>å…¶ä¸­$P^{â€˜}$æ˜¯ç‰¹å¾ç‚¹è½¬æ¢åˆ°ç›¸æœºåæ ‡ç³»ä¸‹çš„ç©ºé—´åæ ‡ï¼š</p><script type="math/tex; mode=display">su = KP^{'}\\u = f_x \frac{X^{'}}{Z^{'}} + c_x\\v = f_y \frac{X^{'}}{Z^{'}} + c_y</script><p>å› æ­¤è¯¯å·®é¡¹å¯¼æ•°çš„ç¬¬ä¸€é¡¹ä¸ºï¼š</p><script type="math/tex; mode=display">\frac{\partial e}{\partial P^{'}} = -\begin{bmatrix}\frac{\partial u}{\partial X^{'}} & \frac{\partial u}{\partial Y^{'}} & \frac{\partial u}{\partial Z^{'}}\\\frac{\partial v}{\partial X^{'}} & \frac{\partial v}{\partial Y^{'}} & \frac{\partial v}{\partial Z^{'}}\end{bmatrix}=-\begin{bmatrix}\frac{f_x}{Z^{'}} & 0 & -\frac{f_xX^{'}}{Z^{'2}}\\0 & \frac{f_y}{Z^{'}}  & -\frac{f_yY^{'}}{Z^{'2}}\\\end{bmatrix}</script><p>è¯¯å·®é¡¹çš„ç¬¬äºŒé¡¹ä¸ºå˜æ¢åçš„ç‚¹å…³äºæä»£æ•°çš„å¯¼æ•°ï¼Œå‚è€ƒ<a href="https://amberzzzz.github.io/2018/05/12/%E4%B8%89%E7%BB%B4%E5%88%9A%E4%BD%93%E8%BF%90%E5%8A%A8-%E6%9D%8E%E4%BB%A3%E6%95%B0/">æä»£æ•°</a>èŠ‚ï¼š</p><script type="math/tex; mode=display">\frac{\partial TP}{\partial \delta \xi} = \begin{bmatrix}I & -P^{'\wedge}\\0^T & 0^T\end{bmatrix}</script><p>å…¶ä¸­$P^{â€˜\wedge}$æ˜¯$P^{â€˜}$çš„åå¯¹ç§°é˜µï¼š</p><script type="math/tex; mode=display">P^{'\wedge} = \begin{bmatrix}0 & -Z^{'} & Y^{'}\\Z^{'} & 0 & -X^{'}\\-Y^{'} & X^{'} & 0\end{bmatrix}</script><p>å› æ­¤å¾—åˆ°å®Œæ•´çš„é›…å¯æ¯”çŸ©é˜µï¼š</p><script type="math/tex; mode=display">\frac{\partial e}{\partial \delta \xi} =\frac{\partial e}{\partial P^{'}}\begin{bmatrix}I & P^{'\wedge}\end{bmatrix}=-\begin{bmatrix}\frac{f_x}{Z^{'}} & 0 & -\frac{f_xX^{'}}{Z^{'2}} & |& -\frac{f_xX^{'}Y^{'}}{Z^{'2}} & f_x + \frac{f_xX^{'2}}{Z^{'2}} & -\frac{f_xY^{'}}{Z^{'}}\\0 & \frac{f_y}{Z^{'}}  & -\frac{f_yY^{'}}{Z^{'2}}& | & - f_y - \frac{f_xY^{'2}}{Z^{'2}}  & \frac{f_yX^{'}Y^{'}}{Z^{'2}} &  \frac{f_yX^{'}}{Z^{'}}\\\end{bmatrix}</script><p>é™¤äº†ä¼˜åŒ–ç›¸æœºä½å§¿ä»¥å¤–ï¼Œè¿˜å¯ä»¥åŒæ—¶ä¼˜åŒ–ç‰¹å¾ç‚¹çš„ç©ºé—´ä½ç½®$P$ï¼š</p><script type="math/tex; mode=display">\frac{\partial e}{\partial P} = \frac{\partial e}{\partial P^{'}} \frac{\partial P^{'}}{\partial P}</script><p>å…¶ä¸­çš„ç¬¬äºŒé¡¹ä¸ºï¼š</p><script type="math/tex; mode=display">P^{'} = exp(\xi^{\wedge})P = RP +t\\\therefore \frac{\partial P^{'}}{\partial P} = R^T</script><h3 id="3-ä¼˜åŒ–åº“ä½¿ç”¨"><a href="#3-ä¼˜åŒ–åº“ä½¿ç”¨" class="headerlink" title="3. ä¼˜åŒ–åº“ä½¿ç”¨"></a>3. ä¼˜åŒ–åº“ä½¿ç”¨</h3><p>æ„å»ºå›¾æ¨¡å‹ï¼š</p><ul><li>ä¼˜åŒ–å˜é‡1ï¼šèŠ‚ç‚¹1ï¼šqueryç›¸æœºä½å§¿ï¼ˆå…­ç»´å‘é‡æä»£æ•°ï¼‰</li><li>ä¼˜åŒ–å˜é‡2ï¼šèŠ‚ç‚¹2ï¼šç‰¹å¾ç‚¹ç©ºé—´ä½ç½®ï¼ˆä¸‰ç»´å‘é‡åæ ‡æè¿°ï¼‰</li><li>é¢„æµ‹å€¼ï¼šè¾¹nï¼šæ ¹æ®å½“å‰estimateçš„ä¼˜åŒ–é‡ï¼ŒæŠ•å½±åˆ°æŠ•å½±å¹³é¢çš„åƒç´ åæ ‡$z_i = h(\xi, P_i)$</li><li>è§‚æµ‹å€¼ï¼šèƒ½å¤Ÿç›´æ¥è¯»å‡ºçš„ï¼Œqueryå¸§ä¸Šå¯¹åº”ç‰¹å¾ç‚¹çš„å®é™…æŠ•å½±åæ ‡$u_i$</li></ul><p>g2oä¸­å·²ç»æä¾›äº†ç›¸è¿‘çš„åŸºç±»ï¼ˆåœ¨g2o/types/sba/types_six_dof_expmap.hä¸­ï¼‰ï¼šæä»£æ•°ä½å§¿èŠ‚ç‚¹VertexSE3Expmapã€ç©ºé—´ç‚¹ä½ç½®èŠ‚ç‚¹VertexSBAPointXYZã€æŠ•å½±æ–¹ç¨‹è¾¹EdgeProjectXYZ2UVã€‚</p><p>è¾¹ç±»é‡Œé¢è¦å…³æ³¨linearizeOpluså‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°æè¿°çš„æ˜¯éçº¿æ€§å‡½æ•°è¿›è¡Œçº¿æ€§åŒ–çš„è¿‡ç¨‹ä¸­ï¼Œæ±‚å¯¼çš„è§£æè§£ï¼ˆå½“ç„¶ä¹Ÿå¯ä»¥ä½¿ç”¨æ•°å€¼è§£ï¼‰ï¼Œæœ€åå‡½æ•°ç»™å‡ºçš„æ˜¯æ¯ä¸ªèŠ‚ç‚¹çš„å¯¼æ•°çŸ©é˜µï¼ˆ$\frac{\partial e}{\partial \delta \xi} $å’Œ$\frac{\partial e}{\partial P_i}$ï¼‰ ã€‚<strong>è¿™ç‚¹æ˜¯Ceresåº“å’Œg2oåº“çš„ä¸€ç‚¹ä¸»è¦å·®åˆ«ï¼šCereséƒ½æ˜¯ä½¿ç”¨è‡ªåŠ¨çš„æ•°å€¼å¯¼æ•°ï¼Œg2oè¦è‡ªå·±æ±‚å¯¼</strong>ã€‚</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> EdgeProjectXYZ2UV::linearizeOplus()</span><br><span class="line">&#123;</span><br><span class="line">    VertexSE3Expmap * vj = <span class="keyword">static_cast</span>&lt;VertexSE3Expmap*&gt;(_vertices[<span class="number">1</span>]);</span><br><span class="line">    VertexSBAPointXYZ* vi = <span class="keyword">static_cast</span>&lt;VertexSBAPointXYZ*&gt;(_vertices[<span class="number">0</span>]);</span><br><span class="line">    ...</span><br><span class="line">    ...</span><br><span class="line">    _jacobianOplusXi = <span class="number">-1.</span>/z * tmp * T.rotation().toRotationMatrix();</span><br><span class="line"> </span><br><span class="line">    _jacobianOplusXj(<span class="number">0</span>,<span class="number">0</span>) = x*y/z^<span class="number">2</span> * cam-&gt;focal_length;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-OpenCVå†…ç½®å‡½æ•°"><a href="#4-OpenCVå†…ç½®å‡½æ•°" class="headerlink" title="4. OpenCVå†…ç½®å‡½æ•°"></a>4. OpenCVå†…ç½®å‡½æ•°</h3><p><strong>basicï¼š</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> cv::solvePnP ( pts3d, pts2d, K, Mat(), r, t, <span class="literal">false</span> );</span><br><span class="line"></span><br><span class="line">Mat r, t, R;</span><br><span class="line">cv::solvePnP ( pts3d, pts2d, K, Mat(), r, t, <span class="literal">false</span> );</span><br><span class="line">cv::Rodrigues ( r, R );     <span class="comment">// æ—‹è½¬å‘é‡å’Œæ—‹è½¬çŸ©é˜µçš„è½¬æ¢</span></span><br></pre></td></tr></table></figure><p><strong>advancedï¼š</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bool</span> cv::solvePnPRansac( </span><br><span class="line">    InputArray objectPoints,      <span class="comment">// 3Dç©ºé—´åæ ‡ vector&lt;cv::Point3f&gt; pts3d</span></span><br><span class="line">    InputArray imagePoints,       <span class="comment">// 2Dåƒç´ åæ ‡ vector&lt;cv::Point2f&gt; pts2d</span></span><br><span class="line">    InputArray cameraMatrix,      <span class="comment">// ç›¸æœºå†…éƒ¨å‚æ•°çŸ©é˜µ K</span></span><br><span class="line">    InputArray distCoeffs,        <span class="comment">// ç•¸å˜ç³»æ•°å‘é‡ cv::Mat()</span></span><br><span class="line">    OutputArray rvec,             <span class="comment">// æ—‹è½¬å‘é‡</span></span><br><span class="line">    OutputArray tvec,             <span class="comment">// å¹³ç§»å‘é‡</span></span><br><span class="line">    <span class="keyword">bool</span> useExtrinsicGuess = <span class="literal">false</span>, <span class="comment">// If true, the function uses the provided rvec and tvec values as initial</span></span><br><span class="line">    <span class="keyword">int</span> iterationsCount = <span class="number">100</span>,    <span class="comment">// Number of iterations</span></span><br><span class="line">    <span class="keyword">float</span> reprojectionError = <span class="number">8.0</span>,<span class="comment">// é‡æŠ•å½±è¯¯å·®æœ€å¤§å€¼</span></span><br><span class="line">    <span class="keyword">double</span> confidence = <span class="number">0.99</span>,</span><br><span class="line">    OutputArray inliers = noArray(), <span class="comment">// Output vector that contains indices of inliers in objectPoints and imagePoints .</span></span><br><span class="line">    <span class="keyword">int</span> flags = SOLVEPNP_ITERATIVE   <span class="comment">// method for solving PnP</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>Ransacï¼šè€ƒè™‘åˆ°æˆ‘ä»¬æä¾›çš„åŒ¹é…é‡Œé¢å­˜åœ¨è¯¯åŒ¹é…çš„æƒ…å†µï¼ŒOpenCVé‡‡ç”¨â€œéšæœºé‡‡æ ·ä¸€è‡´æ€§ç®—æ³•â€ï¼ˆRandom Sample Consensusï¼‰ï¼Œä»ç°æœ‰åŒ¹é…ä¸­éšæœºå–ä¸€éƒ¨åˆ†ç”¨æ¥ä¼°è®¡è¿åŠ¨ï¼ˆPnPçš„è§£æè§£æ³•æœ€å°‘åªéœ€è¦ä¸‰ä¸ªç‚¹å°±èƒ½è®¡ç®—ç›¸å¯¹ä½å§¿ï¼‰ï¼Œæ­£ç¡®çš„åŒ¹é…ç»“æœéƒ½æ˜¯è¿‘ä¼¼çš„ï¼Œä»è€Œå‰”é™¤è¯¯åŒ¹é…ã€‚</li><li>inlierï¼šå†…ç‚¹ï¼Œå‡½æ•°æœ€ç»ˆç»™å‡ºçš„åŒ¹é…å¯ä¿¡çš„ç‚¹ã€‚</li><li>RANSACåªé‡‡ç”¨å°‘æ•°å‡ ä¸ªéšæœºç‚¹æ¥è®¡ç®—PnPï¼Œå®¹æ˜“å—åˆ°å™ªå£°å½±å“ã€‚å·¥ç¨‹ä¸Šé€šå¸¸ä½¿ç”¨RANSACçš„è§£ä½œä¸ºåˆå€¼ï¼Œå†ä½¿ç”¨éçº¿æ€§ä¼˜åŒ–æ–¹æ³•æ±‚è§£æœ€ä¼˜å€¼ã€‚</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Ransacç²—åŒ¹é…</span></span><br><span class="line">cv::solvePnPRansac( pts3d, pts2d, K, Mat(), rvec, tvec, <span class="literal">false</span>, <span class="number">100</span>, <span class="number">4.0</span>, <span class="number">0.99</span>, inliers );</span><br><span class="line">cv::Rodrigues ( rvec, R ); </span><br><span class="line">Eigen::Matrix3d rotation_matrix = R.at&lt;<span class="keyword">double</span>&gt;;</span><br><span class="line">T_c_r_estimated_ = SE3d(</span><br><span class="line">        SO3d(rotation_matrix),</span><br><span class="line">        Vector3d( tvec.at&lt;<span class="keyword">double</span>&gt;(<span class="number">0</span>,<span class="number">0</span>), tvec.at&lt;<span class="keyword">double</span>&gt;(<span class="number">1</span>,<span class="number">0</span>), tvec.at&lt;<span class="keyword">double</span>&gt;(<span class="number">2</span>,<span class="number">0</span>))</span><br><span class="line">    );</span><br><span class="line">    </span><br><span class="line"><span class="comment">// BAå±€éƒ¨ä¼˜åŒ–</span></span><br><span class="line">g2o::VertexSE3Expmap* pose <span class="keyword">new</span> g2o::VertexSE3Expmap();</span><br><span class="line">...</span><br><span class="line">pose-&gt;setEstimate(g2o::SE3Quat( T_c_r_estimated_.rotationMatrix(), T_c_r_estimated_.translation()));</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>karto key concepts</title>
      <link href="/2018/06/05/karto-key-concepts/"/>
      <url>/2018/06/05/karto-key-concepts/</url>
      <content type="html"><![CDATA[<p>åŸºäºæå…¶æ ‡å‡†çš„å›¾ä¼˜åŒ–SLAMæ¡†æ¶æ¥å®ç°ï¼š</p><p><img src="/2018/06/05/karto-key-concepts/1.png" alt="framework"></p><p>æå‡ºäº†é‡‡ç”¨ç¨€ç–ç‚¹è°ƒæ•´ï¼ˆSPAï¼‰çš„æ–¹æ³•æ¥é«˜æ•ˆæ±‚è§£ä½å§¿å›¾ä¼˜åŒ–é—®é¢˜ï¼Œé’ˆå¯¹æœ¬ç®—æ³•çš„æ–‡çŒ®ä¸ºã€ŠEfficient Sparse Pose Adjustment for 2D Mappingã€‹ã€‚</p><p>scan matchingéƒ¨åˆ†çš„å‚è€ƒæ–‡çŒ®ä¸ºã€ŠReal-time correlative scan matchingã€‹ï¼ŒM3RSMçš„å‰èº«ï¼</p><h4 id="key-conceptsï¼š"><a href="#key-conceptsï¼š" class="headerlink" title="key conceptsï¼š"></a>key conceptsï¼š</h4><ul><li><p>keyScanï¼šæœºå™¨äººè¿åŠ¨ä¸€å®šçš„è·ç¦»æˆ–è§’åº¦ï¼ˆå…³é”®å¸§ï¼‰ï¼Œå‚¨å­˜åœ¨sensorManagerï¼Œæ— åœ°å›¾ç¼“å­˜ã€‚</p></li><li><p>look-up tableæŸ¥æ‰¾è¡¨ï¼šæŸ¥æ‰¾è¡¨çš„æ„ä¹‰å°±æ˜¯ç›¸æ¯”äºæš´åŠ›åŒ¹é…ï¼Œä¸éœ€è¦æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—æ¯ä¸ªæ¿€å…‰æ•°æ®ä¿¡æ¯ï¼Œ<strong>ç›¸åŒè§’åº¦ä¸åŒä½ç½®</strong>çš„æ¿€å…‰æ•°æ®ä¿¡æ¯åªéœ€è¦è¢«ç´¢å¼•ä¸€æ¬¡ã€‚</p></li><li><p>responseå“åº”å€¼ï¼šå°†æŸ¥æ‰¾è¡¨ä»¥ä¸€å®šçš„ä½ç§»æŠ•åˆ°å­å›¾ä¸Šï¼Œæ€»å…±æœ‰nä¸ªç‚¹è¢«æŸ¥æ‰¾è¡¨å‡»ä¸­ï¼ˆhitï¼‰ï¼Œå‡»ä¸­çš„æ¯ä¸ªç‚¹å¾—åˆ†ä¸åŒï¼ˆscoreï¼‰ï¼Œç´¯åŠ å¾—åˆ†å¹¶é™¤ä»¥å¯ä»¥è¾¾åˆ°çš„æœ€é«˜åˆ†ã€‚</p><script type="math/tex; mode=display">response = \frac{\sum_{i=0}^n goal_i}{goalmax}</script></li><li><p>åæ–¹å·®ï¼šæ–‡çŒ®ä¸“é—¨ç”¨äº†ä¸€èŠ‚è®¡ç®—åæ–¹å·®ï¼Œä½†æ˜¯æ²¡çœ‹åˆ°ç”¨åœ¨å“ªï¼Œæ˜¯ä¸ºäº†åé¢æ±‚è¯¯å·®åšå‡†å¤‡å—ï¼Ÿï¼Ÿï¼Ÿ</p></li><li><p>addScansæ·»åŠ é¡¶ç‚¹å’Œè¾¹ï¼šè¾¹æ˜¯è¯¯å·®å€¼ï¼Œæ·»åŠ çš„è¾¹çº¦æŸæ¥è‡ªä¸¤éƒ¨åˆ†ï¼Œ</p><p>ï¼ˆ1ï¼‰link to running scansï¼Œè·å½“å‰å¸§ä¸€å®šèŒƒå›´å†…çš„æ¿€å…‰æ•°æ®é“¾ï¼ˆRunningScan chainï¼‰ã€‚</p><p>ï¼ˆ2ï¼‰link to other near chainsï¼Œä»å½“å‰èŠ‚ç‚¹å¼€å§‹å¹¿åº¦ä¼˜å…ˆéå†ä¸€å®šè·ç¦»èŒƒå›´å†…æ‰€æœ‰èŠ‚ç‚¹ï¼Œä¾æ®å½“å‰idä»sensorManagerä¸­åˆ†åˆ«é€’å¢å’Œé€’å‡å¯»æ‰¾ä¸€å®šèŒƒå›´å†…çš„chainï¼ˆä¸ä¸€å®šç›´æ¥ç›¸è¿ï¼‰ã€‚</p></li><li><p>å›ç¯æ£€æµ‹ï¼šæ“ä½œä¸æ·»åŠ è¾¹çº¦æŸç±»ä¼¼ï¼Œä½å§¿å›¾ä¸Šè¦<strong>å»é™¤é‚£äº›å’Œå½“å‰èŠ‚ç‚¹çš„æ—¶é—´ç›¸é‚»çš„èŠ‚ç‚¹</strong>ã€‚</p><p>ï¼ˆ1ï¼‰æ‰¾åˆ°ä¸€å®šè·ç¦»èŒƒå›´å†…ï¼ˆnearï¼‰å’Œç›¸è¿ï¼ˆadjacentï¼‰çš„èŠ‚ç‚¹æ·»åŠ è¿›nearLinkedScansã€‚</p><p>ï¼ˆ2ï¼‰<code>MapperGraph::FindPossibleLoopClosure</code>ï¼šä»sensorManagerä¸­ä»å‰åˆ°åï¼Œä¾æ®åºå·æŒ‘é€‰ä¸å½“å‰èŠ‚ç‚¹åœ¨ä¸€å®šè·ç¦»èŒƒå›´å†…ï¼Œä¸”ä¸åœ¨nearLinkedScansä¸­çš„candidateã€‚è¿”å›æ½œåœ¨chainã€‚å…¶ä¸­æ¶‰åŠä¸¤ä¸ªå‚æ•°ï¼š</p><ul><li>LoopSearchMaximumDistanceï¼šcandidateScanä¸å½“å‰scançš„è·ç¦»å¿…é¡»åœ¨å¯å®¹è®¸çš„è·ç¦»å†…ã€‚</li><li>LoopMatchMinimumChainSizeï¼šchainä¸­çš„èŠ‚ç‚¹æ•°å¿…é¡»ä¸å°äºé™å®šå€¼ã€‚</li></ul><p>ï¼ˆ3ï¼‰<code>MapperGraph::TryCloseLoop</code>ï¼šscan2mapåŒ¹é…ï¼Œå½“responseå’Œcovarianceè¾¾åˆ°ä¸€å®šè¦æ±‚è®¤ä¸ºé—­ç¯æ£€æµ‹åˆ°ï¼Œå¾—åˆ°correct poseï¼ˆä¹Ÿå°±æ˜¯è®¤ä¸ºcandidateScançš„poseæ‰æ˜¯å½“å‰å¸§çš„å®é™…poseï¼‰ã€‚</p><p>ï¼ˆ4ï¼‰add link to loopï¼Œæ„æˆå…¨å±€é—­ç¯ã€‚</p><p>ï¼ˆ5ï¼‰è§¦å‘correctPoseï¼Œè¿›è¡Œspaä¼˜åŒ–ã€‚</p></li></ul><p><img src="/2018/06/05/karto-key-concepts/graph opti.jpeg" alt=""></p><h4 id="ä»£ç éšæ‰‹è®°ï¼š"><a href="#ä»£ç éšæ‰‹è®°ï¼š" class="headerlink" title="ä»£ç éšæ‰‹è®°ï¼š"></a>ä»£ç éšæ‰‹è®°ï¼š</h4><ol><li><p>ROSä¸Šé¢æä¾›ä¸‰ä¸ªå¼€æºåŒ…ï¼šnav2d_karto, open_karto, slam_kartoã€‚</p><p>ROS Wikiä¸Šè¿™ä¹ˆæè¿°nav2d_kartoè¿™ä¸ªpackageï¼šGraph-based Simultaneous Localization and Mapping module. Includes OpenKarto GraphSLAM library by â€œSRI Internationalâ€.</p><p>open_kartoï¼šå¼€æºçš„kartoåŒ…ï¼Œå®ç°åº•å±‚çš„kartoslam</p><p>slam_kartoï¼šroså±‚ï¼Œåº”ç”¨å±‚çš„kartoslamæ¥å£</p></li><li><p>The LaserRangeFinder contains parameters for physical laser sensor used by the mapper for scan matching Also contains information about the maximum range of the sensor and provides a threshold for limiting the range of readings.</p><p>The optimal value for the range threshold depends on the angular resolution of the scan and the desired map resolution.</p></li><li><p>resolutionï¼š0.25 &amp; 0.5 &amp; 1 degree</p><p>number of range readings (beams)ï¼šï¼ˆmaximumAngle - minimumAngleï¼‰ï¼angularResolution + 1</p></li><li><p>GridStatesï¼š0 for Unknownï¼Œ100 for Occupiedï¼Œ 255 for Freeã€‚</p></li><li><p>flipYï¼šæœ€å¼€å§‹æœºå™¨äººåº”è¯¥å¤„åœ¨ä¸–ç•Œåæ ‡ç³»çš„åŸç‚¹ï¼Œä¼ æ„Ÿå™¨åæ ‡ç³»ä¸æœºå™¨äººbaselinkå­˜åœ¨ä¸€ä¸ªåæ ‡å˜æ¢ï¼ŒåŸå§‹çš„ä¼ æ„Ÿå™¨åæ ‡ç³»ä½ç½®åº”è¯¥ä¸åœ°å›¾åæ ‡ç³»é‡åˆï¼Œè¿™å°±æ˜¯worldå’Œgridä¹‹é—´çš„offsetã€‚flipæ˜¯å•¥å‘¢ï¼Ÿï¼Ÿ</p></li><li><p>LookupArray[index]ï¼šCreate lookup tables for point readings <strong>at varying angles</strong> in grid. This is to speed up finding best angle/position for a localized range scan</p></li><li><p>MapperGraphï¼šèŠ±å¼æ„é€ ä½å§¿å›¾</p></li><li><p>CorrelationGridï¼šImplementation of a <strong>correlation</strong> grid used for scan matching</p></li><li><p>Region of Interest ROIï¼š</p></li><li><p>smearï¼šThe point readings are smeared by this value in X and Y to create a smoother response. ä¸ªäººç†è§£è¿™å¥è¯æ˜¯è¯´ç‚¹å®¹æ˜“ç”Ÿæˆçªå˜ï¼Œç”¨ä»¥ç‚¹ä¸ºä¸­å¿ƒçš„ä¸€å°ç‰‡åŒºåŸŸå¹³æ»‘ä¸€ç‚¹ã€‚</p></li><li><p>ScanMatchï¼šè¿”å›å“åº”å€¼response</p><p>   å‰ç«¯åŒ¹é…è°ƒç”¨<code>m_pSequentialScanMatcher-&gt;MatchScan</code></p><p>   é—­ç¯æ£€æµ‹è°ƒç”¨<code>m_pLoopScanMatcher-&gt;MatchScan</code></p><p>   ä¸¤ä¸ªå‡½æ•°ç»§æ‰¿äº<code>ScanMatcher::MatchScan</code>ï¼š</p>   <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">kt_double ScanMatcher::MatchScan(</span><br><span class="line">LocalizedRangeScan* pScan, </span><br><span class="line">    <span class="keyword">const</span> LocalizedRangeScanVector&amp; rBaseScans, </span><br><span class="line">    Pose2&amp; rMean, </span><br><span class="line">    Matrix3&amp; rCovariance, </span><br><span class="line">    kt_bool doPenalize, </span><br><span class="line">    kt_bool doRefineMatch)    <span class="comment">// default is True, å†³å®šæ˜¯å¦åšç²¾åŒ¹é…</span></span><br><span class="line"><span class="comment">// @return: strength of response (best response)</span></span><br></pre></td></tr></table></figure><p>   å…¶ä¸­ä¼šè°ƒç”¨<code>ScanMatcher::CorrelateScan</code>æ–¹æ³•ã€‚<code>ScanMatcher::CorrelateScan</code>æ–¹æ³•ä¸­è°ƒç”¨<code>ScanMatcher::GetResponse</code>æ–¹æ³•è®¡ç®—å“åº”å€¼ã€‚</p>   <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kt_double ScanMatcher::GetResponse(</span><br><span class="line">kt_int32u angleIndex, </span><br><span class="line">kt_int32s gridPositionIndex) <span class="keyword">const</span></span><br></pre></td></tr></table></figure><p>   <code>GetResponse</code>çš„æ ¸å¿ƒåœ¨<code>kt_int8u* pByte</code>å’Œ<code>const LookupArray* pOffsets</code>ä¸¤ä¸ªæ•°æ®ç»“æ„ï¼š</p><ul><li>å‰è€…æ˜¯åœ¨correlationGridèŒƒå›´å†…çš„real sensedå æ®æƒ…å†µã€‚</li><li><p>åè€…æ˜¯lookup-tableä¸­ï¼ˆå·²çŸ¥åœ°å›¾ï¼‰è¯»å–çš„æ …æ ¼å æ®æƒ…å†µï¼ŒåªåŒ…å«å æ®çš„æ …æ ¼ï¼Œkeyæ˜¯angularã€‚</p><p>è®¡ç®—responseåªè¦çœ‹åœ°å›¾ä¸Šçš„å æ®ç‚¹æ˜¯å¦åœ¨è§‚æµ‹ä¸­æ˜¯å¦ä¹Ÿæ˜¯å æ®çš„ï¼š</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (kt_int32u i = <span class="number">0</span>; i &lt; pOffsets-&gt;GetSize(); i++)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">// ignore points that fall off the grid</span></span><br><span class="line">  kt_int32s pointGridIndex = gridPositionIndex + pAngleIndexPointer[i];</span><br><span class="line">  <span class="keyword">if</span> (!math::IsUpTo(pointGridIndex, m_pCorrelationGrid-&gt;GetDataSize()) || pAngleIndexPointer[i] == INVALID_SCAN)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">continue</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// uses index offsets to efficiently find location of point in the grid</span></span><br><span class="line">  response += pByte[pAngleIndexPointer[i]];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>æœ€ç»ˆçš„responseè¦normalizeï¼š</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// normalize response</span></span><br><span class="line">response /= (nPoints * GridStates_Occupied);   <span class="comment">// GridStates_Occupied = 100,</span></span><br><span class="line">assert(<span class="built_in">fabs</span>(response) &lt;= <span class="number">1.0</span>);</span><br></pre></td></tr></table></figure></li></ul></li><li><p>kartoåªåœ¨é—­ç¯çš„æ—¶å€™è§¦å‘åç«¯ä¼˜åŒ–<code>CorrectPoses()</code>ï¼ŒScanSolverçš„å®ç°åœ¨Samplesè·¯å¾„ä¸‹çš„SpaSolverï¼Œè°ƒç”¨äº†ç°æœ‰çš„BAæ±‚è§£å™¨<a href="http://users.ics.forth.gr/~lourakis/sba/" target="_blank" rel="noopener">sba(A Generic Sparse Bundle Adjustment C/C++ Package Based on the Levenberg-Marquardt Algorithm)</a>ã€‚</p></li></ol><h4 id="å‚æ•°-amp-ä¼˜åŒ–æ–¹å‘"><a href="#å‚æ•°-amp-ä¼˜åŒ–æ–¹å‘" class="headerlink" title="å‚æ•°&amp;ä¼˜åŒ–æ–¹å‘"></a><a href="http://docs.ros.org/jade/api/nav2d_karto/html/classkarto_1_1OpenMapper.html" target="_blank" rel="noopener">å‚æ•°</a>&amp;ä¼˜åŒ–æ–¹å‘</h4><ol><li>é—­ç¯ä¸­candidateæ•°é‡çš„è°ƒæ•´ï¼š<ul><li>å‡å°LoopSearchMaximumDistanceï¼Œè¿›å…¥candidateèŒƒå›´çš„èŠ‚ç‚¹æ•°æ®å‡å°‘</li><li>å‡å°LoopMatchMinimumChainSizeï¼Œç”¨æ¥è®¡ç®—ä¼˜åŒ–çš„candidateæ•°é‡å‡å°‘</li><li>å¢å¤§minimum_travel_distanceå’Œminimum_travel_headingï¼Œè¿™æ ·æ€»ä½“çš„èŠ‚ç‚¹æ•°å‡å°‘</li></ul></li><li>Map_update_intervalï¼šå‘å¸ƒåœ°å›¾çš„é—´éš”ï¼Œå…¶ä¸»è¦è¿‡ç¨‹æ˜¯éå†å½“å‰æ‰€æœ‰èŠ‚ç‚¹æ•°æ®ï¼Œå¯¹æ¯ä¸ªæ …æ ¼çš„å æœ‰çŠ¶æ€è¿›è¡Œåˆ¤å®šï¼Œç”Ÿæˆæ …æ ¼åœ°å›¾ã€‚</li><li>ScanBufferSizeå’ŒScanBufferMaximumScanDistanceï¼šæ§åˆ¶bufferä¹Ÿå°±æ˜¯chainçš„å¤§å°ã€‚chainä¸èƒ½å¤ªå¤§ä¹Ÿä¸èƒ½å¤ªå°ï¼Œå¤ªå°ä¼šé€ æˆå‰ç«¯è¯¯å·®ç´¯ç§¯ï¼Œå¤ªå¤§ä¼šå¯¼è‡´æ„å»ºé—­ç¯çš„èŠ‚ç‚¹æ•°ä¸è¶³ã€‚æ¨èå€¼æ˜¯ScanBufferMaximumScanDistanceï¼minimum_travel_distanceã€‚</li><li>ä½å§¿çº æ­£ä¸­ï¼š<ul><li>CorrelationSearchSpaceDimensionï¼šThe size of the <strong>search grid</strong></li><li>CorrelationSearchSpaceResolutionï¼šThe size of the <strong>correlation grid</strong></li></ul></li><li>å›ç¯æ£€æµ‹ä¸­ï¼š<ul><li>LoopSearchMaximumDistanceï¼šé—­ç¯æ£€æµ‹çš„æœç´¢è·ç¦»ï¼Œæ•°å€¼è¶Šå¤§èƒ½è¶Šæ—©å‘ç°é—­ç¯ï¼Œä¹Ÿèƒ½å®¹å¿æ›´å¤§çš„åç¦»è¯¯å·®ã€‚</li><li>LoopMatchMinimumResponseCoarseå’ŒLoopMatchMinimumResponseFineï¼šç²—åŒ¹é…å’Œç²¾åŒ¹é…çš„å“åº”é˜ˆå€¼ï¼Œä¸é—­ç¯ä¸­candidateæ•°é‡ç›¸å…³ã€‚é˜ˆå€¼è¿‡ä½ä¼šå¯¼è‡´candidateè¿…é€Ÿè¢«å¡«æ»¡ï¼ŒçœŸæ­£å¥½çš„ç‚¹è¿˜æ²¡æ‰¾åˆ°ã€‚é˜ˆå€¼è¿‡é«˜ä¼šå¯¼è‡´å›ç¯å¤±è´¥ï¼ˆä¸€ç›´æ‰¾ä¸åˆ°å›ç¯ç‚¹ï¼‰ï¼Œåœ°å›¾ä¸Šå‡ºç°é‡å½±ã€‚</li></ul></li></ol><h4 id="CPU-Usage"><a href="#CPU-Usage" class="headerlink" title="CPU Usage"></a>CPU Usage</h4><p>ç®—æ³•èµ„æºå ç”¨çš„ä¸»è¦å‹åŠ›æ¥æºï¼š</p><ol><li>åœ°å›¾æ›´æ–°</li><li>å›ç¯æ£€æµ‹</li><li>SPAä¼˜åŒ–</li></ol>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>leetcode79 å•è¯æœç´¢</title>
      <link href="/2018/06/04/leetcode79-%E5%8D%95%E8%AF%8D%E6%90%9C%E7%B4%A2/"/>
      <url>/2018/06/04/leetcode79-%E5%8D%95%E8%AF%8D%E6%90%9C%E7%B4%A2/</url>
      <content type="html"><![CDATA[<p>æŒ‚ä¸€é“å¾ˆçŒ¥ççš„é¢˜ï¼ŒäºŒç»´ç½‘æ ¼ä¸­æœç´¢å•è¯ï¼ŒåŒä¸€å•å…ƒæ ¼ä¸èƒ½é‡å¤ä½¿ç”¨ï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">board =</span><br><span class="line">[</span><br><span class="line">  [&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;E&apos;],</span><br><span class="line">  [&apos;S&apos;,&apos;F&apos;,&apos;C&apos;,&apos;S&apos;],</span><br><span class="line">  [&apos;A&apos;,&apos;D&apos;,&apos;E&apos;,&apos;E&apos;]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">ç»™å®š word = &quot;ABCCED&quot;, è¿”å› true.</span><br><span class="line">ç»™å®š word = &quot;SEE&quot;, è¿”å› true.</span><br><span class="line">ç»™å®š word = &quot;ABCB&quot;, è¿”å› false.</span><br></pre></td></tr></table></figure><p>æ²¡å•¥å¥½ç®—æ³•ï¼Œå°±æ˜¯DFSï¼Œä½†æ˜¯å‘åœ¨äºvisitedçš„å­˜å‚¨ï¼Œpythonæ•°ç»„é»˜è®¤æµ…æ‹·è´ï¼Œé€’å½’ä¼ è¿›å»å†å›åˆ°ä¸Šä¸€å±‚ç½‘æ ¼çŠ¶æ€å°±å˜äº†ï¼Œä¹‹å‰ä¸€è´¯çš„åšæ³•å°±æ˜¯æ–°å¼€ä¸€å—å†…å­˜ç©ºé—´ï¼Œä¼ æ–°çš„æ•°ç»„è¿›å»ï¼Œç„¶è€Œè¿™æ¬¡è¶…æ—¶äº†ï¼Œå› ä¸ºæµ‹è¯•ç”¨ä¾‹çš„äºŒç»´æ•°ç»„å°ºå¯¸è´¼å¤§ï¼Œç»ˆäºæœ‰æœºä¼šæ­£è§†è¿™ä¸ªé—®é¢˜ï¼Œå¹¶è·å–æ­£ç¡®çš„æ‰“å¼€æ–¹å¼ï¼š</p><ul><li><p>å°ºå¯¸è´¼å¤§çš„äºŒç»´æ•°ç»„ï¼Œæ¯æ¬¡åªéœ€è¦ä¿®æ”¹ä¸€ä¸ªå€¼ï¼Œé‡æ–°åˆ’ç©ºé—´æ‹·è´å†ä¿®æ”¹æ—¶é—´å¤æ‚åº¦ç¬é—´å¢å¤§O(m*n)å€ï¼Œå¾ˆæ˜æ˜¾ä¼ åŸæ¥çš„æ•°ç»„è¿›å»æ¯”è¾ƒåˆé€‚ã€‚</p></li><li><p>ä½†æ˜¯æ·±å±‚é€’å½’ä¼šä¿®æ”¹ä¼ è¿›å»çš„å‚æ•°ï¼Œå› æ­¤<strong>åœ¨æ¯æ¬¡é€’å½’ä¹‹å‰å…ˆåˆ›å»ºä¸€ä¸ªtmpï¼Œè®°å½•ä¿®æ”¹è¡Œä¸º</strong>ï¼Œé€’å½’å‡½æ•°è¿›è¡Œå®Œä»¥åï¼Œå†æ ¹æ®è®°å½•æ¢å¤åŸæ¥çš„å‚æ•°ï¼Œä¿è¯æœ¬å±‚å‚æ•°ä¸å˜ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search_tail</span><span class="params">(board, word, h, w)</span>:</span></span><br><span class="line">    size = len(word)</span><br><span class="line">    char = word[<span class="number">0</span>]</span><br><span class="line">    height, width = len(board), len(board[<span class="number">0</span>])</span><br><span class="line">    exist = <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">if</span> h - <span class="number">1</span> &gt;= <span class="number">0</span> <span class="keyword">and</span> board[h<span class="number">-1</span>][w] == char:</span><br><span class="line">        <span class="keyword">if</span> size == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tmp = board[h<span class="number">-1</span>][w]</span><br><span class="line">            board[h<span class="number">-1</span>][w] = <span class="string">'INF'</span></span><br><span class="line">            exist = search_tail(board, word[<span class="number">1</span>:], h<span class="number">-1</span>, w)</span><br><span class="line">            <span class="keyword">if</span> exist:</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">            board[h<span class="number">-1</span>][w] = tmp</span><br><span class="line">    <span class="keyword">if</span> h + <span class="number">1</span> &lt; height <span class="keyword">and</span> board[h+<span class="number">1</span>][w] == char:</span><br><span class="line">        ...</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure></li><li><p>ç„¶åé’ˆå¯¹æœ¬é¢˜è¿˜æœ‰ä¸€ä¸ªéªšæ“ä½œï¼Œå¾ˆå¤šäººä¸“é—¨åˆ›å»ºä¸€ä¸ªvisitedè¡¨æ¥è®°å½•æœç´¢è·¯å¾„ï¼Œä½†æ˜¯å› ä¸ºæœ¬é¢˜çš„äºŒç»´æ•°ç»„é™å®šå­˜å‚¨å­—æ¯ï¼Œæ‰€ä»¥ä»»æ„ä¸€ä¸ªéå­—æ¯éƒ½å¯ä»¥ä½œä¸ºæ ‡å¿—ä½ï¼Œç¾æ»‹æ»‹åˆçœä¸‹ä¸€ä¸ªO(m*n)ã€‚</p><p><img src="/2018/06/04/leetcode79-å•è¯æœç´¢/188ms.png" alt="188ms"></p></li></ul>]]></content>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>kd-tree</title>
      <link href="/2018/06/01/kd-tree/"/>
      <url>/2018/06/01/kd-tree/</url>
      <content type="html"><![CDATA[<p><em>Referenceï¼šBentley J L. Multidimensional Binary Search Trees Used for Associative Searching[J]. Communications of the Acm, 1975, 18(9):509-517.</em></p><p>å‰é¢æ›´æ–°<a href="https://amberzzzz.github.io/2018/05/05/ICP-Iterative-Closest-Points/">basic ICP</a>çš„æ—¶å€™ç•™äº†ä¸€ä¸ªå‘â€”â€”æœ€è¿‘é‚»çš„æ±‚æ³•ã€‚çº¿æ€§æ‰«æï¼Ÿæ‰‹åŠ¨æŒ¥æ‰‹ã€‚</p><h4 id="1-å…ˆè¯´è¯´è·ç¦»å§"><a href="#1-å…ˆè¯´è¯´è·ç¦»å§" class="headerlink" title="1. å…ˆè¯´è¯´è·ç¦»å§"></a>1. å…ˆè¯´è¯´è·ç¦»å§</h4><p><strong>1.1 æ¬§å¼è·ç¦»</strong></p><script type="math/tex; mode=display">d(x,y) = \sqrt{(x_1 - y_1)^2 + (x_2-y_2)^2 + ... + (x_n-y_n)^2} = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}</script><p><strong>1.2 æ ‡å‡†åŒ–æ¬§å¼è·ç¦»</strong></p><p>é¦–å…ˆå°†æ•°æ®å„ç»´åº¦åˆ†é‡éƒ½æ ‡å‡†åŒ–åˆ°å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼Œå†æ±‚æ¬§å¼è·ç¦»ï¼š</p><script type="math/tex; mode=display">X_{stand} = \frac{X - \mu}{\sigma}</script><p>ç®€å•æ¨å¯¼åå¯ä»¥å‘ç°ï¼Œæ ‡å‡†åŒ–æ¬§å¼è·ç¦»å®é™…ä¸Šå°±æ˜¯ä¸€ç§åŠ æƒæ¬§å¼è·ç¦»ï¼š</p><script type="math/tex; mode=display">d(x,y) = \sqrt{\sum_{i=1}^n (\frac{x_i-y_i}{s_i})^2}</script><p><strong>1.3 é©¬æ°è·ç¦»ï¼ˆMahalanobis Distanceï¼‰</strong></p><script type="math/tex; mode=display">D(X_i,X_j) = \sqrt{(X_i - X_j)S^{-1}(X_i-X_j)}</script><p>å…¶ä¸­$S$ä¸ºåæ–¹å·®çŸ©é˜µ$Cov$ï¼š</p><script type="math/tex; mode=display">Cov(X,Y) = E\{[X-E(X)][Y-E(Y)]\}</script><p><strong>è‹¥åæ–¹å·®çŸ©é˜µæ˜¯å•ä½é˜µï¼ˆæ ·æœ¬å„ç»´åº¦ä¹‹é—´ç‹¬ç«‹åŒåˆ†å¸ƒï¼‰</strong>ï¼Œé‚£å…¬å¼å°±å˜æˆæ¬§å¼è·ç¦»äº†ï¼Œè‹¥åæ–¹å·®çŸ©é˜µæ˜¯å¯¹è§’é˜µï¼Œé‚£å…¬å¼å°±å˜æˆæ ‡å‡†åŒ–æ¬§å¼è·ç¦»äº†ã€‚</p><p><strong>1.4 ç›¸ä¼¼åº¦</strong></p><p>ç›¸ä¼¼åº¦ä¹Ÿæ˜¯è·ç¦»çš„ä¸€ç§è¡¨å¾æ–¹å¼ï¼Œè·ç¦»è¶Šç›¸è¿‘ï¼Œç›¸ä¼¼åº¦è¶Šé«˜ã€‚</p><ul><li>æ¬§å¼è·ç¦»ç›¸ä¼¼åº¦ï¼šå°†æ¬§å¼è·ç¦»é™å®šåœ¨0 1ä¹‹é—´å˜åŒ–</li></ul><script type="math/tex; mode=display">ç›¸ä¼¼åº¦ = \frac{1}{1+æ¬§å¼è·ç¦»}</script><ul><li>ä½™å¼¦ç›¸ä¼¼åº¦ï¼š-1åˆ°1ä¹‹é—´å˜åŒ–</li></ul><script type="math/tex; mode=display">cos\theta = \frac{A\cdot B}{||A||\ ||B||}</script><ul><li>çš®å°”é€Šç›¸å…³ç³»æ•°ï¼š-1åˆ°1ä¹‹é—´å˜åŒ–</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> numpy.corrcoef(A, B)</span><br></pre></td></tr></table></figure><h4 id="2-KDæ ‘"><a href="#2-KDæ ‘" class="headerlink" title="2. KDæ ‘"></a>2. KDæ ‘</h4><ul><li>k-NNç®—æ³•</li><li>æ¨èç³»ç»Ÿ</li><li>SIFTç‰¹å¾åŒ¹é…</li><li>ICPè¿­ä»£æœ€è¿‘ç‚¹</li><li>æœ€è¿‘åœ¨åšçš„M3RCMä¸­çš„å †ç»“æ„ï¼ˆè¿™ä¸ªæœ‰ç‚¹ç‰µå¼ºï¼Œå› ä¸ºä¸æ˜¯äºŒå‰æ ‘ï¼‰</li></ul><p>æ€»ä¹‹ä»¥ä¸Šè¿™äº›åŸºäºåŒ¹é…ï¼æ¯”è¾ƒçš„ç›®çš„è€Œè¿›è¡Œçš„æ•°æ®åº“æŸ¥æ‰¾ï¼å›¾åƒæ£€ç´¢ï¼Œæœ¬è´¨ä¸Šéƒ½å¯ä»¥å½’ç»“ä¸ºé€šè¿‡è·ç¦»å‡½æ•°åœ¨é«˜ç»´çŸ¢é‡ä¹‹é—´çš„ç›¸ä¼¼æ€§æ£€ç´¢é—®é¢˜ã€‚</p><p>ä¸€ç»´å‘é‡æœ‰äºŒåˆ†æ³•æŸ¥æ‰¾ï¼Œå¯¹åº”åœ°é«˜ç»´ç©ºé—´æœ‰æ ‘å½¢ç»“æ„ä¾¿äºå¿«é€Ÿæ£€ç´¢ã€‚<strong>åˆ©ç”¨æ ‘å½¢ç»“æ„å¯ä»¥çœå»å¯¹å¤§éƒ¨åˆ†æ•°æ®ç‚¹çš„æœç´¢ï¼Œä»è€Œå‡å°‘æ£€ç´¢çš„è®¡ç®—é‡</strong>ã€‚ </p><p>KDæ ‘æ˜¯ä¸€ç§<strong>äºŒå‰æ ‘</strong>ï¼Œé€šè¿‡ä¸æ–­åœ°ç”¨å‚ç›´äºæŸåæ ‡è½´çš„è¶…å¹³é¢å°†kç»´ç©ºé—´åˆ‡åˆ†æ„é€ è€Œæˆã€‚ </p><p><img src="/2018/06/01/kd-tree/2d.png" alt="2d"></p><p><img src="/2018/06/01/kd-tree/3d.png" alt="3d"></p><p><strong>2.1 æ„é€ æ ‘</strong></p><p>é€’å½’åˆ›å»ºèŠ‚ç‚¹ï¼šèŠ‚ç‚¹ä¿¡æ¯åŒ…å«åˆ‡åˆ†åæ ‡è½´å’Œåˆ‡åˆ†ç‚¹ï¼Œä»è€Œç¡®å®šè¶…å¹³é¢ï¼Œå°†å½“å‰ç©ºé—´åˆ‡åˆ†ä¸ºå·¦å³ä¸¤ä¸ªå­ç©ºé—´ï¼Œé€’å½’ç›´åˆ°å½“å‰å­ç©ºé—´å†…æ²¡æœ‰å®ä¾‹ä¸ºæ­¢ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, point, axis)</span>:</span></span><br><span class="line">self.value = point</span><br><span class="line">self.axis = axis</span><br><span class="line">self.left = <span class="keyword">None</span></span><br><span class="line">self.right = <span class="keyword">None</span></span><br></pre></td></tr></table></figure><p>ä¸ºäº†ä½¿å¾—æ„é€ å‡ºçš„KDæ ‘å°½å¯èƒ½å¹³è¡¡ï¼ˆé«˜æ•ˆåˆ†å‰²ç©ºé—´ï¼‰ï¼š</p><ul><li>é€‰æ‹©åæ ‡è½´ï¼šç®€å•ç‚¹çš„æ–¹å¼æ˜¯å¾ªç¯äº¤æ›¿é€‰æ‹©åæ ‡è½´ï¼Œå¤æ‚ç‚¹çš„åšæ³•æ˜¯é€‰æ‹©å½“å‰æ–¹å·®æœ€å¤§çš„è½´ä½œä¸ºåˆ‡åˆ†è½´ã€‚</li><li>é€‰æ‹©åˆ‡åˆ†ç‚¹ï¼šå–é€‰å®šåæ ‡è½´ä¸Šæ•°æ®çš„ä¸­å€¼ä½œä¸ºåˆ‡åˆ†ç‚¹ã€‚</li></ul><p>æ³¨æ„ï¼šKDæ ‘çš„æ„é€ æ—¨åœ¨é«˜æ•ˆåˆ†å‰²ç©ºé—´ï¼Œ<strong>å…¶å¶å­èŠ‚ç‚¹å¹¶éæ˜¯æœ€è¿‘é‚»æœç´¢ç­‰åº”ç”¨åœºæ™¯çš„æœ€ä¼˜è§£</strong>ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kdTree</span><span class="params">(points, depth)</span>:</span></span><br><span class="line"><span class="keyword">if</span> len(points) == <span class="number">0</span>:</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">axis = depth % len(points[<span class="number">0</span>])</span><br><span class="line">points.sort(key=<span class="keyword">lambda</span> x: x[axis])</span><br><span class="line">cut_idx = centreValue(points)</span><br><span class="line">node = Node(points[cut_idx], axis)</span><br><span class="line">node.left = kdTree(points[:cut_idx], depth+<span class="number">1</span>)</span><br><span class="line">node.right = kdTree(points[cut_idx+<span class="number">1</span>:], depth+<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> node</span><br></pre></td></tr></table></figure><p>å¯¹äºåŒ…å«nä¸ªå®ä¾‹çš„kç»´æ•°æ®æ¥è¯´ï¼Œæ„é€ KDæ ‘çš„æ—¶é—´å¤æ‚åº¦ä¸ºO(k*n*log n)ã€‚</p><p><strong>2.2 æ–°å¢èŠ‚ç‚¹</strong></p><p>é€’å½’å®ç°ï¼šä»æ ¹èŠ‚ç‚¹å¼€å§‹åšæ¯”è¾ƒï¼Œå¤§äºåˆ™æ’å…¥å·¦å­æ ‘ï¼Œå°äºåˆ™æ’å…¥å³å­æ ‘ã€‚ç›´åˆ°è¾¾åˆ°å¶å­èŠ‚ç‚¹ï¼Œå¹¶åˆ›å»ºæ–°çš„å¶å­èŠ‚ç‚¹ã€‚</p><p><strong>2.3 åˆ é™¤èŠ‚ç‚¹</strong></p><p>å°†å¾…åˆ é™¤çš„èŠ‚ç‚¹çš„æ‰€æœ‰å­èŠ‚ç‚¹ç»„æˆä¸€ä¸ªé›†åˆï¼Œé‡æ–°æ„å»ºKDå­æ ‘ï¼Œæ›¿æ¢å¾…åˆ é™¤èŠ‚ç‚¹ã€‚</p><p><strong>2.4 æœ€è¿‘é‚»æœç´¢</strong></p><p>æœç´¢æœ€è¿‘é‚»ç®—æ³•ä¸»è¦åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šé¦–å…ˆæ˜¯æ·±åº¦ä¼˜å…ˆéå†ï¼Œç›´åˆ°é‡åˆ°å¶å­èŠ‚ç‚¹ï¼Œç”Ÿæˆæœç´¢è·¯å¾„ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">searchNearest</span><span class="params">(node, target)</span>:</span></span><br><span class="line">    <span class="comment"># input: node: root node of the tree</span></span><br><span class="line">    <span class="comment">#        target: list</span></span><br><span class="line">    <span class="comment"># output: nearest: list</span></span><br><span class="line">    <span class="comment">#       dist: distance between target and nearest</span></span><br><span class="line">    <span class="keyword">if</span> node == <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="comment"># ç”Ÿæˆæœç´¢è·¯å¾„</span></span><br><span class="line">    search_path = deque()</span><br><span class="line">    nearest = node</span><br><span class="line">    print(<span class="string">"search path: "</span>)</span><br><span class="line">    <span class="keyword">while</span> node:</span><br><span class="line">        print(node.value)</span><br><span class="line">        search_path.append(node)</span><br><span class="line">        <span class="comment"># if Dist(nearest.value, target) &gt; Dist(node.value, target):</span></span><br><span class="line">        <span class="comment"># nearest.value = node.value</span></span><br><span class="line">        <span class="comment"># minDist = Dist(node.value, target)</span></span><br><span class="line"></span><br><span class="line">        axis = node.axis</span><br><span class="line">        <span class="keyword">if</span> target[axis] &gt; node.value[axis]:</span><br><span class="line">            node = node.right</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            node = node.left</span><br><span class="line">    ...</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>ç„¶åæ˜¯<strong>å›æº¯æŸ¥æ‰¾</strong>ï¼Œå¦‚æœç›®æ ‡ç‚¹å’Œå½“å‰æœ€è¿‘ç‚¹æ„æˆçš„çƒå½¢åŒºåŸŸä¸å…¶ä¸Šæº¯èŠ‚ç‚¹ç›¸äº¤ï¼Œé‚£ä¹ˆå°±æœ‰ä¸€ç§æ½œåœ¨çš„å¯èƒ½â€”â€”ä¸Šæº¯èŠ‚ç‚¹çš„å¦ä¸€ä¸ªå­ç©ºé—´çš„å®ä¾‹å¯èƒ½ä½äºå½“å‰è¿™ä¸ªçƒå½¢åŒºåŸŸå†…ï¼Œå› æ­¤è¦è¿›è¡Œä¸€æ¬¡åˆ¤æ–­ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">searchNearest</span><span class="params">(node, target)</span>:</span></span><br><span class="line">    <span class="comment"># input: node: root node of the tree</span></span><br><span class="line">    <span class="comment">#        target: list</span></span><br><span class="line">    <span class="comment"># output: nearest: list</span></span><br><span class="line">    <span class="comment">#       dist: distance between target and nearest</span></span><br><span class="line">    ...</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># å›æº¯</span></span><br><span class="line">    print(<span class="string">"\nsearch backwards: "</span>)</span><br><span class="line">    nearest = search_path.pop()</span><br><span class="line">    minDist = Dist(nearest.value, target)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> search_path:</span><br><span class="line">        node = search_path.pop()</span><br><span class="line">        print(node.value)</span><br><span class="line">        <span class="keyword">if</span> node.axis:</span><br><span class="line">            axis = node.axis</span><br><span class="line">            <span class="keyword">if</span> minDist &gt; Dist1(node.value[axis], target[axis]):</span><br><span class="line">                <span class="keyword">if</span> target[axis] &gt; node.value[axis]:</span><br><span class="line">                    search_path.append(node.left)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    search_path.append(node.right)</span><br><span class="line">        <span class="keyword">if</span> Dist(target, nearest.value) &gt; Dist(node.value, target):</span><br><span class="line">            nearest = node</span><br><span class="line">            minDist = Dist(node.value, target)</span><br><span class="line">    <span class="keyword">return</span> nearest.value, minDist</span><br></pre></td></tr></table></figure><p>ä¸¤ä¸ªå‚è€ƒç‚¹ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">samples = [(<span class="number">2</span>,<span class="number">3</span>), (<span class="number">5</span>,<span class="number">4</span>), (<span class="number">9</span>,<span class="number">6</span>), (<span class="number">4</span>,<span class="number">7</span>), (<span class="number">8</span>,<span class="number">1</span>), (<span class="number">7</span>,<span class="number">2</span>)]</span><br><span class="line">target = (<span class="number">2.1</span>, <span class="number">3.1</span>)</span><br><span class="line">target = (<span class="number">2</span>, <span class="number">4.5</span>)</span><br></pre></td></tr></table></figure><p>KDæ ‘æœç´¢çš„æ ¸å¿ƒå°±æ˜¯ï¼šå½“æŸ¥è¯¢ç‚¹çš„é‚»åŸŸä¸åˆ†å‰²è¶…å¹³é¢ä¸¤ä¾§ç©ºé—´äº¤å‰²æ—¶ï¼Œéœ€è¦æŸ¥æ‰¾å¦ä¸€ä¾§å­ç©ºé—´ï¼ï¼ï¼ç®—æ³•å¹³å‡å¤æ‚åº¦O(N logN)ã€‚å®é™…æ—¶é—´å¤æ‚åº¦ä¸å®ä¾‹åˆ†å¸ƒæƒ…å†µæœ‰å…³ï¼Œ$t_{worst} = O(kN^{1-\frac{1}{k}})$ï¼Œé€šå¸¸è¦æ±‚æ•°æ®è§„æ¨¡è¾¾åˆ°$N \geq 2^D$æ‰èƒ½è¾¾åˆ°é«˜æ•ˆçš„æœç´¢ã€‚</p><p><img src="/2018/06/01/kd-tree/search1.jpg" alt="s1"></p><p><img src="/2018/06/01/kd-tree/search2.jpg" alt="s2"></p><h4 id="3-æ”¹è¿›ç®—æ³•ï¼šBBFç®—æ³•"><a href="#3-æ”¹è¿›ç®—æ³•ï¼šBBFç®—æ³•" class="headerlink" title="3. æ”¹è¿›ç®—æ³•ï¼šBBFç®—æ³•"></a>3. æ”¹è¿›ç®—æ³•ï¼šBBFç®—æ³•</h4><p>å›æº¯æ˜¯ç”±æŸ¥è¯¢è·¯å¾„å†³å®šçš„ï¼Œå› æ­¤ä¸€ç§ç®—æ³•æ”¹è¿›æ€è·¯å°±æ˜¯<strong>å°†æŸ¥è¯¢è·¯å¾„ä¸Šçš„ç»“ç‚¹æ’åº</strong>ï¼Œå›æº¯æ£€æŸ¥æ€»æ˜¯ä»ä¼˜å…ˆçº§æœ€é«˜çš„æ ‘èŠ‚ç‚¹å¼€å§‹â€”â€”Best-Bin-First BBFç®—æ³•ã€‚è¯¥ç®—æ³•èƒ½ç¡®ä¿ä¼˜å…ˆæ£€ç´¢åŒ…å«æœ€é‚»è¿‘ç‚¹å¯èƒ½æ€§è¾ƒé«˜çš„ç©ºé—´ã€‚</p><ul><li>ä¼˜å…ˆé˜Ÿåˆ—ï¼šä¼˜å…ˆçº§å–å†³äºå®ƒä»¬ç¦»æŸ¥è¯¢ç‚¹çš„è·ç¦»ï¼Œè·ç¦»è¶Šè¿‘ï¼Œä¼˜å…ˆçº§è¶Šé«˜ï¼Œå›æº¯çš„æ—¶å€™ä¼˜å…ˆéå†ã€‚</li><li>å¯¹å›æº¯å¯èƒ½éœ€è¦è·¯è¿‡çš„ç»“ç‚¹åŠ å…¥é˜Ÿåˆ—ï¼šåˆ‡åˆ†çš„æ—¶å€™ï¼ŒæŠŠæœªé€‰ä¸­çš„é‚£ä¸ªå…„å¼Ÿç»“ç‚¹åŠ å…¥åˆ°é˜Ÿåˆ—ä¸­ã€‚</li></ul>]]></content>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Real-Time loop Loop Closure in 2D Lidar SLAM è®ºæ–‡ç¬”è®°</title>
      <link href="/2018/05/27/Real-Time-loop-Loop-Closure-in-2D-Lidar-SLAM/"/>
      <url>/2018/05/27/Real-Time-loop-Loop-Closure-in-2D-Lidar-SLAM/</url>
      <content type="html"><![CDATA[<p>æ–‡ç« çš„æ ¸å¿ƒæ€æƒ³åœ¨äºè§£å†³loop closureé—®é¢˜ã€‚</p><p>å…¨å±€åœ°å›¾ç”±ä¸€ç³»åˆ—çš„submapæ„æˆï¼Œæ¯ä¸ªsubmapåˆ™ç”±ä¸€ç³»åˆ—çš„ä½å§¿èŠ‚ç‚¹åŠå¯¹åº”çš„scanæ•°æ®æ„æˆã€‚</p><p>æ–‡ç« çš„é‡ç‚¹åœ¨ç¬¬å››éƒ¨åˆ†å’Œç¬¬äº”éƒ¨åˆ†ï¼š</p><ul><li>ç¬¬å››éƒ¨åˆ†ï¼šlocal 2d slamï¼Œå°†scanä¸<strong>å½“å‰submap</strong>çš„åŒ¹é…é—®é¢˜è½¬åŒ–æˆä¸€ä¸ªæœ€å°äºŒä¹˜ä¼˜åŒ–é—®é¢˜ï¼Œç”±ceresæ¥æ±‚è§£ã€‚å‚è€ƒæ–‡çŒ®ã€ŠMany-to-Many Multi-Resolution Scan Matching ã€‹</li><li>ç¬¬äº”éƒ¨åˆ†ï¼šclosing loopï¼Œé‡‡ç”¨SPAè¿›è¡Œåç«¯loop closureï¼Œæå‡ºä¸€ç§å¹¶è¡Œçš„scanä¸<strong>finished submaps</strong>åŒ¹é…çš„æ–¹æ³•BBSï¼Œå¤§å¹…æé«˜ç²¾åº¦å’Œé€Ÿåº¦ã€‚å‚è€ƒæ–‡çŒ®ã€ŠEfficient Sparse Pose Adjustment for 2D Mapping(SPA)ã€‹ã€ã€ŠReal-Time Correlative Scan Matching(BBS)ã€‹</li></ul>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ä¸‰ç»´åˆšä½“è¿åŠ¨ &amp; æä»£æ•°</title>
      <link href="/2018/05/12/%E4%B8%89%E7%BB%B4%E5%88%9A%E4%BD%93%E8%BF%90%E5%8A%A8-%E6%9D%8E%E4%BB%A3%E6%95%B0/"/>
      <url>/2018/05/12/%E4%B8%89%E7%BB%B4%E5%88%9A%E4%BD%93%E8%BF%90%E5%8A%A8-%E6%9D%8E%E4%BB%A3%E6%95%B0/</url>
      <content type="html"><![CDATA[<h4 id="0-å‘é‡"><a href="#0-å‘é‡" class="headerlink" title="0. å‘é‡"></a>0. å‘é‡</h4><ul><li><p>åæ ‡ï¼šé¦–å…ˆç¡®å®šä¸€ä¸ªåæ ‡ç³»ï¼Œä¹Ÿå°±ç¡®å®šäº†ä¸€ç»„åŸº$(e_1, e_2, e_3)$ï¼Œé‚£ä¹ˆå‘é‡$a$çš„åæ ‡ä¸ºï¼š</p><script type="math/tex; mode=display">a = [e_1, e_2, e_3]\begin{bmatrix}a_1\\a_2\\a_3\end{bmatrix} = a_1e_1 + a_2e_2 + a_3e_3</script></li><li><p>å†…ç§¯ï¼šå¯¹å‘é‡$a, b \in R^3$ï¼Œå…¶å†…ç§¯ä¸ºï¼š</p><script type="math/tex; mode=display">a \cdot b = a^Tb = \Sigma a_ib_i = |a||b|cos<a,b></script><p>å†…ç§¯å¯ä»¥æè¿°å‘é‡é—´çš„æŠ•å½±å…³ç³»ã€‚</p></li><li><p>å¤–ç§¯ï¼š</p><script type="math/tex; mode=display">a \times b = \begin{vmatrix}i & j & k\\a_1 & a_2 & a_3\\b_1 & b_2 & b_3\end{vmatrix}=\begin{bmatrix}0 & -a_3 & a_2\\a_3 & 0 & -a_1\\-a_2 & a_1 & 0\end{bmatrix}b=a^{\wedge}b</script><p>å¤–ç§¯çš„æ–¹å‘å‚ç›´ä¸è¿™ä¸¤ä¸ªå‘é‡ï¼Œå¤§å°ä¸º$|a||b|sin<a,b>$ã€‚</a,b></p><p>å¤–ç§¯å¯ä»¥è¡¨ç¤ºå‘é‡çš„æ—‹è½¬ï¼Œå‘é‡$a$åˆ°$b$çš„<strong>æ—‹è½¬å‘é‡</strong>ï¼Œå¤–ç§¯çš„æ–¹å‘æ˜¯æ—‹è½¬å‘é‡çš„æ–¹å‘ï¼Œå¤§å°ç”±å¤¹è§’å†³å®šã€‚</p></li></ul><h4 id="1-æ—‹è½¬çŸ©é˜µRä¸å˜æ¢çŸ©é˜µT"><a href="#1-æ—‹è½¬çŸ©é˜µRä¸å˜æ¢çŸ©é˜µT" class="headerlink" title="1. æ—‹è½¬çŸ©é˜µRä¸å˜æ¢çŸ©é˜µT"></a>1. æ—‹è½¬çŸ©é˜µRä¸å˜æ¢çŸ©é˜µT</h4><ul><li><p>é€šå¸¸è®¾ç½®å›ºå®šçš„ä¸–ç•Œåæ ‡ç³»$O_w$å’Œè¿åŠ¨çš„ç›¸æœºåæ ‡ç³»$O_c$ï¼Œç›¸æœºè¿åŠ¨æ˜¯<strong>åˆšä½“è¿åŠ¨</strong>ï¼Œä¸¤ä¸ªåæ ‡ç³»ä¹‹é—´çš„å˜æ¢ç§°ä¸º<strong>æ¬§å¼å˜æ¢</strong>ã€‚</p></li><li><p>æ—‹è½¬çŸ©é˜µ$R$ï¼šå¯ä»¥æè¿°ç›¸æœºçš„æ—‹è½¬</p><p><strong>åæ ‡ç³»æ—‹è½¬</strong>å‰ååŒä¸€ä¸ªå‘é‡çš„åæ ‡å˜æ¢å…³ç³»ï¼š</p><script type="math/tex; mode=display">a =\begin{bmatrix}a_1\\a_2\\a_3\end{bmatrix}=\begin{bmatrix}e_1^T\\e_2^T\\e_3^T\end{bmatrix}\begin{bmatrix}e_1^{'} & e_2^{'}& e_3^{'}\end{bmatrix} \begin{bmatrix}a_1^{'}\\a_2^{'}\\a_3^{'}\end{bmatrix}=Ra^{'}</script><p>ä¸éš¾éªŒè¯<strong>æ—‹è½¬çŸ©é˜µæ˜¯è¡Œåˆ—å¼ä¸º1çš„æ­£äº¤çŸ©é˜µ</strong>ï¼Œå› æ­¤å¯ä»¥æŠŠæ—‹è½¬çŸ©é˜µçš„é›†åˆ<strong>ç‰¹æ®Šæ­£äº¤ç¾¤</strong>å®šä¹‰å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">SO(n) = \{R \in R^{n*n} | RR^T=I, det(R)=1\}</script><p>ç›¸åçš„æ—‹è½¬ï¼š</p><script type="math/tex; mode=display">a = Ra^{'}\\a^{'} = R^{-1}a = R^Ta</script></li><li><p>æ¬§å¼å˜æ¢ï¼šåŒ…æ‹¬æ—‹è½¬å’Œå¹³ç§»</p><script type="math/tex; mode=display">a^{'} = Ra + t</script></li><li><p>é½æ¬¡åæ ‡ï¼šå°„å½±å‡ ä½•çš„æ¦‚å¿µï¼Œæ¯ä¸ªåˆ†é‡åŒä¹˜ä¸€ä¸ªéé›¶å¸¸æ•°ä»ç„¶è¡¨ç¤ºåŒä¸€ä¸ªç‚¹ï¼š</p><script type="math/tex; mode=display">\tilde{x} = [x,y,z,w]^T=[x/w, y/w, z/w, 1]^T</script></li><li><p>é½æ¬¡å˜æ¢çŸ©é˜µ$T$ï¼šä½¿å¾—æ¬§å¼å˜æ¢ä»æ—§ä¿æŒçº¿æ€§å…³ç³»ï¼š</p><script type="math/tex; mode=display">\begin{bmatrix}a^{'}\\1\end{bmatrix}=\begin{bmatrix}R & t\\0 &1\end{bmatrix}\begin{bmatrix}a\\1\end{bmatrix} =T\begin{bmatrix}a\\1\end{bmatrix}</script><p>å˜æ¢çŸ©é˜µçš„é›†åˆ<strong>ç‰¹æ®Šæ¬§å¼ç¾¤</strong>ï¼š</p><script type="math/tex; mode=display">SE(3) = \left\{T=\begin{bmatrix}R & t\\0 & 1\end{bmatrix} \in R^{4*4} | R\in SO(3), t \in R^3\right\}</script></li></ul><h4 id="2-æ—‹è½¬å‘é‡-Axis-Angle"><a href="#2-æ—‹è½¬å‘é‡-Axis-Angle" class="headerlink" title="2. æ—‹è½¬å‘é‡ Axis-Angle"></a>2. æ—‹è½¬å‘é‡ Axis-Angle</h4><p>ä¸€ä¸ªæ—‹è½¬åªæœ‰3ä¸ªè‡ªç”±åº¦ï¼Œæ—‹è½¬çŸ©é˜µRè¦ç”¨9çš„å‚æ•°æ¥æè¿°ï¼Œæ˜¾ç„¶æ˜¯å†—ä½™çš„ã€‚ä¸€ç§ç´§å‡‘çš„æ–¹å¼â€”â€”ä»»ä½•æ—‹è½¬éƒ½å¯ä»¥ç”¨ä¸€ä¸ªæ—‹è½¬è½´$n$å’Œä¸€ä¸ªæ—‹è½¬è§’$\theta$æ¥åˆ»ç”»ï¼š</p><script type="math/tex; mode=display">R = cos\theta I + (1-cos\theta)nn^T + sin\theta n^{\wedge}\\\theta = arccos(\frac{tr(R)-1}{2})\\</script><p>æ—‹è½¬è½´ä¸Šçš„å‘é‡åœ¨æ—‹è½¬åä¸å‘ç”Ÿæ”¹å˜ï¼Œå› æ­¤æœ‰ï¼š</p><script type="math/tex; mode=display">Rn = n</script><p>è½¬è½´$n$æ˜¯æ—‹è½¬çŸ©é˜µ$R$çš„ç‰¹å¾å€¼1å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œå¯ä»¥ç”±æ­¤æ¥è®¡ç®—è½¬è½´$n$ã€‚</p><h4 id="3-æ¬§æ‹‰è§’-rpy"><a href="#3-æ¬§æ‹‰è§’-rpy" class="headerlink" title="3. æ¬§æ‹‰è§’ rpy"></a>3. æ¬§æ‹‰è§’ rpy</h4><p>æŠŠæ—‹è½¬åˆ†è§£åˆ°3ä¸ªè½´ä¸Šï¼Œrpyè§’çš„æ—‹è½¬é¡ºåºæ˜¯ZYXï¼š</p><ul><li>é¦–å…ˆç»•ç‰©ä½“çš„Zè½´æ—‹è½¬ï¼Œå¾—åˆ°åèˆªè§’yaw</li><li>ç„¶åç»•æ—‹è½¬ä¹‹åçš„Yè½´æ—‹è½¬ï¼Œå¾—åˆ°ä¿¯ä»°è§’pitch</li><li>ç»•æ—‹è½¬ä¹‹åçš„Xè½´æ—‹è½¬ï¼Œå¾—åˆ°æ»šè½¬è§’roll</li></ul><p>ä¸‡å‘é”é—®é¢˜ï¼šåœ¨ä¿¯ä»°è§’ä¸º$\pm 90^{\circ}$æ—¶ï¼Œç¬¬ä¸€æ¬¡å’Œç¬¬ä¸‰æ¬¡æ—‹è½¬ä½¿ç”¨åŒä¸€æ ¹è½´ï¼Œä¸¢äº†è‡ªç”±åº¦â€”â€”<strong>å¥‡å¼‚æ€§é—®é¢˜</strong>ã€‚</p><h4 id="4-å››å…ƒæ•°-q"><a href="#4-å››å…ƒæ•°-q" class="headerlink" title="4. å››å…ƒæ•° q"></a>4. å››å…ƒæ•° q</h4><p>å››å…ƒæ•°æ˜¯ä¸€ç§æ‰©å±•çš„è´Ÿæ•°ï¼Œç”±ä¸€ä¸ªå®éƒ¨å’Œä¸‰ä¸ªè™šéƒ¨ç»„æˆï¼Œå¯ä»¥æŠŠä¸‰ä¸ªè™šéƒ¨è„‘è¡¥æˆç©ºé—´ä¸­çš„ä¸‰æ ¹è½´ï¼š</p><script type="math/tex; mode=display">q = q_0 + q_1i + q_2j + q_3k\\\left\{\begin{split}& i^2 = j^2=k^2=-1\\& ij = k, ji = -k\\& jk = i, kj = -i\\& ki=j, ik=-j\end{split}\right.</script><ul><li><p>ä¹˜ä»¥$i$å¯¹åº”ç€ç»•$i$è½´æ—‹è½¬$180^{\circ}$</p></li><li><p>ä»»æ„çš„æ—‹è½¬å¯ä»¥ç”±ä¸¤ä¸ªäº’ä¸ºç›¸åæ•°çš„å››å…ƒæ•°è¡¨ç¤º</p></li><li><p>ä¸æ—‹è½¬å‘é‡$n = [n_x, n_y, n_z]^T, \theta$è½¬æ¢å…³ç³»ï¼š</p><script type="math/tex; mode=display">q = [cos\frac{\theta}{2}, n_xsin\frac{\theta}{2}, n_ysin\frac{\theta}{2}, n_zsin\frac{\theta}{2}]^T\\\left\{\begin{split}& \theta = 2arccos q_0\\& [n_x, n_y, n_z]^T = [q_1, q_2, q_3]^T/sin\frac{\theta}{2}\end{split}\right.</script></li><li><p>ä¸æ—‹è½¬çŸ©é˜µ$R$çš„å…³ç³»ï¼š</p><script type="math/tex; mode=display">R = \begin{bmatrix}1-2q_2^2 - 2q_3^2 & 2q_1q_2-2q_0q_3 & 2q_1q_3+2q_0q_2\\ 2q_1q_2+2q_0q_3  & 1-2q_1^2 - 2q_3^2 &  2q_2q_3-2q_0q_1\\ 2q_1q_3-2q_0q_2 &  2q_2q_3+2q_0q_1 & 1-2q_1^2 - 2q_2^2 \end{bmatrix}\\q_0 = \frac{\sqrt{tr(R)+1}}{2}, q_1 = \frac{R_{23}-R_{32}}{4q_0}, q_2 = \frac{R_{31}-R_{13}}{4q_0}, q_3 = \frac{R_{12}-R_{21}}{4q_0}</script></li><li><p>è¡¨ç¤ºæ—‹è½¬ï¼š</p><p>ç©ºé—´ä¸­ç‚¹$p = [x, y,z]^T\in R^3$ï¼Œå·²çŸ¥æ—‹è½¬è½´è§’$n,\theta$ï¼Œæ—‹è½¬ä¹‹åç‚¹åæ ‡å˜ä¸º$p^{â€˜}$ï¼Œå¦‚æœç”¨æ—‹è½¬çŸ©é˜µæè¿°ï¼š</p><script type="math/tex; mode=display">p^{'} = Rp</script><p>å››å…ƒæ•°$q = [cos\frac{\theta}{2}, nsin\frac{\theta}{2}]$ï¼Œé‚£ä¹ˆæ—‹è½¬åçš„ç‚¹$p^{â€˜}$å¯ä»¥è¡¨ç¤ºä¸ºï¼š</p><script type="math/tex; mode=display">p^{'} = qpq^{-1}</script></li></ul><h4 id="5-æç¾¤"><a href="#5-æç¾¤" class="headerlink" title="5. æç¾¤"></a>5. æç¾¤</h4><p>ä¸Šé¢æåˆ°äº†æ—‹è½¬çŸ©é˜µæ„æˆçš„ç‰¹æ®Šæ­£äº¤ç¾¤$SO(3)$å’Œç”±å˜æ¢çŸ©é˜µæ„æˆçš„ç‰¹æ®Šæ¬§å¼ç¾¤$SE(3)$ï¼š</p><script type="math/tex; mode=display">SO(n) = \left\{R \in R^{n*n} | RR^T=I, det(R)=1\right\}\\SE(3) = \left\{T=\begin{bmatrix}R & t\\0 & 1\end{bmatrix} \in R^{4*4} | R\in SO(3), t \in R^3\right\}</script><ul><li>$SO(n)$å’Œ$SE(n)$å¯¹<strong>åŠ æ³•ä¸å°é—­ï¼Œå¯¹ä¹˜æ³•æ˜¯å°é—­</strong>çš„ã€‚</li><li>ç¾¤æ˜¯ä¸€ç§é›†åˆ$A$åŠ ä¸Šä¸€ç§è¿ç®—$\ \cdot \ $çš„ä»£æ•°ç»“æ„ï¼Œè®°ä½œ$G = (A, \ \cdot \ )$ï¼Œç¾¤å†…å…ƒç´ æ»¡è¶³å°é—­æ€§ã€ç»“åˆå¾‹ã€å¹ºå…ƒã€å¯é€†å››ä¸ªæ¡ä»¶ã€‚</li><li>æç¾¤æ˜¯æŒ‡å…·æœ‰è¿ç»­æ€§è´¨çš„ç¾¤ã€‚åˆšä½“åœ¨ç©ºé—´ä¸­èƒ½å¤Ÿè¿ç»­åœ°è¿åŠ¨ï¼Œå› æ­¤$SO(n)$å’Œ$SE(n)$æ˜¯æç¾¤ã€‚</li></ul><h4 id="6-æä»£æ•°"><a href="#6-æä»£æ•°" class="headerlink" title="6. æä»£æ•°"></a>6. æä»£æ•°</h4><p><strong>6.1 å¼•å…¥</strong></p><p>å¯¹ä»»æ„æ—‹è½¬çŸ©é˜µ$R$ï¼Œéƒ½æ»¡è¶³$RR^T=I$ã€‚æŠŠå®ƒå†™æˆå…³äºæ—¶é—´çš„å‡½æ•°$R(t)$æœ‰ï¼š</p><script type="math/tex; mode=display">R(t)R(t)^T = I</script><p>å¯¹ç­‰å¼ä¸¤è¾¹æ±‚å¯¼ï¼š</p><script type="math/tex; mode=display">\dot R(t)R(t)^T + R(t)\dot R(t)^T=0\\\dot R(t)R(t)^T =-\big(\dot R(t)R(t)^T \big)^T</script><p>å¯ä»¥çœ‹å‡º$\dot R(t)R(t)^T $æ˜¯ä¸€ä¸ª<strong>åå¯¹ç§°é˜µ</strong>ï¼Œå¯¹ä»»æ„ä¸€ä¸ªåå¯¹ç§°é˜µï¼Œéƒ½å¯ä»¥æ‰¾åˆ°ä¸€ä¸ªä¸ä¹‹å¯¹åº”çš„<strong>å‘é‡</strong>ï¼š</p><script type="math/tex; mode=display">a^{\wedge} = A, A^{\vee}=a</script><p>äºæ˜¯å¯ä»¥æ‰¾åˆ°ä¸€ä¸ªä¸‰ç»´å‘é‡$\phi(t) \in R^3$ä¸ä¹‹å¯¹åº”ï¼š</p><script type="math/tex; mode=display">\dot R(t)R(t)^T  = \phi(t)^{\wedge}\\\dot R(t) = \phi(t)^{\wedge}R(t)</script><p>å¯ä»¥çœ‹åˆ°ï¼Œ<strong>æ¯å¯¹æ—‹è½¬çŸ©é˜µæ±‚ä¸€æ¬¡å¯¼æ•°ï¼Œåªéœ€å·¦ä¹˜ä¸€ä¸ªåå¯¹ç§°é˜µ$\phi(t)^{\wedge}$å³å¯</strong>ã€‚</p><p>æ±‚è§£ä¸Šé¢çš„å¾®åˆ†æ–¹ç¨‹ï¼Œå¯ä»¥å¾—åˆ°$R(t) = exp(\phi^{\wedge}t)$ã€‚ä¹Ÿå°±æ˜¯è¯´$\phi$æè¿°äº†$R$åœ¨å±€éƒ¨çš„å¯¼æ•°å…³ç³»ã€‚</p><p><strong>6.2 æä»£æ•°</strong></p><ul><li><p>æ¯ä¸ªæç¾¤éƒ½æœ‰ä¸ä¹‹å¯¹åº”çš„æä»£æ•°ã€‚æä»£æ•°æè¿°äº†æç¾¤çš„å±€éƒ¨æ€§è´¨ã€‚</p></li><li><p>æä»£æ•°ç”±ä¸€ä¸ªé›†åˆ$V$ï¼Œä¸€ä¸ªæ•°åŸŸ$F$ï¼Œå’Œä¸€ä¸ªäºŒå…ƒè¿ç®—<strong>ææ‹¬å·</strong>$[,]$ç»„æˆï¼Œè®°ä½œ$( V, F, [,])$ã€‚æä»£æ•°çš„å…ƒç´ æ»¡è¶³å°é—­æ€§ã€åŒçº¿æ€§ã€è‡ªåæ€§ã€é›…å¯æ¯”ç­‰ä»·å››æ¡æ€§è´¨ã€‚</p></li><li><p>ä¸Šä¸€èŠ‚çš„$\phi$å°±æ˜¯$SO(3)$å¯¹åº”çš„æä»£æ•°$so(3)$ï¼Œä¸¤è€…çš„å…³ç³»ç”±<strong>æŒ‡æ•°æ˜ å°„</strong>ç»™å®šï¼š</p><script type="math/tex; mode=display">R = exp(\phi^{\wedge})\\so(3) = \left\{ \phi \in R^3, \Phi = \phi^{\wedge} \in R^{3*3}\right\}</script></li><li><p>$SE(3)$å¯¹åº”çš„æä»£æ•°$se(3)$ä½äº$R^6$ç©ºé—´ä¸­ï¼š</p><script type="math/tex; mode=display">se(3) = \left\{ \xi = \begin{bmatrix}\rho\\\phi\end{bmatrix}\in R^6, \rho \in so(3), \xi^{\wedge} =\begin{bmatrix}\phi^{\wedge} & \rho\\0^T & 0\end{bmatrix} \in R^{4*4}\right\}</script></li><li><p>æŒ‡æ•°æ˜ å°„</p><p>ç”±äº$\phi$æ˜¯ä¸€ä¸ªä¸‰ç»´å‘é‡ï¼Œå› æ­¤å¯ä»¥å†™ä½œ$\theta a$çš„å½¢å¼ï¼Œ$a$æ˜¯ä¸€ä¸ªå•ä½å‘é‡ï¼Œå› æ­¤å…·æœ‰ä»¥ä¸‹æ€§è´¨ï¼š</p><script type="math/tex; mode=display">a^{\wedge}a^{\wedge} = aa^T-I\\a^{\wedge}a^{\wedge}a^{\wedge} = -a^{\wedge}</script><p>å¯¹$so(3)$æä»£æ•°çš„æŒ‡æ•°æ˜ å°„åšæ³°å‹’å±•å¼€ï¼Œå¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">\begin{split}&R= exp(\phi^{\wedge}) = exp(\theta a^{\wedge})=\Sigma_{n=0}^{\infty}\frac{1}{n!} (\theta a^{\wedge})^n\\& =cos\theta I + (1-cos\theta)aa^T+sin\theta a^{\wedge}\end{split}</script><p>å¯ä»¥çœ‹åˆ°$so(3)$å®é™…ä¸Šå°±æ˜¯æ—‹è½¬å‘é‡ç»„æˆçš„ç©ºé—´ï¼ŒæŒ‡æ•°æ˜ å°„å³æ˜¯ç½—å¾·é‡Œæ ¼æ–¯å…¬å¼ã€‚</p><p>æŒ‡æ•°æ˜ å°„æ˜¯ä¸€ä¸ª<strong>æ»¡å°„</strong>ï¼Œæ¯ä¸ª$SO(3)$ä¸­çš„å…ƒç´ ï¼Œéƒ½å¯ä»¥æ‰¾åˆ°è‡³å°‘ä¸€ä¸ª$so(3)$å…ƒç´ ä¸ä¹‹å¯¹åº”ï¼ˆ$\theta + 2k\pi$ï¼‰ã€‚</p><p>$se(3)$ä¸Šçš„æŒ‡æ•°æ˜ å°„ä¸ºï¼š</p><script type="math/tex; mode=display">T = exp(\xi^{\wedge}) = \begin{bmatrix}R & J\rho\\0 &1\end{bmatrix}\\J = \frac{sin\theta}{\theta}I + (1-\frac{sin\theta}{\theta})aa^T + \frac{1-cos\theta}{\theta}a^{\wedge}</script></li></ul><p><strong>6.3 æä»£æ•°æ±‚å¯¼ </strong></p><ul><li><p>ä¸¤ä¸ªæä»£æ•°æŒ‡æ•°æ˜ å°„ä¹˜ç§¯çš„å®Œæ•´å½¢å¼ç”±BCHå…¬å¼ç»™å‡ºï¼š</p><script type="math/tex; mode=display">ln(exp(A)exp(B)) = A+B + \frac{1}{2}[A, B] + \frac{1}{12}[A,[A,B]] + ...</script></li><li><p>å¯¹$ln(exp(\phi_1^{\wedge})exp(\phi_2^{\wedge}))^{\vee}$ï¼Œå½“$\phi_1$æˆ–$\phi_2$ä¸ºå°é‡æ—¶ï¼ŒBCHå…¬å¼ç»™å‡ºäº†çº¿æ€§è¿‘ä¼¼è¡¨è¾¾ï¼š</p><script type="math/tex; mode=display">ln(exp(\phi_1^{\wedge})exp(\phi_2^{\wedge}))^{\vee} = \left\{\begin{split}J_l(\phi_2)^{-1}\phi_1 + \phi_2\ \ \ \ \ \ \phi_1ä¸ºå°é‡\\J_r(\phi_1)^{-1}\phi_2 + \phi_1\ \ \ \ \ \ \phi_2ä¸ºå°é‡\\\end{split}\right.</script><p>BCHè¿‘ä¼¼é›…å¯æ¯”$J_l$å°±æ˜¯ä¸Šä¸€èŠ‚çš„$J$ï¼š</p><script type="math/tex; mode=display">J_l = J = \frac{sin\theta}{\theta}I + (1-\frac{sin\theta}{\theta})aa^T + \frac{1-cos\theta}{\theta}a^{\wedge}\\J_l^{-1} = \frac{\theta}{2}cot\frac{\theta}{2}I + (1-\frac{\theta}{2}cot\frac{\theta}{2})aa^T - \frac{\theta}{2}a^{\wedge}\\J_r(\phi) = J_l(-\phi)</script><p>ç”±ä»¥ä¸Šå…¬å¼è¯´æ˜äº†<strong>æç¾¤ä¹˜æ³•</strong>å’Œ<strong>æä»£æ•°åŠ æ³•</strong>çš„è¿‘ä¼¼è½¬æ¢å…³ç³»ã€‚</p></li><li><p>åœ¨$SO(3)ã€SE(3)$ä¸Šæ²¡æœ‰è‰¯å¥½å®šä¹‰çš„åŠ æ³•ï¼Œè€Œæä»£æ•°ç”±å‘é‡ç»„æˆï¼Œæœ‰è‰¯å¥½çš„åŠ æ³•è¿ç®—ã€‚å› æ­¤åœ¨è®¡ç®—ä½å§¿çš„å¯¼æ•°æ—¶ï¼Œé€šå¸¸ä½¿ç”¨æä»£æ•°è§£å†³ï¼Œæä»£æ•°æ±‚å¯¼çš„ä¸¤ç§æ€è·¯ï¼š</p><ul><li>æä»£æ•°æ±‚å¯¼$\delta \phi$ï¼šç”¨æä»£æ•°è¡¨ç¤ºå§¿æ€ï¼Œç„¶åè½¬åŒ–æˆå¯¹æä»£æ•°æ±‚å¯¼$\phi + \delta \phi$</li></ul><script type="math/tex; mode=display">\begin{split}&\frac{\partial (Rp)}{\partial R} = \frac{\partial(exp(\phi^{\wedge})p)}{\partial \phi}\\&= lim \frac{exp((\phi+\delta\phi)^{\wedge})p-exp(\phi^{\wedge})p}{\partial \phi}\\&=-(Rp)^{\wedge}J_l\end{split}</script><ul><li>æ‰°åŠ¨æ¨¡å‹$\Delta R$ï¼šå¯¹$R$è¿›è¡Œæ‰°åŠ¨ï¼Œç„¶åå¯¹æ‰°åŠ¨æ±‚å¯¼$\Delta R R$</li></ul><script type="math/tex; mode=display">\begin{split}&\frac{\partial (Rp)}{\partial R} = lim \frac{exp(\varphi^{\wedge})exp(\phi^{\wedge})p-exp(\phi^{\wedge})p}{\partial \varphi}= -(Rp)^{\wedge}\\& \frac{\partial Tp}{\partial \delta \xi} = \begin{bmatrix}I & -(Rp+t)^{\wedge}\\0 & 0\end{bmatrix} = (Tp)^{\odot}\end{split}</script></li></ul>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>c++ for record</title>
      <link href="/2018/05/10/c-for-record/"/>
      <url>/2018/05/10/c-for-record/</url>
      <content type="html"><![CDATA[<p>æœ€è¿‘å¼€å§‹ç€æ‰‹å†™slamä»£ç ï¼Œçœ‹ä¸€äº›å¸¸ç”¨åº“æºç çš„æ—¶å€™å‘ç°å„ç§åŠ›ä¸ä»å¿ƒï¼Œä¸€äº›c++11çš„éªšæ“ä½œç«Ÿç„¶æ²¡è§è¿‡ï¼Œæ˜¯æ—¶å€™å®Œæ•´æ’¸ä¸€å‘c++ primerç¥­å¤©äº†ã€‚</p><ol><li><p>iostream</p><ul><li>æ ‡å‡†è¾“å…¥ï¼šcin</li><li>æ ‡å‡†è¾“å‡ºï¼šcoutã€cerrã€clog</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> v1=<span class="number">0</span>, v2=<span class="number">0</span>;</span><br><span class="line"><span class="built_in">cin</span> &gt;&gt; v1 &gt;&gt; v2;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; v1+v2 &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cerr</span> &lt;&lt; <span class="string">"This is nonsense."</span> &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure><ul><li>&lt;&lt; å’Œ &gt;&gt; çš„æ–¹å‘è¡¨ç¤ºäº†æ•°æ®æµçš„èµ°å‘ï¼Œä¹Ÿå°±æ˜¯èµ‹å€¼çš„æ–¹å‘ã€‚cerrç”¨æ¥è¾“å‡ºé”™è¯¯ä¿¡æ¯ã€‚</li></ul></li><li><p>æ§åˆ¶æµ</p><ul><li><p>whileï¼šæ¯æ¬¡æ‰§è¡Œå¾ªç¯ä¹‹å‰å…ˆæ£€æŸ¥<strong>å¾ªç¯æ¡ä»¶</strong></p></li><li><p>do whileï¼šå…ˆæ‰§è¡Œå¾ªç¯ä½“åæ£€æŸ¥æ¡ä»¶</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (condition)</span><br><span class="line">    statement</span><br><span class="line">   </span><br><span class="line"><span class="keyword">do</span> </span><br><span class="line">    statement</span><br><span class="line"><span class="keyword">while</span> (condition);</span><br></pre></td></tr></table></figure></li><li><p>forï¼šæ¯æ¬¡æ‰§è¡Œå¾ªç¯ä¹‹å‰å…ˆæ£€æŸ¥<strong>å¾ªç¯æ¡ä»¶</strong>ï¼Œæ‰§è¡Œå¾ªç¯ä¹‹åæ‰§è¡Œ<strong>è¡¨è¾¾å¼</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (init-statement; condition; expression)</span><br><span class="line">&#123;</span><br><span class="line">    statemnt</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// èŒƒå›´forè¯­å¥</span></span><br><span class="line"><span class="keyword">for</span> (declaration : expression)</span><br><span class="line">statement</span><br></pre></td></tr></table></figure></li><li><p>switchï¼š</p><ul><li>case labelï¼šcaseæ ‡ç­¾å¿…é¡»æ˜¯<strong>æ•´å½¢å¸¸é‡</strong>è¡¨è¾¾å¼</li><li>å¦‚æœæŸä¸ªcaseæ ‡ç­¾åŒ¹é…æˆåŠŸï¼Œä¼š<strong>å¾€åé¡ºåºæ‰§è¡Œæ‰€æœ‰caseåˆ†æ”¯</strong>ï¼Œç›´åˆ°ç»“å°¾æˆ–è€…é‡åˆ°break</li><li>defaultæ ‡ç­¾</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">switch</span>(ch)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">'a'</span>: <span class="keyword">case</span> <span class="string">'b'</span>: <span class="keyword">case</span> <span class="string">'c'</span>:</span><br><span class="line">    ++cnt;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>breakï¼šè´Ÿè´£ç»ˆæ­¢ç¦»ä»–æœ€è¿‘çš„whileã€do whileã€foræˆ–switchè¯­å¥ã€‚</p></li><li><p>continueï¼šè´Ÿè´£ç»ˆæ­¢ç¦»ä»–æœ€è¿‘çš„whileã€do whileã€forå¾ªç¯çš„å½“å‰è¿­ä»£ã€‚</p></li><li><p>gotoï¼šæ— æ¡ä»¶è·³è½¬åˆ°åŒä¸€å‡½æ•°å†…çš„æŸä¸ª<strong>å¸¦æ ‡ç­¾è¯­å¥</strong>ã€‚</p><blockquote><p>labeled statement: <code>label: statement</code></p></blockquote></li><li><p>å¼‚å¸¸</p><ul><li><p>throwï¼š<strong>å¼•å‘å¼‚å¸¸</strong>ï¼Œåé¢ç´§éšä¸€ä¸ªå¼‚å¸¸ç±»å‹ï¼Œç»ˆæ­¢å½“å‰å‡½æ•°ï¼Œå°†æ§åˆ¶æƒè½¬ç§»ç»™èƒ½å¤Ÿå¤„ç†è¯¥å¼‚å¸¸çš„ä»£ç ã€‚</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdexcept&gt;</span></span></span><br><span class="line"><span class="comment">// runtime_error æ ‡å‡†åº“å¼‚å¸¸ç±»å‹</span></span><br><span class="line"><span class="keyword">throw</span> runtime_error(<span class="string">"Data must refer to same name"</span>);</span><br></pre></td></tr></table></figure></li><li><p>tryï¼š<strong>å¤„ç†å¼‚å¸¸</strong>ï¼Œåé¢ç´§éšä¸€å¥—catchå­å¥ç”¨æ¥å¤„ç†å¼‚å¸¸ã€‚</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>&#123;</span><br><span class="line">    program statements</span><br><span class="line">&#125; <span class="keyword">catch</span> (exception-declaration) &#123;</span><br><span class="line">    handler-statements</span><br><span class="line">&#125; <span class="keyword">catch</span> (exception-declaration) &#123;</span><br><span class="line">    handler-statements</span><br><span class="line">&#125; ...</span><br></pre></td></tr></table></figure><p>tryè¯­å¥å—å†…å£°æ˜çš„å˜é‡åœ¨å—å¤–æ— æ³•è®¿é—®ï¼Œå³ä½¿æ˜¯catchè¯­å¥ã€‚</p><p>catchä¸€æ—¦å®Œæˆï¼Œç¨‹åºè·³è½¬åˆ°æœ€åä¸€ä¸ªcatchå­å¥ä¹‹åçš„è¯­å¥ã€‚</p></li></ul></li></ul></li><li><p>ç±»</p><p>ç±»å‹ &amp; å¯¹è±¡ï¼ˆå®ä¾‹ï¼‰ï¼Œå˜é‡ &amp; è¡Œä¸ºï¼ˆæ–¹æ³•ï¼‰ã€‚</p><ul><li><p>å­˜åœ¨ç±»å†…é»˜è®¤åˆå§‹åŒ–</p></li><li><p>ç±»é€šå¸¸è¢«å®šä¹‰åœ¨å¤´æ–‡ä»¶ä¸­ï¼Œå¤´æ–‡ä»¶åå­—åº”ä¸ç±»çš„åå­—ä¿æŒä¸€è‡´</p><blockquote><p>å¤´æ–‡ä»¶é€šå¸¸åŒ…å«åªèƒ½è¢«å®šä¹‰ä¸€æ¬¡çš„å®ä½“ï¼Œå¦‚ç±»ã€constç­‰ã€‚</p><p>å¤´æ–‡ä»¶ä¿æŠ¤ç¬¦#ifndefç³»åˆ—ï¼Œåˆ›å»º<strong>é¢„å¤„ç†å˜é‡</strong>ï¼Œé˜²æ­¢å¤šæ¬¡åŒ…å«ã€‚</p></blockquote></li><li><p>æ„é€ å‡½æ•°åˆå§‹å€¼åˆ—è¡¨ï¼šå†’å·ä»¥åŠå†’å·å’ŒèŠ±æ‹¬å·ä¹‹é—´çš„ä»£ç </p><blockquote><p>åˆ—è¡¨åªè¯´æ˜ç”¨äºåˆå§‹åŒ–æˆå‘˜çš„å€¼ï¼Œè€Œä¸é™å®šåˆå§‹åŒ–çš„å…·ä½“é¡ºåºã€‚</p><p>æˆå‘˜çš„åˆå§‹åŒ–é¡ºåºä¸å®ƒä»¬åœ¨ç±»å®šä¹‰ä¸­çš„å‡ºç°é¡ºåºä¸€è‡´ã€‚</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ä¸ºç±»æˆå‘˜åˆå§‹åŒ–</span></span><br><span class="line">Sales_data(<span class="keyword">const</span> <span class="built_in">string</span> &amp;s, <span class="keyword">unsigned</span> n, <span class="keyword">double</span> p)</span><br><span class="line"> : bookNo(s), units_sold(n), revenue(p*n) &#123;&#125;Â </span><br><span class="line"></span><br><span class="line"><span class="comment">// åŒºåˆ«äºèµ‹å€¼</span></span><br><span class="line">Sales_data(<span class="keyword">const</span> <span class="built_in">string</span> &amp;s, <span class="keyword">unsigned</span> n, <span class="keyword">double</span> p)</span><br><span class="line">&#123;</span><br><span class="line">    bookNo = s;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>æ¥å£ä¸å°è£…ï¼š</p><ul><li>å®šä¹‰åœ¨privateè¯´æ˜ç¬¦ä¹‹åçš„æˆå‘˜åªèƒ½è¢«ç±»å†…æˆå‘˜å‡½æ•°è®¿é—®ï¼Œå°è£…äº†ç±»çš„å®ç°ç»†èŠ‚ã€‚</li><li>å®šä¹‰åœ¨publicè¯´æ˜ç¬¦ä¹‹åçš„æˆå‘˜å¯ä»¥åœ¨æ•´ä¸ªç¨‹åºå†…è¢«è®¿é—®ï¼Œå®šä¹‰ç±»çš„æ¥å£ã€‚</li></ul></li><li><p>classå’Œstructçš„åŒºåˆ«ï¼šæˆå‘˜è®¿é—®æƒé™</p><ul><li>structï¼šå®šä¹‰åœ¨ç¬¬ä¸€ä¸ªè¯´æ˜ç¬¦ä¹‹å‰çš„æˆå‘˜æ˜¯public</li><li>classï¼šå®šä¹‰åœ¨ç¬¬ä¸€ä¸ªè¯´æ˜ç¬¦ä¹‹å‰çš„æˆå‘˜æ˜¯private</li></ul></li><li><p>å‹å…ƒï¼šå…è®¸å…¶ä»–ç±»æˆ–å‡½æ•°è®¿é—®å®ƒçš„éå…¬æœ‰æˆå‘˜ï¼Œåœ¨ç±»å†…æ·»åŠ ä»¥friendå…³é”®å­—å¼€å§‹çš„å‹å…ƒå£°æ˜ã€‚</p><blockquote><p>å‹å…ƒçš„å£°æ˜ä»…ä»…æŒ‡å®šäº†è®¿é—®æƒé™ï¼Œè€Œéä¸€ä¸ªé€šå¸¸æ„ä¹‰ä¸Šçš„å‡½æ•°å£°æ˜ã€‚</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sales_data</span> &#123;</span></span><br><span class="line"><span class="comment">// å‹å…ƒå£°æ˜</span></span><br><span class="line"><span class="function"><span class="keyword">friend</span> Sales_data <span class="title">add</span><span class="params">(<span class="keyword">const</span> Sales_data&amp;, <span class="keyword">const</span> Sales_data&amp;)</span></span>;</span><br><span class="line"><span class="comment">// éå…¬æœ‰æˆå‘˜</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"><span class="built_in">string</span> bookNo;</span><br><span class="line"><span class="keyword">double</span> revenue = <span class="number">0.0</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></li><li><p>é™æ€æˆå‘˜staticï¼šä¸ç±»æœ¬èº«ç›¸å…³è”ï¼Œä¸å±äºä»»ä½•ä¸€ä¸ªå¯¹è±¡ï¼Œå› æ­¤ä¸æ˜¯åœ¨åˆ›å»ºç±»å¯¹è±¡çš„æ—¶å€™è¢«å®šä¹‰çš„ï¼Œå› æ­¤é€šå¸¸åœ¨ç±»çš„å¤–éƒ¨å®šä¹‰å’Œåˆå§‹åŒ–ï¼Œåœ¨ç±»å†…éƒ¨æ·»åŠ ä»¥staticå…³é”®å­—å¼€å§‹çš„é™æ€æˆå‘˜å£°æ˜ã€‚</p></li></ul></li><li><p>å†…ç½®ç±»å‹</p><ul><li>å†…å­˜ä¸­çš„ä¸€ä¸ªåœ°å€å¯¹åº”ä¸€ä¸ªå­—èŠ‚</li><li>unsignedç±»å‹è¡¨ç¤º<strong>å¤§äºç­‰äº0</strong>çš„æ•°ï¼ˆ$[0, 2^{n}-1]$ï¼‰ï¼Œè¢«èµ‹ç»™ä¸€ä¸ªè¶…å‡ºè¡¨ç¤ºèŒƒå›´çš„æ•°æ—¶ï¼Œ<strong>è‡ªåŠ¨å–ä½™</strong>ï¼Œä½œä¸ºå¾ªç¯æ¡ä»¶æ—¶å½“å¿ƒè¿›å…¥<strong>æ— é™å¾ªç¯</strong></li><li>signedç±»å‹æ­£è´Ÿå€¼èŒƒå›´å¹³è¡¡ï¼ˆ$[-2^{n-1}, 2^{n-1}-1]$ï¼‰ï¼Œè¢«èµ‹ç»™ä¸€ä¸ªè¶…å‡ºè¡¨ç¤ºèŒƒå›´çš„æ•°æ—¶ï¼Œ<strong>ç»“æœæœªå®šä¹‰</strong></li><li>å­—ç¬¦å‹charï¼Œå•å¼•å·ï¼Œä¸€ä¸ªå­—èŠ‚</li><li>å­—ç¬¦ä¸²å‹ï¼ŒåŒå¼•å·ï¼Œå¸¸é‡å­—ç¬¦æ•°ç»„ï¼Œç»“å°¾éšå«ç©ºå­—ç¬¦ â€˜\0â€™</li><li>nullptr = 0ï¼ˆä¼ ç»ŸNULLåŒ…å«åœ¨cstdlibå¤´æ–‡ä»¶å†…ï¼‰</li></ul></li><li><p>å˜é‡</p><ul><li><p><strong>åˆ—è¡¨åˆå§‹åŒ–</strong>ï¼ŒèŠ±æ‹¬å·</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// æ‹·è´åˆå§‹åŒ–</span></span><br><span class="line"><span class="keyword">int</span> x=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> x=&#123;<span class="number">0</span>&#125;;</span><br><span class="line"><span class="comment">// ç›´æ¥åˆå§‹åŒ–</span></span><br><span class="line"><span class="keyword">int</span> x&#123;<span class="number">0</span>&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">x</span><span class="params">(<span class="number">0</span>)</span></span>;</span><br></pre></td></tr></table></figure></li><li><p>å˜é‡å£°æ˜externï¼Œæºäº<strong>åˆ†ç¦»å¼ç¼–è¯‘</strong>æœºåˆ¶ï¼Œä¸€ä¸ªå˜é‡åªèƒ½è¢«å®šä¹‰ä¸€æ¬¡ï¼Œå¯ä»¥å£°æ˜å¤šæ¬¡</p></li><li><p>ä½œç”¨åŸŸï¼ŒåµŒå¥—ä½œç”¨åŸŸ &amp; å†…éƒ¨é‡å®šä¹‰</p></li></ul></li><li><p>å¤åˆç±»å‹</p><ul><li><p>å¼•ç”¨ï¼Œ<code>typename &amp;declaration</code>ï¼Œæµ…æ‹·è´ï¼Œç»‘å®šä¸€ä¸ªå¯¹è±¡ï¼Œ<strong>å¼•ç”¨ä¸æ˜¯å¯¹è±¡</strong></p></li><li><p>æŒ‡é’ˆï¼Œ<code>typename *declaration</code>ï¼Œå­˜æ”¾å¯¹è±¡åœ°å€</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a;</span><br><span class="line"><span class="keyword">int</span> *p, *q=a;</span><br><span class="line">p = &amp;a;</span><br><span class="line">p = q;</span><br></pre></td></tr></table></figure></li><li><p>å–åœ°å€ç¬¦&amp;</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> *p = a;</span><br><span class="line"><span class="keyword">int</span> *p = &amp;a;</span><br><span class="line"><span class="comment">// a---&gt;å¯¹è±¡ &amp;a---&gt;åœ°å€</span></span><br></pre></td></tr></table></figure></li><li><p>è§£å¼•ç”¨ç¬¦*</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a;</span><br><span class="line"><span class="keyword">int</span> *p;</span><br><span class="line">*p ---&gt; undefined</span><br><span class="line">p = &amp;a;</span><br><span class="line">*p = <span class="number">10</span>;</span><br><span class="line"><span class="comment">// p---&gt;æŒ‡é’ˆ *p---&gt;å¯¹è±¡</span></span><br></pre></td></tr></table></figure></li><li><p>void* æŒ‡é’ˆï¼Œå¯ä»¥æŒ‡å‘ä»»æ„ç±»å‹çš„å¯¹è±¡ï¼Œä½†æ˜¯ä¸èƒ½è¿›è¡Œå¯¹è±¡æ“ä½œ</p></li></ul></li><li><p>consté™å®šç¬¦</p><ul><li><p>å‚ä¸ç¼–è¯‘é¢„å¤„ç†</p></li><li><p>è¦å®ç°å¤šä¸ªæ–‡ä»¶å…±äº«ï¼Œå¿…é¡»åœ¨constå˜é‡<strong>å®šä¹‰</strong>ä¹‹å‰åŠ ä¸Šexternå…³é”®å­—</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// define</span></span><br><span class="line"><span class="keyword">extern</span> <span class="keyword">const</span> <span class="keyword">int</span> bufferSize = fcn();</span><br><span class="line"><span class="comment">// declare</span></span><br><span class="line"><span class="keyword">extern</span> <span class="keyword">const</span> <span class="keyword">int</span> bufferSize;</span><br></pre></td></tr></table></figure></li><li><p>å…è®¸ä»»æ„è¡¨è¾¾å¼ä½œä¸ºåˆå§‹å€¼ï¼ˆå…è®¸éšå¼ç±»å‹è½¬æ¢ï¼‰</p></li><li><p>å¸¸é‡å¼•ç”¨ï¼Œå…è®¸éå¸¸é‡èµ‹å€¼ï¼Œå®é™…å¼•ç”¨ä¸€ä¸ªå†…å­˜ä¸­çš„â€œä¸´æ—¶å€¼â€</p></li><li><p>æŒ‡å‘å¸¸é‡çš„æŒ‡é’ˆï¼Œå…è®¸éå¸¸é‡èµ‹å€¼ï¼Œä½†æ˜¯ä¸èƒ½é€šè¿‡è¯¥æŒ‡é’ˆä¿®æ”¹å¯¹è±¡</p></li><li><p>å¸¸é‡æŒ‡é’ˆï¼ŒæŒ‡é’ˆå§‹ç»ˆæŒ‡å‘åŒä¸€ä¸ªå¯¹è±¡</p></li><li><p>å¸¸é‡è¡¨è¾¾å¼constexprï¼Œè¡¨è¾¾å¼åœ¨ç¼–è¯‘è¿‡ç¨‹ä¸­å°±èƒ½å¾—åˆ°è®¡ç®—ç»“æœ</p></li></ul></li><li><p>å¤„ç†ç±»å‹</p><ul><li><p>ç±»å‹åˆ«åtypedef &amp; using</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ä¼ ç»Ÿ</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">double</span> base;</span><br><span class="line"><span class="keyword">typedef</span> base *p;    <span class="comment">// pæ˜¯doubleæŒ‡é’ˆ</span></span><br><span class="line">base a;</span><br><span class="line">p p1=&amp;a;</span><br><span class="line"><span class="comment">// c++11</span></span><br><span class="line"><span class="keyword">using</span> base = <span class="keyword">double</span>;</span><br></pre></td></tr></table></figure></li><li><p>autoç±»å‹è¯´æ˜ç¬¦ï¼Œè®©ç¼–è¯‘å™¨åˆ†æè¡¨è¾¾å¼æ‰€å±ç±»å‹å¹¶ä¸ºå˜é‡èµ‹å€¼</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ä¸€æ¡ç±»å‹å£°æ˜è¯­å¥ä¸­æ‰€æœ‰å˜é‡çš„ç±»å‹å¿…é¡»ä¿æŒä¸€è‡´</span></span><br><span class="line"><span class="keyword">auto</span> i=<span class="number">0</span>, *p=&amp;i;</span><br></pre></td></tr></table></figure></li><li><p>decltypeç±»å‹æŒ‡ç¤ºç¬¦ï¼Œä»…åˆ†æè¡¨è¾¾å¼è¿”å›ç±»å‹ï¼Œä¸åšèµ‹å€¼ï¼ˆå› æ­¤ä¸åšå®é™…è®¡ç®—ï¼‰</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">decltype</span>(f()) a=x;</span><br></pre></td></tr></table></figure></li></ul></li></ol><ol><li><p>string</p><ul><li><p>è¯»å–ï¼Œ<code>&gt;&gt;</code>ä¸è¯»å–ç©ºç™½ï¼Œé‡åˆ°ç©ºç™½ç¬¦åœæ­¢ï¼Œ<code>getline</code>ä¿ç•™ç©ºç™½ç¬¦ï¼Œé‡åˆ°æ¢è¡Œç¬¦åœæ­¢ã€‚</p></li><li><p>å­—ç¬¦ä¸²å­—é¢å€¼ä¸æ˜¯stringå¯¹è±¡ï¼Œè€Œæ˜¯<strong>Cé£æ ¼å­—ç¬¦ä¸²</strong>ï¼Œ<code>c_str()</code>æˆå‘˜å‡½æ•°èƒ½å¤Ÿå°†stringå¯¹è±¡è½¬åŒ–æˆCé£æ ¼å­—ç¬¦ä¸²</p></li><li><p>éå†ï¼Œ<strong>èŒƒå›´forè¯­å¥</strong>ï¼Œæ¯æ¬¡è¿­ä»£declareçš„å˜é‡ä¼šè¢«åˆå§‹åŒ–ä¸ºexpressionçš„ä¸‹ä¸€ä¸ªå…ƒç´ </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (declaration : expression)</span><br><span class="line">statement</span><br><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">str</span><span class="params">(<span class="string">"some string"</span>)</span></span>;</span><br><span class="line"><span class="comment">// èµ‹å€¼</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> c: str)</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; c &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="comment">// å¼•ç”¨</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> &amp;c: str)</span><br><span class="line">    c = <span class="built_in">toupper</span>(c);</span><br></pre></td></tr></table></figure></li><li><p><code>size()</code>è¿”å›çš„ç±»å‹æ˜¯<code>string::size_type</code>ï¼Œé€šå¸¸ç”¨auto</p></li></ul></li><li><p>vector</p><ul><li>ç±»æ¨¡ç‰ˆï¼Œ<strong>ç›¸åŒç±»å‹</strong>å¯¹è±¡çš„é›†åˆï¼Œå£°æ˜æ—¶å¿…é¡»æä¾›å…ƒç´ ç±»å‹<code>vector&lt;int&gt;</code></li><li>æ·»åŠ å…ƒç´ <code>push_back()</code></li></ul></li><li><p>è¿­ä»£å™¨</p><ul><li>æ‰€æœ‰æ ‡å‡†åº“å®¹å™¨éƒ½æ”¯æŒè¿­ä»£å™¨ï¼Œåªæœ‰å°‘æ•°æ”¯æŒä¸‹æ ‡è®¿é—®</li><li><code>begin()</code>è¿”å›æŒ‡å‘ç¬¬ä¸€ä¸ªå…ƒç´ çš„è¿­ä»£å™¨ï¼Œ<code>end()</code>è¿”å›<strong>å°¾åå…ƒç´ </strong>çš„è¿­ä»£å™¨</li><li><code>cbegin()</code>å’Œ<code>cend()</code>æ“ä½œç±»ä¼¼ï¼Œè¿”å›å€¼æ˜¯const_iteratorï¼Œä¸èƒ½ä¿®æ”¹å¯¹è±¡</li><li>è¿­ä»£å™¨çš„ç±»å‹æ˜¯<code>container::iterator</code>å’Œ<code>container::const_iterator</code>ï¼Œé€šå¸¸ç”¨auto</li><li>è§£å¼•ç”¨è¿­ä»£å™¨å¾—åˆ°å¯¹è±¡</li><li>ç®­å¤´è¿ç®—ç¬¦<code>-&gt;</code>ï¼Œç»“åˆè§£å¼•ç”¨+æˆå‘˜è®¿é—®ä¸¤ä¸ªæ“ä½œ</li><li>è¿­ä»£å™¨å¤±æ•ˆï¼šå®¹å™¨æ”¹å˜å®¹é‡</li></ul></li><li><p>æ•°ç»„</p><ul><li>å¤§å°å›ºå®šï¼Œç¼–è¯‘çš„æ—¶å€™ç»´åº¦åº”è¯¥å·²çŸ¥ï¼Œå› æ­¤å¿…é¡»æ˜¯å¸¸é‡è¡¨è¾¾å¼</li><li>ä¸èƒ½ç”¨åšæ‹·è´å’Œèµ‹å€¼</li></ul></li><li><p>è¡¨è¾¾å¼</p><ul><li><p>å·¦å€¼å’Œå³å€¼</p><p>â€‹    Cè¯­è¨€ä¸­ï¼Œ<strong>å·¦å€¼</strong>æŒ‡çš„æ˜¯æ—¢èƒ½å‡ºç°åœ¨ç­‰å·å·¦è¾¹ä¹Ÿèƒ½å‡ºç°åœ¨ç­‰å·å³è¾¹çš„å˜é‡æˆ–è¡¨è¾¾å¼ï¼Œé€šå¸¸æ¥è¯´å°±æ˜¯æœ‰åå­—çš„å˜é‡ï¼Œè€Œ<strong>å³å€¼</strong>åªèƒ½å‡ºç°åœ¨ç­‰å·å³ä¾§ï¼Œé€šå¸¸å°±æ˜¯ä¸€äº›æ²¡æœ‰åå­—ä¹Ÿå–ä¸åˆ°åœ°å€çš„ä¸­é—´ç»“æœã€‚</p><p>ç»§æ‰¿åˆ°C++ä¸­å½’çº³æ¥è®²å°±æ˜¯ï¼šå½“ä¸€ä¸ªå¯¹è±¡è¢«ç”¨ä½œå³å€¼çš„æ—¶å€™ï¼Œç”¨çš„æ˜¯å¯¹è±¡çš„å€¼ï¼ˆå†…å®¹ï¼‰ï¼Œå½“è¢«ç”¨ä½œå·¦å€¼çš„æ—¶å€™ï¼Œç”¨çš„æ˜¯å¯¹è±¡çš„èº«ä»½ï¼ˆåœ¨å†…å­˜ä¸­çš„ä½ç½®ï¼‰ã€‚</p></li><li><p>æ±‚å€¼é¡ºåº</p><blockquote><p>æœ‰å››ç§è¿ç®—ç¬¦æ˜ç¡®è§„å®šäº†æ±‚å€¼é¡ºåºï¼Œé€»è¾‘ä¸ï¼ˆ&amp;&amp;ï¼‰ã€é€»è¾‘æˆ–ï¼ˆ||ï¼‰ã€æ¡ä»¶ï¼ˆ?:ï¼‰ã€é€—å·ï¼ˆ,ï¼‰è¿ç®—ç¬¦ã€‚</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; i &lt;&lt; ++i &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure></li><li><p>å‰ç½®ç‰ˆæœ¬å’Œåç½®ç‰ˆæœ¬çš„é€’å¢é€’å‡</p><p>ç”¨äºå¤åˆè¿ç®—ä¸­æ—¶ï¼Œ</p><p>å‰ç½®ç‰ˆæœ¬é¦–å…ˆä¿®æ”¹å¯¹è±¡ï¼Œç„¶åå°†å¯¹è±¡æœ¬èº«ä½œä¸ºå·¦å€¼è¿”å›ã€‚</p><p>åç½®ç‰ˆæœ¬å°†å¯¹è±¡<strong>åŸå§‹å€¼çš„å‰¯æœ¬</strong>ä½œä¸ºå³å€¼è¿”å›ã€‚</p></li><li><p>ä½è¿ç®—</p><ul><li>æ•´å½¢æå‡ï¼Œchar8-&gt;int32</li><li>æ·»0ï¼Œè¶Šç•Œä¸¢å¼ƒ</li></ul></li><li><p>é€—å·è¿ç®—ç¬¦ï¼šå«æœ‰ä¸¤ä¸ªè¿ç®—å¯¹è±¡ï¼Œé¦–å…ˆå¯¹å·¦è¡¨è¾¾å¼æ±‚å€¼ï¼Œç„¶åå°†æ±‚å€¼ç»“æœä¸¢å¼ƒæ‰ï¼Œæœ€å³è¾¹çš„è¡¨è¾¾å¼çš„å€¼å°†ä½œä¸ºæ•´ä¸ªé€—å·è¡¨è¾¾å¼çš„å€¼ã€‚æœ¬è´¨ä¸Šï¼Œé€—å·çš„ä½œç”¨æ˜¯å¯¼è‡´ä¸€äº›åˆ—è¿ç®—è¢«é¡ºåºæ‰§è¡Œã€‚</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// åˆ†åˆ«å¯¹é€—å·è¡¨è¾¾å¼å†…å¯¹è±¡èµ‹å€¼ï¼Œç„¶åè¿”å›æœ€å³cntçš„å€¼</span></span><br><span class="line">var = (count=<span class="number">19</span>, incr=<span class="number">10</span>, cnt++)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>å‡½æ•°</p></li></ol><ul><li><p>å±€éƒ¨é™æ€å¯¹è±¡staticï¼šé¦–æ¬¡è°ƒç”¨æ—¶è¢«åˆå§‹åŒ–ï¼Œç›´åˆ°ç¨‹åºç»ˆæ­¢æ‰è¢«é”€æ¯ã€‚</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// åªåˆå§‹åŒ–ä¸€æ¬¡ï¼Œå‡½æ•°è°ƒç”¨ç»“æŸä»¥åè¿™ä¸ªå€¼ä»æœ‰æ•ˆ</span></span><br><span class="line">    <span class="keyword">static</span> cnt = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">return</span> ++cnt;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>å‚æ•°ä¼ é€’ï¼šå¦‚æœå½¢å‚è¢«å£°æ˜ä¸ºå¼•ç”¨ç±»å‹ï¼Œå®ƒå°†<strong>ç»‘å®š</strong>åˆ°å¯¹åº”çš„å®å‚ä¸Šï¼ˆä¼ å¼•ç”¨è°ƒç”¨ï¼‰ï¼Œå¦åˆ™å°†å®å‚çš„å€¼<strong>æ‹·è´</strong>åèµ‹ç»™å½¢å‚ï¼ˆä¼ å€¼è°ƒç”¨ï¼‰ã€‚</p></li><li><p>å«æœ‰å¯å˜å½¢å‚çš„å‡½æ•°</p><ul><li><p>æ‰€æœ‰å®å‚ç±»å‹ç›¸åŒï¼Œå¯ä»¥ä½¿ç”¨initializer_listæ¨¡ç‰ˆç±»å‹çš„å½¢å‚ï¼Œåˆ—è¡¨ä¸­å…ƒç´ æ˜¯constã€‚</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">initializer_list</span>&lt;T&gt; lst;</span><br><span class="line"><span class="built_in">initializer_list</span>&lt;T&gt; lst&#123;a, b, c, ...&#125;;</span><br></pre></td></tr></table></figure></li><li><p>ç¼–å†™å¯å˜å‚æ•°<strong>æ¨¡ç‰ˆ</strong></p></li><li><p>çœç•¥ç¬¦å½¢å‚ï¼šå¯¹åº”çš„å®å‚æ— éœ€ç±»å‹æ£€æŸ¥</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// å¸¦éƒ¨åˆ†å½¢å‚ç±»å‹</span><br><span class="line">void foo(parm_list, ...);</span><br><span class="line"></span><br><span class="line">void foo(...);</span><br></pre></td></tr></table></figure></li></ul></li><li><p>å†…è”å‡½æ•°inclineï¼šé¿å…å‡½æ•°è°ƒç”¨å¼€é”€</p></li><li><p>è°ƒè¯•å¸®åŠ©</p><ul><li>NDEBUGé¢„å¤„ç†å˜é‡ï¼šç”¨äºå…³é—­è°ƒè¯•çŠ¶æ€ï¼Œassertå°†è·³è¿‡ä¸æ‰§è¡Œã€‚</li><li>assert (expr) é¢„å¤„ç†å®ï¼šå¦‚æœè¡¨è¾¾å¼ä¸ºå‡ï¼Œassertè¾“å‡ºä¿¡æ¯å¹¶ç»ˆæ­¢ç¨‹åºã€‚</li></ul><blockquote><p>é¢„å¤„ç†åå­—ç”±é¢„å¤„ç†è€Œéç¼–è¯‘å™¨ç®¡ç†ï¼Œå› æ­¤å¯ä»¥ç›´æ¥ä½¿ç”¨åå­—è€Œæ— é¡»æä¾›usingå£°æ˜ã€‚</p></blockquote></li></ul><ol><li>static_castå’Œdynamic_castå¼ºåˆ¶ç±»å‹è½¬æ¢<ul><li>static_cast \<type-id> (expression)ï¼šæš´åŠ›ç±»å‹è½¬æ¢ï¼Œä¸è¿è¡Œç±»å‹æ£€æŸ¥ã€‚</type-id></li><li>dynamic_cast\<type-id> (expression)ï¼šè¿è¡Œç±»å‹æ£€æŸ¥ï¼Œä¸‹è¡Œè½¬æ¢å®‰å…¨ã€‚</type-id></li></ul></li><li>new &amp; deleteï¼šnew [] è¦å’Œ delete []å¯¹åº”ä¸Šã€‚</li><li>c++çš„oopç‰¹æ€§ï¼ˆprivate publicâ€¦ï¼‰åªåœ¨ç¼–è¯‘æ—¶åˆ»æœ‰æ„ä¹‰ã€‚åŒä¸€ç±»çš„å¯¹è±¡å¯ä»¥äº’ç›¸è®¿é—®ç§æœ‰æˆå‘˜ã€‚</li><li>firendï¼šæ³¨æ„æ–¹å‘æ˜¯give acess toï¼Œæˆæƒfriendè®¿é—®è‡ªå·±çš„privateã€‚ç¼–è¯‘æ—¶åˆ»æ£€æŸ¥ã€‚</li><li>compositionï¼šç»„åˆï¼Œç”¨ä¸€ç³»åˆ—å¯¹è±¡æ„é€ å¯¹è±¡ã€‚</li><li>inheritanceï¼šç»§æ‰¿ï¼Œç”¨ä¸€äº›ç±»æ¥æ„é€ æ–°çš„ç±»ã€‚</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span> :</span> <span class="keyword">public</span> A&#123;</span><br><span class="line">    ....</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>æ„é€ ï¼šå­ç±»æ„é€ çš„æ—¶å€™è¦å…ˆæ„é€ çˆ¶ç±»ï¼Œææ„çš„æ—¶å€™åè¿‡æ¥ï¼Œå…ˆææ„å­ç±»ã€‚</p><p>é‡åï¼šname hidingï¼Œspecial for c++ã€‚</p><ol><li>â€‹    protectedï¼šdesigned for sub classã€‚å­ç±»å¯ä»¥ç›´æ¥è®¿é—®ã€‚å…¶ä»–ç±»çœ‹ä¸åˆ°ã€‚</li><li>overloadï¼šå‚æ•°è¡¨å¿…é¡»ä¸åŒï¼Œå¦åˆ™ç¼–è¯‘å™¨æ— æ³•è¯†åˆ«ã€‚</li><li>default argumentï¼šdefaults must be added from right to leftã€‚must be declared in .h filesã€‚å‘ç”Ÿåœ¨ç¼–è¯‘æ—¶åˆ»ã€‚</li><li>inlineï¼šä¸ç”¨çœŸæ­£è°ƒç”¨å‡½æ•°ï¼Œè€Œæ˜¯ç›´æ¥æ’å…¥æ±‡ç¼–ä»£ç æ®µã€‚tradeoff between space and time consumingã€‚åŒºåˆ«äºå®ï¼Œå®æ˜¯æ²¡æœ‰ç±»å‹æ£€æŸ¥çš„ã€‚</li><li><p>const</p><ul><li>declare a variableï¼šæ˜¯å˜é‡ï¼Œè€Œä¸æ˜¯å¸¸æ•°</li><li></li></ul></li></ol>]]></content>
      
      
        <tags>
            
            <tag> basic </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>cmake for record</title>
      <link href="/2018/05/08/cmake/"/>
      <url>/2018/05/08/cmake/</url>
      <content type="html"><![CDATA[<h4 id="0-å˜é‡"><a href="#0-å˜é‡" class="headerlink" title="0. å˜é‡"></a>0. å˜é‡</h4><p>å˜é‡ä½¿ç”¨ ${ } çš„æ–¹å¼å–å€¼ï¼Œä½†æ˜¯åœ¨ifæ§åˆ¶è¯­å¥ä¸­ç›´æ¥ä½¿ç”¨å˜é‡åã€‚</p><h4 id="1-project"><a href="#1-project" class="headerlink" title="1. project"></a>1. project</h4><p>project ( project_name [CXX] [C] [Java] )</p><p>ç”¨æ¥æŒ‡å®šå·¥ç¨‹åç§°å’Œå·¥ç¨‹è¯­è¨€ï¼ˆå¯çœç•¥ï¼‰ï¼ŒæŒ‡ä»¤éšå¼å®šä¹‰äº†projectname_BINARY_DIRå’Œprojectname_SOURCE_DIRä¸¤ä¸ªå˜é‡ï¼ˆå†™åœ¨cmake_cacheé‡Œé¢ï¼‰ï¼ŒæŒ‡çš„æ˜¯<strong>ç¼–è¯‘å‘ç”Ÿ</strong>çš„å½“å‰ç›®å½•ã€‚</p><h4 id="2-set"><a href="#2-set" class="headerlink" title="2. set"></a>2. set</h4><p>set ( VAR  [VALUE] )</p><p>ç”¨æ¥æ˜¾å¼å®šä¹‰å˜é‡ï¼Œå¦‚set (SRC_LIST main.c t1.c t2.c) ã€‚ï¼ˆç«Ÿç„¶ä¸ç”¨å¥—æ‹¬å·ï¼Ÿï¼‰</p><h4 id="3-message"><a href="#3-message" class="headerlink" title="3. message"></a>3. message</h4><p>message ( [SEND_ERROR | STATUS | FATAL_ERROR]  â€œmessage to displayâ€  VAR ) </p><p>ç”¨æ¥å‘ç»ˆç«¯è¾“å‡ºç”¨æˆ·å®šä¹‰çš„ä¿¡æ¯ã€‚</p><h4 id="4-add-executable"><a href="#4-add-executable" class="headerlink" title="4. add_executable"></a>4. add_executable</h4><p>add_executable ( executable_filename  [source_filename] )</p><p>ç”Ÿæˆåå­—ä¸ºexecutable_filenameçš„å¯æ‰§è¡Œæ–‡ä»¶ï¼Œç›¸å…³çš„æºæ–‡ä»¶ [source_filename] å¯ä»¥æ˜¯ä¸€ä¸ªæºæ–‡ä»¶åˆ—è¡¨ã€‚</p><h4 id="5-æ¸…ç†æ„å»ºç»“æœ"><a href="#5-æ¸…ç†æ„å»ºç»“æœ" class="headerlink" title="5. æ¸…ç†æ„å»ºç»“æœ"></a>5. æ¸…ç†æ„å»ºç»“æœ</h4><p>make clean</p><p>å¯¹æ„å»ºå‡ºçš„å¯æ‰§è¡Œæ–‡ä»¶è¿›è¡Œæ¸…ç†ã€‚</p><h4 id="6-å¤–éƒ¨æ„å»º"><a href="#6-å¤–éƒ¨æ„å»º" class="headerlink" title="6. å¤–éƒ¨æ„å»º"></a>6. å¤–éƒ¨æ„å»º</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br></pre></td></tr></table></figure><p>æ‰€æœ‰ç¼–è¯‘åŠ¨ä½œå‘ç”Ÿåœ¨ç¼–è¯‘ç›®å½•ï¼Œå¯¹åŸæœ‰å·¥ç¨‹æ²¡æœ‰ä»»ä½•å½±å“ã€‚</p><h4 id="7-add-subdirectory"><a href="#7-add-subdirectory" class="headerlink" title="7. add_subdirectory"></a>7. add_subdirectory</h4><p>add_subdirectory ( source_dir  [binary_dir]   [EXCLUDE_FROM_ALL] )</p><p>å‘å½“å‰å·¥ç¨‹ç›®å½•æ·»åŠ å­˜æ”¾æºæ–‡ä»¶çš„å­ç›®å½•source_dirï¼Œå¹¶æŒ‡å®šå­˜æ”¾ä¸­é—´äºŒè¿›åˆ¶æ–‡ä»¶å’Œç›®æ ‡äºŒè¿›åˆ¶æ–‡ä»¶çš„ä½ç½®binary_dirã€‚æŒ‡ä»¤éšå¼ä¿®æ”¹ EXECUTABLE_OUTPUT_PATH å’Œ LIBRARY_OUTPUT_PATH ä¸¤ä¸ªå˜é‡ã€‚</p><h4 id="8-æ›´åŠ åƒä¸€ä¸ªå·¥ç¨‹"><a href="#8-æ›´åŠ åƒä¸€ä¸ªå·¥ç¨‹" class="headerlink" title="8. æ›´åŠ åƒä¸€ä¸ªå·¥ç¨‹"></a>8. æ›´åŠ åƒä¸€ä¸ªå·¥ç¨‹</h4><ul><li>åˆ›å»ºå·¥ç¨‹æ ¹ç›®å½•ï¼Œåˆ›å»ºCMakeLists.txtã€‚</li></ul><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># æŒ‡å®šæœ€ä½ç¼–è¯‘ç‰ˆæœ¬</span></span><br><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.7</span>)</span><br><span class="line"><span class="comment"># æŒ‡å®šå·¥ç¨‹åå­—</span></span><br><span class="line"><span class="keyword">PROJECT</span>(HELLO)</span><br><span class="line"><span class="comment"># æµ‹è¯•ç±»æ‰“å°ä¿¡æ¯</span></span><br><span class="line"><span class="keyword">MESSAGE</span>(STATUS <span class="string">"This is BINARY dir "</span> <span class="variable">$&#123;HELLO_BINARY_DIR&#125;</span>)</span><br><span class="line"><span class="keyword">MESSAGE</span>(STATUS <span class="string">"This is SOURCE dir "</span> <span class="variable">$&#123;HELLO_SOURCE_DIR&#125;</span>)</span><br></pre></td></tr></table></figure><ul><li>æ·»åŠ å­ç›®å½•srcï¼Œç”¨æ¥å­˜æ”¾æºæ–‡ä»¶ï¼Œä¸ºå­ç›®å½•åˆ›å»ºCMakeLists.txtã€‚</li></ul><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># åœ¨æ ¹ç›®å½•CMakeLists.txtä¸­æ·»åŠ å­ç›®å½•å£°æ˜</span></span><br><span class="line"><span class="keyword">add_subdirectory</span>(src bin)</span><br><span class="line"><span class="comment"># ç¼–è¯‘äº§ç”Ÿçš„ä¸­é—´æ–‡ä»¶ä»¥åŠç›®æ ‡æ–‡ä»¶å°†ä¿å­˜åœ¨ç¼–è¯‘æ–‡ä»¶å¤¹çš„binå­ç›®å½•ä¸‹</span></span><br></pre></td></tr></table></figure><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ç¼–å†™å½“å‰å­ç›®å½•çš„CMakeLists.txt</span></span><br><span class="line"><span class="keyword">add_executable</span>(hello main.c)</span><br><span class="line"><span class="comment"># ä¿®æ”¹æœ€ç»ˆç”Ÿæˆçš„å¯æ‰§è¡Œæ–‡ä»¶ä»¥åŠåº“çš„è·¯å¾„ï¼Œè¿™ä¸¤ä¸ªæŒ‡ä»¤è¦è¿½éšå¯¹åº”çš„add_executable()å’Œadd_library()æŒ‡ä»¤</span></span><br><span class="line"><span class="keyword">set</span>(EXECUTABLE_OUTPUT_PATH <span class="variable">$&#123;PROJECT_BINARY_PATH&#125;</span>/bin)</span><br><span class="line"><span class="keyword">set</span>(LIBRARY_OUTPUT_PATH <span class="variable">$&#123;PROJECT_BINARY_DIR&#125;</span>/lib)</span><br></pre></td></tr></table></figure><ul><li>æ·»åŠ å­ç›®å½•buildï¼Œä½œä¸ºå¤–éƒ¨ç¼–è¯‘æ–‡ä»¶å¤¹ï¼ˆ ${PROJECT_BINARY_DIR} ï¼‰ï¼Œå­˜æ”¾ç¼–è¯‘çš„è¿‡ç¨‹å’Œç›®æ ‡æ–‡ä»¶ã€‚    </li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br></pre></td></tr></table></figure><ul><li>æ·»åŠ å­ç›®å½•docï¼Œç”¨æ¥å­˜æ”¾å·¥ç¨‹æ–‡æ¡£hello.txtã€‚</li><li>æ·»åŠ æ–‡æœ¬æ–‡ä»¶READMEï¼ŒCOPYRIGHTã€‚</li><li>æ·»åŠ runhello.shè„šæœ¬ï¼Œç”¨æ¥è°ƒç”¨å¯æ‰§è¡Œæ–‡ä»¶helloã€‚</li></ul><h4 id="9-æ‰“åŒ…å®‰è£…"><a href="#9-æ‰“åŒ…å®‰è£…" class="headerlink" title="9. æ‰“åŒ…å®‰è£…"></a>9. æ‰“åŒ…å®‰è£…</h4><ul><li>åœ¨æ ¹ç›®å½•çš„CMakeList.txtä¸­æ·»åŠ å®‰è£…ä¿¡æ¯</li></ul><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># å®‰è£…COPYRIGHT/READMEåˆ°&lt;prefix&gt;/share/doc/cmake/t2</span></span><br><span class="line"><span class="keyword">INSTALL</span>(FILES COPYRIGHT README DESTINATION share/doc/cmake/t2)</span><br><span class="line"><span class="comment"># å®‰è£…runhello.shåˆ°&lt;prefix&gt;/bin</span></span><br><span class="line"><span class="keyword">INSTALL</span>(PROGRAMS runhello.sh DESTINATION bin)</span><br><span class="line"><span class="comment"># å®‰è£…å·¥ç¨‹æ–‡æ¡£åˆ°&lt;prefix&gt;/share/doc/cmake/t2</span></span><br><span class="line"><span class="keyword">INSTALL</span>(DIRECTORY doc/ DESTINATION share/doc/cmake/t2)</span><br></pre></td></tr></table></figure><ul><li>åœ¨å­ç›®å½•çš„CMakeList.txtä¸­æ·»åŠ å®‰è£…ä¿¡æ¯</li></ul><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># å®‰è£…è„šæœ¬è¦è°ƒç”¨çš„å¯æ‰§è¡Œæ–‡ä»¶helloåˆ°&lt;prefix&gt;/binï¼Œ</span></span><br><span class="line"><span class="comment"># æ³¨æ„install(targets)æŒ‡ä»¤ä¹Ÿè¦è¿½éšå¯¹åº”add_executable()å’Œadd_library()æŒ‡ä»¤çš„è·¯å¾„</span></span><br><span class="line"><span class="keyword">INSTALL</span>(TARGETS hello</span><br><span class="line">RUNTIME DESTINATION bin)</span><br></pre></td></tr></table></figure><ul><li>å®‰è£…ç¨‹åºåŒ…</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cd build</span><br><span class="line"><span class="meta">#</span><span class="bash"> åœ¨cmakeå‘½ä»¤ä¸­æŒ‡æ˜å®‰è£…ç›®å½•çš„å‰ç¼€&lt;prefix&gt;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> CMAKE_INSTALL_PREFIX é»˜è®¤æ˜¯/usr/<span class="built_in">local</span></span></span><br><span class="line">cmake -DCMAKE_INSTALL_PREFIX=/Users/carrol/tmp ..</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> æŸ¥çœ‹ç›®æ ‡æ–‡ä»¶å¤¹</span></span><br><span class="line">j tmp</span><br><span class="line">tree -a</span><br></pre></td></tr></table></figure><h4 id="10-add-library"><a href="#10-add-library" class="headerlink" title="10. add_library"></a>10. add_library</h4><p>add_library ( name  [SHARED | STATIC | MODULE]  [source_filename] )</p><p>ç”Ÿæˆåå­—ä¸º<strong>libname.X</strong>çš„åº“æ–‡ä»¶ã€‚</p><ul><li>SHAREDï¼ŒåŠ¨æ€åº“ï¼Œlibname.dylib</li><li>STATICï¼Œé™æ€åº“ï¼Œlibname.a</li></ul><p><strong>è®¾ç½®ç›®æ ‡åŠ¨æ€åº“å’Œé™æ€åº“åŒå set_target_properties</strong></p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># è®¾ç½®ç›®æ ‡åŠ¨é™æ€åº“åŒå</span></span><br><span class="line"><span class="keyword">add_library</span>(hello SHARED hello.c)</span><br><span class="line"><span class="keyword">add_library</span>(hello_static hello.c)</span><br><span class="line"><span class="keyword">set_target_properties</span>(hello_static</span><br><span class="line">                      PROPERTIES OUTPUT_NAME hello)</span><br></pre></td></tr></table></figure><p><strong>é˜²æ­¢æ„å»ºä¸­æ¸…ç†åŒåæ–‡ä»¶ set_target_properties</strong></p><p>cmakeåœ¨æ„å»ºä¸€ä¸ªtargetæ—¶ï¼Œä¼šå°è¯•æ¸…ç†æ‰å…¶ä»–ä½¿ç”¨è¿™ä¸ªåå­—çš„åº“â€”â€”åœ¨æ„å»ºlibhello.aæ—¶ä¼šæ¸…ç†æ‰libhello.dylibã€‚</p><blockquote><p>æˆ‘å®é™…æ“ä½œæ—¶å€™ä¼šä¿ç•™ä¸¤ä¸ªåº“æ–‡ä»¶ï¼Œä½†æ˜¯åœ¨ä½œä¸ºç¬¬ä¸‰æ–¹è¢«å¼•ç”¨çš„æ—¶å€™ä¼šæŠ¥é”™ï¼š</p><p>dyld: Library not loaded: libhello.dylib</p><p>Reason: image not found</p></blockquote><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SET_TARGET_PROPERTIES</span>(hello </span><br><span class="line">                      PROPERTIES CLEAN_DIRECT_OUTPUT <span class="number">1</span>)</span><br><span class="line"><span class="keyword">SET_TARGET_PROPERTIES</span>(hello_static </span><br><span class="line">                      PROPERTIES CLEAN_DIRECT_OUTPUT <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><strong>è®¾ç½®åŠ¨æ€ç‰ˆæœ¬å· set_target_properties</strong></p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># è®¾ç½®åŠ¨æ€åº“ç‰ˆæœ¬å·</span></span><br><span class="line"><span class="keyword">set_target_properties</span>(hello</span><br><span class="line">                      PROPERTIES VERSION <span class="number">1.2</span></span><br><span class="line">                      SOVERSION <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>ç¼–è¯‘æ–‡ä»¶å¤¹ä¸‹ç”Ÿæˆäº†libhello.1.2.dylibã€libhello.1.dylibã€libhello.dylibä¸‰ä¸ªåŠ¨æ€åº“æ–‡ä»¶ï¼Œåªæœ‰ä¸€ä¸ªæ˜¯çœŸçš„ï¼Œå¦å¤–ä¸¤ä¸ªæ˜¯æ›¿èº«ã€‚</p><p><strong>å®‰è£…å…±äº«åº“å’Œå¤´æ–‡ä»¶</strong></p><p>ä¿®æ”¹åº“çš„æºæ–‡ä»¶å¤¹ä¸‹çš„CMakeLIsts.txt</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># åº“æ–‡ä»¶</span></span><br><span class="line"><span class="keyword">install</span>(TARGETS hello hello_static</span><br><span class="line">        ARCHIVE DESTINATION lib   //é™æ€åº“</span><br><span class="line">        LIBRARY DESTINATION lib)  //åŠ¨æ€åº“</span><br><span class="line"><span class="comment"># å¤´æ–‡ä»¶</span></span><br><span class="line"><span class="keyword">install</span>(FILES hello.h DESTINATION <span class="keyword">include</span>/hello)</span><br></pre></td></tr></table></figure><h4 id="11-include-directories"><a href="#11-include-directories" class="headerlink" title="11. include_directories"></a>11. include_directories</h4><p>include_directories( dir1 dir2 â€¦ )</p><p>ç”¨æ¥å‘å·¥ç¨‹æ·»åŠ å¤šä¸ªç‰¹å®šçš„<strong>å¤´æ–‡ä»¶æœç´¢è·¯å¾„</strong></p><h4 id="12-link-directories-amp-target-link-libraries"><a href="#12-link-directories-amp-target-link-libraries" class="headerlink" title="12. link_directories &amp; target_link_libraries"></a>12. link_directories &amp; target_link_libraries</h4><p>link_directories( dir1 dir2 â€¦ )</p><p>æ·»åŠ éæ ‡å‡†çš„<strong>å…±äº«åº“æœç´¢è·¯å¾„</strong></p><p>target_link_libraries( target lib1 lib2 â€¦ )</p><p>ç”¨æ¥ä¸ºç›®æ ‡targetæ·»åŠ éœ€è¦é“¾æ¥çš„å…±äº«åº“ï¼Œtargetå¯ä»¥æ˜¯ä¸€ä¸ªå¯æ‰§è¡Œæ–‡ä»¶ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªåº“æ–‡ä»¶ã€‚</p><p><strong>æŸ¥çœ‹ç”Ÿæˆç›®æ ‡çš„åº“ä¾èµ–æƒ…å†µ</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ç”Ÿæˆçš„ç›®æ ‡å¯æ‰§è¡Œæ–‡ä»¶ä¸ºmain</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">for</span> OSX</span></span><br><span class="line">otool -L main</span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">for</span> linux</span></span><br><span class="line">ldd main</span><br></pre></td></tr></table></figure><blockquote><p>åªèƒ½åˆ—å‡ºåŠ¨æ€åº“ã€‚</p></blockquote><h4 id="13-å¸¸ç”¨å˜é‡"><a href="#13-å¸¸ç”¨å˜é‡" class="headerlink" title="13. å¸¸ç”¨å˜é‡"></a>13. å¸¸ç”¨å˜é‡</h4><p><strong>PROJECT_BINARY_DIR</strong>ï¼šç¼–è¯‘å‘ç”Ÿçš„ç›®å½•</p><p><strong>PROJECT_SOURCE_DIR</strong>ï¼šå·¥ç¨‹é¡¶å±‚ç›®å½•</p><p><strong>CMAKE_CURRENT_SOURCE_DIR</strong>ï¼šå½“å‰CMakeLists.txtæ‰€åœ¨ç›®å½•</p><p><strong>CMAKE_MODULE_PATH</strong>ï¼šè‡ªå®šä¹‰çš„<strong>cmakeæ¨¡å—</strong>æ‰€åœ¨è·¯å¾„</p><p><strong>LIBRARY_OUTPUT_PATH</strong>ï¼šé‡å®šä¹‰ç›®æ ‡åº“æ–‡ä»¶å­˜æ”¾ç›®å½•</p><p><strong>EXECUTABLE_OUTPUT_PATH</strong>ï¼šé‡å®šä¹‰ç›®æ ‡å¯æ‰§è¡Œæ–‡ä»¶å­˜æ”¾ç›®å½•</p><h4 id="14-findNAME-cmakeæ¨¡å—"><a href="#14-findNAME-cmakeæ¨¡å—" class="headerlink" title="14. findNAME.cmakeæ¨¡å—"></a>14. findNAME.cmakeæ¨¡å—</h4><ol><li>åœ¨å·¥ç¨‹ç›®å½•ä¸­åˆ›å»ºcmakeæ–‡ä»¶å¤¹ï¼Œå¹¶åˆ›å»ºFindHELLO.cmakeæ¨¡å—ï¼š</li></ol><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ç¤ºä¾‹</span></span><br><span class="line"><span class="keyword">FIND_PATH</span>(HELLO_INCLUDE_DIR hello.h /usr/local/<span class="keyword">include</span>/hello)</span><br><span class="line"><span class="keyword">FIND_LIBRARY</span>(HELLO_LIBRARY hello /usr/local/lib)</span><br><span class="line"><span class="keyword">IF</span> (HELLO_INCLUDE_DIR <span class="keyword">AND</span> HELLO_LIBRARY)</span><br><span class="line"><span class="keyword">SET</span>(HELLO_FOUND <span class="keyword">TRUE</span>)</span><br><span class="line"><span class="keyword">ENDIF</span> (HELLO_INCLUDE_DIR <span class="keyword">AND</span> HELLO_LIBRARY)</span><br><span class="line"><span class="keyword">IF</span> (HELLO_FOUND)</span><br><span class="line"><span class="keyword">IF</span> (NOT HELLO_FIND_QUIETLY)</span><br><span class="line"><span class="keyword">MESSAGE</span>(STATUS <span class="string">"Found Hello: $&#123;HELLO_LIBRARY&#125;"</span>)</span><br><span class="line"><span class="keyword">ENDIF</span> (NOT HELLO_FIND_QUIETLY)</span><br><span class="line"><span class="keyword">ELSE</span> (HELLO_FOUND)</span><br><span class="line"><span class="keyword">IF</span> (HELLO_FIND_REQUIRED)</span><br><span class="line"><span class="keyword">MESSAGE</span>(FATAL_ERROR <span class="string">"Could not find hello library"</span>)</span><br><span class="line"><span class="keyword">ENDIF</span> (HELLO_FIND_REQUIRED)</span><br><span class="line"><span class="keyword">ENDIF</span> (HELLO_FOUND)</span><br></pre></td></tr></table></figure><ol><li>åœ¨ä¸»ç›®å½•CMakeLists.txtä¸­æ·»åŠ cmakeæ¨¡å—æ‰€åœ¨è·¯å¾„ï¼š</li></ol><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ä¸ºfind_package()æŒ‡ä»¤æˆåŠŸæ‰§è¡Œ</span></span><br><span class="line"><span class="keyword">set</span>(CMAKE_MODULE_PATH <span class="variable">$&#123;PROJECT_SOURCE_DIR&#125;</span>/cmake)</span><br></pre></td></tr></table></figure><ol><li>ç„¶åå°±å¯ä»¥åœ¨æºæ–‡ä»¶CMakeLists.txtä¸­è°ƒç”¨ find_packageï¼š</li></ol><p>find_package ( name  [QUIET]  [REQUIRED] )</p><p>ç”¨æ¥è°ƒç”¨é¢„å®šä¹‰åœ¨<strong>CMAKE_MODULE_PATH</strong>ä¸‹çš„Find\<name>.cmakeæ¨¡å—ã€‚</name></p><p>æ¯ä¸€ä¸ªæ¨¡å—éƒ½ä¼šå®šä¹‰ä»¥ä¸‹å‡ ä¸ªå˜é‡ï¼š</p><ul><li>NAME_FOUND</li><li>NAME_INCLUDE_DIR or NAME_INCLUDES </li><li>NAME_LIBRARY or NAME_LIBRARIES </li></ul><p>æ ¹æ®æŒ‡ä»¤åé¢çš„å‚æ•°è¿˜ä¼šæœ‰ä»¥ä¸‹å˜é‡ï¼š</p><ul><li><p>NAME_FIND_QUIETLYï¼Œå¦‚æœæŒ‡å®šäº†QUIETå‚æ•°ï¼Œå°±ä¸ä¼šæ‰§è¡Œå¦‚ä¸‹è¯­å¥ï¼š</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">MESSAGE</span>(STATUS <span class="string">"Found Hello: $&#123;NAME_LIBRARY&#125;"</span>)</span><br></pre></td></tr></table></figure></li><li><p>NAME_FIND_REQUIREDï¼Œå¦‚æœæŒ‡å®šäº†REQUIREDå‚æ•°ï¼Œå°±æ˜¯æŒ‡è¿™ä¸ªå…±äº«åº“æ˜¯å·¥ç¨‹å¿…é¡»çš„ï¼Œå¦‚æœæ‰¾ä¸åˆ°ï¼Œå·¥ç¨‹å°±ä¸èƒ½ç¼–è¯‘ï¼Œå¯¹åº”åœ°ä¼šæ‰§è¡Œå¦‚ä¸‹è¯­å¥ï¼š</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">MESSAGE</span>(FATAL_ERROR <span class="string">"Could not find NAME library"</span>)</span><br></pre></td></tr></table></figure></li></ul><p>å¯ä»¥é€šè¿‡\<name>_FOUNDåˆ¤æ–­æ¨¡å—æ˜¯å¦è¢«æ‰¾åˆ°ï¼Œå¹¶æ‰§è¡Œä¸åŒçš„æ“ä½œï¼ˆå¦‚æ·»åŠ éæ ‡å‡†è·¯å¾„ã€è¾“å‡ºé”™è¯¯ä¿¡æ¯ç­‰ï¼‰ã€‚</name></p><h4 id="15-find-æŒ‡ä»¤"><a href="#15-find-æŒ‡ä»¤" class="headerlink" title="15. find_æŒ‡ä»¤"></a>15. find_æŒ‡ä»¤</h4><ul><li><p><strong>find_path</strong></p><p>find_path ( VAR name1 path1 path2 â€¦ )</p><p>VARå˜é‡ä»£è¡¨<strong>åŒ…å«</strong>name1æ–‡ä»¶çš„<strong>è·¯å¾„</strong>â€”â€”è·¯å¾„ã€‚</p></li><li><p><strong>find_library</strong></p><p>find_library ( VAR name1 path1 path2 â€¦)    </p><p>VARå˜é‡åŒ…å«æ‰¾åˆ°çš„åº“çš„<strong>å…¨è·¯å¾„</strong>ï¼Œ<strong>åŒ…æ‹¬åº“æ–‡ä»¶å</strong>â€”â€”è·¯å¾„ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ã€‚</p></li></ul>]]></content>
      
      
        <tags>
            
            <tag> basic </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ICP, Iterative Closest Points</title>
      <link href="/2018/05/05/ICP-Iterative-Closest-Points/"/>
      <url>/2018/05/05/ICP-Iterative-Closest-Points/</url>
      <content type="html"><![CDATA[<h3 id="1-åŸºæœ¬å®ç°"><a href="#1-åŸºæœ¬å®ç°" class="headerlink" title="1 åŸºæœ¬å®ç°"></a>1 åŸºæœ¬å®ç°</h3><p>æ•°æ®ç‚¹äº‘é…å‡†ï¼Œæœ€ç»å…¸çš„æ–¹æ³•å°±æ˜¯ICPè¿­ä»£æœ€è¿‘ç‚¹æ³•ã€‚</p><ul><li><strong>æœ€è¿‘ç‚¹</strong>ï¼šæ¬§å‡ é‡Œå¾—æ„ä¹‰ä¸Šè·ç¦»æœ€è¿‘çš„ç‚¹ã€‚</li><li><strong>è¿­ä»£</strong>ï¼šè¿­ä»£ç›®æ ‡æ˜¯é€šè¿‡ä¸æ–­æ›´æ–°è¿åŠ¨å‚æ•°ï¼Œä½¿å¾—ä¸¤ä¸ªç‚¹äº‘çš„é‡å éƒ¨åˆ†å……åˆ†å»åˆã€‚</li></ul><p>ICPçš„æ±‚è§£åˆ†ä¸ºä¸¤ç§æ–¹å¼ï¼š</p><ul><li>åˆ©ç”¨çº¿æ€§ä»£æ•°æ±‚è§£ï¼ˆSVDï¼‰ï¼Œ<strong>åœ¨ç»™å®šäº†åŒ¹é…çš„æƒ…å†µä¸‹</strong>ï¼Œæœ€å°äºŒä¹˜é—®é¢˜å®é™…ä¸Šå…·æœ‰è§£æè§£ã€‚</li><li>åˆ©ç”¨éçº¿æ€§ä¼˜åŒ–æ–¹å¼æ±‚è§£ï¼Œç±»ä¼¼äºBAæ–¹æ³•ï¼Œ<strong>é€‚ç”¨äºåŒ¹é…æœªçŸ¥çš„æƒ…å†µ</strong>ã€‚</li></ul><h3 id="2-SVDæ–¹æ³•æ±‚è§£"><a href="#2-SVDæ–¹æ³•æ±‚è§£" class="headerlink" title="2 SVDæ–¹æ³•æ±‚è§£"></a>2 SVDæ–¹æ³•æ±‚è§£</h3><p>ç®—æ³•æ¨å¯¼å¦‚ä¸‹ï¼š</p><ol><li><p>é¦–å…ˆå°†ç‚¹äº‘æ–‡ä»¶è¿›è¡Œ<strong>ç²—åŒ¹é…</strong>ï¼Œå¦‚ORBç‰¹å¾ç‚¹åŒ¹é…ã€‚</p></li><li><p>ä»ç‚¹é›†$P={\overrightarrow{p_1}, \overrightarrow{p_2}, â€¦, \overrightarrow{p_n}}$ä¸­éšæœºé€‰å–æŒ‡å®šæ•°é‡çš„ç‚¹$\{\overrightarrow{p_t}\}$ä½œä¸º<strong>å‚è€ƒç‚¹</strong>ï¼Œå‚è€ƒç‚¹çš„æ•°é‡å†³å®šäº†ICPç®—æ³•çš„è®¡ç®—æ•ˆç‡å’Œé…å‡†ç²¾åº¦ã€‚</p></li><li><p>åœ¨å¦ä¸€ä¸ªç‚¹é›†$Q={\overrightarrow{q_1}, \overrightarrow{q_2}, â€¦, \overrightarrow{q_m}}$æ˜¯å¾…åŒ¹é…çš„ç‚¹<code>query points</code>ï¼Œé‚£ä¹ˆæƒ³è¦æ‰¾åˆ°ä¸€ä¸ªæ¬§å¼å˜æ¢$R, t$ï¼Œä½¿å¾—$\forall i, p_i = Rq_i + t$ã€‚</p></li><li><p>æ±‚è§£æ¬§å¼å˜æ¢$T^k$ï¼Œä½¿å¾—$E^k=\Sigma| \overrightarrow{p_t} - T^k \overrightarrow{q_t}|^2$æœ€å°åŒ–ã€‚ å°†ç©ºé—´å˜æ¢åˆ†è§£ä¸ºæ—‹è½¬å’Œå¹³ç§»ä¸¤éƒ¨åˆ†ï¼Œé¦–å…ˆå®šä¹‰ä¸¤ä¸ªç‚¹äº‘çš„è´¨å¿ƒï¼š</p><script type="math/tex; mode=display">\overrightarrow{p} = \frac{1}{n} \Sigma \overrightarrow{p_t}, \ \  \overrightarrow{q} = \frac{1}{n} \Sigma \overrightarrow{q_t}ï¼Œè´¨å¿ƒ,\ ç”¨äºæè¿°å¹³ç§»\\\overrightarrow p_i =  \overrightarrow{p_t} - \overrightarrow{p}, \ \ \overrightarrow q_i = \overrightarrow{q_t} - \overrightarrow pï¼Œä¸­å¿ƒåŒ–ç‚¹äº‘,\ ç”¨äºæè¿°æ—‹è½¬\\</script><p>äºæ˜¯æœ‰ç›®æ ‡å‡½æ•°ï¼š</p><script type="math/tex; mode=display">\begin{split} E^k &  = \Sigma|\overrightarrow{p_t} - T^k \overrightarrow{q_t}|^2 = \Sigma|(p+p_i) -T (q+q_i)|^2\\& = \Sigma|(p+p_i) -R (q+q_i) -t|^2\\& = \Sigma |(p_i - Rq_i) + (p - Rq -t)|^2\\& = \Sigma( |p_i - Rq_i|^2 + |p - Rq -t|^2)\\& = \Sigma( |p_i - Rq_i|^2\\ J &= \frac{1}{2} \sum e_i = \frac{1}{2} E^k\end{split}</script><p>å¯¹ç›®æ ‡å‡½æ•°å±•å¼€ï¼Œè€Œä¸”å·²çŸ¥æ—‹è½¬çŸ©é˜µæ˜¯æ­£äº¤é˜µï¼Œ$R^TR=Iâ€‹$ï¼Œæ‰€ä»¥ç›®æ ‡å‡½æ•°çš„å‰ä¸¤é¡¹éƒ½ä¸$Râ€‹$æ— å…³ï¼š</p><script type="math/tex; mode=display">R^* = argmin_R J = \frac{1}{2}\sum p_i^Tp_i + q_i^TR^TRq_i - 2p_i^TRq_i</script><p>åªæœ‰æœ€åä¸€é¡¹ä¸$R$æœ‰å…³ï¼Œäºæ˜¯å¾—åˆ°å…³äº$R$çš„ç›®æ ‡å‡½æ•°ï¼š</p><script type="math/tex; mode=display">J(R) = \sum_{unrelated} -\ p_i^TRq_i = \sum - \ tr(Rq_ip_i^T) = -tr(R\sum_{i=1}^nq_ip_i^T)</script><p>ç„¶åé€šè¿‡<strong>SVDå¥‡å¼‚å€¼åˆ†è§£</strong>æ±‚è§£ä¸Šè¿°é—®é¢˜çš„æœ€ä¼˜$R$ï¼Œé¦–å…ˆå®šä¹‰$W = \sum_1^n pq^T$ï¼Œå½“$W$æ»¡ç§©æ—¶ï¼š</p><script type="math/tex; mode=display">W = \sum_{i=1}^{n} \overrightarrow{p_i}*\overrightarrow{q_i^T} = U\begin{bmatrix}\sigma1 & 0 & 0 \\0 & \sigma2  & 0 \\0 & 0 & \sigma3\end{bmatrix}V^T\\R = UV^T</script><p>ç„¶åé—´æ¥å¾—åˆ°å¹³ç§»$t$ï¼š</p><script type="math/tex; mode=display">t = {p} - R{q}</script></li></ol><p>ä»£ç å®ç°å¦‚ä¸‹ï¼š</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">pose_estimation_3d3d</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Point3f&gt;&amp; pts1,         <span class="comment">// point cloud 1</span></span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Point3f&gt;&amp; pts2,         <span class="comment">// point cloud 2</span></span></span></span><br><span class="line"><span class="function"><span class="params">        Mat&amp; R, Mat&amp; t,</span></span></span><br><span class="line"><span class="function"><span class="params">        Eigen::Matrix3d&amp; R_, Eigen::Vector3d&amp; t_</span></span></span><br><span class="line"><span class="function"><span class="params">        )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Point3f p1, p2;         <span class="comment">// center of Mass</span></span><br><span class="line">    <span class="keyword">int</span> N = pts1.size();</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        p1 += pts1[i];</span><br><span class="line">        p2 += pts2[i];</span><br><span class="line">    &#125;</span><br><span class="line">    p1 /= N;</span><br><span class="line">    p2 /= N;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span>&lt;Point3f&gt; q1(N), q2(N);      <span class="comment">// remove the COM</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        q1[i] = pts1[i] - p1;</span><br><span class="line">        q2[i] = pts2[i] - p2;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Eigen::Matrix3d W = Eigen::Matrix3d::Zero();       <span class="comment">// calculate W matrix</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        W += Eigen::Vector3d(q1[i].x, q1[i].y, q1[i].z) * Eigen::Vector3d(q2[i].x, q2[i].y, q2[i].z).transpose();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// SVD decomposition</span></span><br><span class="line">    Eigen::JacobiSVD&lt;Eigen::Matrix3d&gt; svd(W, Eigen::ComputeFullU|Eigen::ComputeFullV);         <span class="comment">// SVD</span></span><br><span class="line">    Eigen::Matrix3d U = svd.matrixU();</span><br><span class="line">    Eigen::Matrix3d V = svd.matrixV();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// calculate R,t</span></span><br><span class="line">    R_ = U * V.transpose();</span><br><span class="line">    t_ = Eigen::Vector3d(p1.x, p1.y, p1.z) - R_ * Eigen::Vector3d(p2.x, p2.y, p2.z);</span><br></pre></td></tr></table></figure><h3 id="3-éçº¿æ€§ä¼˜åŒ–æ–¹æ³•"><a href="#3-éçº¿æ€§ä¼˜åŒ–æ–¹æ³•" class="headerlink" title="3 éçº¿æ€§ä¼˜åŒ–æ–¹æ³•"></a>3 éçº¿æ€§ä¼˜åŒ–æ–¹æ³•</h3><p>å¦ä¸€ç§æ–¹å¼æ˜¯é€šè¿‡è¿­ä»£çš„æ–¹å¼æ¥å¯»æ‰¾æœ€ä¼˜å€¼ï¼Œè¯¯å·®é¡¹çš„è¡¨ç¤ºä¸ä¸Šä¸€èŠ‚ç›¸åŒï¼Œç”¨æä»£æ•°æ¥è¡¨è¾¾ä½å§¿ï¼Œæ—‹è½¬å’Œå¹³ç§»ä¸ç”¨å†è§£è€¦è¡¨ç¤ºï¼Œç›®æ ‡å‡½æ•°ä¸ºï¼š</p><script type="math/tex; mode=display">\xi^* = argmin \frac{1}{2}\sum_{i=1}^n ||p_i - exp(\xi ^{\wedge})q_i||_2^2</script><p><strong>å•ä¸ªè¯¯å·®é¡¹å…³äºä½å§¿çš„å¯¼æ•°</strong>å¯ä»¥ä½¿ç”¨<a href="https://amberzzzz.github.io/2018/05/12/%E4%B8%89%E7%BB%B4%E5%88%9A%E4%BD%93%E8%BF%90%E5%8A%A8-%E6%9D%8E%E4%BB%A3%E6%95%B0/">æä»£æ•°æ‰°åŠ¨æ¨¡å‹</a>æ¥æè¿°ï¼š</p><script type="math/tex; mode=display">\frac{\partial e}{\partial \delta \xi} = (e)^{\odot} = (p_i - exp(\xi^{\wedge})q_i)^{\odot}</script><p>å…¶ä¸­$p_i$ä½œä¸ºå‚è€ƒç‚¹ï¼Œå¯¹æ‰°åŠ¨çš„å¯¼æ•°ä¸º0ï¼Œå› æ­¤ï¼š</p><script type="math/tex; mode=display">\frac{\partial e}{\partial \delta \xi} =   - (exp(\xi^{\wedge})q_i)^{\odot}</script><p>å°†æœ€å°äºŒä¹˜é—®é¢˜è¿›è¡Œå›¾æè¿°ï¼šä¼˜åŒ–å˜é‡ä¸ºæä»£æ•°è¡¨è¾¾çš„ä½å§¿$\xi$ï¼Œå› æ­¤å›¾ä¸­åªæœ‰ä¸€ä¸ªèŠ‚ç‚¹ï¼Œè¯¯å·®é¡¹ä¸ºä¸€å…ƒè¾¹ï¼ˆä»å½“å‰èŠ‚ç‚¹æŒ‡å‘å½“å‰èŠ‚ç‚¹ï¼‰ï¼Œå¯¹è¯¯å·®é¡¹åšçº¿æ€§å±•å¼€ï¼š</p><script type="math/tex; mode=display">e_i(\xi + \delta \xi) = e(\xi) + J(\xi)\delta \xi</script><p>å…¶ä¸­çš„<strong>é›…å¯æ¯”çŸ©é˜µ</strong>ä¹Ÿå°±æ˜¯ä¸Šé¢è¯´çš„ï¼Œå•ä¸ªè¯¯å·®é¡¹å…³äºä½å§¿çš„ä¸€é˜¶å¯¼æ•°ã€‚</p><h3 id="4ç®—æ³•ä¼˜åŒ–"><a href="#4ç®—æ³•ä¼˜åŒ–" class="headerlink" title="4ç®—æ³•ä¼˜åŒ–"></a>4ç®—æ³•ä¼˜åŒ–</h3><ul><li>åˆ é™¤ç‚¹äº‘æ•°æ®é‡‡é›†ä¸­äº§ç”Ÿçš„å™ªå£°åŠå¼‚å¸¸å€¼ã€‚</li><li>æŸ¥æ‰¾æœ€è¿‘ç‚¹çš„è¿‡ç¨‹é‡‡ç”¨KD-Treeæ•°æ®ç»“æ„ï¼Œå‡å°‘æ—¶é—´å¤æ‚åº¦ã€‚</li></ul>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CLion for record</title>
      <link href="/2018/05/03/clion%E4%BD%BF%E7%94%A8/"/>
      <url>/2018/05/03/clion%E4%BD%BF%E7%94%A8/</url>
      <content type="html"><![CDATA[<h3 id="1-cmake"><a href="#1-cmake" class="headerlink" title="1 cmake"></a>1 cmake</h3><p>è¯¦è§cmake for recordã€‚</p><h3 id="2-ç®€å•é…ç½®"><a href="#2-ç®€å•é…ç½®" class="headerlink" title="2 ç®€å•é…ç½®"></a>2 ç®€å•é…ç½®</h3><p>ä¸»è¦å°±æ˜¯keymapå¾ˆä¸é€‚åº”ï¼ŒåŸºæœ¬ä¸Šåˆ é™¤äº†å¤§éƒ¨åˆ†editingçš„é…ç½®ï¼Œå› ä¸ºå¯ä»¥ç”¨vimã€‚å‰©ä¸‹çš„ä¿®æ”¹ä¸»è¦å»¶ç»­sublimeå’ŒOSXçš„ä¹ æƒ¯ã€‚</p><h4 id="2-1-æœç´¢"><a href="#2-1-æœç´¢" class="headerlink" title="2.1 æœç´¢"></a>2.1 æœç´¢</h4><p>å…¨å±€æœç´¢ï¼šcmd + F</p><p>å‰©ä¸‹çš„äº¤ç»™vimã€‚</p><h4 id="2-2-å¯¼èˆª"><a href="#2-2-å¯¼èˆª" class="headerlink" title="2.2 å¯¼èˆª"></a>2.2 å¯¼èˆª</h4><p>search for fileï¼šcmd + O</p><p>search for classï¼šopt + cmd + O</p><p>search for symbolï¼šshift + cmd + O</p><p>go to lineï¼šcmd + G</p><p>backï¼šctrl + cmd + left</p><p>forwardï¼šctrl + cmd + right</p><p>å‰©ä¸‹çš„äº¤ç»™vimã€‚</p><h4 id="2-3-æ³¨é‡Š"><a href="#2-3-æ³¨é‡Š" class="headerlink" title="2.3 æ³¨é‡Š"></a>2.3 æ³¨é‡Š</h4><p>ä»£ç å—æ³¨é‡Šï¼šshift + cmd + ï¼</p><h4 id="2-4-æ™ºèƒ½æç¤º"><a href="#2-4-æ™ºèƒ½æç¤º" class="headerlink" title="2.4 æ™ºèƒ½æç¤º"></a>2.4 æ™ºèƒ½æç¤º</h4><p>çœ‹è§å°ç¯æ³¡å°±ï¼šopt + enter</p><h4 id="2-5-run-amp-build"><a href="#2-5-run-amp-build" class="headerlink" title="2.5 run &amp; build"></a>2.5 run &amp; build</h4><p>runï¼šcmd + R</p><p>buildï¼šcmd + B</p><h4 id="2-6-ä»£ç ç”Ÿæˆ"><a href="#2-6-ä»£ç ç”Ÿæˆ" class="headerlink" title="2.6 ä»£ç ç”Ÿæˆ"></a>2.6 ä»£ç ç”Ÿæˆ</h4><p>insertï¼šcmd + J</p><blockquote><p>æœ€è¿‘åœ¨ç†Ÿæ‚‰Eigenåº“ï¼Œç»å¸¸è¦æ‰“å°ä¸œè¥¿ï¼ŒåŠ äº†ä¸€ä¸ªsplitæ¨¡ç‰ˆå¿«é€Ÿåˆ†å‰²ä»£ç ç‰‡æ®µã€‚</p></blockquote><p>generateï¼šcmd + N</p><p>è¿˜æœ‰ä¸€äº›vimä¸ideå†²çªçš„é”®ï¼Œå¯ä»¥æ‰‹åŠ¨é€‰æ‹©æ˜¯æœä»ideè¿˜æ˜¯vimã€‚ </p>]]></content>
      
      
        <tags>
            
            <tag> ide </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Graph-based Optimization</title>
      <link href="/2018/05/02/graph-based-optimization/"/>
      <url>/2018/05/02/graph-based-optimization/</url>
      <content type="html"><![CDATA[<h3 id="1-ç»¼è¿°"><a href="#1-ç»¼è¿°" class="headerlink" title="1 ç»¼è¿°"></a>1 ç»¼è¿°</h3><p>åŸºäºå›¾ä¼˜åŒ–çš„slamä¸»è¦åˆ†ä¸ºä»¥ä¸‹ä¸‰ä¸ªéƒ¨åˆ†ï¼š</p><ul><li>å‰ç«¯ï¼šåŸºäºä¼ æ„Ÿå™¨æ•°æ®å»ºå›¾ï¼Œ<strong>åŒ¹é…ç›¸é‚»å¸§</strong>ï¼Œæ·»åŠ èŠ‚ç‚¹å’Œè¾¹ï¼ˆraw graphï¼‰ï¼ŒèŠ‚ç‚¹è¡¨ç¤ºæœºå™¨äººçš„ä½å§¿ï¼Œè¾¹è¡¨ç¤ºèŠ‚ç‚¹ä¹‹é—´çš„ä½å§¿è”ç³»ã€‚<strong>ä½å§¿ä¿¡æ¯å¯ä»¥æ¥è‡ªé‡Œç¨‹è®¡è®¡ç®—ï¼Œå¯ä»¥æ¥è‡ªICPæ¿€å…‰ç‚¹äº‘åŒ¹é…ï¼Œä¹Ÿå¯ä»¥æ¥è‡ªé—­ç¯æ£€æµ‹åé¦ˆ</strong>ã€‚</li><li>åç«¯ï¼š<strong>ä¼˜åŒ–</strong>å›¾ï¼Œ<strong>åŸºäºå†å²ä¿¡æ¯çš„çº¦æŸ</strong>ï¼Œè°ƒæ•´æ–°åŠ å…¥çš„æœºå™¨äººä½å§¿é¡¶ç‚¹ä½¿å…¶å°½é‡æ»¡è¶³è¾¹çš„çº¦æŸï¼ˆoptimized graphï¼‰ã€‚</li><li>å®è§‚çš„é—­ç¯æ£€æµ‹ï¼šæ ¹æ®é—­ç¯ä¿¡æ¯<strong>ä¼˜åŒ–</strong>çŸ«æ­£æ•´ä¸ªæ‹“æ‰‘å›¾ã€‚</li></ul><p>è¿™é‡Œé¢æ¶‰åŠåˆ°äº†<strong>ä¸¤ä¸ªä¼˜åŒ–</strong>ï¼Œä¸€ä¸ªæ˜¯åç«¯å±€éƒ¨ä¼˜åŒ–ï¼Œä¸€ä¸ªæ˜¯å…¨å±€é—­ç¯ä¼˜åŒ–ï¼Œä¸¤è€…è®¡ç®—çš„æ€è·¯æ˜¯ä¸€æ ·çš„ã€‚</p><h3 id="2-ä¼˜åŒ–"><a href="#2-ä¼˜åŒ–" class="headerlink" title="2 ä¼˜åŒ–"></a>2 ä¼˜åŒ–</h3><h4 id="2-1-å…¨å±€é—­ç¯ä¼˜åŒ–ï¼Œç”¨äºçŸ«æ­£æ•´ä¸ªæ‹“æ‰‘å›¾"><a href="#2-1-å…¨å±€é—­ç¯ä¼˜åŒ–ï¼Œç”¨äºçŸ«æ­£æ•´ä¸ªæ‹“æ‰‘å›¾" class="headerlink" title="2.1 å…¨å±€é—­ç¯ä¼˜åŒ–ï¼Œç”¨äºçŸ«æ­£æ•´ä¸ªæ‹“æ‰‘å›¾"></a>2.1 å…¨å±€é—­ç¯ä¼˜åŒ–ï¼Œç”¨äºçŸ«æ­£æ•´ä¸ªæ‹“æ‰‘å›¾</h4><p>å‰ç«¯åç«¯å®Œæˆçš„äº‹æƒ…æ˜¯<strong>æ¢ç´¢å¹¶åˆ›å»ºæ–°çš„èŠ‚ç‚¹</strong>ï¼Œè·å¾—æ–°çš„æµ‹é‡å€¼ï¼Œæ·»åŠ æ–°çš„ä½å§¿å…³ç³»æ–¹ç¨‹ï¼š</p><script type="math/tex; mode=display">\begin{eqnarray}\begin{split}& x_0 + z_{01} = x_1\\& x_1 + z_{12} = x_2\\& ...\\& x_{k-1} + z_{k-1, k} = x_{k}\\\end{split}\end{eqnarray}</script><p>è€Œå…¨å±€é—­ç¯æ£€æµ‹<strong>æ·»åŠ å·²çŸ¥èŠ‚ç‚¹ä¹‹é—´</strong>çš„ä½å§¿çº¦æŸå…³ç³»ï¼š</p><script type="math/tex; mode=display">\begin{equation}x_i + z_{i j} = x_j, \ \ \ \ i,j\in [0, k]\end{equation}</script><p>å†æ·»åŠ ä¸€ä¸ª<strong>åˆå§‹æ¡ä»¶</strong>ï¼ˆä¸æ˜¯å¿…é¡»çš„ï¼Œä½†æ˜¯åé¢å®éªŒè¡¨æ˜å›ºå®šä¸€ä¸ªé¡¶ç‚¹æ¯”ä¸å›ºå®šæ•ˆæœè¦å¥½â€”â€”ç›¸å½“äºæœ‰ä¸€ä¸ªæ˜ç¡®å¯ä¿¡çš„åŸºå‡†ï¼‰ï¼š</p><script type="math/tex; mode=display">x_0 = 0</script><ul><li>ä»¥ä¸Šçº¿æ€§æ–¹ç¨‹ç»„ä¸­ï¼Œ<strong>é—­ç¯æ£€æµ‹éƒ¨åˆ†çš„æ–¹ç¨‹ä¸­çš„ä¸¤ä¸ªç»“ç‚¹éƒ½åœ¨å‰é¢å‡ºç°è¿‡ï¼Œå› æ­¤ä¸å¢åŠ çŸ©é˜µçš„ç§©</strong>ï¼Œå› æ­¤æœ€ç»ˆè¦æ±‚è§£åŒ…å«$k$ä¸ªæ–¹ç¨‹$k+1$ä¸ªæœªçŸ¥æ•°çš„çº¿æ€§æ–¹ç¨‹ç»„ã€‚</li><li>é—­ç¯çš„å…³é”®æ€§ï¼šå¦‚æœæ²¡æœ‰é—­ç¯æ¡ä»¶ï¼Œæ–¹ç¨‹ç»„$Ax=b$å·¦å³ä¸¤è¾¹ç§©æ˜¯ç›¸ç­‰çš„â€”â€”æœ‰å”¯ä¸€è§£ï¼Œæ·»åŠ äº†é—­ç¯æ¡ä»¶ä»¥åï¼Œç›¸å½“äºæ–¹ç¨‹ç»„å·¦ä¾§$A$çš„ç§©ä¸å˜ï¼Œä½†æ˜¯å³ä¾§$b$çš„ç§©åˆ™å¢åŠ äº†ï¼Œ$rank(A) &lt; rank(A, b)$â€”â€”æ²¡æœ‰è§£æè§£ï¼Œåªæœ‰æœ€ä¼˜ã€‚</li><li>å®é™…ä¸ŠçŠ¶æ€$\textbf x$æ˜¯ä¸€ä¸ªåŒ…å«å¤¹è§’$\theta$çš„å‘é‡$[x, y, \theta]$ï¼Œå®é™…ç›¸å¯¹ä½å§¿çš„è®¡ç®—<strong>å¹¶éç®€å•çš„çº¿æ€§å åŠ </strong>ï¼š</li></ul><script type="math/tex; mode=display">\textbf x \oplus \Delta \textbf x = \begin{pmatrix}x + \Delta x cos\theta - \Delta y sin \theta  \\y + \Delta x sin\theta + \Delta y cos \theta  \\normAngle(\theta + \Delta \theta)\end{pmatrix}</script><p><strong>ä¸¾ä¸ªæ —å­</strong>ï¼š</p><blockquote><p>æœºå™¨äººä»èµ·å§‹ä½ç½®$x_0=0$å¤„å‡ºå‘ï¼Œé‡Œç¨‹è®¡æµ‹å¾—å®ƒå‘å‰ç§»åŠ¨äº†1mï¼Œåˆ°è¾¾$x_1$ï¼Œæ¥ç€æµ‹å¾—å®ƒå‘åç§»åŠ¨äº†0.8mï¼Œåˆ°è¾¾$x_2$ï¼Œè¿™æ—¶é€šè¿‡é—­ç¯æ£€æµ‹ï¼Œå‘ç°ä»–å›åˆ°äº†èµ·å§‹ä½ç½®ã€‚</p></blockquote><p>é¦–å…ˆæ ¹æ®ç»™å‡ºä¿¡æ¯æ„å»ºå›¾ï¼š</p><script type="math/tex; mode=display">x_0 + 1 = x_1\\x_1 - 0.8 = x_2</script><p>ç„¶åæ ¹æ®é—­ç¯æ¡ä»¶æ·»åŠ çº¦æŸï¼š</p><script type="math/tex; mode=display">x_2 = x_0</script><p>è¡¥å……åˆå§‹æ¡ä»¶ï¼š</p><script type="math/tex; mode=display">x_0 = 0</script><p>ä½¿ç”¨<strong>æœ€å°äºŒä¹˜æ³•</strong>æ±‚ä¸Šè¿°æ–¹ç¨‹ç»„çš„æœ€ä¼˜è§£ï¼Œé¦–å…ˆæ„å»º<strong>æ®‹å·®å¹³æ–¹å’Œå‡½æ•°</strong>ï¼š</p><script type="math/tex; mode=display">\begin{eqnarray}\begin{split}& f_1 = x_0 = 0\\& f_2 = x_1 - x_0 - 1 = 0\\& f_3 = x_2 - x_1 + 0.8 = 0\\& f_4 = x_2 - x_0 = 0\end{split}\end{eqnarray}</script><script type="math/tex; mode=display">c = \Sigma_{i=1}^4 f_i^2 = x_0^2 + (x_1-x_0-1)^2 + (x_2-x_1+0.8)^2 + (x_2-x_0)^2</script><p>ç„¶åå¯¹æ¯ä¸ªå‚æ•°<strong>æ±‚åå¯¼</strong>ï¼š</p><script type="math/tex; mode=display">\frac{\partial c}{\partial x_1} = -x_0 + 2x_1-x_2 -1.8=0\\\frac{\partial c}{\partial x_2} = -x_0 - x_1 +2x_2 + 0.8 = 0</script><p>è§£å¾—$x_1 = 0.93, x_2 = 0.07$ï¼Œå¯ä»¥çœ‹åˆ°é—­ç¯çŸ«æ­£äº†æ‰€æœ‰èŠ‚ç‚¹çš„ä½å§¿ï¼Œä¼˜åŒ–äº†æ•´ä¸ªæ‹“æ‰‘å›¾ã€‚</p><h4 id="2-2-åç«¯å±€éƒ¨ä¼˜åŒ–ï¼Œç”¨äºçŸ«æ­£å±€éƒ¨åœ°å›¾"><a href="#2-2-åç«¯å±€éƒ¨ä¼˜åŒ–ï¼Œç”¨äºçŸ«æ­£å±€éƒ¨åœ°å›¾" class="headerlink" title="2.2 åç«¯å±€éƒ¨ä¼˜åŒ–ï¼Œç”¨äºçŸ«æ­£å±€éƒ¨åœ°å›¾"></a>2.2 åç«¯å±€éƒ¨ä¼˜åŒ–ï¼Œç”¨äºçŸ«æ­£å±€éƒ¨åœ°å›¾</h4><p>å†ä¸¾ä¸ªæ —å­ï¼š</p><blockquote><p>æœºå™¨äººä»èµ·å§‹ä½ç½®$x_0=0$å¤„å‡ºå‘ï¼Œå¹¶è§‚æµ‹åˆ°å…¶æ­£å‰æ–¹2må¤„æœ‰ä¸€ä¸ªè·¯æ ‡$l_0$ï¼Œé‡Œç¨‹è®¡æµ‹å¾—å®ƒå‘å‰ç§»åŠ¨äº†1mï¼Œåˆ°è¾¾$x_1$ï¼Œè¿™æ—¶è§‚æµ‹åˆ°è·¯æ ‡åœ¨å…¶æ­£å‰æ–¹0.8må¤„ã€‚</p></blockquote><p>é¦–å…ˆæ ¹æ®å‰ç«¯ä¿¡æ¯å»ºå›¾ raw graphï¼ˆè¿™æ ·å»ºå›¾æ˜æ˜¾æ˜¯å­˜åœ¨ç´¯ç§¯è¯¯å·®çš„ï¼‰ï¼š</p><script type="math/tex; mode=display">x_0 + 1 = x_1</script><p>ç„¶åæ·»åŠ é—­ç¯çº¦æŸï¼š</p><script type="math/tex; mode=display">x_1 + 0.8 = l_0\\x_0 + 2 = l_0</script><p>åˆå§‹æ¡ä»¶ï¼š</p><script type="math/tex; mode=display">x_0 = 0</script><p>æ„å»ºæ®‹å·®å¹³æ–¹å’Œï¼š</p><script type="math/tex; mode=display">c = \Sigma_{i=1}^4 f_i^2 = x_0^2 + (x_1-x_0-1)^2 +(l_0-x_0-2)^2 + (l_0-x_1-0.8)^2</script><p>æ±‚åå¯¼æ±‚è§£ï¼š$x_1 = 1.07, l_0 = 1.93$ï¼Œå¯ä»¥çœ‹åˆ°åç«¯æ˜¯å¯¹å‰ç«¯æ–°æ·»åŠ è¿›æ¥çš„èŠ‚ç‚¹ä½å§¿åšäº†çŸ«æ­£ï¼Œæ¶ˆé™¤éƒ¨åˆ†æµ‹é‡è¯¯å·®ã€‚</p><p>è¿™é‡Œé¢æ¶‰åŠåˆ°ä¸¤ç§ä¼ æ„Ÿå™¨ä¿¡æ¯â€”â€”é‡Œç¨‹è®¡å’Œæ¿€å…‰é›·è¾¾ï¼Œä¸¤ç§ä¼ æ„Ÿå™¨çš„ç²¾åº¦æ˜¯æœ‰å·®åˆ«çš„ï¼Œæˆ‘ä»¬å¯¹å…¶çš„ä¿¡ä»»ç¨‹åº¦ä¹Ÿåº”è¯¥ä¸åŒï¼Œåæ˜ åˆ°å…¬å¼ä¸­å°±æ˜¯<strong>è¦ä¸ºä¸åŒä¼ æ„Ÿå™¨ä¿¡æ¯èµ‹äºˆä¸åŒçš„æƒé‡</strong>ã€‚å‡è®¾ç¼–ç å™¨ä¿¡æ¯æ›´å‡†ç¡®ï¼Œé‚£ä¹ˆï¼š</p><script type="math/tex; mode=display">c = \Sigma_{i=1}^4 f_i^2 = x_0^2 + \textbf{10}(x_1-x_0-1)^2 +(l_0-x_0-2)^2 + (l_0-x_1-0.8)^2</script><p>è°ƒæ•´æƒé‡ä¹‹åè§£å¾—ï¼š$x_1 = 1.01, l_0 = 1.9$ï¼Œå¯ä»¥çœ‹åˆ°<strong>è®¡ç®—ç»“æœä¼šå‘ç€æ›´ä¿¡ä»»çš„ä¼ æ„Ÿå™¨çš„æµ‹é‡ç»“æœé è¿‘</strong>ã€‚</p><h4 id="2-3-ä¸¥æ ¼æ¨å¯¼"><a href="#2-3-ä¸¥æ ¼æ¨å¯¼" class="headerlink" title="2.3 ä¸¥æ ¼æ¨å¯¼"></a>2.3 ä¸¥æ ¼æ¨å¯¼</h4><p><strong>2.3.1 ä¿¡æ¯çŸ©é˜µï¼ˆè¯¯å·®æƒé‡çŸ©é˜µ)</strong></p><p>å›¾ä¼˜åŒ–é—®é¢˜è½¬åŒ–ä¸ºæœ€å°äºŒä¹˜é—®é¢˜ï¼Œé¦–å…ˆæ˜¯å¸¦æƒé‡çš„æ®‹å·®å¹³æ–¹å’Œå‡½æ•°çš„ä¸€èˆ¬å½¢å¼ï¼š</p><script type="math/tex; mode=display">F(x) = \Sigma_{i,j} e(x_i, x_j, z_{i,j})^T\Omega_{i,j}e(x_i, x_j, z_{i,j})</script><script type="math/tex; mode=display">x^{*} = argminF(x)</script><p>å…¶ä¸­çš„$\Omega_{i,j}$é¡¹å°±æ˜¯ä¸Šæ–‡æåˆ°çš„è¯¯å·®æƒé‡çŸ©é˜µï¼Œå®ƒçš„æ­£å¼åå­—å«<strong>ä¿¡æ¯çŸ©é˜µ</strong>ã€‚</p><p>ä¼ æ„Ÿå™¨çš„æµ‹é‡å€¼ï¼Œå¯ä»¥çœ‹ä½œæ˜¯ä»¥çœŸå€¼ä¸ºä¸­å¿ƒçš„å¤šå…ƒé«˜æ–¯åˆ†å¸ƒï¼š</p><script type="math/tex; mode=display">f_x(x_1, x_2, ..., x_k) = \frac{1}{\sqrt{}(2\pi)^k|\Sigma|}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))</script><p>åæ–¹å·®çŸ©é˜µ$\Sigma$å¯¹è§’çº¿ä¸Šçš„å€¼è¡¨ç¤ºæ¯ä¸€ç»´å¯¹åº”çš„æ–¹å·®ï¼Œè¯¥æ–¹å·®å€¼è¶Šå¤§è¡¨ç¤ºè¯¥ç»´åº¦ä¸ç¡®å®šæ€§è¶Šå¤§ï¼Œå¯¹åº”çš„ä¿¡æ¯æƒé‡åº”è¯¥è¶Šå°ã€‚<strong>å®é™…ä¸Šæ‹“æ‰‘å›¾ä¸Šæ¯æ¡è¾¹å¯¹åº”çš„ä¿¡æ¯çŸ©é˜µå°±æ˜¯å¯¹åº”æµ‹é‡åæ–¹å·®çŸ©é˜µçš„é€†</strong>ã€‚</p><p><strong>2.3.2 éçº¿æ€§</strong></p><p>ä¸Šæ–‡å·²ç»æåˆ°ï¼Œä½å§¿å˜åŒ–éçº¿æ€§â€”â€”<strong>éçº¿æ€§æœ€å°äºŒä¹˜é—®é¢˜</strong>ï¼Œè¦é‡‡ç”¨è¿­ä»£æ³•æ±‚è§£ã€‚è¿­ä»£æ³•éœ€è¦æœ‰ä¸€ä¸ª<strong>å¥½çš„åˆå§‹å‡è®¾å€¼</strong>ï¼Œç„¶ååœ¨è¿™ä¸ªå€¼é™„è¿‘<strong>å¢é‡å¼è¿­ä»£</strong>å¯»æ‰¾æœ€ä¼˜è§£ã€‚</p><script type="math/tex; mode=display">f(x) = \Sigma e^T\Omega e\\æœ€å°äºŒä¹˜é—®é¢˜ç›®æ ‡å‡½æ•°ï¼šmin \frac{1}{2}||f(x)||_2^2</script><p>é¦–å…ˆè¦å°†éçº¿æ€§å‡½æ•°è½¬åŒ–æˆå…³äºå¢é‡$\Delta x$çš„çº¿æ€§å‡½æ•°â€”â€”æ³°å‹’å±•å¼€ï¼Œæ ¹æ®å…·ä½“çš„å±•å¼€å½¢å¼åˆåˆ†ä¸ºï¼š</p><ul><li><p>ä¸€é˜¶ã€äºŒé˜¶æ¢¯åº¦æ³•</p><p>ç›´æ¥<strong>å¯¹ç›®æ ‡å‡½æ•°</strong>åœ¨$x$é™„è¿‘è¿›è¡Œæ³°å‹’å±•å¼€ï¼š</p><script type="math/tex; mode=display">||f(x+\Delta x)||_2^2 \approx ||f(x)||_2^2 +J(x) \Delta x = \frac{1}{2} \Delta x^T H \Delta x</script><p><strong>ä¸€é˜¶æ¢¯åº¦æ³•ï¼ˆæœ€é€Ÿä¸‹é™æ³•ï¼‰</strong>ï¼šåªä¿ç•™ä¸€é˜¶æ¢¯åº¦ï¼Œå¹¶å¼•å…¥æ­¥é•¿$\lambda$ï¼š</p><script type="math/tex; mode=display">\Delta x^* = -\lambda J^T(x)</script><p><strong>äºŒé˜¶æ¢¯åº¦æ³•ï¼ˆç‰›é¡¿æ³•ï¼‰</strong>ï¼šä¿ç•™ä¸€é˜¶å’ŒäºŒé˜¶æ¢¯åº¦ä¿¡æ¯</p><script type="math/tex; mode=display">J^T(x)+H\Delta x^*=0</script><p>æœ€é€Ÿä¸‹é™æ³•è¿‡äºè´ªå¿ƒï¼Œå®¹æ˜“èµ°å‡ºé”¯é½¿è·¯çº¿ï¼Œå¢åŠ è¿­ä»£æ¬¡æ•°ã€‚ç‰›é¡¿æ³•éœ€è¦è®¡ç®—ç›®æ ‡å‡½æ•°çš„äºŒé˜¶å¯¼æ•°ï¼ˆHessiançŸ©é˜µï¼‰ï¼Œè®¡ç®—å›°éš¾ã€‚</p></li></ul><ul><li><p>é«˜æ–¯ç‰›é¡¿æ³•</p><p><strong>å¯¹$f(x)$è€Œä¸æ˜¯ç›®æ ‡å‡½æ•°$f(x)^2$</strong>åœ¨$x$é™„è¿‘è¿›è¡Œä¸€é˜¶æ³°å‹’å±•å¼€ï¼š</p><script type="math/tex; mode=display">f(x+\Delta x) \approx f(x) + J(x) \Delta x</script><p>å¯¹åº”æ¯ä¸€ä¸ªè¯¯å·®å‡½æ•°$e_{ij}$ï¼š</p><script type="math/tex; mode=display">\begin{split}& e_{ij}(x_i+\Delta x_i, x_j+\Delta x_j) = e_{i,j}(x+\Delta x)  \\&\approx e_{ij} +\frac{\partial e_{ij}}{\partial x}\Delta x = e_{ij} +  J_{ij}\Delta x\end{split}</script><p>â€‹            å…¶ä¸­$J_{ij}$ä¸ºåˆå§‹å€¼é™„è¿‘çš„é›…å¯æ¯”çŸ©é˜µï¼ˆå®šä¹‰è§å¡å°”æ›¼æ»¤æ³¢ï¼‰ã€‚</p><p>å¸¦å…¥ç›®æ ‡å‡½æ•°å¾—åˆ°<strong>è¿‘ä¼¼äºŒé˜¶å±•å¼€</strong>ï¼š</p><script type="math/tex; mode=display">\begin{split}& F_{ij}(x+\Delta x) = e_{ij}(x+\Delta x)^T \Omega_{ij}e_{ij}(x+\Delta x)\\& \approx (e_{ij} +  J_{ij}\Delta x)^T \Omega_{ij}(e_{ij} +  J_{ij}\Delta x)\\& = \underbrace{e_{ij}^T\Omega_{ij}e_{ij}}_{c_{ij}} + 2\underbrace{e_{ij}^T\Omega_{ij}J_{ij}}_{b_{ij}^T}\Delta x + \Delta x^T \underbrace{J_{ij}^T\Omega_{ij}J_{ij}}_{H_{ij}}\Delta x\\& = c_{ij} + 2b_{ij}^T\Delta x + \Delta x^T H_{ij}\Delta x\end{split}</script><p>æ±‚è§£å¢é‡$\Delta x$ï¼š</p><script type="math/tex; mode=display">2b + 2H\Delta x^* = 0\\é«˜æ–¯ç‰›é¡¿æ–¹ç¨‹ï¼šH\Delta x^* = -b\\</script><p>å¯¹æ¯”ç‰›é¡¿æ³•å¯è§ï¼Œ<strong>é«˜æ–¯ç‰›é¡¿æ³•ç”¨$J^TJ$ä½œä¸ºäºŒé˜¶HessiançŸ©é˜µçš„è¿‘ä¼¼ï¼Œç®€åŒ–äº†è®¡ç®—</strong>ã€‚</p><p>ä¸Šè¿°ç®—æ³•è¦æ±‚è¿‘ä¼¼$H$çŸ©é˜µæ˜¯æ­£å®šä¸”å¯é€†çš„ï¼Œå®é™…æ•°æ®å¾ˆéš¾æ»¡è¶³ï¼Œå› è€Œåœ¨ä½¿ç”¨é«˜æ–¯ç‰›é¡¿ç®—æ³•æ—¶å¯èƒ½å‡ºç°<strong>$H$ä¸ºå¥‡å¼‚çŸ©é˜µæˆ–ç—…æ€</strong>çš„æƒ…å†µï¼Œå¢é‡ç¨³å®šæ€§è¾ƒå·®ï¼Œå¯¼è‡´ç®—æ³•ä¸æ”¶æ•›ã€‚</p><blockquote><p>å›¾å½¢ä¸Šæ¥æ€è€ƒï¼Œå°±æ˜¯è¿‘ä¼¼åçš„æ¢¯åº¦æ–¹å‘ä¸å†æ˜¯æ¢¯åº¦å˜åŒ–æœ€å¿«çš„æ–¹å‘ï¼Œå¯èƒ½å¼•èµ·ä¸ç¨³å®šã€‚</p></blockquote></li></ul><ul><li><p>åˆ—æ–‡ä¼¯æ ¼â€”é©¬å¤¸å°”ç‰¹æ³•</p><p>ä¸º$\Delta x$æ·»åŠ ä¸€ä¸ªä¿¡èµ–åŒºåŸŸï¼Œä¸è®©å®ƒå› ä¸ºè¿‡å¤§è€Œä½¿å¾—è¿‘ä¼¼$f(x+\Delta x) = f(x) + J(x)\Delta x$ä¸å‡†ç¡®ã€‚</p><script type="math/tex; mode=display">\rho = \frac{f(x+\Delta x) - f(x)}{J(x) \Delta x}</script><p>å¯ä»¥çœ‹åˆ°å¦‚æœ$\rho$æ¥è¿‘1ï¼Œè¯´æ˜è¿‘ä¼¼æ¯”è¾ƒå¥½ã€‚å¦‚æœ$\rho$æ¯”è¾ƒå¤§ï¼Œè¯´æ˜å®é™…å‡å°çš„å€¼è¿œå¤§äºä¼°è®¡å‡å°çš„å€¼ï¼Œéœ€è¦æ”¾å¤§è¿‘ä¼¼èŒƒå›´ï¼Œåä¹‹ä½ æ‡‚çš„ã€‚</p><script type="math/tex; mode=display">||D\Delta x_k||_2^2 \leq \mu</script><p>å°†æ¯æ¬¡è¿­ä»£å¾—åˆ°çš„$\Delta x$é™å®šåœ¨ä¸€ä¸ªåŠå¾„ä¸ºä¿¡èµ–åŒºåŸŸçš„æ¤­çƒä¸­ï¼Œæ ¹æ®$\rho$çš„å¤§å°ä¿®æ”¹ä¿¡èµ–åŒºåŸŸã€‚äºæ˜¯é—®é¢˜è½¬åŒ–æˆä¸ºäº†<strong>å¸¦ä¸ç­‰å¼çº¦æŸçš„ä¼˜åŒ–é—®é¢˜</strong>ï¼š</p><script type="math/tex; mode=display">min_\Delta x \frac{1}{2}||f(x) + J(x)\Delta x||^2_2, \ \ s.t. ||D \Delta x ||^2 = \mu</script><p>ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜å­è½¬åŒ–æˆæ— çº¦æŸé—®é¢˜ï¼š</p><script type="math/tex; mode=display">min_\Delta x \frac{1}{2}||f(x) + J(x)\Delta x||^2_2 + \frac{\lambda}{2}||D \Delta x ||^2</script><p>å±•å¼€åå¾—åˆ°å¦‚ä¸‹å½¢å¼ï¼š</p><script type="math/tex; mode=display">(H + \lambda D^TD)\Delta x^* = -b</script><p>é€šå¸¸æŠŠ$D$å–å€¼ä¸ºå•ä½é˜µ$I$ï¼Œå¾—åˆ°æ›´ç®€åŒ–å½¢å¼ï¼š</p><script type="math/tex; mode=display">(H + \lambda I)\Delta x^* = -b</script><p>å½“$\lambda$è¾ƒå°æ—¶ï¼Œ$H$å ä¸»è¦åœ°ä½ï¼Œè¯´æ˜äºŒæ¬¡è¿‘ä¼¼æ¨¡å‹è¾ƒå¥½ï¼ŒLMç®—æ³•æ›´æ¥è¿‘é«˜æ–¯ç‰›é¡¿æ³•ã€‚å½“$\lambda$è¾ƒå¤§æ—¶ï¼Œ$\lambda I$å ä¸»è¦åœ°ä½ï¼ŒLMç®—æ³•æ›´æ¥è¿‘ä¸€é˜¶æ¢¯åº¦æ³•ã€‚ä¿®æ­£äº†çº¿æ€§æ–¹ç¨‹ç»„çŸ©é˜µçš„ç—…æ€é—®é¢˜ï¼Œæ¯”é«˜æ–¯ç‰›é¡¿æ³•æ›´åŠ å¥å£®ï¼Œä½†æ˜¯æ”¶æ•›é€Ÿåº¦ä¹Ÿæ›´æ…¢ã€‚</p><blockquote><p>å›¾å½¢ä¸Šæ€è€ƒï¼ŒLMç®—æ³•ä¿®æ­£äº†é«˜æ–¯ç‰›é¡¿æ³•å¾—åˆ°çš„æ¢¯åº¦ï¼Œä»¥æ­¤å›ºå®šä¸€ä¸ªæœç´¢åŒºåŸŸï¼Œåœ¨åŒºåŸŸå†…å¯»æ‰¾æœ€ä¼˜ã€‚</p></blockquote></li></ul><p><strong>2.3.3 ç¨€ç–çŸ©é˜µ</strong></p><p>å¯¹äºè¯¯å·®å‡½æ•°$e_{ij}$ï¼Œå®ƒåªå’Œ$e_i$å’Œ$e_j$æœ‰å…³ï¼Œå› æ­¤å®ƒçš„é›…å¯æ¯”çŸ©é˜µæœ‰å¦‚ä¸‹ç»“æ„ï¼ˆè¡Œæ•°æ˜¯$x$çš„ç»´åº¦ï¼Œåˆ—æ•°æ˜¯æ‹“æ‰‘å›¾ä¸­èŠ‚ç‚¹æ˜ å°„å…³ç³»çš„æ•°ç›®ï¼‰ï¼š</p><script type="math/tex; mode=display">J_{ij} = \begin{bmatrix}0 & ... & 0 & \underbrace{\frac{\partial e_{i}}{\partial x_i}}_{A_{ij}} & 0 & ... &  \underbrace{\frac{\partial e_{j}}{\partial x_j}}_{B_{ij}} & 0 & ... & 0\end{bmatrix}</script><p>ç›¸åº”åœ°$b_{ij}$æ˜¯ä¸€ä¸ªåŒ…å«å¾ˆå¤š0çš„åˆ—å‘é‡ï¼š</p><script type="math/tex; mode=display">\begin{split} b_{ij}^T& = e_{ij}^T \Omega_{ij} J_{ij}\\&= e_{ij}^T\Omega_{ij} (0 ... A_{ij}...B_{ij}...0)\\&=(0...e_{ij}^T\Omega_{ij}A_{ij}...e_{ij}^T\Omega_{ij}B_{ij}...0)\end{split}</script><p>$b = \Sigma_{ij} b_{ij}$ï¼š</p><p><img src="/2018/05/02/graph-based-optimization/b.png" alt=""></p><p>$H_{ij}$æ˜¯ä¸€ä¸ªåŒ…å«å¾ˆå¤š0çš„å¯¹ç§°é˜µï¼š</p><script type="math/tex; mode=display">\begin{split}H_{ij}& = J_{ij}^T \Omega_{ij}J_{ij} \\& = \begin{pmatrix}...\\A_{ij}^T\\...\\B_{ij}^T\\...\end{pmatrix}\Omega_{ij}\begin{pmatrix}... & A_{ij} & ... & B_{ij} & ...\end{pmatrix}\\& =\begin{pmatrix}    &  \\& A_{ij}^T\Omega{ij}A_{ij} & A_{ij}^T\Omega_{ij}B_{ij} & \\  & \\& B{ij}^T\Omega_{ij}A_{ij} & B_{ij}^T\Omega_{ij}B_{ij}& \\   & \end{pmatrix}\end{split}</script><p>$H=\Sigma_{ij}H_{ij}$ï¼š</p><p><img src="/2018/05/02/graph-based-optimization/H.png" alt=""></p><p>æ¢³ç†ä¸€ä¸‹è®¡ç®—æµç¨‹ï¼š$e_{ij} \to  J_{ij}  \to A_{ij}, B_{ij} \to b_{ij}, H_{ij} \to b, H \to \Delta x^* \to x$</p><p><strong>2.3.4 è¯¯å·®å‡½æ•°</strong></p><p>å‰é¢å®šä¹‰è¿‡ä½å§¿çš„éçº¿æ€§å åŠ ï¼Œæ˜¾ç„¶ä½å§¿è¯¯å·®ä¹Ÿä¸æ˜¯ç®€å•çš„çº¿æ€§åŠ å‡å…³ç³»ï¼š</p><script type="math/tex; mode=display">e_{ij}(x_i, x_j) = t2v(Z_{ij}^{-1}(X_i^{-1}.X_j))</script><p>å…¶ä¸­çš„$Z_{ij}$ã€$X_i$ã€$X_j$éƒ½æ˜¯çŸ©é˜µå½¢å¼ã€‚$X_i^{-1}X_j$è¡¨ç¤ºèŠ‚ç‚¹jåˆ°èŠ‚ç‚¹iä¹‹é—´çš„ä½å§¿å·®å¼‚$\hat Z_{ij}$ï¼Œå‡è®¾è¿™ä¸ªè½¬ç§»çŸ©é˜µå½¢å¼å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">\hat Z_{ij} = \begin{bmatrix}R_{2*2} & t_{2*1} \\0 & 1\end{bmatrix}</script><p>å‡è®¾æµ‹é‡å€¼$Z_{ij}$å½¢å¼å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">Z_{ij} = \begin{bmatrix}R^{'} & t^{'}\\0 & 1\end{bmatrix}</script><p>åˆ†å—çŸ©é˜µçš„æ±‚é€†è¿‡ç¨‹å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">\begin{split}\begin{bmatrix}R & t\\0 & 1\end{bmatrix}^{-1}& =\begin{bmatrix}\begin{bmatrix}I & t\\0 & 1\end{bmatrix}\begin{bmatrix}R & 0\\0 & 1\end{bmatrix}\end{bmatrix}^{-1}=\begin{bmatrix}R & 0\\0 & 1\end{bmatrix}^{-1}\begin{bmatrix}I & t\\0 & 1\end{bmatrix}^{-1}\\& =\begin{bmatrix}R^T & 0\\0 & 1\end{bmatrix}\begin{bmatrix}I & -t\\0 & 1\end{bmatrix}=\begin{bmatrix}R^T & -R^Tt\\0 & 1\end{bmatrix}\end{split}</script><p>æ‰€ä»¥è¯¯å·®$e_{ij}$è®¡ç®—å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">E_{ij} = Z_{ij}^{-1}\hat Z_{ij} = \begin{bmatrix}R^{'} & t^{'}\\0 & 1\end{bmatrix}^{-1}\begin{bmatrix}R & t\\0 & 1\end{bmatrix}=\begin{bmatrix}R^{'T} & -R^{'T}t^{'}\\0 & 1\end{bmatrix}\begin{bmatrix}R & t\\0 & 1\end{bmatrix}=\begin{bmatrix}R^{'T}R & R^{'T}(t-t^{'})\\0 & 1\end{bmatrix}</script><script type="math/tex; mode=display">e_{ij} = t2v(E_{ij})=\begin{bmatrix}R_z(t_{\hat z} - t_z)_{2*1}\\\theta_\hat z - \theta_z\end{bmatrix}_{3*1}=\begin{bmatrix}R_z(x_{\hat z} - x_z)\\R_z(y_{\hat z} - y_z)\\\theta_\hat z - \theta_z\end{bmatrix}=\begin{bmatrix}R_z[R_i(x_{j} - x_i) - x_z]\\R_z[R_i(y_{j} - y_{i}) - y_z]\\\theta_j - \theta_i - \theta_z\end{bmatrix}</script><p>æ±‚è§£é›…å¯æ¯”çŸ©é˜µ$J_{ij}$ï¼š</p><script type="math/tex; mode=display">A_{ij} = \begin{bmatrix}\frac{\partial e_1}{\partial x_i} & \frac{\partial e_1}{\partial y_i} & \frac{\partial e_1}{\partial \theta_i}\\\frac{\partial e_2}{\partial x_i} & \frac{\partial e_2}{\partial y_i} & \frac{\partial e_2}{\partial \theta_i}\\\frac{\partial e_3}{\partial x_i} & \frac{\partial e_3}{\partial y_i} & \frac{\partial e_3}{\partial \theta_i}\\\end{bmatrix}=\begin{bmatrix}-R_z^TR_i^T & 0 & R_z^T\frac{\partial R_I^T}{\partial \theta_i}(x_j-x_i)\\0 & -R_z^TR_i^T  & R_z^T\frac{\partial R_I^T}{\partial \theta_i}(y_j-y_i)\\0 & 0 & -1\end{bmatrix}</script><script type="math/tex; mode=display">B_{ij} = \begin{bmatrix}R_z^TR_i^T & 0\\0 & 1\end{bmatrix}</script><p>ç´¯åŠ $b$å’Œ$H$çŸ©é˜µï¼š </p><script type="math/tex; mode=display">b_{[i]} += A_{ij}\Omega_{ij}e_{ij}\\b_{[j]} += B_{ij}^T\Omega_{ij}e_{ij}\\H_{[ii]} += A_{ij}^T\Omega_{ij}A_{ij}\\H_{[ij]} += A_{ij}^T\Omega_{ij}B_{ij}\\H_{[ji]} += B_{ij}^T\Omega_{ij}A_{ij}\\H_{[jj]} += B_{ij}^T\Omega_{ij}B_{ij}</script>]]></content>
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>å…³è”åˆ†æ</title>
      <link href="/2018/04/29/%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90/"/>
      <url>/2018/04/29/%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90/</url>
      <content type="html"><![CDATA[<h4 id="1-å¼•è¨€"><a href="#1-å¼•è¨€" class="headerlink" title="1 å¼•è¨€"></a>1 å¼•è¨€</h4><p><strong>é¢‘ç¹é¡¹é›†</strong>ï¼šé›†åˆï¼Œ${a, b, c}$</p><p><strong>å…³è”è§„åˆ™</strong>ï¼šæ˜ å°„ï¼Œ$a\to b$</p><p><strong>æ”¯æŒåº¦</strong>ï¼šé’ˆå¯¹æŸä¸ªé¢‘ç¹é¡¹é›†ï¼Œ$support(é¢‘ç¹é¡¹é›†a) = \frac{freq(é¢‘ç¹é¡¹é›†a)}{freq(æ‰€æœ‰é¡¹é›†)}$</p><p><strong>å¯ä¿¡åº¦</strong>ï¼šè¡¡é‡æŸæ¡å…³è”è§„åˆ™ï¼Œ$confidence(a\to b) = \frac{support(a|b)}{support(a)}$</p><p>å¯¹äºåŒ…å«Nä¸ªå…ƒç´ çš„æ•°æ®é›†ï¼Œå¯èƒ½çš„é›†åˆæœ‰$2^N - 1$ç§ï¼Œæš´åŠ›éå†æ˜¾ç„¶è¯ä¸¸ï¼Œå› æ­¤å¼•å…¥AprioriåŸç†ã€‚</p><p><strong>AprioriåŸç†</strong>ï¼šå‡å°‘å¯èƒ½çš„é¡¹é›†ï¼Œé¿å…æŒ‡æ•°å¢é•¿ã€‚</p><ul><li>backwardsï¼šå¦‚æœæŸä¸ªé¡¹é›†æ˜¯é¢‘ç¹çš„ï¼Œé‚£ä¹ˆå®ƒçš„æ‰€æœ‰å­é›†ä¹Ÿæ˜¯é¢‘ç¹çš„ã€‚</li><li>forwardsï¼šå¦‚æœä¸€ä¸ªé¡¹é›†æ˜¯éé¢‘ç¹é¡¹é›†ï¼Œé‚£ä¹ˆå®ƒçš„æ‰€æœ‰è¶…é›†ä¹Ÿæ˜¯éé¢‘ç¹çš„ã€‚</li></ul><h4 id="2-Aprioriç®—æ³•"><a href="#2-Aprioriç®—æ³•" class="headerlink" title="2 Aprioriç®—æ³•"></a>2 Aprioriç®—æ³•</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apriori</span><span class="params">(dataSet, minSupport=<span class="number">0.5</span>)</span>:</span></span><br></pre></td></tr></table></figure><p>ç®—æ³•æ€è·¯ï¼šä»å•ä¸ªé¡¹é›†å¼€å§‹æ£€æŸ¥ï¼Œå»æ‰é‚£äº›ä¸æ»¡è¶³æœ€å°æ”¯æŒåº¦çš„é¡¹é›†ï¼Œç„¶åå¯¹å‰©ä¸‹çš„é›†åˆè¿›è¡Œç»„åˆï¼Œå¾—åˆ°åŒ…å«ä¸¤ä¸ªå…ƒç´ çš„é¡¹é›†ï¼Œé‡å¤æ‰«æï¼Œç„¶åå°†å‰©ä½™é¡¹é›†ç»„åˆæˆåŒ…å«ä¸‰ä¸ªå…ƒç´ çš„é›†åˆï¼Œä¾æ¬¡ç±»æ¨ï¼Œç›´åˆ°æ‰€æœ‰é¡¹é›†éƒ½è¢«å»æ‰ã€‚</p><ul><li>ä¸ºå•¥æœ€åä¼šå¾—åˆ°ç©ºé›†ï¼šå› ä¸ºåŒ…å«æ‰€æœ‰å…ƒç´ çš„é¡¹é›†ä¸€å®šä¸æ˜¯é¢‘ç¹é¡¹é›†ï¼Œå¦åˆ™æ ¹æ®AprioriåŸç†ï¼Œå®ƒçš„å…¨éƒ¨å­é›†éƒ½æ˜¯é¢‘ç¹é¡¹é›†ã€‚</li><li>å¦‚ä½•ä»åŒ…å«kä¸ªå…ƒç´ çš„é¡¹é›†é›†åˆç”ŸæˆåŒ…å«k+1ä¸ªå…ƒç´ çš„é¡¹é›†é›†åˆï¼šä»kä¸ªå…ƒç´ çš„é¡¹é›†åˆ°k+1ä¸ªå…ƒç´ é¡¹é›†çš„æ‰©å……ï¼Œ<strong>åªå…è®¸æœ‰ä¸€ä¸ªå…ƒç´ çš„ä¸åŒ</strong>ï¼Œç®—æ³•ä¸­ä¸ºäº†é¿å…é‡å¤ç»“æœï¼Œ<strong>åªå¯¹å‰k-1ä¸ªå…ƒç´ ç›¸åŒçš„ä¸¤ä¸ªé¡¹é›†æ±‚å¹¶é›†</strong>ã€‚</li></ul><p>ä»£ç å®ç°è¿‡ç¨‹ä¸­å‘ç°äº†å‡ ä¸ªçŸ¥è¯†è®°å½•ä¸€ä¸‹ï¼š</p><ul><li>mapå‡½æ•°çš„è¿”å›å€¼ï¼špython2ä¸‹ç›´æ¥è¿”å›åˆ—è¡¨ï¼Œpython3ä¸‹è¿”å›çš„æ˜¯è¿­ä»£å™¨ï¼š</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">map(frozenset, C1)</span><br><span class="line"><span class="comment"># è¿”å› &lt;map object at 0x101e78940&gt; </span></span><br><span class="line"></span><br><span class="line">list(map(frozenset, C1))</span><br><span class="line"><span class="comment"># è¿”å› list[frozenset1(), frozenset2(), ...]</span></span><br></pre></td></tr></table></figure><ul><li>å­—å…¸çš„updateæ–¹æ³•ï¼š</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># å°†dict2çš„é”®å€¼æ·»åŠ åˆ°dict1ä¸­ï¼Œåœ¨æ¶‰åŠè¿­ä»£æ“ä½œæ—¶å¯ä»¥çœç•¥ä¼ é€’ä¸­é—´å€¼</span><br><span class="line">dict1.update(dict2)</span><br></pre></td></tr></table></figure><ul><li>set &amp; frozensetï¼šset<strong>æ— æ’åº</strong>ä¸”ä¸é‡å¤ï¼Œå¹¶ä¸”å¯å˜ï¼Œå› æ­¤unhashableã€‚frozensetä¸å¯å˜ï¼Œå¯ä»¥ç”¨ä½œå­—å…¸çš„keyã€‚</li></ul><h4 id="3-å…³è”è§„åˆ™"><a href="#3-å…³è”è§„åˆ™" class="headerlink" title="3 å…³è”è§„åˆ™"></a>3 å…³è”è§„åˆ™</h4><p>å¯¹ä¸€ä¸ªåŒ…å«kä¸ªå…ƒç´ çš„é¢‘ç¹é¡¹é›†ï¼Œå…¶ä¸­å¯èƒ½çš„å…³è”è§„åˆ™æœ‰ï¼š</p><script type="math/tex; mode=display">C_N^1 + C_N^2 + ... + C_N^{N-1}</script><p>æš´åŠ›éå†è‚¯å®šåˆè¯ä¸¸ï¼Œå› æ­¤å»¶ç»­Aprioriçš„æ€è·¯ï¼Œå…³è”è§„åˆ™ä¹Ÿæœ‰ä¸€æ¡ç±»ä¼¼çš„å±æ€§ï¼š</p><ul><li>å¦‚æœæŸæ¡è§„åˆ™çš„å‰ä»¶ä¸æ»¡è¶³æœ€å°å¯ä¿¡åº¦è¦æ±‚ï¼Œé‚£ä¹ˆå®ƒçš„æ‰€æœ‰å­é›†ä¹Ÿä¸æ»¡è¶³æœ€å°å¯ä¿¡åº¦è¦æ±‚ã€‚</li><li>å¯¹åº”çš„ï¼Œå¦‚æœæŸæ¡è§„åˆ™çš„åä»¶ä¸æ»¡è¶³æœ€å°å¯ä¿¡åº¦è¦æ±‚ï¼Œé‚£ä¹ˆå®ƒçš„æ‰€æœ‰è¶…é›†ä¹Ÿä¸æ»¡è¶³ã€‚</li></ul><p>ç®—æ³•æ€è·¯ï¼šå¯¹æ¯ä¸ªè‡³å°‘åŒ…å«ä¸¤ä¸ªå…ƒç´ çš„é¢‘ç¹é¡¹é›†ï¼Œä»åéƒ¨åªåŒ…å«ä¸€ä¸ªå…ƒç´ çš„è§„åˆ™å¼€å§‹ï¼Œå¯¹è¿™äº›è§„åˆ™è¿›è¡Œæµ‹è¯•ï¼Œæ¥ä¸‹æ¥å¯¹æ‰€æœ‰å‰©ä½™è§„åˆ™çš„åä»¶è¿›è¡Œç»„åˆï¼Œå¾—åˆ°åŒ…å«ä¸¤ä¸ªå…ƒç´ çš„åä»¶ï¼ˆå¯¹åº”çš„è¡¥é›†å°±æ˜¯å‰ä»¶ï¼‰ï¼Œä¾æ¬¡ç±»æ¨ï¼Œç›´åˆ°æµ‹è¯•å®Œæ‰€æœ‰å¯èƒ½çš„åä»¶ã€‚</p><ul><li>ä¸ºå•¥åªæ£€æŸ¥<strong>å‰åä»¶äº’è¡¥</strong>çš„è§„åˆ™ï¼šå› ä¸ºä¸€ä¸ªé¢‘ç¹é¡¹é›†çš„æ‰€æœ‰å­é›†ä¹Ÿéƒ½æ˜¯é¢‘ç¹é¡¹é›†ï¼Œæ‰€ä»¥ä¸€ä¸ªé¢‘ç¹é¡¹é›†ä¸­ä¸äº’è¡¥çš„è§„åˆ™å°†æ˜¯è¯¥é¢‘ç¹é¡¹é›†çš„æŸä¸ªå­é›†çš„äº’è¡¥è§„åˆ™ã€‚</li></ul><h4 id="4-FP-growthç®—æ³•"><a href="#4-FP-growthç®—æ³•" class="headerlink" title="4 FP-growthç®—æ³•"></a>4 FP-growthç®—æ³•</h4><p>Aprioriç®—æ³•é¿å…äº†æš´åŠ›éå†å­é¡¹é›†çš„æŒ‡æ•°å¼å¢é•¿ï¼Œä½†æ˜¯å¯¹æ¯ä¸€ä¸ªæ–°ç”Ÿæˆçš„é¢‘ç¹é¡¹é›†ï¼Œéƒ½è¦æ‰«ææ•´ä¸ªæ•°æ®é›†ï¼Œå½“æ•°æ®é›†å¾ˆå¤§æ—¶ï¼Œè¿™ç§æŠ›ç‰©çº¿å¼å¢é•¿çš„æ—¶é—´å¤æ‚åº¦ä¹Ÿä¸å¤ªä»¤äººæ»¡æ„ã€‚</p><p>FP-growthç®—æ³•å€ŸåŠ©ä¸€ç§ç§°ä¸º<strong>FPæ ‘</strong>çš„æ•°æ®ç»“æ„å­˜å‚¨æ•°æ®ï¼Œæ¥æŠ½è±¡åŸå§‹æ•°æ®é›†ï¼š</p><ul><li><strong>é¡¹é›†</strong>ä»¥è·¯å¾„çš„æ–¹å¼å­˜å‚¨åœ¨æ ‘ä¸­</li><li><strong>ç›¸ä¼¼é¡¹</strong>ä¹‹é—´ç›¸è¿æ¥æˆé“¾è¡¨</li></ul><ul><li>ä¸€ä¸ªå…ƒç´ é¡¹å¯ä»¥åœ¨FPæ ‘ä¸­å‡ºç°å¤šæ¬¡</li><li>FPæ ‘å­˜å‚¨çš„æ˜¯å…ƒç´ çš„å‡ºç°é¢‘ç‡</li><li>é¡¹é›†å®Œå…¨ä¸åŒæ—¶ï¼Œæ ‘æ‰ä¼šåˆ†å‰ï¼Œå¦åˆ™ä¼šæœ‰å¤ç”¨è·¯å¾„</li></ul><p>â€‹ç®—æ³•æ€è·¯ï¼šé¦–å…ˆéå†ä¸€éåŸå§‹æ•°æ®é›†ï¼Œè®°å½•å…ƒç´ çš„å‡ºç°é¢‘ç‡ï¼Œå»æ‰ä¸æ»¡è¶³æœ€å°æ”¯æŒåº¦çš„å…ƒç´ ã€‚ç„¶åå†éå†ä¸€éå‰©ä¸‹çš„é›†åˆå…ƒç´ ï¼Œ<strong>æ„å»ºFPæ ‘</strong>ã€‚ç„¶åå°±å¯ä»¥é€šè¿‡FPæ ‘æŒ–æ˜é¢‘ç¹é¡¹é›†ã€‚</p><p>æ„å»ºFPæ ‘ï¼šä¾æ¬¡éå†æ¯ä¸€ä¸ªé¡¹é›†ï¼Œé¦–å…ˆå°†å…¶ä¸­çš„éé¢‘ç¹é¡¹ç§»é™¤ï¼Œå¹¶æŒ‰ç…§å…ƒç´ å‡ºç°é¢‘ç‡å¯¹è¿‡æ»¤åçš„å…ƒç´ è¿›è¡Œé‡æ’åºã€‚<strong>å¯¹è¿‡æ»¤ã€æ’åºåçš„é›†åˆ</strong>ï¼Œå¦‚æœæ ‘ä¸­å·²å­˜åœ¨ç°æœ‰å…ƒç´ ï¼Œåˆ™å¢åŠ ç°æœ‰å…ƒç´ çš„è®¡æ•°å€¼ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œåˆ™å‘æ ‘ä¸­æ·»åŠ ä¸€ä¸ªåˆ†æ”¯ï¼Œæ–°å¢èŠ‚ç‚¹çš„åŒæ—¶è¿˜è¦æ›´æ–°é“¾è¡¨å…ƒç´ ã€‚ä¸»è¦å°±æ¶‰åŠä¸¤ä¸ªæ•°æ®ç»“æ„ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># è‡ªå®šä¹‰èŠ‚ç‚¹æ•°æ®ç»“æ„</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">treeNode</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, nameValue, numOccur, parentNode)</span>:</span></span><br><span class="line">        self.name</span><br><span class="line">        self.count</span><br><span class="line">        self.nodeLink   <span class="comment"># é“¾è¡¨ä¿¡æ¯ï¼ŒæŒ‡å‘ä¸‹ä¸€ä¸ªç›¸ä¼¼é¡¹</span></span><br><span class="line">        self.parent</span><br><span class="line">        self.children</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="comment"># ç”¨äºå­˜å‚¨å…ƒç´ frequencyä»¥åŠé“¾æ¥ç›¸ä¼¼é¡¹çš„å­—å…¸æ•°æ®ç»“æ„</span></span><br><span class="line">freq = &#123;&#125;</span><br><span class="line">freq[node_name] = [frequency, node1, node2, ...]</span><br></pre></td></tr></table></figure><ul><li>å› ä¸ºé›†åˆä¸­å…ƒç´ çš„å‡ºç°é¢‘ç‡å¯èƒ½ç›¸ç­‰ï¼Œå› æ­¤è¿‡æ»¤æ’åºçš„ç»“æœä¸å”¯ä¸€ï¼Œç”Ÿæˆçš„æ ‘ç»“æ„ä¹Ÿä¼šæœ‰å·®å¼‚ã€‚</li><li>ç¬¬ä¸€æ¬¡éå†åˆ é™¤éé¢‘ç¹å…ƒç´ æ—¶ï¼Œå‘ç°å­—å…¸åœ¨è¿­ä»£è¿‡ç¨‹ä¸­ä¸èƒ½åˆ é™¤itemï¼Œæˆ‘è½¬åŒ–æˆlistæš´åŠ›è§£å†³äº†ï¼Œä¸çŸ¥é“æœ‰æ²¡æœ‰ä»€ä¹ˆä¼˜é›…çš„æ–¹å¼ã€‚</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> freq[item]</span><br><span class="line"><span class="comment"># è¿”å› RuntimeError: dictionary changed size during iteration</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> list(freq.keys()):</span><br><span class="line">    <span class="keyword">if</span> freq[item] &lt; minSupport:</span><br><span class="line">        <span class="keyword">del</span>(freq[item])</span><br></pre></td></tr></table></figure><p>æŒ–æ˜é¢‘ç¹é¡¹é›†ï¼šé¦–å…ˆåˆ›å»ºæ¡ä»¶æ¨¡å¼åŸºï¼Œç„¶ååˆ©ç”¨æ¡ä»¶æ¨¡å¼åŸºï¼Œæ„å»ºæ¡ä»¶FPæ ‘ã€‚</p><p><strong>1 æ¡ä»¶æ¨¡å¼åŸº</strong>ï¼šä»¥æ‰€æŸ¥æ‰¾å…ƒç´ é¡¹ä¸ºç»“å°¾çš„<strong>å‰ç¼€è·¯å¾„</strong>é›†åˆï¼Œå¹¶ä¸”æ¯æ¡å‰ç¼€è·¯å¾„éƒ½ä¸<strong>èµ·å§‹å…ƒç´ é¡¹</strong>çš„è®¡æ•°å€¼ç›¸å…³è”ã€‚ï¼ˆè¿™é‡Œé¢ç”¨åˆ°äº†å‰é¢å®šä¹‰çš„parentå’ŒnodeLinkå±æ€§ï¼‰</p><p><strong>2 æ„é€ æ¡ä»¶FPæ ‘</strong>ï¼šä¸æ„é€ æ ‘çš„è¿‡ç¨‹ç›¸åŒï¼Œä½¿ç”¨çš„dataSetæ¢æˆäº†æ¡ä»¶æ¨¡å¼åŸºè€Œå·²ï¼Œå‡½æ•°å‚æ•°countå°±æ˜¯é¢„ç•™å½©è›‹ã€‚è¿™æ ·å¾—åˆ°çš„å°±æ˜¯<strong>æŒ‡å®šé¢‘ç¹é¡¹çš„æ¡ä»¶FPæ ‘</strong>ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateTree</span><span class="params">(cond_set, myTree, freq_header, count)</span>:</span></span><br><span class="line"><span class="comment"># cond_set: ä¸€æ¡path</span></span><br><span class="line"><span class="comment"># myTree: æ ¹èŠ‚ç‚¹</span></span><br><span class="line"><span class="comment"># freq_header: dict[node_name] = [frequency, head_node]</span></span><br><span class="line"><span class="comment"># count: pathå¯¹åº”çš„count</span></span><br></pre></td></tr></table></figure><ul><li>æ„é€ çš„æ¡ä»¶FPæ ‘è¿‡æ»¤æ‰äº†æ¡ä»¶æ¨¡å¼åŸºä¸­çš„ä¸€äº›å…ƒç´ ï¼šè¿™äº›å…ƒç´ æœ¬èº«æ˜¯é¢‘ç¹é¡¹ï¼Œä½†æ˜¯ä¸æŒ‡å®šå…ƒç´ ç»„åˆçš„é›†åˆä¸æ˜¯é¢‘ç¹çš„ã€‚</li><li>ç›¸åº”åœ°ï¼Œæ¡ä»¶æ ‘ä¸­å‰©ä½™å…ƒç´ ä¸æŒ‡å®šé¢‘ç¹é¡¹ç»„åˆçš„é›†åˆæ˜¯é¢‘ç¹çš„ã€‚</li></ul><p><strong>3 è¿­ä»£</strong>ï¼šä»ç”Ÿæˆçš„æ¡ä»¶FPæ ‘ä¸­ï¼Œå¯ä»¥å¾—åˆ°æ›´å¤æ‚çš„é¢‘ç¹é¡¹ã€‚æ±‚è§£å¤æ‚é¢‘ç¹é¡¹çš„æ¡ä»¶æ¨¡å¼åŸºï¼Œè¿›è€Œç”Ÿæˆå¯¹åº”çš„æ¡ä»¶FPæ ‘ï¼Œå°±èƒ½å¾—åˆ°æ›´å¤æ‚çš„é¢‘ç¹é¡¹ï¼Œä¾æ¬¡ç±»æ¨è¿›è¡Œè¿­ä»£ï¼Œç›´åˆ°FPæ ‘ä¸ºç©ºã€‚</p><p>â€‹    </p>]]></content>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>sublimeæ³¨å†Œç è¢«æ— é™æ¬¡ç§»é™¤</title>
      <link href="/2018/04/25/sublime%E6%B3%A8%E5%86%8C%E7%A0%81%E8%A2%AB%E6%97%A0%E9%99%90%E6%AC%A1%E7%A7%BB%E9%99%A4/"/>
      <url>/2018/04/25/sublime%E6%B3%A8%E5%86%8C%E7%A0%81%E8%A2%AB%E6%97%A0%E9%99%90%E6%AC%A1%E7%A7%BB%E9%99%A4/</url>
      <content type="html"><![CDATA[<p>æœ€è¿‘ä¸çŸ¥é“sublime3æŠ½ä»€ä¹ˆé£ï¼Œçªç„¶å¼€å§‹éªŒè¯æ³¨å†Œç äº†ï¼Œè¾“å…¥ä¸€ä¸ªvalid codeåˆ†åˆ†é’Ÿç»™ä½ ç§»é™¤ã€‚</p><p>æ”¶è—ä¸€ä¸ªè§£å†³åŠæ³•ï¼Œæœ‰æ•ˆæ€§å¾…éªŒè¯ï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># add the following to your host file(/private/etc/hosts)</span><br><span class="line">127.0.0.1 license.sublimehq.com</span><br><span class="line">127.0.0.1 45.55.255.55</span><br><span class="line">127.0.0.1 45.55.41.223</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> tool </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>VTKç¼–è¯‘æŠ¥é”™no override found for vtkpolydatamapper</title>
      <link href="/2018/04/12/VTK%E7%BC%96%E8%AF%91%E6%8A%A5%E9%94%99no-override-found-for-vtkpolydatamapper/"/>
      <url>/2018/04/12/VTK%E7%BC%96%E8%AF%91%E6%8A%A5%E9%94%99no-override-found-for-vtkpolydatamapper/</url>
      <content type="html"><![CDATA[<p>æŠ¥é”™åŸå› æ˜¯é€šè¿‡IDEç¼–è¯‘è€Œä¸æ˜¯ç›´æ¥é€šè¿‡cmakeï¼Œå› æ­¤è¦æ·»åŠ å¦‚ä¸‹ä»£ç æ®µï¼š</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"vtkAutoInit.h"</span>   </span></span><br><span class="line">VTK_MODULE_INIT(vtkRenderingOpenGL2); </span><br><span class="line">VTK_MODULE_INIT(vtkInteractionStyle);</span><br></pre></td></tr></table></figure><p>å…ˆè®°å½•è§£å†³åŠæ³•ï¼Œmore details ç•™åˆ°ä»¥åã€‚</p><p>åŸºç¡€æµ‹è¯•ï¼š</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"vtkAutoInit.h"</span></span></span><br><span class="line">VTK_MODULE_INIT(vtkRenderingOpenGL2); <span class="comment">// VTK was built with vtkRenderingOpenGL2</span></span><br><span class="line">VTK_MODULE_INIT(vtkInteractionStyle);</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vtkSphereSource.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vtkPolyData.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vtkSmartPointer.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vtkPolyDataMapper.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vtkActor.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vtkRenderWindow.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vtkRenderer.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vtkRenderWindowInteractor.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span>, <span class="keyword">char</span> *[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">// Create a sphere</span></span><br><span class="line">  vtkSmartPointer&lt;vtkSphereSource&gt; sphereSource =</span><br><span class="line">          vtkSmartPointer&lt;vtkSphereSource&gt;::New();</span><br><span class="line">  sphereSource-&gt;SetCenter(<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>);</span><br><span class="line">  sphereSource-&gt;SetRadius(<span class="number">5.0</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//mapper</span></span><br><span class="line">  vtkSmartPointer&lt;vtkPolyDataMapper&gt; mapper =</span><br><span class="line">          vtkSmartPointer&lt;vtkPolyDataMapper&gt;::New();</span><br><span class="line">  mapper-&gt;SetInputConnection(sphereSource-&gt;GetOutputPort());</span><br><span class="line"></span><br><span class="line">  <span class="comment">//actor</span></span><br><span class="line">  vtkSmartPointer&lt;vtkActor&gt; actor =</span><br><span class="line">          vtkSmartPointer&lt;vtkActor&gt;::New();</span><br><span class="line">  actor-&gt;SetMapper(mapper);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//renderer ,renderWindow, renderWindowInteractor.</span></span><br><span class="line">  vtkSmartPointer&lt;vtkRenderer&gt; renderer =</span><br><span class="line">          vtkSmartPointer&lt;vtkRenderer&gt;::New();</span><br><span class="line">  vtkSmartPointer&lt;vtkRenderWindow&gt; renderWindow =</span><br><span class="line">          vtkSmartPointer&lt;vtkRenderWindow&gt;::New();</span><br><span class="line">  renderWindow-&gt;AddRenderer(renderer);</span><br><span class="line">  vtkSmartPointer&lt;vtkRenderWindowInteractor&gt; renderWindowInteractor =</span><br><span class="line">          vtkSmartPointer&lt;vtkRenderWindowInteractor&gt;::New();</span><br><span class="line">  renderWindowInteractor-&gt;SetRenderWindow(renderWindow);</span><br><span class="line"></span><br><span class="line">  renderer-&gt;AddActor(actor);</span><br><span class="line">  renderer-&gt;SetBackground(<span class="number">.3</span>, <span class="number">.6</span>, <span class="number">.3</span>); <span class="comment">// Background color green</span></span><br><span class="line">  renderWindow-&gt;Render();</span><br><span class="line">  renderWindowInteractor-&gt;Start();</span><br><span class="line">  <span class="keyword">return</span> EXIT_SUCCESS;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> basic </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
