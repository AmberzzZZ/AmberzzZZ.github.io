<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[dag]]></title>
    <url>%2F2021%2F10%2F11%2Fdag%2F</url>
    <content type="text"><![CDATA[ç¾ç ”é™¢çš„è®ºæ–‡ï¼Œæ£€æµ‹ï¼Œç”¨äºè…°æ¤/é«‹å…³èŠ‚å…³é”®ç‚¹æå– preparations hrnet pspModule Structured Landmark Detection via Topology-Adapting Deep Graph Learning åŠ¨æœº landmark detection ç‰¹å¾ç‚¹æ£€æµ‹ identify the locations of predefined fiducial points capture relationships among è§£å‰–å­¦ç‰¹å¾ç‚¹ ä¸€ä¸ªéš¾ç‚¹ï¼šé®æŒ¡/å¤æ‚å‰æ™¯çŠ¶æ€ä¸‹ï¼Œlandmarkçš„å‡†ç¡®æ£€æµ‹å’Œå®šä½â€”â€”structual information the proposed method ç”¨äºfacial and medical landmark detection topology-adaptingï¼šlearnable connectivity learn end-to-end with two GCNs è®ºç‚¹ heatmap regression based methods å°†landmarkså»ºæ¨¡æˆheatmapsï¼Œç„¶åå›å½’ lacking a global representation æ ¸å¿ƒè¦ç´ æœ‰bottom-up/top-down paths &amp; multi-scale fusions &amp; high resolution heatmap outputs coordinate regression based methods potentially incorporate structural knowledge but a lot yet to be explored falls behind heatmap-based ones æ ¸å¿ƒè¦ç´ æ˜¯cascaded &amp; global &amp; local å¥½å¤„æ˜¯ç»“æ„åŒ–ï¼Œä¸ä¸¢ç‚¹ï¼Œä¸å¤šç‚¹ï¼Œä½†æ˜¯ä¸ä¸€å®šå‡† graph methods åŸºäºlandmark locationså’Œlandmark-to-landmark-relationshipsæ„å»ºå›¾ç»“æ„ most methods relies on heatmap detection results we would directly regress landmark locations from raw input image we propose DAGï¼šdeep adaptive graph å°†landmarkså»ºæ¨¡æˆgraphå›¾ employ global-to-local cascaded Graph Convolution Networksé€æ¸å°†landmarkèšç„¦åœ¨ç›®æ ‡ä½ç½® graph signals combines local image features graph shape features cascade two GCNs ç¬¬ä¸€ä¸ªé¢„æµ‹ä¸€ä¸ªglobal transform ç¬¬äºŒä¸ªé¢„æµ‹local offsets to further adjust contributions effectively exploit the structural knowledge allow rich exchange among landmarks narrow the gap between coordinate &amp; heatmap based methods æ–¹æ³• the cascaded-regression framework input image initial landmarks from the mean shape outputs predicted landmark coordinates in multiple steps feature use graph representation G = (V,E,F) Væ˜¯èŠ‚ç‚¹ï¼Œä»£è¡¨landmarksï¼Œä¹Ÿå°±æ˜¯ç‰¹å¾ç‚¹ï¼Œè¡¨ç¤ºä¸º(x,y)çš„åæ ‡ Eæ˜¯è¾¹ï¼Œä»£è¡¨connectivity between landmarksï¼Œè¡¨ç¤ºä¸º(id_i, id_k)çš„æ— å‘/æœ‰å‘æ˜ å°„ï¼Œæ•´ä½“çš„E matrixæ˜¯ä¸ªç¨€ç–çŸ©é˜µ Fæ˜¯graph signalsï¼Œcapturing appearance and shape informationï¼Œè¡¨ç¤ºä¸ºé«˜ç»´å‘é‡ï¼Œå¦‚256-dim vecï¼Œä¸èŠ‚ç‚¹Vä¸€ä¸€å¯¹åº”ï¼Œç”¨äºå‚¨å­˜èŠ‚ç‚¹ä¿¡æ¯ï¼Œåœ¨GCNä¸­å®é™…è¿›è¡Œè®¡ç®—äº¤äº’ overview summary cascadeï¼šä¸€ä¸ªGCN-globalåšç²—å®šä½ï¼Œè¿­ä»£å¤šä¸ªGCN-localåšpreciseå®šä½ interpolationï¼šfeature mapåˆ°feature nodesçš„è½¬æ¢ï¼Œé€šè¿‡interpolationï¼Œã€æ˜¯global interpå—ï¼Œæ˜¯åŸºäºinitial mean coordså—ã€‘ regressionï¼šã€targetsçš„å…·ä½“åæ ‡è¡¨ç¤ºï¼Ÿï¼Ÿï¼Ÿã€‘ inital graphï¼šè®­ç»ƒé›†çš„å¹³å‡å€¼ graph signalï¼švisual featureå’Œshape feature Cascaded GCNs GCN-globalï¼šglobal transformation GCN-localï¼šcoordinate offsets share the same GCN architecture graph convolution æ ¸å¿ƒæ€æƒ³å°±æ˜¯ï¼šç»™å®šä¸€ä¸ªå›¾ç»“æ„ï¼ˆwith connectivity Eï¼‰ï¼Œæ¯ä¸€æ¬¡å †å graph convolutionï¼Œå°±æ˜¯åœ¨å¯¹æ¯ä¸ªå›¾èŠ‚ç‚¹ï¼ŒåŸºäºå…¶è‡ªèº«$f_k^i$å’Œé‚»å±…èŠ‚ç‚¹$f_k^j$çš„å½“å‰graph featureï¼Œweighted aggregatingï¼Œç»“æœä½œä¸ºè¿™ä¸ªèŠ‚ç‚¹è¿™æ¬¡å›¾å·ç§¯çš„è¾“å‡º$f_{k+1}^i$ f_{k+1}^i = W_1 f_k^i + \sum_j e_{ij}W_2 f_k^j learnable weight matrices $W_1$ å’Œ $W_2$ å¯ä»¥çœ‹ä½œæ˜¯é‚»å±…èŠ‚ç‚¹é—´ä¿¡æ¯äº¤äº’çš„ä¸€ç§æ–¹å¼ Global Transformation GCN è¿™ä¸ªmodelçš„ä½œç”¨æ˜¯å°†initial landmarkså˜æ¢åˆ°coarse targets å‚ç…§STNï¼Œ recall STN ä½¿ç”¨perspective transformationé€è§†å˜æ¢ï¼Œå¼•å…¥9ä¸ªscalarsï¼Œè¿›è¡Œå›¾å½¢å˜ workflow given a target image initialize landmark locations $V^0$ using trainingset mean GCN-global + GIN é¢„æµ‹perspective transformation è¿›è€Œå¾—åˆ°å˜æ¢åçš„èŠ‚ç‚¹ä½ç½® graph isomorphism network (GIN) å›¾çš„çº¿æ€§å±‚ è¾“å…¥æ˜¯GCN-globalçš„graph features $\{f_k^i\}$ è¾“å‡ºæ˜¯9-dim vector è®¡ç®—æ–¹å¼ READOUTï¼šsum the features from all nodes CONCATï¼šå¾—åˆ°ä¸€ä¸ªé«˜ç»´å‘é‡ MLPï¼š9-dim fc æœ€åå¾—åˆ°9-dimçš„perspective transformation scalar coordinate update å°†9-dim $f^G$ reshapeæˆ3x3 transformation matrix M ç„¶ååœ¨å½“å‰çš„landmark locations $V^0$ä¸Šæ–½åŠ å˜æ¢â€”â€”çŸ©é˜µå·¦ä¹˜ Local Refinement GCN GCNç»“æ„ä¸globalçš„ä¸€è‡´ï¼Œä½†æ˜¯ä¸shareæƒé‡ æœ€åçš„GINå¤´å˜äº† è¾“å‡ºæ”¹æˆ2-dim vector represents coordinate offsets coordinate update åŠ æ³•ï¼Œåˆ†åˆ«åœ¨x/yè½´åæ ‡ä¸Š we perform T=3 iterations Graph signal with appearance and shape information Visual Feature denote CNNè¾“å‡ºçš„feature map H with D channels encodingæ•´ä¸ªfeature mapï¼šbi-linear interpolation at the landmark location $v_i$ï¼Œè®°ä½œ$p_i$ï¼Œæ˜¯ä¸ªD-dim vector Shape Feature visual featureå¯¹èŠ‚ç‚¹é—´å…³ç³»çš„å»ºæ¨¡ï¼ŒåŸºäºglobal mapå…¨å±€ä¿¡æ¯æå–ï¼Œæ¯”è¾ƒéšå¼ã€é—´æ¥ äº‹å®ä¸Šå›¾ç»“æ„èƒ½å¤Ÿç›´æ¥å¯¹global landmarks shapeè¿›è¡Œencoding æœ¬æ–‡ç”¨displacement vectorsï¼Œå°±æ˜¯è·ç¦»ï¼Œæ¯ä¸ªèŠ‚ç‚¹çš„displacement vectorè®°ä½œ$q_i=\{v_j-v_i\}_{j!=i}$ï¼Œflattenæˆä¸€ç»´ï¼Œå¯¹æœ‰Nä¸ªèŠ‚ç‚¹çš„å›¾ï¼Œæ¯ä¸ªèŠ‚ç‚¹çš„q-vecç»´åº¦ä¸º2*(N-1) shape featureä¿å­˜äº†structural informationï¼Œå½“äººè„¸çš„å˜´è¢«é®ä½çš„æƒ…å†µä¸‹ï¼ŒåŸºäºçœ¼ç›å’Œé¼»å­ä»¥åŠç»“æ„æ€§ä¿¡æ¯ï¼Œå°±èƒ½å¤Ÿæ¨æ–­å˜´çš„ä½ç½®ï¼Œè¿™æ˜¯Visual Featureä¸èƒ½ç›´æ¥è¡¨è¾¾çš„ graph signal concat result in a feature vector $f_i \in R^{D+2(N-1)}$ Landmark graph with learnable connectivity å¤§å¤šæ•°æ–¹æ³•çš„å›¾åŸºäºå…ˆéªŒçŸ¥è¯†æ„å»º we learn task-specific graph connectivity during training phase å›¾çš„connectivity serves as a gateï¼Œç”¨é‚»æ¥çŸ©é˜µè¡¨ç¤ºï¼Œå¹¶å°†å…¶ä½œä¸ºlearnable weights training GCN-global margin loss $v_i^1$æ˜¯GCN-globalçš„é¢„æµ‹èŠ‚ç‚¹åæ ‡ mæ˜¯margin $[u]_+$æ˜¯$max(0,u)$ pushèŠ‚ç‚¹åæ ‡åˆ°æ¯”è¾ƒæ¥è¿‘ground truthå°±åœæ­¢äº†ï¼Œé˜²æ­¢ä¸ç¨³å®š GCN-local L1 loss $v_i^T$æ˜¯ç¬¬Tä¸ªiteration GCN-localçš„é¢„æµ‹èŠ‚ç‚¹åæ ‡ overall loss åŠ æƒå’Œ ç½‘ç»œç»“æ„ GCN-global ä¸‰å±‚basic graph convolution layer with residualï¼ˆid pathï¼‰ concat distance vector ä¸€å±‚basic graph convolution mean axis1ï¼ˆnode axisï¼‰ fcï¼Œè¾“å‡º9-dim scalarï¼Œ(b,9) GCN-local ä¸‰å±‚basic graph convolution layer with residualï¼ˆid pathï¼‰ relu concat distance vector ä¸€å±‚basic graph convolution fcï¼Œè¾“å‡º2-dim coords for each nodeï¼Œ(b,24,2)]]></content>
      <tags>
        <tag>GCN, landmark detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KL Divergence]]></title>
    <url>%2F2021%2F09%2F27%2FKL-Divergence%2F</url>
    <content type="text"><![CDATA[KL divergenceç”¨äºåº¦é‡ä¸¤ä¸ªåˆ†å¸ƒPå’ŒQçš„å·®å¼‚ï¼Œè¿™ç§åº¦é‡ã€ä¸å…·æœ‰ã€‘å¯¹ç§°æ€§ Pæ˜¯å®é™…åˆ†å¸ƒï¼ˆpred probsï¼‰ Qæ˜¯å»ºæ¨¡åˆ†å¸ƒï¼ˆgtï¼‰ $D_{KL}(P||Q)=\sum_i P(i)ln\frac{P(i)}{Q(i)}$ æ•£åº¦å®šä¹‰ä¸ºåˆ†å¸ƒPå’Œåˆ†å¸ƒQä¹‹é—´çš„å¯¹æ•°å·®å¼‚çš„åŠ æƒå’Œï¼Œç”¨Pçš„æ¦‚ç‡å»åŠ æƒ å½“Qæ˜¯one-hot labelçš„æ—¶å€™ï¼Œè¦å…ˆclipå†log æ–¹æ³• torch.nn.functional.kl_div(input, target, size_average=None, reduce=None, reduction=â€™meanâ€™) inputï¼šå¯¹æ•°æ¦‚ç‡ targetï¼šæ¦‚ç‡ tf.distributions.kl_divergence(distribution_a, distribution_b, allow_nan_stats=True, name=None) distribution_a&amp;b æ¥è‡ªtf.distributions.Categorical(logits=None, prob=None, â€¦) ä¼ å…¥logits/probsï¼Œå…ˆè½¬æ¢æˆdistributionï¼Œå†è®¡ç®—kl divergence torch.nn.KLDivLoss tf.keras.losses.KLDivergence tf.keras.losses.kullback_leibler_divergence code 12345678910111213141516171819202122232425# torch versionimport torch.nn as nnimport torch.nn.functional as Fclass KL(nn.Module): def __init__(self, args): super(KL, self).__init__() self.T = args.temperature def forward(self, logits_p, logits_q): log_p = F.log_softmax(logits_p/self.T, dim=1) q = F.softmax(logits_q/self.T, dim=1) loss = F.kl_div(log_p, p_t) # keras versionimport tensorflow as tfimport keras.backend as Kdef kl_div(logits_p, logits_q): T = 4. log_p = tf.nn.log_softmax(logits_p/T) # (b,cls) log_q = tf.nn.log_softmax(logits_q/T) p = K.exp(log_p) return K.sum(p*(log_p-log_q), axis=-1) # (b,)]]></content>
      <tags>
        <tag>æ•°å­¦</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Self-Knowledge Distillation]]></title>
    <url>%2F2021%2F09%2F17%2FSelf-Knowledge-Distillation%2F</url>
    <content type="text"><![CDATA[Refine Myself by Teaching Myself : Feature Refinement via Self-Knowledge Distillation åŠ¨æœº ä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦ by stageï¼šå…ˆè®­ç»ƒåºå¤§çš„teacher self knowledge distillation without the pretrained network åˆ†ä¸ºdata augmentation based approach å’Œ auxiliary network based approach data augmentation approachå¦‚UDAï¼Œé€šè¿‡ç›‘ç£åŸå§‹å›¾åƒå’Œå¢å¼ºå›¾åƒçš„ä¸€è‡´æ€§ï¼Œä½†æ˜¯ä¼šloose local informationï¼Œå¯¹pixel-level tasksä¸å‹å¥½ï¼Œè€Œä¸”ç›‘ç£ä¿¡æ¯æ˜¯ä»logitså±‚ï¼Œæ²¡æœ‰ç›´æ¥å»refine feature maps our approach FRSKD auxiliary network based approach utilize both soft label and featuremap distillation è®ºç‚¹ various distillation methods aæ˜¯ä¼ ç»ŸçŸ¥è¯†è’¸é¦ï¼Œæ·±ç»¿è‰²æ˜¯pretrained teacherï¼Œæµ…ç»¿è‰²æ˜¯studentï¼Œæ©™è‰²ç®­å¤´æ˜¯featureè’¸é¦ï¼Œç»¿è‰²ç®­å¤´æ˜¯soft labelè’¸é¦ bæ˜¯data augmentation based è‡ªè’¸é¦ï¼Œshared ç½‘ç»œï¼ŒåŸå›¾å’Œå¢å¼ºåçš„å›¾ï¼Œç”¨soft logitsæ¥è’¸é¦ cæ˜¯auxiliary classifier based è‡ªè’¸é¦ï¼Œcascadedåˆ†ç±»å¤´ï¼Œæ¯ä¸ªåˆ†ç±»å™¨éƒ½æ¥å‰ä¸€ä¸ªçš„ dæ˜¯æœ¬æ–‡è‡ªè’¸é¦ï¼Œå’Œcæœ€å¤§çš„ä¸åŒæ˜¯bifpnç»“æ„ä½¿å¾—ä¸¤ä¸ªåˆ†ç±»å™¨æ¯ä¸ªlevelçš„ç‰¹å¾å›¾ä¹‹é—´éƒ½æœ‰è¿ç»“ï¼Œç›‘ç£æ–¹å¼ä¸€æ ·çš„ FPN PANetï¼šä¸Šè¡Œ+ä¸‹è¡Œ biFPNï¼šä¸Šè¡Œ+ä¸‹è¡Œ+åŒå±‚çº§è” æ–¹æ³• overview notations dataset $D=\{(x_1,y_1), (x_2,y_2),â€¦, (x_N,y_N)\}$ feature map $F_{i,j}$ï¼Œi-th sampleï¼Œj-th block channel dimension $c_j$ï¼Œj-th block self-teacher network self-teacher networkçš„ç›®çš„æ˜¯æä¾›refined feature mapå’Œsoft labelsä½œä¸ºç›‘ç£ä¿¡æ¯ inputsï¼šfeature maps $F_1, F_2, â€¦, F_n$ï¼Œä¹Ÿå°±æ˜¯è¯´teacheråœ¨è¿›è¡Œæ¢¯åº¦å›ä¼ çš„æ—¶å€™åˆ°Få°±åœæ­¢äº†ï¼Œä¸ä¼šæ›´æ–°student modelçš„å‚æ•° modified biFPN ç¬¬ä¸€ä¸ªä¸åŒï¼šåˆ«çš„FPNéƒ½æ˜¯åœ¨fuseä¹‹å‰å…ˆç”¨ä¸€ä¸ªfixed-dim 1x1 convå°†æ‰€æœ‰levelçš„feature mapè½¬æ¢æˆç›¸åŒé€šé“æ•°ï¼ˆå¦‚256ï¼‰ï¼Œwe design $d_i$ according to $c_i$ï¼Œå¼•å…¥ä¸€ä¸ªå®½åº¦ç³»æ•°widthï¼Œ$d_i=width*c_i$ï¼Œ ç¬¬äºŒä¸ªä¸åŒï¼šä½¿ç”¨depth-wise convolution notations BiFPNï¼šæ¯å±‚dimå›ºå®šçš„ç‰ˆæœ¬ BiFPNcï¼šæ¯å±‚diméšè¾“å…¥å˜åŒ–çš„ç‰ˆæœ¬ self-feature distillation feature distillation adapt attention transfer å¯¹feature mapå…ˆè¿›è¡Œchannel-wiseçš„poolingï¼Œç„¶åL2 normï¼Œæå–spatial information soft label distillation ä¸¤ä¸ªåˆ†ç±»å¤´çš„KL divergence CE with gt ä¸¤ä¸ªåˆ†ç±»å¤´åˆ†åˆ«è¿˜æœ‰æ­£å¸¸çš„CE loss overall æ€»çš„lossæ˜¯4ä¸ªlossç›¸åŠ ï¼š$L_{FRSKD}(x,y,\theta_c, \theta_t, K)=L_{CE}(x,y,\theta_c)+L_{CE}(x,y,\theta_t)+\alpha L_{KD}(x,\theta_c,\theta_t, K) + \beta L_{F}(T,F,\theta_c,\theta_T)$ $\alpha \in [1,2,3]$ $\beta \in [100,200]$ ã€QUESTIONã€‘FRSKD updates the parameters by the distillation lossï¼Œ$L_{KD}$ and $L_F$ï¼Œwhich is only applied to the student networkï¼Œè¿™ä¸ªå•¥æ„æ€æš‚æ—¶æ²¡ç†è§£ å®éªŒ experiment settings FRSKD\Fï¼šåªåšsoft labelçš„ç›‘ç£ï¼Œä¸åšfeature mapçš„ç›‘ç£ FRSKDï¼šæ ‡å‡†çš„æœ¬æ–‡æ–¹æ³• FRSKD+SLAï¼šæœ¬æ–‡æ–¹æ³•çš„åŸºç¡€ä¸Šattach data augmentation based distillation]]></content>
  </entry>
  <entry>
    <title><![CDATA[L2 Regularization and Batch Norm]]></title>
    <url>%2F2021%2F09%2F16%2FL2-Regularization-and-Batch-Norm%2F</url>
    <content type="text"><![CDATA[referenceï¼š https://blog.janestreet.com/l2-regularization-and-batch-norm/ https://zhuanlan.zhihu.com/p/56142484 https://vitalab.github.io/article/2020/01/24/L2-reg-vs-BN.html è§£é‡Šäº†ä¹‹å‰çš„ä¸€ä¸ªç–‘ç‚¹ï¼š åœ¨kerasè‡ªå®šä¹‰çš„BNå±‚ä¸­ï¼Œæ²¡æœ‰ç±»ä¼¼kernel_regularizerè¿™æ ·çš„å‚æ•° åœ¨æˆ‘ä»¬å†™è‡ªå®šä¹‰optmizerçš„æ—¶å€™ï¼ŒBNå±‚ä¹Ÿä¸è¿›è¡Œweight decayçš„ L2 Regularization versus Batch and Weight Normalization åŠ¨æœº ä¸¤ä¸ªcommon tricksï¼šNormalizationï¼ˆBNã€WNã€LNç­‰ï¼‰å’ŒL2 Regularization å‘ç°ä¸¤è€…ç»“åˆæ—¶L2 regularizationå¯¹normalizationå±‚æ²¡æœ‰æ­£åˆ™æ•ˆæœ L2 regularizationåè€Œå¯¹norm layerçš„scaleæœ‰å½±å“ï¼Œé—´æ¥å½±å“äº†learning rate ç°ä»£ä¼˜åŒ–å™¨å¦‚Adamåªèƒ½é—´æ¥æ¶ˆé™¤è¿™ç§å½±å“ è®ºç‚¹ BN popular in training deep networks solve the problem of covariate shift ä½¿å¾—æ¯ä¸ªç¥ç»å…ƒçš„è¾“å…¥ä¿æŒnormalåˆ†å¸ƒï¼ŒåŠ é€Ÿè®­ç»ƒ mean &amp; varianceï¼štraining timeåŸºäºæ¯ä¸ªmini-batchè®¡ç®—ï¼Œtest timeä½¿ç”¨æ‰€æœ‰iterationçš„mean &amp; varianceçš„EMA usually trained with SGD with L2 regularization result in weight decayï¼šä»æ•°å­¦è¡¨ç¤ºä¸Šç­‰ä»·äºå¯¹æƒé‡åšè¡°å‡ æ¯ä¸€æ­¥æƒé‡scaled by a å°äº1çš„æ•° ä½†æ˜¯normalization strategiesæ˜¯å¯¹scale of the weights invariantçš„ï¼Œå› ä¸ºåœ¨è¾“å…¥ç¥ç»å…ƒä¹‹å‰éƒ½ä¼šè¿›è¡Œnorm therefore there is no regularizing effect rather strongly influence the learning rate??ğŸ‘‚ L2 Regularization formulationï¼š åœ¨lossçš„åŸºç¡€ä¸ŠåŠ ä¸€ä¸ªregularization termï¼Œ$L_{\lambda}(w)=L(w)+\lambda ||w||^2_2$ lossæ˜¯æ¯ä¸ªæ ·æœ¬ç»è¿‡ä¸€ç³»åˆ—æƒé‡è¿ç®—ï¼Œ$L(w)=\sum_N l_i (y(X_i;w,\gamma,\beta))$ å½“ä½¿ç”¨normalization layerçš„æ—¶å€™ï¼š$y(X_i;w,\alpha,\beta)=y(X_i;\alpha w,\gamma,\beta)$ï¼Œå³loss termä¸ä¼šå˜ $L_{\lambda}(\alpha w)=L(w)+\lambda||w||^2_2$ åœ¨æœ‰normalization layerçš„æ—¶å€™ï¼ŒL2 penaltyè¿˜æ˜¯èƒ½å¤Ÿé€šè¿‡reg term forceæƒé‡çš„scaleè¶Šæ¥è¶Šå°ï¼Œä½†æ˜¯ä¸ä¼šå½±å“ä¼˜åŒ–è¿›ç¨‹ï¼ˆä¸å½±å“main objective valueï¼‰ï¼Œå› ä¸ºloss termä¸å˜ Effect of the Scale of Weights on Learning Rate BNå±‚çš„è¾“å‡ºæ˜¯scale invariantçš„ï¼Œä½†æ˜¯æ¢¯åº¦ä¸æ˜¯ï¼Œæ¢¯åº¦æ˜¯æˆåæ¯”è¢«æŠ‘åˆ¶çš„ï¼ æ‰€ä»¥weightsåœ¨å˜å°ï¼ŒåŒæ—¶æ¢¯åº¦åœ¨å˜å¤§ï¼ åœ¨å‡å°weight scaleçš„æ—¶å€™ï¼Œç½‘ç»œçš„æ¢¯åº¦ä¼šå˜å¤§ï¼Œç­‰ä»·äºå­¦ä¹ ç‡åœ¨å˜å¤§ï¼Œä¼šå¼•èµ·éœ‡è¡ä¸ç¨³å®š æ‰€ä»¥åœ¨è®¾å®šhyperçš„æ—¶å€™ï¼Œå¦‚æœæˆ‘ä»¬è¦é€‚å½“åŠ å¤§weight decay $\lambda$ï¼Œå°±è¦åæ¯”scaleå­¦ä¹ ç‡ Effect of Regularization on the Scale of Weights during training the scale of weights will change the gradients of the loss function will cause the norm of the weights to grow the regularization term causes the weights to shrink]]></content>
      <tags>
        <tag>æ­£åˆ™åŒ–</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SAM]]></title>
    <url>%2F2021%2F09%2F10%2FSAM%2F</url>
    <content type="text"><![CDATA[google brainï¼Œå¼•ç”¨é‡51ï¼Œä½†æ˜¯ImageNetæ¦œå•/SOTAæ¨¡å‹çš„å¯¹æ¯”å®éªŒé‡Œé¢ç»å¸¸èƒ½å¤Ÿçœ‹åˆ°è¿™ä¸ªSAMï¼Œå‡ºåœˆå½¢å¼ä¸ºåˆ†ç±»æ¨¡å‹+SAM SAMï¼šSharpness-Aware Minimizationï¼Œé”åº¦æ„ŸçŸ¥æœ€å°åŒ– official repoï¼šhttps://github.com/google-research/sam Sharpness-Aware Minimization for Efficiently Improving Generalization åŠ¨æœº heavily overparametered modelsï¼štraining lossèƒ½è®­åˆ°æå°ï¼Œä½†æ˜¯generalization issue we propose Sharpness-Aware Minimization (SAM) åŒæ—¶æœ€å°åŒ–losså’Œloss sharpness improve model generalization robustness to label noise verified on CIFAR 10&amp;100 ImageNet finetuning tasks è®ºç‚¹ typical loss &amp; optimizer population lossï¼šæˆ‘ä»¬å®é™…æƒ³å¾—åˆ°çš„æ˜¯åœ¨å½“å‰è®­ç»ƒé›†æ‰€ä»£è¡¨çš„åˆ†å¸ƒä¸‹çš„æœ€ä¼˜è§£ training set lossï¼šä½†äº‹å®ä¸Šæˆ‘ä»¬åªèƒ½ç”¨æ‰€æœ‰çš„è®­ç»ƒæ ·æœ¬æ¥ä»£è¡¨è¿™ä¸ªåˆ†å¸ƒ å› ä¸ºlosså‡½æ•°æ˜¯non-convexçš„ï¼Œæ‰€ä»¥å¯èƒ½å­˜åœ¨å¤šä¸ªlocal even global minimaå¯¹åº”çš„loss valueæ˜¯ä¸€æ ·çš„ï¼Œä½†æ˜¯generalization performanceç¡®æ˜¯ä¸åŒçš„ æˆç†Ÿçš„å…¨å¥—é˜²æ­¢è¿‡æ‹Ÿåˆæ‰‹æ®µ loss optimizer dropout batch normalization mixed sample augmentations our approach directly leverage the geometry of the loss landscape and its connection to generalization (generalization bound) proved additive to existing techniques æ–¹æ³• motivation rather than å¯»æ‰¾ä¸€ä¸ªweight value that have low lossï¼Œæˆ‘ä»¬å¯»æ‰¾çš„æ˜¯é‚£ç§è¿å¸¦ä»–ä¸´è¿‘çš„valueéƒ½èƒ½æœ‰low lossçš„value ä¹Ÿå°±æ˜¯æ—¢æœ‰low lossåˆæœ‰lowæ›²åº¦ sharpness term $\max \limits_{||\epsilon||_p &lt; \rho} L_s(w+\epsilon) - L_s(w)$ è¡¡é‡æ¨¡å‹åœ¨wå¤„çš„sharpness Sharpness-Aware Minimization (SAM) formulation sharpness termå†åŠ ä¸Štrain losså†åŠ ä¸Šregularization term $L_S^{SAM}(w)=\max\limits_{a} L_s(w+\epsilon)$ $\min \limits_{w} L_S^{SAM}(w) + \lambda ||w||^2_2$ prevent the model from converting to a sharp minimum effective approximation bound with $\frac{1}{p} + \frac{1}{q} = 1$ approximation pseudo code given a min-batch é¦–å…ˆè®¡ç®—å½“å‰batchçš„training lossï¼Œå’Œå½“å‰æ¢¯åº¦ï¼Œ$w_t$ to $w_{t+1}$ ç„¶åè®¡ç®—è¿‘ä¼¼ä¸ºæ¢¯åº¦normçš„æ­¥é•¿$\hat\epsilon(w)$ï¼Œequation2ï¼Œ$w_t$ to $w_{adv}$ï¼Œè¿™é‡Œé¢çš„advè”åŠ¨äº†å¦ä¸€ç¯‡è®ºæ–‡ã€ŠAdvProp: Adversarial Examples Improve Image Recognitionã€‹ ç„¶åè®¡ç®—è¿‘ä¼¼çš„sharpness termï¼Œå¯ä»¥ç†è§£ä¸ºtraining lossåœ¨wé‚»å±…å¤„çš„æ¢¯åº¦ï¼Œequation3ï¼Œåº”è¯¥æ˜¯è“è‰²ç®­å¤´çš„åæ–¹å‘ï¼Œå›¾ä¸Šæ²¡æ ‡è®°å‡ºæ¥ ç”¨wé‚»å±…çš„æ¢¯åº¦æ¥æ›´æ–°wçš„æƒé‡ï¼Œç”¨è´Ÿæ¢¯åº¦ï¼ˆè“è‰²ç®­å¤´ï¼‰ overllå°±æ˜¯ï¼šè¦å‘å‰èµ°ä¹‹å‰ï¼Œå…ˆå›é€€ï¼Œç¼ºç‚¹æ˜¯ä¸¤æ¬¡æ¢¯åº¦è®¡ç®—ï¼Œæ—¶é—´double å®éªŒç»“è®º èƒ½ä¼˜åŒ–åˆ°æŸå¤±çš„æœ€å¹³å¦çš„æœ€å°å€¼çš„åœ°æ–¹ï¼Œå¢å¼ºæ³›åŒ–èƒ½åŠ›]]></content>
  </entry>
  <entry>
    <title><![CDATA[MuSTè°·æ­Œå¤šä»»åŠ¡è‡ªè®­ç»ƒ]]></title>
    <url>%2F2021%2F09%2F01%2FMuST%2F</url>
    <content type="text"><![CDATA[recollect [SimCLR] [MoCo] Multi-Task Self-Training for Learning General Representations åŠ¨æœº learning general feature representations expect a single general model ç›¸æ¯”è¾ƒäºtraining specialized models for various tasks harness from independent specialized teacher models with a multi-task pseudo dataset trained with multi-task learning evalutate on 6 vision tasks image recognition (classification, detection, segmentation) 3D geometry estimation è®ºç‚¹ pretraining &amp; transfer learning transformerä¸€èˆ¬éƒ½æ˜¯è¿™ä¸ªå¥—è·¯ï¼ŒBiT&amp;ViT pretraining supervised / unsupervised learn feature representations transfer learning on downstream tasks the features may not necessarily be useful æœ€å…¸å‹çš„å°±æ˜¯ImageNet pre-trainingå¹¶ä¸èƒ½improve COCO segmentationï¼Œä½†æ˜¯Objects365èƒ½å¤Ÿå¤§å¹…æå‡ pretraining taskså¿…é¡»è¦å’Œdownstream task alignï¼Œlearn specialized featuresï¼Œä¸ç„¶ç™½è´¹ learning general features a model simultaneously do well on multiple tasks NLPçš„bertæ˜¯ä¸€ä¸ªå…¸å‹ç”¨å¤šä»»åŠ¡æå‡general abilityçš„ CVæ¯”è¾ƒéš¾è¿™æ ·åšæ˜¯å› ä¸ºæ ‡ç­¾varietyï¼Œæ²¡æœ‰è¿™æ ·çš„å¤§å‹multi-task dataset multi-task learning shared backbone (å¦‚ResNet-FPN) small task-specific heads self-training use a supervised model to generate pseudo labels on unlabeled data then a student model is trained on the pseudo labeled data åœ¨å„ç±»ä»»åŠ¡ä¸Šéƒ½provedæ¶¨ç‚¹ ä½†æ˜¯è¿„ä»Šä¸ºæ­¢éƒ½æ˜¯focused on a single task in this work lack of large scale multi-task datasetçš„issueï¼Œé€šè¿‡self-training to fixï¼Œç”¨pseudo label specialized/general issueï¼Œé€šè¿‡å¤šä»»åŠ¡ï¼Œè®­ç»ƒç›®æ ‡å°±æ˜¯å…­è¾¹å½¢æˆ˜å£«ï¼Œabsorb the knowledge of different tasks in the shared backbone three steps trains specialized teachers independently on labeled datasets ï¼ˆåˆ†ç±»ã€åˆ†å‰²ã€æ£€æµ‹ã€æ·±åº¦ä¼°è®¡ï¼‰ the specialized teachers are then used to label a larger unlabeled datasetï¼ˆImageNetï¼‰ to create a multi- task pseudo labeled dataset train a student model with multi-task learning MuSTçš„ç‰¹è´¨ improve with more unlabeled dataï¼Œæ•°æ®è¶Šå¤šgeneral featureè¶Šå¥½ can improve upon already strong checkpointsï¼Œåœ¨æµ·é‡ç›‘ç£é«˜ç²¾åº¦æ¨¡å‹åŸºç¡€ä¸Šfine-tuneï¼Œä»æ—§èƒ½åœ¨downstream tasksæ¶¨ç‚¹ æ–¹æ³• Specialized Teacher Models 4 teacher models classificationï¼štrain from scratchï¼ŒImageNet detectionï¼štrain from scratchï¼ŒObject365 segmentationï¼štrain from scratchï¼ŒCOCO depth estimationï¼šfine-tuning from pre-trained checkpoint pseudo labeling unlabeled / partially labeled datasets for detectionï¼šhard score threshold of 0.5 for segmentationï¼šhard score threshold of 0.5 for classificationï¼šsoft labelsâ€”â€”probs distribution for depthï¼šç›´æ¥ç”¨ Multi-Task Student Model æ¨¡å‹ç»“æ„ shared back C5ï¼šfor classification feature pyramids {P3,P4,P5,P6,P7}ï¼šfor detection fused P2ï¼šfor pixel-wise predictionï¼ŒæŠŠfeature pyramids rescaleåˆ°level2ç„¶åsum heads classification headï¼šResNet designï¼ŒGAP C5 + çº¿æ€§å±‚ object detection taskï¼šMask R-CNN designï¼ŒRPNæ˜¯2 hidden convsï¼ŒFast R-CNNæ˜¯4 hidden convs + 1 fc pixel-wise prediction headsï¼š3 hiddent convs + 1 linear conv headï¼Œåˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ä»»åŠ¡independentï¼Œä¸share heads Teacher-student training using the same architecture same data augmentation teacherå’Œstudentçš„main differenceå°±æ˜¯datasetå’Œlabels Learning From Multiple Teachers every image has supervision for all tasks labels may come from supervised or pseudo labels å¦‚æœä½¿ç”¨ImageNetæ•°æ®é›†ï¼Œclassificationå°±æ˜¯çœŸæ ‡ç­¾ï¼Œdet/seg/depth supervisionåˆ™æ˜¯ä¼ªæ ‡ç­¾ balance the loss contribution åŠ æƒå’Œï¼Œtask-specific weights for ImageNetï¼Œuse $w_i = \frac{b_slr_{it}}{b_{it}lr_{s}}$ follow the scaling ruleï¼šlrå’Œbatch sizeæˆæ­£æ¯” except for depth loss Cross Dataset Training training across ImageNet, object365 and COCO æœ‰æ ‡ç­¾çš„å°±ç”¨åŸæ ‡ç­¾ï¼Œæ²¡æœ‰çš„ç”¨ä¼ªæ ‡ç­¾ï¼Œsupervised labels and pseudo labels are treated equallyï¼Œè€Œä¸æ˜¯åˆ†åˆ«é‡‡æ ·å’Œè®­ç»ƒ balance the datasetsï¼šåˆåœ¨ä¸€èµ·ç„¶åå‡åŒ€é‡‡æ · Transfer Learning å¾—åˆ°general student modelä»¥åï¼Œfine-tune on ä¸€ç³»åˆ—downstream tasks è¿™äº›downstream datasetsä¸MuST modelçš„è®­ç»ƒæ•°æ®éƒ½æ˜¯not alignçš„ è¿™ä¸ªå®éªŒè¦è¯æ˜çš„æ˜¯supervised modelï¼ˆå¦‚teacher modelï¼‰å’Œself-supervised modelï¼ˆå¦‚ç”¨pseudo labelè®­ç»ƒå‡ºæ¥çš„student modelï¼‰ï¼Œåœ¨downstream tasksä¸Šè¿ç§»å­¦ä¹ èƒ½performanceæ˜¯å·®ä¸å¤šçš„ï¼Œã€æ³¨æ„âš ï¸ï¼šå¦‚æœè¿ç§»datasetså‰åalignå°±ä¸æ˜¯è¿™æ ·äº†ï¼Œpretrainæ˜¾ç„¶ä¼šæ›´å¥½ï¼ï¼ï¼ã€‘]]></content>
      <tags>
        <tag>multi-taskï¼Œself-trainingï¼Œ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GHM]]></title>
    <url>%2F2021%2F08%2F31%2FGHM%2F</url>
    <content type="text"><![CDATA[families: [class-imbalanced CE] [focal loss] [generalized focal loss] focal loss(CE)çš„è¿ç»­ç‰ˆæœ¬ [ohem] keras implementation: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def weightedCE_loss(y_true, y_pred): alpha = .8 pt = K.abs(y_true-y_pred) # clip pt = K.clip(pt, K.epsilon(), 1-K.epsilon()) # ce ce = -K.log(1.-pt) # pos/neg reweight wce = tf.where(y_true&gt;0.5, alpha* , (1-alpha)* ) return wcedef focal_loss(y_true, y_pred): alpha = .25 gamma = 2 pt = K.abs(y_true-y_pred) # clip pt = K.clip(pt, K.epsilon(), 1-K.epsilon()) # easy/hard reweight fl = -K.pow(pt, gamma) * K.log(1.-pt) # pos/neg reweight fl = tf.where(y_true&gt;0.5, alpha*fl, (1-alpha)*fl) return fl def generalized_focal_loss(y_true, y_pred): # CE = -ytlog(yp)-(1-yt)log(1-yp) # GFL = |yt-yp|^beta * CE beta = 2 # clip y_pred y_pred = K.clip(y_pred, K.epsilon(), 1-K.epsilon()) # ce ce = -y_true*K.log(y_pred) - (1-y_true)*K.log(1-y_pred) # [N,C] # easy/hard reweight gfl = K.pow(K.abs(y_true-y_pred), beta) * ce return gfldef ce_ohem(y_true, y_pred): pt = K.abs(y_true-y_pred) # clip pt = K.clip(pt, K.epsilon(), 1-K.epsilon()) # ce ce = -K.log(1.-pt) # sort loss k = 50 ohem_loss, indices = tf.nn.top_k(ce, k=k) # topk loss: [k,], topk indices: [k,], idx among 0-b mask = tf.where(ce&gt;=ohem_loss[k-1], tf.ones_like(ce), tf.zeros_like(ce)) return mask*ce Gradient Harmonized Single-stage Detector åŠ¨æœº one-stage detector æ ¸å¿ƒchallengeå°±æ˜¯imbalance issue imbalance between positives and negatives imbalance between easy and hard examples è¿™ä¸¤é¡¹éƒ½èƒ½å½’ç»“ä¸ºå¯¹æ¢¯åº¦çš„ä½œç”¨ï¼ša term of the gradient we propose a novel gradient harmonizing mechanism (GHM) balance the gradient flow easy to embed in cls/reg losses like CE/smoothL1 GHM-C for anchor classification GHM-R for bounding box refinement proved substantial improvement on COCO 41.6 mAP surpass FL by 0.8 è®ºç‚¹ imbalance issue easy and hardï¼š OHEM directly abandon examples å¯¼è‡´è®­ç»ƒä¸å……åˆ† positive and negative focal loss æœ‰ä¸¤ä¸ªè¶…å‚ï¼Œè·Ÿdata distributionç»‘å®š not adaptive é€šå¸¸æ­£æ ·æœ¬æ—¢æ˜¯å°‘é‡æ ·æœ¬åˆæ˜¯å›°éš¾æ ·æœ¬ï¼Œè€Œä¸”å¯ä»¥é€šé€šå½’ç»“ä¸ºæ¢¯åº¦åˆ†å¸ƒä¸å‡åŒ€çš„é—®é¢˜ å¤§é‡æ ·æœ¬åªè´¡çŒ®å¾ˆå°çš„æ¢¯åº¦ï¼Œé€šå¸¸å¯¹åº”ç€å¤§é‡è´Ÿæ ·æœ¬ï¼Œæ€»é‡å¤šäº†ä¹Ÿå¯èƒ½ä¼šå¼•å¯¼æ¢¯åº¦ï¼ˆå·¦å›¾ï¼‰ hardæ ·æœ¬è¦æ¯”mediumæ ·æœ¬æ•°é‡å¤§ï¼Œæˆ‘ä»¬é€šå¸¸å°†å…¶çœ‹ä½œç¦»ç¾¤ç‚¹ï¼Œå› ä¸ºæ¨¡å‹ç¨³å®šä»¥åè¿™äº›hard examplesä»æ—§å­˜åœ¨ï¼Œä»–ä»¬ä¼šå½±å“æ¨¡å‹ç¨³å®šæ€§ï¼ˆå·¦å›¾ï¼‰ GHMçš„ç›®æ ‡å°±æ˜¯å¸Œæœ›ä¸åŒæ ·æœ¬çš„gradient contributionä¿æŒharmonyï¼Œç›¸æ¯”è¾ƒäºCEå’ŒFLï¼Œç®€å•æ ·æœ¬å’Œoutlierçš„total contributionéƒ½è¢«downweightï¼Œæ¯”è¾ƒharmonyï¼ˆå³å›¾ï¼‰ we propose gradient harmonizing mechanism (GHM) å¸Œæœ›ä¸åŒæ ·æœ¬çš„gradient contributionä¿æŒharmony é¦–å…ˆç ”ç©¶gradient densityï¼ŒæŒ‰ç…§æ¢¯åº¦èšç±»æ ·æœ¬ï¼Œå¹¶ç›¸åº”reweight é’ˆå¯¹åˆ†ç±»å’Œå›å½’è®¾è®¡GHM-C losså’ŒGHM-R loss verified on COCO GHM-Cæ¯”CEå¥½å¾—å¤šï¼Œsligtly better than FL GHM-Rä¹Ÿæ¯”smoothL1å¥½ attains SOTA dynamic lossï¼šadapt to each batch æ–¹æ³• Problem Description define gradient norm $g = |p - p^*|$ the distribution g from a converged model easyæ ·æœ¬éå¸¸å¤šï¼Œä¸åœ¨ä¸€ä¸ªæ•°é‡çº§ï¼Œä¼šä¸»å¯¼global gradient å³ä½¿æ”¶æ•›æ¨¡å‹ä¹Ÿæ— æ³•handleä¸€äº›æéš¾æ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬æ¢¯åº¦ä¸å…¶ä»–æ ·æœ¬å·®å¼‚è¾ƒå¤§ï¼Œæ•°é‡è¿˜ä¸å°‘ï¼Œä¹Ÿä¼šè¯¯å¯¼æ¨¡å‹ Gradient Density define gradient density $GD(g) = \frac{1}{l_{\epsilon}(g)} \sum_{k=1} \delta_{\epsilon}(g_k,g)$ given a gradient value g ç»Ÿè®¡è½åœ¨ä¸­å¿ƒvalueä¸º$g$ï¼Œå¸¦å®½ä¸º$\epsilon$çš„èŒƒå›´å†…çš„æ¢¯åº¦çš„æ ·æœ¬é‡ å†ç”¨å¸¦å®½å»norm define the gradient density harmony parameter $\beta_i = \frac{N}{GD(g_i)}$ Næ˜¯æ€»æ ·æœ¬é‡ å…¶å®å°±æ˜¯ä¸densityæˆåæ¯” large densityå¯¹åº”æ ·æœ¬ä¼šè¢«downweight GHM-C Loss å°†harmony paramä½œä¸ºloss weightï¼ŒåŠ å…¥ç°æœ‰loss å¯ä»¥çœ‹åˆ°FLä¸»è¦å‹ç®€å•æ ·æœ¬ï¼ˆåŸºäºsample lossï¼‰ï¼ŒGHMä¸¤å¤´å‹ï¼ˆåŸºäºsample densityï¼‰ æœ€ç»ˆharmonize the total gradient contribution of different density group dynamic wrt mini-batchï¼šä½¿å¾—è®­ç»ƒæ›´åŠ efficientå’Œrobust Unit Region Approximation å°†gradient norm [0,1]åˆ†è§£æˆMä¸ªunit region æ¯ä¸ªregionçš„å®½åº¦$\epsilon = \frac{1}{M}$ è½åœ¨æ¯ä¸ªregionå†…çš„æ ·æœ¬æ•°è®¡ä½œ$R_{ind(g)}$ï¼Œ$ind(g)$æ˜¯gæ‰€åœ¨regionçš„start idx the approximate gradient densityï¼š$\hat {GD}(g) = \frac{R_{ind(g)}}{\epsilon} =R_{ind(g)}M $ approximate harmony parameter &amp; lossï¼š we can attain good performance with quite small M ä¸€ä¸ªå¯†åº¦åŒºé—´å†…çš„æ ·æœ¬å¯ä»¥å¹¶è¡Œè®¡ç®—ï¼Œè®¡ç®—å¤æ‚åº¦O(MN) EMA ä¸€ä¸ªmini-batchå¯èƒ½æ˜¯ä¸ç¨³å®šçš„ æ‰€ä»¥é€šè¿‡å†å²ç´¯ç§¯æ¥æ›´æ–°ç»´ç¨³ï¼šSGDMå’ŒBNéƒ½ç”¨äº†EMA ç°åœ¨æ¯ä¸ªregioné‡Œé¢çš„æ ·æœ¬ä½¿ç”¨åŒä¸€ç»„æ¢¯åº¦ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªregionçš„æ ·æœ¬é‡åº”ç”¨äº†EMA t-th iteraion j-th region we have $R_j^t$ apply EMAï¼š$S_j^t = \alpha S_j^(t-1) + (1-\alpha )R_j^t$ $\hat GD(g) = S_{ind(g)} M$ è¿™æ ·gradient densityä¼šæ›´smooth and insensitive to extreme data GHM-R loss smooth L1ï¼š é€šå¸¸åˆ†ç•Œç‚¹è®¾ç½®æˆ$\frac{1}{9}$ SL1åœ¨çº¿æ€§éƒ¨åˆ†çš„å¯¼æ•°æ°¸è¿œæ˜¯å¸¸æ•°ï¼Œæ²¡æ³•å»distinguishing of examples ç”¨$|d|$ä½œä¸ºgradient normåˆ™å­˜åœ¨inf æ‰€ä»¥å…ˆæ”¹é€ smooth L1ï¼šAuthentic Smooth L1 $\mu=0.02$ æ¢¯åº¦èŒƒå›´æ­£å¥½åœ¨[0,1) define gradient norm as $gr = |\frac{d}{\sqrt{d^2+\mu^2}}|$ è§‚å¯Ÿconverged modelâ€˜s gradient norm for ASL1ï¼Œå‘ç°å¤§é‡æ˜¯outliers åŒæ ·ç”¨gradient densityè¿›è¡Œreweighting æ”¶æ•›çŠ¶æ€ä¸‹ï¼Œä¸åŒç±»å‹çš„æ ·æœ¬å¯¹æ¨¡å‹çš„gradient contribution regressionæ˜¯å¯¹æ‰€æœ‰æ­£æ ·æœ¬è¿›è¡Œè®¡ç®—ï¼Œä¸»è¦æ˜¯é’ˆå¯¹ç¦»ç¾¤ç‚¹è¿›è¡Œdownweighting è¿™é‡Œé¢çš„ä¸€ä¸ªè§‚ç‚¹æ˜¯ï¼šåœ¨regression taské‡Œé¢ï¼Œå¹¶éæ‰€æœ‰easyæ ·æœ¬éƒ½æ˜¯ä¸é‡è¦çš„ï¼Œåœ¨åˆ†ç±»taské‡Œé¢ï¼Œeasyæ ·æœ¬å¤§éƒ¨åˆ†éƒ½æ˜¯ç®€å•çš„èƒŒæ™¯ç±»ï¼Œä½†æ˜¯regressionåˆ†æ”¯é‡Œé¢çš„easy sampleæ˜¯å‰æ™¯boxï¼Œè€Œä¸”still deviated from ground truthï¼Œä»æ—§å…·æœ‰å……åˆ†çš„ä¼˜åŒ–ä»·å€¼ æ‰€ä»¥GHM-Rä¸»è¦æ˜¯upweight the important part of easy samples and downweight the outliers å®éªŒ]]></content>
      <tags>
        <tag>single-stage detector, data imbalance, loss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R-FCN]]></title>
    <url>%2F2021%2F08%2F31%2FR-FCN%2F</url>
    <content type="text"><![CDATA[referenceï¼šhttps://zhuanlan.zhihu.com/p/32903856 å¼•ç”¨é‡ï¼š4193 R-FCN: Object Detection via Region-based Fully Convolutional Networks åŠ¨æœº region-basedï¼š å…ˆæ¡†å®šregion of interestçš„æ£€æµ‹ç®—æ³• previous methodsï¼šFast/Faster R-CNNï¼Œapply costly per-region subnetwork hundreds of times fully convolutional æ—¨åœ¨è§£å†³Faster R-CNNç¬¬äºŒé˜¶æ®µè®¡ç®—ä¸å…±äº«ï¼Œæ•ˆç‡ä½çš„é—®é¢˜ we propose position-sensitive score maps translation-invariance in image classification translation-variance in object detection verified on PASCAL VOC è®ºç‚¹ ä¸»æµçš„ä¸¤é˜¶æ®µæ£€æµ‹æ¶æ„ two subnetworks a shared fully convolutionalï¼šè¿™ä¸€éƒ¨åˆ†æå–é€šç”¨ç‰¹å¾ï¼Œä½œç”¨äºå…¨å›¾ an RoI-wise subnetworkï¼šè¿™ä¸€éƒ¨åˆ†ä¸èƒ½å…±äº«è®¡ç®—ï¼Œä½œç”¨äºproposalsï¼Œå› ä¸ºæ˜¯è¦é’ˆå¯¹æ¯ä¸ªä½ç½®çš„ROIè¿›è¡Œåˆ†ç±»å’Œå›å½’ ä¹Ÿå°±æ˜¯è¯´ï¼Œç¬¬ä¸€éƒ¨åˆ†æ˜¯ä½ç½®ä¸æ•æ„Ÿçš„ï¼Œç¬¬äºŒéƒ¨åˆ†æ˜¯ä½ç½®æ•æ„Ÿçš„ ç½‘ç»œè¶Šæ·±è¶Štranslation invariantï¼Œç›®æ ‡æ€ä¹ˆæ‰­æ›²ã€å¹³ç§»æœ€ç»ˆçš„åˆ†ç±»ç»“æœéƒ½ä¸å˜ï¼Œå¤šå±‚poolingåçš„å°feature mapä¸Šä¹Ÿæ„ŸçŸ¥ä¸åˆ°å°ä½ç§»ï¼Œå¹³ç§»å¯å˜æ€§ï¼ˆtranslation varianceï¼‰ï¼Œå¯¹å®šä½ä»»åŠ¡ä¸å‹å¥½ æ‰€ä»¥resnet-back-detectoræˆ‘ä»¬æ˜¯æŠŠROI Poolingæ”¾åœ¨stage4åé¢ï¼Œè·Ÿä¸€ä¸ªRoI-wiseçš„stage5 improves accuracy lower speed due to RoI-wise R-FCN è¦è§£å†³çš„æ ¹æœ¬é—®é¢˜æ˜¯RoI-wiseéƒ¨åˆ†ä¸å…±äº«ï¼Œé€Ÿåº¦æ…¢ï¼š300ä¸ªproposalè¦è®¡ç®—300æ¬¡ å•çº¯åœ°å°†ç½‘ç»œæå‰æ”¾åˆ°shared backé‡Œé¢ä¸è¡Œï¼Œä¼šé€ æˆtranslation invariantï¼Œä½ç½®ç²¾åº¦ä¼šä¸‹é™ å¿…é¡»é€šè¿‡å…¶ä»–æ–¹æ³•åŠ å¼ºç½‘ç»œçš„å¹³ç§»å¯å˜æ€§ï¼Œæ‰€ä»¥æå‡ºäº†position-sensitive score map å°†å…¨å›¾åˆ’åˆ†ä¸ºkxkä¸ªåŒºåŸŸ position-sensitive score mapï¼šç”Ÿæˆkxkx(C+1)ä¸ªç‰¹å¾å›¾ æ¯ä¸ªä½ç½®å¯¹åº”C+1ä¸ªç‰¹å¾å›¾ åšRoIPoolingçš„æ—¶å€™ï¼Œæ¯ä¸ªbinæ¥è‡ªæ¯ä¸ªpositionå¯¹åº”çš„C+1ä¸ªmapï¼ˆè¿™å’‹æƒ³çš„ï¼Œspace dimåˆ°channel dimå†åˆ°space dimï¼Ÿï¼‰ æ–¹æ³• overview two-stage region proposalï¼šRPN region classificationï¼šthe R-FCN R-FCN å…¨å·ç§¯ è¾“å‡ºconvå±‚æœ‰kxkx(C+1)ä¸ªchannel kxkå¯¹åº”grid positions C+1å¯¹åº”Cä¸ªå‰æ™¯+background æœ€åæ˜¯position-sensitive RoI pooling layer aggregates from last conv and RPNï¼Ÿ generate scores for each RoI each bin aggregates responses fromå¯¹åº”çš„positionçš„channel score mapsï¼Œè€Œä¸æ˜¯å…¨éƒ¨é€šé“ forceæ¨¡å‹åœ¨é€šé“ä¸Šå½¢æˆå¯¹ä¸åŒä½ç½®çš„æ•æ„Ÿèƒ½åŠ› R-FCN architecture backï¼šResNet-101ï¼Œpre-trained on ImageNetï¼Œblock5 è¾“å‡ºæ˜¯2048-d ç„¶åæ¥äº†random initialized 1x1 convï¼Œé™ç»´ cls brach æ¥$k^2(C+1)$çš„convç”Ÿæˆscore maps ç„¶åæ˜¯Position-sensitive RoI pooling å°†æ¯ä¸ªROIå‡åŒ€åˆ‡åˆ†æˆkxkä¸ªbins æ¯ä¸ªbinåœ¨å¯¹åº”çš„Position-sensitive score mapsä¸­æ‰¾åˆ°å”¯ä¸€çš„é€šé“ï¼Œè¿›è¡Œaverage pooling æœ€ç»ˆå¾—åˆ°kxkçš„pooling mapï¼ŒC+1ä¸ªé€šé“ å°†pooling map performs average poolingï¼Œå¾—åˆ°C+1çš„vectorï¼Œç„¶åsoftmax box branch æ¥$4k^2$çš„convç”Ÿæˆscore maps Position-sensitive RoI pooling å¾—åˆ°kxkçš„pooling mapï¼Œ4ä¸ªé€šé“ average poolingï¼Œå¾—åˆ°4d vectorï¼Œä½œä¸ºå›å½’å€¼$(t_x,t_y,t_w,t_h)$ there is no learnable layer after the ROI layerï¼Œenable nearly cost-free region-wise computation Training R-FCN positives / negativesï¼šå’Œgt boxçš„IoU&gt;0.5çš„proposasl adopt OHEM sort all ROI loss and select the highest 128 å…¶ä»–settingsåŸºæœ¬å’ŒFaster-RCNNä¸€è‡´ Atrous and stride ç‰¹åˆ«åœ°ï¼Œå¯¹resnetçš„block5è¿›è¡Œäº†æ”¹å˜ stride2æ”¹æˆstride1 æ‰€æœ‰çš„convæ”¹æˆç©ºæ´å·ç§¯ RPNæ˜¯æ¥åœ¨block4çš„è¾“å‡ºä¸Šï¼Œæ‰€ä»¥ä¸å—ç©ºæ´å·ç§¯çš„å½±å“ï¼Œåªå½±å“R-FCN head]]></content>
      <tags>
        <tag>ç›®æ ‡æ£€æµ‹ï¼Œå…¨å·ç§¯ï¼Œregion-basedï¼Œtwo-stage detector</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Meta Pseudo Labels]]></title>
    <url>%2F2021%2F08%2F23%2FMeta-Pseudo-Labels%2F</url>
    <content type="text"><![CDATA[papers [MPL 2021] Meta Pseudo Labels [UDA 2009] Unsupervised Data Augmentation for Consistency Training [Entropy Minimization 2004] Semi-supervised Learning by Entropy Minimization Meta Pseudo Labels åŠ¨æœº semi-supervised learning Pseudo Labelsï¼šfixed teacher Meta Pseudo Labelsï¼šconstantly adapted teacher by the feedback of the student SOTA on ImageNetï¼štop-1 acc 90.2% è®ºç‚¹ Pseudo Labels methods teacher generates pseudo labels on unlabeled images pseudo labeled images are then combined with labeled images to train the student confirmation bias problemï¼šstudentçš„ç²¾åº¦å–å†³äºä¼ªæ ‡ç­¾çš„è´¨é‡ we propose Meta Pseudo Labels teacher observes how its pseudo labels affect the student then correct the bias the feedback signal is the performance of the student on the labeled dataset æ€»çš„æ¥è¯´ï¼Œteacherå’Œstudentæ˜¯train in parallelçš„ student learns from pseudo labels from the teacher teacher learns from reward signal from how well student perform on labeled set dataset ImageNet as labeled set JFT-300M as unlabeled set model teacherï¼šEfficientNet-L2 studentï¼šEfficientNet-L2 main difference Pseudo Labelsæ–¹æ³•ä¸­ï¼Œteacheråœ¨å•å‘çš„å½±å“student Meta Pseudo Labelsæ–¹æ³•ä¸­ï¼Œteacherå’Œstudentæ˜¯äº¤äº’ä½œç”¨çš„ æ–¹æ³• notations models teacher model T &amp; $\theta_T$ student model S &amp; $\theta_S$ data labeled set $(x_l, y_l)$ unlabeled set $(x_u)$ predictions soft predictions by teacher $T(x_u, \theta_T)$ student $S(x_u, \theta_S)$ &amp; $S(x_l, \theta_S)$ loss $CE(q,p)$ï¼Œå…¶ä¸­$q$æ˜¯one-hot labelï¼Œe.g. $CE(y_l, S(x_l, \theta_S))$ Pseudo Labels given a fixed teacher $\theta_T$ train the student model to minimize the cross-entropy loss on unlabeled data \theta_S^{PL} = argmin_{\theta_S}CE(T(x_u,\theta_T), S(x_u, \theta_S)) $\theta_S^{PL}$ also achieve a low loss on labeled data $\theta_S^{PL}$ explicitly depends on $\theta_T$ï¼š$\theta_S^{PL}(\theta_T)$ student loss on labeled data is also a function of $\theta_T$ï¼š$L_l(\theta_S^{PL}(\theta_T))$ Meta Pseudo Labels intuitionï¼šminimize $L_l$ with respect to $\theta_T$ ä½†æ˜¯å®é™…ä¸Šdependency of $\theta_S^{PL}(\theta_T)$ on $\theta_T$ éå¸¸å¤æ‚ å› ä¸ºæˆ‘ä»¬ç”¨äº†teacher predictionçš„hard labelså»è®­ç»ƒstudent an alternating optimization procedure teacherâ€™s auxiliary losses augment the teacherâ€™s training with a supervised learning objective and a semi-supervise learning objective supervised objective train on labeled data CE semi-supervised objective train on unlabeled data UDA(Unsupervised Data Augmentation)ï¼šå°†æ ·æœ¬è¿›è¡Œç®€å•å¢å¼ºï¼Œé€šè¿‡è¡¡é‡ä¸€è‡´æ€§æŸå¤±ï¼Œæ¨¡å‹çš„æ³›åŒ–æ•ˆæœå¾—åˆ°æå‡ consistency training lossï¼šKLæ•£åº¦ finetuning student åœ¨meta pseudo labelsè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œstudent only learns from the unlabeled data æ‰€ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ç»“æŸåï¼Œå¯ä»¥finetune it on labeled data to improve accuracy overall algorithm * è¿™é‡Œé¢æœ‰ä¸€å¤„ä¸‹æ ‡å†™é”™äº†ï¼Œå°±æ˜¯teacherçš„UDA gradientï¼Œæ˜¯åœ¨unlabeled dataä¸Šé¢ç®—çš„ï¼Œé‚£ä¸¤ä¸ª$x_l$å¾—æ”¹æˆ$x_u$ * UDA lossè®ºæ–‡é‡Œä½¿ç”¨ä¸¤ä¸ªpredicted logitsçš„æ•£åº¦ï¼Œè¿™é‡Œæ˜¯CE Unsupervised Data Augmentation for Consistency Training åŠ¨æœº data augmentation in previous works èƒ½åœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®çš„é—®é¢˜ å¤šç”¨åœ¨supervised modelä¸Š achieved limited gains we propose UDA apply data augmentation in semi-supervised learning setting use harder and more realistic noise to generate the augmented samples encourage the prediction to be consistent between unlabeled &amp; augmented unlabeled sample åœ¨è¶Šå°çš„æ•°æ®é›†ä¸Šæå‡è¶Šå¤§ verified on six language tasks three vision tasks ImageNet-10%ï¼šï¼štop1/top5 68.7/88.5% ImageNet-extra unlabeledï¼štop1/top5 79.0/94.5% è®ºç‚¹ semi-supervised learning three categories graph-based label propagation via graph convolution and graph embeddings modeling prediction target as latent variables consistency / smoothness enforcing æœ€åè¿™ä¸€ç±»æ–¹æ³•shown to work wellï¼Œ enforce the model predictions on the two examples to be similar ä¸»è¦åŒºåˆ«åœ¨äºperturbation functionçš„è®¾è®¡ we propose UDA use state-of-the-art data augmentation methods we show that better augmentation methods(AutoAugment) lead to greater improvements minimizes the KL divergence can be applied even the class distributions of labeled and unlabeled data mismatch we propose TSA a training technique prevent overfitting when much more unlabeled data is avaiable than labeled data æ–¹æ³• formulation given an input $x\in U$ and a small noise $\epsilon$ compute the output distribution $p_{\theta}(y|x)$ and $p_{\theta}(y|x,\epsilon)$ minimize the divergence between two predicted distributions $D(p_{\theta}(y|x)||p_{\theta}(y|x,\epsilon))$ add a CE loss on labeled data UDAçš„ä¼˜åŒ–ç›®æ ‡ enforce the model to be insensitive to perturbation thus smoother to the changes in the input space $\lambda=1$ for most experiments use different batchsize for labeled &amp; unlabeled Augmentation Strategies for Different Tasks AutoAugment for Image Classification é€šè¿‡RLæœå‡ºæ¥çš„ä¸€ç»„optimal combination of aug operations Back translation for Text Classification TF-IDF based word replacing for Text Classification Trade-off Between Diversity and Validity for Data Augmentation å¯¹åŸå§‹sampleåšå˜æ¢çš„æ—¶å€™ï¼Œæœ‰ä¸€å®šæ¦‚ç‡å¯¼è‡´gt labelå˜åŒ– AutoAugmentå·²ç»æ˜¯optmial trade-offäº†ï¼Œæ‰€ä»¥ä¸ç”¨ç®¡ text taskséœ€è¦è°ƒèŠ‚temperature Additional Training Techniques TSA(Training Signal Annealing) situationï¼šunlabeled dataè¿œæ¯”labeled dataå¤šçš„æƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦large enough modelå»å……åˆ†åˆ©ç”¨å¤§æ•°æ®ï¼Œä½†åˆå®¹æ˜“å¯¹å°trainsetè¿‡æ‹Ÿåˆ for each training step set a threshold $\frac{1}{K}\leq \eta_t\leq 1$ï¼ŒK is the number of categories å¦‚æœæ ·æœ¬åœ¨gt clsä¸Šçš„é¢„æµ‹æ¦‚ç‡å¤§äºè¿™ä¸ªthresholdï¼Œå°±æŠŠè¿™ä¸ªæ ·æœ¬çš„losså»æ‰ $\eta_t$ serves as a ceiling to prevent the model from over-training on examples that the model is already confident about gradually release the training signals of the labeled examplesï¼Œç¼“è§£overfitting schedules of $\eta_t$ log-scheduleï¼š$\lambda_t = 1-exp(-\frac{t}{T}*5)$ linear-scheduleï¼š$\lambda_t = \frac{t}{T}$ exp-scheduleï¼š$\lambda_t = exp((\frac{t}{T}-1)*5)$ å¦‚æœæ¨¡å‹éå¸¸å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œç”¨exp-scheduleï¼Œåè¿‡æ¥ï¼ˆabundant labeled data/effective regularizationsï¼‰ï¼Œç”¨log-schedule Sharpening Predictions situationï¼šthe predicted distributions on unlabeled examples tend to be over-flat across categoriesï¼Œtaskæ¯”è¾ƒå›°éš¾ï¼Œè®­ç»ƒæ•°æ®æ¯”è¾ƒå°‘æ—¶ï¼Œåœ¨unlabeled dataä¸Šæ¯ç±»çš„é¢„æµ‹æ¦‚ç‡éƒ½å·®ä¸å¤šä½ï¼Œæ²¡æœ‰å€¾å‘æ€§ è¿™æ—¶å€™KL divergenceçš„ç›‘ç£ä¿¡æ¯å°±å¾ˆå¼± thus we need to sharpen the predicted distribution on unlabeled examples Confidence-based maskingï¼šå°†current model not confident enough to predictçš„æ ·æœ¬è¿‡æ»¤æ‰ï¼Œåªä¿ç•™æœ€å¤§é¢„æµ‹æ¦‚ç‡å¤§äº0.6çš„æ ·æœ¬è®¡ç®—consistency loss Entropy minimizationï¼šadd an entropy term to the overall objective softmax temperatureï¼šåœ¨è®¡ç®—softmaxæ—¶å…ˆå¯¹logitsè¿›è¡Œrescaleï¼Œ$Softmax(logits/\tau)$ï¼Œa lower temperature corresponds to a sharper distribution in practiceå‘ç°Confidence-based maskingå’Œsoftmax temperatureæ›´é€‚ç”¨äºå°labeled setï¼ŒEntropy minimizationé€‚ç”¨äºç›¸å¯¹å¤§ä¸€ç‚¹çš„labeled set Domain-relevance Data Filtering å…¶å®ä¹Ÿæ˜¯Confidence-based maskingï¼Œå…ˆç”¨labeled dataè®­ç»ƒä¸€ä¸ªbase modelï¼Œç„¶åinference the out-of-domain datasetï¼ŒæŒ‘å‡ºé¢„æµ‹æ¦‚ç‡è¾ƒå¤§çš„æ ·æœ¬ Semi-supervised Learning by Entropy Minimization]]></content>
      <tags>
        <tag>semi-supervised learning, teacher-student, classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Generalized Focal Loss]]></title>
    <url>%2F2021%2F08%2F20%2FGeneralized-Focal-Loss%2F</url>
    <content type="text"><![CDATA[Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection åŠ¨æœº one-stage detectors dense prediction three fundamental elements class branch box localization branch an individual quality branch to estimate the quality of localization current problems the inconsistent usage of the quality estimation in train &amp; test the inflexible Dirac delta distributionï¼šå°†box regressionçš„valueå»ºæ¨¡æˆçœŸå€¼é™„è¿‘çš„è„‰å†²åˆ†å¸ƒï¼Œç”¨æ¥æè¿°è¾¹ç•Œä¸æ¸…æ™°/é®æŒ¡çš„caseå¯èƒ½ä¸å‡†ç¡® we design new representations for these three elements merge quality estimation into class predictionï¼šå°†objectness/centernessæ•´åˆè¿›cls predictionï¼Œç›´æ¥ç”¨ä½œNMS score continout labels propose GFL(Generalized Focal Loss) that generalizes Focal Loss from discrete form into continous version test on COCO ResNet-101-?-GFL: 45.0% AP defeat ATSS è®ºç‚¹ inconsistent usage of localization quality estimation and classification score è®­ç»ƒçš„æ—¶å€™qualityå’Œcls branchæ˜¯independent branch box branchçš„supervisionåªä½œç”¨åœ¨positiveæ ·æœ¬ä¸Šï¼šwhich is unreliable on predicting negatives æµ‹è¯•é˜¶æ®µå°†qualityå’Œcls scoreä¹˜èµ·æ¥æœ‰å¯èƒ½æ‹‰é«˜è´Ÿæ ·æœ¬çš„åˆ†æ•°ï¼Œä»¥è‡³äºåœ¨NMSé˜¶æ®µæŠŠä½åˆ†æ­£æ ·æœ¬æŒ¤æ‰ inflexible representation of bounding boxes most methodå»ºæ¨¡æˆè„‰å†²åˆ†å¸ƒï¼šåªåœ¨IoUå¤§äºä¸€å®šé˜ˆå€¼çš„æ ¼å­ä¸Šæœ‰å“åº”ï¼Œåˆ«çš„æ ¼å­éƒ½æ˜¯0 some recent workå»ºæ¨¡æˆé«˜æ–¯åˆ†å¸ƒ in fact the real distribution can be more arbitrary and flexibleï¼Œè¿ç»­ä¸”ä¸ä¸¥æ ¼é•œåƒ thus we propose merge the quality representation into the class branchï¼š class vectorçš„æ¯ä¸ªå…ƒç´ ä»£è¡¨äº†æ ¼å­çš„localization quality(å¦‚IoU score) åœ¨inferenceé˜¶æ®µä¹Ÿæ˜¯ç›´æ¥ç”¨ä½œcls score propose arbitrary/general distribution æœ‰æ˜ç¡®è¾¹ç•Œçš„ç›®æ ‡çš„è¾¹çš„åˆ†å¸ƒæ˜¯æ¯”è¾ƒsharpçš„ æ²¡æœ‰æ˜æ˜¾è¾¹ç•Œçš„è¾¹åˆ†å¸ƒå°±æ˜¯flattenä¸€ç‚¹ Generalized Focal Loss (GFL) joint class representationæ˜¯continuous IoU label (0âˆ¼1) imbalanceé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œä½†æ˜¯standart Focal Lossä»…æ”¯æŒ[0,1] sample ä¿®æ”¹æˆcontinuouså½¢å¼ï¼ŒåŒæ—¶specialized into Quality Focal Loss (QFL) and Distribution Focal Loss (DFL) QFL for cls branchï¼šfocuses on a sparse set of hard examples DFL for box branchï¼š focus on learning the probabilities of values around the continuous target locations æ–¹æ³• Focal Loss (FL) standard CE partï¼š$-log(p_t)$ scaling factorï¼šdown-weights the easy examplesï¼Œfocus on hard examples Quality Focal Loss (QFL) soft one-hot labelï¼šæ­£æ ·æœ¬åœ¨å¯¹åº”ç±»åˆ«ä¸Šæœ‰ä¸ª(0,1]çš„float scoreï¼Œè´Ÿæ ·æœ¬å…¨0 float scoreå®šä¹‰ä¸ºé¢„æµ‹æ¡†å’Œgt boxçš„IoU score we adopt multiple binary classification with sigmoid modify FL CE part æ”¹æˆcomplete formï¼š$-ylog(\hat y)-(1-y)log(1-\hat y)$ scaling partç”¨vector distanceæ›¿æ¢å‡æ³•ï¼š$|y-\hat y |^{\beta}$ $\beta$ controls the down-weighting rate smoothly &amp; $\beta=2$ works best Distribution Focal Loss (DFL) use relative offsets from the location to the four sides of a bounding box as the regression targets å›å½’é—®é¢˜formulation è¿ç»­ï¼š$\hat y = \int_{y_0}^{y_n}P(x)xdx$ ç¦»æ•£åŒ–ï¼š$\hat y = \sum_{i=0}^n P(y_i)y_i$ P(x) can be easily implemented through a softmax layer containing n+1 unitsï¼š DFL force predictions to focus values near label $y$ï¼šexplicitly enlarge the probabilities of $y_i$å’Œ$y_{i+1}$ï¼Œgiven $y_i \leq y \leq y_{i+1}$ $log(S_i)$ force the probabilities gap balance the ä¸Šä¸‹é™ï¼Œä½¿å¾—$\hat y$çš„global mininum solutionæ— é™é€¼è¿‘çœŸå€¼$y$ï¼Œå¦‚æœçœŸå€¼æ¥è¿‘çš„æ˜¯$\hat y_{i+1}$ï¼Œå¯ä»¥çœ‹åˆ°$log(S_i)$é‚£é¡¹è¢«downscaleäº† Generalized Focal Loss (GFL) ä»¥å‰çš„cls preditionsåœ¨æµ‹è¯•é˜¶æ®µè¦ç»“åˆquality predictionsä½œä¸ºNMS scoreï¼Œç°åœ¨ç›´æ¥å°±æ˜¯ ä»¥å‰regression targetsæ¯ä¸ªå›å½’ä¸€ä¸ªå€¼ï¼Œç°åœ¨æ˜¯n+1ä¸ªå€¼ overall ç¬¬ä¸€é¡¹cls lossï¼Œå°±æ˜¯QFLï¼Œdense on æ‰€æœ‰æ ¼å­ï¼Œç”¨æ­£æ ·æœ¬æ•°å»norm ç¬¬äºŒé¡¹box lossï¼ŒGIoU loss + DFLï¼Œ$\lambda_0$é»˜è®¤2ï¼Œ$\lambda_1$é»˜è®¤1/4ï¼Œåªè®¡ç®—æœ‰IoUçš„æ ¼å­ we also utilize the quality scores to weight $L_B$ and $L_D$ during training å½©è›‹ IoU branch always superior than centerness-branch centernesså¤©ç”Ÿå€¼è¾ƒå°ï¼Œå½±å“å¬å›ï¼ŒIoUçš„å€¼è¾ƒå¤§]]></content>
      <tags>
        <tag>one-stage detector, object-detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[soft teacher]]></title>
    <url>%2F2021%2F08%2F12%2Fsoft-teacher%2F</url>
    <content type="text"><![CDATA[keywordsï¼šsemi-supervised, curriculum, pseudo labels, End-to-End Semi-Supervised Object Detection with Soft Teacher åŠ¨æœº end-to-end trainingï¼šç›¸æ¯”è¾ƒäºå…¶ä»–æ–¹æ³•çš„multi-stage semi-supervisedï¼šç”¨å¤–éƒ¨unlabeledæ•°æ®ï¼Œä»¥åŠpseudo-label based approach propose two techniques soft teacher mechanismï¼špseudoæ ·æœ¬çš„classification lossç”¨teacher modelçš„prediction scoreæ¥åŠ æƒ box jittering mechanismï¼šæŒ‘é€‰reliable pseudo boxes verified use SWIN-L as baseline metric on COCOï¼š60.4 mAP if pretrained with Object365ï¼š61.3 mAP è®ºç‚¹ we present this end-to-end pseudo-label based semi-supervised object detection framework simultaneously performs pseudo-labelingï¼šteacher training detector use the current pseudo-labels &amp; a few training sampleï¼šstudent teacher is an exponential moving average (EMA) of the student model mutually enforce each other soft teacher approach teacher modelçš„ä½œç”¨æ˜¯ç»™student modelç”Ÿæˆçš„box candidatesæ‰“åˆ†ï¼Œ é«˜äºä¸€å®šé˜ˆå€¼çš„ä¸ºå‰æ™¯ï¼Œä½†æ˜¯å¯èƒ½æœ‰éƒ¨åˆ†å‰æ™¯è¢«å½’ç±»ä¸ºèƒŒæ™¯ï¼Œæ‰€ä»¥ç”¨è¿™ä¸ªscoreä½œä¸ºreliability measureï¼Œç»™æ ‡è®°ä¸ºèƒŒæ™¯æ¡†çš„cls lossè¿›è¡ŒåŠ æƒ reliability measure æ–¹æ³• overview ä¸¤ä¸ªmodelï¼šstudentå’Œteacher teacher modelç”¨æ¥ç”Ÿæˆpseudo labelsï¼štwo set of pseudo boxesï¼Œä¸€ä¸ªç”¨äºclass branchï¼Œä¸€ä¸ªç”¨äºregression branch student modelç”¨supervised&amp;unsupervised sampleçš„lossæ¥æ›´æ–° teacher modelç”¨student modelçš„EMAæ¥æ›´æ–° two crucial designs soft teacher box jittering æ•´ä½“çš„å·¥ä½œæµç¨‹å°±æ˜¯ï¼Œæ¯ä¸ªtraining iterationï¼Œå…ˆæŒ‰ç…§ä¸€å®šæ¯”ä¾‹æŠ½å–labeled&amp;unlabeled sampleæ„æˆdata batchï¼Œç„¶åç”¨teacher modelç”Ÿæˆunlabeled dataçš„pseudo labelï¼ˆthousands of box candidates+NMS+score filterï¼‰ï¼Œç„¶åå°†å…¶ä½œä¸ºunlabeled sampleçš„ground truthï¼Œè®­ç»ƒstudent modelï¼Œoverall lossæ˜¯supervised losså’Œunsupervised lossçš„åŠ æƒå’Œ åœ¨è®­ç»ƒå¼€å§‹é˜¶æ®µï¼Œä¸¤ä¸ªæ¨¡å‹éƒ½æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œteacheræ¨¡å‹éšç€studentæ¨¡å‹çš„æ›´æ–°è€Œæ›´æ–° FixMatchï¼š è¾“å…¥ç»™teacheræ¨¡å‹çš„æ ·æœ¬ä½¿ç”¨weak aug è¾“å…¥ç»™studentæ¨¡å‹çš„æ ·æœ¬ä½¿ç”¨strong aug soft teacher detectorçš„pseudo-labelè´¨é‡å¾ˆé‡è¦ æ‰€ä»¥ç”¨score thresh=0.9å»å®šä¹‰box candidatesçš„å‰/èƒŒæ™¯ ä½†æ˜¯è¿™æ—¶å€™å¦‚æœç”¨ä¼ ç»Ÿçš„IoUæ¥å®šä¹‰student modelçš„box candidatesçš„pos/negï¼Œä¼šæœ‰ä¸€éƒ¨åˆ†å‰æ™¯æ¡†è¢«å½“ä½œèƒŒæ™¯ to alleviate assess the reliability of each student-generated box candidate to be a real background given a student-generated box candidateï¼Œç”¨teacher modelçš„detection headå»é¢„æµ‹è¿™ä¸ªæ¡†çš„background score overall unsupervised cls loss L_u^{cls} = \frac{1}{N_b^{fg}} \sum_i^{N_b^{fg}} l_{cls} (b_i^{fg}, G_{cls}) + \sum_j^{N_b^{bg}}w_j l_{cls} (b_j^{bg}, G_{cls})\\ w_j = \frac{r_j}{\sum_{k=1}^{N_b^{bg}}r_k} $G_{cls}$æ˜¯the set of boxes teacher generated for classificationï¼Œå°±æ˜¯teacher modelé¢„æµ‹çš„top1000ç»è¿‡nmså’Œscore filterä¹‹åçš„boxes $b_i^{fg}$æ˜¯student candidatesä¸­è¢«assignä¸ºå‰æ™¯çš„æ¡†ï¼Œ$b_i^{bg}$æ˜¯student candidatesä¸­è¢«assignä¸ºèƒŒæ™¯çš„æ¡†ï¼Œassignçš„åŸåˆ™å°±æ˜¯score&gt;0.9 $w_j$æ˜¯å¯¹assignä¸ºèƒŒæ™¯çš„æ¡†çš„åŠ æƒ $r_k$æ˜¯reliability scoreï¼Œæ˜¯student modelé€šè¿‡hard score thresh assignä¸ºèƒŒæ™¯çš„æ¡†ï¼Œç”¨teacher modelçš„detection headå»é¢„æµ‹çš„bg score box jittering fg score threshå’Œbox iouå¹¶ä¸å‘ˆç°strong positive correlationï¼Œè¯´æ˜åŸºäºè¿™ä¸ªåŸåˆ™äº§ç”Ÿçš„æ¡†pseudo-labelså¹¶ä¸ä¸€å®šé€‚åˆbox regression localization reliabilityï¼š è¡¡é‡ä¸€ä¸ªpseudo boxçš„consistency given a pseudo boxï¼Œsampleä¸€ç³»åˆ—jitter box around itï¼Œå†ç”¨teacher modelå»é¢„æµ‹è¿™äº›jitter boxå¾—åˆ°refined boxes refined boxå’Œpseudo boxçš„varianceè¶Šå°ï¼Œè¯´æ˜è¿™ä¸ªæ¡†çš„localization reliabilityè¶Šé«˜ \hat b_i = refine(jitter(b_i))\\ \overline \sigma_i = \frac{1}{4} \sum_1^4 \hat \sigma_k\\ \hat \sigma_k = \frac{\sigma_k}{0.5 (h(b_i)+w(b_i))} $\hat b_i$æ˜¯refined boxes $\sigma_k$æ˜¯refine boxesçš„å››ä¸ªåæ ‡åŸºäºåŸå§‹boxçš„æ ‡å‡†å·® $\hat \sigma_k$æ˜¯ä¸Šé¢é‚£ä¸ªæ ‡å‡†å·®åŸºäºåŸå§‹boxçš„å°ºåº¦è¿›è¡Œå½’ä¸€åŒ– $\overline\sigma$æ˜¯refine boxeså››ä¸ªåæ ‡çš„normed stdçš„å¹³å‡å€¼ åªè®¡ç®—teacher box candidatesé‡Œé¢ï¼Œfg score&gt;0.5çš„é‚£éƒ¨åˆ† overall unsupervised reg loss L_u^{reg} = \frac{1}{N_b^{fg}} \sum_1^{N_b^{fg}} l_{reg} (b_i^{fg}, G_{reg}) $b_i^{fg}$æ˜¯student candidatesä¸­è¢«assignä¸ºå‰æ™¯çš„æ¡†ï¼Œå³cls score&gt;0.9é‚£äº›é¢„æµ‹æ¡† $G_{cls}$æ˜¯the set of boxes teacher generated for regressionï¼Œå°±æ˜¯jittered reliabilityå¤§äºä¸€å®šé˜ˆå€¼çš„candidates overall unsupervised lossï¼šcls losså’Œreg lossä¹‹å’Œï¼Œç„¶åç”¨æ ·æœ¬æ•°è¿›è¡Œnorm å®éªŒ]]></content>
      <tags>
        <tag>semi-supervised, object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GNN&GCN]]></title>
    <url>%2F2021%2F07%2F13%2FGNN-GCN%2F</url>
    <content type="text"><![CDATA[ç»¼è¿° referenceï¼šhttps://www.cnblogs.com/siviltaram/p/graph_neural_network_2.html key concepts å›¾ç¥ç»ç½‘ç»œï¼ˆGraph Neural Networkï¼ŒGNNï¼‰ å›¾å·ç§¯ç¥ç»ç½‘ç»œï¼ˆGraph Convolutional Neural Networkï¼‰ é¢‘åŸŸï¼ˆSpectral-domainï¼‰ ç©ºåŸŸï¼ˆSpatial-domainï¼‰ å›¾ç¥ç»ç½‘ç»œ image &amp; graph èŠ‚ç‚¹ï¼ˆNodeï¼‰ æ¯ä¸ªèŠ‚ç‚¹æœ‰å…¶ç‰¹å¾ï¼Œç”¨$x_v$è¡¨ç¤º è¾¹ï¼ˆEdgeï¼‰ è¿æ¥ä¸¤ä¸ªèŠ‚ç‚¹çš„è¾¹ä¹Ÿæœ‰å…¶ç‰¹å¾ï¼Œç”¨$x_{v,u}$è¡¨ç¤º éšè—çŠ¶æ€ å›¾çš„å­¦ä¹ ç›®æ ‡æ˜¯è·å¾—æ¯ä¸ªèŠ‚ç‚¹çš„éšè—çŠ¶æ€ å±€éƒ¨è¾“å‡ºå‡½æ•° é€‰å–ä¸€ä¸ªèŠ‚ç‚¹ å›¾å·ç§¯ ä¸€å¼ å›¾ç‰‡å°±å¯ä»¥çœ‹ä½œä¸€ä¸ªéå¸¸ç¨ å¯†çš„å›¾ï¼Œé˜´å½±éƒ¨åˆ†ä»£è¡¨å·ç§¯æ ¸ï¼Œå³ä¾§åˆ™æ˜¯ä¸€ä¸ªæ™®é€šçš„å›¾ï¼Œå’Œå›¾å·ç§¯æ ¸ åœ¨imageä¸ºä»£è¡¨çš„æ¬§å¼ç©ºé—´ä¸­ï¼Œç»“ç‚¹çš„é‚»å±…æ•°é‡éƒ½æ˜¯å›ºå®šçš„ï¼Œä½†åœ¨graphè¿™ç§éæ¬§ç©ºé—´ä¸­ï¼Œç»“ç‚¹æœ‰å¤šå°‘é‚»å±…å¹¶ä¸å›ºå®š ä¼ ç»Ÿçš„å·ç§¯æ ¸ä¸èƒ½ç›´æ¥ç”¨äºæŠ½å–å›¾ä¸Šç»“ç‚¹çš„ç‰¹å¾ ä¸¤ä¸ªä¸»æµæ€è·¯ æŠŠéæ¬§ç©ºé—´çš„å›¾è½¬æ¢æˆæ¬§å¼ç©ºé—´ï¼Œç„¶åä½¿ç”¨ä¼ ç»Ÿå·ç§¯ æ‰¾å‡ºä¸€ç§å¯å¤„ç†å˜é•¿é‚»å±…ç»“ç‚¹çš„å·ç§¯æ ¸åœ¨å›¾ä¸ŠæŠ½å–ç‰¹å¾]]></content>
      <tags>
        <tag>å›¾ç¥ç»ç½‘ç»œï¼Œå›¾å·ç§¯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[few-shot]]></title>
    <url>%2F2021%2F06%2F22%2Ffew-shot%2F</url>
    <content type="text"><![CDATA[ç»¼è¿° few-shot few-shot learningï¼šé€šè¿‡å°‘é‡æ ·æœ¬å­¦ä¹ è¯†åˆ«æ¨¡å‹ é—®é¢˜ï¼šè¿‡æ‹Ÿåˆ&amp;æ³›åŒ–æ€§ï¼Œæ•°æ®å¢å¼ºå’Œæ­£åˆ™èƒ½åœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£ä½†ä¸è§£å†³ï¼Œè¿˜æ˜¯æ¨èä»å¤§æ•°æ®ä¸Šè¿ç§»å­¦ä¹  å…±è¯†ï¼š æ ·æœ¬é‡æœ‰é™çš„æƒ…å†µä¸‹ï¼Œä¸ä¾é å¤–éƒ¨æ•°æ®å¾ˆéš¾å¾—åˆ°ä¸é”™çš„ç»“æœï¼Œå½“ä¸‹æ‰€æœ‰çš„è§£å†³æ–¹æ¡ˆéƒ½æ˜¯å€ŸåŠ©å¤–éƒ¨æ•°æ®ä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼Œæ„é€ å­¦ä¹ ä»»åŠ¡ è¿ç§»æ•°æ®ä¹Ÿä¸æ˜¯éšä¾¿æ‰¾çš„ï¼Œæ•°æ®é›†çš„domain differenceè¶Šå¤§ï¼Œè¿ç§»æ•ˆæœè¶Šå·®ï¼ˆe.g. ç”¨miniImagenetåšç±»é—´è¿ç§»ï¼Œæ•ˆæœä¸é”™ï¼Œä½†æ˜¯ç”¨miniImagenetåšbase classç”¨CUBåšnovel classï¼Œå­¦ä¹ æ•ˆæœä¼šæ˜æ˜¾ä¸‹é™ï¼‰ æ•°æ®é›†ï¼š miniImagenetï¼šè‡ªç„¶å›¾åƒï¼Œ600å¼ ï¼Œ100ç±» Omniglotï¼šæ‰‹å†™å­—ç¬¦ï¼Œ1623å¼ ï¼Œ50ç±» CUBï¼šé¸Ÿé›†ï¼Œ11788å¼ ï¼Œ200ç±»ï¼Œå¯ç”¨äºç»†ç²’åº¦ï¼Œå¯ä»¥ç”¨äºzero-shot methods pretraining + finetuning pretrainingé˜¶æ®µç”¨base classè®­ç»ƒä¸€ä¸ªfeature extractor finetuningé˜¶æ®µfix feature extractoré‡æ–°è®­ç»ƒä¸€ä¸ªclassifier åŸºäºåº¦é‡å­¦ä¹  å¼•å…¥distance metricå…¶å®éƒ½ç®—åº¦é‡å­¦ä¹ ï¼Œæ‰€ä»¥ä¸Šé¢ï¼ˆpretraining+finetuningï¼‰å’Œä¸‹é¢ï¼ˆmeta learningï¼‰çš„æ–¹æ³•éƒ½æœ‰å±äºåº¦é‡å­¦ä¹ çš„æ–¹æ³• åŸºäºå…ƒå­¦ä¹  base class&amp;novel classï¼šbase classæ˜¯å·²æœ‰çš„å¤§æ•°æ®é›†ï¼Œå¤šç±»åˆ«ï¼Œå¤§æ ·æœ¬é‡ï¼Œnovel classæ˜¯æˆ‘ä»¬è¦è§£å†³çš„å°æ•°æ®é›†ï¼Œç±»åˆ«å°‘ï¼Œæ¯ç±»æ ·æœ¬ä¹Ÿå°‘ N-way-K-shotï¼šåŸºäºnovel classå…ˆåœ¨base classä¸Šæ„å»ºå¤šä¸ªå­ä»»åŠ¡ï¼ŒN-wayå°±æ˜¯æ„å»ºéšæœºNä¸ªç±»åˆ«çš„åˆ†ç±»ä»»åŠ¡ï¼ŒK-shotå°±æ˜¯æ¯ä¸ªç±»åˆ«å¯¹åº”æ ·æœ¬é‡ä¸ºK supportset S &amp; queryset Qï¼šN-way-K-shotçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œæ¥è‡ªbase classä¸­ç›¸åŒçš„ç±»åˆ«ï¼Œå‡ç”¨äºtraining procedure ä¸ä¼ ç»Ÿåˆ†ç±»ä»»åŠ¡å¯¹æ¯”ï¼š leaderboardï¼šhttps://few-shot.yyliu.net/miniimagenet.html papers [2015 siamese]ï¼šSiamese Neural Networks for One-shot Image Recognitionï¼Œæ ¸å¿ƒæ€æƒ³å°±æ˜¯åŸºäºå­ªç”Ÿç½‘ç»œæ„å»ºsimilarityä»»åŠ¡ï¼Œç”¨ä¸€ä¸ªå¤§æ•°æ®é›†æ„é€ çš„same/diff pairså»è®­ç»ƒï¼Œç„¶åç›´æ¥ç”¨åœ¨novel setä¸Šï¼Œmetricæ˜¯reweighted L1 [2016 MatchingNet]ï¼šMatching Networks for One Shot Learningï¼Œæœ¬è´¨ä¸Šä¹Ÿæ˜¯å­ªç”Ÿç½‘ç»œ+metric learningï¼Œç›‘ç£çš„æ˜¯support set Så’Œtest set Bçš„ç›¸ä¼¼åº¦â€”â€”åœ¨Sä¸‹è®­ç»ƒçš„æ¨¡å‹åœ¨Bçš„é¢„æµ‹ç»“æœè¯¯å·®æœ€å°ï¼Œç½‘ç»œä¸Šçš„åˆ›æ–°æ˜¯ç”¨äº†memory&amp;attentionï¼Œtrain procedureçš„åˆ›æ–°åœ¨äºâ€œtest and train conditions must match N-way-K-shotâ€ï¼Œ [2017 ProtoNet]ï¼šPrototypical Networks for Few-shot Learningï¼Œ [2019 few-shotç»¼è¿°]ï¼šA CLOSER LOOK AT FEW-SHOT CLASSIFICATION Siamese Neural Networks for One-shot Image Recognition åŠ¨æœº learning good features is expensive when little data is availableï¼šä¸€ä¸ªå…¸å‹ä»»åŠ¡one-shot learning we desire generalize to the new distribution without extensive retraining we propose train a siamese network to rank similarity between inputs capitalize on powerful discriminative features generalize the network to new data/new classes experiment on character recognition æ–¹æ³• general strategy learn image representationï¼šsupervised metric-based approachï¼Œsiamese neural network reuse the feature extractorï¼šon new dataï¼Œwithout any retraining why siamese we hypothesize that networks which do well at verification tasks should generalize to one-shot classification siamese nets twin networks accept distinct inputs that are joined by an energy function at the top twin back shares the weightsï¼šsymmetric åŸå§‹è®ºæ–‡ç”¨äº†contrastive energy functionï¼šcontains dual terms to increase like-pairs energy &amp; decrease unlike-pairs energy in this paper we use weighted L1 + sigmoid model conv-relu-maxpoolingï¼šconv of varying sizes æœ€åä¸€ä¸ªconv-reluå®Œäº†æ¥flatten-fc-sigmoidå¾—åˆ°å½’ä¸€åŒ–çš„feature vector ç„¶åæ˜¯joined layerï¼šè®¡ç®—ä¸¤ä¸ªfeature vectorçš„L1 distanceålearnable reweighting ç„¶åæ¥sigmoid loss binary classifier regularized CE loss functioné‡Œé¢åŠ äº†layer-wise-L2æ­£åˆ™ bpçš„æ—¶å€™ä¸¤ä¸ªå­ªç”Ÿç½‘ç»œçš„bp gradientæ˜¯additiveçš„ weight initialization conv weightsï¼šmean 0 &amp; std var 0.01 conv biasï¼šmean 0.5 &amp; std var 0.01 fc weightsï¼šmean 0 &amp; std var 0.2 fc biasï¼šmean 0.5 &amp; std var 0.01 learning schedule uniform lr decay 0.01 individual lr rate &amp; momentum annealing augmentation individual affine distortions æ¯ä¸ªaffine paramçš„probability 0.5 &lt;img src=&quot;few-shot/affine.png&quot; width=&quot;45%;&quot; /&gt; å®éªŒ dataset Omniglotï¼š50ä¸ªå­—æ¯ï¼ˆinternational/lesser known/fictitiousï¼‰ è®­ç»ƒç”¨çš„å­é›†ï¼š60% of the total dataï¼Œ12ä¸ªdraweråˆ›å»ºçš„30ä¸ªå­—æ¯ï¼Œæ¯ç±»æ ·æœ¬æ•°ä¸€æ ·å¤š validationï¼š4ä¸ªdrawerçš„10ä¸ªå­—æ¯ testï¼š4ä¸ªdrawerçš„10ä¸ªå­—æ¯ 8ä¸ªaffine transformsï¼š9å€æ ·æœ¬é‡ï¼Œsame&amp;different pairs åœ¨ä¸ç»è¿‡å¾®è°ƒè®­ç»ƒçš„æ¡ä»¶ä¸‹ï¼Œæ¨¡å‹ç›´æ¥åº”ç”¨åœ¨MNISTæ•°æ®é›†ï¼Œä»æœ‰70%çš„å‡†ç¡®ç‡ï¼šæ³›åŒ–èƒ½åŠ› è¯„ä»· å­ªç”Ÿç½‘ç»œå¯¹äºä¸¤ä¸ªå›¾åƒä¹‹é—´çš„å·®å¼‚æ˜¯éå¸¸æ•æ„Ÿçš„ ä¸€åªé»„è‰²çš„çŒ«å’Œé»„è‰²çš„è€è™ä¹‹é—´çš„å·®åˆ«è¦æ¯”ä¸€åªé»„è‰²çš„çŒ«å’Œé»‘è‰²çš„çŒ«ä¹‹é—´çš„å·®åˆ«æ›´å° ä¸€ä¸ªç‰©ä½“å‡ºç°åœ¨å›¾åƒçš„å·¦ä¸Šè§’å’Œå›¾åƒçš„å³ä¸‹è§’æ—¶å…¶æå–åˆ°çš„ç‰¹å¾ä¿¡æ¯å¯èƒ½æˆªç„¶ä¸åŒ å°¤å…¶ç»è¿‡å…¨è¿æ¥å±‚ï¼Œç©ºé—´ä½ç½®ä¿¡æ¯è¢«ç ´å æ‰‹å†™å­—ç¬¦æ•°æ®é›†ç›¸æ¯”è¾ƒäºImageNetå¤ªç®€å•äº† ä¼˜åŒ–ç½‘ç»œç»“æ„ï¼šMatchingNet æ›´å¥½çš„è®­ç»ƒç­–ç•¥ï¼šmeta learning ç°åœ¨å»å¤ç°å·²ç»æ²¡å•¥æ„ä¹‰ï¼Œç®—æ˜¯metric learningåœ¨å°æ ·æœ¬å­¦ä¹ ä¸Šçš„ä¸€ä¸ªstartupå§ MatchingNet: Matching Networks for One Shot Learning åŠ¨æœº learning new concepts rapidly from little data employ ideas metric learning memory cell define one-shot learning problems Omniglot ImageNet language tasks è®ºç‚¹ parametric models learns slow and require large datasets non-parametric models rapidly assimilate new examples we aim to incorporate both we propose Matching Nets uses recent advances in attention and memory that enable rapid learning test and train conditions must matchï¼šå¦‚æœè¦æµ‹è¯•ä¸€ä¸ªnç±»çš„æ–°åˆ†å¸ƒï¼Œå°±è¦åœ¨mç±»å¤§æ•°æ®é›†ä¸Šè®­ç±»ä¼¼çš„minibatchâ€”â€”æŠ½nä¸ªç±»ï¼Œæ¯ç±»show a few examples æ–¹æ³• build one-shot learning within the set-to-set framework è®­ç»ƒä»¥åçš„æ¨¡å‹ä¸éœ€è¦è¿›ä¸€æ­¥tuningå°±èƒ½produce sensible test labels for unobserved classes given a small support set $S=\{(x_i,y_i)\}^k_{i=0}$ train a classifier $c_S$ given a test example $\hat x$ï¼šwe get a probability distribution $\hat y=c_S(\hat x)$ define the mappingï¼š$S \rightarrow c_S $ to be $P(\hat y| \hat x ,S)$ when given a new support set $S^{â€˜}=\{\hat x\}$ï¼šç›´æ¥ç”¨æ¨¡å‹På»é¢„æµ‹$\hat y$å°±å¯ä»¥äº† simplest formï¼š \hat y = \sum_{i=1}^k a(\hat x, x_i)y_i aæ˜¯attention mechanismï¼šå¦‚æœå’Œæµ‹è¯•æ ·æœ¬$\hat x$æœ€è¿œçš„bä¸ªæ”¯æŒæ ·æœ¬$x_i$çš„attentionæ˜¯0ï¼Œå…¶ä½™ä¸ºä¸€ä¸ªå®šå€¼ï¼Œè¿™å°±ç­‰ä»·äºä¸€ä¸ªk-b-NNæœºåˆ¶ $y_i$ act as memoriesï¼šå¯ä»¥æŠŠæ¯ä¸ª$y_i$çœ‹ä½œæ˜¯æ¯ä¸ª$x_i$æå–åˆ°çš„ä¿¡æ¯ä¿å­˜æˆmemory workflowå®šä¹‰ï¼šgiven a inputï¼Œæˆ‘ä»¬åŸºäºattentioné”å®šcorresponding samples in the support setï¼Œå¹¶retrieve the label attention kernel ç”¨ä¸€ä¸ªembedding functionå…ˆå°†$\hat x$å’Œ$x_i$è½¬åŒ–æˆembeddings ç„¶åè®¡ç®—å’Œæ¯ä¸ª$x_i$ embeddingçš„cosine distance ç„¶åsoftmaxï¼Œå¾—åˆ°æ¯ä¸ªçš„attention value softmaxä¹‹åçš„attention valueï¼Œå¤§éƒ¨åˆ†æ˜¯Né€‰1ï¼Œå¦‚æœæ¯ä¸ªattention valueéƒ½ä¸é«˜ï¼Œè¯´æ˜query sampleå’Œè®­ç»ƒé›†æ¯ç±»éƒ½ä¸åƒï¼Œæ˜¯ä¸ªnovel Full Context Embeddingsï¼ˆFCEï¼‰ ç®€å•çš„æ¨¡å¼ä¸‹få’Œgå°±æ˜¯ä¸¤ä¸ªshared weightsçš„CNN feature extractorï¼ŒFCEæ˜¯æ¥åœ¨å¸¸è§„feature vectoråé¢ç²¾å¿ƒè®¾è®¡çš„ä¸€ä¸ªç»“æ„ è®¾è®¡æ€è·¯ gï¼šsupport set donâ€™t get embedded individually fï¼šsupport set modify how we embed the test image the first issueï¼š bidirectional Long-Short Term Memory encoder the whole support set as contextsï¼Œeach time stepçš„è¾“å…¥æ˜¯$g^{â€˜}(x_i)$ skip connection g(x_i) = \overrightarrow{h_i}+\overleftarrow{h_i}+g^{'}(x_i) the second issue LSTM with read attention over the whole set S $f(\hat x, S)=attLSTM(f^{â€˜}(\hat x), g(S), K)$ $f^{â€˜}(\hat x)$æ˜¯query sampleçš„feature vectorï¼Œä½œä¸ºLSTM each time stepçš„è¾“å…¥ $K$æ˜¯fixed number of unrolling stepsï¼Œé™åˆ¶LSTMè®¡ç®—çš„stepï¼Œä¹Ÿå°±æ˜¯feature vectorå‚ä¸LSTMå¾ªç¯è®¡ç®—çš„æ¬¡æ•°ï¼Œæœ€ç»ˆçš„è¾“å‡ºæ˜¯$h_K$ skip connection as above support set Sçš„å¼•å…¥ï¼š content based attention + softmax $r_{k-1}$å’Œ$h_{k-1}$æ˜¯concatåˆ°ä¸€èµ·ï¼Œä½œä¸ºhidden statesï¼šã€QUESTIONã€‘è¿™æ ·lstm cellçš„hidden sizeå°±å˜äº†å•Šï¼Ÿï¼Ÿï¼Ÿ attention of K fixed unrolling steps encode $x_i$ in the context of the support set S training strategy the training procedure has to be chosen carefully so as to match the never seen task defineï¼šä»å…¨é›†ä¸­é€‰å–few unique classes(e.g. 5)ï¼Œæ¯ä¸ªç±»åˆ«é€‰å–few examples(e.g. 1-5)ï¼Œæ„æˆsupport set Sï¼Œå†ä»å¯¹åº”ç±»åˆ«æŠ½ä¸€ä¸ªbatch Bï¼Œè®­ç»ƒç›®æ ‡å°±æ˜¯minimise the error predicting the labels in the batch B conditioned on the support set S batch Bçš„é¢„æµ‹è¿‡ç¨‹å°±æ˜¯figure1ï¼šéœ€è¦$g(S(x_i,y_i))$å’Œ$f(\hat x)$è®¡ç®—$P(\hat y|\hat x, S)$ï¼Œç„¶åå’Œ$gt(\hat y)$è®¡ç®—log loss å®éªŒ æ¨¡å¼ N-way-K-shot train one-shot testï¼šç”¨å”¯ä¸€çš„one-shot novel sampleç”Ÿæˆå¯¹åº”ç±»åˆ«çš„feature vectorï¼Œç„¶åå¯¹æ¯ä¸ªtest sampleè®¡ç®—cosine distanceï¼Œé€‰æ‹©æœ€è¿‘çš„ä½œä¸ºå…¶ç±»åˆ« comparing methods baseline classifier + NN MANN Convolutional Siamese Net + NN further finetuningï¼šone-shot ç»“è®º using more examples for k-shot classification helps all models 5-way is easier than 20-way siamese netåœ¨5-shotçš„æ—¶å€™è·Ÿour methodå·®ä¸å¤šï¼Œä½†æ˜¯one-shot degrades rapidly FCEåœ¨ç®€å•æ•°æ®é›†ï¼ˆOmniglotï¼‰ä¸Šæ²¡å•¥ç”¨ï¼Œåœ¨harder taskï¼ˆminiImageNetï¼‰æ˜¾è‘—æå‡ A CLOSER LOOK AT FEW-SHOT CLASSIFICATION åŠ¨æœº ä¸ºä¸»æµæ–¹æ³•æä¾›ä¸€ä¸ªconsistent comparative analysisï¼Œå¹¶ä¸”å‘ç°ï¼š deeper backbones significantly reduce differences reducing intra-class variation is an important factor when shallow backbone propose a modified baseline method achieves com- petitive performance verified on miniImageNet &amp; CUB in realistic cross-domain settings generalization analysis baseline method with standard fine-tuning win è®ºç‚¹ three main categories of methods initialization based aims to learn good model initialization to achieve rapid adaption with a limited number of training samples have difficulty in handling domain shifts metric learning based è®­ç»ƒç›®æ ‡æ˜¯learn to compare if a model can determine the similarity of two images, it can classify an unseen input image with the labeled instancesï¼šæœ¬è´¨æ˜¯similarityè®¡ç®—å™¨ï¼Œè„±ç¦»label level èŠ±å¼è®­ç»ƒç­–ç•¥ï¼šmeta learning/graph èŠ±å¼è·ç¦»metricï¼šcosine/Euclidean turns outå¤§å¯ä¸å¿…ï¼š a simple baseline method with a distance- based classifier is competitive to the sophisticated algorithms simply reducing intra-class variation in a baseline method leads to competitive performance hallucination based ç”¨base classè®­ç»ƒä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼Œç„¶åç”¨ç”Ÿæˆæ¨¡å‹ç»™novel classé€ å‡æ•°æ® é€šå¸¸å’Œmetric-basedæ¨¡å‹ç»“åˆèµ·æ¥ç”¨ï¼Œä¸å•ç‹¬åˆ†æ two main challenges æ²¡æ³•ç»Ÿä¸€æ¨ªå‘æ¯”è¾ƒ implementation detailsæœ‰å·®å¼‚ï¼Œbaseline approachè¢«under-estimatedï¼šæ— æ³•å‡†ç¡®é‡åŒ–the relative performance gain lack of domain shift between base &amp; novel datasetsï¼šmakes the evaluation scenarios unrealistic our work é’ˆå¯¹ä»£è¡¨æ€§æ–¹æ³•conduct consistent comparative experiments on common ground discoveries on deeper backbones è½»å¾®æ”¹åŠ¨baseline methodè·å¾—æ˜¾è‘—æå‡ replace the linear classifier with distance-based classifier practical sceneries with domain shift å‘ç°è¿™ç§ç°å®åœºæ™¯ä¸‹ï¼Œé‚£äº›ä»£è¡¨æ€§çš„few-shot methodsåè€Œå¹²ä¸è¿‡baseline method open source codeï¼šhttps://github.com/wyharveychen/CloserLookFewShot æ–¹æ³• baseline standard transfer learningï¼špre-training + fine-tuning training stage train a feature extractor $f_{\theta}$ and a classifier $C_{W_b}$ use abundant base class labeled data standard CE loss fine-tuning stage fix feature extractor $f_{\theta}$ train a new classifier $C_{W_n}$ use the few labeled novel samples standard CE loss baseline++ variant of the baselineï¼šå”¯ä¸€çš„ä¸åŒå°±åœ¨äºclassifier design æ˜¾å¼åœ°reduce intra-class varation among features during trainingï¼Œå’Œcenter lossæ€è·¯æœ‰ç‚¹åƒï¼Œä½†æ˜¯center lossçš„è´¨å¿ƒæ˜¯æ»‘åŠ¨å¹³å‡çš„ï¼Œè¿™é‡Œé¢çš„è´¨å¿ƒæ˜¯learnableçš„ training stage write the weight matrix $W_b$ as $[w_1, w_2, â€¦, w_c]$ï¼Œç±»ä¼¼æ¯ç±»çš„ç°‡å¿ƒ for an input featureï¼Œcompute cosine similarity multiply a class-wise learnable scalar to adjust origin [-1,1] value to fit softmax ç„¶åç”¨softmaxå¯¹similarity vectorè¿›è¡Œå½’ä¸€åŒ–ï¼Œä½œä¸ºpredict label the softmax function prevents the learned weight vectors collapsing to zerosï¼šæ¯ç±»çš„é¢„æµ‹distanceéƒ½æ˜¯0æ˜¯ç½‘ç»œæ¯”è¾ƒå®¹æ˜“é™·å…¥çš„å±€éƒ¨æœ€ä¼˜è§£ ã€in fine-tuning stageï¼Ÿï¼Ÿã€‘ meta-learning algorithms three distance metric based methodsï¼šMatchingNetï¼ŒProtoNetï¼ŒRelationNet one initialization based methodï¼šMAML meta-training stage a collection of N-way-K-shot tasks ä½¿å¾—æ¨¡å‹$M(*|S)$å­¦ä¼šçš„æ˜¯ä¸€ç§å­¦ä¹ æ¨¡å¼â€”â€”åœ¨æœ‰é™æ•°æ®ä¸‹åšé¢„æµ‹ meta-testing stage æ‰€æœ‰çš„novel dataéƒ½ä½œä¸ºå¯¹åº”ç±»åˆ«çš„support set (class mean) æ¨¡å‹å°±ç”¨è¿™ä¸ªæ–°çš„support setæ¥è¿›è¡Œé¢„æµ‹ Different meta-learning methodsä¸»è¦åŒºåˆ«åœ¨äºå¦‚ä½•åŸºäºsupport setåšé¢„æµ‹ï¼Œä¹Ÿå°±æ˜¯classifierçš„è®¾è®¡ MatchingNetè®¡ç®—çš„æ˜¯queryå’Œsupport setçš„æ¯ä¸ªcosine distanceï¼Œç„¶åmean per class ProtoNetæ˜¯å…ˆå¯¹support featuresæ±‚class meanï¼Œç„¶åEuclidean distance RelationNetå…ˆå¯¹support featuresæ±‚class meanï¼Œç„¶åå°†è·ç¦»è®¡ç®—æ¨¡å—æ›¿æ¢æˆlearnable relation module å®éªŒ three scenarios generic object recognitionï¼šmini-ImageNetï¼Œ100ç±»ï¼Œ600å¼ per classï¼Œã€64-baseï¼Œ16-valï¼Œ20-novelã€‘ fine-grained image classificationï¼šCUB-200-2011ï¼Œ200ç±»ï¼Œæ€»å…±11,788å¼ ï¼Œã€random 100-baseï¼Œ50-valï¼Œ50-novelã€‘ cross-domain adaptationï¼šmini-ImageNet â€”&gt; CUBï¼Œã€100-mini-ImageNet-baseï¼Œ50-CUB-valï¼Œ50-CUB-testã€‘ training details baselineå’Œbaseline++æ¨¡å‹ï¼štrain 400 epochsï¼Œbatch size 16 meta learning methodsï¼š train 60000 episodes for 5-way-1-shot tasksï¼Œtrain 40000 episodes for 5-way-5-shot tasks use validation set to select the training episodes with the best acc k-shot for support setï¼Œ16 instances for query set Adam with 1e-3 initial lr standard data augmentationï¼šcropï¼Œleft-right flipï¼Œcolor jitter testing stage average over 600 experiments each experiment randomly choose 5-way-k-shot support set + 16 instances query set meta learning methodsç›´æ¥åŸºäºsupport setç»™å‡ºå¯¹query setçš„é¢„æµ‹ç»“æœ baseline methodsåŸºäºsupport setè®­ç»ƒä¸€ä¸ªæ–°çš„åˆ†ç±»å¤´ï¼Œ100 iterationsï¼Œbatch size 4 æ¨¡å‹details baseline++çš„similarityä¹˜ä¸Šäº†class-wise learnable scalar MachingNetç”¨äº†FCE classification layer without fine-tuningç‰ˆæœ¬ï¼Œä¹Ÿä¹˜äº†class-wise learnable scalar RelationNetå°†L2 normæ›¿æ¢æˆsoftmaxåŠ é€Ÿè®­ç»ƒ MAMLä½¿ç”¨äº†ä¸€é˜¶æ¢¯åº¦è¿‘ä¼¼for efficiency åˆæ­¥ç»“æœ 4-layer conv backbone input size 84x84 originå’Œre-implementationçš„ç²¾åº¦å¯¹æ¯” åŸå§‹çš„baselineæ²¡åŠ data augmentationï¼Œæ‰€ä»¥è¿‡æ‹Ÿåˆäº†ç²¾åº¦å·®ï¼Œè¢«underestimatedäº† MatchingNetåŠ äº†é‚£ä¸ªscalar shiftçš„æ”¹è¿›ä»¥åç²¾åº¦æœ‰æ˜¾è‘—æå‡ ProtoNetåŸè®ºæ–‡æ˜¯20-shot&amp;30-shotï¼Œæœ¬æ–‡ä¸»è¦æ¯”è¾ƒ1-shotå’Œ5-shotï¼Œç²¾åº¦éƒ½æ”¾å‡ºæ¥äº† our experiment settingä¸‹å„æ¨¡å‹çš„ç²¾åº¦å¯¹æ¯” baseline++å¤§å¹…æå‡ç²¾åº¦ï¼Œå·²ç»è·Ÿmeta learning methodså·®ä¸å¤šäº† è¯´æ˜few-shotçš„key factoræ˜¯reduce intra-class variation ä½†æ˜¯è¦æ³¨æ„çš„æ˜¯è¿™æ˜¯åœ¨4-layer-convçš„backbone settingä¸‹ï¼Œdeeper backbone can inherently reduce intra-class variation å¢åŠ ç½‘ç»œæ·±åº¦ ä¸Šé¢è¯´äº†ï¼Œdeeper backboneèƒ½å¤Ÿéšå¼åœ°é™ä½ç±»å†…è·ç¦» deeper models conv4 conv6ï¼šç›¸å¯¹äºconv4é‚£ä¸ªæ¨¡å‹ï¼ŒåŠ äº†ä¸¤å±‚conv blocks without pooling resnet10ï¼šç®€åŒ–ç‰ˆresnet18ï¼Œr18é‡Œé¢conv blockçš„ä¸¤å±‚å·ç§¯æ¢æˆä¸€å±‚ resnet18ï¼šorigin paper resnet34ï¼šorigin paper éšç€ç½‘ç»œåŠ æ·±ï¼Œå„æ–¹æ³•çš„ç²¾åº¦å·®å¼‚ç¼©å°ï¼Œbaselineæ–¹æ³•ç”šè‡³åè¶…äº†ä¸€äº›meta learningæ–¹æ³• effect of domain shift ä¸€ä¸ªç°å®åœºæ™¯ï¼šmini-ImageNet â€”&gt; CUBï¼Œæ”¶é›†general class dataç›¸å¯¹å®¹æ˜“ï¼Œæ”¶é›†fine-grainedæ•°æ®é›†åˆ™æ›´å›°éš¾ ç”¨resnet18å®éªŒ Baseline outperforms all meta-learning methods under this scenario å› ä¸ºmeta learning methodsçš„å­¦ä¹ å®Œå…¨ä¾èµ–äºbase support classï¼Œnot able to adapt éšç€domain difference get largerï¼ŒBaselineç›¸å¯¹äºå…¶ä»–æ–¹æ³•çš„gapä¹Ÿé€æ¸æ‹‰å¤§ è¯´æ˜äº†åœ¨domain shiftåœºæ™¯ä¸‹ï¼Œadaptation based methodçš„å¿…è¦æ€§ further adapt meta-learning methods MatchingNet &amp; ProtoNetï¼šè·Ÿbaselineæ–¹æ³•ä¸€æ ·ï¼Œfix feature extractorï¼Œç„¶åç”¨novel set train a new classifier MAMLï¼šnot feasible to fix the featureï¼Œç”¨novel set finetuneæ•´ä¸ªç½‘ç»œ RelationNetï¼šfeaturesæ˜¯conv mapsè€Œä¸æ˜¯vectorï¼Œrandomly splitä¸€éƒ¨åˆ†novel setä½œä¸ºè®­ç»ƒé›† MatchingNet &amp; MAMLéƒ½æœ‰å¤§å¹…ç²¾åº¦æå‡ï¼Œå°¤å…¶åœ¨domain shiftåœºæ™¯ä¸‹ï¼Œä½†æ˜¯ProtoNetä¼šæ‰ç‚¹ï¼Œè¯´æ˜adaptationæ˜¯å½±å“ç²¾åº¦çš„key factorï¼Œä½†æ˜¯è¿˜æ²¡æœ‰å®Œç¾è§£å†³æ–¹æ¡ˆ]]></content>
      <tags>
        <tag>å°æ ·æœ¬</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ATSS]]></title>
    <url>%2F2021%2F06%2F17%2FATSS%2F</url>
    <content type="text"><![CDATA[ATSS: Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection åŠ¨æœº anchor-basedå’Œanchor-freeæ–¹æ³•çš„æœ¬è´¨åŒºåˆ«æ˜¯å¯¹æ­£è´Ÿæ ·æœ¬çš„å®šä¹‰ï¼Œè¿™ä¹Ÿç›´æ¥å¯¼è‡´äº†performance gap we propose ATSS adaptive training sample selection automatically select positive and negative samples according to statistical characteristics of objects anchor-based&amp;anchor-freeæ¨¡å‹ä¸Šéƒ½æ¶¨ç‚¹ discuss tiling multiple anchors è®ºç‚¹ ä¸»æµanchor-basedæ–¹æ³• one-stage/two-stage tile a large number of preset anchors on the image output these refined anchors as detection results anchor-free detectorsä¸»è¦åˆ†æˆä¸¤ç§ key-point basedï¼šé¢„æµ‹è§’ç‚¹/è½®å»“ç‚¹/heatmapï¼Œç„¶åboundè½®å»“å¾—åˆ°æ¡† center-basedï¼šé¢„æµ‹ä¸­å¿ƒç‚¹ï¼Œç„¶ååŸºäºä¸­å¿ƒç‚¹å›å½’4ä¸ªè·ç¦» æ¶ˆé™¤pre-defined anchorsçš„hyper-paramsï¼šå¼ºåŒ–generalization ability ä¸¾ä¾‹å¯¹æ¯”RetinaNet&amp;FCOS RetinaNetï¼šone-stage anchor-based FCOSï¼šcenter-based anchor-free ä¸»è¦åŒºåˆ«1ï¼šanchoræ•°é‡ï¼ŒRetinaNetæ˜¯hxwx9ï¼ŒFCOSæ˜¯hxwx1 ä¸»è¦åŒºåˆ«2ï¼šæ­£æ ·æœ¬å®šä¹‰ï¼ŒRetinaNetæ˜¯ä¸gt boxçš„IOUå¤§äºä¸€å®šé˜ˆå€¼çš„anchorï¼ŒFCOSæ˜¯featuremapä¸Šæ‰€æœ‰è½è¿›æ¡†å†…çš„æ ¼å­ç‚¹ ä¸»è¦åŒºåˆ«3ï¼šå›å½’æ–¹å¼ï¼ŒRetinaNetæ˜¯å›å½’gtç›¸å¯¹pos anchorçš„ç›¸å¯¹åç§»é‡ï¼ŒFCOSæ˜¯é¢„æµ‹å››æ¡è¾¹ç›¸å¯¹ä¸­å¿ƒç‚¹çš„ç»å¯¹è·ç¦» Difference Analysis of Anchor-based and Anchor-free Detection we focus on the last two differencesï¼šæ­£è´Ÿæ ·æœ¬å®šä¹‰ &amp; å›å½’starting status è®¾å®šRetinaNetä¹Ÿæ˜¯one square anchor per locationï¼Œå’ŒFCOSä¿æŒä¸€è‡´ experiment setting MS COCOï¼š80ç±»å‰æ™¯ï¼Œcommon split ImageNet pretrained ResNet-50 resize input SGDï¼Œ90K iterationsï¼Œ0.9 momentumï¼Œ1e-4 weight decayï¼Œ16 batch sizeï¼Œ0.01 lr with 0.1 lr decay/60K testingï¼š 0.05 score to filter out bg boxes output top 1000 detections per feature pyramid 0.6 IoU thresh per class NMS to give final top 100 detections per image inconsistency removal äº”å¤§improvementsåŠ åœ¨FCOSä¸Šè¿›ä¸€æ­¥boost the gap æˆ‘ä»¬å°†å…¶é€æ­¥åŠ åœ¨RetinaNetä¸Šï¼Œèƒ½å¤Ÿæ‹‰åˆ°37%ï¼Œå’ŒFCOSè¿˜æœ‰0.8ä¸ªç‚¹çš„å·®è· åˆ†æessential difference è®­ç»ƒä¸€ä¸ªæ£€æµ‹æ¨¡å‹ï¼Œé¦–å…ˆè¦åˆ†å‡ºæ­£è´Ÿæ ·æœ¬ï¼Œç„¶åç”¨æ­£æ ·æœ¬æ¥å›å½’ Classification RetinaNetç”¨anchor boxesä¸gt boxçš„IoUå†³å®šæ­£è´Ÿæ ·æœ¬ï¼šbest match anchorå’Œå¤§äºä¸€å®šIoU threshçš„anchoræ˜¯æ­£æ ·æœ¬ï¼Œå°äºä¸€å®šIoU threshçš„anchoræ˜¯è´Ÿæ ·æœ¬ï¼Œå…¶ä»–çš„æ˜¯ignoreæ ·æœ¬ FCOSç”¨spatial and scale constraintsé€‰æ‹©æ­£è´Ÿæ ·æœ¬ï¼šgt boxä»¥å†…çš„æ‰€æœ‰åƒç´ ä½œä¸ºå€™é€‰æ­£æ ·æœ¬ï¼Œç„¶åå»æ‰éƒ¨åˆ†å°ºåº¦ä¸åŒ¹é…çš„å€™é€‰æ ·æœ¬ï¼Œæ­£æ ·æœ¬ä»¥å¤–éƒ½æ˜¯è´Ÿæ ·æœ¬ï¼Œæ²¡æœ‰ignore ä¸¤ä¸ªæ¨¡å‹åœ¨ä¸¤ç§æ ·æœ¬é€‰æ‹©ç­–ç•¥ä¸Šå®éªŒï¼šSpatial and Scale Constraintç›¸æ¯”è¾ƒäºIoUéƒ½ä¼šæ˜¾è‘—æç‚¹ å½“ä¸¤ç§æ–¹æ³•éƒ½ä½¿ç”¨Spatial and Scale Constraintç­–ç•¥é€‰æ‹©æ­£è´Ÿæ ·æœ¬ï¼Œæ¨¡å‹ç²¾åº¦å°±æ²¡å•¥å·®åˆ«äº† Regression RetinaNet regresses from the anchor box with 4 offsetsï¼šå›å½’gt boxç›¸å¯¹äºanchor boxçš„åç§»é‡ï¼Œregression starting statusæ˜¯ä¸ªbox FCOS regresses from the anchor point with 4 distancesï¼šå›å½’gt boxå››æ¡è¾¹ç›¸å¯¹äºanchor centerçš„è·ç¦»ï¼Œregression starting statusæ˜¯ä¸ªpoint ä¸Šé¢é‚£ä¸ªè¡¨è¯´æ˜äº†é€‰æ‹©åŒæ ·çš„æ­£è´Ÿæ ·æœ¬ï¼Œregression starting statuså°±æ˜¯ä¸ªæ— å…³é¡¹ï¼Œä¸å½±å“ç²¾åº¦ Adaptive Training Sample Selection ï¼ˆATSSï¼‰ å½±å“æ£€æµ‹æ¨¡å‹ç²¾åº¦çš„essential differenceåœ¨äºhow to define positive and negative training samples previous strategieséƒ½æœ‰sensitive hyperparametersï¼ˆanchors/scaleï¼‰ï¼Œsome outer objects may be neglected we propose ATSS almost no hyper divides pos/neg samples according to data statistical characteristics å¯¹æ¯ä¸ªgt boxï¼Œé¦–å…ˆåœ¨æ¯ä¸ªlevelä¸Šï¼ŒåŸºäºL2 center distanceï¼Œæ‰¾åˆ°k-closest anchorâ€”â€”k*Lä¸ªcandidates per gt box è®¡ç®—æ¯ä¸ªcandidatesçš„mean &amp; var åŸºäºmean &amp; var è®¡ç®—è¿™ä¸ªgt boxçš„IoU thresh åœ¨candidatesé‡Œé¢é€‰å–å¤§äºç­‰äºIoU threshï¼ŒåŒæ—¶anchor centeråœ¨gt boxå†…çš„ï¼Œç•™ä½œæ­£æ ·æœ¬ å¦‚æœä¸€ä¸ªacnhor boxåŒ¹é…äº†å¤šä¸ªgt boxï¼Œé€‰æ‹©IoUå¤§çš„é‚£ä¸ªä½œä¸ºæ ‡ç­¾ åŸºäºcenter distanceé€‰æ‹©anchor boxï¼šå› ä¸ºè¶Šé è¿‘ç›®æ ‡ä¸­å¿ƒï¼Œè¶Šå®¹æ˜“produceé«˜å“è´¨æ¡† ç”¨mean+varä½œä¸ºIoU threshï¼š higher mean indicates high-quality candidatesï¼Œå¯¹åº”çš„IoU threshåº”è¯¥é«˜ä¸€ç‚¹ higher variation indicates level specificï¼Œmean+varä½œä¸ºthreshèƒ½å°†candidatesé‡Œé¢IoUè¾ƒé«˜çš„ç­›é€‰å‡ºæ¥ limit the anchor center in objectï¼šanchorä¸­å¿ƒä¸åœ¨ç›®æ ‡æ¡†å†…æ˜¾ç„¶ä¸æ˜¯ä¸ªå¥½æ¡†ï¼Œç”¨äºç­›æ‰å‰ä¸¤æ­¥é‡Œçš„æ¼ç½‘ä¹‹é±¼ï¼ŒåŒä¿é™© fair between different objects ç»Ÿè®¡ä¸‹æ¥æ¯ç±»ç›®æ ‡éƒ½æœ‰å·®ä¸å¤š0.2kLä¸ªæ­£æ ·æœ¬ï¼Œä¸å°ºåº¦æ— å…³ ä½†æ˜¯RetinaNetå’ŒFCOSéƒ½æ˜¯å¤§ç›®æ ‡æ­£æ ·æœ¬å¤šï¼Œå°ç›®æ ‡æ­£æ ·æœ¬å°‘ hyperparam-freeï¼šåªæœ‰ä¸€ä¸ªkï¼Œã€è¿˜æœ‰anchor-settingå‘¢ï¼Ÿï¼Ÿï¼Ÿã€‘ verification lite versionï¼šè¢«FCOSå®˜æ–¹å¼•ç”¨å¹¶ç§°ä½œcenter samplingï¼Œscale limit still exists in this version full versionï¼šæœ¬æ–‡ç‰ˆæœ¬ ä¸¤ä¸ªæ–¹æ³•é€‰æ‹©candidatesçš„æ–¹æ³•å®Œå…¨ä¸€è‡´ï¼Œå°±æ˜¯select final postivesçš„æ–¹æ³•ä¸åŒ hyperparamçš„é²æ£’æ€§ kåœ¨ä¸€å®šèŒƒå›´å†…ï¼ˆ7-17ï¼‰ç›¸å¯¹insensitiveï¼Œå¤ªå¤šäº†ä½è´¨é‡æ¡†å¤ªå¤šï¼Œå¤ªå°‘äº†less statistical å°è¯•ä¸åŒçš„fix-ratio anchor scaleå’Œfix-scale anchor ratioï¼Œå‘ç°ç²¾åº¦ç›¸å¯¹ç¨³å®šï¼Œè¯´æ˜robust to anchor settings multi-anchors settings RetinaNetåœ¨ä¸åŒçš„anchor settingä¸‹ï¼Œç²¾åº¦åŸºæœ¬ä¸å˜ï¼Œè¯´æ˜ä¸»è¦æ­£æ ·æœ¬é€‰çš„å¥½ï¼Œä¸ç®¡ä¸€ä¸ªlocationç»‘å®šå‡ ä¸ªanchorç»“æœéƒ½ä¸€æ ·]]></content>
      <tags>
        <tag>ç›®æ ‡æ£€æµ‹ï¼Œanchor-free&amp;anchor-basedï¼Œtricks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[re-labeling]]></title>
    <url>%2F2021%2F05%2F27%2Fre-labeling%2F</url>
    <content type="text"><![CDATA[Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels åŠ¨æœº label noise single-label benchmark but contains multiple classes in one sample a random crop may contain an entirely different object from the gt label exhaustive multi-label annotations per image is too cost mismatch researches refine the validation set with multi-labels propose new multi-label evaluation metrics ä½†æ˜¯é€ æˆæ•°æ®é›†çš„mismatch we propose re-label use a strong image classifier trained on extra source of data to generate the multi-labels use pixel-wise multi-label predictions before GAPï¼šaddtional location-specific supervision then trained on re-labeled samples further boost with CutMix from single to multi-labelsï¼šå¤šæ ‡ç­¾ from global to localizedï¼šdense prediction map è®ºç‚¹ single-label å’Œmulti-label validation setçš„mismatch random crop augmentationåŠ å‰§äº†é—®é¢˜ é™¤äº†å¤šç›®æ ‡è¿˜æœ‰å‰èƒŒæ™¯ï¼Œåªæœ‰23%çš„random crops IOU&gt;0.5 ideally label the full set of classesâ€”â€”multi-label where each objectsâ€”â€”localized label results in a dense pixel labeling $L\in \{0,1\}^{HWC}$ we propose a re-labeling strategy ReLabel strong classifier external training data generate feature map predictions LabelPooling with dense labels &amp; random crop pooling the label scores from crop region evaluations baseline r50ï¼š77.5% r50 + ReLabelï¼š78.9% r50 + ReLabel + CutMixï¼š80.2% ã€QUESTIONã€‘åŒæ ·æ˜¯å¼•å…¥å¤–éƒ¨æ•°æ®å®ç°æ— ç—›é•¿ç‚¹ï¼Œä¸noisy studentçš„åŒºåˆ«/å¥½åï¼Ÿï¼Ÿï¼Ÿ ç›®å‰è®ºæ–‡æåˆ°çš„å°±åªæœ‰efficiencyï¼ŒReLabelæ˜¯one-time costçš„ï¼ŒçŸ¥è¯†è’¸é¦æ˜¯iterative&amp;on-the-flyçš„ æ–¹æ³• Re-labeling super annotator state-of-the-art classifier trained on super large dataset fine-tuned on ImageNet and predict ImageNet labels we use open-source trained weights as annotators though trained with single-label supervision still tend to make multi-label predictions EfficientNet-L2 input size 475 feature map size 15x15x5504 output dense label size 15x15x1000 location-specific labels remove GAP heads add a 1x1 conv è¯´ç™½äº†å°±æ˜¯ä¸€ä¸ªfcn original classifierçš„fcå±‚æƒé‡ä¸æ–°æ·»åŠ çš„1x1 convå±‚çš„æƒé‡æ˜¯ä¸€æ ·çš„ labelçš„æ¯ä¸ªchannelå¯¹åº”äº†ä¸€ä¸ªç±»åˆ«çš„heatmapï¼Œå¯ä»¥çœ‹åˆ°disjointly located at each objectâ€™s position LabelPooling loads the pre-computed label map region pooling (RoIAlign) on the label map GAP + softmax to get multi-label vector train a classifier with the multi-label vector uses CE choices space consumption ä¸»è¦æ˜¯å­˜å‚¨label mapçš„ç©ºé—´ store only top-5 predictions per imageï¼š10GB time consumption ä¸»è¦æ˜¯è¯´ç”Ÿæˆlabel mapçš„one-shot-inference timeå’ŒlabelPoolingå¼•å…¥çš„é¢å¤–è®¡ç®—æ—¶é—´ relabelingï¼š10-GPU hours labelPoolingï¼š0.5% additiona training time more efficient than KD annotators æ ‡æ³¨å·¥å…·å“ªå®¶å¼ºï¼šç›®å‰çœ‹ä¸‹æ¥eff-L2çš„supervisionæ•ˆæœæœ€å¼º supervision confidence éšç€image cropä¸å‰æ™¯ç‰©ä½“çš„IOUå¢å¤§ï¼Œconfidenceé€æ¸å¢åŠ  è¯´æ˜supervision provides some uncertainty when low IOU å®éªŒ]]></content>
      <tags>
        <tag>pretaining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mlpç³»åˆ—]]></title>
    <url>%2F2021%2F05%2F27%2Fmlp%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[[papers] [MLP-Mixer] MLP-Mixer: An all-MLP Architecture for Visionï¼ŒGoogle [ResMLP] ResMLP: Feedforward networks for image classification with data-efficient trainingï¼ŒFacebook [references] https://mp.weixin.qq.com/s?__biz=MzUyMjE2MTE0Mw==&amp;mid=2247493478&amp;idx=1&amp;sn=2be608d776b2469b3357da30c42d9770&amp;chksm=f9d2b9fecea530e8cbf07847c2029a1dabb131dbc1d6bd91ed227e41a396dd333afc83b64cf8&amp;scene=21#wechat_redirect https://mp.weixin.qq.com/s/8f9yC2P3n3HYygsOo_5zww MLP-Mixer: An all-MLP Architecture for Vision åŠ¨æœº image classification task neither of [CNN, attention] are necessary our proposed MLP-Mixer ä»…åŒ…å«multi-layer-perceptrons independently to image patches repeated applied across either spatial locations or feature channels two types applied independently to image patches applied across patches æ–¹æ³• overview è¾“å…¥æ˜¯token sequences non-overlapping image patches linear projected to dimension C Mixer Layer maintain the input dimension channel-mixing MLP operate on each token independently å¯ä»¥çœ‹ä½œæ˜¯1x1çš„conv token-mixing MLP operate on each channel independently take each spatial vectors (hxw)x1 as inputs å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªglobal depth-wise convï¼Œs1ï¼Œsame padï¼Œkernel sizeæ˜¯(h,w) æœ€åå¯¹token embeddingåšGAPï¼Œæå–sequence vecï¼Œç„¶åè¿›è¡Œç±»åˆ«é¢„æµ‹ idea behind Mixer clearly separate the per-location operations &amp; cross-location operations CNNæ˜¯åŒæ—¶è¿›è¡Œè¿™ä¿©çš„ transformerçš„MSAåŒæ—¶è¿›è¡Œè¿™ä¿©ï¼ŒMLPåªè¿›è¡Œper-location operations Mixer Layer two MLP blocks given input $X\in R^{SC}$ï¼ŒS for spatial dimï¼ŒC for channel dim å…ˆæ˜¯token-mixing MLP acts on S dim maps $R^S$ to $R^S$ share across C-axis LN-FC-GELU-FC-residual ç„¶åæ˜¯channel-mixing MLP acts on C dim maps $R^C$ to $R^C$ share across S-axis LN-FC-GELU-FC-residual fixed widthï¼Œæ›´æ¥è¿‘transformer/RNNï¼Œè€Œä¸æ˜¯CNNé‚£ç§é‡‘å­—å¡”ç»“æ„ ä¸ä½¿ç”¨positional embeddings the token-mixing MLPs are sensitive to the order of the input tokens may learn to represent locations å®éªŒ ResMLP: Feedforward networks for image classification with data-efficient training åŠ¨æœº entirely build upon MLP alternates from a simple residual network a linear layer to interact with image patches a two-layer FFN to interact independently with each patch affine transformæ›¿ä»£LNæ˜¯ä¸€ä¸ªç‰¹åˆ«ä¹‹å¤„ trained with modern strategy heavy data-augmentation optionally distillation show good performace on ImageNet classification è®ºç‚¹ strongly inspired by ViT but simpler æ²¡æœ‰attentionå±‚ï¼Œåªæœ‰fcå±‚+gelu æ²¡æœ‰normå±‚ï¼Œå› ä¸ºmuch more stable to trainï¼Œä½†æ˜¯ç”¨äº†affine transformation æ–¹æ³• overview takes flattened patches as inputs typically N=16ï¼š16x16 linear project the patches into embeddings form $N^2$ d-dim embeddings ResMLP Layer main the dim throughout $[N^2,d]$ a simple linear layer interaction between the patches applied to all channels independently ç±»ä¼¼depth-wise conv with global kernelçš„ä¸œè¥¿ï¼Œçº¿æ€§ï¼ï¼ a two-layer-mlp fc-GELU-fc independently applied to all patches éçº¿æ€§ï¼ï¼ average pooled $[d-dim]$ + linear classifier $cls-dim$ Residual Multi-Layer Perceptron Layer a linear layer + a FFN layer each layer is paralleled with a skip-connection æ²¡ç”¨LNï¼Œä½†æ˜¯ç”¨äº†learnable affine transformation $Aff_{\alpha, \beta} (x) = Diag(\alpha) x + \beta$ rescale and shifts the input component-wiseï¼šå¯¹æ¯ä¸ªpatchï¼Œåˆ†åˆ«åšaffineå˜æ¢ åœ¨æ¨ç†é˜¶æ®µå¯ä»¥ä¸ä¸Šä¸€å±‚çº¿æ€§å±‚åˆå¹¶ï¼šno cost ç”¨äº†ä¸¤æ¬¡ ç¬¬ä¸€ä¸ªç”¨åœ¨main pathä¸Šç”¨æ¥æ›¿ä»£LNï¼šåˆå€¼ä¸ºidentity transform(1,0) ç¬¬äºŒä¸ªåœ¨residual pathé‡Œé¢ï¼Œdown scale to boostï¼Œç”¨ä¸€ä¸ªsmall valueåˆå§‹åŒ– given inputï¼š $d\times N^2$ matrix $X$ affineåœ¨d-dimä¸Šåš ç¬¬ä¸€ä¸ªLinear layeråœ¨$N^2-dim$ä¸Šåšï¼šå‚æ•°é‡$N^2 \times N^2$ ç¬¬äºŒã€ä¸‰ä¸ªLinear layeråœ¨$d-dim$ä¸Šåšï¼šå‚æ•°é‡$d \times 4d$ &amp; $4d \times d$]]></content>
      <tags>
        <tag>mlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[torch-note]]></title>
    <url>%2F2021%2F05%2F24%2Ftorch-note%2F</url>
    <content type="text"><![CDATA[torch.cuda.amp è‡ªåŠ¨æ··åˆç²¾åº¦ï¼šFloatTensor &amp; HalfTensor torch.jit.script å°†æ¨¡å‹ä»çº¯Pythonç¨‹åºè½¬æ¢ä¸ºèƒ½å¤Ÿç‹¬ç«‹äºPythonè¿è¡Œçš„TorchScriptç¨‹åº torch.nn.DataParallel torch.flatten(input, start_dim=0, end_dim=-1) å±•å¼€start_dimåˆ°end_dimä¹‹é—´çš„dimæˆä¸€ç»´]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LV-ViT]]></title>
    <url>%2F2021%2F05%2F21%2FLV-ViT%2F</url>
    <content type="text"><![CDATA[[LV-ViT 2021] Token Labeling: Training an 85.4% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNetï¼Œæ–°åŠ å¡å›½ç«‹&amp;å­—èŠ‚ï¼Œä¸»ä½“ç»“æ„è¿˜æ˜¯ViTï¼Œdeeper+narrower+multi-layer-cnn-patch-projection+auxiliary label&amp;loss åŒç­‰å‚æ•°é‡ä¸‹ï¼Œèƒ½å¤Ÿè¾¾åˆ°ä¸CNNç›¸å½“çš„åˆ†ç±»ç²¾åº¦ 26Mâ€”â€”84.4% ImageNet top1 acc 56Mâ€”â€”85.4% ImageNet top1 acc 150Mâ€”â€”86.2% ImageNet top1 acc ImageNet &amp; ImageNet-1kï¼šThe ImageNet dataset consists of more than 14M images, divided into approximately 22k different labels/classes. However the ImageNet challenge is conducted on just 1k high-level categories (probably because 22k is just too much) Token Labeling: Training an 85.4% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNet åŠ¨æœº develop a bag of training techniques on vision transformers slightly tune the structure introduce token labelingâ€”â€”a new training objective ImageNet classificaiton task è®ºç‚¹ former ViTs ä¸»è¦é—®é¢˜å°±æ˜¯éœ€è¦å¤§æ•°æ®é›†pretrainï¼Œä¸ç„¶ç²¾åº¦ä¸Šä¸å» ç„¶åæ¨¡å‹ä¹Ÿæ¯”è¾ƒå¤§ï¼Œneed huge computation resources DeiTå’ŒT2T-ViTæ¢ç´¢äº†data augmentation/å¼•å…¥additional tokenï¼Œèƒ½å¤Ÿåœ¨æœ‰é™çš„æ•°æ®é›†ä¸Šæ‹‰ç²¾åº¦ our work rely on purely ImageNet-1k data rethink the way of performing patch embedding introduce inductive bias we add a token labeling objective loss beside cls token predition provide practical advice on adjusting vision transformer structures æ–¹æ³• overview &amp; comparison ä¸»ä½“ç»“æ„ä¸å˜ï¼Œå°±æ˜¯å¢åŠ äº†ä¸¤é¡¹ a MixToken method a token labeling objective review the vision transformer patch embedding å°†å›ºå®šå°ºå¯¸çš„å›¾ç‰‡è½¬æ¢æˆpatch sequenceï¼Œä¾‹å¦‚224x224çš„å›¾ç‰‡ï¼Œpatch size=16ï¼Œé‚£å°±æ˜¯14x14ä¸ªsmall patches å°†æ¯ä¸ªpatch(16x16x3=768-dim) linear projectæˆä¸€ä¸ªtoken(embedding-dim) concat a class tokenï¼Œæ„æˆå…¨éƒ¨çš„input tokens position encoding added to input tokens fixed sinusoidal / learnable multi-head self-attention ç”¨æ¥å»ºç«‹long-range dependency multi-headsï¼šæ‰€æœ‰attention headsçš„è¾“å‡ºåœ¨channel-dimä¸Šconcatï¼Œç„¶ålinear projectå›å•ä¸ªheadçš„channel-dim feed-forward layers fc1-activation-fc2 score predition layer åªç”¨äº†cls tokenå¯¹åº”çš„è¾“å‡ºembeddingï¼Œå…¶ä»–çš„discard training techniques network depth add more transformer blocks åŒæ—¶decrease the hidden dim of FFN explicit inductive bias CNNé€æ­¥æ‰©å¤§æ„Ÿå—é‡ï¼Œæ“…é•¿æå–å±€éƒ¨ç‰¹å¾ï¼Œå…·æœ‰å¤©ç„¶çš„å¹³ç§»ä¸å˜æ€§ç­‰ transformerè¢«å‘ç°failed to capture the low-level and local structures we use convolutions with a smaller stride to provide an overlapped information for each nearby tokens åœ¨patch embeddingçš„æ—¶å€™ä¸æ˜¯independent cropï¼Œè€Œæ˜¯æœ‰overlap ç„¶åç”¨å¤šå±‚convï¼Œé€æ­¥æ‰©å¤§æ„Ÿå—é‡ï¼Œsmaller kernel sizeåŒæ—¶é™ä½äº†è®¡ç®—é‡ rethinking residual connection ç»™æ®‹å·®åˆ†æ”¯add a smaller ratio $\alpha$ enhance the residual connection since less information will go to the residual branch improve the generalization ability re-labeling label is not always accurate after cropping situations are worse on smaller images re-assign each image with a K-dim score mapï¼Œåœ¨1kç±»æ•°æ®é›†ä¸ŠK=1000 cheap operation compared to teacher-student è¿™ä¸ªlabelæ˜¯é’ˆå¯¹whole imageçš„labelï¼Œæ˜¯é€šè¿‡å¦ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹è·å– token-labeling based on the dense score map provided by re-labelingï¼Œwe can assign each patch an individual label auxiliary token labeling loss æ¯ä¸ªtokenéƒ½å¯¹åº”äº†ä¸€ä¸ªK-dim score map å¯ä»¥è®¡ç®—ä¸€ä¸ªce given outputs of the transformer $[X^{cls}, X^1, â€¦, X^N]$ K-dim score map $[y^1, y^2, â€¦, y^N]$ whole image label $y^{cls}$ loss auxiliary token labeling lossï¼š$L_{aux} = \frac{1}{N} \sum_1^N CE(X^i, y^i)$ cls lossï¼š$L_{cls} = CE(X^{cls}, y^{cls})$ total lossï¼š$L_{total} = L_{cls}+\beta L_{aux}$ï¼Œ$\beta=0.5$ MixToken ä»Mixup&amp;CutMixå¯å‘æ¥çš„ ä¸ºäº†ç¡®ä¿each token have clear contentï¼Œæˆ‘ä»¬åŸºäºtoken embeddingè¿›è¡Œmixup given token sequence $T_1=[t^1_1, t^2_1, â€¦, t^N_1]$ &amp; $T_2=[t^1_2, t^2_2, â€¦, t^N_2]$ token labels $y_1=[y^1_1, y^2_1, â€¦, y^N_1]$ &amp; $Y_2=[y^1_2, y^2_2, â€¦, y^N_2]$ binary mask M MixToken mixed token sequenceï¼š$\hat T = T_1 \odot M + T_2 \odot (1-M)$ mixed labelsï¼š$\hat Y = Y_1 \odot M + Y_2 \odot (1-M)$ mixed cls labelï¼š$\hat {Y^{cls}} = \overline M y_1^{cls} + (1-\overline M) y_2^{cls}$ï¼Œ$\overline M$ is the average of $M$ å®éªŒ training details AdamW linear lr scalingï¼šlarger when use token labeling weight decay dropoutï¼šhurts small modelsï¼Œuse Stochastic Depth instead Training Technique Analysis more convs in patch embedding enhanced residual smaller scaling factor the weight get larger gradients in residual branch more information can be preserved in main branch better performance faster convergence re-labeling use NFNet-F6 to re-label the ImageNet dataset and obtain the 1000-dimensional score map for each image NFNet-F6 is trained from scratch given input 576x576ï¼Œè·å¾—çš„score mapæ˜¯18x18x1000ï¼ˆs32ï¼‰ store the top5 probs for each position to save storage MixToken æ¯”baselineçš„CutMix methodè¦å¥½ åŒæ—¶çœ‹åˆ°token labelingæ¯”relabelingè¦å¥½ token labeling relabelingæ˜¯åœ¨whole imageä¸Š token labelingæ˜¯è¿›ä¸€æ­¥åœ°ï¼Œåœ¨token levelæ·»åŠ labelå’Œloss augmentation techniques å‘ç°MixUpä¼šhurt Model Scaling è¶Šå¤§è¶Šå¥½]]></content>
      <tags>
        <tag>visual transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[memory bank]]></title>
    <url>%2F2021%2F05%2F19%2Fmemory-bank%2F</url>
    <content type="text"><![CDATA[2018å¹´çš„paper official codeï¼šhttps://github.com/zhirongw/lemniscate.pytorch memory bank NCE Unsupervised Feature Learning via Non-Parametric Instance Discrimination åŠ¨æœº unsupervised learning can we learn good feature representation that captures apparent similarity among instances instead of classes formulate a non-parametric classification problem at instance-level use noise contrastive estimation our non-parametric model highly compactï¼š128-d feature per imageï¼Œonly 600MB storage in total enable fast nearest neighbour retrieval ã€QUESTIONã€‘æ— ç±»åˆ«æ ‡ç­¾ï¼Œå•é similarityï¼Œæœ€ç»ˆçš„åˆ†ç±»æ¨¡å‹æ˜¯å¦‚ä½•å»ºç«‹çš„ï¼Ÿ verified on ImageNet 1K classification semi-supervised learning object detection tasks è®ºç‚¹ observations ImageNet top-5 errè¿œæ¯”top-1 errå° second highest responding class is more likely to be visually related è¯´æ˜æ¨¡å‹éšå¼åœ°å­¦åˆ°äº†similarity apparent similarity is learned not from se- mantic annotations, but from the visual data themselves å°†class-wise supervisionæ¨åˆ°ä¸€ä¸ªæé™ å°±å˜æˆäº†instance-level ç±»åˆ«æ•°å˜æˆäº†the whole training setï¼šsoftmax to many more classes becomes infeasible approximate the full softmax distribution with noise-contrastive estimation(NCE) use a proximal regularization to stablize the learning process train &amp; test é€šå¸¸çš„åšæ³•æ˜¯learned representationsåŠ ä¸€ä¸ªçº¿æ€§åˆ†ç±»å™¨ e.g. SVMï¼šä½†æ˜¯trainå’Œtestçš„feature spaceæ˜¯ä¸ä¸€è‡´çš„ æˆ‘ä»¬ç”¨äº†KNNï¼šsame metric space æ–¹æ³• overview to learn a embedding function $f_{\theta}$ distance metric $d_{\theta}(x,y) = ||f_{\theta}(x)-f_{\theta}(y)||$ to map visually similar images closer instance-levelï¼što distinct between instances Non-Parametric Softmax Classifier common parametric classifier givenç½‘ç»œé¢„æµ‹çš„N-dim representation $v=f_{\theta}(x)$ è¦é¢„æµ‹C-classesçš„æ¦‚ç‡ï¼Œéœ€è¦ä¸€ä¸ª$W^{NC}$çš„projectionï¼š$P(i|v) = \frac{exp (W^T_iv)}{\sum exp (W^T_jv)}$ Non-Parametric version enforce $||v||=1$ via L2 norm replace $W^T$ with $v^T$ then the probabilityï¼š$P(i|v) = \frac{exp (v^T_iv/\tau)}{\sum exp (v^T_jv / \tau)}$ temperature param $\tau$ï¼šcontrols the concentration level of the distribution the goal is to minimize the negative log-likelihood æ„ä¹‰ï¼šL2 normå°†æ‰€æœ‰çš„representationæ˜ å°„åˆ°äº†ä¸€ä¸ª128-d unit sphereä¸Šé¢ï¼Œ$v_i^T v_j$åº¦é‡äº†ä¸¤ä¸ªprojection vecçš„similarityï¼Œæˆ‘ä»¬å¸Œæœ›åŒç±»çš„vecå°½å¯èƒ½é‡åˆï¼Œä¸åŒç±»çš„vecå°½å¯èƒ½æ­£äº¤ class weights $W$ are not generalized to new classes but feature representations $V$ does memory bank å› ä¸ºæ˜¯instance levelï¼ŒC-classeså¯¹åº”æ•´ä¸ªtraining setï¼Œä¹Ÿå°±æ˜¯è¯´${v_i}$ for all the images are needed for loss Let $V={v_i}$ è¡¨ç¤ºmemory bankï¼Œåˆå§‹ä¸ºunit random vectors every learning iterations $f_\theta$ is optimized by SGD è¾“å…¥$x_i$æ‰€å¯¹åº”çš„$f_i$æ›´æ–°åˆ°$v_i$ä¸Š ä¹Ÿå°±æ˜¯åªæœ‰mini-batchä¸­åŒ…å«çš„æ ·æœ¬ï¼Œåœ¨è¿™ä¸€ä¸ªstepï¼Œæ›´æ–°projection vec Noise-Contrastive Estimation non-parametric softmaxçš„è®¡ç®—é‡éšç€æ ·æœ¬é‡çº¿æ€§å¢é•¿ï¼Œmillions levelæ ·æœ¬é‡çš„æƒ…å†µä¸‹ï¼Œè®¡ç®—å¤ªheavyäº† we use NCE to approximate the full softmax assume noise samplesçš„uniform distributionï¼š$P_n =\frac{1}{n}$ noise samples are $m$ times frequent than data samples é‚£ä¹ˆsample $i$ matches vec $v$çš„åéªŒæ¦‚ç‡æ˜¯ï¼š$h(i,v)=\frac{P(i|v)}{P(i|v)+mP_n}$ approximated training object is to minimize the negative log-likelihood of $h(i,v)$ normalizing constant $Z$çš„è¿‘ä¼¼ ä¸»è¦å°±æ˜¯åˆ†æ¯è¿™ä¸ª$Z_i$çš„è®¡ç®—æ¯”è¾ƒheavyï¼Œæˆ‘ä»¬ç”¨Monte Carloé‡‡æ ·æ¥è¿‘ä¼¼ï¼š ${j_k}$ is a random subset of indicesï¼šéšæœºæŠ½äº†memory bankçš„ä¸€ä¸ªå­é›†æ¥approxå…¨é›†çš„åˆ†æ¯ï¼Œå®éªŒå‘ç°å–batch sizeå¤§å°çš„å­é›†å°±å¯ä»¥ï¼Œm=4096 Proximal Regularization the learning process oscillates a lot we have one instance per class during each training epoch each class is only visited once we introduce an additional term overall workflowï¼šåœ¨æ¯ä¸€ä¸ªiteration tï¼Œfeature representationæ˜¯$v_i^t=f_{\theta}(x_i)$ï¼Œè€Œmemory banké‡Œé¢çš„representationsæ¥è‡ªä¸Šä¸€ä¸ªiteration step $V={v^{t-1}}$ï¼Œæˆ‘ä»¬ä»memory banké‡Œé¢é‡‡æ ·ï¼Œå¹¶è®¡ç®—NCE lossï¼Œç„¶åbpæ›´æ–°ç½‘ç»œæƒé‡ï¼Œç„¶åå°†è¿™ä¸€è½®fpçš„representations updateåˆ°memory bankçš„æŒ‡å®šæ ·æœ¬ä¸Šï¼Œç„¶åä¸‹ä¸€è½® å¯ä»¥å‘ç°ï¼Œåœ¨åˆå§‹randomé˜¶æ®µï¼Œæ¢¯åº¦æ›´æ–°ä¼šæ¯”è¾ƒå¿«è€Œä¸”ä¸ç¨³å®š æˆ‘ä»¬ç»™positive sampleçš„lossä¸Šé¢å¤–åŠ äº†ä¸€ä¸ª$\lambda ||v_i^t-v_i^{t-1}||^2_2$ï¼Œæœ‰ç‚¹ç±»ä¼¼weight decayé‚£ç§ä¸œè¥¿ï¼Œå¼€å§‹é˜¶æ®µl2 lossä¼šå ä¸»å¯¼ï¼Œå¼•å¯¼ç½‘ç»œæ”¶æ•› stabilize speed up convergence improve the learned representations Weighted k-Nearest Neighbor Classifier a test timeï¼Œå…ˆè®¡ç®—feature representationï¼Œç„¶åè·Ÿmemory bankçš„vectorsåˆ†åˆ«è®¡ç®—cosine similarity $s_i=cos(v_i, f)$ï¼Œé€‰å‡ºtopk neighbours $N_k$ï¼Œç„¶åè¿›è¡Œweighted voting weighted votingï¼š å¯¹æ¯ä¸ªclass cï¼Œè®¡ç®—å®ƒåœ¨topk neighboursçš„total weightï¼Œ$w_c =\sum_{i \in N_k} \alpha_i 1(c_i=c)$ $\alpha_i = exp(s_i/\tau)$ k = 200 $\tau = 0.07$]]></content>
      <tags>
        <tag>Unsupervised Learning, NCE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MoCoç³»åˆ—]]></title>
    <url>%2F2021%2F04%2F30%2FMoCo%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[papersï¼š [2019 MoCo v1] Momentum Contrast for Unsupervised Visual Representation Learningï¼Œkaiming [2020 SimCLR] A Simple Framework for Contrastive Learning of Visual Representationsï¼ŒGoogle Brainï¼Œæ··è¿›æ¥æ˜¯å› ä¸ºå®ƒimprove based on MoCo v1ï¼Œè€ŒMoCo v2/v3åˆéƒ½æ˜¯åŸºäºå®ƒæ”¹è¿› [2020 MoCo v2] Improved Baselines with Momentum Contrastive Learningï¼Œkaiming [2021 MoCo v3] An Empirical Study of Training Self-Supervised Visual Transformersï¼Œkaiming preview: è‡ªç›‘ç£å­¦ä¹  Self-supervised Learning referenceï¼šhttps://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html overview å°±æ˜¯æ— ç›‘ç£ é’ˆå¯¹çš„ç—›ç‚¹ï¼ˆæœ‰ç›‘ç£è®­ç»ƒæ¨¡å‹ï¼‰ æ ‡æ³¨æˆæœ¬é«˜ è¿ç§»æ€§å·® ä¼šåŸºäºæ•°æ®ç‰¹ç‚¹ï¼Œè®¾ç½®Pretext tasksï¼ˆæœ€å¸¸è§çš„ä»»åŠ¡å°±æ˜¯ç”Ÿæˆ/é‡å»ºï¼‰ï¼Œæ„é€ Pesdeo Labelsæ¥è®­ç»ƒç½‘ç»œ é€šå¸¸æ¨¡å‹ç”¨æ¥ä½œä¸ºå…¶ä»–å­¦ä¹ ä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹ è¢«è®¤ä¸ºæ˜¯ç”¨æ¥å­¦ä¹ å›¾åƒçš„é€šç”¨è§†è§‰è¡¨ç¤º methods ä»ç»“æ„ä¸ŠåŒºåˆ†ä¸»è¦å°±æ˜¯ä¸¤å¤§ç±»æ–¹æ³• ç”Ÿæˆå¼ï¼šé€šè¿‡encoder-decoderç»“æ„è¿˜åŸè¾“å…¥ï¼Œç›‘ç£ä¿¡å·æ˜¯è¾“å…¥è¾“å‡ºå°½å¯èƒ½ç›¸ä¼¼ é‡å»ºä»»åŠ¡å¼€é”€å¤§ æ²¡æœ‰å»ºç«‹ç›´æ¥çš„è¯­ä¹‰å­¦ä¹  å¤–åŠ GANçš„åˆ¤åˆ«å™¨ä½¿å¾—ä»»åŠ¡æ›´åŠ å¤æ‚éš¾è®­ åˆ¤åˆ«å¼ï¼šè¾“å…¥ä¸¤å¼ å›¾ç‰‡ï¼Œé€šè¿‡encoderç¼–ç ï¼Œç›‘ç£ä¿¡å·æ˜¯åˆ¤æ–­ä¸¤å¼ å›¾æ˜¯å¦ç›¸ä¼¼ï¼Œåˆ¤åˆ«å¼æ¨¡å‹ä¹Ÿå«Contrastive Learning ä»Pretext tasksä¸Šåˆ’åˆ†ä¸»è¦åˆ†ä¸ºä¸‰ç±» åŸºäºä¸Šä¸‹æ–‡ï¼ˆContext basedï¼‰ ï¼šå¦‚bertçš„MLMï¼Œåœ¨å¥å­/å›¾ç‰‡ä¸­éšæœºæ‰£æ‰ä¸€éƒ¨åˆ†ï¼Œç„¶åæ¨åŠ¨æ¨¡å‹åŸºäºä¸Šä¸‹æ–‡/è¯­ä¹‰ä¿¡æ¯é¢„æµ‹è¿™éƒ¨åˆ†/ç›¸å¯¹ä½ç½®å…³ç³» åŸºäºæ—¶åºï¼ˆTemporal Basedï¼‰ï¼šå¦‚bertçš„NSPï¼Œè§†é¢‘/è¯­éŸ³ï¼Œåˆ©ç”¨ç›¸é‚»å¸§çš„ç›¸ä¼¼æ€§ï¼Œæ„å»ºä¸åŒæ’åºçš„åºåˆ—ï¼Œåˆ¤æ–­Bæ˜¯å¦æ˜¯Açš„ä¸‹ä¸€å¥/æ˜¯å¦ç›¸é‚»å¸§ åŸºäºå¯¹æ¯”ï¼ˆContrastive Basedï¼‰ï¼šæ¯”è¾ƒæ­£è´Ÿæ ·æœ¬ï¼Œæœ€å¤§åŒ–ç›¸ä¼¼åº¦çš„lossåœ¨è¿™é‡Œé¢è¢«å«åšInfoNCE memory-bank Contrastive Basedæ–¹æ³•æœ€å¸¸è§çš„æ–¹å¼æ˜¯åœ¨ä¸€ä¸ªbatchä¸­æ„å»ºæ­£è´Ÿæ ·æœ¬è¿›è¡Œå¯¹æ¯”å­¦ä¹  end-to-end æ¯ä¸ªmini-batchä¸­çš„å›¾åƒå¢å¼ºå‰åçš„ä¸¤å¼ å›¾ç‰‡äº’ä¸ºæ­£æ ·æœ¬ å­—å…¸å¤§å°å°±æ˜¯minibatchå¤§å° memory bankåŒ…å«æ•°æ®é›†ä¸­æ‰€æœ‰æ ·æœ¬ç¼–ç åç‰¹å¾ éšæœºé‡‡æ ·ä¸€éƒ¨åˆ†ä½œä¸ºkeys æ¯ä¸ªè¿­ä»£åªæ›´æ–°è¢«é‡‡æ ·çš„æ ·æœ¬ç¼–ç  å› ä¸ºæ ·æœ¬ç¼–ç æ¥è‡ªä¸åŒçš„training stepï¼Œä¸€è‡´æ€§å·® MoCo åŠ¨æ€ç¼–ç åº“ï¼šout-of-dateçš„ç¼–ç å‡ºåˆ— momentum updateï¼šä¸€è‡´æ€§æå‡ InfoNCE deep mindåœ¨CPC(Contrastive Predictive Coding)æå‡ºï¼Œè®ºæ–‡ä»¥åæœ‰æœºä¼šå†å±•å¼€ unsupervised encoderï¼šencode x into latent space representations zï¼Œresnet blocks autoregressive modelï¼šsummarize each time-step set of {z} into a context representation cï¼ŒGRUs probabilistic contrastive loss Noise-Contrastive Estimation Importance Sampling è®­ç»ƒç›®æ ‡æ˜¯è¾“å…¥æ•°æ®xå’Œcontext vector cä¹‹é—´çš„mutual information æ¯æ¬¡ä»$p(x_{t+k}|c_t)$ä¸­é‡‡æ ·ä¸€ä¸ªæ­£æ ·æœ¬ï¼šæ­£æ ·æœ¬æ˜¯è¿™ä¸ªåºåˆ—æ¥ä¸‹æ¥é¢„æµ‹çš„ä¸œè¥¿ï¼Œå’Œcçš„ç›¸ä¼¼æ€§è‚¯å®šè¦é«˜äºä¸æƒ³å¹²çš„token ä»$p(x_{t+k})$ä¸­é‡‡æ ·N-1ä¸ªè´Ÿæ ·æœ¬ï¼šè´Ÿæ ·æœ¬æ˜¯åˆ«çš„åºåˆ—é‡Œé¢éšæœºé‡‡æ ·çš„ä¸œè¥¿ ç›®æ ‡æ˜¯è®©æ­£æ ·æœ¬ä¸contextç›¸å…³æ€§é«˜ï¼Œè´Ÿæ ·æœ¬ä½ MoCo v1: Momentum Contrast for Unsupervised Visual Representation Learning åŠ¨æœº unsupervised visual representation learning contrastive learning dynamic dictionary large consisitent verified on 7 down-stream tasks ImageNet classification VOC &amp; COCO det/seg è®ºç‚¹ Unsupervised representation learning highly successful in NLPï¼Œin CV supervised is still the main-stream ä¸¤ä¸ªæ ¸å¿ƒ pretext tasks loss functions loss functions ç”Ÿæˆå¼æ–¹æ³•çš„lossæ˜¯åŸºäºpredictionå’Œä¸€ä¸ªfix targetæ¥è®¡ç®—çš„ contrastive-basedçš„key targetåˆ™æ˜¯vary on-the-fly during training Adversarial lossesæ²¡å±•å¼€ pretext tasks tasks involving recoverï¼šauto-encoder task involving pseudo-labelsï¼šé€šå¸¸æœ‰ä¸ªexemplar/anchorï¼Œç„¶åè®¡ç®—contrastive loss contrastive learning VS pretext tasks å¤§é‡pretext taskså¯ä»¥é€šè¿‡è®¾è®¡ä¸€äº›contrastive lossæ¥å®ç° recent approaches using contrastive loss dynamic dictionaries ç”±keysç»„æˆï¼šsampled from data &amp; represented by an encoder train the encoder to perform dictionary look-up given an encoded query similar to its matching key and dissimilar to others desirable dictionary largeï¼šbetter sample consistentï¼štraining target consistent MoCoï¼šMomentum Contrast queue æ¯ä¸ªit stepçš„mini-batchçš„ç¼–ç å…¥åº“ the oldest are dequeued EMAï¼š a slowly progressing key encoder momentum-based moving average of the query encoder similarçš„å®šä¹‰ï¼šq &amp; k are from the same image æ–¹æ³• contrastive learning a encoded query $q$ a set of encoded samples $\{k_0, k_1, â€¦\}$ assumeï¼šthere is a single key $k_+$ in the dictionary that $q$ matches similarity measurementï¼šdot product InfoNCEï¼š $L_q = -log \frac{exp(qk_+/\tau)}{\sum_0^K exp(qk/\tau)}$ 1 positive &amp; K negtive samples æœ¬è´¨ä¸Šæ˜¯ä¸ªsoftmax-based classifierï¼Œå°è¯•å°†$q$åˆ†ç±»æˆ$k_+$ unsupervised workflow with a encoder network $f_q$ &amp; $f_k$ thus we have query &amp; sample representation $q=f_q(x^q)$ &amp; $k=f_k(x^k)$ inputs $x$ can be images/patches/context(patches set) $f_q$ &amp; $f_k$ can be identical/partially shared/different momentum contrast dictionary as a key the dictionary always represents a sampled subset of all data the current mini-batchå…¥åˆ— the oldest mini-batchå‡ºåˆ— momentum update large dictionaryæ²¡æ³•å¯¹keysè¿›è¡Œback-propagationï¼šå› ä¸ºsampleå¤ªå¤šäº† only $f_q$ are updated by back-propagationï¼šmini-batch naive solutionï¼šcopy $f_q$çš„å‚æ•°ç»™$f_k$ï¼Œyields poor resultsï¼Œå› ä¸ºkey encoderå‚æ•°å˜åŒ–å¤ªé¢‘ç¹äº†ï¼Œrepresentation inconsistent issue momentum updateï¼š$f_k = mf_k + (1-m)f_q$ï¼Œ$m=0.999$ ä¸‰ç§æ›´æ–°æ–¹å¼å¯¹æ¯” ç¬¬ä¸€ç§end-to-end methodï¼š use samples in current mini-batch as the dictionary keys are consistently encoded dictionary size is limited ç¬¬äºŒç§memory bank A memory bank consists of the representations of all samples in the dataset the dictionary for each mini-batch is randomly sampled from the memory bankï¼Œä¸è¿›è¡Œbpï¼Œthus enables large dictionary key representation is updated when it was last seenï¼šinconsistent æœ‰äº›ä¹Ÿç”¨momentum updateï¼Œä½†æ˜¯æ˜¯ç”¨åœ¨representationä¸Šï¼Œè€Œä¸æ˜¯encoderå‚æ•° pretext task define positive pairï¼šif the query and the key come from the same image æˆ‘ä»¬ä»å›¾ä¸Štake two random views under random augmentation to form a positive pair ç„¶åç”¨å„è‡ªçš„encoderç¼–ç æˆq &amp; k æ¯ä¸€å¯¹è®¡ç®—similarityï¼špos similarity ç„¶åå†è®¡ç®—input querieså’Œdictionaryçš„similarityï¼šneg similarity è®¡ç®—ceï¼Œupdate $f_q$ ç”¨$f_q$ update $f_k$ æŠŠkåŠ å…¥dictionaryé˜Ÿåˆ— æŠŠæœ€æ—©çš„mini-batchå‡ºåˆ— æŠ€æœ¯ç»†èŠ‚ resnetï¼šlast fc dim=128ï¼ŒL2 norm temperature $\tau=0.07$ augmentation random resize + random(224,224) crop random color jittering random horizontal flip random grayscale conversion shuffling BN å®éªŒå‘ç°ä½¿ç”¨resneté‡Œé¢çš„BNä¼šå¯¼è‡´ä¸å¥½çš„ç»“æœï¼šçŒœæµ‹æ˜¯intra-batch communicationå¼•å¯¼æ¨¡å‹å­¦ä¹ äº†ä¸€ç§cheatingçš„low-loss solution å…·ä½“åšæ³•æ˜¯ç»™$f_k$çš„è¾“å…¥mini-batchå…ˆshuffle the orderï¼Œç„¶åè¿›è¡Œfpï¼Œç„¶åå†shuffle backï¼Œè¿™æ ·$f_q$å’Œ$f_k$çš„BNè®¡ç®—çš„mini-batchçš„staticså°±æ˜¯ä¸åŒçš„ å®éªŒ SimCLR: A Simple Framework for Contrastive Learning of Visual Representations åŠ¨æœº simplify recently proposed contrastive self-supervised learning algorithms systematically study the major components data augmentations learnable unlinear prediction head larger batch size and more training steps outperform previous self-supervised &amp; semi-supervised learning methods on ImageNet è®ºç‚¹ discriminative approaches based on contrastive learning maximizing agreement between differently augmented views of the same data sample via a contrastive loss in the latent space major components &amp; conclusions æ•°æ®å¢å¼ºå¾ˆé‡è¦ï¼Œunsupervisedæ¯”supervised benefits more å¼•å…¥çš„learnable nonlinear transformationæå‡äº†representation quality contrastive cross entropy losså—ç›Šäºnormalized embeddingå’Œadjusted temperature parameter larger batch size and more training stepså¾ˆé‡è¦ï¼Œunsupervisedæ¯”supervised benefits more æ–¹æ³• common framework 4 major components éšæœºæ•°æ®å¢å¼º results in two views of the same sampleï¼Œæ„æˆpositive pair crop + resize back + color distortions + gaussian blur base encoder ç”¨å•¥éƒ½è¡Œï¼Œæœ¬æ–‡ç”¨äº†resnet including the GAP a projection head å°†representation dimæ˜ å°„åˆ°the space where contrastive loss is appliedï¼ˆgiven 1 pos pair &amp; N neg pairï¼Œå°±æ˜¯N+1 dimï¼‰ ä¹‹å‰æœ‰æ–¹æ³•ç›´æ¥ç”¨linear projection æˆ‘ä»¬ç”¨äº†å¸¦ä¸€ä¸ªhidden layerçš„MLPï¼šfc-bn-relu-fc a contrastive loss overall workflow random sample a minibatch of N random augmentation results in 2N data points å¯¹æ¯ä¸ªæ ·æœ¬æ¥è®²ï¼Œæœ‰1ä¸ªpositive pairï¼Œå…¶ä½™2(N-1)ä¸ªdata pointséƒ½æ˜¯negative samples set cosine similarity $sim(u,v)=u^Tv/|u||v|$ given positive pair $(i,j)$ then the loss is $l_{i,j} = -log \frac{exp(s_{i,j}/\tau)}{\sum_{k\neq i}^{2N} exp(s_{i,k}/\tau)}$ å¯¹æ¯ä¸ªpositive pairéƒ½è®¡ç®—ï¼ŒåŒ…æ‹¬$(i,j)$ å’Œ$(j,i)$ï¼Œå«é‚£ä¸ªsymmetrized loss update encoder training with large batch size batch 8192ï¼Œnegatives 16382 å¤§batchæ—¶ï¼Œlinear learning rate scalingå¯èƒ½ä¸ç¨³å®šï¼Œæ‰€ä»¥ç”¨äº†LARS optmizer global BNï¼Œaggregate BN mean &amp; variance over all devices TPU MoCo v2: Improved Baselines with Momentum Contrastive Learning åŠ¨æœº still working on contrastive unsupervised learning simple modifications on MoCo introduce two effective SimCLRâ€™s designsï¼š an MLP head more data augmentation requires smaller batch size than SimCLRï¼Œmaking it possible to run on GPU verified on ImageNet classification VOC detection è®ºç‚¹ MoCo &amp; SimCLR contrastive unsupervised learning frameworks MoCo v1 shows promising SimCLR further reduce the gap we found two design imrpovements in SimCLR åœ¨ä¸¤ä¸ªæ–¹æ³•ä¸­éƒ½workï¼Œè€Œä¸”ç”¨åœ¨MoCoä¸­shows better transfer learning results an MLP projection head stronger data augmentation åŒæ—¶MoCo frameworkç›¸æ¯”è¾ƒäºSimCLR ï¼Œè¿œä¸éœ€è¦large training batches SimCLR based on end-to-end mechanismï¼Œéœ€è¦æ¯”è¾ƒå¤§çš„batch sizeï¼Œæ¥æä¾›è¶³å¤Ÿå¤šçš„negative pair MoCoåˆ™ç”¨äº†åŠ¨æ€é˜Ÿåˆ—ï¼Œæ‰€ä»¥ä¸é™åˆ¶batch size SimCLR improves the end-to-end method larger batchï¼što provide more negative samples output layerï¼šreplace fc with a MLP head stronger data augmentation MoCo a large number of negative samples are readily available æ‰€ä»¥å°±æŠŠåä¸¤é¡¹å¼•å…¥è¿›æ¥äº† æ–¹æ³• MLP head 2-layer MLP(hidden dim=2048, ReLU) ä»…å½±å“unsupervised trainingï¼Œæœ‰ç›‘ç£transfer learningçš„æ—¶å€™æ¢å¤´ temperature paramè°ƒæ•´ï¼šä»default 0.07 è°ƒæ•´æˆoptimal value 0.2 augmentation add blur SimCLRè¿˜ç”¨äº†stronger color distortionï¼šwe found stronger color distortion in SimCLR hurts in our MoCoï¼Œæ‰€ä»¥æ²¡åŠ  å®éªŒ ablation MLPï¼šåœ¨åˆ†ç±»ä»»åŠ¡ä¸Šçš„æå‡æ¯”æ£€æµ‹å¤§ augmentationï¼šåœ¨æ£€æµ‹ä¸Šçš„æå‡æ¯”åˆ†ç±»å¤§ comparison large batches are not necessary for good accï¼šSimCLR longer trainingé‚£ä¸ªç‰ˆæœ¬ç²¾åº¦æ›´é«˜ end-to-endçš„æ–¹æ³•è‚¯å®šmore costly in memory and timeï¼šå› ä¸ºè¦bpä¸¤ä¸ªencoder MoCo v3: An Empirical Study of Training Self-Supervised Visual Transformers åŠ¨æœº self-supervised frameworks that based on Siamese network, including MoCo ViTï¼šstudy the fundamental components for training self-supervised ViT MoCo v3ï¼šan incremental improvement of MoCo v1/2ï¼Œstriking for a better balance of simplicity &amp; accuracy &amp; scalability instability is a major issue scaling up ViT models ViT-Large ViT-Huge è®ºç‚¹ we go back to the basics and investigate the fundamental components of training deep neural networks batch size learning rate optmizer instability instability is a major issue that impacts self-supervised ViT training but may not result in catastrophic failureï¼Œåªä¼šå¯¼è‡´ç²¾åº¦æŸå¤± æ‰€ä»¥ç§°ä¹‹ä¸ºhidden degradation use a simple trick to improve stabilityï¼šfreeze the patch projection layer in ViT and observes increasement in acc NLPé‡Œé¢åŸºäºmasked auto-encodingçš„frameworkæ•ˆæœè¦æ¯”åŸºäºcontrastvieçš„frameworkå¥½ï¼Œå›¾åƒæ­£å¥½åè¿‡æ¥ æ–¹æ³• MoCo v3 take two crops for each image under random augmentation encoded by two encoders $f_q$ &amp; $f_k$ into vectors $q$ &amp; $k$ we use the keys that naturally co-exist in the same batch abandon the memory queueï¼šå› ä¸ºå‘ç°batch sizeè¶³å¤Ÿå¤§ï¼ˆ4096ï¼‰çš„æ—¶å€™ï¼Œmemory queueå°±æ²¡å•¥acc gainäº† å›å½’åˆ°batch-based sample pair ä½†æ˜¯encoder kä»æ—§ä¸å›ä¼ æ¢¯åº¦ï¼Œè¿˜æ˜¯åŸºäºencoder qè¿›è¡ŒåŠ¨é‡æ›´æ–° symmetrized lossï¼š $ctr(q_1, k_2) + ctr(q_2,k_1)$ InfoNCE temperature ä¸¤ä¸ªcropsåˆ†åˆ«è®¡ç®—ctr encoder encoder $f_q$ a backbone a projection head an extra prediction head encoder $f_k$ a backbone a projection head encoder $f_k$ is updated by the moving average of $f_q$ï¼Œexcluding the prediction head baseline acc basic settingsï¼Œä¸»è¦å˜åŠ¨å°±æ˜¯ä¸¤ä¸ªï¼š dynamic queueæ¢æˆlarge batch encoder $f_q$çš„extra prediction head use ViT ç›´æ¥ç”¨ViTæ›¿æ¢resnet back met instability issue batch size ViTé‡Œé¢çš„ä¸€ä¸ªè§‚ç‚¹å°±æ˜¯ï¼Œmodelæœ¬èº«æ¯”è¾ƒheavyï¼Œæ‰€ä»¥large batch is desirable å®éªŒå‘ç° a batch of 1k &amp; 2k produces reasonably smooth curvesï¼šIn this regime, the larger batch improves accuracy thanks to more negative samples a batch of 4k æœ‰æ˜æ˜¾çš„untable dipsï¼š a batch of 6k has worse failure patternsï¼šæˆ‘ä»¬è§£è¯»ä¸ºåœ¨è·³æ°´ç‚¹ï¼Œtraining is partially restarted and jumps out of the current local optimum learning rate lrè¾ƒå°ï¼Œtrainingæ¯”è¾ƒç¨³å®šï¼Œä½†æ˜¯å®¹æ˜“æ¬ æ‹Ÿåˆ lrè¿‡å¤§ï¼Œä¼šå¯¼è‡´unstableï¼Œä¹Ÿä¼šå½±å“acc æ€»ä½“æ¥è¯´ç²¾åº¦è¿˜æ˜¯å†³å®šäºstability optimizer default adamWï¼Œbatch size 4096 æœ‰äº›æ–¹æ³•ç”¨äº†LARS &amp; LAMB for large-batch training LAMB sensitive to lr optmal lr achieves slightly better accuracy than AdamW ä½†æ˜¯lrä¸€æ—¦è¿‡å¤§ï¼Œaccæé€Ÿdrop ä½†æ˜¯training curves still smoothï¼Œè™½ç„¶ä¸­é—´è¿‡ç¨‹æœ‰dropï¼šæˆ‘ä»¬è§£è¯»ä¸ºLAMB can avoid sudden change in the gradientsï¼Œä½†æ˜¯é¿å…ä¸äº†negative compactï¼Œè¿˜æ˜¯ä¼šç´¯åŠ  a trick for improving stability we found a spike in gradient causes a dip in the training curve we also observe that gradient spikes happen earlier in the first layer (patch projection) æ‰€ä»¥å°è¯•freezing the patch projection layer during trainingï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªrandomçš„patch projection layer This stability benefits the final accuracy The improvement is bigger for a larger lr åœ¨åˆ«çš„ViT-back-frameworkä¸Šä¹Ÿæœ‰æ•ˆï¼ˆSimCLRã€BYOLï¼‰ we also tried BNï¼ŒWNï¼Œgradient clip BN/WN does not improve gradient clipåœ¨thresholdè¶³å¤Ÿå°çš„æ—¶å€™æœ‰ç”¨ï¼Œæ¨åˆ°æé™å°±æ˜¯freezingäº† implementation details AdamW batch size 4096 lrï¼šwarmup 40 eps then cosine decay MLP heads projection headï¼š3-layersï¼Œ4096-BN-ReLU-4096-BN-ReLU-256 prediction headï¼š2-layersï¼Œ4096-BN-ReLU-256 loss ctré‡Œé¢æœ‰ä¸ªscaleçš„å‚æ•°ï¼Œ$2\tau$ makes it less sensitive to $\tau$ value $\tau=0.2$ ViT architecture è·ŸåŸè®ºæ–‡ä¿æŒä¸€è‡´ è¾“å…¥æ˜¯224x244çš„imageï¼Œåˆ’åˆ†æˆ16x16/14x14çš„patch sequenceï¼Œprojectæˆ256d/196dçš„embedding åŠ ä¸Šsine-cosine-2Dçš„PE å†concatä¸€ä¸ªcls token ç»è¿‡ä¸€ç³»åˆ—transformer blocks The class token after the last block (and after the final LayerNorm) is treated as the output of the backboneï¼Œand is the input to the MLP heads]]></content>
      <tags>
        <tag>self-supervised learning, transformer, contrastive loss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[optimizersä¼˜åŒ–å™¨]]></title>
    <url>%2F2021%2F03%2F15%2Foptimizers%E4%BC%98%E5%8C%96%E5%99%A8%2F</url>
    <content type="text"><![CDATA[0. overviewkeywordsï¼šSGD, moment, Nesterov, adaptive, ADAM, Weight decay ä¼˜åŒ–é—®é¢˜Optimization to minimizeç›®æ ‡å‡½æ•° grandient decent gradient numericalï¼šæ•°å€¼æ³•ï¼Œapproxï¼Œslow analyticalï¼šè§£ææ³•ï¼Œexactï¼Œfast Stochastic ç”¨minibatchçš„æ¢¯åº¦æ¥approximateå…¨é›† $\theta_{k+1} = \theta_k - v_{t+1}(x_i,y_i)$ classic optimizersï¼šSGDï¼ŒMomentumï¼ŒNesterovâ€˜s momentum adaptive optimizersï¼šAdaGradï¼ŒAdadeltaï¼ŒRMSPropï¼ŒAdam Newton modern optimizers for large-batch * AdamW LARS LAMB common updating steps for current step tï¼š step1ï¼šè®¡ç®—ç›´æ¥æ¢¯åº¦ï¼Œ$g_t = \nabla f(w_t)$ step2ï¼šè®¡ç®—ä¸€é˜¶åŠ¨é‡å’ŒäºŒé˜¶åŠ¨é‡ï¼Œ$m_t \&amp; V_t$ step3ï¼šè®¡ç®—å½“å‰æ—¶åˆ»çš„ä¸‹é™æ¢¯åº¦ï¼Œ$\eta_t = \alpha m_t/\sqrt {V_t}$ step4ï¼šå‚æ•°æ›´æ–°ï¼Œ$w_{t+1} = w_t - \eta_t$ å„ç§ä¼˜åŒ–ç®—æ³•çš„ä¸»è¦å·®åˆ«åœ¨step1å’Œstep2ä¸Š æ»‘åŠ¨å¹³å‡/æŒ‡æ•°åŠ æƒå¹³å‡/moving average/EMA å±€éƒ¨å‡å€¼ï¼Œä¸ä¸€æ®µæ—¶é—´å†…çš„å†å²ç›¸å…³ $v_t = \beta v_{t-1}+(1-\beta)\theta_t$ï¼Œå¤§è‡´ç­‰äºè¿‡å»$1/(1-\beta)$ä¸ªæ—¶åˆ»çš„$\theta$çš„å¹³å‡å€¼ï¼Œä½†æ˜¯åœ¨èµ·å§‹ç‚¹é™„è¿‘åå·®è¾ƒå¤§ $v_{tbiased} = \frac{v_t}{1-\beta^t}$ï¼Œåšäº†bias correction tè¶Šå¤§ï¼Œè¶Šä¸éœ€è¦ä¿®æ­£ï¼Œä¸¤ä¸ªæ»‘åŠ¨å‡å€¼çš„ç»“æœè¶Šæ¥è¿‘ ä¼˜ç¼ºç‚¹ï¼šä¸ç”¨ä¿å­˜å†å²ï¼Œä½†æ˜¯è¿‘ä¼¼ SGD SGDæ²¡æœ‰åŠ¨é‡çš„æ¦‚å¿µï¼Œ$m_t=g_t$ï¼Œ$V_t=I^2$ï¼Œ$w_{t+1} = w_t - \alpha g_t$ ä»…ä¾èµ–å½“å‰è®¡ç®—çš„æ¢¯åº¦ ç¼ºç‚¹ï¼šä¸‹é™é€Ÿåº¦æ…¢ï¼Œå¯èƒ½é™·åœ¨local optimaä¸ŠæŒç»­éœ‡è¡ SGDW (with weight decay) åœ¨æƒé‡æ›´æ–°çš„åŒæ—¶è¿›è¡Œæƒé‡è¡°å‡ $w_{t+1} = (1-\lambda)w_t - \alpha g_t$ åœ¨SGD formçš„ä¼˜åŒ–å™¨ä¸­weight decayç­‰ä»·äºåœ¨lossä¸ŠL2 regularization ä½†æ˜¯åœ¨adaptive formçš„ä¼˜åŒ–å™¨ä¸­æ˜¯ä¸ç­‰ä»·çš„ï¼ï¼å› ä¸ºhistorical funcï¼ˆERMï¼‰ä¸­regularizerå’Œgradientä¸€èµ·è¢«downscaleäº†ï¼Œå› æ­¤not as much as they would get regularized in SGDW SGD with Momentum å¼•å…¥ä¸€é˜¶åŠ¨é‡ï¼Œ$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$ï¼Œä½¿ç”¨æ»‘åŠ¨å‡å€¼ï¼ŒæŠ‘åˆ¶éœ‡è¡ æ¢¯åº¦ä¸‹é™çš„ä¸»è¦æ–¹å‘æ˜¯æ­¤å‰ç´¯ç§¯çš„ä¸‹é™æ–¹å‘ï¼Œç•¥å¾®å‘å½“å‰æ—¶åˆ»çš„æ–¹å‘è°ƒæ•´ SGD with Nesterov Acceleration look ahead SGD-momentum åœ¨local minimaçš„æ—¶å€™ï¼Œå››å‘¨æ²¡æœ‰ä¸‹é™çš„æ–¹å‘ï¼Œä½†æ˜¯å¦‚æœèµ°ä¸€æ­¥å†çœ‹ï¼Œå¯èƒ½å°±ä¼šæ‰¾åˆ°ä¼˜åŒ–æ–¹å‘ å…ˆè·Ÿç€ç´¯ç§¯åŠ¨é‡èµ°ä¸€æ­¥ï¼Œæ±‚æ¢¯åº¦ï¼š$g_t = \nabla f(w_t-\alpha m_{t-1}/\sqrt {V_{t-1}})$ ç”¨è¿™ä¸ªç‚¹çš„æ¢¯åº¦æ–¹å‘æ¥è®¡ç®—æ»‘åŠ¨å¹³å‡ï¼Œå¹¶æ›´æ–°æ¢¯åº¦ Adagrad å¼•å…¥äºŒé˜¶åŠ¨é‡ï¼Œå¼€å¯â€œè‡ªé€‚åº”å­¦ä¹ ç‡â€ï¼Œ$V_t = \sum_0^t g_k^2$ï¼Œåº¦é‡å†å²æ›´æ–°é¢‘ç‡ å¯¹äºç»å¸¸æ›´æ–°çš„å‚æ•°ï¼Œæˆ‘ä»¬å·²ç»ç§¯ç´¯äº†å¤§é‡å…³äºå®ƒçš„çŸ¥è¯†ï¼Œä¸å¸Œæœ›è¢«å•ä¸ªæ ·æœ¬å½±å“å¤ªå¤§ï¼Œå¸Œæœ›å­¦ä¹ é€Ÿç‡æ…¢ä¸€äº›ï¼›å¯¹äºå¶å°”æ›´æ–°çš„å‚æ•°ï¼Œæˆ‘ä»¬äº†è§£çš„ä¿¡æ¯å¤ªå°‘ï¼Œå¸Œæœ›èƒ½ä»æ¯ä¸ªå¶ç„¶å‡ºç°çš„æ ·æœ¬èº«ä¸Šå¤šå­¦ä¸€äº›ï¼Œå³å­¦ä¹ é€Ÿç‡å¤§ä¸€äº› $\eta_t = \alpha m_t / \sqrt{V_t}$ï¼Œæœ¬è´¨ä¸Šä¸ºæ¯ä¸ªå‚æ•°ï¼Œå¯¹å­¦ä¹ ç‡åˆ†åˆ«rescale ç¼ºç‚¹ï¼šäºŒé˜¶åŠ¨é‡å•è°ƒé€’å¢ï¼Œå¯¼è‡´å­¦ä¹ ç‡å•è°ƒè¡°å‡ï¼Œå¯èƒ½ä¼šä½¿å¾—è®­ç»ƒè¿‡ç¨‹æå‰ç»“æŸ AdaDelta/RMSProp å‚è€ƒmomentumï¼Œå¯¹äºŒé˜¶åŠ¨é‡ä¹Ÿè®¡ç®—æ»‘åŠ¨å¹³å‡ï¼Œ$V_t = \beta_2 V_{t-1} + (1-\beta_2)g_t^2$ é¿å…äº†äºŒé˜¶åŠ¨é‡æŒç»­ç´¯ç§¯ã€å¯¼è‡´è®­ç»ƒè¿‡ç¨‹æå‰ç»“æŸ Adam é›†å¤§æˆè€…ï¼šæŠŠä¸€é˜¶åŠ¨é‡å’ŒäºŒé˜¶åŠ¨é‡éƒ½ç”¨èµ·æ¥ï¼ŒAdaptive Momentum SGD-Måœ¨SGDåŸºç¡€ä¸Šå¢åŠ äº†ä¸€é˜¶åŠ¨é‡ AdaGradå’ŒAdaDeltaåœ¨SGDåŸºç¡€ä¸Šå¢åŠ äº†äºŒé˜¶åŠ¨é‡ ä¸€é˜¶åŠ¨é‡æ»‘åŠ¨å¹³å‡ï¼š$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$ äºŒé˜¶åŠ¨é‡æ»‘åŠ¨å¹³å‡ï¼š$V_t = \beta_2 V_{t-1} + (1-\beta_2)g_t^2$ Nadam look ahead Adam æŠŠNesterovçš„one step tryåŠ ä¸Šï¼š$g_t = \nabla f(w_t-\alpha m_{t-1}/\sqrt {V_{t-1}})$ å†Adamæ›´æ–°ä¸¤ä¸ªåŠ¨é‡ ç»éªŒè¶…å‚ $momentum=0.9$ $\beta_1=0.9$ $\beta_2=0.999$ $m_0 = 0$ $V_0 = 0$ ä¸Šé¢çš„å›¾ä¸Šå¯ä»¥çœ‹å‡ºï¼ŒåˆæœŸçš„$m_t$å’Œ$V_t$ä¼šæ— é™æ¥è¿‘äº0ï¼Œæ­¤æ—¶å¯ä»¥è¿›è¡Œè¯¯å·®ä¿®æ­£ï¼š$factor=\frac{1}{1-\beta^t}$ AdamW åœ¨adaptive methodsä¸­ï¼Œè§£è€¦weight-decayå’Œloss-based gradientåœ¨ERMè¿‡ç¨‹ä¸­çš„ç»‘å®šdownscaleçš„å…³ç³» å®è´¨å°±æ˜¯å°†å¯¼æ•°é¡¹åç§»]]></content>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[regnet]]></title>
    <url>%2F2021%2F03%2F11%2Fregnet%2F</url>
    <content type="text"><![CDATA[RegNet: Designing Network Design Spaces åŠ¨æœº study the network design principles design RegNet outperforms efficientNet and 5x faster top1 errorï¼š20.1 ï¼ˆeff-b5ï¼š21.5ï¼‰ larger batch size 1/4 çš„ train/test latency è®ºç‚¹ manual network design AlexNet, ResNet family, DenseNet, MobileNet focus on discovering new design choices that improve acc the recent popular approach NAS search the best in a fixed search space of possible networks limitationsï¼šgeneralize to new settingsï¼Œlack of interpretability network scaling ä¸Šé¢ä¸¤ä¸ªfocus on æ‰¾å‡ºä¸€ä¸ªbasenet for a specific regime scaling rules aims at tuning the optimal network in any target regime comparing networks the reliable comparison metric to guide the design process our method combines the disadvantages of manual design and NAS first AnyNet then RegNet æ–¹æ³•]]></content>
  </entry>
  <entry>
    <title><![CDATA[mongodb]]></title>
    <url>%2F2021%2F03%2F09%2Fmongodb%2F</url>
    <content type="text"><![CDATA[downloadï¼šhttps://www.mongodb.com/try/download/enterprise install 123456789101112# å°†è§£å‹ä»¥åçš„æ–‡ä»¶å¤¹æ”¾åœ¨/usr/localä¸‹sudo mv mongodb-osx-x86_64-4.0.9/ /usr/local/sudo ln -s mongodb-macos-x86_64-4.4.4 mongodb# ENV PATHexport PATH=/usr/local/mongodb/bin:$PATH# åˆ›å»ºæ—¥å¿—åŠæ•°æ®å­˜æ”¾çš„ç›®å½•sudo mkdir -p /usr/local/var/mongodbsudo mkdir -p /usr/local/var/log/mongodbsudo chown [amber] /usr/local/var/mongodbsudo chown [amber] /usr/local/var/log/mongodb configuration 12345678# åå°å¯åŠ¨mongod --dbpath /usr/local/var/mongodb --logpath /usr/local/var/log/mongodb/mongo.log --fork# æ§åˆ¶å°å¯åŠ¨mongod --config /usr/local/etc/mongod.conf# æŸ¥çœ‹çŠ¶æ€ps aux | grep -v grep | grep mongod run 123# åœ¨dbç¯å¢ƒä¸‹å¯åŠ¨ä¸€ä¸ªç»ˆç«¯cd /usr/local/mongodb/bin ./mongo original settings 123456789101112131415161718192021# æ˜¾ç¤ºæ‰€æœ‰æ•°æ®çš„åˆ—è¡¨&gt; show dbsadmin 0.000GBconfig 0.000GBlocal 0.000GB# ä¸‰ä¸ªç³»ç»Ÿä¿ç•™çš„ç‰¹æ®Šæ•°æ®åº“# è¿æ¥/åˆ›å»ºä¸€ä¸ªæŒ‡å®šçš„æ•°æ®åº“&gt; use localswitched to db local# æ˜¾ç¤ºå½“å‰æ•°æ®åº“, å¦‚æœæ²¡useé»˜è®¤ä¸ºtest&gt; dbtest# ã€ï¼ï¼é‡è¦ã€‘å…³é—­æœåŠ¡ä¹‹å‰æœåŠ¡å™¨è¢«kill -9å¼ºåˆ¶å…³é—­ï¼Œæ•°æ®åº“ä¸¢å¤±äº†&gt; use adminswitched to db admin&gt; db.shutdownServer()server should be down... concepts æ–‡æ¡£document ä¸€ç»„key-valueå¯¹ï¼Œå¦‚ä¸Šé¢å·¦å›¾ä¸­çš„ä¸€è¡Œè®°å½•ï¼Œå¦‚ä¸Šé¢å³å›¾ä¸­çš„ä¸€ä¸ªdict é›†åˆcollection ä¸€å¼ è¡¨ï¼Œå¦‚ä¸Šé¢å·¦å›¾å’Œä¸Šé¢å³å›¾ ä¸»é”®primary key å”¯ä¸€ä¸»é”®ï¼ŒObjectIdç±»å‹ï¼Œè‡ªå®šç”Ÿæˆï¼Œæœ‰æ ‡å‡†æ ¼å¼ å¸¸ç”¨å‘½ä»¤ 10.1 åˆ›å»º/åˆ é™¤/é‡å‘½ådb 12345678910111213141516171819202122232425262728# åˆ‡æ¢è‡³æ•°æ®åº“test1&gt; use test1# æ’å…¥ä¸€æ¡doc, db.COLLECTION_NAME.insert(document)# dbè¦åŒ…å«è‡³å°‘ä¸€æ¡æ–‡æ¡£ï¼Œæ‰èƒ½åœ¨show dbsçš„æ—¶å€™æ˜¾ç¤ºï¼ˆæ‰çœŸæ­£åˆ›å»ºï¼‰&gt; db.sheet1.insert(&#123;'name': img0&#125;)# æ˜¾ç¤ºå½“å‰å·²æœ‰æ•°æ®åº“&gt; show dbs# åˆ é™¤æŒ‡å®šæ•°æ®åº“&gt; use test1&gt; db.dropDatabase()# æ—§ç‰ˆæœ¬(before4.0)é‡å‘½åï¼šå…ˆæ‹·è´ä¸€ä»½ï¼Œåœ¨åˆ é™¤æ—§çš„&gt; db.copyDatabase('OLDNAME', 'NEWNAME');&gt; use old_name&gt; db.dropDatabase()# æ–°ç‰ˆæœ¬é‡å‘½åï¼šdump&amp;restoreï¼Œè¿™ä¸ªä¸œè¥¿åœ¨mongodb toolsé‡Œé¢ï¼Œè¦å¦å¤–ä¸‹è½½ï¼Œå¯æ‰§è¡Œæ–‡ä»¶æ”¾åœ¨binä¸‹# mongodump # å°†æ‰€æœ‰æ•°æ®åº“å¯¼å‡ºåˆ°bin/dump/ä»¥æ¯ä¸ªdbåå­—å‘½åçš„æ–‡ä»¶å¤¹ä¸‹# mongodump -h dbhost -d dbname -o dbdirectory# -h: æœåŠ¡å™¨åœ°å€:ç«¯å£å·# -d: éœ€è¦å¤‡ä»½çš„æ•°æ®åº“# -o: å­˜æ”¾ä½ç½®ï¼ˆéœ€è¦å·²å­˜åœ¨ï¼‰mongodump -d test -o tmp/# åœ¨æ¢å¤å¤‡ä»½æ•°æ®åº“çš„æ—¶å€™æ¢ä¸ªåå­—ï¼šmongorestore -h dbhost -d dbname pathmongorestore -d test_bkp tmp/test# è¿™æ—¶å€™å¯ä»¥çœ‹åˆ°ä¸€ä¸ªæ–°å¢äº†ä¸€ä¸ªå«test_bkpçš„db 10.2 åˆ›å»º/åˆ é™¤/é‡å‘½åcollection 12345678910111213141516# åˆ›å»ºï¼šdb.createCollection(name, options)&gt; db.createCollection('case2img')# æ˜¾ç¤ºå·²æœ‰tables&gt; show collections# ä¸ç”¨æ˜¾ç¤ºåˆ›å»ºï¼Œåœ¨db insertçš„æ—¶å€™ä¼šè‡ªåŠ¨åˆ›å»ºé›†åˆ&gt; db.sheet2.insert(&#123;"name" : "img2"&#125;)# åˆ é™¤ï¼šdb.COLLECTION_NAME.drop()&gt; db.sheet2.drop()# é‡å‘½åï¼šdb.COLLECTION_NAME.renameCollection('NEWNAME')&gt; db.sheet2.renameCollection('sheet3')# å¤åˆ¶ï¼šdb.COLLECTION_NAME.aggregate(&#123;$out: 'NEWNAME'&#125;)&gt; db.sheet2.aggregate(&#123; $out : "sheet3" &#125;) 10.3 æ’å…¥/æ˜¾ç¤º/æ›´æ–°/åˆ é™¤document 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# æ’å…¥db.COLLECTION_NAME.insert(document)db.COLLECTION_NAME.save(document)db.COLLECTION_NAME.insertOne()db.COLLECTION_NAME.insertMany()# æ˜¾ç¤ºå·²æœ‰docdb.COLLECTION_NAME.find()# æ›´æ–°docçš„éƒ¨åˆ†å†…å®¹db.COLLECTION_NAME.update( &lt;query&gt;, # æŸ¥è¯¢æ¡ä»¶ &lt;update&gt;, # æ›´æ–°æ“ä½œ &#123; upsert: &lt;boolean&gt;, # if true å¦‚æœä¸å­˜åœ¨åˆ™æ’å…¥ multi: &lt;boolean&gt;, # find fist/all match writeConcern: &lt;document&gt; &#125;)&gt; db.case2img.insert(&#123;"case": "s0", "name": "img0"&#125;)&gt; db.case2img.insert(&#123;"case": "s1", "name": "img1"&#125;)&gt; db.case2img.find()&gt; db.case2img.update(&#123;'case': 's1'&#125;, &#123;$set: &#123;'case': 's2', 'name': 'img2'&#125;&#125;)&gt; db.case2img.find()# ç»™docçš„æŸä¸ªkeyé‡å‘½ådb.COLLECTION_NAME.updateMany(&#123;&#125;,&#123;'$rename': &#123;"old_key": "new_key"&#125;&#125;)# æ›´æ–°æ•´æ¡æ–‡æ¡£by object_iddb.COLLECTION_NAME.save( &lt;document&gt;, &#123; writeConcern: &lt;document&gt; &#125;)&gt; db.case2img.save(&#123;"_id": ObjectId("60474e4b77e21bad9bd4655a"), "case":"s3", "name":"img3"&#125;)# åˆ é™¤æ»¡è¶³æ¡ä»¶çš„docdb.COLLECTION_NAME.remove( &lt;query&gt;, &#123; justOne: &lt;boolean&gt;, # find fist/all match writeConcern: &lt;document&gt; &#125;)&gt; db.case2img.remove(&#123;"case": "s0"&#125;)# åˆ é™¤æ‰€æœ‰doc&gt; db.case2img.remove(&#123;&#125;) 10.4 ç®€å•æŸ¥è¯¢find 1234567891011121314151617&gt; db.case2img.insert(&#123;"case": "s0", "name": "img0"&#125;)&gt; db.case2img.insert(&#123;"case": "s1", "name": "img1"&#125;)&gt; db.case2img.insert(&#123;"case": "s2", "name": "img2"&#125;)&gt; db.case2img.insert(&#123;"case": "s2", "name": "img3"&#125;)# æŸ¥è¯¢è¡¨ä¸­çš„docï¼šdb.COLLECTION_NAME.find(&#123;query&#125;)&gt; db.case2img.find(&#123;'case': s2&#125;)&gt; db.case2img.find(&#123;'case': 's1'&#125;, &#123;"name":1&#125;) # projectionçš„valueåœ¨å¯¹åº”çš„key-valueæ˜¯listçš„æ—¶å€™æœ‰æ„ä¹‰# æ ¼å¼åŒ–æ˜¾ç¤ºæŸ¥è¯¢ç»“æœï¼šdb.COLLECTION_NAME.find(&#123;query&#125;).pretty()&gt; db.case2img.find(&#123;'case': s2&#125;).pretty()# è¯»å–æŒ‡å®šæ•°é‡çš„æ•°æ®è®°å½•ï¼šdb.COLLECTION_NAME.find(&#123;query&#125;).limit(NUMBER)&gt; db.case2img.find(&#123;'case': &#123;$type: 'string'&#125;&#125;).limit(1)# è·³è¿‡æŒ‡å®šæ•°é‡çš„æ•°æ®ï¼šdb.COLLECTION_NAME.find(&#123;query&#125;).skip(NUMBER)&gt; db.case2img.find(&#123;'case': &#123;$type: 'string'&#125;&#125;).skip(1) 10.5 æ¡ä»¶æ“ä½œç¬¦ 123456789101112131415(&gt;) å¤§äº - $gt(&lt;) å°äº - $lt(&gt;=) å¤§äºç­‰äº - $gte(&lt;=) å°äºç­‰äº - $lte(or) æˆ– - $or&gt; db.case2img.update(&#123;'case':'s1'&#125;, &#123;$set: &#123;"name":'img1', 'size':100&#125;&#125;)WriteResult(&#123; "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 &#125;)&gt; db.case2img.update(&#123;'case':'s2'&#125;, &#123;$set: &#123;"name":'img2', 'size':200&#125;&#125;)# æŸ¥è¯¢size&gt;150çš„doc&gt; db.case2img.find(&#123;'size': &#123;$gt: 150&#125;&#125;)# æŸ¥è¯¢æ»¡è¶³ä»»æ„ä¸€ä¸ªæ¡ä»¶çš„doc&gt; db.case2img.find(&#123;'$or': [&#123;'case':'s1'&#125;, &#123;'size': &#123;$gt: 150&#125;&#125;]&#125;) 10.6 æ•°æ®ç±»å‹æ“ä½œç¬¦ 12345type(KEY)ç­‰äº - $type# æ¯”è¾ƒå¯¹è±¡å¯ä»¥æ˜¯å­—ç¬¦ä¸²/å¯¹åº”çš„reflect NUM&gt; db.case2img.find(&#123;'case': &#123;$type: 'string'&#125;&#125;)&gt; db.case2img.find(&#123;'case': &#123;$type: '0'&#125;&#125;) 10.7 æ’åºfind().sort 1234# é€šè¿‡æŒ‡å®šå­—æ®µ&amp;æŒ‡å®šå‡åº/é™åºæ¥å¯¹æ•°æ®æ’åºï¼šdb.COLLECTION_NAME.find().sort(&#123;KEY:1/-1&#125;)&gt; db.case2img.find().sort(&#123;'name':1&#125;)# skip(), limilt(), sort()ä¸‰ä¸ªæ”¾åœ¨ä¸€èµ·æ‰§è¡Œçš„æ—¶å€™ï¼Œæ‰§è¡Œçš„é¡ºåºæ˜¯å…ˆ sort(), ç„¶åæ˜¯ skip()ï¼Œæœ€åæ˜¯æ˜¾ç¤ºçš„ limit()ã€‚ 10.8 ç´¢å¼• skip 10.9 èšåˆaggregate 123456789101112131415161718# ç”¨äºç»Ÿè®¡ db.COLLECTION_NAME.aggregate(AGGREGATE_OPERATION)# by group&gt; db.case2img.aggregate([&#123;$group: &#123;_id: '$case', img_num:&#123;$sum:1&#125;&#125;&#125;])group by key value 'case'count number of items in each grouprefer to the number as img_num&gt; db.case2img.aggregate([&#123;$group: &#123;_id: '$case', img_num:&#123;$sum:'$size'&#125;&#125;&#125;])è®¡ç®—æ¯ä¸€ä¸ªgroupå†…ï¼Œsizeå€¼çš„æ€»å’Œ# by match&gt; db.case2img.aggregate([&#123;$match: &#123;'size': &#123;$gt:150&#125;&#125;&#125;, &#123;$group:&#123;_id: null, totalsize: &#123;$sum: '$size'&#125;&#125;&#125;])ç±»ä¼¼shellçš„ç®¡é“ï¼Œmatchç”¨æ¥ç­›é€‰æ¡ä»¶ï¼Œç¬¦åˆæ¡ä»¶çš„é€å…¥ä¸‹ä¸€æ­¥ç»Ÿè®¡&gt; db.case2img.aggregate([&#123;$skip: 4&#125;, &#123;$group:&#123;_id: null, totalsize: &#123;$sum: '$size'&#125;&#125;&#125;]) å¿«é€Ÿç»Ÿè®¡distinct 12db.case2img.distinct(TAG_NAME)# æ³¨æ„å¦‚æœdistinctçš„å†…å®¹å¤ªé•¿ï¼Œè¶…è¿‡16Mï¼Œä¼šæŠ¥distinct too bigçš„errorï¼Œæ¨èç”¨èšåˆæ¥åšç»Ÿè®¡ 123456789101112 12. pymongo ç”¨pythonä»£ç æ¥æ“ä½œæ•°æ®åº“ å…ˆå®‰è£…ï¼špip install pymongo 11.1 è¿æ¥client ```python from pymongo import MongoClient Client = MongoClient() 11.2 è·å–æ•°æ®åº“ 12db = Client.DB_NAMEdb = Client['DB_NAME'] 11.3 è·å–collection 12collection = db.COLLECTION_NAMEcollection = db['COLLECTION_NAME'] 11.4 æ’å…¥doc 123456789101112# insert onedocument1 = &#123;'x':1&#125;document2 = &#123;'x':2&#125;post_1 = collection.insert_one(document1).inserted_idpost_2 = collection.insert_one(document2).inserted_idprint(post_1)# insert manynew_document = [&#123;'x':1&#125;,&#123;'x':2&#125;]# new_document = [document1,document2] æ³¨æ„docæ˜¯ç¥æ‹·è´ï¼Œåªèƒ½ä½œä¸ºä¸€æ¡docè¢«æ’å…¥ä¸€æ¬¡result = collection.insert_many(new_document).inserted_idsprint(result) 11.5 æŸ¥æ‰¾ 12345678910from bson.objectid import ObjectId# find one è¿”å›ä¸€æ¡docresult = collection.find_one()result = collection.find_one(&#123;'case': 's0'&#125;)result = collection.find_one(&#123;'_id': ObjectId('604752f277e21bad9bd46560')&#125;)# find è¿”å›ä¸€ä¸ªè¿­ä»£å™¨for _, item in enumerate(collection.find()): print(item) 11.6 æ›´æ–° 123456# update onecollection.update_one(&#123;'case':'s1'&#125;,&#123;'$set':&#123;'size':300&#125;&#125;)collection.update_one(&#123;'case':'s1'&#125;,&#123;'$push':&#123;'add':1&#125;&#125;) # è¿½åŠ æ•°ç»„å†…å®¹# update manycollection.update_many(&#123;'case':'s1'&#125;,&#123;'$set':&#123;'size':300&#125;&#125;) 11.7 åˆ é™¤ 123# åœ¨mongo shellé‡Œé¢æ˜¯removeæ–¹æ³•ï¼Œåœ¨pymongoé‡Œé¢è¢«deprecatedæˆdeleteæ–¹æ³•collection.delete_one(&#123;"case": "s2"&#125;)collection.delete_many(&#123;"case": "s1"&#125;) 11.8 ç»Ÿè®¡ 12345# è®¡æ•°ï¼šcountæ–¹æ³•å·²ç»è¢«é‡æ„print(collection.count_documents(&#123;'case':'s0'&#125;))# uniqueï¼šdistinctæ–¹æ³•print(collection.distinct('case')) â€‹ 11.9 æ­£åˆ™ â€‹ mongo shellå‘½ä»¤è¡Œé‡Œçš„æ­£åˆ™å’Œpymongoè„šæœ¬é‡Œçš„æ­£åˆ™å†™æ³•æ˜¯ä¸ä¸€æ ·çš„ï¼Œå› ä¸ºpythoné‡Œé¢æœ‰å°è£…æ­£åˆ™æ–¹æ³•ï¼Œç„¶åé€šè¿‡bsonå°†pythonçš„æ­£åˆ™è½¬æ¢æˆæ•°æ®åº“çš„æ­£åˆ™ 12345678910# pymongoimport reimport bsonpattern = re.compile(r'(.*)-0[345]-(.*)')regex = bson.regex.Regex.from_native(pattern)result = collection.aggregate([&#123;'$match': &#123;'date': regex&#125;&#125;])# mongo shell&gt; db.collection.find(&#123;date:&#123;$regex:"(.*)-0[345]-(.*)"&#125;&#125;)]]></content>
      <tags>
        <tag>æ•°æ®åº“ï¼ŒNoSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker]]></title>
    <url>%2F2021%2F03%2F04%2Fdocker%2F</url>
    <content type="text"><![CDATA[shartup éƒ¨ç½²æ–¹æ¡ˆ å¤æ—©å¹´ä»£ è™šæ‹Ÿæœº docker imageé•œåƒ &amp; containerå®¹å™¨ &amp; registryä»“åº“ é•œåƒï¼šç›¸å½“äºæ˜¯ä¸€ä¸ª root æ–‡ä»¶ç³»ç»Ÿï¼Œæä¾›å®¹å™¨è¿è¡Œæ—¶æ‰€éœ€çš„ç¨‹åºã€åº“ã€èµ„æºã€é…ç½®ç­‰ å®¹å™¨ï¼šé•œåƒè¿è¡Œæ—¶çš„å®ä½“ï¼Œå¯ä»¥è¢«åˆ›å»ºã€å¯åŠ¨ã€åœæ­¢ã€åˆ é™¤ã€æš‚åœç­‰ ä»“åº“ï¼šç”¨æ¥ä¿å­˜é•œåƒ å®˜æ–¹ä»“åº“ï¼šdocker hubï¼šhttps://hub.docker.com/r/floydhub/tensorflow/tags?page=1&amp;ordering=last_updated å¸¸ç”¨å‘½ä»¤ æ‹‰é•œåƒ docker pull [é€‰é¡¹] [Docker Registry åœ°å€[:ç«¯å£å·]/]ä»“åº“å[:æ ‡ç­¾] åœ°å€å¯ä»¥æ˜¯å®˜æ–¹åœ°å€ï¼Œä¹Ÿå¯ä»¥æ˜¯ç¬¬ä¸‰æ–¹ï¼ˆå¦‚Harborï¼‰ ä»“åº“åç”±ä½œè€…åå’Œè½¯ä»¶åç»„æˆï¼ˆå¦‚zhangruiming/skinï¼‰ æ ‡ç­¾ç”¨æ¥æŒ‡å®šæŸä¸ªç‰ˆæœ¬çš„imageï¼Œçœç•¥åˆ™é»˜è®¤latest åˆ—å‡ºæ‰€æœ‰é•œåƒ docker images åˆ é™¤é•œåƒ docker rmi [-f] [é•œåƒid] åˆ é™¤é•œåƒä¹‹å‰è¦kill/rmæ‰€æœ‰ä½¿ç”¨è¯¥é•œåƒçš„containerï¼šdocker rm [å®¹å™¨id] è¿è¡Œé•œåƒå¹¶åˆ›å»ºä¸€ä¸ªå®¹å™¨ docker run [-it] [ä»“åº“å] [å‘½ä»¤] é€‰é¡¹ -itï¼šä¸ºå®¹å™¨é…ç½®ä¸€ä¸ªäº¤äº’ç»ˆç«¯ é€‰é¡¹ -dï¼šåå°è¿è¡Œå®¹å™¨ï¼Œå¹¶è¿”å›å®¹å™¨IDï¼ˆä¸ç›´æ¥è¿›å…¥ç»ˆç«¯ï¼‰ é€‰é¡¹ â€”name=â€™xxxâ€™ï¼šä¸ºå®¹å™¨æŒ‡å®šä¸€ä¸ªåç§° é€‰é¡¹-v /host_dir:/container_dirï¼šå°†ä¸»æœºä¸ŠæŒ‡å®šç›®å½•æ˜ å°„åˆ°å®¹å™¨çš„æŒ‡å®šç›®å½• [å‘½ä»¤]å‚æ•°å¿…é¡»è¦åŠ ï¼Œè€Œä¸”è¦æ˜¯é‚£ç§ä¸€ç›´æŒ‚èµ·çš„å‘½ä»¤ï¼ˆ/bin/bashï¼‰ï¼Œå¦‚æœæ˜¯ls/cd/ç›´æ¥ä¸å¡«ï¼Œé‚£ä¹ˆå‘½ä»¤è¿è¡Œå®Œå®¹å™¨å°±ä¼šåœæ­¢è¿è¡Œï¼Œdocker ps -aæŸ¥çœ‹çŠ¶æ€ï¼Œå‘ç°éƒ½æ˜¯Exited åˆ›å»ºå®¹å™¨ docker run æŸ¥çœ‹æ‰€æœ‰å®¹å™¨ docker ps å¯åŠ¨ä¸€ä¸ªå·²ç»åœæ­¢çš„å®¹å™¨/åœæ­¢æ­£åœ¨è¿è¡Œçš„å®¹å™¨ docker start [å®¹å™¨id] docker stop [å®¹å™¨id] è¿›å…¥å®¹å™¨ docker exec -it [å®¹å™¨id] [linuxå‘½ä»¤] åˆ é™¤å®¹å™¨ docker rm [å®¹å™¨id] åˆ é™¤æ‰€æœ‰ä¸æ´»è·ƒçš„å®¹å™¨ docker container prune æäº¤é•œåƒåˆ°è¿œç«¯ä»“åº“ docker tag [é•œåƒid] [ç”¨æˆ·å]/[ä»“åº“]:[æ ‡ç­¾] # é‡å‘½å docker login # ç™»é™†ç”¨æˆ· docker push æ¡ˆä¾‹ 123456789101112131415161718192021222324252627282930313233# æ‹‰é•œåƒdocker pull åœ°å€/ä»“åº“:æ ‡ç­¾# æ˜¾ç¤ºé•œåƒdocker images# è¿è¡ŒæŒ‡å®šé•œåƒdocker run -itd --name='test' åœ°å€/ä»“åº“:æ ‡ç­¾# æŸ¥çœ‹è¿è¡Œçš„å®¹å™¨docker ps# è¿›å…¥å®¹å™¨docker exec -it å®¹å™¨idæˆ–name /bin/bash# ä¸€é¡¿æ“ä½œå®Œé€€å‡ºå®¹å™¨exit# å°†ä¿®æ”¹åçš„å®¹å™¨ä¿å­˜ä¸ºé•œåƒdocker commit å®¹å™¨idæˆ–name æ–°é•œåƒåå­—docker imageså¯ä»¥çœ‹åˆ°è¿™ä¸ªé•œåƒäº†# ä¿å­˜é•œåƒåˆ°æœ¬åœ°docker save -o tf_torch.rar tf_torch# è¿˜åŸé•œåƒdocker load --input tf_torch.tar# é‡å‘½åé•œåƒdocker tag 3db0b2f40a70 amberzzzz/tf1.14_torch1.4_cuda10.0:v1# æäº¤é•œåƒdocker push amberzzzz/tf1.14-torch0.5-cuda10.0:v1 dockerfile Dockerfile æ˜¯ç”¨æ¥è¯´æ˜å¦‚ä½•è‡ªåŠ¨æ„å»º docker image çš„æŒ‡ä»¤é›†æ–‡ä»¶ å¸¸ç”¨å‘½ä»¤ FROM image_nameï¼ŒæŒ‡å®šä¾èµ–çš„é•œåƒ RUN commandï¼Œåœ¨ shell æˆ–è€… exec çš„ç¯å¢ƒä¸‹æ‰§è¡Œçš„å‘½ä»¤ ADD srcfile_path_inhost dstfile_incontainerï¼Œå°†æœ¬æœºæ–‡ä»¶å¤åˆ¶åˆ°å®¹å™¨ä¸­ CMD [â€œexecutableâ€,â€param1â€,â€param2â€]ï¼ŒæŒ‡å®šå®¹å™¨å¯åŠ¨é»˜è®¤æ‰§è¡Œçš„å‘½ä»¤ WORKDIR path_incontainerï¼ŒæŒ‡å®š RUNã€CMD ä¸ ENTRYPOINT å‘½ä»¤çš„å·¥ä½œç›®å½• VOLUME [â€œ/dataâ€]ï¼Œæˆæƒè®¿é—®ä»å®¹å™¨å†…åˆ°ä¸»æœºä¸Šçš„ç›®å½• basic image ä»nvidia dockerå¼€å§‹ï¼šhttps://hub.docker.com/r/nvidia/cuda/tags?page=1&amp;ordering=last_updated&amp;name=10. é€‰ä¸€ä¸ªå–œæ¬¢çš„ï¼šå¦‚docker pull nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04 ç„¶åç¼–è¾‘dockerfile 1234567891011121314151617181920FROM nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04MAINTAINER amber &lt;amber.zhang@tum.de&gt;# install basic dependenciesRUN apt-get update RUN apt-get install -y wget vim cmake# install Anaconda3RUN wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.2.0-Linux-x86_64.sh -O ~/anaconda3.shRUN bash ~/anaconda3.sh -b -p /home/anaconda3 &amp;&amp; rm ~/anaconda3.sh ENV PATH /home/anaconda3/bin:$PATH# RUN echo "export PATH=/home/anaconda3/bin:$PATH" &gt;&gt; ~/.bashrc &amp;&amp; /bin/bash -c "source /root/.bashrc" # change mirrorRUN mkdir ~/.pip \ &amp;&amp; cd ~/.pip RUN echo '[global]\nindex-url = https://pypi.tuna.tsinghua.edu.cn/simple/' &gt;&gt; ~/.pip/pip.conf# install tensorflowRUN /home/anaconda3/bin/pip install tensorflow-gpu==1.8.0 ç„¶åbuild dockerfile 1docker build -t &lt;docker_name&gt; .]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[layer norm]]></title>
    <url>%2F2021%2F03%2F02%2Flayer-norm%2F</url>
    <content type="text"><![CDATA[ç»¼è¿° papers [batch norm 2015] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shiftï¼ŒinceptionV2ï¼ŒGoogle Teamï¼Œå½’ä¸€åŒ–å±‚çš„å§‹ç¥–ï¼ŒåŠ é€Ÿè®­ç»ƒ&amp;æ­£åˆ™ï¼ŒBNè¢«åè¾ˆè¿½ç€æ‰“çš„ä¸»è¦ç—›ç‚¹ï¼šapproximation by mini-batchï¼Œtest phase frozen [layer norm 2016] Layer Normalizationï¼ŒToronto+Googleï¼Œé’ˆå¯¹BNä¸é€‚ç”¨small batchå’ŒRNNçš„é—®é¢˜ï¼Œä¸»è¦ç”¨äºRNNï¼Œåœ¨CNNä¸Šä¸å¥½ï¼Œåœ¨testçš„æ—¶å€™ä¹Ÿæ˜¯activeçš„ï¼Œå› ä¸ºmean&amp;varianceç”±äºå½“å‰æ•°æ®å†³å®šï¼Œæœ‰è´Ÿè´£rescaleå’Œreshiftçš„layer params [weight norm 2016] Weight normalization: A simple reparameterization to accelerate training of deep neural networksï¼ŒOpenAIï¼Œ [cosine norm 2017] Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networksï¼Œä¸­ç§‘é™¢ï¼Œ [instance norm 2017] Instance Normalization: The Missing Ingredient for Fast Stylizationï¼Œé«˜æ ¡reportï¼Œé’ˆå¯¹é£æ ¼è¿ç§»ï¼ŒINåœ¨testçš„æ—¶å€™ä¹Ÿæ˜¯activeçš„ï¼Œè€Œä¸æ˜¯freezeçš„ï¼Œå•çº¯çš„instance-independent normï¼Œæ²¡æœ‰layer params [group norm 2018] Group Normalizationï¼ŒFAIR Kaimingï¼Œé’ˆå¯¹BNåœ¨small batchä¸Šæ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºbatch-independentçš„ [weight standardization 2019] Weight Standardizationï¼ŒJohns Hopkinsï¼Œ [batch-channel normalization &amp; weight standardization 2020] BCN&amp;WS: Micro-Batch Training with Batch-Channel Normalization and Weight Standardizationï¼ŒJohns Hopkinsï¼Œ why Normalization ç‹¬ç«‹åŒåˆ†å¸ƒï¼šindependent and identically distribute ç™½åŒ–ï¼šwhiteningï¼ˆ[PCA whitening][http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/]ï¼‰ å»é™¤ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ ä½¿æ‰€æœ‰ç‰¹å¾å…·æœ‰ç›¸åŒçš„å‡å€¼å’Œæ–¹å·® æ ·æœ¬åˆ†å¸ƒå˜åŒ–ï¼šInternal Covariate Shift å¯¹äºç¥ç»ç½‘ç»œçš„å„å±‚è¾“å…¥ï¼Œç”±äºstacking internel byproductï¼Œæ¯å±‚çš„åˆ†å¸ƒæ˜¾ç„¶å„ä¸ç›¸åŒï¼Œä½†æ˜¯å¯¹äºæŸä¸ªç‰¹å®šçš„æ ·æœ¬è¾“å…¥ï¼Œä»–ä»¬æ‰€æŒ‡ç¤ºçš„labelæ˜¯ä¸å˜çš„ å³æºç©ºé—´å’Œç›®æ ‡ç©ºé—´çš„æ¡ä»¶æ¦‚ç‡æ˜¯ä¸€è‡´çš„ï¼Œä½†æ˜¯è¾¹ç¼˜æ¦‚ç‡æ˜¯ä¸åŒçš„ P_s(Y|X=x) = P_t(Y|X=x) \\ P_s(X) \neq P_t(X) æ¯ä¸ªç¥ç»å…ƒçš„æ•°æ®ä¸å†æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒï¼Œç½‘ç»œéœ€è¦ä¸æ–­é€‚åº”æ–°çš„åˆ†å¸ƒï¼Œä¸Šå±‚ç¥ç»å…ƒå®¹æ˜“é¥±å’Œï¼šç½‘ç»œè®­ç»ƒåˆæ…¢åˆä¸ç¨³å®š how to Normalization preparation unitï¼šä¸€ä¸ªç¥ç»å…ƒï¼ˆä¸€ä¸ªopï¼‰ï¼Œè¾“å…¥[b,N,C_in]ï¼Œè¾“å‡º[b,N,1] layerï¼šä¸€å±‚çš„ç¥ç»å…ƒï¼ˆä¸€ç³»åˆ—opï¼Œ$W\in R^{M*N}$ï¼‰ï¼Œåœ¨channel-dimä¸Šconcatå½“å‰å±‚æ‰€æœ‰unitçš„è¾“å‡º[b,N,C_out] dims bï¼šbatch dimension Nï¼šspatial dimensionï¼Œ1/2/3-dims Cï¼šchannel dimension unified representationï¼šæœ¬è´¨ä¸Šéƒ½æ˜¯å¯¹æ•°æ®åœ¨è§„èŒƒåŒ– $h = f(g*\frac{x-\mu}{\sigma}+b)$ï¼šå…ˆå½’ä¸€åŒ–ï¼Œåœ¨rescale &amp; reshift $\mu$ &amp; $\sigma$ï¼šcompute fromä¸Šä¸€å±‚çš„ç‰¹å¾å€¼ $g$ &amp; $b$ï¼šlearnable paramsåŸºäºå½“å‰å±‚ $f$ï¼šneuronsâ€™ weighting operation å„æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºmean &amp; varianceçš„è®¡ç®—ç»´åº¦ å¯¹æ•°æ® BNï¼šä»¥ä¸€å±‚æ¯ä¸ªç¥ç»å…ƒçš„è¾“å‡ºä¸ºå•ä½ï¼Œå³æ¯ä¸ªchannelçš„mean&amp;varç›¸äº’ç‹¬ç«‹ LNï¼šä»¥ä¸€å±‚æ‰€æœ‰ç¥ç»å…ƒçš„è¾“å‡ºä¸ºå•ä½ï¼Œå³æ¯ä¸ªsampleçš„mean&amp;varç›¸äº’ç‹¬ç«‹ INï¼šä»¥æ¯ä¸ªsampleåœ¨æ¯ä¸ªç¥ç»å…ƒçš„è¾“å‡ºä¸ºå•ä½ï¼Œæ¯ä¸ªsampleåœ¨æ¯ä¸ªchannelçš„mean&amp;varéƒ½ç›¸äº’ç‹¬ç«‹ GNï¼šä»¥æ¯ä¸ªsampleåœ¨ä¸€ç»„ç¥ç»å…ƒçš„è¾“å‡ºä¸ºå•ä½ï¼Œä¸€ç»„åŒ…å«ä¸€ä¸ªç¥ç»å…ƒçš„æ—¶å€™å˜æˆINï¼Œä¸€ç»„åŒ…å«ä¸€å±‚æ‰€æœ‰ç¥ç»å…ƒçš„æ—¶å€™å°±æ˜¯LN ç¤ºæ„å›¾ï¼š å¯¹æƒé‡ WNï¼šå°†æƒé‡åˆ†è§£ä¸ºå•ä½å‘é‡å’Œä¸€ä¸ªå›ºå®šæ ‡é‡ï¼Œç›¸å½“äºç¥ç»å…ƒçš„ä»»æ„è¾“å…¥vecç‚¹ä¹˜äº†ä¸€ä¸ªå•ä½vecï¼ˆdownscaleï¼‰ï¼Œå†rescaleï¼Œè¿›ä¸€æ­¥åœ°ç›¸å½“äºæ²¡æœ‰åšshiftå’Œreshiftçš„æ•°æ®normalization WSï¼šå¯¹æƒé‡åšå…¨å¥—ï¼ˆå½’ä¸€åŒ–å†recaleï¼‰ï¼Œæ¯”WNå¤šäº†shiftï¼Œâ€œzero-center is the keyâ€ å¯¹op CosNï¼š å°†çº¿æ€§å˜æ¢opæ›¿æ¢æˆcos opï¼š$f_w(x) = cos = \frac{w \cdot x}{|w||x|}$ æ•°å­¦æœ¬è´¨ä¸Šåˆé€€åŒ–æˆäº†åªæœ‰downscaleçš„å˜æ¢ï¼Œè¡¨å¾èƒ½åŠ›ä¸è¶³ Whiteningç™½åŒ– purpose imagesçš„adjacent pixel values are highly correlatedï¼Œthus redundant linearly move the origin distributionï¼Œmaking the inputs share the same mean &amp; variance method é¦–å…ˆè¿›è¡ŒPCAé¢„å¤„ç†ï¼Œå»æ‰correlation mean on sampleï¼ˆæ³¨æ„ä¸æ˜¯mean on imageï¼‰ \overline x = \frac{1}{N}\sum_{i=1}^N x_i\\ x^{'} = x - \overline x åæ–¹å·®çŸ©é˜µ X \in R^{d*N}\\ S = \frac{1}{N}XX^T å¥‡å¼‚å€¼åˆ†è§£svd(S) S = U \Sigma V^T $\Sigma$ä¸ºå¯¹è§’çŸ©é˜µï¼Œå¯¹è§’ä¸Šçš„å…ƒç´ ä¸ºå¥‡å¼‚å€¼ $U=[u_1,u_2,â€¦u_N]$ä¸­æ˜¯å¥‡å¼‚å€¼å¯¹åº”çš„æ­£äº¤å‘é‡ æŠ•å½±å˜æ¢ X^{'} = U_p^T X å–æŠ•å½±çŸ©é˜µ$U_p$ from $U$ï¼Œ$U_p \in R^{N*d}$è¡¨ç¤ºå°†æ•°æ®ç©ºé—´ä»Nç»´æŠ•å½±åˆ°$U_p$æ‰€åœ¨çš„dç»´ç©ºé—´ä¸Š recoverï¼ˆæŠ•å½±é€†å˜æ¢ï¼‰ X^{''} = U_p^T X^{'} * å–æŠ•å½±çŸ©é˜µ$U_r=U_p^T$ï¼Œå°±æ˜¯å°† æ•°æ®ç©ºé—´ä»dç»´ç©ºé—´å†æŠ•å½±å›Nç»´ç©ºé—´ä¸Š * PCAç™½åŒ–ï¼š * å¯¹PCAæŠ•å½±åçš„æ–°åæ ‡ï¼Œåšå½’ä¸€åŒ–å¤„ç†ï¼šåŸºäºç‰¹å¾å€¼è¿›è¡Œç¼©æ”¾ $$ X_{PCAwhite} = \Sigma^{-\frac{1}{2}}X^{&#39;} = \Sigma^{-\frac{1}{2}}U^TX $$ * $X_{PCAwhite}$çš„åæ–¹å·®çŸ©é˜µ$S_{PCAwhite} = I$ï¼Œå› æ­¤æ˜¯å»äº†correlationçš„ * ZCAç™½åŒ–ï¼šåœ¨ä¸Šä¸€æ­¥åšå®Œä¹‹åï¼Œå†æŠŠå®ƒå˜æ¢åˆ°åŸå§‹ç©ºé—´ï¼Œæ‰€ä»¥ZCAç™½åŒ–åçš„ç‰¹å¾å›¾æ›´æ¥è¿‘åŸå§‹æ•°æ® * å¯¹PCAç™½åŒ–åçš„æ•°æ®ï¼Œå†åšä¸€æ­¥recover $$ X_{ZCAwhite} = U X_{PCAwhite} $$ * åæ–¹å·®çŸ©é˜µä»æ—§æ˜¯Iï¼Œåˆæ³•ç™½åŒ– Layer Normalization åŠ¨æœº BN reduces training time compute by each neuron require moving average depend on mini-batch size how to apply to recurrent neural nets propose layer norm [unlike BN] compute by each layer [like BN] with adaptive bias &amp; gain [unlike BN] perform the same computation at training &amp; test time [unlike BN] straightforward to apply to recurrent nets work well for RNNs è®ºç‚¹ BN reduce training time &amp; serves as regularizer require moving averageï¼šintroduce dependencies between training cases the approxmation of mean &amp; variance expectations constraints on the size of a mini-batch intuition norm layeræå‡è®­ç»ƒé€Ÿåº¦çš„æ ¸å¿ƒæ˜¯é™åˆ¶ç¥ç»å…ƒè¾“å…¥è¾“å‡ºçš„å˜åŒ–å¹…åº¦ï¼Œç¨³å®šæ¢¯åº¦ åªè¦æ§åˆ¶æ•°æ®åˆ†å¸ƒï¼Œå°±èƒ½ä¿æŒè®­ç»ƒé€Ÿåº¦ æ–¹æ³• compute over all hidden units in the same layer different training cases have different normalization terms æ²¡å•¥å¥½è¯´çš„ï¼Œå°±æ˜¯åœ¨channelç»´åº¦è®¡ç®—norm furtherçš„GNæŠŠchannelç»´åº¦åˆ†ç»„åšnormï¼ŒINåœ¨ç›´æ¥æ¯ä¸ªç‰¹å¾è®¡ç®—norm gain &amp; bias ä¹Ÿæ˜¯åœ¨å¯¹åº”ç»´åº¦ï¼š(hwd)c-dim https://tobiaslee.top/2019/11/21/understanding-layernorm/ åç»­æœ‰å®éªŒå‘ç°ï¼Œå»æ‰ä¸¤ä¸ªlearnable rescale paramsåè€Œæç‚¹ è€ƒè™‘æ˜¯åœ¨training setä¸Šçš„è¿‡æ‹Ÿåˆ å®éªŒ RNNä¸Šæœ‰ç”¨ CNNä¸Šæ¯”æ²¡æœ‰norm layerå¥½ï¼Œä½†æ˜¯æ²¡æœ‰BNå¥½ï¼šå› ä¸ºchannelæ˜¯ç‰¹å¾ç»´åº¦ï¼Œç‰¹å¾ç»´åº¦ä¹‹é—´æœ‰æ˜æ˜¾çš„æœ‰ç”¨/æ²¡ç”¨ï¼Œä¸èƒ½ç®€å•çš„norm Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks åŠ¨æœº reparameterizing the weights decouple length &amp; direction no dependency between samples which suits well for recurrent reinforcement generative no additional memory and computation testified on MLP with CIFAR generative model VAE &amp; DRAW reinforcement DQN è®ºç‚¹ a neuronï¼š get inputs from former layers(neurons) weighted sum over the inputs add a bias elementwise nonlinear transformation batch outputsï¼šone value per sample intuition of normalizationï¼š give gradients that are more like whitened natural gradients BNï¼šmake the outputs of each neuronæœä»std norm our WNï¼š inspired by BN does not share BNâ€™s across-sample property no addition memory and tiny addition computation Instance Normalization: The Missing Ingredient for Fast Stylization åŠ¨æœº stylizationï¼šé’ˆå¯¹é£æ ¼è¿ç§»ç½‘ç»œ with a small changeï¼šswapping BN with IN achieve qualitative improvement è®ºç‚¹ stylized image a content image + a style image both style and content statistics are obtained from a pretrained CNN for image classification methods optimization-basedï¼šiterative thus computationally inefficient generator-basedï¼šsingle pass but never as good as our work revisit the feed-forward method replace BN in the generator with IN keep them at test time as opposed to freeze æ–¹æ³• formulation given a fixed stype image $x_0$ given a set of content images $x_t, t= 1,2,â€¦,n$ given a pre-trained CNN with a variable z controlling the generation of stylization results compute the stylied image g($x_t$, z) compare the statisticsï¼š$min_g \frac{1}{n} \sum^n_{t=1} L(x_0, x_t, g(x_t, z))$ comparing targetï¼šthe contrast of the stylized image is similar to the constrast of the style image observations the more training examples, the poorer the qualitive results the result of stylization still depent on the constrast of the content image intuition é£æ ¼è¿ç§»æœ¬è´¨ä¸Šå°±æ˜¯å°†style imageçš„contrastç”¨åœ¨content imageçš„ï¼šä¹Ÿå°±æ˜¯rescale content imageçš„contrast constrastæ˜¯per sampleçš„ï¼š$\frac{pixel}{\sum pixels\ on\ the\ map}$ BNåœ¨normçš„æ—¶å€™å°†batch samplesæ…åˆåœ¨äº†ä¸€èµ· IN instance-specfic normalization also known as contrast normalization å°±æ˜¯per imageåšæ ‡å‡†åŒ–ï¼Œæ²¡æœ‰trainable/frozen paramsï¼Œåœ¨test phaseä¹Ÿä¸€æ ·ç”¨ Group Normalization åŠ¨æœº for small batch size do normalization in channel groups batch-independent behaves stably over different batch sizes approach BNâ€™s accuracy è®ºç‚¹ BN requires sufficiently large batch size (e.g. 32) Mask R-CNN frameworks use a batch size of 1 or 2 images because of higher resolution, where BN is â€œfrozenâ€ by transforming to a linear layer synchronized BN ã€BR LN &amp; IN effective for training sequential models or generative models but have limited success in visual recognition GNèƒ½è½¬æ¢æˆLNï¼IN WN normalize the filter weights, instead of operating on features æ–¹æ³• group it is not necessary to think of deep neural network features as unstructured vectors ç¬¬ä¸€å±‚å·ç§¯æ ¸é€šå¸¸å­˜åœ¨ä¸€ç»„å¯¹ç§°çš„filterï¼Œè¿™æ ·å°±èƒ½æ•è·åˆ°ç›¸ä¼¼ç‰¹å¾ è¿™äº›ç‰¹å¾å¯¹åº”çš„channel can be normalized together normalization transform the feature xï¼š$\hat x_i = \frac{1}{\sigma}(x_i-\mu_i)$ the mean and the standard deviationï¼š \mu_i=\frac{1}{m}\sum_{k\in S_i}x_k\\ \sigma_i=\sqrt {\frac{1}{m}\sum_{k\in S_i}(x_k-\mu_i)^2+\epsilon} the set $S_i$ BNï¼š $S_i=\{k|k_C = i_C\}$ pixels sharing the same channel index are normalized together for each channel, BN computes Î¼ and Ïƒ along the (N, H, W) axes LN $S_i=\{k|k_N = i_N\}$ pixels sharing the same batch index (per sample) are normalized together LN computes Î¼ and Ïƒ along the (C,H,W) axes for each sample IN $S_i=\{k|k_N = i_N, k_C=i_C\}$ pixels sharing the same batch index and the same channel index are normalized together LN computes Î¼ and Ïƒ along the (H,W) axes for each sample GN $S_i=\{k|k_N = i_N, [\frac{k_C}{C/G}]=[\frac{i_C}{C/G}]\}$ computes Î¼ and Ïƒ along the (H, W ) axes and along a group of C/G channels linear transform to keep representational ability per channel scale and shiftï¼š$y_i = \gamma \hat x_i + \beta$ relation to LN LN assumes all channels in a layer make â€œsimilar contributionsâ€ which is less valid with the presence of convolutions GN improved representational power over LN to IN IN can only rely on the spatial dimension for computing the mean and variance it misses the opportunity of exploiting the channel dependence ã€QUESTIONã€‘BNä¹Ÿæ²¡è€ƒè™‘é€šé“é—´çš„è”ç³»å•Šï¼Œä½†æ˜¯è®¡ç®—meanå’Œvarianceæ—¶è·¨äº†sample implementation reshape learnable $\gamma \&amp; \beta$ computable mean &amp; var å®éªŒ GNç›¸æ¯”äºBNï¼Œtraining erroræ›´ä½ï¼Œä½†æ˜¯val errorç•¥é«˜äºBN GN is effective for easing optimization loses some regularization ability it is possible that GN combined with a suitable regularizer will improve results é€‰å–ä¸åŒçš„groupæ•°ï¼Œæ‰€æœ‰çš„group&gt;1å‡å¥½äºgroup=1ï¼ˆLNï¼‰ é€‰å–ä¸åŒçš„channelæ•°ï¼ˆCï¼Gï¼‰ï¼Œæ‰€æœ‰çš„channel&gt;1å‡å¥½äºchannel=1ï¼ˆINï¼‰ Object Detection frozenï¼šå› ä¸ºhigher resolutionï¼Œbatch sizeé€šå¸¸è®¾ç½®ä¸º2/GPUï¼Œè¿™æ—¶çš„BN frozenæˆä¸€ä¸ªçº¿æ€§å±‚$y=\gamma(x-\mu)/\sigma+beta$ï¼Œå…¶ä¸­çš„$\mu$å’Œ$sigma$æ˜¯loadäº†pre-trained modelä¸­ä¿å­˜çš„å€¼ï¼Œå¹¶ä¸”frozenæ‰ï¼Œä¸å†æ›´æ–° denote as BN* replace BN* with GN during fine-tuning use a weight decay of 0 for the Î³ and Î² parameters WS: Weight Standardization åŠ¨æœº accelerate training micro-batchï¼š ä»¥BN with large-batchä¸ºåŸºå‡† ç›®å‰BN with micro-batchåŠå…¶ä»–normalization methodséƒ½ä¸èƒ½matchè¿™ä¸ªbaseline operates on weights instead of activations æ•ˆæœ match or outperform BN smooth the loss è®ºç‚¹ two facts BNçš„performance gainä¸reduction of internal covariate shiftæ²¡ä»€ä¹ˆå…³ç³» BNä½¿å¾—optimization landscape significantly smoother å› æ­¤our target is to find another technique achieves smooth landscape work with micro-batch normalization methods focus on activations ä¸å±•å¼€ focus on weights WNï¼šjust length-direction decoupling æ–¹æ³• Lipschitz constants BN reduces the Lipschitz constants of the loss function makes the gradient more Lipschitz BN considers the Lipschitz constants with respect to activationsï¼Œnot the weights that the optimizer is directly optimizing our inspiration standardize the weightsä¹ŸåŒæ ·èƒ½å¤Ÿsmooth the landscape æ›´ç›´æ¥ smoothing effects on activations and weightsæ˜¯å¯ä»¥ç´¯ç§¯çš„ï¼Œå› ä¸ºæ˜¯çº¿æ€§è¿ç®— Weight Standardization reparameterize the original weights $W$ å¯¹å·ç§¯å±‚çš„æƒé‡å‚æ•°åšå˜æ¢ï¼Œno bias $W \in R^{O * I}$ $O=C_{out}$ $I=C_{in}*kernel_size$ optimize the loss on $\hat W$ compute mean &amp; var on I-dim åªåšæ ‡å‡†åŒ–ï¼Œæ— éœ€affineï¼Œå› ä¸ºé»˜è®¤åç»­è¿˜è¦æ¥ä¸€ä¸ªnormalization layerå¯¹ç¥ç»å…ƒè¿›è¡Œrefine WS normalizes gradients æ‹†è§£ï¼š eq5ï¼š$W$ to $\dot W$ï¼Œå‡å‡å€¼ï¼Œzero-centered eq6ï¼š$\dot W$ to $\hat W$ï¼Œé™¤æ–¹å·®ï¼Œone-varianced eq8ï¼š$\delta \hat W$ç”±å‰ä¸€æ­¥çš„æ¢¯åº¦normalizeå¾—åˆ° eq9ï¼š$\delta \dot W$ä¹Ÿç”±å‰ä¸€æ­¥çš„æ¢¯åº¦normalize æœ€ç»ˆç”¨äºæ¢¯åº¦æ›´æ–°çš„æ¢¯åº¦æ˜¯zero-centered WS smooths landscape åˆ¤å®šæ˜¯å¦smoothå°±çœ‹Lipschitz constantçš„å¤§å° eq5å’Œeq6éƒ½èƒ½reduce the Lipschitz constant å…¶ä¸­eq5 makes the major improvements eq6 slightly improvesï¼Œå› ä¸ºè®¡ç®—é‡ä¸å¤§ï¼Œæ‰€ä»¥ä¿ç•™ å®éªŒ ImageNet BNçš„batchsizeæ˜¯64ï¼Œå…¶ä½™éƒ½æ˜¯1ï¼Œå…¶ä½™çš„æ¢¯åº¦æ›´æ–°iterationsæ”¹æˆ64â€”â€”ä½¿å¾—å‚æ•°æ›´æ–°æ¬¡æ•°åŒæ­¥ æ‰€æœ‰çš„normalization methodsåŠ ä¸ŠWSéƒ½æœ‰æå‡ è£¸çš„normalization methodsé‡Œé¢batchsize1çš„GNæœ€å¥½ï¼Œæ‰€ä»¥é€‰ç”¨GN+WSåšè¿›ä¸€æ­¥å®éªŒ GN+WS+AFï¼šåŠ ä¸Šconv weightçš„affineä¼šharm code 123456# official release# æ”¾åœ¨WSConv2Då­ç±»çš„callé‡Œé¢kernel_mean = tf.math.reduce_mean(kernel, axis=[0, 1, 2], keepdims=True, name='kernel_mean')kernel = kernel - kernel_meankernel_std = tf.keras.backend.std(kernel, axis=[0, 1, 2], keepdims=True)kernel = kernel / (kernel_std + 1e-5)]]></content>
      <tags>
        <tag>CNN, layer, normalization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NFNet]]></title>
    <url>%2F2021%2F02%2F22%2FNFNet%2F</url>
    <content type="text"><![CDATA[NFNet: High-Performance Large-Scale Image Recognition Without Normalization åŠ¨æœº NFï¼š normalization-free aims to match the test acc of batch-normalized networks attain new SOTA 86.5% pre-training + fine-tuningä¸Šä¹Ÿè¡¨ç°æ›´å¥½89.2% batch normalization ä¸æ˜¯å®Œç¾è§£å†³æ–¹æ¡ˆ depends on batch size non-normalized networks accuracy instabilitiesï¼šdevelop adaptive gradient clipping è®ºç‚¹ vast majority models variants of deep residual + BN allow deeper, stable and regularizing disadvantages of batch normalization computational expensive introduces discrepancy between training &amp; testing models &amp; increase params breaks the independence among samples methods seeks to replace BN alternative normalizers study the origin benefits of BN train deep ResNets without normalization layers key theme when removing normalization suppress the scale of the residual branch simplest wayï¼šapply a learnable scalar recent workï¼šsuppress the branch at initialization &amp; apply Scaled Weight Standardizationï¼Œèƒ½è¿½ä¸ŠResNetå®¶æ—ï¼Œä½†æ˜¯æ²¡è¿½ä¸ŠEffå®¶æ— our NFNetsâ€™ main contributions propose AGCï¼šè§£å†³unstableé—®é¢˜ï¼Œallow larger batch size and stronger augmentatons NFNetså®¶æ—åˆ·æ–°SOTAï¼šåˆå¿«åˆå‡† pretraining + finetuningçš„æˆç»©ä¹Ÿæ¯”batch normed modelså¥½ æ–¹æ³• Understanding Batch Normalization four main benefits downscale the residual branchï¼šä»initializationå°±ä¿è¯æ®‹å·®åˆ†æ”¯çš„scaleæ¯”è¾ƒå°ï¼Œä½¿å¾—ç½‘ç»œhas well-behaved gradients early in trainingï¼Œä»è€Œefficient optimization eliminates mean-shiftï¼šReLUæ˜¯ä¸å¯¹ç§°çš„ï¼Œstacking layersä»¥åæ•°æ®åˆ†å¸ƒä¼šç´¯ç§¯åç§» regularizing effectï¼šmini-batchä½œä¸ºsubsetå¯¹äºå…¨é›†æ˜¯æœ‰åçš„ï¼Œè¿™ç§noiseå¯ä»¥çœ‹ä½œæ˜¯regularizer allows efficient large-batch trainingï¼šæ•°æ®åˆ†å¸ƒç¨³å®šæ‰€ä»¥losså˜åŒ–ç¨³å®šï¼ŒåŒæ—¶å¤§batchæ›´æ¥è¿‘çœŸå®åˆ†å¸ƒï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ›´å¤§çš„learning rateï¼Œä½†æ˜¯è¿™ä¸ªpropertyä»…åœ¨ä½¿ç”¨å¤§batch sizeçš„æ—¶å€™æœ‰æ•ˆ NF-ResNets recovering the benefits of BNï¼šå¯¹residual branchè¿›è¡Œscaleå’Œmean-shift residual blockï¼š$h_{i+1} = h_i + \alpha f_i (h_i/\beta_i)$ $\beta_i = Var(h_i)$ï¼šå¯¹è¾“å…¥è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆæ–¹å·®ä¸º1ï¼‰ï¼Œè¿™æ˜¯ä¸ªexpected valueï¼Œä¸æ˜¯ç®—å‡ºæ¥çš„ï¼Œç»“æ„å®šæ­»å°±å®šæ­»äº† Scaled Weight Standardization &amp; scaled activation æ¯”åŸç‰ˆçš„WSå¤šäº†ä¸€ä¸ª$\sqrt N$çš„åˆ†æ¯ æºç å®ç°ä¸­æ¯”åŸç‰ˆWSè¿˜å¤šäº†learnable affine gain ä½¿å¾—conv-reluä»¥åè¾“å‡ºè¿˜æ˜¯æ ‡å‡†åˆ†å¸ƒ $\alpha=0.2$ï¼šrescale residual branchä¸Šï¼Œæœ€ç»ˆçš„è¾“å‡ºä¸º$\alpha*$æ ‡å‡†åˆ†å¸ƒï¼Œæ–¹å·®æ˜¯$\alpha^2$ id pathä¸Šï¼Œè¾“å‡ºè¿˜æ˜¯$h_{i}$ï¼Œæ–¹å·®æ˜¯$Var(h_i)$ updateè¿™ä¸ªblockè¾“å‡ºçš„æ–¹å·®ä¸º$Var(h_{i+1}) = Var(h_i)+\alpha^2$ï¼Œæ¥æ›´æ–°ä¸‹ä¸€ä¸ªblockçš„ $\beta$ variance reset æ¯ä¸ªtransition blockä»¥åï¼ŒæŠŠvarianceé‡æ–°è®¾å®šä¸º$1+\alpha^2$ åœ¨æ¥ä¸‹æ¥çš„non-transition blockä¸­ï¼Œç”¨ä¸Šé¢çš„updateå…¬å¼æ›´æ–°expected std å†åŠ ä¸Šadditional regularizationï¼ˆDropoutå’ŒStochastic Depthä¸¤ç§æ­£åˆ™æ‰‹æ®µï¼‰ï¼Œå°±æ»¡è¶³äº†BN benefitsçš„å‰ä¸‰æ¡ åœ¨batch sizeè¾ƒå°çš„æ—¶å€™èƒ½å¤Ÿcatch upç”šè‡³è¶…è¶Šbatch normalized models ä½†æ˜¯large batch sizeçš„æ—¶å€™perform worse å¯¹äºä¸€ä¸ªæ ‡å‡†çš„conv-bn-reluï¼Œä»workflowä¸Šçœ‹ originï¼šinputâ€”â€”ä¸€ä¸ªfreeçš„conv weightingâ€”â€”BNï¼ˆnorm &amp; rescaleï¼‰â€”â€”activation NFNetï¼šinputâ€”â€”standard normâ€”â€”normed weighting &amp; activationâ€”â€”rescale Adaptive Gradient Clipping for Efficient Large-Batch Training æ¢¯åº¦è£å‰ªï¼š clip by normï¼šç”¨ä¸€ä¸ªclipping threshold $\lambda$ è¿›è¡Œrescaleï¼Œtraining stability was extremely sensitive to è¶…å‚çš„é€‰æ‹©ï¼Œsettingsï¼ˆmodel depth, the batch size, or the learning rateï¼‰ä¸€å˜è¶…å‚å°±è¦é‡æ–°è°ƒ clip by valueï¼šç”¨ä¸€ä¸ªclipping valueè¿›è¡Œä¸Šä¸‹é™æˆªæ–­ AGC given æŸå±‚çš„æƒé‡$W \in R^{NM}$ å’Œ å¯¹åº”æ¢¯åº¦$G \in R^{NM}$ ratio $\frac{||G||_F}{||W||_F}$ å¯ä»¥çœ‹ä½œæ˜¯æ¢¯åº¦å˜åŒ–å¤§å°çš„measurement æ‰€ä»¥æˆ‘ä»¬ç›´è§‚åœ°æƒ³åˆ°å°†è¿™ä¸ªratioè¿›è¡Œé™å¹…ï¼šæ‰€è°“çš„adaptiveå°±æ˜¯åœ¨æ¢¯åº¦è£å‰ªçš„æ—¶å€™ä¸æ˜¯å¯¹æ‰€æœ‰æ¢¯åº¦ä¸€åˆ€åˆ‡ï¼Œè€Œæ˜¯è€ƒè™‘å…¶å¯¹åº”æƒé‡å¤§å°ï¼Œä»è€Œè¿›è¡Œæ›´åˆç†çš„è°ƒèŠ‚ ä½†æ˜¯å®éªŒä¸­å‘ç°unit-wiseçš„gradient normè¦æ¯”layer-wiseçš„å¥½ï¼šæ¯ä¸ªunitå°±æ˜¯æ¯è¡Œï¼Œå¯¹äºconv weightså°±æ˜¯(hxwxCin)ä¸­çš„ä¸€ä¸ª scalar hyperparameter $\lambda$ * the optimal value may depend on the choice of optimizer, learning rate and batch size * empirically we found $\lambda$ should be smaller for larger batches ablations for AGC ç”¨pre-activation NF-ResNet-50 å’Œ NF-ResNet-200 åšå®éªŒï¼Œbatch sizeé€‰æ‹©ä»256åˆ°4096ï¼Œå­¦ä¹ ç‡ä»0.1å¼€å§‹åŸºäºbatch sizeçº¿æ€§å¢é•¿ï¼Œè¶…å‚$\lambda$çš„å–å€¼è§å³å›¾ å·¦å›¾ç»“è®º1ï¼šåœ¨batch sizeè¾ƒå°çš„æƒ…å†µä¸‹ï¼ŒNF-Netsèƒ½å¤Ÿè¿½ä¸Šç”šè‡³è¶…è¶Šnormed modelsçš„ç²¾åº¦ï¼Œä½†æ˜¯batch sizeä¸€å¤§ï¼ˆ2048ï¼‰æƒ…å†µå°±æ¶åŒ–äº†ï¼Œä½†æ˜¯æœ‰AGCçš„NF-Netsåˆ™èƒ½å¤Ÿmaintaining performance comparable or better thanï½ï½ï½ å·¦å›¾ç»“è®º2ï¼šthe benefits of using AGC are smaller when the batch size is small å³å›¾ç»“è®º1ï¼šè¶…å‚$\lambda$çš„å–å€¼æ¯”è¾ƒå°çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯¹æ¢¯åº¦çš„clippingæ›´strongï¼Œè¿™å¯¹äºä½¿ç”¨å¤§batch sizeè®­ç»ƒçš„ç¨³å®šæ€§æ¥è¯´éå¸¸é‡è¦ whether or not AGC is beneficial for all layers * it is always better to not clip the final linear layer * æœ€å¼€å§‹çš„å·ç§¯ä¸åšæ¢¯åº¦è£å‰ªä¹Ÿèƒ½ç¨³å®šè®­ç»ƒ æœ€ç»ˆwe apply AGC to every layer except for the final linear layer Normalizer-Free Architectures begin with SE-ResNeXt-D model about group width * set group width to 128 the reduction in compute density means that åªå‡å°‘äº†ç†è®ºä¸Šçš„FLOPsï¼Œæ²¡æœ‰å®é™…åŠ é€Ÿ about stages * Rç³»åˆ—æ¨¡å‹åŠ æ·±çš„æ—¶å€™æ˜¯éçº¿æ€§å¢é•¿ï¼Œç–¯ç‹‚å åŠ stage3çš„blockæ•°ï¼Œå› ä¸ºè¿™ä¸€å±‚resolutionä¸å¤§ï¼Œchannelä¹Ÿä¸æ˜¯æœ€å¤šï¼Œå…¼é¡¾äº†ä¸¤ä¾§è®¡ç®—é‡ æˆ‘ä»¬ç»™F0è®¾ç½®ä¸º[1,2,6,3]ï¼Œç„¶ååœ¨deeper variantsä¸­å¯¹æ¯ä¸ªstageçš„blockæ•°ç”¨ä¸€ä¸ªscalar Nçº¿å½¢å¢é•¿ about width * ä»æ—§å¯¹stage3ä¸‹æ‰‹ï¼Œ[256,512,1536,1536] * roughly preserves the training speed ä¸€ä¸ªè®ºç‚¹ï¼šstage3 is the best place to add capacityï¼Œå› ä¸ºdeeper enoughåŒæ—¶have access to deeper levelsåŒæ—¶åˆæ¯”æœ€åä¸€å±‚æœ‰slightly higher resolution about block * å®éªŒå‘ç°æœ€æœ‰ç”¨çš„æ“ä½œæ˜¯adding an additional 3 Ã— 3 grouped conv after the first * overview about scaling variants * effç³»åˆ—é‡‡ç”¨çš„æ˜¯Rã€Wã€Dä¸€èµ·å¢é•¿ï¼Œå› ä¸ºeffçš„blockæ¯”è¾ƒè½»é‡ ä½†æ˜¯å¯¹Rç³»åˆ—æ¥è¯´ï¼Œåªå¢é•¿Då’ŒRå°±å¤Ÿäº† è¡¥å……ç»†èŠ‚ * åœ¨inferenceé˜¶æ®µä½¿ç”¨æ¯”è®­ç»ƒé˜¶æ®µslightly higher resolution éšç€æ¨¡å‹åŠ å¤§increase the regularization strengthï¼š scale the drop rate of Dropout è°ƒæ•´stochastic depth rateå’Œweight decayåˆ™not effective se-blockçš„scaleä¹˜ä¸ª2 SGD params: Nesterov=True, momentum=0.9, clipnorm=0.01 lrï¼š å…ˆwarmupå†ä½™å¼¦é€€ç«ï¼šincrease from 0 to 1.6 over 5 epochs, then decay to zero with cosine annealing ä½™å¼¦é€€ç«cosine annealing summary æ€»ç»“æ¥è¯´ï¼Œå°±æ˜¯æ‹¿æ¥ä¸€ä¸ªSE-ResNeXt-D å…ˆåšç»“æ„ä¸Šçš„è°ƒæ•´ï¼Œmodified width and depth patternsä»¥åŠa second spatial convolutionï¼Œè¿˜æœ‰drop rateï¼Œresolution å†åšå¯¹æ¢¯åº¦çš„è°ƒæ•´ï¼šé™¤äº†æœ€åä¸€ä¸ªçº¿å½¢åˆ†ç±»å±‚ä»¥å¤–ï¼Œå…¨ç”¨AGCï¼Œ$\lambda=0.01$ æœ€åæ˜¯è®­ç»ƒä¸Šçš„trickï¼šstrong regularization and data augmentation detailed view of NFBlocks transition blockï¼šæœ‰ä¸‹é‡‡æ ·çš„block æ®‹å·®branchä¸Šï¼Œbottleneckçš„narrow ratioæ˜¯0.5 æ¯ä¸ªstageçš„3x3 convçš„group widthæ°¸è¿œæ˜¯128ï¼Œè€Œgroupæ•°ç›®æ˜¯åœ¨éšç€block widthå˜çš„ skip pathæ¥åœ¨ $\beta$ downscaling ä¹‹å skip pathä¸Šæ˜¯avg pooling + 1x1 conv non-transition blockï¼šæ— ä¸‹é‡‡æ ·çš„block bottleneck-ratioä»æ—§æ˜¯0.5 3x3convçš„group widthä»æ—§æ˜¯128 skip pathæ¥åœ¨$\beta$ downscaling ä¹‹å‰ skip pathå°±æ˜¯id å®éªŒ]]></content>
      <tags>
        <tag>CNN, classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[repVGG]]></title>
    <url>%2F2021%2F02%2F09%2FrepVGG%2F</url>
    <content type="text"><![CDATA[RepVGG: Making VGG-style ConvNets Great Again åŠ¨æœº plain ConvNets simply efficient but poor performance propose a CNN architecture RepVGG èƒ½å¤Ÿdecoupleä¸ºtraining-timeå’Œinference-timeä¸¤ä¸ªç»“æ„ é€šè¿‡structure re-paramterization technique inference-time architecture has a VGG-like plain body faster 83% faster than ResNet-50 or 101% faster than ResNet-101 accuracy-speed trade-off reaches over 80% top-1 accuracy outperforms ResNets by a large margin verify on classification &amp; semantic segmentation tasks è®ºç‚¹ well-designed CNN architectures Inceptionï¼ŒResNetï¼ŒDenseNetï¼ŒNAS models deliver higher accuracy drawbacks multi-branch designsï¼šslow down inference and reduce memory utilizationï¼Œå¯¹é«˜å¹¶è¡ŒåŒ–çš„è®¾å¤‡ä¸å‹å¥½ some componentsï¼šdepthwise &amp; channel shuffleï¼Œincrease memory access cost MAC(memory access cost) constitutes a large time usage in groupwise convolutionï¼šæˆ‘çš„groupconvå®ç°é‡Œcardinalityç»´åº¦ä¸Šè®¡ç®—ä¸å¹¶è¡Œ FLOPså¹¶ä¸èƒ½precisely reflect actual speedï¼Œä¸€äº›ç»“æ„çœ‹ä¼¼æ¯”old fashioned VGG/resnetçš„FLOPså°‘ï¼Œä½†å®é™…å¹¶æ²¡æœ‰å¿« multi-branch é€šå¸¸multi-branch modelè¦æ¯”plain modelè¡¨ç°å¥½ å› ä¸ºmakes the model an implicit ensemble of numerous shallower models so that avoids gradient vanishing benefits are all for training drawbacks are undesired for inference the proposed RepVGG advantages plain architectureï¼šno branches 3x3 conv &amp; ReLUç»„æˆ æ²¡æœ‰è¿‡é‡çš„äººå·¥è®¾è®¡ç—•è¿¹ training time use identity &amp; 1x1 conv branches at inference time identity å¯ä»¥çœ‹åšdegraded 1x1 conv 1x1 conv å¯ä»¥çœ‹åšdegraded 3x3 conv æœ€ç»ˆæ•´ä¸ªconv-bn branchesèƒ½å¤Ÿæ•´åˆæˆä¸€ä¸ª3x3 conv inference-time modelåªåŒ…å«convå’ŒReLUï¼šæ²¡æœ‰max poolingï¼ï¼ fewer memory unitsï¼šåˆ†æ”¯ä¼šå å†…å­˜ï¼Œç›´åˆ°åˆ†æ”¯è®¡ç®—ç»“æŸï¼Œplainç»“æ„çš„memoryåˆ™æ˜¯immediately released æ–¹æ³• training-time ResNet-like block id + 1x1 conv + 3x3 conv multi-branches use BN in each branch with n blocks, the model can be interpreted as an ensemble of $3^n$ models stride2çš„blockåº”è¯¥æ²¡æœ‰id pathå§ï¼Ÿï¼Ÿ simply stack serveral blocks to construct the training model inference-time re-param inference-time BNä¹Ÿæ˜¯ä¸€ä¸ªçº¿æ€§è®¡ç®— ä¸¤ä¸ª1x1 convéƒ½å¯ä»¥è½¬æ¢æˆä¸­é€šçš„3x3 kernelï¼Œæœ‰æƒ/æ— æƒ è¦æ±‚å„branch has the same strides &amp; padding pixelè¦å¯¹é½ architectural specification varietyï¼šdepth and width does not use maxpoolingï¼šåªæœ‰ä¸€ç§operatorï¼š3x3 conv+relu headï¼šGAP + fc / task specific 5 stages ç¬¬ä¸€ä¸ªstageå¤„ç†high resolutionï¼Œstride2 ç¬¬äº”ä¸ªstage shall have more channelsï¼Œæ‰€ä»¥åªç”¨ä¸€å±‚ï¼Œsave parameters ç»™å€’æ•°ç¬¬äºŒä¸ªstageæœ€å¤šå±‚ï¼Œè€ƒè™‘paramså’Œcomputationçš„balance RepVGG-Aï¼š[1,2,4,14,1]ï¼Œç”¨æ¥compete againstè½»é‡å’Œä¸­é‡çº§model RepVGG-Bï¼šdeeper in s2,3,4ï¼Œ[1,4,6,16,1]ï¼Œç”¨æ¥compete against high-performance ones basic widthï¼š[64, 128, 256, 512] width multiplier a &amp; b aæ§åˆ¶å‰4ä¸ªstageå®½åº¦ï¼Œbæ§åˆ¶æœ€åä¸€ä¸ªstage [64a, 128a, 256a, 512b] ç¬¬ä¸€ä¸ªstageçš„å®½åº¦åªæ¥å—å˜å°ä¸æ¥å—å˜å¤§ï¼Œå› ä¸ºå¤§resolutionå½±å“è®¡ç®—é‡ï¼Œmin(64,64a) further reduce params &amp; computation groupwise 3x3 conv è·³ç€å±‚æ¢ï¼šä»ç¬¬ä¸‰å¼€å§‹ï¼Œç¬¬ä¸‰ã€ç¬¬äº”ã€ number of groupsï¼š1ï¼Œ2ï¼Œ4 globally å®éªŒ åˆ†æ”¯çš„ä½œç”¨ ç»“æ„ä¸Šçš„å¾®è°ƒ id pathå»æ‰BN æŠŠæ‰€æœ‰çš„BNç§»åŠ¨åˆ°addçš„åé¢ æ¯ä¸ªpathåŠ ä¸Šrelu ImageNetåˆ†ç±»ä»»åŠ¡ä¸Šå¯¹æ ‡å…¶ä»–æ¨¡å‹ simple augmentation strongï¼šAutoaugment, label smoothing and mixup]]></content>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[transform in CNN]]></title>
    <url>%2F2021%2F02%2F03%2Ftransform-in-CNN%2F</url>
    <content type="text"><![CDATA[ç»¼è¿° å‡ ä½•å˜æ¢ STNï¼š æ™®é€šçš„CNNèƒ½å¤Ÿéšå¼çš„å­¦ä¹ ä¸€å®šçš„å¹³ç§»ã€æ—‹è½¬ä¸å˜æ€§ï¼Œè®©ç½‘ç»œèƒ½å¤Ÿé€‚åº”è¿™ç§å˜æ¢ï¼šé™é‡‡æ ·ç»“æ„æœ¬èº«èƒ½å¤Ÿä½¿å¾—ç½‘ç»œå¯¹å˜æ¢ä¸æ•æ„Ÿ ä»æ•°æ®è§’åº¦å‡ºå‘ï¼Œæˆ‘ä»¬è¿˜ä¼šå¼•å…¥å„ç§augmentationï¼Œå¼ºåŒ–ç½‘ç»œå¯¹å˜åŒ–çš„ä¸å˜èƒ½åŠ› deepMindä¸ºç½‘ç»œè®¾è®¡äº†ä¸€ä¸ªæ˜¾å¼çš„å˜æ¢æ¨¡å—æ¥å­¦ä¹ å„ç§å˜åŒ–ï¼Œå°†distortedçš„è¾“å…¥å˜æ¢å›å»ï¼Œè®©ç½‘ç»œå­¦ä¹ æ›´ç®€å•çš„ä¸œè¥¿ å‚æ•°é‡ï¼šå°±æ˜¯å˜æ¢çŸ©é˜µçš„å‚æ•°ï¼Œé€šå¸¸æ˜¯2x3çš„çººå°„å˜åŒ–çŸ©é˜µï¼Œä¹Ÿå°±æ˜¯6ä¸ªå‚æ•° deformable convï¼š based on STN é’ˆå¯¹åˆ†ç±»å’Œæ£€æµ‹åˆ†åˆ«æå‡ºdeformable convolutionå’Œdeformable RoI poolingï¼š æ„Ÿè§‰deformable RoI poolingå’Œguiding anchoré‡Œé¢çš„feature adaptionæ˜¯ä¸€ä¸ªä¸œè¥¿ å‚æ•°é‡ï¼šregular kernel params 3x3 + deformable offsets 3x3x2 whatâ€™s newï¼Ÿ ä¸ªäººè®¤ä¸ºå¼•å…¥æ›´å¤šçš„å‚æ•°å¼•å…¥çš„å˜åŒ– é¦–å…ˆSTNæ˜¯ä»outputåˆ°inputçš„æ˜ å°„ï¼Œä½¿ç”¨å˜æ¢çŸ©é˜µMé€šå¸¸åªèƒ½è¡¨ç¤ºdepictable transformationï¼Œä¸”å…¨å›¾åªæœ‰1ä¸ªtransformation å…¶æ¬¡STNçš„sampling kernelä¹Ÿæ˜¯é¢„å®šä¹‰çš„ç®—æ³•ï¼Œå¯¹kernelå†…çš„æ‰€æœ‰pixelä½¿ç”¨ç›¸åŒçš„å˜åŒ–ï¼Œä¹Ÿå°±æ˜¯1ä¸ªweight factor deformable convæ˜¯ä»inputåˆ°outputçš„æ˜ å°„ï¼Œæ˜ å°„å¯ä»¥æ˜¯ä»»æ„çš„transformationï¼Œä¸”3x3x2çš„å‚æ•°æœ€å¤šå¯ä»¥åŒ…å«3x3ç§transformation sampling kernelå¯¹kernelå†…çš„æ¯ä¸ªç‚¹ï¼Œä¹Ÿå¯ä»¥æœ‰ä¸åŒçš„æƒé‡ï¼Œä¹Ÿå°±æ˜¯3x3ä¸ªweight factor è¿˜æœ‰å•¥è·Ÿå½¢å˜ç›¸å…³çš„ attentionæœºåˆ¶ spatial attentionï¼šSTNï¼ŒsSE channel attentionï¼šSENet åŒæ—¶ä½¿ç”¨ç©ºé—´attentionå’Œé€šé“attentionæœºåˆ¶ï¼šCBAM papers [STN] STN: Spatial Transformer Networksï¼ŒSTNçš„å˜æ¢æ˜¯pre-definedçš„ï¼Œæ˜¯é’ˆå¯¹å…¨å±€featuremapçš„å˜æ¢ [DCN 2017] Deformable Convolutional Networks ï¼ŒDCNçš„å˜æ¢æ˜¯æ›´éšæœºçš„ï¼Œæ˜¯é’ˆå¯¹å±€éƒ¨kernelåˆ†åˆ«è¿›è¡Œçš„å˜åŒ–ï¼ŒåŸºäºå·ç§¯æ ¸æ·»åŠ location-specific shift [DCNv2 2018] Deformable ConvNets v2: More Deformable, Better Resultsï¼Œè¿›ä¸€æ­¥æ¶ˆé™¤irrelevant contextï¼ŒåŸºäºå·ç§¯æ ¸æ·»åŠ weighted-location-specific shiftï¼Œæå‡performance [attentionç³»åˆ—paper] [SENet &amp;SKNet &amp; CBAM &amp; GC-Net][https://amberzzzz.github.io/2020/03/13/attention%E7%B3%BB%E5%88%97/] STN: Spatial Transformer Networks åŠ¨æœº ä¼ ç»Ÿå·ç§¯ï¼šlack the ability of spacially invariant propose a new learnable module can be inserted into CNN spatially manipulate the data without any extra supervision models learn to be invariant to transformations è®ºç‚¹ spatially invariant the ability of being invariant to large transformations of the input data max-pooling åœ¨ä¸€å®šç¨‹åº¦ä¸Šspatially invariant å› ä¸ºreceptive fields are fixed and local and small å¿…é¡»å åŠ åˆ°æ¯”è¾ƒæ·±å±‚çš„æ—¶å€™æ‰èƒ½å®ç°ï¼Œintermediate feature layerså¯¹large transformationsä¸å¤ªè¡Œ æ˜¯ä¸€ç§pre-defined mechanismï¼Œè·Ÿsampleæ— å…³ spatial transformation module conditioned on individual samples dynamic mechanism produce a transformation and perform it on the entire feature map taskåœºæ™¯ distorted digitsåˆ†ç±»ï¼šå¯¹è¾“å…¥åštranformèƒ½å¤Ÿsimplifyåé¢çš„åˆ†ç±»ä»»åŠ¡ co-localisationï¼š spatial attention related work ç”Ÿæˆå™¨ç”¨æ¥ç”Ÿæˆtransformed imagesï¼Œä»è€Œåˆ¤åˆ«å™¨èƒ½å¤Ÿå­¦ä¹ åˆ†ç±»ä»»åŠ¡from transformation supervision ä¸€äº›methodsè¯•å›¾ä»ç½‘ç»œç»“æ„ã€feature extractorsçš„è§’åº¦çš„è·å¾—invariant representationsï¼Œwhile STN aims to achieve this by manipulating the data manipulating the dataé€šå¸¸å°±æ˜¯åŸºäºattention mechanismï¼Œcropæ¶‰åŠdifferentiableé—®é¢˜ æ–¹æ³• formulation localisation networkï¼špredict transform parameters grid generatorï¼šåŸºäºpredicted paramsç”Ÿæˆsampling grid samplerï¼šelement-multiply localisation network input feature map $U \in R^{hwc}$ same transformation is applied to each channel generate parameters of transformation $\theta$ï¼š1-d vector fc / conv + final regression layer parameterised sampling grid sampling kernel applied by pixel general affine transformationï¼šcroppingï¼Œtranslationï¼Œrotationï¼Œscaleï¼Œskew ouput mapä¸Šä»»æ„ä¸€ç‚¹ä¸€å®šæ¥è‡ªå˜æ¢å‰çš„æŸä¸€ç‚¹ï¼Œåä¹‹ä¸ä¸€å®šï¼Œinput mapä¸ŠæŸä¸€ç‚¹å¯èƒ½æ˜¯bgï¼Œè¢«cropæ‰äº†ï¼Œæ‰€ä»¥pointwise transformationå†™æˆåè¿‡æ¥çš„ï¼š target pointsæ„æˆçš„ç‚¹é›†å°±æ˜¯sampling points on the input feature map differentiable image sampling é€šè¿‡ä¸Šä¸€æ­¥çš„çŸ©é˜µtransformationï¼Œå¾—åˆ°input mapä¸Šéœ€è¦ä¿ç•™çš„source point set å¯¹ç‚¹é›†ä¸­æ¯ä¸€ç‚¹apply kernel é€šç”¨çš„æ’å€¼è¡¨è¾¾å¼ï¼š æœ€è¿‘é‚»kernelæ˜¯ä¸ªpulseå‡½æ•° bilinear kernelæ˜¯ä¸ªdistance&gt;1çš„å…¨muteæ‰ï¼Œåˆ†æ®µå¯å¯¼ STNï¼šSpatial Transformer Networks æŠŠspatial transformeråµŒè¿›CNNå»ï¼šlearn how to actively transform the features to help minimize the overall cost computationally fast å‡ ç§ç”¨æ³• feed the output of the localization network $\theta$ to the rest of the networkï¼šå› ä¸ºtransformå‚æ•°explicitly encodesç›®æ ‡çš„ä½ç½®å§¿æ€ä¿¡æ¯ place multiple spatial transformers at increasing depthï¼šä¸²è¡Œèƒ½å¤Ÿè®©æ·±å±‚çš„transformerå­¦ä¹ æ›´æŠ½è±¡çš„å˜æ¢ place multiple spatial transformers in parallelï¼šå¹¶è¡Œçš„å˜æ¢ä½¿å¾—æ¯ä¸ªå˜æ¢é’ˆå¯¹ä¸åŒçš„object å®éªŒ Rã€RTSã€Pã€Eï¼šdistortion ahead affã€projã€TPSï¼štransformer predefined affï¼šç»™å®šè§’åº¦ï¼Ÿï¼Ÿ TPSï¼šè–„æ¿æ ·æ¡æ’å€¼ Deformable Convolutional Networks åŠ¨æœº CNNï¼šfixed geometric structures enhance the transformation modeling capability deformable convolution deformable RoI pooling without additional supervision share similiar spirit with STN è®ºç‚¹ to accommodate geometric variations data augmentation is limited to model large, unknown transformations fixed receptive fields is undesirable for high level CNN layers that encode the semantics ä½¿ç”¨å¤§é‡å¢å¹¿çš„æ•°æ®ï¼Œæšä¸¾ä¸å…¨ï¼Œè€Œä¸”æ”¶æ•›æ…¢ï¼Œæ‰€éœ€ç½‘ç»œå‚æ•°é‡å¤§ å¯¹äºæå–è¯­ä¹‰ç‰¹å¾çš„é«˜å±‚ç½‘ç»œæ¥è®²ï¼Œå›ºå®šçš„æ„Ÿå—é‡å¯¹ä¸åŒç›®æ ‡ä¸å‹å¥½ introduce two new modules deformable convolution learning offsets for each kernel via additional convolutional layers deformable RoI pooling learning offset for each bin partition of the previous RoI pooling æ–¹æ³• overview operate on the 2D spatial domain remains the same across the channel dimension deformable convolution æ­£å¸¸çš„å·ç§¯ï¼š $y(p_0) = \sum w(p_n)*x(p_0 + p_n)$ $p_n \in R\{(-1,-1),(-1,0),â€¦, (0,0), (1,1)\}$ deformable convï¼šwith offsets $\Delta p_n$ $y(p_0) = \sum w(p_n)*x(p_0 + p_n + \Delta p_n)$ offset value is typically fractional bilinear interpolationï¼š $x(p) = \sum_q G(q,p)x(q)$ å…¶ä¸­$G(q,p)$æ˜¯æ¡ä»¶ï¼š$G(q,p)=max(0, 1-|q_x-p_x|)*max(0, 1-|q_y-p_y|)$ åªè®¡ç®—å’Œoffsetç‚¹è·ç¦»å°äº1ä¸ªå•ä½çš„é‚»è¿‘ç‚¹ å®ç° offsets convå’Œç‰¹å¾æå–convæ˜¯ä¸€æ ·çš„kernelï¼šsame spatial resolution and dilationï¼ˆNä¸ªpositionï¼‰ the channel dimension 2Nï¼šå› ä¸ºæ˜¯xå’Œyä¸¤ä¸ªæ–¹å‘çš„offset deformable RoI pooling RoI pooling converts an input feature map of arbitrary size into fixed size features å¸¸è§„çš„RoI pooling divides ROI into k*k bins and for each binï¼š$y(i,j) = \sum_{p \in bin(i,j)} x(p_0+p)/n_{ij}$ å¯¹feature mapä¸Šåˆ’åˆ†åˆ°æ¯ä¸ªbiné‡Œé¢æ‰€æœ‰çš„ç‚¹ deformable RoI poolingï¼šwith offsets $\Delta p_{ij}$ $y(i,j) = \sum_{p \in bin(i,j)} x(p_0+p+\Delta p_{ij})/n_{ij}$ scaled normalized offsetsï¼š$\Delta p_{ij} = \gamma \Delta p_{ij} (w,h) $ normalized offset value is fractional bilinear interpolation on the pooled map as above å®ç° fc layerï¼šk*k*2ä¸ªelementï¼ˆsigmoidï¼Ÿï¼‰ position sensitive RoI Pooling fully convolutional input feature mapå…ˆé€šè¿‡å·ç§¯æ‰©å±•æˆk*k*(C+1)é€šé“ å¯¹æ¯ä¸ªC+1(åŒ…å«kkä¸ªfeature map)ï¼Œconvå‡ºå…¨å›¾çš„offset(2\k*kä¸ª) deformable convNets initialized with zero weights learning rates are set to $\beta$ times of the learning rate for the existing layers $\beta=1.0$ for conv $\beta=0.01$ for fc feature extraction backï¼šResNet-101 &amp; Aligned-Inception-ResNet withoutTopï¼šA randomly initialized 1x1 conv is added at last to reduce the channel dimension to 1024 last block stride is changed from 2 to 1 the dilation of all the convolution filters with kernel size&gt;1 is changed from 1 to 2 Optionally last block use deformable conv in res5a,b,c segmentation and detection deeplab predicts 1x1 score maps Category-Aware RPN run region proposal with specific class modified faster R-CNNï¼šadd ROI pooling at last conv optional faster R-CNNï¼šuse deformable ROI pooling R-FCNï¼šstate-of-the-art detector optional R-FCNï¼šuse deformable ROI pooling å®éªŒ Accuracy steadily improves when more deformable convolution layers are usedï¼šä½¿ç”¨è¶Šå¤šå±‚deform convè¶Šå¥½ï¼Œç»éªŒå–äº†3 the learned offsets are highly adaptive to the image contentï¼šå¤§ç›®æ ‡çš„é—´è·å¤§ï¼Œå› ä¸ºreception fieldå¤§ï¼Œconsistent in different layers atrous convolution also improvesï¼šdefault networks have too small receptive fieldsï¼Œä½†æ˜¯dilationéœ€è¦æ‰‹è°ƒåˆ°æœ€ä¼˜ using deformable RoI pooling alone already produces noticeable performance gains, using both obtains significant accuracy improvements Deformable ConvNets v2: More Deformable, Better Results åŠ¨æœº DCNèƒ½å¤Ÿadaptä¸€å®šçš„geometric variationsï¼Œä½†æ˜¯ä»å­˜åœ¨extend beyond image contentçš„é—®é¢˜ to focus on pertinent image regions increased modeling power more deformable layers updated DCNv2 modules stronger training propose feature mimicking scheme verified on incorporated into Faster-RCNN &amp; Mask RCNN COCO for det &amp; set still lightweight and easy to incorporate è®ºç‚¹ DCNv1 deformable convï¼šåœ¨standard convçš„åŸºç¡€ä¸Šgenerate location-specific offsets which are learned from the preceding feature maps deformable poolingï¼šoffsets are learned for the bin positions in RoIpooling é€šè¿‡å¯è§†åŒ–æ•£ç‚¹å›¾å‘ç°æœ‰éƒ¨åˆ†æ•£ç‚¹è½åœ¨ç›®æ ‡å¤–å›´ propose DCNv2 equip more convolutional layers with offset modified module each sample not only undergoes a learned offset but also a learned feature amplitude effective trainin use RCNN as the teacher network since RCNN learns features unaffected by irrelevant info outside the ROI feature mimicking loss æ–¹æ³• stacking more deformable conv layers replace more regular conv layers by their deformable counterpartsï¼š resnet50çš„stage3ã€4ã€5çš„3x3convéƒ½æ›¿æ¢æˆdeformable convï¼š13ä¸ªconv layer DCNv1æ˜¯æŠŠstage5çš„3ä¸ªresblockçš„3x3 convæ›¿æ¢æˆdeformable convï¼š3ä¸ªdeconv layer å› ä¸ºDCNv1é‡Œé¢åœ¨PASCALä¸Šé¢å®éªŒå‘ç°å†å¤šçš„deconvç²¾åº¦å°±é¥±å’Œäº†ï¼Œä½†æ˜¯DCNv2æ˜¯åœ¨harder dataset COCOä¸Šé¢çš„best-acc-efficiency-tradeoff modulated deformable conv modulate the input feature amplitudes from different spacial locations/bins set the learnable offset &amp; scalar for the k-th locationï¼š$\Delta p_k$å’Œ$\Delta m_k$ set the conv kernel dilationï¼š$p_k$ï¼Œresneté‡Œé¢éƒ½æ˜¯1 the value for location p isï¼š$y(p) = \sum_{k=1}^K w_k x(p+p_k+\Delta p_k)\Delta m_k$ï¼Œbilinear interpolation ç›®çš„æ˜¯æŠ‘åˆ¶æ— å…³ä¿¡å· learnable offset &amp; scalar obtained via a separate conv layer over the same input feature map x è¾“å‡ºæœ‰3Kä¸ªchannelï¼š2K for xy-offsetï¼ŒK for scalar offsetçš„convåé¢æ²¡æ¿€æ´»å‡½æ•°ï¼Œå› ä¸ºèŒƒå›´æ— é™ scalarçš„convåé¢æœ‰ä¸ªsigmoidï¼Œå°†rangeæ§åˆ¶åœ¨[0,1] ä¸¤ä¸ªconvå…¨0åˆå§‹åŒ– ä¸¤ä¸ªconv layerçš„learning rateæ¯”existing layerså°ä¸€ä¸ªæ•°é‡çº§ modulated deformable RoIpooling given an input ROI split into K(7x7) spatial bins average pooling over the sampling points for each binè®¡ç®—binçš„value the bin value isï¼š$y(k) = \sum_{j=1}^{n_k} x(p_{kj}+\Delta p_k)\Delta m_k /n_k$ï¼Œbilinear interpolation a sibling branch 2ä¸ª1024d-fcï¼šgaussian initialization with 0.01 std dev 1ä¸ª3Kd-fcï¼šå…¨0åˆå§‹åŒ– last K channels + sigmoid learning rateè·Ÿexisting layersä¿æŒä¸€è‡´ RCNN feature mimicking å‘ç°æ— è®ºæ˜¯convè¿˜æ˜¯deconvï¼Œerror-boundéƒ½å¾ˆå¤§ å°½ç®¡ä»è®¾è®¡æ€è·¯ä¸Šï¼ŒDCNv2æ˜¯å¸¦æœ‰mute irrelevantçš„èƒ½åŠ›çš„ï¼Œä½†æ˜¯äº‹å®ä¸Šå¹¶æ²¡åšåˆ° è¯´æ˜such representation cannot be learned well through standard FasterRCNN training procedureï¼š è¯´ç™½äº†å°±æ˜¯supervisionåŠ›åº¦ä¸å¤Ÿ éœ€è¦additional guidance feature mimic loss enforced only on positive ROIsï¼šå› ä¸ºèƒŒæ™¯ç±»å¾€å¾€éœ€è¦æ›´é•¿è·ç¦»/æ›´å¤§èŒƒå›´çš„contextä¿¡æ¯ architecture add an additional RCNN branch RCNN input cropped imagesï¼Œgenerate 14x14 featuremapsï¼Œç»è¿‡ä¸¤ä¸ªfcå˜æˆ1024-d å’ŒFasterRCNNé‡Œå¯¹åº”çš„counterpartï¼Œè®¡ç®—cosine similarity è¿™ä¸ªå¤ªæ‰¯äº†ä¸å±•å¼€äº†]]></content>
      <tags>
        <tag>å‡ ä½•å˜æ¢</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spineNet]]></title>
    <url>%2F2021%2F01%2F28%2FspineNet%2F</url>
    <content type="text"><![CDATA[SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization åŠ¨æœº object detection task requiring simultaneous recognition and localization solely encoder performs not well while encoder-decoder architectures are ineffective propose SpineNet scale-permuted intermediate features cross-scale connections searched by NAS on detection COCO can transfer to classification tasks åœ¨è½»é‡å’Œé‡é‡backçš„ä¸€é˜¶æ®µç½‘ç»œä¸­éƒ½æ¶¨ç‚¹é¢†å…ˆ è®ºç‚¹ scale-decreasing backbone throws away the spatial information by down-sampling challenging to recover æ¥ä¸€ä¸ªè½»é‡çš„FPNï¼š scale-permuted model scales of features can increase/decrease anytimeï¼šretain the spacial information connections go across scalesï¼šmulti-scale fusion searched by NAS æ˜¯ä¸€ä¸ªå®Œæ•´çš„FPNï¼Œä¸æ˜¯encoder-decoderé‚£ç§å¯åˆ†çš„å½¢å¼ directly connect to classification and bounding box regression subnets base on ResNet50 use bottleneck feature blocks two inputs for each feature blocks roughly the same computation æ–¹æ³• formulation overall architecture stemï¼šscale-decreased architecture scale-permuted network blocks in the stem network can be candidate inputs for the following scale-permuted network scale-permuted network building blocksï¼š$B_k$ feature levelï¼š$L_3 - L_7$ output featuresï¼š1x1 convï¼Œ$P_3 - P_7$ search space scale-permuted networkï¼š blockåªèƒ½ä»å‰å¾€åconnect based on resNet blocks channel 256 for $L_5, L_6, L_7$ cross-scale connectionsï¼š two input connections for each block from lower ordering block / stem resampling narrow factor $\alpha$ï¼š1x1 conv ä¸Šé‡‡æ ·ï¼šinterpolation ä¸‹é‡‡æ ·ï¼š3x3 s2 conv element-wise add block adjustment intermediate blocks can adjust its scale level &amp; type level from {-1, 0, 1, 2} select from bottleneck / residual block family of models R[N] - SP[M]ï¼šN feature layers in stem &amp; M feature layers in scale-permuted layers gradually shift from stem to SP with size decreasing spineNet family basicï¼šspineNet-49 spineNet-49Sï¼šchannelæ•°scaled down by 0.65 spineNet-96ï¼šdouble the number of blocks spineNet-143ï¼šrepeat 3 timesï¼Œfusion narrow factor $\alpha=1$ spineNet-190ï¼šrepeat 4 timesï¼Œfusion narrow factor $\alpha=1$ï¼Œchannelæ•°scaled up by 1.3 å®éªŒ åœ¨mid/heavyé‡çº§ä¸Šï¼Œæ¯”resnet-family-FPNæ¶¨å‡ºä¸¤ä¸ªç‚¹ åœ¨lighté‡çº§ä¸Šï¼Œæ¯”mobileNet-family-FPNæ¶¨å‡ºä¸€ä¸ªç‚¹]]></content>
      <tags>
        <tag>backbone</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[guided anchoring]]></title>
    <url>%2F2021%2F01%2F27%2Fguided-anchoring%2F</url>
    <content type="text"><![CDATA[åŸä½œè€…çŸ¥ä¹referenceï¼šhttps://zhuanlan.zhihu.com/p/55854246 ä¸å®Œå…¨æ˜¯anchor-freeï¼Œå› ä¸ºè¿˜æ˜¯æœ‰decision grid to choose fromçš„ï¼Œåº”è¯¥è¯´æ˜¯adaptive anchor instead of hand-picked ä¸ºäº†ç‰¹å¾å’Œadaptive anchorå¯¹é½ï¼Œå¼•å…¥deformable conv Region Proposal by Guided Anchoring åŠ¨æœº most methods predefined anchors do a uniformed dense prediction our method use sematic features to guide the anchoring anchor sizeä¹Ÿæ˜¯ç½‘ç»œé¢„æµ‹å‚æ•°ï¼Œcompute from feature map arbitrary aspect ratios feature inconsistency ä¸åŒçš„anchor locéƒ½æ˜¯å¯¹åº”feature mapä¸ŠæŸä¸€ä¸ªç‚¹ å˜åŒ–çš„anchor sizeå’Œå›ºå®šçš„ä½ç½®å‘é‡ä¹‹é—´å­˜åœ¨inconsistency å¼•å…¥feature adaption module use high-quality proposals GA-RPNæå‡äº†proposalçš„è´¨é‡ å› æ­¤æˆ‘ä»¬å¯¹proposalè¿›å…¥stage2çš„æ¡ä»¶æ›´ä¸¥æ ¼ adopt in Fast R-CNN, Faster R-CNN and RetinaNetå‡æ¶¨ç‚¹ RPNæå‡æ˜¾è‘—ï¼š9.1 MAPä¹Ÿæœ‰æ¶¨ç‚¹ï¼š1.2-2.7 è¿˜å¯ä»¥boosting trained models boosting a two-stage detector by a fine-tuning schedule è®ºç‚¹ alignment &amp; consistency æˆ‘ä»¬ç”¨feature mapçš„pixelsä½œä¸ºanchor representationsï¼Œé‚£ä¹ˆanchor centerså¿…é¡»è·Ÿfeature pixelsä¿æŒalign ä¸åŒpixelçš„reception fieldå¿…é¡»è·Ÿå¯¹åº”çš„anchor sizeä¿æŒåŒ¹é… previous sliding window schemeå¯¹æ¯ä¸ªpixeléƒ½åšä¸€æ ·çš„æ“ä½œï¼Œç”¨åŒæ ·ä¸€ç»„anchorï¼Œå› æ­¤æ˜¯alignå’Œconsistçš„ previous progressly refining schemeå¯¹anchorçš„ä½ç½®å¤§å°åšäº†refinementï¼Œignore the alignment &amp; consistency issueï¼Œæ˜¯ä¸å¯¹çš„ï¼ï¼ disadvantage of predefined anchors hard hyperparams huge pos/neg imbalance &amp; computation we propose GA-RPN learnable anchor shapes to mitigate the hand-picked issue feature adaptation to solve the consistency issue key concerns in this paper learnable anchors joint anchor distribution alignment &amp; consistency high-quality proposals æ–¹æ³• formulation $p(x,y,w,h|I) = p(x,y|I)p(w,h|x,y,I)$ å°†é—®é¢˜è§£è€¦æˆä½ç½®å’Œå°ºå¯¸çš„é¢„æµ‹ï¼Œé¦–å…ˆanchorçš„locæœä»full imageçš„å‡åŒ€åˆ†å¸ƒï¼Œanchorçš„sizeå»ºç«‹åœ¨locå­˜åœ¨çš„åŸºç¡€ä¸Š two branches for loc &amp; shape prediction locï¼šbinary classificationï¼Œhxwx1 shapeï¼šlocation-dependent shapesï¼Œhxwx2 anchorsï¼šloc probabilities above a certain threshold &amp; correponding â€˜most probableâ€™ anchor shape multi-scale the anchor generation parameters are shared feature adaptation module adapts the feature according to the anchor shape anchor location prediction indicates the probability of an objectâ€™s center ä¸€å±‚å·ç§¯ï¼š1x1 convï¼Œchannel1ï¼Œsigmoid transform backï¼šeach grid(i,j) corresponds to coords ((i+0.5)*s, (j+0.5)*s) on the origin map filter out 90% of the regions thus replace the ensuing conv layers by masked convs groud truth binary label map each levelï¼šcenter region &amp; ignore region &amp; outside regionï¼ŒåŸºäºobject centerçš„æ–¹æ¡† $\sigma_1=0.2ï¼Œ\sigma_2=0.5$ï¼šregion boxçš„é•¿å®½ç³»æ•° ï¼Ÿï¼Ÿï¼Ÿç”¨centerNetçš„heatmapä¼šä¸ä¼šæ›´å¥½ï¼Ÿï¼Ÿï¼Ÿ focal loss $L_{loc}$ anchor shape prediction predicts the best shape for each location best shapeï¼ša shape that lead to best iou with the nearest gt box ä¸€å±‚å·ç§¯ï¼š1x1 convï¼Œchannel2ï¼Œ[-1,1] transform layerï¼štransform direct [-1,1] outputs to real box shape $w = \sigma s e^{dw}$ $h = \sigma s e^{dh}$ sï¼šstride $\sigma$ï¼šç»éªŒå‚æ•°ï¼Œ8 in experiments set 9 pairs of (w,h) as RetinaNetï¼Œcalculate the IoU of these sampled anchors with gtï¼Œtake the max as target value bounded iou lossï¼š$L_{shape} = L_1(1-min(\frac{w}{w_g}, \frac{w_g}{w})) + L_1(1-min(\frac{h}{h_g}, \frac{h_g}{h}))$ feature adaptation intuitionï¼šthe feature corresponding to different size of anchor shapesåº”è¯¥encode different content region inputsï¼šfeature map &amp; anchor shape location-dependent transformationï¼š3x3 deformable conv deformable convçš„offsetæ˜¯anchor shapeå¾—åˆ°çš„ outputsï¼šadapted features with adapted features then perform further classification and bounding-box regression training jointly optimizeï¼š$L = \lambda_1 L_{loc} + \lambda_2 L_{shape} + L_{cls} + L_{reg}$ $\lambda_1=0.2ï¼Œ\lambda_2=0.5$ each level of feature map should only target objects of a specific scale rangeï¼šä½†æ˜¯ASFFè®ºæ–‡ä¸»å¼ è¯´è¿™ç§arrange by scaleçš„æ¨¡å¼ä¼šå¼•å…¥å‰èƒŒæ™¯inconsistencyï¼Ÿï¼Ÿ High-quality Proposals set a higher positive/negative threshold use fewer samples]]></content>
      <tags>
        <tag>ç›®æ ‡æ£€æµ‹ï¼Œone/two-stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ASFF]]></title>
    <url>%2F2021%2F01%2F25%2FASFF%2F</url>
    <content type="text"><![CDATA[Learning Spatial Fusion for Single-Shot Object Detection åŠ¨æœº inconsistency when fuse across different feature scales propose ASFF suppress the inconsistency spatially filter conflictive informationï¼šæƒ³æ³•åº”è¯¥è·ŸSSE-blockç±»ä¼¼ build on yolov3 introduce a bag of tricks anchor-free pipeline è®ºç‚¹ ssd is one of the first to generate pyramidal feature representations deeper layers reuse the formers bottom-up path small instances suffers low acc because containing insufficient semanic info FPN use top-down path shares rich semantics at all levels improvementï¼šmore strengthening feature fusion åœ¨ä½¿ç”¨FPNæ—¶ï¼Œé€šå¸¸ä¸åŒscaleçš„ç›®æ ‡ç»‘å®šåˆ°ä¸åŒçš„levelä¸Šé¢ inconsistencyï¼šå…¶ä»–levelçš„feature mapå¯¹åº”ä½ç½®çš„ä¿¡æ¯åˆ™ä¸ºèƒŒæ™¯ some methods set ignore region in adjacent features æ–¹æ³• introduce advanced techniques mixup cosine learning rate schedule sync-bn an anchor-free branch to run jointly with anchor-based ones L1 loss + IoU loss fusion å…¨è”æ¥è€Œéadjacent mergeï¼šä¸‰ä¸ªlevelçš„fuse mapéƒ½æ¥è‡ªä¸‰ä¸ªlevelçš„feature map ä¸Šé‡‡æ ·ï¼š 1x1 convï¼šå¯¹é½channel upsamp with interpolation ä¸‹é‡‡æ ·ï¼š s2ï¼š3x3 s2 conv s4ï¼šmaxpooling + 3x3 s2 conv adaptive fusion pixel levelçš„reweight shared across channelsï¼šhxwx1 å¯¹æ¥è‡ªä¸‰ä¸ªlevelçš„feature mapï¼Œresolutionå¯¹é½ä»¥åï¼Œåˆ†åˆ«1x1convï¼Œchannel 1 norm the weightsï¼šsoftmax ä¸ºå•¥èƒ½suppress inconsistencyï¼šä¸‰ä¸ªlevelçš„åƒç´ ç‚¹ï¼Œåªæ¿€æ´»ä¸€ä¸ªå¦å¤–ä¸¤ä¸ªæ˜¯0çš„æƒ…å†µæ˜¯ç»å¯¹ä¸harmçš„ï¼Œç›¸å½“äºä¸Šé¢ignoreé‚£ä¸ªæ–¹æ³•æ‹“å±•æˆadaptive training apply mixup on the classification pretraining of D53 turn off mixup augmentation for the last 30 epochs. inference the detection header at each level first predicts the shape of anchorsï¼Ÿï¼Ÿï¼Ÿè¿™ä¸ªä¸å¤ªæ‡‚ ASFF &amp; ASFF* enhanced version of ASFF by integrating other lightweight modules dropblock &amp; RFB å®ç° 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768class ASFF(nn.Module): def __init__(self, level, activate, rfb=False, vis=False): super(ASFF, self).__init__() self.level = level self.dim = [512, 256, 128] self.inter_dim = self.dim[self.level] if level == 0: self.stride_level_1 = conv_bn(256, self.inter_dim, kernel=3, stride=2) self.stride_level_2 = conv_bn(128, self.inter_dim, kernel=3, stride=2) self.expand = conv_bn(self.inter_dim, 512, kernel=3, stride=1) elif level == 1: self.compress_level_0 = conv_bn(512, self.inter_dim, kernel=1) self.stride_level_2 = conv_bn(128, self.inter_dim, kernel=3, stride=2) self.expand = conv_bn(self.inter_dim, 256, kernel=3, stride=1) elif level == 2: self.compress_level_0 = conv_bn(512, self.inter_dim, kernel=1, stride=1) self.compress_level_1= conv_bn(256,self.inter_dim,kernel=1,stride=1) self.expand = conv_bn(self.inter_dim, 128, kernel=3, stride=1) compress_c = 8 if rfb else 16 self.weight_level_0 = conv_bn(self.inter_dim, compress_c, 1, 1, 0) self.weight_level_1 = conv_bn(self.inter_dim, compress_c, 1, 1, 0) self.weight_level_2 = conv_bn(self.inter_dim, compress_c, 1, 1, 0) self.weight_levels = conv_bias(compress_c * 3, 3, kernel=1, stride=1, padding=0) self.vis = vis def forward(self, x_level_0, x_level_1, x_level_2): # è·Ÿè®ºæ–‡æè¿°ä¸€æ ·ï¼šä¸Šé‡‡æ ·å…ˆ1x1convå¯¹é½ï¼Œå†upinterpï¼Œä¸‹é‡‡æ ·3x3 s2 conv if self.level == 0: level_0_resized = x_level_0 level_1_resized = self.stride_level_1(x_level_1) level_2_downsampled_inter = F.max_pool2d(x_level_2, 3, stride=2, padding=1) level_2_resized = self.stride_level_2(level_2_downsampled_inter) elif self.level == 1: level_0_compressed = self.compress_level_0(x_level_0) sh = torch.tensor(level_0_compressed.shape[-2:])*2 level_0_resized = F.interpolate(level_0_compressed, tuple(sh), 'nearest') level_1_resized = x_level_1 level_2_resized = self.stride_level_2(x_level_2) elif self.level == 2: level_0_compressed = self.compress_level_0(x_level_0) sh = torch.tensor(level_0_compressed.shape[-2:])*4 level_0_resized = F.interpolate(level_0_compressed, tuple(sh), 'nearest') level_1_compressed = self.compress_level_1(x_level_1) sh = torch.tensor(level_1_compressed.shape[-2:])*2 level_1_resized = F.interpolate(level_1_compressed, tuple(sh),'nearest') level_2_resized = x_level_2 # è¿™é‡Œå¾—åˆ°çš„resizedç‰¹å¾å›¾ä¸ç›´æ¥è½¬æ¢æˆä¸€é€šé“çš„weighting mapï¼Œ # è€Œæ˜¯å…ˆ1x1convé™ç»´åˆ°8/16ï¼Œç„¶åconcatï¼Œç„¶å3x3ç”Ÿæˆ3é€šé“çš„weighting map # weighting mapç›¸å½“äºä¸€ä¸ªprediction headï¼Œæ‰€ä»¥æ˜¯conv_bias_softmaxï¼Œæ— bn level_0_weight_v = self.weight_level_0(level_0_resized) level_1_weight_v = self.weight_level_1(level_1_resized) level_2_weight_v = self.weight_level_2(level_2_resized) levels_weight_v = torch.cat((level_0_weight_v, level_1_weight_v, level_2_weight_v), 1) levels_weight = self.weight_levels(levels_weight_v) levels_weight = F.softmax(levels_weight, dim=1) # reweighting fused_out_reduced = level_0_resized * levels_weight[:, 0:1, :, :] + \ level_1_resized * levels_weight[:, 1:2, :, :] + \ level_2_resized * levels_weight[:, 2:, :, :] # 3x3çš„convï¼Œæ˜¯ç‰¹å¾å›¾å¹³æ»‘ out = self.expand(fused_out_reduced) if self.vis: return out, levels_weight, fused_out_reduced.sum(dim=1) else: return out]]></content>
      <tags>
        <tag>ç›®æ ‡æ£€æµ‹ï¼Œyolov3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VoVNet]]></title>
    <url>%2F2021%2F01%2F22%2FVoVNet%2F</url>
    <content type="text"><![CDATA[An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection åŠ¨æœº denseNet dense pathï¼šdiverse receptive fields heavy memory cost &amp; low efficiency we propose a backbone preserve the benefit of concatenation improve denseNet efficiency VoVNet comprised of One-Shot Aggregation (OSA) apply to one/two stage object detection tasks outperforms denseNet &amp; resNet based ones better small object detection performance è®ºç‚¹ main difference between resNet &amp; denseNet aggregationï¼šsummation &amp; concatenation summation would washed out the early features concatenation last as it preserves GPU parallel computation computing utilization is maximized when operand tensor is larger many 1x1 convs for reducing dimension dense connections in intermediate layers are inducing the inefficiencies VoVNet hypothesize that the dense connections are redundant OSAï¼šaggregates intermediate features at once test as object detection backboneï¼šoutperforms DenseNet &amp; ResNet with better energy efficiency and speed factors for efficiency FLOPS and model sizes are indirect metrics energy per image and frame per second are more practical MACï¼š memory accesses costï¼Œ$hw(c_i+c_o) + k^2 c_ic_o$ memory usageä¸æ­¢è·Ÿå‚æ•°é‡æœ‰å…³ï¼Œè¿˜è·Ÿç‰¹å¾å›¾å°ºå¯¸ç›¸å…³ MAC can be minimized when input channel size equals the output FLOPs/s splitting a large convolution operation into several fragmented smaller operations makes GPU computation inefficient as fewer computations are processed in parallel æ‰€ä»¥depthwise/bottleneckç†è®ºä¸Šé™ä½äº†è®¡ç®—é‡FLOPï¼Œä½†æ˜¯ä»GPUå¹¶è¡Œçš„è§’åº¦efficiencyé™ä½ï¼Œå¹¶æ²¡æœ‰æ˜¾è‘—æé€Ÿï¼šcause more sequential computations ä»¥æ—¶é—´ä¸ºå•ä½çš„FLOPsæ‰æ˜¯fairçš„ æ–¹æ³• hypothesize dense connection makes similar between neighbor layers redundant OSA dense connectionï¼šformer features concats in every following features one-shot connectionï¼šformer features concats once in the last feature æœ€å¼€å§‹è·Ÿdense blockä¿æŒå‚æ•°ä¸€è‡´ï¼šä¸€ä¸ªblocké‡Œé¢12ä¸ªlayersï¼Œchannel20ï¼Œå‘ç°æ·±å±‚ç‰¹å¾contributes lessï¼Œæ‰€ä»¥æ¢æˆæµ…å±‚ï¼Œ5ä¸ªlayersï¼Œchannel43ï¼Œå‘ç°æœ‰æ¶¨ç‚¹ï¼šimplies that building deep intermediate feature via dense connection is less effective than expected in/out channelæ•°ç›¸åŒ much less MACï¼š denseNet40ï¼š3.7M OSAï¼š5layersï¼Œchannel43ï¼Œ2.5M å¯¹äºhigher resolutionçš„detectionä»»åŠ¡impies more fast and energy efficient GPU efficiency ä¸éœ€è¦é‚£å¥½å‡ åä¸ª1x1 architecture stemï¼š3ä¸ª3x3conv downsampï¼šs2çš„maxpooling stagesï¼šincreasing channels enables more rich semantic high-level informationï¼Œbetter feature representation deeperï¼šmakes more modules in stage3/4 å®éªŒ one-stageï¼šrefineDet two-stageï¼šMask-RCNN]]></content>
  </entry>
  <entry>
    <title><![CDATA[GCN]]></title>
    <url>%2F2021%2F01%2F18%2FGCN%2F</url>
    <content type="text"><![CDATA[referenceï¼šhttps://mp.weixin.qq.com/s/SWQHgogAP164Kr082YkF4A å›¾ $G = (V,E)$ï¼šèŠ‚ç‚¹ &amp; è¾¹ï¼Œè¿é€šå›¾ &amp; å­¤ç«‹ç‚¹ é‚»æ¥çŸ©é˜µAï¼šNxNï¼Œæœ‰å‘ &amp; æ— å‘ åº¦çŸ©é˜µDï¼šNxNå¯¹è§’çŸ©é˜µï¼Œæ¯ä¸ªèŠ‚ç‚¹è¿æ¥çš„èŠ‚ç‚¹ ç‰¹å¾çŸ©é˜µXï¼šNxFï¼Œæ¯ä¸ª1-dim Fæ˜¯æ¯ä¸ªèŠ‚ç‚¹çš„ç‰¹å¾å‘é‡ ç‰¹å¾å­¦ä¹  å¯ä»¥ç±»æ¯”CNNï¼šå¯¹å…¶é‚»åŸŸï¼ˆkernelï¼‰å†…ç‰¹å¾è¿›è¡Œçº¿æ€§å˜æ¢ï¼ˆwåŠ æƒï¼‰ï¼Œç„¶åæ±‚å’Œï¼Œç„¶åæ¿€æ´»å‡½æ•° $H^{k+1} = f(H^{k},A) = \sigma(AH^{k}W^{k})$ Hï¼šrunning updating ç‰¹å¾çŸ©é˜µï¼ŒNxFk Aï¼š0-1é‚»æ¥çŸ©é˜µï¼ŒNxN Wï¼šæƒé‡ï¼Œ$F_k$x$F_{k+1}$ æƒé‡æ‰€æœ‰èŠ‚ç‚¹å…±äº« èŠ‚ç‚¹çš„é‚»æ¥èŠ‚ç‚¹å¯ä»¥çœ‹åšæ„Ÿå—é‡ ç½‘ç»œåŠ æ·±ï¼Œæ„Ÿå—é‡å¢å¤§ï¼šèŠ‚ç‚¹çš„ç‰¹å¾èåˆäº†æ›´å¤šèŠ‚ç‚¹çš„ä¿¡æ¯ å›¾å·ç§¯ Aä¸­æ²¡æœ‰è€ƒè™‘è‡ªå·±çš„ç‰¹å¾ï¼šæ·»åŠ è‡ªè¿æ¥ A = A + I åŠ æ³•è§„åˆ™å¯¹åº¦å¤§çš„èŠ‚ç‚¹ï¼Œç‰¹å¾ä¼šè¶Šæ¥è¶Šå¤§ï¼šå½’ä¸€åŒ– ä½¿å¾—é‚»æ¥çŸ©é˜µæ¯è¡Œå’Œä¸º1ï¼šå·¦ä¹˜åº¦çŸ©é˜µçš„é€† æ•°å­¦å®è´¨ï¼šæ±‚å¹³å‡ one step furtherï¼šä¸å•å¯¹è¡Œåšå¹³å‡ï¼Œå¯¹åº¦è¾ƒå¤§çš„é‚»æ¥èŠ‚ç‚¹ä¹Ÿåšpunish GCNç½‘ç»œ å®ç° weightsï¼šin x outï¼Œkaiming_uniform_initialize biasï¼šoutï¼Œzero_initialize activationï¼šrelu A x H x Wï¼šå·¦ä¹˜æ˜¯ç³»æ•°çŸ©é˜µä¹˜æ³• é‚»æ¥çŸ©é˜µçš„ç»“æ„ä»è¾“å…¥å¼€å§‹å°±ä¸å˜äº†ï¼Œå’Œæ¯å±‚çš„ç‰¹å¾çŸ©é˜µä¸€èµ·ä½œä¸ºè¾“å…¥ï¼Œä¼ å…¥GCN åˆ†ç±»å¤´ï¼šæœ€åä¸€å±‚é¢„æµ‹Nxn_classçš„ç‰¹å¾å‘é‡ï¼Œæå–æ„Ÿå…´è¶£èŠ‚ç‚¹F(n_class)ï¼Œç„¶åsoftmaxï¼Œå¯¹å…¶åˆ†ç±» å½’ä¸€åŒ– 123456789101112131415161718# å¯¹ç§°å½’ä¸€åŒ–def normalize_adj(adj): """compute L=D^-0.5 * (A+I) * D^-0.5""" adj += sp.eye(adj.shape[0]) degree = np.array(adj.sum(1)) d_hat = sp.diags(np.power(degree, -0.5).flatten()) norm_adj = d_hat.dot(adj).dot(d_hat) return norm_adj # å‡å€¼å½’ä¸€åŒ–def normalize_adj(adj): """compute L=D^-1 * (A+I)""" adj += sp.eye(adj.shape[0]) degree = np.array(adj.sum(1)) d_hat = sp.diags(np.power(degree, -1).flatten()) norm_adj = d_hat.dot(adj) return norm_adj åº”ç”¨åœºæ™¯ [åŠç›‘ç£åˆ†ç±»GCN]ï¼šSEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKSï¼Œæå‡ºGCN [skin GCN]ï¼šLearning Differential Diagnosis of Skin Conditions with Co-occurrence Supervision using Graph Convolutional Networksï¼Œä½“ç´ ï¼Œä¸€ä¸ªå•ç‹¬çš„åŸºäºå›¾çš„ç›¸å…³æ€§åˆ†æ”¯ï¼Œç»™featureåŠ æƒ [Graph Attention]ï¼šGraph Attention Networksï¼Œå›¾æ³¨æ„åŠ›ç½‘ç»œ Learning Differential Diagnosis of Skin Conditions with Co-occurrence Supervision using Graph Convolutional Networks åŠ¨æœº çš®è‚¤ç—…ï¼šå‘ç—…ç‡é«˜ï¼Œexpertså°‘ differential diagnosisï¼šé‰´åˆ«è¯Šæ–­ï¼Œå°±æ˜¯ä»ä¼—å¤šç–¾ç—…ç±»åˆ«ä¸­è·³å‡ºæ­£ç¡®ç±»åˆ« still challengingï¼štimely and accurate propose a DLS(deep learning system) clinical images multi-label classification 80 conditionsï¼Œè¦†ç›–ç—…ç§ labels incompletenessï¼šç”¨GCNå»ºæ¨¡æˆCo-occurrence supervisionï¼Œbenefit top5 è®ºç‚¹ googleçš„DLS 26ä¸­ç–¾ç—… å»ºæ¨¡æˆmulti-class classification problemï¼šé0å³1çš„å¤šæ ‡ç­¾è¡¨è¾¾ç ´åäº†ç±»åˆ«é—´çš„correlation our DLSï¼šGCN-CNN multi-label classification task over 80 conditions incomplete image labelsï¼šGCN that characterizes label co-occurrence supervision combine the classification network with the GCN æ•°æ®é‡ï¼š136,462 clinical images ç²¾åº¦ï¼štest on 12,378 user taken imagesï¼Œtop-5 acc 93.6% GCN original applicationï¼š nodes classificationï¼Œonly a small subset of nodes had their labels availableï¼šåŠç›‘ç£æ–‡æœ¬åˆ†ç±»é—®é¢˜ï¼Œåªæœ‰ä¸€éƒ¨åˆ†èŠ‚ç‚¹ç”¨äºè®­ç»ƒ the graph structure is contructed from data ML-GCNï¼š multi-label classification task correlation mapï¼ˆå›¾ç»“æ„ï¼‰åˆ™æ˜¯é€šè¿‡æ•°æ®ç›´æ¥å»ºç«‹ å›¾èŠ‚ç‚¹æ˜¯æ¯ä¸ªç±»åˆ«çš„semantic embeddings æ–¹æ³• overview ä¸€ä¸ªtrainableçš„CNNï¼Œå°†å›¾ç‰‡è½¬åŒ–æˆfeature vector ä¸€ä¸ªGCN branchï¼šä¸¤å±‚å›¾å·ç§¯ï¼Œéƒ½æ˜¯order-1ï¼Œå›¾ç»“æ„æ˜¯åŸºäºè®­ç»ƒé›†è®¡ç®—ï¼Œæ— å‘å›¾ï¼Œencodingçš„æ˜¯å›¾åƒlabelsä¹‹é—´çš„dependencyï¼Œç”¨å®ƒ implicitly supervises the classification task ç„¶åä¸¤ä¸ªfeature vectorç›¸ä¹˜ï¼Œç»™å‡ºæœ€ç»ˆç»“æœ GCN branch two graph convolutional (GC) layers ä¸€ç§estimatedå›¾ç»“æ„ï¼šbuild co-occurence graph using only training data node embed semantic meaning to labels è¾¹çš„å€¼å®šä¹‰æœ‰ç‚¹åƒç±»åˆ«é—´çš„ç›¸å…³æ€§å¼ºåº¦ï¼š$e_{ij} = 1(\frac{C(i,j)}{C(i)+C(j)} \geq t)$ï¼Œåˆ†å­æ˜¯æœ‰ä¸¤ç§æ ‡ç­¾çš„æ ·æœ¬é‡ï¼Œåˆ†æ¯æ˜¯å„è‡ªæ ·æœ¬é‡ ä¸€ç§designedå›¾ç»“æ„ï¼šintial valueæ˜¯åŸºäºæœ‰ç»éªŒçš„ä¸“å®¶æ„å»º node representation graph branchçš„è¾“å…¥ label embedding ç”¨äº†BioSentVecï¼Œä¸€ä¸ªåŸºäºç”Ÿç‰©åŒ»å­¦è¯­æ–™åº“è®­ç»ƒçš„word bag GCN randomly initialize GCN-0ï¼šdim 700 GCN-1ï¼šdim 1024 GCN-2ï¼šdim 2048 æœ€ç»ˆå¾—åˆ°(cls,2048)çš„node features cls branch inputï¼šdownsized to 448x448 resnet101ï¼šæ‰§è¡Œåˆ°FC-2048ï¼Œä½œä¸ºimage features å…ˆè®­ç»ƒ300 epochsï¼Œlr 0.1ï¼Œstep decay GCN-CNN å…ˆé¢„è®­ç»ƒresnet backboneï¼Œ ç„¶åæ•´ä½“ä¸€èµ·è®­ç»ƒ300 epochsï¼Œlr 0.0003ï¼Œ image featureå’Œnode featuresé€šè¿‡dot productèåˆï¼Œå¾—åˆ°(cls, )çš„cls vecï¼Œ å®éªŒ å›¾ç»“æ„ä¸èƒ½random initializationï¼Œä¼šä½¿ç»“æœå˜å·® åŸºäºæ•°æ®é›†ä¼°è®¡çš„graph initializationæœ‰æ˜¾è‘—æå‡ åŸºäºä¸“å®¶è®¾è®¡çš„graph initializationæœ‰è¿›ä¸€æ­¥æå‡ï¼Œä½†æ˜¯ä¸æ˜æ˜¾ï¼Œè€ƒè™‘åˆ°æ ‡æ³¨å·¥ä½œç¹é‡ä¸å¤ªæ¨è SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS reference http://tkipf.github.io/graph-convolutional-networks/ï¼Œå®˜æ–¹åšå®¢ https://zhuanlan.zhihu.com/p/35630785ï¼ŒçŸ¥ä¹ç¬”è®° è®ºç‚¹ åœºæ™¯ semi-supervised learning on graph-structured data æ¯”å¦‚ï¼šåœ¨ä¸€ä¸ªcitation networkï¼Œclassifying nodes (such as documents)ï¼Œlabels are only available for a small subset of nodesï¼Œä»»åŠ¡çš„ç›®æ ‡æ˜¯å¯¹å¤§éƒ¨åˆ†æœªæ ‡è®°çš„èŠ‚ç‚¹é¢„æµ‹ç±»åˆ« previous approach Standard Approach lossç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šå•ä¸ªèŠ‚ç‚¹çš„fitting errorï¼Œå’Œç›¸é‚»èŠ‚ç‚¹çš„distance error åŸºäºä¸€ä¸ªå‡è®¾ï¼šç›¸é‚»èŠ‚ç‚¹é—´çš„labelç›¸ä¼¼ é™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ› Embedding-based Approach åˆ†ä¸¤æ­¥è¿›è¡Œï¼šå…ˆå­¦ä¹ èŠ‚ç‚¹çš„embeddingï¼Œå†åŸºäºembeddingè®­ç»ƒåˆ†ç±»å™¨ ä¸end-to-endï¼Œä¸¤ä¸ªtaskåˆ†åˆ«æ‰§è¡Œï¼Œä¸èƒ½ä¿è¯å­¦åˆ°çš„embeddingæ˜¯é€‚åˆç¬¬äºŒä¸ªä»»åŠ¡çš„ æ€è·¯ train on a supervised target for nodes with labels ç„¶åé€šè¿‡å›¾çš„è¿é€šæ€§ï¼Œtrainable adjacency matrixï¼Œä¼ é€’æ¢¯åº¦ç»™unlabeled nodes ä½¿å¾—å…¨å›¾å¾—åˆ°ç›‘ç£ä¿¡æ¯ contributions introduce a layer-wise propagation ruleï¼Œä½¿å¾—ç¥ç»ç½‘ç»œèƒ½å¤Ÿoperate on graphï¼Œå®ç°end-to-endçš„å›¾ç»“æ„åˆ†ç±»å™¨ use this graph-based neural network modelï¼Œè®­ç»ƒä¸€ä¸ªsemi-supervised classification of nodesçš„ä»»åŠ¡ æ–¹æ³• fast approximate convolutions on graphs givenï¼š layer inputï¼š$H^l$ layer outputï¼š$H^{l+1}$ kernel patternï¼š$A$ï¼Œåœ¨å·ç§¯é‡Œé¢æ˜¯fixed kxk æ–¹æ ¼ï¼Œåœ¨å›¾é‡Œé¢å°±æ˜¯è‡ªç”±åº¦æ›´é«˜çš„é‚»æ¥çŸ©é˜µ kernel weightsï¼š$W$ general layer formï¼š$H^{l+1}=f(H^l,A)$ inspirationï¼šå·ç§¯å…¶å®æ˜¯ä¸€ç§ç‰¹æ®Šçš„å›¾ï¼Œæ¯ä¸ªgridçœ‹ä½œä¸€ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½åŠ ä¸Šå…¶é‚»å±…èŠ‚ç‚¹çš„ä¿¡æ¯ï¼Œä¹Ÿå°±æ˜¯ï¼š H^{l+1}=\sigma(AH^lW) Wæ˜¯åœ¨å¯¹gridsåŠ æƒ Aæ˜¯åœ¨å¯¹æ¯ä¸ªgridsåŠ ä¸Šä»–çš„é‚»æ¥èŠ‚ç‚¹ details in practice è‡ªç¯ï¼šä¿ç•™è‡ªèº«èŠ‚ç‚¹ä¿¡æ¯ï¼Œ$\hat A=A+I$ æ­£åˆ™åŒ–ï¼šstabilize the scaleï¼Œ$H^{l+1}=\sigma(\hat D^{-\frac{1}{2}}\hat A\hat D^{-\frac{1}{2}}H^lW)$ ä¸€ä¸ªå®éªŒï¼šåªåˆ©ç”¨å›¾çš„é‚»æ¥çŸ©é˜µï¼Œå°±èƒ½å¤Ÿå­¦å¾—æ•ˆæœä¸é”™ semi-supervised node classification æ€è·¯å°±æ˜¯åœ¨æ‰€æœ‰æœ‰æ ‡ç­¾èŠ‚ç‚¹ä¸Šè®¡ç®—äº¤å‰ç†µloss æ¨¡å‹ç»“æ„ inputï¼šXï¼Œ(b,N,D) ä¸¤å±‚å›¾å·ç§¯ GCN1-reluï¼šhidden Fï¼Œ(b,N,F) GCN2-softmaxï¼šoutput Zï¼Œ(b,N,cls) è®¡ç®—äº¤å‰ç†µ code torch/keras/tfå®˜æ–¹éƒ½æœ‰ï¼š https://github.com/tkipf/gcnï¼Œè®ºæ–‡é‡Œç»™çš„tfè¿™ä¸ªé“¾æ¥ torchå’Œkerasçš„readmeé‡Œé¢æœ‰è¯´æ˜ï¼Œinitialization scheme, dropout scheme, and dataset splitså’Œtfç‰ˆæœ¬ä¸åŒï¼Œä¸æ˜¯ç”¨æ¥å¤ç°è®ºæ–‡ python setup.py bdist_wheel æ•°æ®é›†ï¼šCora datasetï¼Œæ˜¯ä¸€ä¸ªå›¾æ•°æ®é›†ï¼Œç”¨äºåˆ†ç±»ä»»åŠ¡ï¼Œæ•°æ®é›†ä»‹ç»https://blog.csdn.net/yeziand01/article/details/93374216 cora.contentæ˜¯æ‰€æœ‰è®ºæ–‡çš„ç‹¬è‡ªçš„ä¿¡æ¯ï¼Œæ€»å…±2708ä¸ªæ ·æœ¬ï¼Œæ¯ä¸€è¡Œéƒ½æ˜¯è®ºæ–‡ç¼–å·+è¯å‘é‡1433-dim+è®ºæ–‡ç±»åˆ« cora.citesæ˜¯è®ºæ–‡ä¹‹é—´çš„å¼•ç”¨è®°å½•ï¼ŒA to Bçš„reflect pairï¼Œ5429è¡Œï¼Œç”¨äºåˆ›å»ºé‚»æ¥çŸ©é˜µ]]></content>
      <tags>
        <tag>å›¾å·ç§¯ï¼Œgraph-conv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[transformers]]></title>
    <url>%2F2021%2F01%2F18%2Ftransformers%2F</url>
    <content type="text"><![CDATA[startupreference1ï¼šhttps://mp.weixin.qq.com/s/Rm899vLhmZ5eCjuy6mW_HA reference2ï¼šhttps://zhuanlan.zhihu.com/p/308301901 NLP &amp; RNN æ–‡æœ¬æ¶‰åŠä¸Šä¸‹æ–‡å…³ç³» RNNæ—¶åºä¸²è¡Œï¼Œå»ºç«‹å‰åå…³ç³» ç¼ºç‚¹ï¼šå¯¹è¶…é•¿ä¾èµ–å…³ç³»å¤±æ•ˆï¼Œä¸å¥½å¹¶è¡ŒåŒ– NLP &amp; CNN æ–‡æœ¬æ˜¯1ç»´æ—¶é—´åºåˆ— 1D CNNï¼Œå¹¶è¡Œè®¡ç®— ç¼ºç‚¹ï¼šCNNæ“…é•¿å±€éƒ¨ä¿¡æ¯ï¼Œå·ç§¯æ ¸å°ºå¯¸å’Œé•¿è·ç¦»ä¾èµ–çš„balance NLP &amp; transformer å¯¹æµå…¥çš„æ¯ä¸ªå•è¯ï¼Œå»ºç«‹å…¶å¯¹è¯åº“çš„æƒé‡æ˜ å°„ï¼Œæƒé‡ä»£è¡¨attention è‡ªæ³¨æ„åŠ›æœºåˆ¶ å»ºç«‹é•¿è·ç¦»ä¾èµ– put in CV æ’å…¥ç±»ä¼¼çš„è‡ªæ³¨æ„åŠ›å±‚ å®Œå…¨æŠ›å¼ƒå·ç§¯å±‚ï¼Œä½¿ç”¨Transformers RNN &amp; LSTM &amp; GRU cell æ ‡å‡†è¦ç´ ï¼šè¾“å…¥xã€è¾“å‡ºyã€éšå±‚çŠ¶æ€h RNN RNN cellæ¯æ¬¡æ¥æ”¶ä¸€ä¸ªå½“å‰è¾“å…¥$x_t$ï¼Œå’Œå‰ä¸€æ­¥çš„éšå±‚è¾“å‡º$h_{t-1}$ï¼Œç„¶åäº§ç”Ÿä¸€ä¸ªæ–°çš„éšå±‚çŠ¶æ€$h_t$ï¼Œä¹Ÿæ˜¯å½“å‰çš„è¾“å‡º$y_t$ formulationï¼š$y_t, h_t = f(x_t, h_{t-1})$ same parameters for each time stepï¼šåŒä¸€ä¸ªcellæ¯ä¸ªtime stepçš„æƒé‡å…±äº« ä¸€ä¸ªé—®é¢˜ï¼šæ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ è€ƒè™‘hidden statesâ€™ chainçš„ç®€åŒ–å½¢å¼ï¼š$h_t = \theta^t h_0$ï¼Œä¸€ä¸ªsequence forwardä¸‹å»å°±æ˜¯same weights multiplied over and over again å¦å¤–tanhä¹Ÿæ˜¯ä¼šè®©ç¥ç»å…ƒæ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ LSTM key ingredient cellï¼šå¢åŠ äº†ä¸€æ¡cell state workflowï¼Œä¼˜åŒ–æ¢¯åº¦æµ gateï¼šé€šè¿‡é—¨ç»“æ„åˆ é€‰æºå¸¦ä¿¡æ¯ï¼Œä¼˜åŒ–é•¿è·ç¦»å…³è” å¯ä»¥çœ‹åˆ°LSTMçš„å¾ªç¯çŠ¶æ€æœ‰ä¸¤ä¸ªï¼šç»†èƒçŠ¶æ€$c_t$å’Œéšå±‚çŠ¶æ€$h_t$ï¼Œè¾“å‡ºçš„$y_t$ä»æ—§æ˜¯$h_t$ GRU LSTMçš„å˜ä½“ï¼Œä»æ—§æ˜¯é—¨ç»“æ„ï¼Œæ¯”LSTMç»“æ„ç®€å•ï¼Œå‚æ•°é‡å°ï¼Œæ®è¯´æ›´å¥½è®­ç»ƒ papers [ä¸€ä¸ªåˆ—äº†å¾ˆå¤šè®ºæ–‡çš„ä¸»é¡µ] https://github.com/dk-liang/Awesome-Visual-Transformer [ç»å…¸è€ƒå¤] â€‹ * [Seq2Seq 2014] Sequence to Sequence Learning with Neural Networksï¼ŒGoogleï¼Œæœ€æ—©çš„encoder-decoder stacking LSTMç”¨äºæœºç¿» â€‹ * [self-attention/Transformer 2017] Transformer: Attention Is All You Needï¼ŒGoogleï¼Œ â€‹ * [bert 2019] BERT: Pre-training of Deep Bidirectional Transformers for Language Understandingï¼ŒGoogleï¼ŒNLPï¼Œè¾“å…¥single sentence/patched sentencesï¼Œç”¨Transformer encoderæå–bidirectional cross sentence representationï¼Œç”¨è¾“å‡ºçš„ç¬¬ä¸€ä¸ªlogitè¿›è¡Œåˆ†ç±» [ç»¼è¿°] â€‹ * [ç»¼è¿°2020] Efficient Transformers: A Surveyï¼ŒGoogleï¼Œ â€‹ * [ç»¼è¿°2021] Transformers in Vision: A Surveyï¼Œè¿ªæ‹œï¼Œ â€‹ * [ç»¼è¿°2021] A Survey on Visual Transformerï¼Œåä¸ºï¼Œ [classification] â€‹ * [ViT 2020] AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALEï¼ŒGoogleï¼Œåˆ†ç±»ä»»åŠ¡ï¼Œç”¨transformerçš„encoderæ›¿æ¢CNNå†åŠ åˆ†ç±»å¤´ï¼Œæ¯ä¸ªfeature patchä½œä¸ºä¸€ä¸ªinput embeddingï¼Œchannel dimæ˜¯vector dimï¼Œå¯ä»¥çœ‹åˆ°è·ŸbertåŸºæœ¬ä¸€æ ·ï¼Œå°±æ˜¯input sequenceæ¢æˆpatchï¼Œåç»­åŸºäºå®ƒçš„æå‡æœ‰DeiTã€LV-ViT â€‹ * [BotNet 2021] Bottleneck Transformers for Visual Recognitionï¼ŒGoogleï¼Œå°†CNN backboneæœ€åå‡ ä¸ªstageæ›¿æ¢æˆMSA â€‹ * [CvT 2021] CvT: Introducing Convolutions to Vision Transformersï¼Œå¾®è½¯ï¼Œ [detection] â€‹ * [DeTR 2020] DeTR: End-to-End Object Detection with Transformersï¼ŒFacebookï¼Œç›®æ ‡æ£€æµ‹ï¼ŒCNN+transformer(en-de)+é¢„æµ‹å¤´ï¼Œæ¯ä¸ªfeature pixelä½œä¸ºä¸€ä¸ªinput embeddingï¼Œchannel dimæ˜¯vector dim â€‹ * [Swin 2021] Swin Transformer: Hierarchical Vision Transformer using Shifted Windowsï¼Œå¾®è½¯ [segmentation] â€‹ * [SETR] Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformersï¼Œå¤æ—¦ï¼Œæ°´ï¼Œæ„Ÿè§‰å°±æ˜¯æŠŠFCNçš„backæ¢æˆtransformer [Unet+Transformer]ï¼š â€‹ * [UNETR 2021] UNETR: Transformers for 3D Medical Image Segmentationï¼Œè‹±ä¼Ÿè¾¾ï¼Œç›´æ¥ä½¿ç”¨transformer encoderåšunet encoder â€‹ * [TransUNet 2021] TransUNet: Transformers Make Strong Encoders for Medical Image Segmentationï¼Œencoder streamé‡Œé¢åŠ transformer block â€‹ * [TransFuse 2021] TransFuse: Fusing Transformers and CNNs for Medical Image Segmentationï¼Œå¤§å­¦ï¼ŒCNN featureå’ŒTransformer featureè¿›è¡Œbifusion Sequence to Sequence [a keras tutorial][https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html] general case extract the information of the entire input sequence then start generate the output sequence seq2seq model workflow a (stacking of) RNN layer acts as encoder processes the input sequence returns its own internal stateï¼šä¸è¦RNNçš„outputsï¼Œåªè¦internal states encoderç¼–ç å¾—åˆ°çš„ä¸œè¥¿å«Context Vector a (stacking of) RNN layer acts as decoder given previous characters of the target sequence it is trained to predict the next characters of the target sequence teacher forcingï¼š è¾“å…¥æ˜¯target sequenceï¼Œè®­ç»ƒç›®æ ‡æ˜¯ä½¿æ¨¡å‹è¾“å‡ºoffset by one timestepçš„target sequence ä¹Ÿå¯ä»¥ä¸teacher forcingï¼šç›´æ¥æŠŠé¢„æµ‹ä½œä¸ºnext stepçš„è¾“å…¥ Context Vectorçš„åŒè´¨æ€§ï¼šæ¯ä¸ªstepï¼Œdecoderéƒ½è¯»å–ä¸€æ ·çš„Context Vectorä½œä¸ºinitial_state when inference ç¬¬ä¸€æ­¥è·å–input sequenceçš„state vectors repeat ç»™decoderè¾“å…¥input stateså’Œout sequence(begin with a èµ·å§‹ç¬¦) ä»predictionä¸­æ‹¿åˆ°next character append the character to the output sequence untilï¼šå¾—åˆ°end character / hit the character limit implementation https://github.com/AmberzzZZ/transformer/blob/master/seq2seq.py one step further æ”¹è¿›æ–¹å‘ bi-directional RNNï¼šç²—æš´åè½¬åºåˆ—ï¼Œæœ‰æ•ˆæ¶¨ç‚¹ attentionï¼šæœ¬è´¨æ˜¯å°†encoderçš„è¾“å‡ºContext VectoråŠ æƒ ConvS2Sï¼šè¿˜æ²¡çœ‹ ä¸»è¦éƒ½æ˜¯é’ˆå¯¹RNNçš„ç¼ºé™·æå‡º åŠ¨æœº present a general end-to-end sequence learning approach multi-layered LSTMs encode the input seq to a fix-dim vector decode the target seq from the fix-dim vector LSTM did not have difficulty on long sentences reversing the order of the words improved performance æ–¹æ³• standard RNN given a sequence $(x_1, x_2, â€¦, x_T)$ iteratingï¼š h_t = sigm(W^{hx} x_t + W^{hh}h_{t-1})\\ y_t = W^{yh} h_t å¦‚æœè¾“å…¥ã€è¾“å‡ºçš„é•¿åº¦äº‹å…ˆå·²çŸ¥ä¸”å›ºå®šï¼Œä¸€ä¸ªRNNç½‘ç»œå°±èƒ½å»ºæ¨¡seq2seq modeläº† å¦‚æœè¾“å…¥ã€è¾“å‡ºçš„é•¿åº¦ä¸åŒã€å¹¶ä¸”æœä»ä¸€äº›æ›´å¤æ‚çš„å…³ç³»ï¼Ÿå°±å¾—ç”¨ä¸¤ä¸ªRNNç½‘ç»œï¼Œä¸€ä¸ªå°†input seqæ˜ å°„æˆfixed-sized vectorï¼Œå¦ä¸€ä¸ªå°†vectoræ˜ å°„æˆoutput seqï¼Œbut long-term-dependency issue LSTM LSTMæ˜¯å§‹ç»ˆå¸¦ç€å…¨éƒ¨seqçš„ä¿¡æ¯çš„ï¼Œå¦‚ä¸Šå›¾é‚£æ · our actual model use two LSTMsï¼šencoder-decoderèƒ½å¤Ÿå¢åŠ å‚æ•°é‡ an LSTM with four layersï¼šdeeper input sequenceå€’åºï¼šçœŸæ­£çš„å¥é¦–æ›´æ¥è¿‘transçš„å¥é¦–ï¼Œmakes it easy for SGD to establish communication training details LSTMï¼š4 layersï¼Œ1000 cells word-embeddingï¼š1000-dimï¼Œ(input vocab 160,000, output vocab 80,000) naive softmax uniform initializationï¼š(-0.08, 0.08) SGDï¼Œlr=0.7ï¼Œhalf by every half epochï¼Œtotal 7.5 epochs gradient norm [10, 25] all sentences in a minibatch are roughly of the same length Transformer: Attention Is All You Need åŠ¨æœº sequence2sequence models encoder + decoder RNN / CNN + an attention path we propose Transformer base solely on attention mechanisms more parallelizable and less training time è®ºç‚¹ sequence modeling ä¸»æµï¼šRNNï¼ŒLSTMï¼Œgated align the positions to computing time steps sequentialæœ¬è´¨é˜»ç¢å¹¶è¡ŒåŒ– Attention mechanisms acts as a integral part in previous work used in conjunction with the RNN ä¸ºäº†å¹¶è¡ŒåŒ– some methods use CNN as basic building blocks difficult to learn dependencies between distant positions we propose Transformer rely entirely on an attention mechanism draw global dependencies self-attention relating different positions of a single sequence to generate a overall representation of the sequence æ–¹æ³• encoder-decoder encoderï¼šdoc2emb given an input sequence of symbol representation $(x_1, x_2, â€¦, x_n)$ map to a sequence of continuous representations $(z_1, z_2, â€¦, z_n)$ï¼Œ(embeddings) decoderï¼šhidden layers given embeddings z generate an output sequence $(y_1, y_2, â€¦, y_m)$ one element at a time the previous generated symbols are served as additional input when computing the current time step Transformer Architecture Transformer use for both encoder and decoder stacked self-attention and point-wise fully-connected layers encoder N=6 identical layers each layer has 2 sub-layers multi-head self-attention mechanism postision-wise fully connected layer residual for two sub-layers independently add &amp; layer norm d=512 decoder N=6 identical layers 3 sub-layers [new] masked multi-head self-attentionï¼šcombineäº†å…ˆéªŒçŸ¥è¯†ï¼Œoutput embeddingåªèƒ½åŸºäºåœ¨å®ƒä¹‹å‰çš„time-stepçš„embeddingè®¡ç®— multi-head self-attention mechanism postision-wise fully connected layer residual attention referenceï¼šhttps://bbs.cvmart.net/articles/4032 step1ï¼šproject embedding to query-key-value pairs $Q = W_Q^{dd} A^{dN}$ $K = W_K^{dd} A^{dN}$ $V = W_V^{dd} A^{dN}$ step2ï¼šscaled dot-product attention $A^{N*N}=softmax(K^TQ/\sqrt{d})$ $B^{dN} = V^{dN}A^{N*N}$ multi-head attention ä»¥ä¸Šçš„step1&amp;step2æ“ä½œperforms a single attention function äº‹å®ä¸Šæˆ‘ä»¬å¯ä»¥ç”¨å¤šç»„projectionå¾—åˆ°å¤šç»„$\{Q,K,V\}^h$ï¼Œin parallelåœ°æ‰§è¡Œattentionè¿ç®—ï¼Œå¾—åˆ°å¤šç»„$\{B^{d*N}\}^h$ concat &amp; project concat in d-dimï¼š$B\in R^{(dh)N}$ linear projectï¼š$out = W^{d(dh)} B$ h=8 $d_{in}/h=64$ï¼šembeddingçš„dim $d_{out}=64$ï¼šquery-key-valueçš„dim positional encoding æ•°å­¦æœ¬è´¨æ˜¯ä¸€ä¸ªhand-craftedçš„æ˜ å°„çŸ©é˜µ$W^P$å’Œone-hotçš„ç¼–ç å‘é‡$p$ï¼š \left[ \begin{array}{ccc} a\\ e \end{array} \right ] = [W^I, W^P] \left[ \begin{array}{ccc} x\\ p \end{array} \right ] ç”¨PEè¡¨ç¤ºe posæ˜¯sequence xä¸Šçš„position 2iå’Œ2i+1æ˜¯embedding aä¸Šçš„idx point-wise feed-forward network fc-ReLU-fc dim_fc=2048 dim_in &amp; dim_out = 512 è¿è¡Œè¿‡ç¨‹ encoderæ˜¯å¯ä»¥å¹¶è¡Œè®¡ç®—çš„ è¾“å…¥æ˜¯sequence embeddingå’Œpositional embeddingï¼š$A\in R^{d*N}$ ç»è¿‡repeated blocks è¾“å‡ºæ˜¯å¦å¤–ä¸€ä¸ªsequenceï¼š$B\in R^{d*N}$ self-attentionï¼šQã€Kã€Væ˜¯ä¸€ä¸ªä¸œè¥¿ encoderçš„æœ¬è´¨å°±æ˜¯åœ¨è§£æè‡ªæ³¨æ„åŠ›ï¼š å¹¶è¡Œçš„å…¨å±€ä¸¤ä¸¤æ¯”è¾ƒï¼Œä¸€æ­¥åˆ°ä½ RNNè¦by step CNNè¦stack layers decoderæ˜¯åœ¨è®­ç»ƒé˜¶æ®µæ˜¯å¯ä»¥å¹¶è¡Œçš„ï¼Œåœ¨inferenceé˜¶æ®µby step è¾“å…¥æ˜¯encoderçš„è¾“å‡ºå’Œä¸Šä¸€ä¸ªtime-step decoderçš„è¾“å‡ºembedding è¾“å‡ºæ˜¯å½“å‰time-stepå¯¹åº”positionçš„è¾“å‡ºè¯çš„æ¦‚ç‡ ç¬¬ä¸€ä¸ªattention layeræ˜¯out embeddingçš„self-attentionï¼šè¦å®ç°åƒRNNä¸€æ ·ä¾æ¬¡è§£ç å‡ºæ¥ï¼Œæ¯ä¸ªtime stepè¦ç”¨åˆ°ä¸Šä¸€ä¸ªä½ç½®çš„è¾“å‡ºä½œä¸ºè¾“å…¥â€”â€”masking givenè¾“å…¥sequenceæ˜¯\ I have a catï¼Œ5ä¸ªå…ƒç´  é‚£ä¹ˆmaskå°±æ˜¯$R^{5*5}$çš„ä¸‹ä¸‰è§’çŸ©é˜µ è¾“å…¥embeddingç»è¿‡transformationå˜æˆQã€Kã€Vä¸‰ä¸ªçŸ©é˜µ ä»æ—§æ˜¯$A=K^TQ$è®¡ç®—attention è¿™é‡Œæœ‰ä¸€äº›attentionæ˜¯éæ³•çš„ï¼šä½ç½®é å‰çš„queryåªèƒ½ç”¨åˆ°æ¯”ä»–ä½ç½®æ›´é å‰çš„queryï¼Œå› æ­¤è¦ä¹˜ä¸ŠmaskçŸ©é˜µï¼š$A=M A$ softmaxï¼š$A=softmax(A)$ scaleï¼š$B = VA$ concat &amp; projection ç¬¬äºŒä¸ªattention layeræ˜¯in &amp; out sequenceçš„æ³¨æ„åŠ›ï¼Œå…¶keyå’Œvalueæ¥è‡ªencoderï¼Œqueryæ¥è‡ªä¸Šä¸€ä¸ªdecoder blockçš„è¾“å‡º why self-attention è¡¡é‡ç»´åº¦ total computational complexity per layer amount of computation that can be parallelized path-length between long-range dependencies given input sequence with length N &amp; dim $d_{in}$ï¼Œoutput sequence with dim $d_{out}$ RNN need N sequencial operations of $W\in R^{d_{in} * d_{out}}$ CNN need N/k stacking layers of $d_{in}d_{out}$ sequence operations of $W\in R^{kk}$ï¼Œgenerallyæ˜¯RNNçš„kå€ training optimizerï¼š$Adam(lr, \beta_1=0.9, \beta_2=0.98, \epsilon=10^{-9})$ lrscheduleï¼šwarmup by 4000 stepsï¼Œthen decay dropout residual dropoutï¼šå°±æ˜¯stochastic depth dropout to the sum of embeddings &amp; PE for both encoder and decoder drop_rate = 0.1 label smoothingï¼šsmooth_factor = 0.1 å®éªŒ Aï¼švary the number of attention headsï¼Œå‘ç°å¤šäº†å°‘äº†éƒ½hurts Bï¼šreduce the dim of attention keyï¼Œå‘ç°hurts C &amp; Dï¼šå¤§æ¨¡å‹+dropout helps Eï¼šlearnable &amp; sincos PEï¼šnearly identical æœ€åæ˜¯big modelçš„å‚æ•° BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding åŠ¨æœº BERTï¼šBidirectional Encoder Representations from Transformers Bidirectional Encoder Representations Transformers workflow pretrain bidirectional representations from unlabeled text tune with one additional output layer to obtain the model SOTA GLUE score 80.5% è®ºç‚¹ pretraining is effective in NLP tasks feature-based methodï¼šuse task-specfic architecturesï¼Œä»…ä½¿ç”¨pretrained modelçš„ç‰¹å¾ fine-tuining methodï¼šç›´æ¥fine-tuneé¢„è®­ç»ƒæ¨¡å‹ ä¸¤ç§æ–¹æ³•åœ¨é¢„è®­ç»ƒé˜¶æ®µè®­ç»ƒç›®æ ‡ä¸€è‡´ï¼šuse unidirectional language models to learn general language representations reduce the need for many heavily-engineered task- specific architectures current methodsâ€™ limitations unidirectionalï¼š limit the choice of architectures äº‹å®ä¸Štokençš„ä¸Šä¸‹æ–‡éƒ½å¾ˆé‡è¦ï¼Œä¸èƒ½åªçœ‹ä¸Šæ–‡ ç®€å•çš„concatä¸¤ä¸ªindependentçš„L2Rå’ŒR2Læ¨¡å‹ï¼ˆbiRNNï¼‰ independent shallow concat BERT masked language modelï¼šåœ¨ä¸€ä¸ªsequenceä¸­é¢„æµ‹è¢«é®æŒ¡çš„è¯ next sentence predictionï¼štrains text-pair representations æ–¹æ³• two steps pre-training unlabeled data different pretraining tasks fine-tuning labeled data of the downstream tasks fine-tune all the params ä¸¤ä¸ªé˜¶æ®µçš„æ¨¡å‹ï¼Œåªæœ‰è¾“å‡ºå±‚ä¸åŒ ä¾‹å¦‚é—®ç­”æ¨¡å‹ pretrainingé˜¶æ®µï¼Œè¾“å…¥æ˜¯ä¸¤ä¸ªsentenceï¼Œè¾“å…¥çš„èµ·å§‹æœ‰ä¸€ä¸ªCLS symbolï¼Œä¸¤ä¸ªå¥å­çš„åˆ†éš”æœ‰ä¸€ä¸ªSEP symbol fine-tuningé˜¶æ®µï¼Œè¾“å…¥åˆ†åˆ«æ˜¯é—®å’Œç­”ï¼Œã€è¾“å‡ºæ˜¯å•¥ï¼Ÿã€‘ architecture multi-layer bidirectional Transformer encoder number of transfomer blocks L hidden size H number of self-attention heads A FFN dim 4H Bert baseï¼šL=12ï¼ŒH=768ï¼ŒA=12 Bert largeï¼šL=24ï¼ŒH=1024ï¼ŒA=16 input/output representations a single sentence / two packed up sentenceï¼š æ‹¼æ¥çš„sentenceç”¨ç‰¹æ®Štoken SEPè¡”æ¥ segment embeddingï¼šåŒæ—¶add a learned embedding to every token indicating who it belongs use WordPiece embeddings with 30000 token vocabulary è¾“å…¥sequenceçš„ç¬¬ä¸€ä¸ªtokenæ°¸è¿œæ˜¯ä¸€ä¸ªç‰¹æ®Šç¬¦å·CLSï¼Œå®ƒå¯¹åº”çš„final stateè¾“å‡ºä½œä¸ºsentenceæ•´ä½“çš„representationï¼Œç”¨äºåˆ†ç±»ä»»åŠ¡ overallç½‘ç»œçš„input representationæ˜¯é€šè¿‡å°†token embeddingsæ‹¼æ¥ä¸Šä¸Šç‰¹æ®Šç¬¦å·ï¼ŒåŠ ä¸ŠSEå’ŒPEå¾—åˆ° pre-training two unsupervised tasks Masked LM (MLM) mask some percentage of the input tokens at randomï¼š15% 80%çš„æ¦‚ç‡ç”¨MASK tokenæ›¿æ¢ 10%çš„æ¦‚ç‡ç”¨random tokenæ›¿æ¢ 10%çš„æ¦‚ç‡unchanged then predict those masked tokens the final hidden states corresponding to the masked tokens are fed into a softmax ç›¸æ¯”è¾ƒäºä¼ ç»Ÿçš„left2right/right2left/concatæ¨¡å‹ æ—¢æœ‰å‰æ–‡åˆæœ‰åæ–‡ åªé¢„æµ‹masked tokenï¼Œè€Œä¸æ˜¯å…¨å¥é¢„æµ‹ Next Sentence Prediction (NSP) å¯¹äºrelationship between sentencesï¼š ä¾‹å¦‚question&amp;answerï¼Œå¥å­æ¨æ–­ not direatly captured by language modelingï¼Œæ¨¡å‹ç›´è§‚å­¦ä¹ çš„æ˜¯token relationship binarized next sentence prediction task é€‰å–sentence A&amp;Bï¼š 50%çš„æ¦‚ç‡æ˜¯çœŸçš„ä¸Šä¸‹æ–‡ï¼ˆIsNextï¼‰ 50%çš„æ¦‚ç‡æ˜¯randomï¼ˆNotNextï¼‰ æ„æˆäº†ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ï¼šä»æ—§ç”¨CLS tokenå¯¹åº”çš„hidden state Cæ¥é¢„æµ‹ fine-tuning BERTå…¼å®¹many downstream tasksï¼šsingle text or text pairs ç›´æ¥ç»„å¥½è¾“å…¥ï¼Œend-to-end fine-tuningå°±è¡Œ è¾“å‡ºè¿˜æ˜¯ç”¨CLS tokenå¯¹åº”çš„hidden state Cæ¥é¢„æµ‹ï¼Œæ¥åˆ†ç±»å¤´ A Survey on Visual Transformer åŠ¨æœº provide a comprehensive overview of the recent advances in visual transformers discuss the potential directions for further improvement develop timeline æŒ‰ç…§åº”ç”¨åœºæ™¯åˆ†ç±» backboneï¼šåˆ†ç±» high/mid-level visionï¼šé€šå¸¸æ˜¯è¯­ä¹‰ç›¸å…³çš„ï¼Œæ£€æµ‹/åˆ†å‰²/å§¿æ€ä¼°è®¡ low-level visionï¼šå¯¹å›¾åƒæœ¬èº«è¿›è¡Œæ“ä½œï¼Œè¶…åˆ†/å›¾åƒç”Ÿæˆï¼Œç›®å‰åº”ç”¨è¾ƒå°‘ video processing revisiting transformer key-conceptsï¼šsentenceã€embeddingã€positional encodingã€encoderã€decoderã€self-attention layerã€encoder-decoder attention layerã€multi-head attentionã€feed-forward neural network self-attention layer input vector is transformed into 3 vectors input vector is embedding+PE(pos,i)ï¼šposæ˜¯wordåœ¨sequenceä¸­çš„ä½ç½®ï¼Œiæ˜¯PE-elementåœ¨embedding vecä¸­çš„ä½ç½® query vec q key vec k value vec v $d_q = d_k = d_v = d_{model} = 512$ then calculateï¼š$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$ encoder-decoder attention layer Kå’ŒVæ˜¯ä»encoderä¸­æ‹¿åˆ° Qæ˜¯ä»å‰ä¸€å±‚æ‹¿åˆ° è®¡ç®—æ˜¯ç›¸ä¼¼çš„ multi-head attention ä¸€ä¸ªattentionæ˜¯ä¸€ä¸ªsoftmaxï¼Œå¯¹åº”äº†ä¸€å¯¹å¼ºç›¸å…³ï¼ŒåŒæ—¶æŠ‘åˆ¶äº†å…¶ä»–wordçš„ç›¸å…³æ€§ è€ƒè™‘ä¸€ä¸ªè¯å¾€å¾€ä¸å‡ ä¸ªè¯å¼ºç›¸å…³ï¼Œè¿™å°±éœ€è¦å¤šä¸ªattention multi-headï¼šdifferent QKV matrices are used for different heads given a input vectorï¼Œthe number of heads h å…ˆäº§ç”Ÿhä¸ª pairs $d_q=d_k=d_v=d_{model}/h=64$ è¿™hä¸ªpairï¼Œåˆ†åˆ«è®¡ç®—attention vectorï¼Œå¾—åˆ°hä¸ª[b,d]çš„context vector concat along-d-axis and linear projection to final [b,d] vector residual &amp; layer-normï¼šlayer-normåœ¨residual-addä»¥å feed-forward network fc-GeLU-fc $d_h=2048$ final-layer in decoder dense+softmax $d_{words}=$ number of words in the vocabulary when applied in CV tasks most transformers adopt the original transformerâ€™s encoder module used as a feature selector ç›¸æ¯”è¾ƒäºCNNï¼Œèƒ½å¤Ÿcapture long-distance characteristicsï¼Œderive global information ç›¸æ¯”è¾ƒäºRNNï¼Œèƒ½å¤Ÿå¹¶è¡Œè®¡ç®— è®¡ç®—é‡ é¦–å…ˆæ˜¯ä¸‰ä¸ªçº¿æ€§å±‚ï¼šçº¿æ€§æ—¶é—´å¤æ‚åº¦O(n)ï¼Œè®¡ç®—é‡ä¸$d_{model}$æˆæ­£æ¯” ç„¶åæ˜¯self-attentionå±‚ï¼šQKVçŸ©é˜µä¹˜æ³•è¿ç®—ï¼Œå¹³æ–¹æ—¶é—´å¤æ‚åº¦O(n^2) multi-headçš„è¯ï¼Œè¿˜æœ‰ä¸€ä¸ªçº¿æ€§å±‚ï¼šå¹³æ–¹æ—¶é—´å¤æ‚åº¦O(n^2) revisiting transformers for NLP æœ€æ—©æœŸçš„RNN + attentionï¼šrnnçš„sequentialæœ¬è´¨å½±å“äº†é•¿è·ç¦»/å¹¶è¡ŒåŒ–/å¤§æ¨¡å‹ transformerçš„solely attentionç»“æ„ï¼šè§£å†³ä»¥ä¸Šé—®é¢˜ï¼Œä¿ƒè¿›äº†large pre-trained models (PTMs) for NLP BERT and its variants are a series of PTMs built on the multi-layer transformer encoder architecture pre-trained Masked language modeling Next sentence prediction fine-tuned add an output layer Generative Pre-trained Transformer models (GPT) are another type of PTMs based on the transformer decoder architecture masked self-attention mechanisms pre-trained ä¸BERTæœ€å¤§çš„ä¸åŒæ˜¯æœ‰å‘æ€§ visual transformer ã€category1ã€‘: backbone for image classification transformerçš„è¾“å…¥æ˜¯tokensï¼Œåœ¨NLPé‡Œæ˜¯embeddingå½¢å¼çš„åˆ†è¯åºåˆ—ï¼Œåœ¨CVé‡Œå°±æ˜¯representing a certain semantic conceptçš„visual token visual tokenå¯ä»¥æ¥è‡ªCNNçš„feature ä¹Ÿå¯ä»¥ç›´æ¥æ¥è‡ªimageçš„å°patch purely use transformeræ¥åšimage classificationä»»åŠ¡çš„æ¨¡å‹æœ‰iGPTã€ViTã€DeiT iGPT pretraining stage + finetuning stage pre-training stage self-supervisedï¼šè‡ªç›‘ç£ï¼Œæ‰€ä»¥ç»“æœè¾ƒå·® given an unlabeled dataset train the model by minimizing the -log(density)ï¼Œæ„Ÿè§‰æ˜¯åœ¨forceå…‰æ …æ’åºæ­£ç¡® fine-tuning stage average pool + fc + softmax jointly train with L_gen &amp; L_CE ViT pre-trained on large datasets standard transformerâ€™s encoder + MLP head treats all patches equally æœ‰ä¸€ä¸ªç±»ä¼¼BERT class tokençš„ä¸œè¥¿ ä»è®­ç»ƒçš„è§’åº¦ï¼Œgather knowledge of the entire class inferenceçš„æ—¶å€™ï¼Œåªæ‹¿äº†è¿™ç¬¬ä¸€ä¸ªlogitç”¨æ¥åšé¢„æµ‹ fine-tuning æ¢ä¸€ä¸ªzero-initializedçš„MLP head use higher resolution &amp; æ’å€¼pe DeiT Data-efficient image transformer better performance with a more cautious training strategy and a token-based distillation ã€category2ã€‘: High/Mid-level Vision ã€category3ã€‘: Low-level Vision ã€category4ã€‘: Video Processing efficient transformerï¼šç˜¦èº«&amp;åŠ é€Ÿ Pruning and Decomposition Knowledge Distillation Quantization Compact Architecture Design ViT: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE åŠ¨æœº attention in vision either in conjunction with CNN or replace certain part of a CNN overalléƒ½è¿˜æ˜¯CNN-based use a pure transformer to sequence of image patches verified on image classification tasks in supervised fashion è®ºç‚¹ transformer lack some inductive biases inherent to CNNsï¼Œæ‰€ä»¥åœ¨insufficient dataä¸Šnot generalize well however large scale training trumps inductive biasï¼Œå¤§æ•°æ®é›†ä¸ŠViTæ›´å¥½ naive application of self-attention å»ºç«‹pixelä¹‹é—´çš„ä¸¤ä¸¤å…³è”ï¼šè®¡ç®—é‡å¤ªå¤§äº† éœ€è¦approximationï¼šlocal/æ”¹å˜size we use transformer wih global self-attention to full-sized images æ–¹æ³• input 1D-embedding sequence å°†image $x\in R^{HWC}$ å±•å¼€æˆpatches $\{x_p \in R^{P^2C}\}$ thus sequence length $N=HW/P^2$ patch embeddingï¼š use a trainable linear projection fixed dimension size through-all position embeddingï¼š add to patch embedding standard learnable 1D position embedding prepended embeddingï¼š å‰ç½®çš„learnable embedding $x_{class}$ similar to BERTâ€™s class token ä»¥ä¸Šä¸‰ä¸ªembeddingç»„åˆèµ·æ¥ï¼Œä½œä¸ºè¾“å…¥sequence transformer encoder follow the original Transformer äº¤æ›¿çš„MSAå’ŒMLP layer norm LN residual GELU hybrid architecture input sequenceä¹Ÿå¯ä»¥æ¥æºäºCNNçš„feature maps patch sizeå¯ä»¥æ˜¯1x1 classification head attached to $z_L^0$ï¼šæ˜¯class tokenç”¨æ¥åšé¢„æµ‹ pre-trainingçš„æ—¶å€™æ˜¯MLP fine-tuningçš„æ—¶å€™æ¢ä¸€ä¸ªzero-initializedçš„single linear layer workflow typicallyå…ˆpre-train on large datasets å†fine-tune to downstream tasks fine-tuneçš„æ—¶å€™æ›¿æ¢ä¸€ä¸ªzero-initializedçš„æ–°çº¿æ€§åˆ†ç±»å¤´ when feeding images with higher resolution keep the patch size results in larger sequence length è¿™æ—¶å€™pre-trained PEå°±no longer meaningfuläº† we therefore perform 2D interpolationåŸºäºå®ƒåœ¨åŸå›¾ä¸Šçš„ä½ç½® training details Adamï¼š$\beta_1=0.9ï¼Œ\beta_2=0.999$ batch size 4096 high weight decay 0.1 linear lr warmup &amp; decay fine-tuning details SGDM cosine LR no weight decay ã€ï¼Ÿï¼Ÿï¼Ÿï¼Ÿã€‘average 0.9999 Swin Transformer: Hierarchical Vision Transformer using Shifted Windows åŠ¨æœº use Transformer as visual tasksâ€™ backbone challenges of Transformer in vision domain large variations of scales of the visual entities high resolution of pixels we propose hierarchical Transformer shifted windows self-attention in local windows cross-window connection verified on classificationï¼šImageNet top1 acc 86.4 detectionï¼šCOCO box-MAP 58.7 segmentationï¼šADE20K è®ºç‚¹ when transfer Transformerâ€™s high performance in NLP domain to CV domain differences between the two modalities scaleï¼šNLPé‡Œé¢ï¼Œword tokens serves as the basic elementï¼Œä½†æ˜¯CVé‡Œé¢ï¼Œpatchçš„å½¢æ€å¤§å°éƒ½æ˜¯å¯å˜çš„ï¼Œprevious methodsé‡Œé¢ï¼Œéƒ½æ˜¯ç»Ÿä¸€è®¾å®šå›ºå®šå¤§å°çš„patch token resolutionï¼šä¸»è¦é—®é¢˜å°±æ˜¯self-attentionçš„è®¡ç®—å¤æ‚åº¦ï¼Œæ˜¯image sizeçš„å¹³æ–¹ we propose Swin Transformer hierarchial feature maps linear computatoinal complexity to image size hierarchical start from small patches merge in deeper layers æ‰€ä»¥å¯¹ä¸åŒå°ºåº¦çš„ç‰¹å¾patchè¿›è¡Œäº†èåˆ linear complexity compute self-attention locally in each window æ¯ä¸ªwindowçš„number of patchesæ˜¯è®¾å®šå¥½çš„ï¼Œwindowæ•°æ˜¯ä¸image sizeæˆæ­£æ¯”çš„ æ‰€ä»¥æ˜¯çº¿æ€§ shifted window approach è·¨å±‚çš„window shiftï¼Œå»ºç«‹èµ·ç›¸é‚»windowé—´çš„æ¡¥æ¢ ã€QUESTIONã€‘all query patches within a window share the same key set previous attemptations of Transformer self-attention based backbone architectures å°†éƒ¨åˆ†/å…¨éƒ¨conv layersæ›¿æ¢æˆself-attention æ¨¡å‹ä¸»ä½“æ¶æ„è¿˜æ˜¯ResNet slightly better acc larger latency caused by self-att self-attention complement CNNs ä½œä¸ºadditional blockï¼Œç»™åˆ°backbone/headï¼Œæä¾›é•¿è·ç¦»ä¿¡æ¯ æœ‰äº›æ£€æµ‹/åˆ†å‰²ç½‘ç»œä¹Ÿå¼€å§‹ç”¨äº†transformerçš„encoder-decoderç»“æ„ transformer-based vision backbones ä¸»è¦å°±æ˜¯ViTåŠå…¶è¡ç”Ÿå“ ViT requires large-scale training sets DeiT introduces training strategies ä½†æ˜¯è¿˜å­˜åœ¨high resolutionè®¡ç®—é‡çš„é—®é¢˜ æ–¹æ³• overview Swin-Tï¼štiny version ç¬¬ä¸€æ­¥æ˜¯patch partitionï¼š å°†RGBå›¾åˆ‡æˆnon-overlapping patches patchesï¼štokenï¼Œbasic element feature input dimï¼šwith patch size 4x4ï¼Œdim=4x4x3=48 ç„¶åæ˜¯linear embedding layer å°†raw feature re-projectionåˆ°æŒ‡å®šç»´åº¦ æŒ‡å®šç»´åº¦Cï¼šdefault=96 æ¥ä¸‹æ¥æ˜¯Swin Transformer blocks the number of tokens maintain patch merging layersè´Ÿè´£reduce the number of tokens ç¬¬ä¸€ä¸ªpatch merging layer concat æ‰€æœ‰2x2çš„neighbor patchesï¼š4C-dim vec each ç„¶åç”¨äº†ä¸€ä¸ªçº¿æ€§å±‚re-projection number of tokensï¼ˆresolutionï¼‰ï¼šï¼ˆH/4*W/4ï¼‰/4 = ï¼ˆH/8*W/8ï¼‰ï¼Œè·Ÿå¸¸è§„çš„CNNä¸€æ ·å˜åŒ–çš„ token dimsï¼š2C åé¢æ¥ä¸Šä¸€ä¸ªTransformer blocks åˆèµ·æ¥å«stage2ï¼ˆstage3ã€stage4ï¼‰ Swin Transformer blocks è·ŸåŸå§‹çš„Transformer blockæ¯”ï¼Œå°±æ˜¯æŠŠåŸå§‹çš„MSAæ›¿æ¢æˆäº†window-basedçš„MSA åŸå§‹çš„attentionï¼šglobal computation leads to quadratic complexity window-based attentionï¼š attentionçš„è®¡ç®—åªå‘ç”Ÿåœ¨æ¯ä¸ªwindowå†…éƒ¨ non-overlapping partition å¾ˆæ˜¾ç„¶lacks connections across windows shifted window partitioning in successive blocks ä¸¤ä¸ªattention block ç¬¬ä¸€ä¸ªç”¨å¸¸è§„çš„window partitioning strategyï¼šä»å·¦ä¸Šè§’å¼€å§‹ï¼Œtake M=4ï¼Œwindow size 4x4ï¼ˆä¸€ä¸ªwindowé‡Œé¢åŒ…å«4x4ä¸ªpatchï¼‰ ç¬¬äºŒå±‚çš„windowï¼ŒåŸºäºå‰ä¸€å±‚ï¼Œå„å¹³ç§»M/2 introduce connections between neighbor non-overlapping windows in the previous layer efficient computation shifted windowä¼šå¯¼è‡´windowå°ºå¯¸ä¸ä¸€è‡´ï¼Œä¸åˆ©äºå¹¶è¡Œè®¡ç®— relative position bias æˆ‘ä»¬åœ¨MxMçš„windowå†…éƒ¨è®¡ç®—local attentionï¼šä¹Ÿå°±æ˜¯input sequenceçš„time-stepæ˜¯$M^2$ Qã€Kã€V $\in R ^ {M^2 d}$ $Attention(Q,K,V)=Softmax(QK^T/\sqrt{d}+B)V$ è¿™ä¸ªBä½œä¸ºlocalçš„position biasï¼Œåœ¨äºŒç»´ä¸Šï¼Œåœ¨æ¯ä¸ªè½´ä¸Šçš„å˜åŒ–èŒƒå›´[-M+1,M-1] we parameterized a smaller-sized bias matrix $\hat B\in R ^{(2M-1)*(2M-1)}$ values in $B \in R ^ {M^2*M^2}$ are taken from $\hat B$ the learnt relative position biaså¯ä»¥ç”¨æ¥initialize fine-tuned model Architecture variants base modelï¼šSwin-Bï¼Œå‚æ•°é‡å¯¹æ ‡ViT-B Swin-Tï¼š0.25xï¼Œå¯¹æ ‡ResNet-50 (DeiT-S) Swin-Sï¼š0.5xï¼Œå¯¹æ ‡ResNet-101 Swin-Lï¼š2x window sizeï¼šM=7 query dimï¼šd=32ï¼Œï¼ˆæ¯ä¸ªstageçš„input sequence dimé€æ¸x2ï¼Œheads numé€æ¸x2ï¼‰ MLPï¼šexpansion ratio=4 channel number Cï¼šç¬¬ä¸€ä¸ªstageçš„embdding dimï¼Œï¼ˆåç»­é€æ¸x2ï¼‰ hypersï¼š acc DETR: End-to-End Object Detection with Transformers åŠ¨æœº new task formulationï¼ša direct set prediction problem main gradients a set-based global loss a transformer en-de architecture remove the hand-designed componets like nms &amp; anchor acc &amp; run-time on par with Faster R-CNN on COCO significantly better performance on large objects lower performances on small objects è®ºç‚¹ modern detectors run object detection in an indirect way åŸºäºæ ¼å­/anchor/proposalsè¿›è¡Œå›å½’å’Œåˆ†ç±» ç®—æ³•æ€§èƒ½å—åˆ¶äºnmsæœºåˆ¶ã€anchorè®¾è®¡ã€target-anchorçš„åŒ¹é…æœºåˆ¶ end-to-end approach transformerçš„self-attentionæœºåˆ¶ï¼Œexplicitly model all pairwise interactions between elementsï¼šå†…å«äº†å»é‡ï¼ˆnmsï¼‰çš„èƒ½åŠ› bipartite matchingï¼šset loss functionï¼Œå°†é¢„æµ‹å’Œgtçš„boxä¸€ä¸€åŒ¹é…ï¼Œrun in parallel DETR does not require any customized layers, thus can be reproduced easily expand to segmentation taskï¼ša simple segmentation head trained on top of a pre-trained DETR set predictionï¼što predict a set of bounding boxes and the categories for each basicï¼šmultilabel classification detection task has near-duplicates issues set predictionæ˜¯postprocessing-freeçš„ï¼Œå®ƒçš„global inference schemesèƒ½å¤Ÿavoid redundancy usual lossï¼šbipartite match object detection set-based loss modern detectors use non-unique assignment rules together with NMS bipartite matchingæ˜¯targetå’Œpredä¸€ä¸€å¯¹åº” æ–¹æ³• overall three main components a CNN backbone an encoder-decoder transformer a simple FFN backbone conventional r50 inputï¼š$[H_0, W_0, 3]$ outputï¼š$[H,W,C], H=\frac{H_0}{32}, W=\frac{W_0}{32}, C=2048$ transformer encoder reduce channel dim to $d$ï¼š1x1 convï¼Œ$d=512$ collapse the spatial dimensionsï¼šfeature sequence [d, HW]ï¼Œæ¯ä¸ªspatial pixelä½œä¸ºä¸€ä¸ªfeature fixed positional encodingsï¼š added to the input of each attention layer ã€QUESTIONã€‘åŠ åœ¨Kå’ŒQä¸Šè¿˜æ˜¯embeddingä¸Šï¼Ÿ transformer decoder è¾“å…¥Nä¸ªdim=dçš„embedding å«object queriesï¼šè¡¨ç¤ºæˆ‘ä»¬é¢„æµ‹å›ºå®šå€¼Nä¸ªç›®æ ‡ å› ä¸ºdecoderä¹Ÿæ˜¯permutation-invariantçš„ï¼ˆå› ä¸ºall sharedï¼‰ï¼Œæ‰€ä»¥è¦è¾“å…¥Nä¸ªä¸ä¸€æ ·çš„embedding learnt positional encodings add them to the input of each attention layer decodes the N objects in parallel prediction FFN 3 layerï¼ŒReLUï¼Œ box predictionï¼šnormalized center coords &amp; height &amp; width class predictionï¼š an additional class label $\varnothing$ è¡¨ç¤ºno object auxiliary losses each decoder layeråé¢éƒ½æ¥ä¸€ä¸ªFFN predictionå’ŒHungarian loss shared FFN an additional shared LN to norm the inputs of FFN three components of the loss class lossï¼šCE loss box loss GIOU loss L1 loss technical details AdamWï¼š initial transformer lr=10e-4 initial backbone lr=10e-5 weight decay=10e-4 Xavier init imagenet-pretrained resnet weights with frozen batchnorm layersï¼šr50 &amp; r101ï¼ŒDETR &amp; DETR-R101 a variantï¼š increase feature resolution version remove stage5â€™s stride and add a dilation DETR-DC5 &amp; DETR-DC5-R101 improve performance for small objects overall 2x computation increase augmentation resize input random cropï¼šwith 0.5 prob then resize transformer default dropout 0.1 lr schedule 300 epochs drop by factor 10 after 200 epochs 4 images per GPUï¼Œtotal batch 64 for segmentation taskï¼šå…¨æ™¯åˆ†å‰² ç»™decoder outputsåŠ mask head compute multi-head attention among decoder box predictions encoder outputs generate M attention heatmaps per object add a FPN styled CNN to recover resolution pixel-wise argmax UNETR: Transformers for 3D Medical Image Segmentation åŠ¨æœº unetç»“æ„ç”¨äºåŒ»å­¦åˆ†å‰² encoder learns global context decoder utilize the representations to predict the semanic ouputs the locality of CNN limits long-range spatial dependency our method use a pure transformer as the encoder learn sequence representations of the input volume global multi-scale encoder directly connects to decoder with skip connections è®ºç‚¹ unetç»“æ„ encoderç”¨æ¥æå–å…¨å›¾ç‰¹å¾ decoderç”¨æ¥recover skip connectionsç”¨æ¥è¡¥å……spatial information that is lost during downsampling localized receptive fieldsï¼š disadvantage in capturing multi-scale contextual information å¦‚ä¸åŒå°ºå¯¸çš„è„‘è‚¿ç˜¤ ç¼“å’Œæ‰‹æ®µï¼šatrous convsï¼Œstill limited transformer self-attention mechanism in NLP highlight the important features of word sequences learn its long-range dependencies in ViT an image is represented as a patch embedding sequence our method formulation 1D seq2seq problem use embedded patches the first completely transformer-based encoder other unet- transformer methods 2D (ours 3D) employ only in the bottleneck (ours pure transformer) CNN &amp; transformer in separate streams and fuse æ–¹æ³• overview transformer encoder inputï¼š1D sequence of input embeddings given 3D volume $x \in R^{HWDC}$ divide into flattened uniform non-overlapping patches $x\in R^{LCN^3}$ $L=HWD/N^3$ï¼šthe sequence length $N^3$ï¼špatch dimension linear projection to K-dim $E \in R^{LCK}$ï¼šremain constant through transformer 1D learnable positional embedding $E_{pos} \in R^LD$ 12 self-att blocksï¼šMSA + MLP decoder &amp;skip connections é€‰å–encoderç¬¬{3,6,9,12}ä¸ªblockçš„è¾“å‡º reshape back to 3D volume $[\frac{H}{N},\frac{W}{N},\frac{D}{N},C]$ consecutive 3x3x3 conv+BN+ReLU bottleneck deconv by 2 to increase resolution then concat with the previous resized feature then jointly consecutive conv then upsample with deconvâ€¦ concatåˆ°åŸå›¾resolutionä»¥åï¼Œconsecutive convä»¥åï¼Œå†1x1x1 conv+softmax loss dice loss diceï¼šfor each class channelï¼Œè®¡ç®—diceï¼Œç„¶åæ±‚ç±»å¹³å‡ 1-dice ce loss for each pixelï¼Œæ±‚bceï¼Œç„¶åæ±‚æ‰€æœ‰pixelçš„å¹³å‡]]></content>
      <tags>
        <tag>transformer, self-attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pre-training & self-training]]></title>
    <url>%2F2021%2F01%2F17%2Fpre-training-self-training%2F</url>
    <content type="text"><![CDATA[[pre-training] Rethinking ImageNet Pre-trainingï¼ŒHe Kaimingï¼ŒimageNet pre-trainingå¹¶æ²¡æœ‰çœŸæ­£helps accï¼Œåªæ˜¯speedupï¼Œrandom initializationèƒ½å¤Ÿreach no worseçš„ç»“æœï¼Œå‰ææ˜¯æ•°æ®å……è¶³å¢å¼ºå¤ŸçŒ›ï¼Œå¯¹å°é—¨å°æˆ·è¿˜æ˜¯æ²¡å•¥ç”¨ï¼Œæˆ‘ä»¬å¸Œæœ›speedup [pre-training &amp; self-training] Rethinking Pre-training and Self-trainingï¼ŒGoogle Brainï¼Œæå‡ºtask-specificçš„pseudo labelè¦æ¯”pre-trainingä¸­æå‡ºæ¥çš„å„ç§æ ‡ç­¾è¦å¥½ï¼Œå‰æè¿˜æ˜¯å †æ•°æ®ï¼Œå¯¹å°é—¨å°æˆ·æ²¡å•¥ç”¨ï¼Œlow-dataä¸‹è¿˜æ˜¯pre-trainä¿å¹³å®‰ æ€»ä½“ä¸Šéƒ½æ˜¯é’ˆå¯¹è·¨ä»»åŠ¡ä¸‹ï¼ŒimageNet pre-trainingæ„ä¹‰çš„æ¢è®¨ï¼Œ åˆ†ç±»é—®é¢˜è¿˜æ˜¯å¯ä»¥ç»§ç»­pretrained kaimingè¿™ä¸ªåªæ˜¯factï¼Œæ²¡æœ‰ç°å®æŒ‡å¯¼æ„ä¹‰ googleè¿™ä¸ªone step furtherï¼Œæå‡ºäº†self-trainingåœ¨ç°å®æ¡ä»¶ä¸­å¯ä»¥ä¸€è¯• Rethinking Pre-training and Self-training åŠ¨æœº given factï¼šImageNet pre-training has limited impact on COCO object detection investigate self-training to utilize the additional data è®ºç‚¹ common practice pre-training supervised pre-training é¦–å…ˆè¦æ±‚æ•°æ®æœ‰æ ‡ç­¾ pre-train the backbone on ImageNet as a classification task å¼±ç›‘ç£å­¦ä¹  with pseudo/noisy label kaimingï¼šExploring the limits of weakly supervised pretraining self-supervised pre-training æ— æ ‡ç­¾çš„æµ·é‡æ•°æ® æ„é€ å­¦ä¹ ç›®æ ‡ï¼šautoencoderï¼Œcontrastiveï¼Œâ€¦ https://zhuanlan.zhihu.com/p/108906502 self-training paradigm on COCO train an object detection model on COCO generate pseudo labels on ImageNet both labeled data are combined to train a new model åŸºæœ¬åŸºäºnoisy studentçš„æ–¹æ³• observations with stronger data augmentation, pre-training hurts the accuracy, but helps in self-training both supervised and self-supervised pre-training methods fails the benefit of pre-training does not cancel out the gain by self-training flexible about unlabeled data sources, model architectures and computer vision tasks æ–¹æ³• data augmentation vary the strength of data augmentation as 4 levels pre-training efficientNet-B7 AutoAugment weights &amp; noisy student weights self-training noisy student scheme å®éªŒå‘ç°self-training with this standard loss function can be unstable implement a loss normalization technique experimental settings object detection COCO dataset for supervised learning unlabeled ImageNet and OpenImages dataset for self-trainingï¼šscore thresh 0.5 to generate pesudo labels retinaNet &amp; spineNet batchï¼šhalf supervised half pesudo semantic segmentation PASCAL VOC 2012 for supervised learning augmented PASCAL &amp; COCO &amp; ImageNet for self-trainingï¼šscore thresh 0.5 to generate pesudo masks &amp; multi-scale NAS-FPN å®éªŒ pre-training Pre-training hurts performance when stronger data augmentation is usedï¼šå› ä¸ºä¼šsharpenæ•°æ®å·®å¼‚ï¼Ÿ More labeled data diminishes the value of pre-trainingï¼šé€šå¸¸æˆ‘ä»¬çš„å®éªŒæ•°æ®fractionéƒ½æ¯”è¾ƒå°çš„ç›¸å¯¹imageNetï¼Œæ‰€ä»¥ç†è®ºä¸Šä¸ä¼šharmï¼Ÿ self-supervised pre-trainingä¹Ÿä¼šä¸€æ ·harmï¼Œåœ¨augmentåŠ å¼ºçš„æ—¶å€™ self-training Self-training helps in high data/strong augmentation regimes, even when pre-training hurtsï¼šä¸åŒçš„augment levelï¼Œself-trainingå¯¹æœ€ç»ˆç»“æœéƒ½æœ‰åŠ æˆ Self-training works across dataset sizes and is additive to pre-trainingï¼šä¸åŒçš„æ•°æ®é‡ï¼Œä¹Ÿéƒ½æœ‰åŠ æˆï¼Œä½†æ˜¯low data regimeä¸‹enjoys the biggest gain discussion weak performance of pre-training is that pre-training is not aware of the task of interest and can fail to adapt jointly training also helpsï¼šaddress the mismatch between two dataset noisy labeling is worse than targeted pseudo labeling æ€»ä½“ç»“è®ºï¼šå°æ ·æœ¬é‡çš„æ—¶å€™ï¼Œpre-trainingè¿˜æ˜¯æœ‰åŠ æˆçš„ï¼Œå†åŠ ä¸Šself-trainingè¿›ä¸€æ­¥æå‡ï¼Œæ ·æœ¬å¤šçš„æ—¶å€™å°±ç›´æ¥self-training Rethinking ImageNet Pre-training åŠ¨æœº thinking random initialization &amp; pre-training ImageNet pre-training speed up but not necessarily improving random initialization can achieve no worse result robust to data size, models, tasks and metrics rethink current paradigm of â€˜pre- training and fine-tuningâ€™ è®ºç‚¹ no fundamental obstacle preventing us from training from scratch if use normalization techniques appropriately if train sufficiently long pre-training speed up when fine-tuning on small dataset new hyper-parameters must be selected to avoid overfitting localization-sensitive task benefits limited from pre-training aimed at communities that donâ€™t have enough data or computational resources æ–¹æ³• normalization form normalized parameter initialization normalization layers BN layers makes training from scratch difficult small batch size degrade the acc of BN fine-tuningå¯ä»¥freeze BN alternatives GNï¼šå¯¹batch sizeä¸æ•æ„Ÿ syncBN with appropriately normalized initializationå¯ä»¥train from scratch VGGè¿™ç§ä¸ç”¨BNå±‚çš„ convergence pre-training model has learned low-level features that do not need to be re-learned during random-initial training need more iterations to learn both low-level and semantic features å®éªŒ investigate maskRCNN æ›¿æ¢BNï¼šGN/sync-BN learning rateï¼š training longer for the first (large) learning rate is useful but training for longer on small learning rates often leads to overfitting 10k COCOå¾€ä¸Šï¼Œtrain from scratch resultsèƒ½å¤Ÿcatch up pretraining resultsï¼Œåªè¦è®­çš„å¤Ÿä¹… 1kå’Œ3.5kçš„COCOï¼Œconverges show no worseï¼Œä½†æ˜¯åœ¨éªŒè¯é›†ä¸Šå·®ä¸€äº›ï¼šstrong overfitting due to lack of data PASCALçš„ç»“æœä¹Ÿå·®ä¸€ç‚¹ï¼Œå› ä¸ºinstanceå’Œcategoryéƒ½æ›´å°‘ï¼Œnot directly comparable to the same number of COCO imagesï¼šfewer instances and categories has a similar negative impact as insufficient training data]]></content>
  </entry>
  <entry>
    <title><![CDATA[long-tailed]]></title>
    <url>%2F2021%2F01%2F11%2Flong-tailed%2F</url>
    <content type="text"><![CDATA[[bag of tricks] Bag of Tricks for Long-Tailed Visual Recognition with Deep Convolutional Neural Networksï¼šç»“è®ºå°±æ˜¯ä¸¤é˜¶æ®µï¼Œinput mixup + CAM-based DRS + muted mixup fine-tuningç»„åˆä½¿ç”¨æœ€å¥½ [balanced-meta softmax] Balanced Meta-Softmax for Long-Tailed Visual Recognitionï¼šå•†æ±¤ [eql] Equalization Loss for Long-Tailed Object Recognition [eql2] Equalization Loss v2: A New Gradient Balance Approach for Long-tailed Object Detection [Class Rectification Loss] Imbalanced Deep Learning by Minority Class Incremental Rectificationï¼šæå‡ºCRLä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«åˆ†å¸ƒç¨€ç–çš„å°ç±»ä»¬çš„è¾¹ç•Œï¼Œä»¥æ­¤é¿å…å¤§ç±»ä¸»å¯¼çš„å½±å“ Bag of Tricks for Long-Tailed Visual Recognition with Deep Convolutional Neural Networks åŠ¨æœº to give a detailed experimental guideline of common tricks to obtain the effective combinations of these tricks propose a novel data augmentation approach è®ºç‚¹ long-tailed datasets poor accuray on the under-presented minority long-tailed CIFARï¼š æŒ‡æ•°å‹è¡°å‡ imbalance factorï¼š50/100 test set unchanged ImageNet-LT sampling the origin set follow the pareto distribution test set is balanced iNaturalist extremely imbalanced real world dataset fine-grained problem different learning paradigms metric learning meta learning knowledge transfer suffer from high sensitivity to hyper-parameters training tricks re-weighting re-sample mixup two-stage training different tricks might hurt each other propose a novel data augmentation approach based on CAMï¼šgenerate images with transferred foreground and unchanged background æ–¹æ³• start from baseline re-weighting baselineï¼šCE re-weighting methodsï¼š cost-sensitive CEï¼šæŒ‰ç…§æ ·æœ¬é‡çº¿æ€§åŠ æƒ$\frac{n_c}{n_{min}}$ focal lossï¼šå›°éš¾æ ·æœ¬åŠ æƒ class-balanced lossï¼š effective number rather than æ ·æœ¬é‡$n_c$ hyperparameter $\beta$ and weighting factorï¼š$\frac{1-\beta}{1-\beta^{n_c}}$ åœ¨cifar10ä¸Šæœ‰æ•ˆï¼Œä½†æ˜¯cifar100ä¸Šå°±ä¸å¥½äº† directly application in training procedure is not a proper choice especially whenç±»åˆ«å¢å¤šï¼ŒimbalanceåŠ å‰§çš„æ—¶å€™ re-sampling re-sampling methods over-samplingï¼š éšæœºå¤åˆ¶minority might leads to overfitting under-sampling éšæœºå»æ‰ä¸€äº›majority be preferable to over-sampling æœ‰è§„å¾‹åœ°sampling å¤§ä½“éƒ½æ˜¯imbalancedå‘ç€lighter imbalancedå‘ç€balancedæ¨åŠ¨ artificial sampling methods create artificial samples sample based on gradients and features likely to introduce noisy data è§‚å¯Ÿåˆ°æå‡æ•ˆæœä¸æ˜æ˜¾ mixup input mixupï¼šinput mixup can be further improved if we remove the mixup in last several epochs manifold mixupï¼šon only one layer è§‚å¯Ÿåˆ°ä¸¤ç§mixupåŠŸæ•ˆå·®ä¸å¤šï¼Œåé¢å‘ç°input mixupæ›´å¥½äº› input mixupå»æ‰å†finetuningå‡ ä¸ªepochç»“æœåˆæå‡ï¼Œmanifoldåˆ™ä¼šå˜å·® two-stage training imbalanced training + balanced fine-tuning vanilla training schedule on imbalanced data å…ˆå­¦ç‰¹å¾ fine-tune on balanced subsets å†è°ƒæ•´recognition accuracy deferred re-balancing by re-sampling (DRS) ï¼špropose CAM-based sampling deferred re-balancing by re-weighting (DRW) proposed CAM-based sampling DRS only replicate or remove for each sampled image, apply the trained model &amp; its ground truth label to generate CAM ç”¨heatmapçš„å¹³å‡å€¼ä½œä¸ºé˜ˆå€¼æ¥åŒºåˆ†å‰èƒŒæ™¯ å¯¹å‰æ™¯apply transformations horizontal flipping translation rotating scaling å‘ç°fine-tuningæ—¶å€™å†resampleæ¯”ç›´æ¥resampleçš„ç»“æœå¥½ proposed CAM-based samplingå¥½äºå…¶ä»–samplingï¼Œå…¶ä¸­CAM-based balance- samplingæœ€å¥½ ImageTrans balance-samplingåªåšå˜æ¢ï¼Œä¸ç”¨CAMåŒºåˆ†å‰èƒŒæ™¯ï¼Œç»“æœä¸å¦‚CAM-basedï¼Œè¯æ˜CAMæœ‰ç”¨ å‘ç°fine-tuningæ—¶å€™å†reweightæ¯”ç›´æ¥reweightçš„ç»“æœå¥½ å…¶ä¸­CSCEï¼ˆæŒ‰ç…§æ ·æœ¬é‡çº¿æ€§åŠ æƒï¼‰æœ€å¥½ æ•´ä½“æ¥çœ‹DRSçš„ç»“æœç¨å¾®æ¯”DRWå¥½ä¸€ç‚¹ trick combinations two-stageçš„CAM-based DRSç•¥å¥½äºDRWï¼Œä¸¤ä¸ªåŒæ—¶ç”¨ä¸ä¼šfurther improve å†åŠ ä¸Šmixupçš„è¯ï¼Œinputæ¯”manifoldå¥½ä¸€äº› ç»“è®ºå°±æ˜¯ï¼šinput mixup + CAM-based DRS + mute fine-tuningï¼Œapply the tricks incrementally Balanced Meta-Softmax for Long-Tailed Visual Recognition åŠ¨æœº long-tailedï¼šmismatch between training and testing distributions softmaxï¼šbiased gradient estimation under the long-tailed setup propose Balanced Softmaxï¼šan elegant unbiased extension of Softmax apply a complementary Meta Samplerï¼šoptimal sample rate classification &amp; segmentation è®ºç‚¹ raw baselineï¼ša model that minimizes empirical risk on long-tailed training datasets often underperforms on a class-balanced test set most methods use re-sampling or re-weighting to simulate a balanced dataset may under-class the majority or have gradient issue meta-learning optimize the weight per sample need a clean and unbiased dataset decoupled training å°±æ˜¯ä¸Šé¢ä¸€ç¯‡è®ºæ–‡ä¸­çš„ä¸¤é˜¶æ®µï¼Œç¬¬ä¸€é˜¶æ®µå…ˆå­¦è¡¨å¾ï¼Œç¬¬äºŒé˜¶æ®µè°ƒæ•´åˆ†å¸ƒfine-tuning not adequate for datasets with extremely high imbalance factor LDAM Label-Distribution-Aware Margin Loss larger generalization error bound for minority suit for binary classification we propose BALMS Balanced Meta-Softmax theoretically equivalent with generalization error bound for datasets with high imbalance factors should combine Meta Sampler æ–¹æ³• balanced softmax biasedï¼šä»è´å¶æ–¯æ¡ä»¶æ¦‚ç‡å…¬å¼çœ‹ï¼Œstandard softmaxä¸Šé»˜è®¤äº†å‡åŒ€é‡‡æ ·çš„p(y)ï¼Œåœ¨é•¿å°¾åˆ†å¸ƒçš„æ—¶å€™ï¼Œå°±æ˜¯æœ‰åçš„ åŠ æƒï¼š åŠ åœ¨softmaxé¡¹é‡Œé¢ åŸºäºæ ·æœ¬é‡çº¿æ€§åŠ æƒ æ•°å­¦æ„ä¹‰ä¸Šï¼šwe need to focus on minimizing the training loss of the tail classes meta sampler resampleå’Œreweightç›´æ¥combineå¯èƒ½ä¼šworsen performance class balance resampleå¯èƒ½æœ‰over-balance issue combination procedures å¯¹å½“å‰åˆ†å¸ƒï¼Œå…ˆè®¡ç®—balanced-softmaxï¼Œä¿å­˜ä¸€ä¸ªæ¢¯åº¦æ›´æ–°åçš„æ¨¡å‹ è®¡ç®—è¿™ä¸ªä¸´æ—¶æ¨¡å‹åœ¨meta setä¸Šçš„CEï¼Œå¯¹åˆ†å¸ƒembeddingè¿›è¡Œæ¢¯åº¦æ›´æ–°ï¼šè¯„ä¼°å½“å‰åˆ†å¸ƒå’‹æ ·ï¼Œå¾€ä¸€å®šæ–¹å‘çŸ«æ­£ å¯¹çœŸæ­£çš„æ¨¡å‹ï¼Œç”¨æœ€æ–°çš„åˆ†å¸ƒï¼Œè®¡ç®—balanced-softmaxï¼Œè¿›è¡Œæ¢¯åº¦æ›´æ–°ï¼šç”¨ä¼˜åŒ–åçš„åˆ†å¸ƒï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹  å®éªŒ CEçš„ç»“æœå‘ˆç°æ˜æ˜¾çš„é•¿å°¾åŒåˆ†å¸ƒè¶‹åŠ¿ CBSæœ‰ç¼“è§£ BSæ›´å¥½ BS+CBSä¼šover sample BS+metaæœ€å¥½ Imbalanced Deep Learning by Minority Class Incremental Rectification åŠ¨æœº significantly imbalanced training data propose batch-wise incremental minority class rectification model Class Rectification Loss (CRL) bring benefits to both minority and majority class boundary learning è®ºç‚¹ Most methods produce learning bias towards the majority classes to eliminate bias lifting the importance of minority classesï¼šover-sampling can easily cause model overfittingï¼Œå¯èƒ½é€ æˆå¯¹å°ç±»åˆ«çš„è¿‡åˆ†å…³æ³¨ï¼Œè€Œå¯¹å¤§ç±»åˆ«ä¸å¤Ÿé‡è§†ï¼Œå½±å“æ¨¡å‹æ³›åŒ–èƒ½åŠ› cost-sensitive learningï¼šdifficult to optimise threshold-adjustment techniqueï¼šgiven by experts previous methods mainly investigate single-label binary-class with small imbalance ratio real data large ratioï¼špower-law distributions Subtle appearance discrepancy hard sample mining hard negatives are more informative than easy negatives as they violate a model class boundary we only consider hard mining on the minority classes for efficiency our batch-balancing hard mining strategyï¼šeliminating exhaustive searching LMLE å”¯ä¸€çš„ç«å“ï¼šè€ƒè™‘äº†data imbalanceçš„ç»†ç²’åº¦åˆ†ç±» not end-to-end global hard mining computationally complex and expensive æ–¹æ³• CRL overview explicitly imposing structural discrimination of minority classes batch-wise operate on CE forcus on minority class onlyï¼šthe conventional CE loss can already model the majority classes well limitations of CE CE treat the individual samples and classes as equally important the learned model is suboptimal boundaries are biased towards majority classes profile the class distribution for each class hard mining overview minority class hard sample mining selectively â€œborrowingâ€ majority class samples from class decision boundary to minority classâ€™s perspectiveï¼šmining both hard-positive and hard-negative samples define minority classï¼šselected in each mini-batch Incremental refinementï¼š eliminates the LMLEâ€™s drawback in assuming that local group structures of all classes can be estimated reliably by offline global clustering mini-batchçš„data distributionå’Œè®­ç»ƒé›†ä¸æ˜¯å®Œå…¨ä¸€è‡´çš„ steps profile the minority and majority classes per label in each training mini-batch for each sampleï¼Œfor each class $j$ï¼Œfor each pred class $k$ï¼Œwe have $h^j=[h_1^j, â€¦, h_k^j, â€¦, h_{n_cls}^j]$ sort $h_k^j$ in descent orderï¼Œdefine the minority classes for each class with $C_{min}^j = \sum_{k\in C_{min}^j}h_k^j \leq \rho * n_{bs}$ï¼Œwith $\rho=0.5$ hard mining hardness score basedï¼šprediction scoreï¼Œclass-level feature basedï¼šfeature distanceï¼Œinstance-level class-levelï¼Œfor class c hard-positivesï¼šsame gt classï¼Œbut low prediction hard-negativeï¼šdifferent gt classï¼Œwith high prediction instance-levelï¼Œfor each sample in class c hard-positivesï¼šsame gt classï¼Œlarge distance with current sample hard-negativeï¼šdifferent gt classï¼Œsmall distance with current sample top-k mining hard-positivesï¼šbottom-k scored on c/top-k distance on c hard-negativeï¼štop-k scored on c/bottom-k distance on c score-based yields superior to distance-based CRL final weighted lossï¼š$L = \alpha L_{crl}+(1-\alpha)L_{ce}$ï¼Œ$\alpha=\eta\Omega_{imbalance}$ class imbalance measure $\Omega$ï¼šmore weighting is assigned to more imbalanced labels form triplet lossï¼šç±»å†…+ç±»é—´ contrastive lossï¼šç±»å†… modelling the distribution relationship of positive and negative pairsï¼šæ²¡çœ‹æ‡‚ æ€»ç»“ å°±æ˜¯å¥—ç”¨ç°æœ‰çš„metric learningï¼Œå®šä¹‰äº†ä¸€ä¸ªå˜åŒ–çš„minority classï¼Œåƒåœ¾ã€‚ è¯´åˆ°åº•å°±æ˜¯å¤§æ•°æ®â€”â€”CEï¼Œå°æ•°æ®â€”â€”metric learningã€‚]]></content>
      <tags>
        <tag>é•¿å°¾åˆ†å¸ƒ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[refineDet]]></title>
    <url>%2F2021%2F01%2F08%2FrefineDet%2F</url>
    <content type="text"><![CDATA[å’ŒrefineNetæ²¡æœ‰ä»»ä½•å…³ç³» RefineDet: Single-Shot Refinement Neural Network for Object Detectio åŠ¨æœº inherit the merits of both two-stage and one-stageï¼šaccuracy and efficiency single-shot multi-task refineDet anchor refinement module (ARM) object detection module (ODM) transfer connection block (TCB) è®ºç‚¹ three advantages that two-stage superior than one-stage RPNï¼šhandle class imbalance two step regressï¼šcoarse to refine two stage featureï¼šRPNä»»åŠ¡å’Œregressionä»»åŠ¡æœ‰å„è‡ªçš„feature æ¨¡æ‹ŸäºŒé˜¶æ®µæ£€æµ‹çš„RPNï¼ŒæŠŠclassifierä»»åŠ¡ä¸­çš„å¤§é‡é˜´æ€§æ¡†å…ˆæ’æ‰ï¼Œä½†ä¸æ˜¯ä»¥ä¸¤ä¸ªé˜¶æ®µçš„å½¢å¼ï¼Œè€Œæ˜¯multi-taskå¹¶è¡Œ å°†ä¸€é˜¶æ®µæ£€æµ‹çš„objectnesså’Œbox regressionä»»åŠ¡è§£è€¦ï¼Œä¸¤ä¸ªä»»åŠ¡é€šè¿‡transfer blockè¿æ¥ ARM remove negative anchors to reduce search space for the classifier coarsely adjust the locations and sizes of anchors to provide better initialization for regression ODM further improve the regression predict multi labels TCB transfer the features in the ARM to handle the more challenging tasks in the ODM æ–¹æ³• Transfer Connection Block æ²¡ä»€ä¹ˆæ–°çš„ä¸œè¥¿ï¼Œä¸Šé‡‡æ ·ç”¨äº†deconvï¼Œconv-reluï¼Œelement-wise add Two-Step Cascaded Regression fisrt step ARM prediction for each cellï¼Œfor each predefined anchor boxesï¼Œpredict 4 offsets and 2 scores obtain refined anchor boxes second step ODM prediction with justified feature mapï¼Œwith refined anchor boxes generate accurate boxes offset to refined boxes and multi-class scoresï¼Œc+4 Negative Anchor Filtering reject well-classified negative anchors if the negative confidence is larger than 0.99ï¼Œdiscard it in training the ODM ODMæ¥æ”¶æ‰€æœ‰pred positiveå’Œhard negative Training and Inference details backï¼šVGG16 &amp; resnet101 fc6 &amp; fc7å˜æˆä¸¤ä¸ªconv different feature scales L2 norm two extra convolution layers and one extra residual block 4 feature strides each levelï¼š1 scale &amp; 3 ratios ensures that different scales of anchors have the same tiling density on the image matching æ¯ä¸ªGT box matchä¸€ä¸ªscoreæœ€é«˜çš„anchor box ä¸ºæ¯ä¸ªanchor boxæ‰¾åˆ°æœ€åŒ¹é…çš„iouå¤§äº0.5çš„gt box ç›¸å½“äºæŠŠignoreé‚£éƒ¨åˆ†ä¹Ÿä½œä¸ºæ­£æ ·æœ¬äº† Hard Negative Mining select negative anchor boxes with top loss values n &amp; p ratioï¼š3:1 Loss Function ARM loss binary classï¼šåªè®¡ç®—æ­£æ ·æœ¬ï¼Ÿï¼Ÿï¼Ÿ boxï¼šåªè®¡ç®—æ­£æ ·æœ¬ ODM loss pass the refined anchors with the negative confidence less than the threshold multi-classï¼šè®¡ç®—å‡è¡¡çš„æ­£è´Ÿæ ·æœ¬ boxï¼šåªè®¡ç®—æ­£æ ·æœ¬ æ­£æ ·æœ¬æ•°ä¸º0çš„æ—¶å€™ï¼Œlosså‡ä¸º0ï¼šçº¯é˜´æ€§æ ·æœ¬æ— æ•ˆï¼Ÿï¼Ÿ]]></content>
      <tags>
        <tag>ç›®æ ‡æ£€æµ‹</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CPNDet]]></title>
    <url>%2F2021%2F01%2F05%2FCPNDet%2F</url>
    <content type="text"><![CDATA[Corner Proposal Network for Anchor-free, Two-stage Object Detection åŠ¨æœº anchor-free two-stage å…ˆæ‰¾potential corner keypoints classify each proposal corner-basedæ–¹æ³•ï¼šå¯¹äºobjects of various scalesæœ‰æ•ˆï¼Œåœ¨è®­ç»ƒä¸­é¿å…äº§ç”Ÿè¿‡å¤šçš„å†—ä½™false-positive proposalsï¼Œä½†æ˜¯åœ¨ç»“æœä¸Šä¼šå‡ºç°æ›´å¤šçš„fp å¾—åˆ°çš„æ˜¯competitive results è®ºç‚¹ anchor-based methodså¯¹å½¢çŠ¶å¥‡æ€ªçš„ç›®æ ‡å®¹æ˜“æ¼æ£€ anchor-free methodså®¹æ˜“å¼•å…¥å‡é˜³caused by mistakely grouping thus an individual classifier is strongly required Corner Proposal Network (CPN) use key-point detection in CornerNet ä½†æ˜¯groupé˜¶æ®µä¸å†ç”¨embedding distanceè¡¡é‡ï¼Œè€Œæ˜¯ç”¨a binary classifier ç„¶åæ˜¯multi-class classifierï¼Œoperate on the survived objects æœ€åsoft-NMS]]></content>
      <tags>
        <tag>ç›®æ ‡æ£€æµ‹ï¼Œtwo-stageï¼Œanchor-free</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[refineNet]]></title>
    <url>%2F2021%2F01%2F05%2FrefineNet%2F</url>
    <content type="text"><![CDATA[RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentationå¼•ç”¨é‡1452ï¼Œä½†æ˜¯æ²¡æœ‰å‡ ç¯‡æŠ€æœ¯åšå®¢ï¼Ÿï¼Ÿ åŠ¨æœº è¯­ä¹‰åˆ†å‰² dense classification on every single pixel refineNet long-range residual connections chained residual pooling è®ºç‚¹ pooling/conv strideï¼š losing finer image structure deconv is not able to recover the lost info atrous high resoï¼šlarge computation dilated convï¼šcoarse sub-sampling of feature FCN fuse features from all levels stage-wise rather than end-to-end???å­˜ç–‘ this paper main ideaï¼šeffectively exploit middle layer features RefineNet fuse all level feature residual connections with identity skip chained residual pooling to capture background contextï¼šçœ‹æè¿°æ„Ÿè§‰åƒinception downsamp end-to-end æ˜¯æ•´ä¸ªåˆ†å‰²ç½‘ç»œä¸­çš„ä¸€ä¸ªcomponent æ–¹æ³• backbone pretrained resnet 4 blocksï¼šx4 - x32ï¼Œeach blockï¼špool-residual connectionï¼šæ¯ä¸ªè¾“å‡ºè¿æ¥ä¸€ä¸ªRefineNet unit 4-cascaded architecture final ouputï¼š high-resolution feature maps dense soft-max bilinear interpolation to origin resolution cascade inputs output from backbone block ouput from previous refineNet block refineNet block adapt convï¼š to adapt the dimensionality and refine special task BN layers are removed channel 512 for R4ï¼Œchannel 256 for the rest fusionï¼š å…ˆç”¨conv to adapt dimension and recale the paths ç„¶åupsamp summation å¦‚æœsingle inputï¼šwalk through and stay unchanged chained residual poolingï¼š aims to capture background context from a large image region chainedï¼šefficiently pool features with multiple window sizes pooling blocksï¼šs1 maxpooling+conv in practiceç”¨äº†ä¸¤ä¸ªpooling blocks use one ReLU in the chained residual pooling block output convï¼š ä¸€ä¸ªresidualï¼što employ non-linearity dimension remains unchanged final levelï¼štwo additional RCUs before the final softmax prediction residual identity mappings a clean information path not block by any non-linearityï¼šæ‰€æœ‰reluéƒ½åœ¨residual pathé‡Œé¢ åªæœ‰chained residual poolingæ¨¡å—èµ·å§‹æ—¶å€™æœ‰ä¸ªReLUï¼šone single ReLU in each RefineNet block does not noticeably reduce the effectiveness of gradient flow linear operationsï¼š within the fusion block dimension reduction operations upsamp operations å…¶ä»–ç»“æ„ çº§è”çš„å°±å«cascaded ä¸€ä¸ªblockå°±å«single å¤šä¸ªinput resolutionå°±å«mult-scale å®éªŒ 4-cascaded works better than 1-cas &amp; 2-cas 2-scale works better than 1-scale]]></content>
      <tags>
        <tag>è¯­ä¹‰åˆ†å‰²</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centerNet]]></title>
    <url>%2F2020%2F12%2F29%2FcenterNet%2F</url>
    <content type="text"><![CDATA[[papers] [centerNet] çœŸcenterNet: Objects as Pointsï¼Œutexasï¼Œè¿™ä¸ªæ˜¯çœŸçš„centerNetï¼ŒåŸºäºåˆ†å‰²æ¶æ„ï¼Œé¢„æµ‹ä¸­å¿ƒç‚¹çš„heatmapï¼Œä»¥åŠ2-Nä¸ªchannelå…¶ä»–ç›¸å…³å‚æ•°çš„å›å½’ [cornet-centerNet] centerNet: Keypoint Triplets for Object Detectionï¼Œè¿™ä¸ªæŠ¢å…ˆå«äº†centerNetï¼Œä½†æ˜¯æˆ‘è§‰å¾—å«corner-centerNetæ›´åˆé€‚ï¼Œå®ƒæ˜¯åŸºäºcornerNetè¡ç”Ÿçš„ï¼Œåœ¨cornerNetçš„åŸºç¡€ä¸Šå†åŠ ä¸€åˆ€åˆ¤å®šï¼ŒåŸºäºè§’ç‚¹pairçš„ä¸­å¿ƒç‚¹æ˜¯å¦æ˜¯å‰æ™¯æ¥å†³å®šæ˜¯å¦ä¿ç•™è¿™ä¸ªæ¡† [centerNet2] Probabilistic two-stage detectionï¼Œutexasï¼Œ centerNet: Objects as Points åŠ¨æœº anchor-based exhaustive list of potential locations wasteful, inefficient, requires additional post-processing our detector centerï¼šuse keypoint estimation to find center points other propertiesï¼šregress tasks object detection 3d object detection multi-person human pose estimation è®ºç‚¹ ç›¸æ¯”è¾ƒäºä¼ ç»Ÿä¸€é˜¶æ®µã€äºŒé˜¶æ®µæ£€æµ‹ anchorï¼š box &amp; kpï¼šä¸€ä¸ªæ˜¯æ¡†ï¼Œä¸€ä¸ªæ˜¯å‡»ä¸­æ ¼å­ nmsï¼štake local peaksï¼Œno need of nms larger resolutionï¼šhourglassæ¶æ„ï¼Œè¾“å‡ºx4çš„heatmapï¼Œeliminates the need for multiple anchors ç›¸æ¯”è¾ƒäºkey point estimantion network themï¼šrequire grouping stage ourï¼šåªå®šä½ä¸€ä¸ªcenter pointï¼Œno need for group or post-processing æ–¹æ³• loss å…³é”®ç‚¹loss center pointå…³é”®ç‚¹å®šä¹‰ï¼šæ¯ä¸ªç›®æ ‡çš„gt pointåªæœ‰ä¸€ä¸ªï¼Œä»¥å®ƒä¸ºä¸­å¿ƒï¼Œåšobject size-adaptiveçš„é«˜æ–¯penalty reductionï¼Œoverlapçš„åœ°æ–¹å–max focal lossï¼šåŸºæœ¬ä¸cornetNetä¸€è‡´ L_k = \frac{-1}{N}\sum_{x,y,c} \begin{cases} (1-\hat Y)^\alpha log(\hat Y), if Y=1\\ (1-Y)^\beta \hat Y^\alpha log(1-\hat Y), otherwise \end{cases} $\alpha=2, \beta=4$ background pointsæœ‰penaltyï¼Œæ ¹æ®gtçš„é«˜æ–¯è¡°å‡æ¥çš„ offset loss åªæœ‰ä¸¤ä¸ªé€šé“(x_offset &amp; y_offset)ï¼šshared among categories gtçš„offsetæ˜¯åŸå§‹resolution/output strideå‘ä¸‹å–æ•´å¾—åˆ° L1 loss centerNet output ç¬¬ä¸€ä¸ªéƒ¨åˆ†ï¼šä¸­å¿ƒç‚¹ï¼Œ[h,w,c]ï¼Œbinary mask for each category ç¬¬äºŒä¸ªéƒ¨åˆ†ï¼šoffsetï¼Œ[h,w,2]ï¼Œshared among ç¬¬ä¸‰ä¸ªéƒ¨åˆ†ï¼šsizeï¼Œ[h,w,2]ï¼Œshared among L1 lossï¼Œuse raw pixel coordinates overall C+4 channelsï¼Œè·Ÿä¼ ç»Ÿæ£€æµ‹çš„formulationæ˜¯ä¸€è‡´çš„ï¼Œåªä¸è¿‡ä¼ ç»Ÿæ£€æµ‹gtæ˜¯åŸºäºanchorè®¡ç®—çš„ç›¸å¯¹å€¼ï¼Œæœ¬æ–‡ç›´æ¥å›å½’ç»å¯¹å€¼ $L_{det} = L_k + \lambda_{size} L_{size} + \lambda_{off} L_{off}$ å…¶ä»–taskçš„formulationçœ‹ç¬¬ä¸€å¼ å›¾ inference workflow local peaksï¼š for each category channel all responses greater or equal to its 8-connected neighborsï¼š3x3 max pooling keep the top100 generate bounding boxes ç»„åˆoffset &amp; size predictions ï¼Ÿï¼Ÿï¼Ÿï¼Ÿæ²¡æœ‰åå¤„ç†äº†ï¼Ÿï¼Ÿï¼Ÿå‡é˜³ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ encoder-decoder backboneï¼šx4 hourglass104 stemï¼šx4 modulesï¼šä¸¤ä¸ª resnet18/101+deformable conv upsampling 3x3 deformable conv, 256/128/64 bilinear interpolation DLA34+deformable conv upsampling heads independent heads one 3x3 convï¼Œ256 1x1 conv for prediction æ€»ç»“ ä¸ªäººæ„Ÿè§‰ï¼ŒcenterNetå’Œanchor-basedçš„formulationå…¶å®æ˜¯ä¸€æ ·çš„ï¼Œ centerçš„å›å½’å¯¹æ ‡confidenceçš„å›å½’ï¼ŒåŒºåˆ«åœ¨äºé«˜æ–¯/[0,1]/[0,-1,1] sizeçš„å›å½’å˜æˆäº†raw pixelï¼Œä¸å†åŸºäºanchor hourglassç»“æ„å°±æ˜¯fpnï¼Œçº§è”çš„hourglasså¯ä»¥å¯¹æ ‡bi-fpn å¤šå°ºåº¦å˜æˆäº†å•ä¸€å¤§resolutionç‰¹å¾å›¾ï¼Œä¹Ÿå¯ä»¥ç”¨å¤šå°ºåº¦é¢„æµ‹ï¼Œéœ€è¦åŠ NMS centerNet2: Probabilistic two-stage detection åŠ¨æœº two-stage probabilistic interpretation the suggested pipeline stage1ï¼šinfer proper object-backgroud likelihoodï¼Œä¸“æ³¨å‰èƒŒæ™¯åˆ†ç¦» stage2ï¼šinform the overall score verified on COCO faster and more accurate than both one and two stage detectors outperform yolov4 extreme large modelï¼š56.4 mAP standard ResNeXt- 32x8d-101-DCN backï¼š50.2 mAP è®ºç‚¹ one-stage detectors dense predict jointly predict class &amp; location anchor-basedï¼šRetinaNetç”¨focal lossæ¥deal with å‰èƒŒæ™¯imbalance anchor-freeï¼šFCOS &amp; CenterNetä¸åŸºäºanchoråŸºäºgridï¼Œç¼“è§£imbalance deformable convï¼šAlignDetåœ¨outputå‰é¢åŠ ä¸€å±‚deformable conv to get richer features sound probablilistic interpretation heavier separate classification and regression branches than two-stage modelsï¼šå¦‚æœç±»åˆ«ç‰¹åˆ«å¤šçš„æƒ…å†µï¼Œå¤´ä¼šéå¸¸é‡ï¼Œä¸¥é‡å½±å“æ€§èƒ½ misaligned issueï¼šä¸€é˜¶æ®µé¢„æµ‹æ˜¯åŸºäºlocal featureï¼Œæ„Ÿå—é‡ã€anchor settingséƒ½ä¼šå½±å“ä¸ç›®æ ‡çš„å¯¹é½ç¨‹åº¦ two-stage detectors first RPN generates coarse object proposals then per-region head to classify and refine ROI headsï¼šFaster-RCNNç”¨äº†ä¸¤ä¸ªfcå±‚ä½œä¸ºROI heads cascadedï¼šCascadeRCNNç”¨äº†ä¸‰ä¸ªè¿ç»­çš„Faster-RCNNï¼Œwith a different positive threshold semantic branchï¼šHTCç”¨äº†é¢å¤–çš„åˆ†å‰²åˆ†æ”¯enhance the inter-stage feature flow decoupleï¼šTSDå°†cls&amp;posä¸¤ä¸ªROI headsè§£è€¦ weak RPNï¼šå› ä¸ºå°½å¯èƒ½æå‡å¬å›ç‡ï¼Œproposal scoreä¹Ÿä¸å‡†ï¼Œä¸§å¤±äº†ä¸€ä¸ªclearçš„probabilistic interpretation independent probabilistic interpretationï¼šä¸¤ä¸ªé˜¶æ®µå„è®­å„çš„ï¼Œæœ€åçš„cls scoreä»…ç”¨ç¬¬äºŒé˜¶æ®µçš„ slowï¼šproposalså¤ªå¤šäº†æ‰€ä»¥slow down other detectors point-basedï¼šcornetNeté¢„æµ‹&amp;ç»„åˆä¸¤ä¸ªè§’ç‚¹ï¼ŒcenterNeté¢„æµ‹ä¸­å¿ƒç‚¹å¹¶åŸºäºå®ƒå›å½’é•¿å®½ transformerï¼šDETRç›´æ¥é¢„æµ‹a set of bounding boxesï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„ç»“æ„åŒ–çš„dense output ç½‘ç»œç»“æ„ one/two-stage detectorsï¼šimage classification network + lightweight upsampling layers + heads point-basedï¼šFCNï¼Œæœ‰symmetric downsampling and upsampling layerï¼Œé¢„æµ‹ä¸€ä¸ªå°strideçš„heatmap DETRï¼šfeature extraction + transformer decoder our method ç¬¬ä¸€ä¸ªé˜¶æ®µ åšäºŒåˆ†ç±»çš„one-stage detectorï¼Œæå‰æ™¯ï¼Œ å®ç°ä¸Šå°±ç”¨region-level feature+classifierï¼ˆFCN-basedï¼‰ ç¬¬äºŒé˜¶æ®µ åšposition-basedç±»åˆ«é¢„æµ‹ å®ç°ä¸Šæ—¢å¯ä»¥ç”¨ä¸€ä¸ªFaster-RCNNï¼Œä¹Ÿå¯ä»¥ç”¨classifier æœ€ç»ˆçš„lossç”±ä¸¤ä¸ªé˜¶æ®µåˆå¹¶å¾—åˆ°ï¼Œè€Œä¸æ˜¯åˆ†é˜¶æ®µè®­ç»ƒ è·Ÿformer two-stage frameworkçš„ä¸»è¦ä¸åŒæ˜¯ åŠ äº†joint probabilistic objective over both stages ä»¥å‰çš„äºŒé˜¶æ®µRPNçš„ç”¨é€”ä¸»è¦æ˜¯æœ€å¤§åŒ–recallï¼Œdoes not produce accurate likelihoods faster and more accurate é¦–å…ˆæ˜¯ç¬¬ä¸€ä¸ªé˜¶æ®µçš„proposalæ›´å°‘æ›´å‡† å…¶æ¬¡æ˜¯ç¬¬äºŒä¸ªé˜¶æ®µmakes full use of years of progress in two-stage detectionï¼ŒäºŒé˜¶æ®µçš„è®¾è®¡ç«™åœ¨ä¼Ÿäººçš„è‚©è†€ä¸Š æ–¹æ³• joint class distributionï¼šå…ˆä»‹ç»æ€ä¹ˆå°†ä¸€äºŒé˜¶æ®µè”åŠ¨ ã€ç¬¬ä¸€é˜¶æ®µçš„å‰èƒŒæ™¯scoreã€‘ ä¹˜ä¸Š ã€ç¬¬äºŒé˜¶æ®µçš„class scoreã€‘ $P(C_k) = \sum_o P(C_k|O_k=o)P(O_k=o)$ maximum likelihood estimation for annotated objects é€€é˜¶æˆindependent maximum-likelihood $log P(C_k) = log P(C_k|O_k=1) + log P(O_k=1)$ for background class ä¸åˆ†è§£ $log P(bg) = log( P(bg|O_k=1) * P(O_k=1) + P(O_k=0))$ lower boundsï¼ŒåŸºäºjensenä¸ç­‰å¼å¾—åˆ°ä¸¤ä¸ªä¸ç­‰å¼ $log P(bg) \ge P(O_k=1) * log( P(bg|O_k=1))$ï¼šå¦‚æœä¸€é˜¶æ®µå‰æ™¯ç‡è´¼å¤§ï¼Œé‚£ä¹ˆå°± $log P(bg) \ge P(O_k=0)$ï¼š optimize both bounds jointly works better network designï¼šä»‹ç»æ€ä¹ˆåœ¨one-stage detectorçš„åŸºç¡€ä¸Šæ”¹é€ å‡ºä¸€ä¸ªtwo-stage probabilistic detector experiment with 4 different designs for first-stage RPN RetinaNet RetinaNetå…¶å®å’Œtwo-stageçš„RPNé«˜åº¦ç›¸ä¼¼æ ¸å¿ƒåŒºåˆ«åœ¨äºï¼š a heavier head designï¼š4-conv vs 1-conv RetinaNetæ˜¯backbone+fpn+individual heads RPNæ˜¯backbone+fpn+shared convs+individual heads a stricter positive and negative anchor definitionï¼šéƒ½æ˜¯IoU-based anchor selectionï¼Œthreshä¸ä¸€æ · focal loss first-stage design ä»¥ä¸Šä¸‰ç‚¹éƒ½åœ¨probabilistic modelé‡Œé¢ä¿ç•™ ç„¶åå°†separated headsæ”¹æˆshared heads centerNet æ¨¡å‹å‡çº§ å‡çº§æˆmulti-scaleï¼šuse ResNet-FPN backï¼ŒP3-P7 å¤´æ˜¯FCOSé‚£ç§å¤´ï¼šindividual headsï¼Œä¸share convï¼Œç„¶åcls branché¢„æµ‹centerness+clsï¼Œreg branché¢„æµ‹regress params æ­£æ ·æœ¬ä¹Ÿæ˜¯æŒ‰ç…§FCOSç­–ç•¥ï¼šposition &amp; scale-based å‡çº§æ¨¡å‹è¿›è¡Œone-stage &amp; two-stageå®éªŒï¼šcenterNet* ATSS æ˜¯ä¸€ä¸ªadaptive IoU threshçš„æ–¹æ³•ï¼Œcenternessæ¥è¡¨ç¤ºä¸€ä¸ªæ ¼å­çš„score æˆ‘ä»¬å°†centerness*classification scoreå®šä¹‰ä¸ºè¿™ä¸ªæ¨¡å‹çš„proposal score å¦å¤–å°±æ˜¯two-stageä¸‹è¿˜æ˜¯å°†RPNçš„cls &amp; reg headsåˆå¹¶ GFLï¼šè¿˜æ²¡çœ‹è¿‡ï¼Œå…ˆè·³è¿‡å§ second-stage designsï¼šFasterRCNN &amp; CascadeR- CNN deformable convï¼šè¿™ä¸ªåœ¨centerNetv1çš„ResNetå’ŒDLA backé‡Œé¢éƒ½ç”¨äº†ï¼Œåœ¨v2é‡Œé¢ï¼Œä¸»è¦æ˜¯ç”¨ResNeXt-32x8d-101-DCNï¼Œ hyperparameters for two-stage probabilistic model ä¸¤é˜¶æ®µæ¨¡å‹é€šå¸¸æ˜¯ç”¨P2-P6ï¼Œä¸€é˜¶æ®µé€šå¸¸ç”¨P3-P7ï¼šæˆ‘ä»¬ç”¨P3-P7 increase the positive IoU thresholdï¼š0.5 to [0.6,0.7,0.8] maximum of 256 proposals ï¼ˆå¯¹æ¯”originçš„1kï¼‰ increase nms threshold from 0.5 to 0.7 SGDï¼Œ90K iterations base learning rateï¼š0.02 for two-stage &amp; 0.01 for one-stageï¼Œ0.1 decay multi-scale trainingï¼šçŸ­è¾¹[640,800]ï¼Œé•¿è¾¹ä¸è¶…è¿‡1333 fix-scale testingï¼šçŸ­è¾¹ç”¨800ï¼Œé•¿è¾¹ä¸è¶…è¿‡1333 first stage loss weightï¼š0.5ï¼Œå› ä¸ºone-stage detectoré€šå¸¸ç”¨0.01 lrå¼€å§‹è®­ç»ƒ å®éªŒ 4ç§designçš„å¯¹æ¯” æ‰€æœ‰çš„probabilistic modeléƒ½æ¯”one-stage modelå¼ºï¼Œç”šè‡³è¿˜å¿«ï¼ˆå› ä¸ºç®€åŒ–äº†è„‘è¢‹ï¼‰ æ‰€æœ‰çš„probabilistic FasterRCNNéƒ½æ¯”åŸå§‹çš„RPN-based FasterRCNNå¼ºï¼Œä¹Ÿå¿«ï¼ˆå› ä¸ºP3-P7æ¯”P2-P6çš„è®¡ç®—é‡å°ä¸€åŠï¼Œè€Œä¸”ç¬¬äºŒé˜¶æ®µfewer proposalsï¼‰ CascadeRCNN-CenterNet design performs the bestï¼šæ‰€ä»¥ä»¥åå°±æŠŠå®ƒå«CenterNet2 å’Œå…¶ä»–real-time modelså¯¹æ¯” å¤§å¤šæ˜¯real-time modelséƒ½æ˜¯ä¸€é˜¶æ®µæ¨¡å‹ å¯ä»¥çœ‹åˆ°äºŒé˜¶æ®µä¸ä»…èƒ½å¤Ÿæ¯”ä¸€é˜¶æ®µæ¨¡å‹è¿˜å¿«ï¼Œç²¾åº¦è¿˜æ›´é«˜ SOTAå¯¹æ¯” æŠ¥äº†ä¸€ä¸ª56.4%çš„sotaï¼Œä½†æ˜¯å¤§å®¶å£ç¢‘ä¸Šå¥½åƒæ•ˆæœå¾ˆå·® ä¸æ”¾å›¾äº† corner-centerNet: Keypoint Triplets for Object Detection åŠ¨æœº based on cornerNet triplet corner keypointsï¼šweak grouping ability cause false positives correct predictions can be determined by checking the central parts cascade corner pooling and center poolling è®ºç‚¹ whats new in CenterNet triplet inference workflow after a proposal is generated as a pair of corner keypoints checking if there is a center keypoint of the same class center pooling for predicting center keypoints by making the center keypoints on feature map having the max sum Hori+Verti responses cascade corner pooling equips the original corner pooling module with the ability of perceiving internal information not only consider the boundary but also the internal directions CornetNetç—›ç‚¹ fp rateé«˜ small objectçš„fp rateå°¤å…¶é«˜ ä¸€ä¸ªideaï¼šcornerNet based RPN ä½†æ˜¯åŸç”ŸRPNéƒ½æ˜¯å¤ç”¨çš„ è®¡ç®—æ•ˆç‡ï¼Ÿ æ–¹æ³• center pooling geometric centers &amp; semantic centers center poolingèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†è¯­ä¹‰ä¿¡æ¯æœ€ä¸°å¯Œçš„ç‚¹ï¼ˆsemantic centersï¼‰ä¼ è¾¾åˆ°ç‰©ç†ä¸­å¿ƒç‚¹ï¼ˆgeometric centersï¼‰ï¼Œä¹Ÿå°±æ˜¯central region]]></content>
      <tags>
        <tag>ç›®æ ‡æ£€æµ‹ï¼Œanchor-free, one-stage&amp;two-stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[equlization loss]]></title>
    <url>%2F2020%2F12%2F21%2Fequlization-loss%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[megDet]]></title>
    <url>%2F2020%2F12%2F18%2FmegDet%2F</url>
    <content type="text"><![CDATA[MegDet: A Large Mini-Batch Object Detector åŠ¨æœº past methods mainly come from novel framework or loss design this paper studies the mini-batch size enable training with a large mini-batch size warmup learning rate policy cross-gpu batch normalization faster &amp; better acc è®ºç‚¹ potential drawbacks with small mini-batch sizes long training time inaccurate statistics for BNï¼šprevious methods use fixed statistics from ImageNet which is a sub-optimal trade-off positive &amp; negative training examples are more likely imblanced åŠ å¤§batch sizeä»¥åï¼Œæ­£è´Ÿæ ·æœ¬æ¯”ä¾‹æœ‰æå‡ï¼Œæ‰€ä»¥yolov3ä¼šå…ˆé”ç€backå¼€å¤§batchsizeåšwarmup learning rate dilemma large min-batch size usually requires large learning rate large learning rate is likely leading to convergence failure a smaller learning rate often obtains inferior results solution of the paper linear scaling rule warmup Cross-GPU Batch Normalization (CGBN) æ–¹æ³• warmup set up the learning rate small enough at the be- ginning then increase the learning rate with a constant speed after every iteration, until fixed Cross-GPU Batch Normalization ä¸¤æ¬¡åŒæ­¥ tensorpacké‡Œé¢æœ‰ ä¸€æ¬¡åŒæ­¥ å¼‚æ­¥BNï¼šbatch size è¾ƒå°æ—¶ï¼Œæ¯å¼ å¡è®¡ç®—å¾—åˆ°çš„ç»Ÿè®¡é‡å¯èƒ½ä¸æ•´ä½“æ•°æ®æ ·æœ¬å…·æœ‰è¾ƒå¤§å·®å¼‚ åŒæ­¥ï¼š éœ€è¦åŒæ­¥çš„æ˜¯æ¯å¼ å¡ä¸Šè®¡ç®—çš„ç»Ÿè®¡é‡ï¼Œå³BNå±‚ç”¨åˆ°çš„å‡å€¼$\mu$å’Œæ–¹å·®$\sigma^2$ è¿™æ ·å¤šå¡è®­ç»ƒç»“æœæ‰ä¸å•å¡è®­ç»ƒæ•ˆæœç›¸å½“ ä¸¤æ¬¡åŒæ­¥ï¼š ç¬¬ä¸€æ¬¡åŒæ­¥å‡å€¼ï¼šè®¡ç®—å…¨å±€å‡å€¼ ç¬¬äºŒæ¬¡åŒæ­¥æ–¹å·®ï¼šåŸºäºå…¨å±€å‡å€¼è®¡ç®—å„è‡ªæ–¹å·®ï¼Œå†å–å¹³å‡ ä¸€æ¬¡åŒæ­¥ï¼š æ ¸å¿ƒåœ¨äºæ–¹å·®çš„è®¡ç®— é¦–å…ˆå‡å€¼ï¼š$\mu = \frac{1}{m} \sum_{i=1}^m x_i$ ç„¶åæ˜¯æ–¹å·®ï¼š \sigma^2 = \frac{1}{m} \sum_{i=1}^m (x_i-\mu)^2 = \frac{1}{m} \sum_{i=1}^m x_i^2 - \mu^2\\ =\frac{1}{m} \sum_{i=1}^m x_i^2 - (\frac{1}{m} \sum_{i=1}^m x_i)^2 * è®¡ç®—æ¯å¼ å¡çš„$\sum x_i$å’Œ$\sum x_i^2$ï¼Œå°±å¯ä»¥ä¸€æ¬¡æ€§ç®—å‡ºæ€»å‡å€¼å’Œæ€»æ–¹å·®]]></content>
      <tags>
        <tag>ç›®æ ‡æ£€æµ‹ï¼Œlarge mini-batch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RFB]]></title>
    <url>%2F2020%2F12%2F16%2FRFB%2F</url>
    <content type="text"><![CDATA[RFB: Receptive Field Block Net for Accurate and Fast Object Detection åŠ¨æœº RF blockï¼šReceptive Fields strengthen the lightweight features using a hand-crafted mechanismï¼šè½»é‡ï¼Œç‰¹å¾è¡¨è¾¾èƒ½åŠ›å¼º assemble RFB to the top of SSD è®ºç‚¹ lightweight enhance feature representation äººç±» ç¾¤æ™ºæ„Ÿå—é‡ï¼ˆpRFï¼‰çš„å¤§å°æ˜¯å…¶è§†ç½‘è†œå›¾ä¸­åå¿ƒç‡çš„å‡½æ•° æ„Ÿå—é‡éšç€åå¿ƒç‡è€Œå¢åŠ  æ›´é è¿‘ä¸­å¿ƒçš„åŒºåŸŸåœ¨è¯†åˆ«ç‰©ä½“æ—¶æ‹¥æœ‰æ›´é«˜çš„æ¯”é‡æˆ–ä½œç”¨ å¤§è„‘åœ¨å¯¹äºå°çš„ç©ºé—´å˜åŒ–ä¸æ•æ„Ÿ fixed sampling grid (conv) probably induces some loss in the feature discriminability as well as robustness inception RFs of multiple sizes but at the same center ASPP with different atrous rates the resulting feature tends to be less distinctive Deformable CNN sampling grid is flexible but all pixels in an RF contribute equally RFB varying kernel sizes applies dilated convolution layers to control their eccentricities ç»„åˆæ¥æ¨¡æ‹Ÿhuman visual system concat 1x1 conv for fusion main contributions RFB module: enhance deep features of lightweight CNN networks RFB Net: gain on SSD assemble on MobileNet æ–¹æ³• Receptive Field Block ç±»ä¼¼inceptionçš„multi-branch dilated pooling or convolution layer RFB Net SSD-base å¤´ä¸Šæœ‰è¾ƒå¤§åˆ†è¾¨ç‡çš„ç‰¹å¾å›¾çš„convå±‚are replaced by the RFB module ç‰¹åˆ«å¤´ä¸Šçš„convå±‚å°±ä¿ç•™äº†ï¼Œå› ä¸ºtheir feature maps are too small to apply filters with large kernels like 5 Ã— 5 stride2 moduleï¼šæ¯ä¸ªconv stride2ï¼Œé‚£id pathå¾—å˜æˆ1x1 convï¼Ÿ]]></content>
      <tags>
        <tag>ç›®æ ‡æ£€æµ‹</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PANet]]></title>
    <url>%2F2020%2F12%2F02%2FPANet%2F</url>
    <content type="text"><![CDATA[PANet: Path Aggregation Network for Instance Segmentation åŠ¨æœº boost the information flow bottom-up path shorten information path enhance accurate localization adaptive feature pooling aggregate all levels avoiding arbitrarily assigned results mask prediction head fcn + fc captures different views, possess complementary properties subtle extra computational è®ºç‚¹ previous skills: fcn, fpn, residual, dense findings é«˜å±‚ç‰¹å¾ç±»åˆ«å‡†ï¼Œåº•å±‚ç‰¹å¾å®šä½å‡†ï¼Œä½†æ˜¯é«˜å±‚å’Œåº•å±‚ç‰¹å¾ä¹‹é—´çš„pathå¤ªé•¿äº†ï¼Œä¸åˆ©äºåŒé«˜ past proposals make predictions based on one level PANet bottom-up path shorten information path enhance accurate localization adaptive feature pooling aggregate all levels avoiding arbitrarily assigned results mask prediction head fcn + fc captures different views, possess complementary properties æ–¹æ³• framework b: bottom-up path c: adaptive feature pooling e: fusion mask branch bottom-up path fpnâ€™s top-down path: to propagate strong semantical information to ensure reasonable classification capability long path: red line, 100+ layers bottom-up path: enhances the localization capability short path: green line, less than 10 layers for each level $N_l$ input: $N_{l+1}$ &amp; $P_l$ $N_{l+1}$ 3x3 conv &amp; $P_l$ id path - add - 3x3 conv channel 256 ReLU after conv adaptive feature pooling pool features from all levels, then fuse, then predict steps map each proposal to all feature levels roi align go through one layer of the following sub-networks independently fusion operation (element-wise max or sum) ä¾‹å¦‚ï¼Œbox branchæ˜¯ä¸¤ä¸ªfcå±‚ï¼Œæ¥è‡ªå„ä¸ªlevelçš„roi alignä¹‹åçš„proposal featuresï¼Œå…ˆå„è‡ªç»è¿‡ä¸€ä¸ªfcå±‚ï¼Œå†share the following till the headï¼Œmask branchæ˜¯4ä¸ªconvå±‚ï¼Œæ¥è‡ªå„ä¸ªlevelçš„roi alignä¹‹åçš„proposal featuresï¼Œå…ˆå„è‡ªç»è¿‡ä¸€ä¸ªconvå±‚ï¼Œå†share the following till the head fusion mask branch fc layers are location sensitive helpful to differentiate instances and recognize separate parts belonging to the same object convåˆ†æ”¯ 4ä¸ªè¿ç»­conv+1ä¸ªdeconvï¼š3x3 convï¼Œchannel256ï¼Œdeconv factor=2 predict mask of each classï¼šoutput channel n_classes fcåˆ†æ”¯ from convåˆ†æ”¯çš„conv3è¾“å‡º 2ä¸ªè¿ç»­convï¼Œchannel256ï¼Œchannel128 fcï¼Œdim=28x28ï¼Œç‰¹å¾å›¾å°ºå¯¸ï¼Œç”¨äºå‰èƒŒæ™¯åˆ†ç±» final maskï¼šadd å®éªŒ heavier head 4 consecutive 3x3 convs shared among reg &amp; cls åœ¨multi-taskçš„æƒ…å†µä¸‹ï¼Œå¯¹boxçš„é¢„æµ‹æœ‰æ•ˆ]]></content>
      <tags>
        <tag>å®ä¾‹åˆ†å‰²</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSPNet]]></title>
    <url>%2F2020%2F11%2F17%2FCSPNet%2F</url>
    <content type="text"><![CDATA[CSPNET: A NEW BACKBONE THAT CAN ENHANCE LEARNING CAPABILITY OF CNN åŠ¨æœº propose a network from the respect of the variability of the gradients reduces computations superior accuracy while being lightweightening è®ºç‚¹ CNN architectures design ResNeXtï¼šcardinality can be more effective than width and depth DenseNetï¼šreuse features partial ResNetï¼šhigh cardinality and sparse connectionï¼Œthe concept of gradient combination introduce Cross Stage Partial Network (CSPNet) strengthening learning ability of a CNNï¼šsufficient accuracy while being lightweightening removing computational bottlenecksï¼šhoping evenly distribute the amount of computation at each layer in CNN reducing memory costsï¼šadopt cross-channel pooling during fpn æ–¹æ³• ç»“æ„ Partial Dense Blockï¼šèŠ‚çœä¸€åŠè®¡ç®— Partial Transition Layerï¼šfusion lastèƒ½å¤Ÿsave computationåŒæ—¶ç²¾åº¦ä¸æ‰å¤ªå¤š è®ºæ–‡è¯´fusion firstä½¿å¾—å¤§é‡æ¢¯åº¦å¾—åˆ°é‡ç”¨ï¼Œcomputation cost is significantly droppedï¼Œfusion lastä¼šæŸå¤±éƒ¨åˆ†æ¢¯åº¦é‡ç”¨ï¼Œä½†æ˜¯ç²¾åº¦æŸå¤±ä¹Ÿæ¯”è¾ƒå°(0.1)ã€‚ it is obvious that if one can effectively reduce the repeated gradient information, the learning ability of a network will be greatly improved. Apply CSPNet to Other Architectures å› ä¸ºåªæœ‰ä¸€åŠçš„channelå‚ä¸resnet blockçš„è®¡ç®—ï¼Œæ‰€ä»¥æ— éœ€å†å¼•å…¥bottleneckç»“æ„äº† æœ€åä¸¤ä¸ªpathçš„è¾“å‡ºconcat EFM fusion * ç‰¹å¾é‡‘å­—å¡”ï¼ˆFPNï¼‰ï¼šèåˆå½“å‰å°ºåº¦å’Œä»¥å‰å°ºåº¦çš„ç‰¹å¾ã€‚ * å…¨å±€èåˆæ¨¡å‹ï¼ˆGFMï¼‰ï¼šèåˆæ‰€æœ‰å°ºåº¦çš„ç‰¹å¾ã€‚ * ç²¾ç¡®èåˆæ¨¡å‹ï¼ˆEFMï¼‰ï¼šèåˆanchorå°ºå¯¸ä¸Šçš„ç‰¹å¾ã€‚ EFM assembles features from the three scalesï¼šå½“å‰å°ºåº¦&amp;ç›¸é‚»å°ºåº¦ åŒæ—¶åˆåŠ äº†ä¸€ç»„bottom-upçš„èåˆ Maxout techniqueå¯¹ç‰¹å¾æ˜ å°„è¿›è¡Œå‹ç¼© ç»“è®º ä»å®éªŒç»“æœæ¥çœ‹ï¼Œ åˆ†ç±»é—®é¢˜ä¸­ï¼Œä½¿ç”¨CSPNetå¯ä»¥é™ä½è®¡ç®—é‡ï¼Œä½†æ˜¯å‡†ç¡®ç‡æå‡å¾ˆå°ï¼› åœ¨ç›®æ ‡æ£€æµ‹é—®é¢˜ä¸­ï¼Œä½¿ç”¨CSPNetä½œä¸ºBackboneå¸¦æ¥çš„æå‡æ¯”è¾ƒå¤§ï¼Œå¯ä»¥æœ‰æ•ˆå¢å¼ºCNNçš„å­¦ä¹ èƒ½åŠ›ï¼ŒåŒæ—¶ä¹Ÿé™ä½äº†è®¡ç®—é‡ã€‚æœ¬æ–‡æ‰€æå‡ºçš„EFMæ¯”GFMæ…¢2fpsï¼Œä½†APå’ŒAP50åˆ†åˆ«æ˜¾è‘—æé«˜äº†2.1%å’Œ2.4%ã€‚]]></content>
  </entry>
  <entry>
    <title><![CDATA[nms]]></title>
    <url>%2F2020%2F10%2F29%2Fnms%2F</url>
    <content type="text"><![CDATA[Non-maximum suppressionï¼šéæå¤§å€¼æŠ‘åˆ¶ç®—æ³•ï¼Œæœ¬è´¨æ˜¯æœç´¢å±€éƒ¨æå¤§å€¼ï¼ŒæŠ‘åˆ¶éæå¤§å€¼å…ƒç´  [nms]ï¼šstandard nmsï¼Œå½“ç›®æ ‡æ¯”è¾ƒå¯†é›†ã€å­˜åœ¨é®æŒ¡æ—¶ï¼Œæ¼æ£€ç‡é«˜ [soft nms]ï¼šæ”¹å˜nmsçš„hard thresholdï¼Œç”¨è¾ƒä½çš„åˆ†æ•°æ›¿ä»£0ï¼Œæå‡recall [softer nms]ï¼šå¼•å…¥box position confidenceï¼Œé€šè¿‡åå¤„ç†æé«˜å®šä½ç²¾åº¦ [DIoU nms]ï¼šé‡‡ç”¨DIoUçš„è®¡ç®—æ–¹å¼æ›¿æ¢IoUï¼Œå› ä¸ºDIoUçš„è®¡ç®—è€ƒè™‘åˆ°äº†ä¸¤æ¡†ä¸­å¿ƒç‚¹ä½ç½®çš„ä¿¡æ¯ï¼Œæ•ˆæœæ›´ä¼˜ [fast nms]ï¼šYOLOACTå¼•å…¥çŸ©é˜µä¸‰è§’åŒ–ï¼Œä¼šæ¯”Traditional NMSæŠ‘åˆ¶æ›´å¤šçš„æ¡†ï¼Œæ€§èƒ½ç•¥å¾®ä¸‹é™ [cluster nms]ï¼šCIoUæå‡ºï¼Œå¼¥è¡¥Fast NMSçš„æ€§èƒ½ä¸‹é™ï¼Œè¿ç®—æ•ˆç‡æ¯”Fast NMSä¸‹é™äº†ä¸€äº› [mask nms]ï¼šmask iouè®¡ç®—æœ‰ä¸å¯å¿½ç•¥çš„å»¶è¿Ÿï¼Œå› æ­¤æ¯”box nmsæ›´è€—æ—¶ [matrix nms]ï¼šSOLOå°†mask IoUå¹¶è¡ŒåŒ–ï¼Œæ¯”FAST-NMSè¿˜å¿«ï¼Œæ€è·¯å’ŒFAST-NMSä¸€æ ·ä»ä¸Šä¸‰è§’IoUçŸ©é˜µå‡ºå‘ï¼Œå¯èƒ½é€ æˆè¿‡å¤šæŠ‘åˆ¶ã€‚ [WBF]ï¼šåŠ æƒæ¡†èåˆï¼ŒKaggleèƒ¸ç‰‡å¼‚ç‰©æ¯”èµ›claimæœ‰ç”¨ï¼Œé€Ÿåº¦æ…¢ï¼Œå¤§æ¦‚æ¯”æ ‡å‡†NMSæ…¢3å€ï¼ŒWBFå®éªŒä¸­æ˜¯åœ¨å·²ç»å®ŒæˆNMSçš„æ¨¡å‹ä¸Šè¿›è¡Œçš„ nms è¿‡æ»¤+è¿­ä»£+éå†+æ¶ˆé™¤ é¦–å…ˆè¿‡æ»¤æ‰å¤§é‡ç½®ä¿¡åº¦è¾ƒä½çš„æ¡†ï¼Œå¤§äºconfidence threshçš„boxä¿ç•™ å°†æ‰€æœ‰æ¡†çš„å¾—åˆ†æ’åºï¼Œé€‰ä¸­æœ€é«˜åˆ†çš„æ¡† éå†å…¶ä½™çš„æ¡†ï¼Œå¦‚æœå’Œå½“å‰æœ€é«˜åˆ†æ¡†çš„IOUå¤§äºä¸€å®šé˜ˆå€¼(nms thresh)ï¼Œå°±å°†æ¡†åˆ é™¤(score=0) ä»æœªå¤„ç†çš„æ¡†ä¸­ç»§ç»­é€‰ä¸€ä¸ªå¾—åˆ†æœ€é«˜çš„ï¼Œé‡å¤ä¸Šè¿°è¿‡ç¨‹ when evaluation iou threshï¼šç•™ä¸‹çš„boxé‡Œé¢ï¼Œä¸gt boxçš„iouå¤§äºiou threshçš„boxä½œä¸ºæ­£ä¾‹ï¼Œç”¨äºè®¡ç®—å‡ºAPå’ŒmAPï¼Œé€šè¿‡è°ƒæ•´confidence threshå¯ä»¥ç”»å‡ºPRæ›²çº¿ softnms åŸºæœ¬æµç¨‹è¿˜æ˜¯nmsçš„è´ªå©ªæ€è·¯ï¼Œè¿‡æ»¤+è¿­ä»£+éå†+è¡°å‡ re-score functionï¼šhigh overlap decays more linearï¼š for each $iou(M,b_i)&gt;th$ï¼Œ $s_i=s_i(1-iou)$ not continuousï¼Œsudden penalty gaussianï¼š for all remaining detection boxesï¼Œ$s_i=s_i e^{-\frac{iou(M,b_i)}{\sigma}}$ ç®—æ³•æµç¨‹ä¸Šæœªåšä¼˜åŒ–ï¼Œæ˜¯é’ˆå¯¹ç²¾åº¦çš„ä¼˜åŒ– softer nms è·Ÿsoft nmsæ²¡å…³ç³» å…·æœ‰é«˜åˆ†ç±»ç½®ä¿¡åº¦çš„è¾¹æ¡†å…¶ä½ç½®å¹¶ä¸æ˜¯æœ€ç²¾å‡†çš„ æ–°å¢åŠ äº†ä¸€ä¸ªå®šä½ç½®ä¿¡åº¦çš„é¢„æµ‹ï¼Œä½¿å…¶æœä»é«˜æ–¯åˆ†å¸ƒ inferé˜¶æ®µè¾¹æ¡†çš„æ ‡å‡†å·®å¯ä»¥è¢«çœ‹åšè¾¹æ¡†çš„ä½ç½®ç½®ä¿¡åº¦ï¼Œä¸åˆ†ç±»ç½®ä¿¡åº¦åšåŠ æƒå¹³å‡ï¼Œä½œä¸ºtotal score ç®—æ³•æµç¨‹ä¸Šæœªåšä¼˜åŒ–ï¼Œå®Œå…¨æ˜¯ç²¾åº¦çš„ä¼˜åŒ– DIoU nms ä¹Ÿæ˜¯ä¸ºäº†è§£å†³hard nmsåœ¨å¯†é›†åœºæ™¯ä¸­æ¼æ£€ç‡é«˜çš„é—®é¢˜ ä½†æ˜¯ä¸åŒäºsoft nmsçš„æ˜¯ï¼ŒDçš„æ”¹è¿›åœ¨iouè®¡ç®—ä¸Šï¼Œè€Œä¸æ˜¯åœ¨score diouçš„è®¡ç®—ï¼š$diou = iou-\frac{\rho^2(b_1, b_2)}{c^2}$ ç®—æ³•æµç¨‹ä¸Šæœªåšä¼˜åŒ–ï¼Œä»æ—§æ˜¯ç²¾åº¦çš„ä¼˜åŒ– fast nms yoloactæå‡º ä¸»è¦æ•ˆç‡æå‡åœ¨äºç”¨çŸ©é˜µæ“ä½œæ›¿æ¢éå†ï¼Œæ‰€æœ‰æ¡†åŒæ—¶è¢«filteræ‰ï¼Œè€Œéä¾æ¬¡éå†åˆ é™¤ iouä¸Šä¸‰è§’çŸ©é˜µ iouä¸Šä¸‰è§’çŸ©é˜µçš„æ¯ä¸€ä¸ªå…ƒç´ éƒ½æ˜¯è¡Œå·å°äºåˆ—å· iouä¸Šä¸‰è§’çŸ©é˜µçš„æ¯ä¸€ä¸ªè¡Œï¼Œå¯¹åº”ä¸€ä¸ªbnd boxï¼Œä¸å…¶ä»–æ‰€æœ‰scoreå°äºå®ƒçš„bnd boxçš„iou iouä¸Šä¸‰è§’çŸ©é˜µçš„æ¯ä¸€ä¸ªåˆ—ï¼Œå¯¹åº”ä¸€ä¸ªbnd boxï¼Œä¸å…¶ä»–æ‰€æœ‰scoreå¤§äºå®ƒçš„bnd boxçš„iou fast nmsåœ¨iouçŸ©é˜µæ¯ä¸€åˆ—ä¸Šæ±‚æœ€å¤§å€¼ï¼Œå¦‚æœè¿™ä¸ªæœ€å¤§å€¼å¤§äºiou threshï¼Œè¯´æ˜å½“å‰åˆ—å¯¹åº”çš„bnd boxï¼Œå­˜åœ¨ä¸€ä¸ªscoreå¤§äºå®ƒï¼Œä¸”å’Œå®ƒé‡å åº¦è¾ƒé«˜çš„bnd boxï¼Œå› æ­¤è¦æŠŠè¿™ä¸ªboxè¿‡æ»¤æ‰ æœ‰ç²¾åº¦æŸå¤± åœºæ™¯ï¼š å¦‚æœæ˜¯hard nmsçš„è¯ï¼Œé¦–å…ˆéå†b1çš„å…¶ä»–boxï¼Œb2å°±è¢«åˆ é™¤äº†ï¼Œè¿™æ˜¯b3å°±ä¸å­˜åœ¨é«˜é‡å æ¡†äº†ï¼Œb3å°±ä¼šè¢«ç•™ä¸‹ï¼Œä½†æ˜¯åœ¨fast nmsåœºæ™¯ä¸‹ï¼Œæ‰€æœ‰æ¡†è¢«åŒæ—¶åˆ é™¤ï¼Œå› æ­¤b2ã€b3éƒ½æ²¡äº†ã€‚ cluster nms é’ˆå¯¹fast nmsæ€§èƒ½ä¸‹é™çš„å¼¥è¡¥ fast nmsæ€§èƒ½ä¸‹é™ï¼Œä¸»è¦é—®é¢˜åœ¨äºè¿‡åº¦æŠ‘åˆ¶ï¼Œå¹¶è¡Œæ“ä½œæ— æ³•åŠæ—¶æ¶ˆé™¤high scoreæ¡†æŠ¹æ‰å¯¹åç»­low scoreæ¡†åˆ¤æ–­çš„å½±å“ ç®—æ³•æµç¨‹ä¸Šï¼Œå°†fast nmsçš„ä¸€æ¬¡é˜ˆå€¼æ“ä½œï¼Œè½¬æ¢æˆå°‘æ•°å‡ æ¬¡çš„è¿­ä»£æ“ä½œï¼Œæ¯æ¬¡éƒ½æ˜¯ä¸€ä¸ªfast nms å›¾ä¸­Xè¡¨ç¤ºiouçŸ©é˜µï¼Œbè¡¨ç¤ºnmsé˜ˆå€¼äºŒå€¼åŒ–ä»¥åçš„å‘é‡ï¼Œä¹Ÿå°±æ˜¯fast nmsé‡Œé¢é‚£ä¸ªä¿ç•™ï¼æŠ‘åˆ¶å‘é‡ æ¯æ¬¡è¿­ä»£ï¼Œç®—æ³•å°†bå±•å¼€æˆä¸€ä¸ªå¯¹è§’çŸ©é˜µï¼Œç„¶åå·¦ä¹˜iouçŸ©é˜µ ç›´åˆ°å‡ºç°æŸä¸¤æ¬¡è¿­ä»£åï¼Œ bä¿æŒä¸å˜äº†ï¼Œé‚£ä¹ˆè¿™å°±æ˜¯æœ€ç»ˆçš„b cluster nmsçš„è¿­ä»£æ“ä½œï¼Œå…¶å®å°±æ˜¯åœ¨çœç•¥ä¸Šä¸€æ¬¡Fast NMSè¿­ä»£ä¸­è¢«æŠ‘åˆ¶çš„æ¡†å¯¹å…¶ä»–æ¡†çš„å½±å“ æ•°å­¦å½’çº³æ³•è¯æ˜ï¼Œcluster nmsçš„ç»“æœä¸hard nmså®Œå…¨ä¸€è‡´ï¼Œè¿ç®—æ•ˆç‡æ¯”fast nmsä¸‹é™äº†ä¸€äº›ï¼Œä½†æ˜¯æ¯”hard nmså¿«å¾—å¤š cluster nmsçš„è¿ç®—æ•ˆç‡ä¸ä¸clusteræ•°é‡æœ‰å…³ï¼Œåªä¸éœ€è¦è¿­ä»£æ¬¡æ•°æœ€å¤šçš„é‚£ä¸€ä¸ªclusteræœ‰å…³ mask nms ä»æ£€æµ‹æ¡†å½¢çŠ¶çš„è§’åº¦æ‹“å±•å‡ºæ¥ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºmask nmsã€polygon nmsä»¥åŠinclined nms iouçš„è®¡ç®—æ–¹å¼æœ‰ä¸€ç§æ˜¯mmiï¼š$mmi=max(\frac{I}{I_A}, \frac{I}{I_B})$ matrix nms å­¦ä¹ soft nmsï¼šdecay factor one step furtherï¼šè¿­ä»£æ”¹å¹¶è¡Œ å¯¹äºæŸä¸ªobject $m_j$çš„scoreè¿›è¡Œpenaltyçš„æ—¶å€™è€ƒè™‘ä¸¤éƒ¨åˆ†å½±å“ è¿­ä»£æŸä¸ª$m_i$æ—¶ï¼Œå¯¹åç»­lower scoreçš„$m_j$çš„å½±å“ ä¸€æ˜¯æ­£é¢å½±å“$f(iou_{i,j})\ linear/guassian$ï¼šè¿™ä¸ªæ¡†ä¿ç•™ï¼Œé‚£ä¹ˆåç»­æ¡†éƒ½è¦åŸºäºä¸å…¶çš„iouåšdecay äºŒæ˜¯åå‘å½±å“$f(iou_{*,i})=max_{\forall s_k&gt;s_i}f(iou_{k,i})$ï¼šå¦‚æœè¿™ä¸ªæ¡†ä¸ä¿ç•™ï¼Œé‚£ä¹ˆå¯¹äºåç»­æ¡†æ¥è®²ï¼Œåº”è¯¥æ¶ˆé™¤è¿™ä¸ªæ¡†å¯¹å…¶çš„decayï¼Œé€‰æœ€å¤§å€¼çš„æ„ä¹‰æ˜¯å½“å‰maskè¢«æŠ‘åˆ¶æœ€æœ‰å¯èƒ½å°±æ˜¯å’Œä»–é‡å åº¦æœ€å¤§çš„é‚£ä¸ªmaskå¹²çš„ï¼ˆå› ä¸ºå¯¹åº”çš„æ­£é¢å½±å“1-iouæœ€å°ï¼‰ final decay factorï¼š$decay_j=min_{\forall s_i &gt; s_j}\frac{f(iou_{i,j})}{f(iou_{*,i})}$ ç®—æ³•æµç¨‹ &lt;img src=&quot;nms/matrixnms.png&quot; width=&quot;50%;&quot; /&gt; * æŒ‰ç…§åŸè®ºæ–‡çš„å®ç°ï¼Œdecayæ°¸è¿œå¤§äºç­‰äº1ï¼Œå› ä¸ºæ¯ä¸€åˆ—çš„iou_cmaxæ°¸è¿œå¤§äºç­‰äºiouï¼Œä»è®ºæ–‡çš„æ€è·¯æ¥çœ‹ï¼Œæ¯ä¸ªmaskçš„decayæ˜¯å®ƒä¹‹å‰æ‰€æœ‰maskçš„å½±å“å åŠ åœ¨ä¸€èµ·ï¼Œæ‰€ä»¥åº”è¯¥æ˜¯ä¹˜ç§¯è€Œä¸æ˜¯minï¼š 123456789101112# åŸè®ºæ–‡å®ç°if method=='gaussian': decay = np.exp(-(np.square(iou)-np.square(iou_cmax))/sigma)else: decay = (1-iou)/(1-iou_cmax)decay = np.min(decay, axis=0)# æ”¹è¿›å®ç°if method=='gaussian': decay = np.exp(-(np.sum(np.square(iou),axis=0)-np.square(iou_cmax))/sigma)else: decay = np.prod(1-iou)/(1-iou_cmax)]]></content>
      <tags>
        <tag>ç›®æ ‡æ£€æµ‹</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MimicDet]]></title>
    <url>%2F2020%2F10%2F14%2FMimicDet%2F</url>
    <content type="text"><![CDATA[[MimicDet] ResNeXt-101 backbone on the COCO: 46.1 mAP MimicDet: Bridging the Gap Between One-Stage and Two-Stage Object Detection åŠ¨æœº mimic taskï¼šknowledge distillation mimic the two-stage features a shared backbone two heads for mimicking end-to-end training specialized designs to facilitate mimicking dual-path mimicking staggered feature pyramid reach two-stage accuracy è®ºç‚¹ one-stage detectors adopt a straightforward fully convolutional architecture two-stage detectors use RPN + R-CNN advantages of two-stage detectors avoid class imbalance less proposals enables larger cls net and richer features RoIAlign extracts location consistent feature -&gt; better represenation regress the object location twice -&gt; better refined one-stage detectorsâ€™ imitation RefineDetï¼šcascade detection flow AlignDetï¼šRoIConv layer still leaves a big gap network mimicking knowledge distillation use a well-trained large teacher model to supervise difference mimic in heads instead of backbones teacher branch instead of model trained jointly this method not only mimic the structure design, but also imitate in the feature level contains both one-stage detection head and two-stage detection head during training share the same backbone two-stage detection head, called T-head one-stage detection head, called S-head similarity loss for matching featureï¼šguided deformable conv layer together with detection losses specialized designs decomposed detection heads conduct mimicking in classification and regression branches individually staggered feature pyramid æ–¹æ³• overview back &amp; fpn RetinaNet fpnï¼šwith P6 &amp; P7 crucial modificationï¼šP2 ï½ P7 staggered feature pyramid high-res set {P2 to P6}ï¼šfor T-head &amp; accuray low-res set {P3 to P7}ï¼šfor S-head &amp; computation speed refinement module filter out easy negativesï¼šmitigate the class imbalance issue adjust the location and size of pre-defined anchor boxesï¼šanchor initialization module on top of the feature pyramid one 3x3 conv two sibling 1x1 convs binary classificationï¼šbce loss bounding box regressionï¼šthe same as Faster R-CNNï¼ŒL1 loss top-ranked boxes transferred to T-head and S-head one anchor on each positionï¼šavoid feature sharing among proposals assign the objects to feature pyramid according to their scale positive areaï¼š0.3 times shrinking of gt boxes from center positive sampleï¼š valid scale rangeï¼šgt target belongs to this level central point of anchor lies in the positive area detection heads T-head heavy head run on a sparse set of anchor boxes use the staggered feature pyramid generate 7x7 location-sensitive features for each anchor box cls branch two 1024-d fc layers one 81-d fc layer + softmaxï¼šce loss reg branch four 3x3 convsï¼Œch256 flatten 1024-d fc 4-d fcï¼šL1 loss mimicking target 81-d classification logits 1024-d regression feature S-head light-weight directly dense detection on fpn ã€ä¸å¤ªç†è§£ã€‘introducing the refinement module will break the location consistency between the anchor box and its corresponding featuresï¼šæˆ‘çš„ç†è§£æ˜¯refineä»¥åçš„anchorå’ŒåŸå§‹anchorå¯¹åº”çš„ç‰¹å¾å›¾misalignäº†ï¼ŒT-headç”¨çš„æ˜¯refined anchorï¼ŒS-headç”¨çš„æ˜¯original gridï¼Œæ‰€ä»¥misalign use deformable convolution to capture the misaligned feature deformation offset is computed by a micro-network takes the regression output of the refinement module as input three 1x1 convsï¼Œch64/128ï¼18(50) 3x3 Dconv for P3 and 5x5 for othersï¼Œch256 two sibling 1x1 convsï¼Œch1024 cls branchï¼š1x1 convï¼Œch80 reg branchï¼š1x1 convï¼Œch4 head mimicking cosine similarity cls logits &amp; refine params To get the S-head feature of an adjusted anchor box trace back to its initial position extract the pixel at that position in the feature map lossï¼š$L_{mimic} = 1 - cosine(F_i^T, F_i^S)$ multi-task training loss $L = L_R + L_S + L_T + L_{mimic}$ $L_R$ï¼šrefine module lossï¼Œbce+L1 $L_S$ï¼šS-head lossï¼Œce+L1 $L_T$ï¼šT-head lossï¼Œce+L1 $L_{mimic}$ï¼šmimic loss training details networkï¼šresnet50/101ï¼Œresize image with shorter side 800 refinement module run NMS with 0.8 IoU threshold on anchor boxes select top 2000 boxes T-head sample 128 boxes from proposal pï¼nï¼š1/3 S-head hard miningï¼šselect 128 boxes with top loss value inference take top 1000 boxes from refine module NMS with 0.6 IoU threshold and 0.005 score threshold ã€ï¼Ÿï¼Ÿã€‘finally top 100 scoring boxesï¼šè¿™å—ä¸å¤ªç†è§£ï¼Œæœ€ååº”è¯¥ä¸æ˜¯ç»“æ„åŒ–è¾“å‡ºäº†å•Šï¼Œåº”è¯¥æ˜¯ä¸€é˜¶æ®µæ£€æµ‹å¤´çš„re-refineè¾“å‡ºå•Š]]></content>
      <tags>
        <tag>ç›®æ ‡æ£€æµ‹</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[metrics]]></title>
    <url>%2F2020%2F10%2F09%2Fmetrics%2F</url>
    <content type="text"><![CDATA[åˆ†ç±»æŒ‡æ ‡ recallï¼šå¬å›ç‡ precisionï¼šå‡†ç¡®ç‡ accuracyï¼šæ­£ç¡®ç‡ F-Measure sensitivityï¼šçµæ•åº¦ specificityï¼šç‰¹å¼‚åº¦ TPR FPR ROC AUC æ··æ·†çŸ©é˜µ | | gt is p | gt is n | | :â€”â€”â€”-: | :â€”â€”â€”â€”: | :â€”â€”â€”â€”â€”: | | pred is p | tp | fpï¼ˆå‡é˜³æ€§ï¼‰ | | pred is n | fnï¼ˆæ¼æ£€ï¼‰ | tn | æ³¨æ„åŒºåˆ†fpå’Œfn fpï¼šè¢«é”™è¯¯åœ°åˆ’åˆ†ä¸ºæ­£ä¾‹çš„ä¸ªæ•°ï¼Œå³å®é™…ä¸ºè´Ÿä¾‹ä½†è¢«åˆ†ç±»å™¨åˆ’åˆ†ä¸ºæ­£ä¾‹çš„å®ä¾‹æ•° fnï¼šè¢«é”™è¯¯åœ°åˆ’åˆ†ä¸ºè´Ÿä¾‹çš„ä¸ªæ•°ï¼Œå³å®é™…ä¸ºæ­£ä¾‹ä½†è¢«åˆ†ç±»å™¨åˆ’åˆ†ä¸ºè´Ÿä¾‹çš„å®ä¾‹æ•° recall è¡¡é‡æŸ¥å…¨ç‡ å¯¹gt is påšç»Ÿè®¡ $recall = \frac{tp}{tp+fn}$ precision è¡¡é‡æŸ¥å‡†ç‡ å¯¹pred is påšç»Ÿè®¡ $precision = \frac{tp}{tp+fp}$ accuracy å¯¹çš„é™¤ä»¥æ‰€æœ‰ $accuracy = \frac{tp+tn}{p+n}$ sensitivity è¡¡é‡åˆ†ç±»å™¨å¯¹æ­£ä¾‹çš„è¯†åˆ«èƒ½åŠ› å¯¹gt is påšç»Ÿè®¡ $sensitivity = \frac{tp}{p}=\frac{tp}{tp+fn}$ specificity è¡¡é‡åˆ†ç±»å™¨å¯¹è´Ÿä¾‹çš„è¯†åˆ«èƒ½åŠ› å¯¹gt is nåšç»Ÿè®¡ $specificity =\frac{tn}{n}= \frac{tn}{fp+tn}$ F-measure ç»¼åˆè€ƒè™‘På’ŒRï¼Œæ˜¯Precisionå’ŒRecallåŠ æƒè°ƒå’Œå¹³å‡ $F = \frac{(a^2+1)PR}{a^2*P+R}$ $F_1 = \frac{2PR}{P+R}$ TPR å°†æ­£ä¾‹åˆ†å¯¹çš„æ¦‚ç‡ å¯¹gt is tåšç»Ÿè®¡ $TPR = \frac{tp}{tp+fn}$ FPR å°†è´Ÿä¾‹é”™åˆ†ä¸ºæ­£ä¾‹çš„æ¦‚ç‡ å¯¹gt is nåšç»Ÿè®¡ $FPR = \frac{fp}{fp+tn}$ FPR = 1 - ç‰¹å¼‚åº¦ ROC æ¯ä¸ªç‚¹çš„æ¨ªåæ ‡æ˜¯FPRï¼Œçºµåæ ‡æ˜¯TPR æç»˜äº†åˆ†ç±»å™¨åœ¨TPï¼ˆçœŸæ­£çš„æ­£ä¾‹ï¼‰å’ŒFPï¼ˆé”™è¯¯çš„æ­£ä¾‹ï¼‰é—´çš„trade-off é€šè¿‡å˜åŒ–é˜ˆå€¼ï¼Œå¾—åˆ°ä¸åŒçš„åˆ†ç±»ç»Ÿè®¡ç»“æœï¼Œè¿æ¥è¿™äº›ç‚¹å°±å½¢æˆROC curve æ›²çº¿åœ¨å¯¹è§’çº¿å·¦ä¸Šæ–¹ï¼Œç¦»å¾—è¶Šè¿œè¯´æ˜åˆ†ç±»æ•ˆæœå¥½ P/Rå’ŒROCæ˜¯ä¸¤ä¸ªä¸åŒçš„è¯„ä»·æŒ‡æ ‡å’Œè®¡ç®—æ–¹å¼ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæ£€ç´¢ç”¨å‰è€…ï¼Œåˆ†ç±»ã€è¯†åˆ«ç­‰ç”¨åè€… AUC AUCçš„å€¼å°±æ˜¯å¤„äºROC curveä¸‹æ–¹çš„é‚£éƒ¨åˆ†é¢ç§¯çš„å¤§å° é€šå¸¸ï¼ŒAUCçš„å€¼ä»‹äº0.5åˆ°1.0ä¹‹é—´]]></content>
      <tags>
        <tag>è¯„ä»·æŒ‡æ ‡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[metric learningç³»åˆ—]]></title>
    <url>%2F2020%2F09%2F25%2Fmetric-learning%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[å‚è€ƒï¼šhttps://gombru.github.io/2019/04/03/ranking_loss/ï¼Œåšä¸»å®éªŒä¸‹æ¥è§‰å¾—Triplet Loss outperforms Cross-Entropy Loss ç»¼è¿° metric learning å¸¸è§„çš„cls lossç³»åˆ—(CEã€BCEã€MSE)çš„ç›®æ ‡æ˜¯predict a label metric lossç³»åˆ—çš„ç›®æ ‡åˆ™æ˜¯predict relative distances between inputs å¸¸ç”¨åœºæ™¯ï¼šäººè„¸ &amp; fine-grained relation between samples first get the embedded representation then compute the similarity score binary (similar / dissimilar) regression (euclidian distance) å¤§ç±»ï¼šä¸ç®¡å«å•¥ï¼Œä¸»ä½“ä¸Šå°±ä¸¤ç±»ï¼ŒäºŒå…ƒç»„å’Œä¸‰å…ƒç»„ common targetï¼šæ‹‰è¿‘ç±»å†…è·ç¦»ï¼Œæ‹‰å¤§ç±»é—´è·ç¦» pairs anchor + sample positive pairsï¼šdistance â€”&gt; 0 negative pairsï¼šdisctance &gt; a margin triplets anchor + pos sample + neg sample targetï¼š(dissimilar distance - similar distance) â€”&gt; a margin papers [siamese network] Signature Verification using a â€˜Siameseâ€™ Time Delay Neural Networkï¼š1993ï¼Œlecunï¼Œå­ªç”Ÿç½‘ç»œå§‹ç¥–ï¼Œä¿©ä¸ªå­ç½‘ç»œsharing weightsï¼Œè·ç¦»ç”¨çš„æ˜¯cosine distanceï¼Œlossç›´æ¥ä¼˜åŒ–è·ç¦»ï¼Œä¼˜åŒ–targetæ˜¯ä¸ªå®šå€¼cosine=1.0/-1.0 [contrastive loss] Dimensionality Reduction by Learning an Invariant Mappingï¼š2006ï¼Œlecunï¼Œcontrastive losså§‹ç¥–ï¼Œç ”ç©¶çš„æ˜¯é«˜ç»´ç‰¹å¾å‘é‡å‘ä½ç»´æ˜ å°„çš„éçº¿æ€§å±‚ï¼Œè·ç¦»ç”¨çš„æ˜¯euclidean distanceï¼Œlossä¼˜åŒ–çš„æ˜¯squared distanceï¼Œä¼˜åŒ–targetæ˜¯0å’Œmï¼Œsimilar pairsä»æ—§ä¼šè¢«æ¨å‘ä¸€ä¸ªå®šç‚¹ï¼Œæ²¡æœ‰è§£å†³è®ºæ–‡å£°ç§°çš„uniform distribution [triplet-loss] Learning Fine-grained Image Similarity with Deep Rankingï¼š2014ï¼ŒGoogleï¼Œç”¨äº†ä¸‰å…ƒç»„ï¼Œæå‡ºäº†triplet-loss [facenet] FaceNet: A Unified Embedding for Face Recognition and Clusteringï¼š2015ï¼ŒGoogleï¼Œç”¨æ¥è¯†åˆ«äººè„¸ï¼Œç”¨äº†ä¸‰å…ƒç»„å’Œtriplet-lossï¼Œsquared euclidean distanceï¼Œä¼˜åŒ–ç›®æ ‡æ˜¯åŒç±»å’Œå¼‚ç±»pairä¹‹é—´çš„ç›¸å¯¹è·ç¦»ï¼Œå›°éš¾æ ·æœ¬ï¼ˆsemi-hard &amp; hardï¼‰å¯¹æ”¶æ•›èµ·ä½œç”¨ï¼ˆåŠ é€Ÿï¼local minimaï¼‰ï¼Œtriplet-lossè€ƒè™‘äº†ç±»é—´çš„ç¦»æ•£æ€§ï¼Œä½†æ²¡æœ‰è€ƒè™‘ç±»å†…çš„ç´§å‡‘æ€§ [center-loss] A Discriminative Feature Learning Approach for Deep Face Recognitionï¼š2016ï¼Œä¹Ÿæ˜¯ç”¨åœ¨äººè„¸ä»»åŠ¡ä¸Šï¼Œä¼˜åŒ–ç›®æ ‡æ˜¯ç±»å†…çš„ç»å¯¹è·ç¦»ï¼Œè€Œä¸æ˜¯å»ºæ¨¡ç›¸å¯¹å…³ç³»ï¼Œcenter-lossç›´æ¥ä¼˜åŒ–çš„æ˜¯ç±»é—´çš„é—´å‡‘æ€§ï¼Œç±»é—´çš„ç¦»æ•£æ€§é çš„æ˜¯softmax loss [triplet-center-loss] Triplet-Center Loss for Multi-View 3D Object Retrievalï¼š2018ï¼Œä¸œæ‹¼è¥¿å‡‘æ°´è®ºæ–‡ [Hinge-loss] SVM margin [circle-loss] Circle Loss: A Unified Perspective of Pair Similarity Optimizationï¼š2020CVPRï¼Œæ—·è§†ï¼Œæå‡ºäº†cls losså’Œmetric lossçš„ç»Ÿä¸€å½¢å¼$minimize(s_n - s_p+m)$ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šæå‡ºcircle lossä½œä¸ºä¼˜åŒ–ç›®æ ‡$(\alpha_n s_n - \alpha_p s_p) = m$ï¼Œåœ¨toy scenarioä¸‹å±•ç¤ºäº†åˆ†ç±»è¾¹ç•Œå’Œæ¢¯åº¦çš„æ”¹å–„ã€‚ ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ [Hierarchical Similarity] Learning Hierarchical Similarity Metricsï¼š2012CVPRï¼Œ [Hierarchical Triplet Loss] Deep Metric Learning with Hierarchical Triplet Lossï¼š2018ECCVï¼Œ Hierarchical classicificationåº”è¯¥å•ç‹¬åšä¸€ä¸ªç³»åˆ—ï¼Œtobeadded ä¸€äº›å¾…æ˜ç¡®çš„é—®é¢˜ anchoræ€ä¹ˆé€‰ï¼šfacenetä¸­è¯´æ˜ï¼Œæ¯ä¸ªmini-batchä¸­æ¯ä¸ªç±»åˆ«å¿…é¡»éƒ½æœ‰ pairsæ€ä¹ˆå®ç°ï¼ˆå›°éš¾çš„å®šä¹‰ï¼‰ï¼šfacenetä¸­è¯´æ˜ï¼Œhard distance sample in mini-batch hingeloss &amp; SVMæ¨å¯¼ å¸¸è§„ä½¿ç”¨ï¼Ÿç»“åˆcls losså’Œmetric lossè¿˜æ˜¯åªç”¨metric lossï¼šcls losså’Œmetric lossæœ¬è´¨ä¸Šæ˜¯ä¸€æ ·çš„ï¼Œéƒ½æ˜¯å¸Œæœ›åŒç±»æ ·æœ¬è¾“å‡ºä¸€æ ·ï¼Œä¸åŒç±»æ ·æœ¬è¾“å‡ºä¸ä¸€æ ·ï¼Œåªä¸è¿‡å‰è€…å…·æœ‰æ¦‚ç‡æ„ä¹‰ï¼Œåè€…å…·æœ‰è·ç¦»æ„ä¹‰ã€‚ä¸Šé¢åˆ—å‡ºæ¥çš„åªæœ‰center lossæ˜¯è¦è·Ÿcls lossç»“åˆèµ·æ¥ç”¨çš„ï¼Œå› ä¸ºä»–åªé’ˆå¯¹ç±»å†…ï¼Œä¸è¶³ä»¥æ¨åŠ¨æ•´ä¸ªæ¨¡å‹ã€‚ Signature Verification using a â€˜Siameseâ€™ Time Delay Neural Network åŠ¨æœº verification of written signatures propose Siamese two identical sub-networks joined at their outputs measure the distance verification process a stored feature vector a chosen threshold æ–¹æ³• network two inputsï¼šextracting features two sub-networksï¼šshare the same weights one outputï¼šcosine of the angle between two feature vectors target two real signaturesï¼šcosine=1.0 with one forgeryï¼šcosine=-0.9 and cosine=-1.0 dataset 50% genuine:genuine pairs 40% genuine:forgery pairs 10% genuine:zero-effort pairs Dimensionality Reduction by Learning an Invariant Mapping åŠ¨æœº dimensionality reduction propose Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) globally co- herent non-linear function relies solely on neighbor- hood relationships invariant to certain transformations of the inputs è®ºç‚¹ most existing dimensionality reduction techniques they do not produce a function (or a mapping) from input to manifold new points with unknown relationships with training samples cannot be processed they tend to cluster points in output space a uniform distribution in the outer manifolds is desirable proposed DrLIM globally coherent non-linear function neighborhood relationships that are independent from any distance metric invariant to complicated non-linear trnasformations lighting changes geometric distortions can be used to map new samples empoly contrastive loss neighbors are pulled together non-neighbors are pushed apart energy based model euclidean distance approximates the â€œsemantic similarityâ€of the inputs in input space æ–¹æ³• contrastive loss conventional loss sum over samples contrastive loss sum over pairs $(X_1, X_2, Y)$ similar pairsï¼š$Y=0$ dissimilarï¼š$Y=1$ euclidean distance $L = (1-Y)\sum L_s ||G_w(X_1)-G_w(X_2)||_2 + Y\sum L_d ||G_w(X_1)-G_w(X_2)||_2$ $L_s$ should results in low values for similar pairs $L_d$ should results in high values for dissimilar pairs exact formï¼š$L(W,Y,X_1,X_2) = (1-Y)\frac{1}{2}(D^2) + (Y)\frac{1}{2} \{max(0,m-D)\}^2$ spring model analogy similar partial lossç›¸å½“äºç»™å¼¹ç°§æ–½åŠ äº†ä¸€ä¸ªæ’å®šçš„åŠ›ï¼Œå‘ä¸­å¿ƒç‚¹æŒ¤å‹ dissimilar partial lossåªå¯¹åœˆå†…çš„ç‚¹æ–½åŠ›ï¼Œæ¨å‡ºåœˆå¤–å°±ä¸ç®¡äº† FaceNet: A Unified Embedding for Face Recognition and Clustering åŠ¨æœº face tasks face verification: is this the same person face recognition: who is the person clustering: find common people among the faces learn a mapping compact Euclidean space where the Euclidean distance directly correspond to face similarity è®ºç‚¹ traditionally training classification layer generalizes well to new facesï¼Ÿ indirectness large dimension feature representation inefficiency use siamese pairs the loss encourages all faces of one identity to project onto a single point this paper employ triplet loss targetï¼šseparate the positive pair from the negative by a distance margin allows the faces of one identity to live on a manifold obtain face embedding l2 norm a fixed d-dims hypersphere large dataset to attain the appropriate invariances to pose, illumination, and other variational conditions architecture explore two different deep network æ–¹æ³• inputï¼šä¸‰å…ƒç»„ï¼Œconsist of two matching face thumbnails and a non-matching one ouputï¼šç‰¹å¾æè¿°ï¼Œa compact 128-D embedding living on the fixed hypersphere $||f(x)||_2=1$ triple-loss targetï¼šall anchor-pos distances are smaller than any anchor-neg distances with a least margin $\alpha$ $L = \sum_i^N [||f(x_i^a) - f(x_i^p)||_2^2 - ||f(x_i^a) - f(x_i^n)||_2^2 + \alpha]$ hard triplets hard samples $argmax_{x_i^p}||f(x_i^a) - f(x_i^p)||_2^2$ $argmin_{x_i^n}||f(x_i^a) - f(x_i^n)||_2^2$ infeasible to compute over the whole setï¼šmislabelled and poorly imaged faces would dominate the hard positives and negatives off-lineï¼šuse recent checkpoint to compute on a subset onlineï¼šselect in mini-batch mini-batchï¼š æ¯ä¸ªç±»åˆ«éƒ½å¿…é¡»æœ‰æ­£æ ·æœ¬ è´Ÿæ ·æœ¬æ˜¯randomly sampled hard sample use all anchor-positive pairs selecting the hard negatives hardest negatives can lead to bad local minima in early stage å…ˆpick semi-hardï¼š$||f(x_i^a) - f(x_i^p)||_2^2 &lt; ||f(x_i^a) - f(x_i^n)||_2^2$ network ä¸€ç§straightçš„ç½‘ç»œï¼Œå¼•å…¥äº†1x1 convå…ˆå‹ç¼©é€šé“ Inception modelsï¼š20x fewer paramsï¼Œ5x fewer FLOPS metric sameï¼differentæ˜¯ç”±a squared L2 distanceå†³å®š å› æ­¤æµ‹è¯•ç»“æœæ˜¯dçš„å‡½æ•° å®šä¹‰true acceptsï¼šåœˆå†…å¯¹çš„ï¼Œ$TA(d)=\{(i,j)\in P_{same}, with D(x_i,x_j)\leq d\}$ å®šä¹‰false acceptsï¼šåœˆå†…é”™çš„ï¼Œ$FA(d)=\{(i,j)\in P_{diff}, with D(x_i,x_j)\leq d\}$ å®šä¹‰validation rateï¼š$VAL(d) = \frac{|TA(d)|}{|P_{same}|}$ å®šä¹‰false accept rateï¼š$FAR(d) = \frac{|FA(d)|}{|P_{diff}|}$ A Discriminative Feature Learning Approach for Deep Face Recognition åŠ¨æœº enhance the discriminationative power of the deeply learned features joint supervision softmax loss center loss two key learning objectives inter-class dispension intra-class compactness è®ºç‚¹ face recognition task requirement the learned features need to be not only separable but also discriminative generalized enough for the new unseen samples the softmax loss only encourage the separability of features å¯¹åˆ†ç±»è¾¹ç•Œã€ç±»å†…ç±»é—´åˆ†å¸ƒæ²¡æœ‰ç›´æ¥çº¦æŸ contrastive loss &amp; triplet loss training pairs or triplets dramatically grows slow convergence and instability we propose learn a center simultaneously update the center and optimize the distances joint supervision softmax loss forces the deep features of different classes staying apart center loss efficiently pulls the deep features of the same class to their centers to be more discriminationative the inter-class features differences are enlarged the intra-class features variations are reduced æ–¹æ³• softmax vis æœ€åä¸€å±‚hidden layerä½¿ç”¨ä¸¤ä¸ªç¥ç»å…ƒ so that we can directly plot separable but still show significant intra-class variations center loss $L_c = \frac{1}{2} \sum_1^m ||x_i - c_{y_i}||_2^2$ update class center on mini-batchï¼š \frac{\partial L_c}{\partial x_i} = x_i - c_{y_i}\\ \Delta c_j = \frac{\sum_i^m \delta (y_i=j) * (c_j - x_i)}{1+\sum_i^m \delta (y_i=j)} joint supervisionï¼š L = L_{CE} + \lambda L_c discussion necessity of joint supervision solely softmax loss â€”-&gt; large intra-class variations solely center loss â€”-&gt; features and centers will degraded to zeros compared to contrastive loss and triplet loss using pairsï¼šsuffer from dramatic data expansion hard miningï¼šcomplex recombination optimizing targetï¼š center lossç›´æ¥é’ˆå¯¹intra-class compactnessï¼Œç±»å†…ç”¨è·ç¦»æ¥çº¦æŸï¼Œç±»é—´ç”¨softmaxæ¥çº¦æŸ contrastive lossä¹Ÿæ˜¯ç›´æ¥ä¼˜åŒ–ç»å¯¹è·ç¦»ï¼Œç±»å†…&amp;ç±»é—´éƒ½ç”¨è·ç¦»æ¥çº¦æŸ triplet lossæ˜¯å»ºæ¨¡ç›¸å¯¹å…³ç³»ï¼Œç±»å†…&amp;ç±»é—´éƒ½ç”¨è·ç¦»æ¥çº¦æŸ architecture local convolution layerï¼šå½“æ•°æ®é›†åœ¨ä¸åŒçš„åŒºåŸŸæœ‰ä¸åŒçš„ç‰¹å¾åˆ†å¸ƒæ—¶ï¼Œé€‚åˆç”¨local-Convï¼Œå…¸å‹çš„ä¾‹å­å°±æ˜¯äººè„¸è¯†åˆ«ï¼Œä¸€èˆ¬äººçš„é¢éƒ¨éƒ½é›†ä¸­åœ¨å›¾åƒçš„ä¸­å¤®ï¼Œå› æ­¤æˆ‘ä»¬å¸Œæœ›å½“convçª—å£æ»‘è¿‡è¿™å—åŒºåŸŸçš„æ—¶å€™ï¼Œæƒé‡å’Œå…¶ä»–è¾¹ç¼˜åŒºåŸŸæ˜¯ä¸åŒçš„ å‚æ•°é‡æš´å¢ï¼škernel_size kernel_size output_size output_size input_channel * output_channel å®éªŒ hyperparamï¼š$\lambda$ and $\alpha$ fix $\alpha=0.5$ and vary $\lambda$ from 0-0.1 fix $\lambda=0.003$ and vary $\alpha$ from 0.01-1 ç»“è®ºæ˜¯remains stable across a large rangeï¼Œæ²¡æœ‰ç»™å‡ºæœ€ä½³ï¼å»ºè®® æˆ‘çš„å®éªŒ åŠ æ¯”ä¸åŠ è®­ç»ƒæ…¢å¾—å¤š åœ¨Mnistä¸Šæµ‹è¯•åŒæ ·çš„epochåŠ æ¯”ä¸åŠ å‡†ç¡®ç‡ä½ ä¹‹æ‰€ä»¥Center Lossæ˜¯é’ˆå¯¹äººè„¸è¯†åˆ«çš„Lossæ˜¯æœ‰åŸå› çš„ï¼Œä¸ªäººè®¤ä¸ºäººè„¸çš„ä¸­å¿ƒæ€§æ›´å¼ºä¸€äº›ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸€ä¸ªäººçš„æ‰€æœ‰è„¸å–å¹³å‡å€¼ä¹‹åçš„äººè„¸æˆ‘ä»¬è¿˜æ˜¯å¯ä»¥è¾¨è¯†æ˜¯ä¸æ˜¯è¿™ä¸ªäººï¼Œæ‰€ä»¥Center Lossæ‰èƒ½å‘æŒ¥ä½œç”¨ Circle Loss: A Unified Perspective of Pair Similarity Optimization åŠ¨æœº pair similarity circular decision boundary unify cls-based &amp; metric-based data class-level labels pair-wise labels è®ºç‚¹ there is no intrinsic difference between softmax loss &amp; metric loss minimize between-class similarity $s_n$ maximize within- class similarity $s_p$ reduce $s_n - s_p$ short-commings lack of flexibilityï¼š$s_p$å’Œ$s_n$çš„ä¼˜åŒ–é€Ÿåº¦å¯èƒ½ä¸åŒï¼Œä¸€ä¸ªå¿«æ”¶æ•›äº†ä¸€ä¸ªè¿˜å¾ˆå·®ï¼Œè¿™æ—¶å€™ç”¨åŒæ ·çš„æ¢¯åº¦å»æ›´æ–°å°±éå¸¸inefficient and irrationalï¼Œå°±å·¦å›¾æ¥è¯´ï¼Œä¸‹é¢çš„ç‚¹ç›¸å¯¹ä¸Šé¢çš„ç‚¹ï¼Œ$s_n$æ›´å°ï¼ˆæ›´æ¥è¿‘opï¼‰ï¼Œ$s_p$æ›´å°ï¼ˆæ›´è¿œç¦»opï¼‰ï¼Œvice versaï¼Œä½†æ˜¯å†³ç­–å¹³é¢å¯¹ä¸‰ä¸ªç‚¹ç›¸å¯¹äº$s_n$å’Œ$s_p$çš„æ¢¯åº¦éƒ½æ˜¯ä¸€æ ·çš„ï¼ˆ1å’Œ-1ï¼‰ã€‚ ambiguous convergence statusï¼šç”¨ä¸€ä¸ªhard distance marginæ¥æè¿°decision boundaryè¿˜ä¸å¤Ÿdiscriminativeï¼Œhard decision boundaryä¸Šå„ç‚¹å…¶å®è¿˜æ˜¯æœ‰å·®åˆ«çš„ï¼Œå‡è®¾å­˜åœ¨ä¸€ä¸ªoptimumï¼ˆ$s_p=1 \ \&amp; \ s_n=0$ï¼‰ï¼Œé‚£ä¹ˆå·¦å›¾å†³ç­–å¹³é¢ä¸Šä¸¤ä¸ªç‚¹ï¼Œç›¸å¯¹optimumçš„æ„ä¹‰æ˜æ˜¾ä¸ä¸€æ ·ï¼Œå†³ç­–å¹³é¢åº”è¯¥æ˜¯ä¸ªå›´ç»•optimumçš„åœ†åœˆã€‚ propose circle loss independent weighting factorsï¼šç¦»optimumè¶Šè¿œçš„penalty strengthè¶Šå¤§ï¼Œè¿™ä¸€é¡¹ç›´æ¥ä»¥è·ç¦»ä¸ºä¼˜åŒ–ç›®æ ‡çš„losséƒ½æ˜¯æ»¡è¶³çš„ different penalty strengthï¼š$s_p$å’Œ$s_n$ learn at different pacesï¼Œç±»å†…åŠ æƒï¼ŒåŠ æƒç³»æ•°æ˜¯learnable params $(\alpha_n s_n - \alpha_p s_p) = m$ï¼šyielding a circle shape æ–¹æ³• æ ¸å¿ƒï¼š$(\alpha_n s_n - \alpha_p s_p) = m$ self-paced weighting given optimum $O_p$ and $O_n$ï¼Œfor each similarity scoreï¼š \begin{cases} a_p^i = [O_p - s_p^i]_+ \\ a_n^j = [s_n^j - O_n]_+ \end{cases} cut-off at zero å¯¹äºè¿œç¦»optimumçš„ç‚¹æ¢¯åº¦æ”¾å¤§ï¼Œæ¥è¿‘optimumçš„ç‚¹ï¼ˆå¿«æ”¶æ•›ï¼‰æ¢¯åº¦ç¼©å° softmaxé‡Œé¢é€šå¸¸ä¸ä¼šå¯¹åŒç±»æ ·æœ¬é—´åšè¿™ç§rescalingçš„ï¼Œå› ä¸ºå®ƒå¸Œæœ›æ‰€æœ‰æ ·æœ¬valueéƒ½è¾¾åˆ°è´¼å¤§ Circle loss abandons the interpretation of classifying a sample to its target class with a large probability margin adding a margin m reinforces the optimization take toy scenario æœ€ç»ˆæ•´ç†æˆï¼š$(s_n-0)^2 + (s_p-1)^2 = 2m^2$ op targetï¼š$s_p &gt; 1-m$ï¼Œ$s_n &lt; m$ relaxation factor $m$ï¼šcontrols the radius of the decision boundary unified perspective tranverse all the similarity pairsï¼š$\{s_p^i\}^K$å’Œ$\{s_n^j\}^N$ to reduce $(s_n^j - s_p^i)$ï¼š$L_{uni}=log[1+\sum^K_i \sum^N_j exp(\lambda (s_n^j - s_p^i + m))]$ è§£è€¦ï¼ˆä¸ä¼šåŒæ—¶æ˜¯$s_p$å’Œ$s_n$ï¼‰ï¼š$L_{uni}=log[1+\sum^N_j exp(\lambda (s_n^j + m))\sum^K_i exp(\lambda (-s_p^i))]$ given class labelsï¼š we get $(N-1)$ between-class similarity scores and $(1)$ within-class similarity score åˆ†æ¯ç¿»ä¸Šå»ï¼š$L = -log \frac{exp(\lambda (s_p-m))}{exp(\lambda (s_p-m)) + \sum^{N-1}_j exp(\lambda (s_n^j))}$ å°±æ˜¯softmax given pair-wise labelsï¼š triplet loss with hard miningï¼šfind pairs with large $s_n$ and low $s_p$ use infiniteï¼š$L=lim_{\lambda \to \inf} \frac{1}{\lambda} L_{uni}$ å®éªŒ Face recognition noisy and long-tailed dataï¼šå»å™ªå¹¶ä¸”å»æ‰ç¨€ç–æ ·æœ¬ resnet &amp; 512-d feature embeddings &amp; cosine distance $\lambda=256$ï¼Œ$m=0.25$ Person re-identification $\lambda=128$ï¼Œ$m=0.25$ Fine-grained image retrieval è½¦é›†å’Œé¸Ÿé›† bn-inception &amp; 512-d embeddings P-K sampling $\lambda=80$ï¼Œ$m=0.4$ hyper-params the scale factor $\lambda$ï¼š determines the largest scale of each similarity score Circle loss exhibits high robustness on $\lambda$ the other two becomes unstable with larger $\lambda$ owing to the decay factor the relaxation factor mï¼š determines the radius of the circular decision boundary surpasses the best performance of the other two in full range robustness inference å¯¹äººè„¸ç±»ä»»åŠ¡ï¼Œé€šå¸¸ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆä¸€ä¸ªäººè„¸æ ‡å‡†åº•åº“ï¼Œç„¶åæ¯æ¬¡æ¨ç†çš„æ—¶å€™å¾—åˆ°æµ‹è¯•æ•°æ®çš„ç‰¹å¾å‘é‡ï¼Œå¹¶åœ¨æ ‡å‡†åº•åº“ä¸­æœç´¢ç›¸ä¼¼åº¦æœ€é«˜çš„ç‰¹å¾ï¼Œå®Œæˆäººè„¸è¯†åˆ«è¿‡ç¨‹ã€‚]]></content>
      <tags>
        <tag>åº¦é‡å­¦ä¹ ï¼Œloss &amp; network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[efficientå‘¨è¾¹]]></title>
    <url>%2F2020%2F09%2F23%2Fefficient%E5%91%A8%E8%BE%B9%2F</url>
    <content type="text"><![CDATA[å› ä¸ºä¸æ˜¯googlenetå®¶æ—å®˜æ–¹å‡ºå“ï¼Œæ‰€ä»¥æ”¾åœ¨å¤–é¢ [EfficientFCN] EfficientFCN: Holistically-guided Decoding for Semantic Segmentationï¼šå•†æ±¤ï¼Œä¸»è¦é’ˆå¯¹upsamplingæ˜¯å±€éƒ¨æ„Ÿå—é‡ï¼Œé‡å»ºå¤±çœŸå¤šï¼Œåˆ†å‰²ç²¾åº¦å·®çš„é—®é¢˜ï¼Œæå‡ºäº†Holistically-guided Decoder (HGD) ï¼Œç”¨æ¥recover the high-resolution (OS=8) feature mapsï¼Œæƒ³æ³•ä¸Šæ¥è¿‘SCSE-blockï¼Œæ•°å­¦è¡¨è¾¾ä¸Šæ¥è¿‘bilinear-CNNï¼Œæ€§èƒ½æå‡ä¸»è¦å½’å› äºeff backå§ã€‚ EfficientFCN: Holistically-guided Decoding for Semantic Segmentation åŠ¨æœº Semantic Segmentation dilatedFCNï¼šcomputational complexity encoder-decoderï¼šperformance proposed EfficientFCN common back without dilated convolution holistically-guided decoder balance performance and efficiency è®ºç‚¹ key elements for semantic segmentation high-resolution feature maps pre-trained weights OS32 feature mapï¼šthe fine-grained structural information is discarded dilated convolutionï¼šno extra parameters introduced but equire high computational complexity and memory consumption encoder-decoder based methods repeated upsampling + skip connection procedure upsampling concatï¼add successive convs Even with the skip connections, lower-level high-resolution feature maps cannot provide abstractive enough features for achieving high- performance segmentation The bilinear upsampling or deconvolution operations are conducted in a local manner(from a limited receptive filed) improvements reweightï¼šSE-block scales each feature channel but maintains the original spatial size and structuresï¼šã€scse blockå¯¹spacialæœ‰åŠ æƒå•Šã€‘ propose EfficientFCN widely used classification model Holistically-guided Decoder (HGD) take OS8, OS16, OS32 feature maps from backbone OS8å’ŒOS16ç”¨æ¥spatially guiding the feature upsampling process OS32ç”¨æ¥encode the global contextç„¶ååŸºäºguidanceè¿›è¡Œä¸Šé‡‡æ · linear assembly at each high-resolution spatial locationï¼šæ„Ÿè§‰å°±æ˜¯å¯¹ä¸Šé‡‡æ ·ç‰¹å¾å›¾åšäº†åŠ æƒ æ–¹æ³• Holistically-guided Decoder multi-scale feature fusion holistic codebook generation from high-level feature maps holistic codewordsï¼šwithout any spatial order codeword assembly multi-scale feature fusion we observe the fusion of multi-scale feature maps generally result in better performance compressï¼šseparate 1x1 convs bilinear downsampï¼upsamp concatenate fused OS32 $m_{32}$ &amp; fused OS8 $m_8$ holistic codebook generation from $m_{32}$ two separate 1x1 conv a codeword based map $B \in R^{1024(H/32)(W/32)}$ï¼šæ¯ä¸ªä½ç½®ç”¨ä¸€ä¸ª1024-dimçš„vectoræ¥æè¿° n spatial weighting map $A\in R^{n(H/32)(W/32)}$ï¼šhighlight ç‰¹å¾å›¾ä¸Šä¸åŒåŒºåŸŸ softmax norm in spatial-dim $\widetilde A_i(x,y)=\frac{exp(A_i(x,y))}{\sum_{p,q} exp(A_i(p,q))}, i\in [0,n)$ codeword $c_i \in R^{1024}$ global description for each weighting map weighted average of B on all locations $c_i = \sum_{p,q} \widetilde A_i(p,q) B(p,q)$ each codeword captures certain aspect of the global context orderless high-level global features $C \in R^{1024*n}$ $C = [c_1, â€¦, c_n]$ codeword assembly raw guidance map $G \in R^{1024(H/8)(W/8)}$ï¼š1x1 conv on $m_8$ fuse semantic-rich feature map $\overline B \in R^{1024}$ï¼šglobal average vector novel guidance feature map $\overline G = G \oplus \overline B $ï¼šlocation-wise additionã€ï¼Ÿï¼Ÿï¼Ÿï¼Ÿã€‘ linear assembly weights of the n codewords $W \in R^{n(H/8)(W/8)}$ï¼š1x1 conv on $\overline G$ holistically-guided upsampled feature $\tilde f_8 = W^T C$ï¼šreshape &amp; dot final feature map $f_8$ï¼šconcat $\tilde f_8$ and $G$ final segmentation 1x1 conv further upsampling å®éªŒ numer of holistic codewords 32-512ï¼šincrease 512-1024ï¼šslight drop we observe the number of codewords needed is approximately 4 times than the number of classes]]></content>
      <tags>
        <tag>è¯­ä¹‰åˆ†å‰²</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[data aug]]></title>
    <url>%2F2020%2F09%2F18%2Fdata-aug%2F</url>
    <content type="text"><![CDATA[[mixup] mixup: BEYOND EMPIRICAL RISK MINIMIZATIONï¼šå¯¹ä¸åŒç±»åˆ«çš„æ ·æœ¬ï¼Œä¸ä»…å¯ä»¥ä½œä¸ºæ•°æ®å¢å¹¿æ‰‹æ®µï¼Œè¿˜å¯ä»¥ç”¨äºsemi-supervised learningï¼ˆMixMatchï¼‰ [mixmatch] MixMatch: A Holistic Approach to Semi-Supervised Learningï¼šé’ˆå¯¹åŠç›‘ç£æ•°æ®çš„æ•°æ®å¢å¹¿ [mosaic] from YOLOv4 [AutoAugment] AutoAugment: Learning Augmentation Policies from Dataï¼šgoogle [RandAugment] RandAugment: Practical automated data augmentation with a reduced search spaceï¼šgoogle RandAugment: Practical automated data augmentation with a reduced search space åŠ¨æœº AutoAugment separate search phase run on a subset of a huge dataset unable to adjust the regularization strength based on model or dataset size RandAugment significantly reduced search space can be used uniformly across tasks and datasets match or exceeds the previous val acc æ–¹æ³• formulation always select a transformation with uniform prob $\frac{1}{K}$ given N transformations for an imageï¼šthere are $K^N$ potential policies fixied magnitude schedule Mï¼šwe choose Constantï¼Œå› ä¸ºåªè¦ä¸€ä¸ªhyper run naive grid search ç–‘é—®ï¼šè¿™æ ·æ¯ä¸ªopç­‰æ¦‚ç‡ï¼Œå°±ä¸å†data-specificäº†ï¼Œä¹Ÿçœ‹ä¸å‡ºè‡ªç„¶å›¾åƒæ›´prefer color transformationè¿™ç§ç»“è®ºäº† AutoAugment: Learning Augmentation Policies from Data åŠ¨æœº search for data augmentation policies propose AutoAugment create a search space composed of augmentation sub-policies one sub-policy is randomly choosed per image per mini-batch a sub-policy consists of two base operations find the best policyï¼šyields the highest val acc on the target dataset the learned policy can transfer è®ºç‚¹ data augmentation to teach a model about invariance in data domain is easier than hardcoding it into model architecture currently dataset-specific and often do not transferï¼š MNISTï¼šelastic distortions, scale, translation, and rotation CIFAR &amp; ImageNetï¼šrandom cropping, image mirroring and color shifting / whitening GANï¼šç›´æ¥ç”Ÿæˆå›¾åƒï¼Œæ²¡æœ‰å½’çº³policy we aim to automate the process of finding an effective data augmentation policy for a target dataset each policyï¼š operations in certain order probabilities after applying magnitudes use reinforcement learning as the search algorithm contributions SOTA on CIFAR &amp; ImageNet &amp; SVHN new insight on transfer learningï¼šä½¿ç”¨é¢„è®­ç»ƒæƒé‡æ²¡æœ‰æ˜¾è‘—æå‡çš„datasetä¸Šï¼Œä½¿ç”¨åŒæ ·çš„aug policiesåˆ™ä¼šæ¶¨ç‚¹ æ–¹æ³• formulation search space of policies policyï¼ša policy consists of 5 sub-policies sub-policyï¼šeach sub-policy consisting of two image operations operationï¼šeach operation is also associated with two hyperparameters probabilityï¼šof applying the operationï¼Œuniformly discrete into 11 values magnitudeï¼šof the operationï¼Œuniformly discrete into 10 values a mini-batch share the same chosen sub-policy operationsï¼š16 in totalï¼Œmainly use PIL https://blog.csdn.net/u011583927/article/details/104724419æœ‰å„ç§operationçš„å¯è§†åŒ–æ•ˆæœ shearæ˜¯ç æ‰å›¾åƒä¸€ä¸ªè§’çš„ç•¸å˜ equalizeæ˜¯ç›´æ–¹å›¾å‡è¡¡åŒ– solarizeæ˜¯åŸºäºä¸€å®šé˜ˆå€¼çš„invertï¼Œé«˜äºé˜ˆå€¼invertï¼Œä½äºé˜ˆå€¼ä¸å˜ posterizeä¹Ÿæ˜¯ä¸€ç§åƒç´ å€¼æˆªæ–­æ“ä½œ coloræ˜¯è°ƒæ•´é¥±å’Œåº¦ï¼Œmag&lt;1è¶‹è¿‘ç°åº¦å›¾ sharpnesså†³å®šå›¾åƒæ¨¡ç³Š/é”åŒ– sample pairingï¼šä¸¤å¼ å›¾åŠ æƒæ±‚å’Œï¼Œä½†æ˜¯ä¸æ”¹å˜æ ‡ç­¾ searching goal with $(161011)^2$ choices of sub-policies we want 5 example ä¸€ä¸ªsub-policyåŒ…å«ä¸¤ä¸ªoperation æ¯ä¸ªoperationæœ‰ä¸€å®šçš„possibilityåš/ä¸åš æ¯ä¸ªoperationæœ‰ä¸€å®šçš„magnitudeå†³å®šåšåçš„æ•ˆæœ ç»“è®º On CIFAR-10, AutoAugment picks mostly color-based transformations on ImageNet, AutoAugment focus on color-based transformations as well, besides geometric transformation and rotate is commonly used one of the best policy overall results mixup: BEYOND EMPIRICAL RISK MINIMIZATION åŠ¨æœº classification task memorization and sensitivity issue reduces the memorization of corrupt labels increases the robustness to adversarial examples improves the generalization can be used to stabilize the training of GANs propose convex combinations of pairs of examples and their labels è®ºç‚¹ ERM(Empirical Risk Minimization)ï¼šissue of generalization allows large neural networks to memorize (instead of generalize from) the training data even in the presence of strong regularization neural networks change their predictions drastically when evaluated on examples just outside the training distribution VRM(Vicinal Risk Minimization)ï¼šintroduce data augmentation e.g. define the vicinity of one image as the set of its horizontal reflections, slight rotations, and mild scalings vicinity share the same class does not model the vicinity relation across examples of different classes ERMä¸­çš„training setå¹¶ä¸æ˜¯æ•°æ®çš„çœŸå®åˆ†å¸ƒï¼Œåªæ˜¯ç”¨æœ‰é™æ•°æ®æ¥è¿‘ä¼¼çœŸå®åˆ†å¸ƒï¼Œmemorizationä¹Ÿä¼šæœ€å°åŒ–training errorï¼Œä½†æ˜¯å¯¹training segä»¥å¤–çš„sampleå°±leads to undesirable behaviour mixupå°±æ˜¯VRMçš„ä¸€ç§ï¼Œpropose a generic vicinal distributionï¼Œè¡¥å……vicinity relation across examples of different classes æ–¹æ³• mixup constructs virtual training examples x = \lambda x_i + (1-\lambda)x_j \\ y = \lambda y_i + (1-\lambda)y_j use two examples drawn at randomï¼šraw inputs &amp; raw one-hot labels ç†è®ºåŸºç¡€ï¼šlinear interpolations of feature vectors should lead to linear interpolations of the associated targets hyper-parameter $\alpha$ $\lambda = np.random.beta(\alpha, \alpha)$ controls the strength of interpolation åˆæ­¥ç»“è®º three or more examples mixup does not provide further gain but more computation interpolating only between inputs with equal label did not lead to the performance gains key elemetsâ€”â€”two inputs with different label vis decision boundariesæœ‰äº†ä¸€ä¸ªçº¿æ€§è¿‡æ¸¡ æ›´å‡†ç¡® &amp; æ¢¯åº¦æ›´å°ï¼šerrorå°‘æ‰€ä»¥losså°æ‰€ä»¥æ¢¯åº¦å°ï¼Ÿï¼Ÿ å®éªŒ åˆæ­¥åˆ†ç±»å®éªŒ $\alpha \in [0.1, 0.4]$ leads to improved performanceï¼Œlargers leads to underfitting models with higher capacities and/or longer training runs are the ones to benefit the most from mixup memorization of corrupted labels å°†æ•°æ®é›†ä¸­ä¸€éƒ¨åˆ†labelæ¢æˆrandom noise ERMç›´æ¥è¿‡æ‹Ÿåˆï¼Œåœ¨corrupted sampleä¸Šé¢training erroræœ€å°ï¼Œæµ‹è¯•é›†ä¸Štest erroræœ€å¤§ dropoutæœ‰æ•ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä½†æ˜¯mixup outperformså®ƒ corrupted labelå¤šçš„æƒ…å†µä¸‹ï¼Œdropout+mixup performs the best robustness to adversarial examples Adversarial examples are obtained by adding tiny (visually imperceptible) perturbations å¸¸è§„æ“ä½œdata augmentationï¼šproduce and train on adversarial examples add significant computationalï¼šæ ·æœ¬æ•°é‡å¢å¤šï¼Œæ¢¯åº¦å˜åŒ–å¤§ mixup results in a smaller loss and gradient normï¼šå› ä¸ºmixupç”Ÿæˆçš„å‡æ ·æœ¬â€œæ›´åˆç†ä¸€ç‚¹â€ï¼Œæ¢¯åº¦å˜åŒ–æ›´å° ablation study mixup is the bestï¼šç»å¯¹é¢†å…ˆç¬¬äºŒmix input + label smoothing the effect of regularization ERMéœ€è¦å¤§weight decayï¼Œmixupéœ€è¦å°çš„â€”â€”è¯´æ˜mixupæœ¬èº«çš„regularization effectsæ›´å¼º é«˜å±‚ç‰¹å¾mixupéœ€è¦æ›´å¤§çš„weight decayâ€”â€”éšç€å±‚æ•°åŠ æ·±regularization effectså‡å¼± AC+RPæœ€å¼º label smoothingå’Œadd Gaussian noise to inputs ç›¸å¯¹æ¯”è¾ƒå¼± mix inputs only(SMOTE) shows no gain MixMatch: A Holistic Approach to Semi-Supervised Learning åŠ¨æœº semi-supervised learning unify previous methods proposed mixmatch guessing low-entropy labels mixup labeled and unlabeled data useful for differentially private learning è®ºç‚¹ semi-supervised learning add a loss term computed on unlabeled data and encourages the model to generalize better to unseen data the loss term entropy minimizationï¼šdecision boundaryåº”è¯¥å°½å¯èƒ½è¿œç¦»æ•°æ®ç°‡ï¼Œå› æ­¤prediction on unlabeled dataä¹Ÿåº”è¯¥æ˜¯high confidence consistency regularizationï¼šå¢å¼ºå‰åçš„unlabeled dataè¾“å‡ºåˆ†å¸ƒä¸€è‡´ generic regularizationï¼šweight decay &amp; mixup MixMatch unified all above introduces a unified loss term for unlabeled data æ–¹æ³• overview givenï¼ša batch of labeled examples $X$ and a batch of labeled examples $U$ augment+label guessï¼ša batch of augmented labeled examples $X^{â€˜}$ and a batch of augmented labeled examples $U^{â€˜}$ computeï¼šseparate labeled and unlabeled loss terms $L_X$ and $L_U$ combineï¼šweighted sum MixMatch data augmentation å¸¸è§„augmentation ä½œç”¨äºæ¯ä¸€ä¸ª$x_b$å’Œ$u_b$ $u_b$åš$K$æ¬¡å¢å¼º label guessing å¯¹å¢å¼ºçš„$K$ä¸ª$u_b$åˆ†åˆ«é¢„æµ‹ï¼Œç„¶åå–å¹³å‡ average class prediction sharpening reduce the entropy of the label distribution æ‹‰é«˜æœ€å¤§predictionï¼Œæ‹‰å°å…¶ä»–çš„ $Sharpen (p, T)_i =\frac{p_i^{\frac{1}{T}}}{\sum^{N}_j p_j^{\frac{1}{T}}} $ $T$è¶‹è¿‘äº0çš„æ—¶å€™ï¼Œprocessed labelå°±æ¥è¿‘one-hotäº† mixup slightly modified form of mixup to make the generated sample being more closer to the original loss function labeled lossï¼štypical cross-entropy loss unlabeled lossï¼šsquared L2ï¼Œbounded and less sensitive to completely incorrect predictions hyperparameters sharpening temperature $T$ï¼šfixed 0.5 number of unlabeled augmentations $K$ï¼šfixed 2 MixUp Beta parameter $\alpha$ï¼š0.75 for start unsupervised loss weight $\lambda_U$ï¼š100 for start Algorithm å®éªŒ [mosaic] from YOLOv4]]></content>
      <tags>
        <tag>æ•°æ®å¢å¼ºï¼Œæ ·æœ¬ä¸å¹³è¡¡ï¼ŒåŠç›‘ç£å­¦ä¹ ï¼Œåº¦é‡å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hrnet]]></title>
    <url>%2F2020%2F09%2F18%2Fhrnet%2F</url>
    <content type="text"><![CDATA[papers [v1 2019] Deep High-Resolution Representation Learning for Human Pose Estimationï¼šbase HRNetï¼Œæå‡ºparallel multi-resolution subnetworksï¼Œhighest resolution outputä½œä¸ºè¾“å‡º [v2 2019] High-Resolution Representations for Labeling Pixels and Regionsï¼šsimple modificationï¼Œåœ¨æœ«ç«¯è¾“å‡ºçš„æ—¶å€™åŠ äº†ä¸€æ­¥èåˆï¼Œå°†æ‰€æœ‰resolution-levelçš„featureä¸Šé‡‡æ ·åˆ°output-levelç„¶åconcat Deep High-Resolution Representation Learning for Human Pose Estimation åŠ¨æœº human pose estimation high-resolution representations through existing methods recover high-res feature from the lowï¼Œå¤§å¤šæ•°æ–¹æ³•æ˜¯recoverç³» this methods maintain the high-res from start to the endï¼Œæœ¬æ–‡æ˜¯maintainingç³» add high-to-low resolution subnetworks repeated multi-scale fusions more accurate and spatially more precise estimate on the high-res outputï¼Œæœ€åçš„high-res representationä½œä¸ºè¾“å‡ºï¼Œæ¥å„ç§task heads è®ºç‚¹ in parallel rather than in seriesï¼špotentially spatially more preciseï¼Œç›¸æ¯”è¾ƒäºrecoverç±»çš„æ¶æ„ï¼Œä¸ä¼šå¯¼è‡´è¿‡å¤šçš„spatial resolution lossï¼Œrecoverç±»çš„æ¶æ„æœ‰æ—¶ä¼šç”¨ç©ºæ´å·ç§¯æ¥ç»´æŒresolutionæ¥é™ä½spatial resolution loss repeated multi- scale fusionsï¼šboost both high&amp;low representationsï¼Œmore accurate pose estimation probabilistic graphical model regression heatmap High-to-low and low-to-high frameworks Symmetric high-to-low and low-to-highï¼šHourglass Heavy high-to-low and light low-to-highï¼šResNet back + simple bilinear upsampling Heavy high-to-low with dilated convolutions and further lighter low-to-highï¼šResNet with atrous conv + fewer bilinear upsampling high-to-low partå’Œlow-to-hight partï¼šæœ‰å¯¹ç§°å’Œä¸å¯¹ç§°ä¸¤ç§ï¼Œå¯¹ç§°å°±å¦‚Hourglassï¼Œä¸å¯¹ç§°å°±æ˜¯down-pathä½¿ç”¨heavy classification backboensï¼Œup-pathä½¿ç”¨è½»é‡çš„ä¸Šé‡‡æ · fusionï¼š aå’Œbéƒ½æœ‰skip-connectionsï¼Œå°†down-pathå’Œup-pathçš„ç‰¹å¾èåˆï¼Œç›®çš„æ˜¯èåˆlow-levelå’Œhigh-levelçš„ç‰¹å¾ aé‡Œé¢è¿˜æœ‰ä¸åŒresolution levelçš„èåˆ fusionæ–¹å¼æœ‰sum/concat refinenetï¼šä¹Ÿå°±æ˜¯up-pathï¼Œå¯ä»¥ç”¨upSampling/transpose convs æ–¹æ³• task description human pose estimation = keypoint detection detect K keypoints from an Image (H,W,3) state-of-the art methodsï¼špredict K heatmapsï¼Œeach indicates one of the keypoint a stem with 2 strided conv a body outputting features with the same input resolution a regressor estimating heatmaps we focus on the design of the main body sequential &amp; parallel multi-resolution networks notationï¼š$N_{sr}$ s is the stage r is the resolution indexï¼Œdenotes $\frac{1}{2^{r-1}}$ of the resolution of the first subnetwork sequential parallel overview four stages channels double when halve the res 1st stage ç¬¬ä¸€ä¸ªstageæ˜¯ä¸€ä¸ªhigh-resolution subnetworkï¼Œæ²¡æœ‰ä¸‹é‡‡æ ·ï¼Œæ²¡æœ‰parallelåˆ†æ”¯ 4 residual unitsï¼Œbottleneck resblock width=64 3x3 conv reducing width to C 2ã€3ã€4 stages æ¥ä¸‹æ¥çš„stage gradually add high-to-low subnetwork æ˜¯multi-resolution subnetworks æ¯ä¸ªsubnetworkéƒ½æ¯”å‰ä¸€ä¸ªå¤šä¸€ä¸ªextra lower one resolution contain 1, 4, 3 exchange blocks respectively exchange block convï¼š4 residual unitsï¼Œtwo 3x3 conv exchange unit width Cï¼šwidth of the high-resolution subnetworks in last three stages other three parallel subnetworks HRNet-W32ï¼š64, 128, 256 HRNet-W48ï¼š96, 192, 384 repeated multi-scale fusion exchange blocksï¼šæ¯ä¸ªhigh-to-low subnetworkåŒ…å«å¤šä¸ªparallelåˆ†æ”¯ï¼Œæ¯æ¡pathç§°ä¸ºexchange blockï¼Œæ¯ä¸ªexchange blockåŒ…å«ä¸€ç³»åˆ—3-conv-units + a exchange unit 3-conv-unitsï¼šå †å å·ç§¯æ ¸ï¼Œæå–ç‰¹å¾ï¼ŒåŠ æ·±ç½‘ç»œ exchange unitï¼šäº¤æ¢ä¸åŒresolution levelçš„ä¿¡æ¯ notationsï¼šä¸€ç³»åˆ—è¾“å…¥$\{X_1,X_2, â€¦, X_r\}$ï¼Œä¸€ç³»åˆ—è¾“å‡º$\{Y_1,Y_2, â€¦, Y_r\}$ï¼Œå¦‚æœè·¨stageè¿˜æœ‰ä¸€ä¸ª$Y_{r+1}$ æ¯ä¸ª$Y_k$éƒ½æ˜¯ä¸€ä¸ªaggregation of the input mapsï¼š$Y_k=\sum^s_i a(X_i,k)$ i&lt;kï¼šéœ€è¦ä¸‹é‡‡æ ·ï¼Œæ¯ä¸‹é‡‡æ ·ä¸€å€éƒ½æ˜¯ä¸€ä¸ªstride2-3x3-conv i=kï¼šidentify connection i&gt;kï¼šéœ€è¦ä¸Šé‡‡æ ·ï¼Œnearest neighbor upsamp + 1x1-align-conv k=$r+1$ï¼šéœ€è¦åœ¨$Y_r$çš„åŸºç¡€ä¸Šï¼Œåœ¨æ‰§è¡Œä¸€æ¬¡stride2-3x3-convä¸‹é‡‡æ ·å¾—åˆ° fusionï¼šsumï¼Œæ‰€ä»¥ä¸Š/ä¸‹é‡‡æ ·éƒ½éœ€è¦é€šé“å¯¹é½ï¼Œè¾“å‡ºmapå’Œå¯¹åº”levelçš„è¾“å…¥mapä¿æŒå°ºå¯¸ä¸å˜ heatmap estimation from the last high-res exchange unit mse gt gassian mapï¼šstd=1 network instantiation stem + 4 stages æ¯ä¸ªnew stage inputï¼šres halved and channel doubled stem ä¸¤ä¸ªs2-conv-bn-reluï¼Œchannel 64 first stageï¼š ä½¿ç”¨å’ŒResNet-50ä¸­ä¸€æ ·çš„4ä¸ªresidual unitsï¼Œchannel 64 ç„¶åç”¨ä¸€ä¸ª3x3-convè°ƒæ•´channelåˆ°ä¸€ä¸ªèµ·å§‹channel C 2/3/4 stage å †å exchange blocksï¼Œåˆ†åˆ«æœ‰1/4/3ä¸ªexchange block æ¯ä¸ªexchange blockä½¿ç”¨4ä¸ªresidual unitså’Œ1ä¸ªexchange unit ä¹Ÿå°±æ˜¯æ€»å…±æœ‰8æ¬¡multi-scale fusion channel C/2C/4C HRNet-32ï¼šC=64 HRNet-48ï¼šC=96 HRNet v2: High-Resolution Representations for Labeling Pixels and Regions åŠ¨æœº High-resolution representationå¾ˆé‡è¦ HRNet v1å·²ç»æœ‰ä¸é”™çš„ç»“æœ a further study on high resolution representations a small modificationï¼šä¹‹å‰åªå…³æ³¨high-resolution representationsï¼Œç°åœ¨å…³æ³¨æ‰€æœ‰levelçš„output representations è®ºç‚¹ è·å¾—high resolution representationçš„ä¸¤å¤§æ–¹å¼ recoverç³»ï¼šå…ˆä¸‹é‡‡æ ·ï¼Œç„¶åç”¨low-resolutioné‡å»ºï¼ŒHourglassï¼ŒU-netï¼Œencoder-decoder maintainç³»ï¼šå§‹ç»ˆä¿ç•™high-resolutionçš„representationï¼ŒåŒæ—¶ä¸æ–­ç”¨parallel low-resolution representationsæ¥strengthenï¼ŒHRNet HRNet maintains high-resolution representations connecting high-to-low resolution convolutions in parallel repeatedly conducting multi-scale fusions across levels ç®€å•æ¥è¯´ï¼Œå°±æ˜¯åœ¨æ¯ä¸ªé˜¶æ®µï¼Œä¿ç•™ç°æœ‰resolution levelï¼ŒåŒæ—¶ ä¸ä»…representationè¶³å¤Ÿå¼ºå¤§ï¼ˆèåˆäº†low-level high semantic infoï¼‰ï¼Œè¿˜spatially precise our modification HRNetV2 HRNet é‡Œé¢æˆ‘ä»¬åªå…³æ³¨æœ€ä¸Šé¢çš„high-resolution representation HRNet V2é‡Œé¢æˆ‘ä»¬æ¢ç´¢æ‰€æœ‰high-to-low parallel pathsä¸Šé¢çš„representations åœ¨è¯­æ„åˆ†å‰²ä»»åŠ¡ä¸­æˆ‘ä»¬ä½¿ç”¨output high resolution representationsæ¥ç”Ÿæˆheatmaps åœ¨æ£€æµ‹ä»»åŠ¡ä¸­æˆ‘ä»¬å°†multi-levelçš„representationsç»™åˆ°FastRCNN æ–¹æ³• Architecture multi-resolution block multi-resolution group convolutionï¼šåœ¨æ¯ä¸ªrepresentation levelåˆ†åˆ«æ‰§è¡Œåˆ†ç»„å·ç§¯ï¼Œdeeper multi-resolution convolutionï¼šå‘ç”Ÿåœ¨æ‰€æœ‰representation levelä¸Š ä¸‹é‡‡æ ·ï¼šstride-2 3x3 conv ä¸Šé‡‡æ ·ï¼šbilinear /nearest neighbor Modification HRNetV1ï¼šåªæŠŠæœ€åä¸€ä¸ªé˜¶æ®µ highest resolutionçš„representationä½œä¸ºè¾“å‡º HRNetV2ï¼šæœ€åä¸€ä¸ªé˜¶æ®µï¼Œæ¯ä¸ªresolution levelçš„representationséƒ½ä¸Šé‡‡æ ·åˆ°highestï¼Œç„¶åconcatä½œä¸ºè¾“å‡ºï¼Œç”šè‡³è¿˜å°†è¿™ä¸ªè¾“å‡ºè¿›ä¸€æ­¥ä¸‹é‡‡æ ·å¾—åˆ°feature pyramid HRNet for classificationï¼šä¹Ÿå¯ä»¥åå‘æ“ä½œï¼Œå°†æœ€åä¸€ä¸ªé˜¶æ®µæ¯ä¸ªresolution levelçš„representationséƒ½ä¸‹é‡‡æ ·åˆ°lowestï¼Œç„¶åsumï¼Œæœ€åoutput 2048-dim representation is fed into the classifier å®éªŒ]]></content>
      <tags>
        <tag>é«˜åˆ†è¾¨ç‡ï¼Œäººä½“å§¿æ€ä¼°è®¡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bilinear CNN]]></title>
    <url>%2F2020%2F09%2F18%2Fbilinear-CNN%2F</url>
    <content type="text"><![CDATA[17å¹´çš„paperï¼Œå¼•ç”¨é‡15ï¼Œæå‡ºäº†ç½‘è·¯ç»“æ„ï¼Œä½†æ˜¯æ²¡åˆ†æä¸ºå•¥æœ‰æ•ˆï¼Œåƒåœ¾ Bilinear CNNs for Fine-grained Visual Recognition åŠ¨æœº fine-grained classification propose a pooled outer product of features derived from two CNNs 2 CNNs a bilinear layer a pooling layer outperform existing models and fairly efficient effective at other image classification tasks such as material, texture, and scene recognition è®ºç‚¹ fine-grained classification tasks require recognition of highly localized attributes of objects while being invariant to their pose and location in the image previous techniques part-based models construct representations by localizing parts more accurate but requires part annotations holistic models construct a representation of the entire image texture descriptorsï¼šFVï¼ŒSIFT STNï¼šaugment CNNs with parameterized image transformations attentionï¼šuse segmentation as a weakly-supervised manner Our key insight is that several widely-used texture representations can be written as a pooled outer product of two suitably designed features several widely-used texture representations two suitably designed features the bilinear features are highly redundant dimensionality reduction trade-off between accuracy We also found that feature normalization and domain-specific fine-tuning offers additional benefits combination concatenateï¼šadditional parameters to fuse an outer productï¼šno parameters sum productï¼šcan achieve similar approximations â€œtwo-streamâ€ architectures one used to model two- factor variations such as â€œstyleâ€ and â€œcontentâ€ for images in our case is to model two factor variations in location and appearance of partsï¼šä½†å¹¶ä¸æ˜¯explicit modelingå› ä¸ºæœ€ç»ˆæ˜¯ä¸ªåˆ†ç±»å¤´ one used to analyze videos modeling the temporal aspect and the spatial aspect dimension reduction two 512-dim feature results in 512x512-dim earlier work projects one feature to a lower-dimensional space, e.g. 64-dimâ€”&gt;512x64-dim we use compact bilinear pooling to generate low-dimensional embeddings (8-32x) æ–¹æ³• architecture input $(l,I)$ï¼štakes an image and a locationï¼Œlocation generally contains position and scale quadruple $B=(f_A, f_B, P, C)$ Aã€Bä¸¤ä¸ªCNNï¼šconv+pooling layersï¼Œ Pï¼špooling function combined A&amp;B outputs using the matrix outer product average pooling Cï¼šlogistic regression or linear SVM we found that linear models are effective on top of bilinear features CNN independentï¼partial sharedï¼fully shared bilinear combination for each location $bilinear(l,I,f_A,f_B)=f_A(l,I)^T f_B(l,I)$ pooling function combines bilinear features across all locations $\Phi (I) = \sum_{l\in L} bilinear(l,I,f_A,f_B)$ same feature dimension K for A &amp; Bï¼Œe.g. KxM &amp; KxN respectivelyï¼Œ$\Phi(I)$ is size MxN Normalization a signed square rootï¼š$y=sign(x)\sqrt {|x|}$ follow a l2 normï¼š$z = \frac{y}{||y||_2}$ improves performance in practice classification logistic regression or linear SVM we found that linear models are effective on top of bilinear features back propagation $\frac{dl}{dA}=B(\frac{dl}{dx})^T$ï¼Œ$\frac{dl}{dB}=A(\frac{dl}{dx})^T$ Relation to classical texture representationsï¼šæ”¾åœ¨è¿™ä¸€èŠ‚æ’‘ç¯‡å¹…ï¼Ÿï¼Ÿ texture representations can be defined by the choice of the local features, the encoding function, the pooling function, and the normalization function choice of local featuresï¼šorderless aggregation with sumï¼max operation encoding functionï¼šA non-linear encoding is typically applied to the local feature before aggregation normalizationï¼šnormalization of the aggregated feature is done to increase invariance end-to-end trainable]]></content>
      <tags>
        <tag>ç»†ç²’åº¦ï¼Œç‰¹å¾èåˆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[label smoothing]]></title>
    <url>%2F2020%2F09%2F14%2Flabel-smoothing%2F</url>
    <content type="text"><![CDATA[åŠ¨æœº to understand label smoothing improving generalization improves model calibration changes the representations learned by the penultimate layer of the network effect on knowledge distillation of a student network soft targetsï¼ša hard target and the uniform distribution of other classes è®ºç‚¹ label smoothing implicitly calibrates the learned models èƒ½è®©confidencesæ›´æœ‰è§£é‡Šæ€§â€”â€”more aligned with the accuracies of their predictions label smoothing impairs distillationâ€”â€”teacherç”¨äº†label smoothingï¼Œstudentä¼šè¡¨ç°å˜å·®ï¼Œthis adverse effect results from loss of information in the digits æ–¹æ³• modeling penultimate layerï¼šfc with activation $p_k = \frac{e^{wx}}{\sum e^{wx}}$ outputsï¼šloss $H(y,p)=\sum_{k=1}^K -y_klog(p_k)$ hard targetsï¼š$y_k$ is 1 for the correct class and 0 for the rest label smoothingï¼š$y_k^{LS} = y_k(1-\alpha)+ \alpha /K$ visualization schem å°†dimK activation vectoræŠ•å½±åˆ°æ­£äº¤å¹³é¢ä¸Šï¼Œa dim2 vector per example clusters are much tighter because label smoothing encourages that each example in training set to be equidistant from all the other classâ€™s templates 3 classes shows triangle structure since â€˜equidistantâ€™ predictionsâ€˜ absolute values are much bigger without LM, representing over-confident semantically similar classes are harder to separateï¼Œä½†æ˜¯æ€»ä½“ä¸Šclusterå½¢æ€è¿˜æ˜¯å¥½ä¸€ç‚¹ training without label smoothing there is continuous degree of change between two semantically similar classesï¼Œç”¨äº†LMä»¥åå°±è§‚å¯Ÿä¸åˆ°äº†â€”â€”ç›¸ä¼¼classä¹‹é—´çš„è¯­ä¹‰ç›¸å…³æ€§è¢«ç ´åäº†ï¼Œâ€™erasure of informationâ€™ have similar accuracies despite qualitatively different clusteringï¼Œå¯¹åˆ†ç±»ç²¾åº¦çš„æå‡ä¸æ˜æ˜¾ï¼Œä½†æ˜¯ä»clusterå½¢æ€ä¸Šçœ‹æ›´å¥½çœ‹ model calibration making the confidence of its predictions more accurately represent their accuracy metricï¼šexpected calibration error (ECE) reliability diagram better calibration compared to the unscaled network Despite trying to collapse the training examples to tiny clusters, these networks generalize and are calibratedï¼šåœ¨è®­ç»ƒé›†ä¸Šçš„clusteråˆ†å¸ƒéå¸¸ç´§å‡‘ï¼Œencourageæ¯ä¸ªæ ·æœ¬éƒ½å’Œå…¶ä»–ç±»åˆ«çš„clusterä¿æŒç›¸åŒçš„è·ç¦»ï¼Œä½†æ˜¯åœ¨æµ‹è¯•é›†ä¸Šï¼Œæ ·æœ¬çš„åˆ†å¸ƒå°±æ¯”è¾ƒæ¾æ•£äº†ï¼Œä¸ä¼šé™å®šåœ¨å°å°çš„ä¸€å¨å†…ï¼Œè¯´æ˜ç½‘ç»œæ²¡æœ‰over-confidentï¼Œrepresenting the full range of confidences for each prediction knowledge distillation even when label smoothing improves the accuracy of the teacher network, teachers trained with label smoothing produce inferior student networks As the representations collapse to small clusters of points, much of the information that could have helped distinguish examples is lost çœ‹training setçš„scatterï¼ŒLMä¼šå€¾å‘äºå°†ä¸€ç±»sampleé›†ä¸­æˆä¸ºç›¸ä¼¼çš„è¡¨å¾ï¼Œsampleä¹‹é—´çš„å·®å¼‚æ€§ä¿¡æ¯ä¸¢äº†ï¼šTherefore a teacher with better accuracy is not necessarily the one that distills better]]></content>
      <tags>
        <tag>åˆ†ç±»ï¼Œloss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[noisy student]]></title>
    <url>%2F2020%2F09%2F11%2Fnoisy-student%2F</url>
    <content type="text"><![CDATA[Self-training with Noisy Student improves ImageNet classification åŠ¨æœº semi-supervised learningï¼ˆSSLï¼‰ semi-supervised approach when labeled data is abundant use unlabeled images to improve SOTA model improve self-training and distillation accuracy and robustness better acc, mCE, mFR EfficientNet model on labeled images student even or larger student model on labeled &amp; pseudo labeled images noise, stochastic depth, data augmentation generalizes better process iteration by putting back the student as the teacher è®ºç‚¹ supervised learning which requires a large corpus of labeled images to work well robustness noisy dataï¼šunlabeled images that do not belong to any category in ImageNet large margins on much harder test sets training process teacher EfficientNet model on labeled images student even or larger student model on labeled &amp; pseudo labeled images noise, stochastic depth, data augmentation generalizes better process iteration by putting back the student as the teacher improve in two ways it makes the student largerï¼šå› ä¸ºç”¨äº†æ›´å¤šæ•°æ® noised student is forced to learn harderï¼šå› ä¸ºlabelæœ‰pseudo labelsï¼Œinputæœ‰å„ç±»augmentationï¼Œç½‘ç»œæœ‰dropoutï¼stochastic depth main difference compared with Knowledge Distillation use noise â€”â€”â€” KD do not use use equal/larger student â€”â€”â€” KD use smaller student to learn faster think of as Knowledge Expansion giving the student model enough capacity and difficult environments want the student to be better than the teacher æ–¹æ³• algorithm train teacher use labeled images use teacher to inference unlabedled images, generating pseudo labels, soft/one-hot train student model use labeled &amp; unlabeld images make student the new teacher, jump to the inter step noise enforcing invariancesï¼šè¦æ±‚studentç½‘ç»œèƒ½å¤Ÿå¯¹å„ç§å¢å¼ºåçš„æ•°æ®é¢„æµ‹labelä¸€æ ·ï¼Œensure consistency required to mimic a more powerful ensemble modelï¼šteacherç½‘ç»œåœ¨inferenceé˜¶æ®µè¿›è¡Œdropoutå’Œstochastic depthï¼Œbehaves like an ensembleï¼Œwhereas the student behaves like a single modelï¼Œè¿™å°±push studentç½‘ç»œå»å­¦ä¹ ä¸€ä¸ªæ›´å¼ºå¤§çš„æ¨¡å‹ other techniques data filtering we filter images that the teacher model has low confidences è¿™éƒ¨åˆ†dataä¸training dataçš„åˆ†å¸ƒèŒƒå›´å†… data balancing duplicate images in classes where there are not enough images take the images with the highest confidence when there are too many softï¼hard pseudo labels both work soft slightly better å®éªŒ dataset benchmarked datasetï¼šImageNet 2012 ILSVRC unlabeled datasetï¼šJFT fillter &amp; balancingï¼š use EfficientNet-B0 trained on ImageNetï¼Œinference over JFT take images with confidence over 0.3 130M at most per class models EfficientNet-L2 further scale up EfficientNet-B7 wider &amp; deeper lower resolution train-test resolution discrepancy first perform normal training with a smaller resolution for 350 epochs then finetune the model with a larger resolution for 1.5 epochs on unaugmented labeled images shallow layers are fixed during finetuning noise stochastic depthï¼šstochastic depth 0.8 for the final layer and follow the linear decay rule for other layers dropoutï¼šdropout 0.5 for the final layer RandAugmentï¼štwo random operations with magnitude set to 27 iterative training ã€teacherã€‘first trained an EfficientNet-B7 on ImageNet ã€studentã€‘then trained an EfficientNet-L2 with the unlabeled batch size set to 14 times the labeled batch size ã€new teacherã€‘trained a new EfficientNet-L2 ã€new studentã€‘trained an EfficientNet-L2 with the unlabeled batch size set to 28 times the labeled batch size ã€iterationã€‘â€¦ robustness test difficult images common corruptions and perturbations FGSM attack metrics improves the top-1 accuracy reduces mean corruption error (mCE) reduces mean flip rate (mFR) ablation study noisy å¦‚æœä¸noise the studentï¼Œå½“student modelçš„é¢„æµ‹å’Œteacheré¢„æµ‹çš„unlabeledæ•°æ®å®Œå…¨ä¸€æ ·çš„æƒ…å†µä¸‹ï¼Œlossä¸º0ï¼Œä¸å†å­¦ä¹ ï¼Œè¿™æ ·studentå°±ä¸èƒ½outperform teacheräº† injecting noise to the student model enables the teacher and the student to make different predictions The student performance consistently drops with noise function removed removing noise leads to a smaller drop in training lossï¼Œè¯´æ˜noiseçš„ä½œç”¨ä¸æ˜¯ä¸ºäº†preventing overfittingï¼Œå°±æ˜¯ä¸ºäº†enhance model iteration iterative training is effective in producing increas- ingly better models larger batch size ratio for latter iteration]]></content>
      <tags>
        <tag>classification, semi-supervised, teacher-student</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[complement cross entropy]]></title>
    <url>%2F2020%2F09%2F08%2Fcomplement-cross-entropy%2F</url>
    <content type="text"><![CDATA[summary ä½¿ç”¨complement lossçš„ä¸»è¦åŠ¨æœºæ˜¯one-hotçš„labelä¸‹ï¼Œceåªå…³æ³¨æ‹‰é«˜æ­£æ ·æœ¬æ¦‚ç‡ï¼Œä¸§å¤±æ‰äº†å…¶ä»–incorrectç±»åˆ«çš„ä¿¡æ¯ äº‹å®ä¸Šå¯¹äºincorrectç±»åˆ«ï¼Œå¯ä»¥è®©å…¶è¾“å‡ºæ¦‚ç‡å€¼åˆ†å¸ƒçš„ç†µå°½å¯èƒ½çš„å¤§â€”â€”ä¹Ÿå°±æ˜¯å°†è¿™ä¸ªåˆ†å¸ƒå°½å¯èƒ½æ¨å‘å‡åŒ€åˆ†å¸ƒï¼Œè®©å®ƒä»¬ä¹‹é—´äº’ç›¸éåˆ¶ä»è€Œå‡¸æ˜¾å‡ºground truthçš„æ¦‚ç‡ ä½†è¿™æ˜¯å»ºç«‹åœ¨â€œå„ä¸ªæ ‡ç­¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹â€è¿™ä¸ªå‡è®¾ä¸Šï¼Œå¦‚æœç±»åˆ«é—´æœ‰hierarchicalçš„å…³ç³»ï¼multi-labelï¼Œå°±ä¸è¡Œäº†ã€‚ åœ¨æ•°å­¦è¡¨è¾¾ä¸Šï¼Œ é¦–å…ˆä»ç„¶æ˜¯ç”¨ceä½œç”¨äºcorrect labelï¼Œå¸Œæœ›æ­£æ ·æœ¬æ¦‚ç‡gt_predå°½å¯èƒ½æé«˜ï¼Œæ¥è¿‘çœŸå®å€¼ ç„¶åæ˜¯ä½œç”¨äºincorrect labelçš„cceï¼Œåœ¨é™¤äº†æ­£ä¾‹pred possibilityä»¥å¤–çš„å‡ ä¸ªæ¦‚ç‡ä¸Šï¼Œè®¡ç®—äº¤å‰ç†µï¼Œå¸Œæœ›è¿™å‡ ä¸ªæ¦‚ç‡å°½å¯èƒ½æœä»å‡åŒ€åˆ†å¸ƒï¼Œæ¦‚ç‡æ¥è¿‘$\frac{1-gt_pred}{K-1}$ æˆ‘æ„Ÿè§‰è¿™å°±æ˜¯label smoothingï¼Œä¸»è¦åŒºåˆ«å°±æ˜¯cceä¸Šæœ‰ä¸ªnormé¡¹ï¼Œlabel smoothinåœ¨è®¡ç®—ceçš„æ—¶å€™ï¼Œvectorä¸­æ¯ä¸€ä¸ªincorrect labelçš„ç†µéƒ½ä¸correct labelç­‰æƒé‡ï¼Œcceå¯¹æ•´ä¸ªincorrect vectorçš„æƒé‡ä¸correct labelç­‰åŒï¼Œä¸”å¯ä»¥è°ƒæ•´ã€‚ Imbalanced Image Classification with Complement Cross Entropy åŠ¨æœº class-balanced datasets motivated by COT(complement objective training) suppressing softmax probabilities on incorrect classes during training propose cce keep ground truth probability overwhelm the other classes neutralizing predicted probabilities on incorrect classes è®ºç‚¹ class imbalace limits generalization resample oversampling on minority classes undersampling on majority classes reweight neglect the fact that samples on minority classes may have noise or false annotations might cause poor generalization observed degradation in imbalanced datasets using CE cross entropy mostly ignores output scores on wrong classes neutralizing predicted probabilities on incorrect classes helps improve accuracy of prediction for imbalanced image classification æ–¹æ³• complement entropy calculated on incorrect classes N samplesï¼ŒK-dims class vector $C(y,\hat y)=-\frac{1}{N}\sum_{i=1}^N\sum_{j=1,j \neq g}^K \frac{\hat y^j}{1-\hat y^g}log\frac{\hat y^j}{1-\hat y^g} $ the purpose is to encourage larger gap between ground truth and other classes â€”â€” when the incorrect classes obey normal distribution it reaches optimal balanced complement entropy add balancing factor $C^{â€˜}(y,\hat y) = \frac{1}{K-1}C(y,\hat y)$ forming COTï¼š twice back-propagation per each iteration first cross entropy second complement entropy CCE (Complement Cross Entropy) add modulating factorï¼š$\tilde C(y, \hat y) = \frac{\gamma}{K-1}C(y, \hat y)$ï¼Œ$\gamma=-1$ combinationï¼šCE+CCE å®éªŒ datasetï¼š cifar class-balanced originally construct imbalanced variants with imbalance ratio $\frac{N_{min}}{N_{max}}$ test acc è®ºæ–‡çš„å®éªŒç»“æœéƒ½æ˜¯åœ¨cifarä¸Šcceå¥½äºcotå¥½äºfocal lossï¼Œåœ¨roadä¸Šcceå¥½äºcotï¼Œæ²¡æ”¾fl å’±ä¹Ÿä¸çŸ¥é“ã€‚ã€‚ã€‚]]></content>
      <tags>
        <tag>loss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[regression loss]]></title>
    <url>%2F2020%2F09%2F07%2Fregression-loss%2F</url>
    <content type="text"><![CDATA[æŸå¤±å‡½æ•°ç”¨æ¥è¯„ä»·æ¨¡å‹é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„ä¸ä¸€æ ·ç¨‹åº¦ ä¸¤ç³»æŸå¤±å‡½æ•°ï¼š ç»å¯¹å€¼loss $L(Y,f(x))=|Y-f(x)|$ å¹³å‡ç»å¯¹å€¼æŸå¤±ï¼ŒMAEï¼ŒL1 å¯¹å¼‚å¸¸ç‚¹æœ‰æ›´å¥½çš„é²æ£’æ€§ æ›´æ–°çš„æ¢¯åº¦å§‹ç»ˆç›¸åŒï¼Œå¯¹äºå¾ˆå°çš„æŸå¤±å€¼ï¼Œæ¢¯åº¦ä¹Ÿå¾ˆå¤§ï¼Œä¸åˆ©äºæ¨¡å‹å­¦ä¹ â€”â€”æ‰‹åŠ¨è¡°å‡å­¦ä¹ ç‡ å¹³æ–¹å·®loss $L(Y, f(x)) = (Y-f(x))^2$ å‡æ–¹è¯¯å·®æŸå¤±ï¼ŒMSEï¼ŒL2 å› ä¸ºå–äº†å¹³æ–¹ï¼Œä¼šèµ‹äºˆå¼‚å¸¸ç‚¹æ›´å¤§çš„æƒé‡ï¼Œä¼šä»¥ç‰ºç‰²å…¶ä»–æ ·æœ¬çš„è¯¯å·®ä¸ºä»£ä»·ï¼Œæœç€å‡å°å¼‚å¸¸ç‚¹è¯¯å·®çš„æ–¹å‘æ›´æ–°ï¼Œé™ä½æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ Huber loss $L = \begin{cases} \frac{1}{2}(y-f(x))^2,\text{ for }|y-f(x)|&lt;\delta,\\ \delta |y-f(x)|-\frac{1}{2}\delta^2, \text{ otherwise} \end{cases} $ è¶…å‚å†³å®šäº†å¯¹ä¸å¼‚å¸¸ç‚¹çš„å®šä¹‰ï¼Œåªå¯¹è¾ƒå°çš„å¼‚å¸¸å€¼æ•æ„Ÿ å¯¹æ•°loss L(Y, P(Y|X)) = -log(P(Y|X)) cross-entropy loss äºŒåˆ†ç±»åŒè¾¹è®¡ç®—ï¼š L = ylna + (1-y)ln(1-a) å¤šåˆ†ç±»å•è¾¹è®¡ç®—ï¼š L = ylna æŒ‡æ•°loss L(Y, f(x)) = exp[-yf(x)] Hinge loss L(Y, f(x)) = max(0, 1-yf(x)) perceptron loss L(Y, f(x)) = max(0, -yf(x)) cross-entropy loss äºŒåˆ†ç±»åŒè¾¹è®¡ç®—ï¼š L = ylna + (1-y)ln(1-a) å¤šåˆ†ç±»å•è¾¹è®¡ç®—ï¼š L = ylna]]></content>
      <tags>
        <tag>å›å½’</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pseudo-3d]]></title>
    <url>%2F2020%2F09%2F02%2Fpseudo-3d%2F</url>
    <content type="text"><![CDATA[[3d resnet] Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognitionï¼šçœŸ3dï¼Œfor comparisonï¼Œåˆ†ç±» [C3d] Learning Spatiotemporal Features with 3D Convolutional Networksï¼šçœŸ3dï¼Œfor comparisonï¼Œåˆ†ç±» [Pseudo-3D resnet] Learning Spatio-Temporal Representation with Pseudo-3D Residual Networksï¼šä¼ª3dï¼Œresblockï¼ŒSå’ŒTèŠ±å¼è¿æ¥ï¼Œåˆ†ç±» [2.5d Unet] Automatic Segmentation of Vestibular Schwannoma from T2-Weighted MRI by Deep Spatial Attention with Hardness-Weighted Lossï¼špatchè¾“å…¥ï¼Œå…ˆ2då3dï¼Œé’ˆå¯¹å„å‘å¼‚æ€§ï¼Œåˆ†å‰² [two-pathway U-Net] Combining analysis of multi-parametric MR images into a convolutional neural network: Precise target delineation for vestibular schwannoma treatment planningï¼špatchè¾“å…¥ï¼Œ3dç½‘ç»œï¼Œxyå’Œzå¹³é¢åˆ†åˆ«conv &amp; concatï¼Œåˆ†å‰² [Projection-Based 2.5D U-net] Projection-Based 2.5D U-net Architecture for Fast Volumetric Segmentationï¼šmipï¼Œ2dç½‘ç»œï¼Œåˆ†å‰²ï¼Œé‡å»º [New 2.5D Representation] A New 2.5D Representation for Lymph Node Detection using Random Sets of Deep Convolutional Neural Network Observationsï¼šæ¨ªå† çŸ¢ä¸‰ä¸ªå¹³é¢ä½œä¸ºä¸‰ä¸ªchannelè¾“å…¥ï¼Œ2dç½‘ç»œï¼Œæ£€æµ‹ Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks åŠ¨æœº spatio-temporal video the development of a very deep 3D CNN from scratch results in expensive computational cost and memory demand new framework 1x3x3 &amp; 3x1x1 Pseudo-3D Residual Net which exploits all the variants of blocks outperforms 3D CNN and frame-based 2D CNN è®ºç‚¹ 3d CNNçš„model sizeï¼šmaking it extremely difficult to train a very deep model fine-tuning 2d å¥½äº train from scrach 3d RNN builds only the temporal connections on the high-level featuresï¼Œleaving the correlations in the low-level forms not fully exploited we propose 1x3x3 &amp; 3x1x1 in parallel or cascaded å…¶ä¸­çš„3x3 convå¯ä»¥ç”¨2d convæ¥åˆå§‹åŒ– a family of bottleneck building blocksï¼šenhance the structural diversity æ–¹æ³• P3D Blocks directï¼indirect influenceï¼šSå’ŒTä¹‹é—´æ˜¯ä¸²è”è¿˜æ˜¯å¹¶è” directï¼indirect connected to the final outputï¼šSå’ŒTçš„è¾“å‡ºæ˜¯å¦ç›´æ¥ä¸identity pathç›¸åŠ  bottleneckï¼š å¤´å°¾å„æ¥ä¸€ä¸ª1x1x1çš„conv å¤´ç”¨æ¥narrow channelï¼Œå°¾ç”¨æ¥widen back å¤´æœ‰reluï¼Œå°¾æ²¡æœ‰relu Pseudo-3D ResNet mixing blocksï¼šå¾ªç¯ABC better performance &amp; small increase in model size fine-tuning resnet50ï¼š randomly cropped 224x224 freeze all BN except for the first one add an extra dropout layer with 0.9 dropout rate further fine-tuning P3D resnetï¼š initialize with r50 in last step randomly cropped 16x160x160 horizontally flipped mini-batch as 128 frames future work attention mechanism will be incorporated Projection-Based 2.5D U-net Architecture for Fast Volumetric Segmentation åŠ¨æœº MIPï¼š2D images containing information of the full 3D image faster, less memory, accurate æ–¹æ³• 2d unet MIPï¼š$\alpha=36$ 3x3 conv, s2 pooling, transpose conv, concat, BN, relu, filtersï¼šbegin with 32, end with 512 dropoutï¼š0.5 in the deepest convolutional block and 0.2 in the second deepest blocks 3d unet overfitting &amp; memory space filtersï¼šbegin with 4, end with 16 dropoutï¼š0.5 in the deepest convolutional block and 0.4 in the second deepest blocks Projection-Based 2.5D U-net 2d sliceï¼šloss of connection 2d mipï¼šdisappointing results 2d volumeï¼šlong training time the proposed 2.5D U-netï¼š N(x) = T R_p F_p \left[ \begin{matrix} U M_{\alpha_1}(x) \\ ... \\ U M_{\alpha_p}(x) \end{matrix} \right] $M_{i}$ï¼šMIPï¼Œp=12 $U$ï¼š2d-Unet like above $F_p$ï¼šlearnable filtrationï¼Œ1x3 convï¼Œfor each projectionï¼ŒæŠ‘åˆ¶é‡å»ºä¼ªå½± $R_p$ï¼šreconstruction operator $T$ï¼šfine-tuning operatorï¼Œshift &amp; scale back to 0-1 mask Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition åŠ¨æœº 3D kernels tend to overfit 3D CNNs is relatively shallow propose a 3D CNNs based on ResNets better performance not overfit deeper than C3D è®ºç‚¹ two-stream architectureï¼šconsists of RGB and optical flow streams is often used to represent spatio-temporal information 3D CNNsï¼štrained on relatively small video datasets performs worse than 2D CNNs pretrained on large datasets Very deep 3D CNNsï¼šnot explored yet due to training difficulty æ–¹æ³• Network Architecture main differenceï¼škernel dimensions stemï¼šstride2 for Sï¼Œstride1 for T resblockï¼šconv_bn_relu&amp;conv + id identity shortcutsï¼šuse zero-padding for increasing dimensionsï¼Œto avoid increasing the number of parameters stride2 convï¼šconv3_1ã€ conv4_1ã€ conv5_1 input clipsï¼š3x16x112x112 large learning rate and batch size was important å®éªŒ åœ¨å°æ•°æ®é›†ä¸Š3d-r18ä¸å¦‚C3Dï¼Œoverfitäº†ï¼šshallow architecture of the C3D and pretraining on the Sports-1M dataset prevent the C3D from overfitting åœ¨å¤§æ•°æ®é›†ä¸Š3d-r34å¥½äºC3Dï¼ŒåŒæ—¶C3Dçš„val accæ˜æ˜¾é«˜äºtrain accâ€”â€”å¤ªshallowæ¬ æ‹Ÿåˆäº†ï¼Œr34åˆ™è¡¨ç°æ›´å¥½ï¼Œè€Œä¸”ä¸éœ€è¦é¢„è®­ç»ƒ RGB-I3D achieved the best performance 3d-r34æ˜¯æ›´deeperçš„ RGB-I3Dç”¨äº†æ›´å¤§çš„batch sizeï¼šLarge batch size is important to train good models with batch normalization High resolutionsï¼š3x64x224x224 Learning Spatiotemporal Features with 3D Convolutional Networks åŠ¨æœº generic efficient simple 3d ConvNet with 3x3x3 conv &amp; a simple linear classifier è®ºç‚¹ 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets 2D ConvNets lose temporal information of the input signal right after every convolution operation 2d convåœ¨channelç»´åº¦ä¸Šæƒé‡éƒ½æ˜¯ä¸€æ ·çš„ï¼Œç›¸å½“äºtemporal dimsä¸Šæ²¡æœ‰é‡è¦æ€§ç‰¹å¾æå– æ–¹æ³• basic network settings 5 conv layers + 5 pooling layers + 2 fc layers + softmax filtersï¼š[64ï¼Œ128ï¼Œ256ï¼Œ256ï¼Œ256] fc dimsï¼š[2048ï¼Œ2048] conv kernelï¼šdx3x3 pooling kernelï¼š2x2x2ï¼Œs2 except for the first layer with the intention of not to merge the temporal signal too early also to satisfy the clip length of 16 frames varing settings temporal kernel depth homogeneousï¼šdepth-1/3/5/7 throughout varyingï¼šincreasing-3-3-5-5-7 &amp; decreasing-7- 5-5-3-3 depth-3 throughout performs the best depth-1 is significantly worse We also verify that 3D ConvNet consistently performs better than 2D ConvNet on a large-scale internal dataset C3D 8 conv layers + 5 pooling layers + 2 fc layers + softmax homogeneousï¼š3x3x3 s1 conv thtoughout pool1ï¼š1x2x2 kernel size &amp; strideï¼Œrest 2x2x2 fc dimsï¼š4096 C3D video descriptorï¼šfc6 activations + L2-norm deconvolution visualizingï¼š conv5b feature maps starts by focusing on appearance in the first few frames tracks the salient motion in the subsequent frames compactness PCA å‹ç¼©åˆ°50-100dimä¸å¤ªæŸå¤±acc å‹ç¼©åˆ°10dimä»æ—§æ˜¯æœ€é«˜acc projected to 2-dimensional space using t-SNE C3D features are semantically separable compared to Imagenet quantitatively observe that C3D is better than Imagenet Action Similarity Labeling predicting action similarity extract C3D features: prob, fc7, fc6, pool5 for each clip L2 normalization compute the 12 different distances for each featureï¼š48 in total linear SVM is trained on these 48-dim feature vectors C3D significantly outperforms the others]]></content>
      <tags>
        <tag>3d CNN, 2.5d CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSD]]></title>
    <url>%2F2020%2F08%2F13%2FSSD%2F</url>
    <content type="text"><![CDATA[SSD: Single Shot MultiBox Detector åŠ¨æœº single network speed &amp; accuracy 59 FPS / 74.3% mAP è®ºç‚¹ prev methods two-stageï¼šç”Ÿæˆç¨€ç–çš„å€™é€‰æ¡†ï¼Œç„¶åå¯¹å€™é€‰æ¡†è¿›è¡Œåˆ†ç±»ä¸å›å½’ one-stageï¼šå‡åŒ€åœ°åœ¨å›¾ç‰‡çš„ä¸åŒä½ç½®ï¼Œé‡‡ç”¨ä¸åŒå°ºåº¦å’Œé•¿å®½æ¯”ï¼Œè¿›è¡Œå¯†é›†æŠ½æ ·ï¼Œç„¶ååˆ©ç”¨CNNæå–ç‰¹å¾åç›´æ¥è¿›è¡Œåˆ†ç±»ä¸å›å½’ fundamental speed improvement eliminating bounding box proposals eliminating feature resampling other improvements small convolutional filter for bbox categories and offsetsï¼ˆé’ˆå¯¹yolov1çš„å…¨è¿æ¥å±‚è¯´ï¼‰ separate predictors by aspect ratio multiple scales è¿™äº›æ“ä½œéƒ½ä¸æ˜¯åŸåˆ› The core of SSD is predicting category scores and box offsets for a fixed set of default bounding boxes using small convolutional filters applied to feature maps. æ–¹æ³• Model Multi-scale feature maps for detectionï¼šé‡‡ç”¨äº†å¤šå°ºåº¦çš„ç‰¹å¾å›¾ï¼Œé€æ¸ç”¨s2é™ç»´ï¼Œå¤§å°ºåº¦ç‰¹å¾å›¾ä¸Šæœ‰æ›´å¤šçš„å•å…ƒï¼Œç”¨æ¥å›å½’å°ç‰©ä½“ Convolutional predictors for detectionï¼šé’ˆå¯¹yolov1é‡Œé¢çš„fcå±‚ Default boxes and aspect ratiosï¼šä¸€ä¸ªå•å…ƒ4ç§sizeçš„å…ˆéªŒæ¡†ï¼Œå¯¹æ¯ä¸ªå…ˆéªŒæ¡†éƒ½é¢„æµ‹ä¸€ç»„4+(c+1)ï¼Œå…¶ä¸­çš„1å¯ä»¥çœ‹ä½œèƒŒæ™¯ç±»ï¼Œä¹Ÿå¯ä»¥çœ‹åšæ˜¯æœ‰æ— ç›®æ ‡çš„ç½®ä¿¡åº¦ï¼Œå„ç”¨ä¸€ä¸ªconv3x3çš„head backbone å‚è€ƒï¼šhttps://www.cnblogs.com/sddai/p/10206929.html VGG16å‰å››ä¸ªconv blockä¿ç•™ æ— dropoutå’Œfc conv5çš„æ± åŒ–ç”±2x2-s2å˜æˆ3x3-s1 conv6å’Œconv7æ˜¯3x3x1024å’Œ1x1x1024çš„ç©ºæ´å·ç§¯ï¼Œè¾“å‡º19x19x1024 conv8æ˜¯1x1x256å’Œ3x3x512 s2çš„convï¼Œè¾“å‡º10x10x512 conv9éƒ½æ˜¯1x1x128å’Œ3x3x256 s2çš„convï¼Œè¾“å‡º5x5x256 conv10ã€conv11éƒ½æ˜¯1x1x128å’Œ3x3x256 s1 p0çš„convï¼Œè¾“å‡º3x3x256ã€1x1x256 Training Matching strategyï¼šmatch default boxå’Œgt box é¦–å…ˆä¸ºæ¯ä¸€ä¸ªgt boxæ‰¾åˆ°ä¸€ä¸ªoverlapæœ€å¤§çš„default box ç„¶åæ‰¾åˆ°æ‰€æœ‰ä¸gt boxçš„overlapå¤§äº0.5çš„default box ä¸€ä¸ªgt boxå¯èƒ½å¯¹åº”å¤šä¸ªdefault box ä¸€ä¸ªdefault boxåªèƒ½å¯¹åº”ä¸€ä¸ªgt boxï¼ˆoverlapæœ€å¤§çš„ï¼‰ Objective loss loc lossï¼šsmooth L1ï¼Œoffsets like Faster R-CNN cls lossï¼šsoftmax loss weighted sumï¼š$L = \frac{1}{N} (L_{cls} + \alpha L_{loc})$ï¼Œ N is the number of matched default boxes loss=0 when N=0 Choosing scales and aspect ratios for default boxes æ¯ä¸ªlevelçš„feature mapæ„Ÿå—é‡ä¸åŒï¼Œdefault boxçš„å°ºå¯¸ä¹Ÿä¸åŒ æ•°é‡ä¹Ÿä¸åŒï¼Œconv4ã€conv10å’Œconv11æ˜¯4ä¸ªï¼Œconv7ã€conv8ã€conv9æ˜¯6ä¸ª ratioï¼š{1,2,3,1/2,1/3}ï¼Œ4ä¸ªçš„æ²¡æœ‰3å’Œ1/3 L2 normalization for conv4ï¼š $y_i = \frac{x_i}{\sqrt{\sum_{k=1}^n x_k^2}}$ ä½œç”¨æ˜¯å°†ä¸åŒå°ºåº¦çš„ç‰¹å¾éƒ½å½’ä¸€åŒ–æˆæ¨¡ä¸º1çš„å‘é‡ scaleï¼šå¯ä»¥æ˜¯å›ºå®šå€¼ï¼Œä¹Ÿå¯ä»¥æ˜¯å¯å­¦ä¹ å‚æ•° ä¸ºå•¥åªé’ˆå¯¹conv4ï¼Ÿä½œè€…çš„å¦ä¸€ç¯‡paper(ParseNet)ä¸­å‘ç°conv4å’Œå…¶ä»–å±‚ç‰¹å¾çš„scaleæ˜¯ä¸ä¸€æ ·çš„ predictions all default boxes with different scales and aspect ratio from all locations of many feature maps significant imbalance for positive/negative Hard negative mining sort using the highest confidence loss pick the top ones with n/p at most 3:1 faster optimization and a more stable training Data augmentation sample a patch with specific IoU resize æ€§è´¨ much worse performance on smaller objects, increasing the input size can help improve Data augmentation is crucial, resulting in a 8.8% mAP improvement Atrous is faster, ä¿ç•™pool5ä¸å˜çš„è¯ï¼Œthe result is about the same while the speed is about 20% slower]]></content>
      <tags>
        <tag>ç›®æ ‡æ£€æµ‹</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pythonå¤šçº¿ç¨‹&å¤šè¿›ç¨‹]]></title>
    <url>%2F2020%2F08%2F04%2Fpython%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E5%A4%9A%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Referenceï¼š https://www.cnblogs.com/kaituorensheng/p/4465768.html https://zhuanlan.zhihu.com/p/46368084 https://www.runoob.com/python3/python3-multithreading.html åè¯ è¿›ç¨‹(process)å’Œçº¿ç¨‹(thread) cpuåœ¨å¤„ç†ä»»åŠ¡æ—¶ï¼ŒæŠŠæ—¶é—´åˆ†æˆè‹¥å¹²ä¸ªå°æ—¶é—´æ®µï¼Œè¿™äº›æ—¶é—´æ®µå¾ˆå°çš„ï¼Œç³»ç»Ÿä¸­æœ‰å¾ˆå¤šè¿›ç¨‹ï¼Œæ¯ä¸ªè¿›ç¨‹ä¸­åˆåŒ…å«å¾ˆå¤šçº¿ç¨‹ï¼Œåœ¨åŒä¸€æ—¶é—´æ®µ å†…ï¼Œç”µè„‘CPUåªèƒ½å¤„ç†ä¸€ä¸ªçº¿ç¨‹ï¼Œä¸‹ä¸€ä¸ªæ—¶é—´æ®µï¼Œå¯èƒ½åˆå»æ‰§è¡Œåˆ«çš„çº¿ç¨‹äº†ï¼ˆæ—¶é—´ç‰‡è½®è½¬ï¼Œä»è€Œå®ç°ä¼ªå¤šä»»åŠ¡ï¼‰ï¼Œå…·ä½“é¡ºåºå–å†³äºå…¶è°ƒåº¦é€»è¾‘ å¤šæ ¸cpuå¯ä»¥å®ç°çœŸæ­£çš„å¹¶è¡Œï¼ŒåŒä¸€ä¸ªæ—¶åˆ»æ¯ä¸ªcpuä¸Šéƒ½å¯ä»¥è·‘ä¸€ä¸ªä»»åŠ¡ å¤šè¿›ç¨‹ï¼šæ¯ä¸ªè¿›ç¨‹åˆ†åˆ«æ‰§è¡ŒæŒ‡å®šä»»åŠ¡ï¼Œè¿›ç¨‹é—´äº’ç›¸ç‹¬ç«‹ï¼Œæ¯ä¸ªæ—¶åˆ»å¹¶è¡Œçš„å®é™…è¿›ç¨‹æ•°å–å†³äºcpuæ•°é‡ å¤šçº¿ç¨‹ï¼šå•ä¸ªcpuåŒä¸€æ—¶åˆ»åªèƒ½å¤„ç†ä¸€ä¸ªçº¿ç¨‹ï¼Œä¸€ä¸ªä»»åŠ¡å¯èƒ½ç”±å¤šä¸ªå·¥äººæ¥å®Œæˆï¼Œå·¥äººä»¬ç›¸äº’ååŒï¼Œè¿™åˆ™æ˜¯å¤šçº¿ç¨‹ pythonçš„å¤šè¿›ç¨‹ï¼šmultiprocessæ¨¡å— pythonçš„å¤šçº¿ç¨‹ï¼šthreadingæ¨¡å— æ¯ä¸ªè¿›ç¨‹åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­æ‹¥æœ‰ç‹¬ç«‹çš„å†…å­˜å•å…ƒï¼Œè€Œä¸€ä¸ªè¿›ç¨‹çš„å¤šä¸ªçº¿ç¨‹åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­å…±äº«å†…å­˜ã€‚ å¤šè¿›ç¨‹multiprocess æ¯è¿›ç¨‹ï¼šå½“æˆ‘ä»¬æ‰§è¡Œä¸€ä¸ªpythonè„šæœ¬ï¼Œif mainä¸‹é¢å®é™…è¿è¡Œçš„ä¸»ä½“å°±æ˜¯æ¯è¿›ç¨‹ å­è¿›ç¨‹ï¼šæˆ‘ä»¬ä½¿ç”¨multiprocessæ˜¾å¼åˆ›å»ºçš„è¿›ç¨‹ï¼Œéƒ½æ˜¯å­è¿›ç¨‹ join()æ–¹æ³•ï¼šç”¨æ¥è®©æ¯è¿›ç¨‹é˜»å¡ï¼Œç­‰å¾…æ‰€æœ‰å­è¿›ç¨‹æ‰§è¡Œå®Œæˆå†ç»“æŸ ä½¿ç”¨multiprocessçš„å¤šè¿›ç¨‹ï¼Œå¯ä»¥é€šè¿‡processæ–¹æ³•å’Œpoolæ–¹æ³• processæ–¹æ³•ï¼šé€‚ç”¨è¿›ç¨‹è¾ƒå°‘æ—¶å€™ï¼Œæ— æ³•æ‰¹é‡å¼€å¯/å…³é—­ poolæ–¹æ³•ï¼šæ‰¹é‡ç®¡ç† å‚æ•°ï¼šè¾“å…¥å‚æ•°éƒ½å·®ä¸å¤šï¼Œç¬¬ä¸€ä¸ªæ˜¯è¦æ‰§è¡Œçš„å‡½æ•°æ–¹æ³•target/funcï¼Œç¬¬äºŒä¸ªæ˜¯è¾“å…¥å‚æ•°args ğŸŒ°Processæ–¹æ³•ï¼š 1234567891011121314151617181920212223from multiprocessing import Processimport osimport timedef long_time_task(i): print('å­è¿›ç¨‹: &#123;&#125; - ä»»åŠ¡&#123;&#125;'.format(os.getpid(), i)) time.sleep(2) print("ç»“æœ: &#123;&#125;".format(8 ** 20))if __name__=='__main__': print('å½“å‰æ¯è¿›ç¨‹: &#123;&#125;'.format(os.getpid())) start = time.time() p1 = Process(target=long_time_task, args=(1,)) p2 = Process(target=long_time_task, args=(2,)) print('ç­‰å¾…æ‰€æœ‰å­è¿›ç¨‹å®Œæˆã€‚') p1.start() p2.start() p1.join() p2.join() end = time.time() print("æ€»å…±ç”¨æ—¶&#123;&#125;ç§’".format((end - start))) processæ–¹æ³•ä½¿ç”¨Processå®ä¾‹åŒ–ä¸€ä¸ªè¿›ç¨‹å¯¹è±¡ï¼Œç„¶åè°ƒç”¨å®ƒçš„startæ–¹æ³•å¼€å¯è¿›ç¨‹ ğŸŒ°Poolæ–¹æ³•ï¼š 123456789101112131415161718192021222324252627282930from multiprocessing import Pool, cpu_countimport osimport timedef long_time_task(i): print('å­è¿›ç¨‹: &#123;&#125; - ä»»åŠ¡&#123;&#125;'.format(os.getpid(), i)) time.sleep(2) print("ç»“æœ: &#123;&#125;".format(8 ** 20)) return True # ç”¨äºæ¼”ç¤ºpoolé€‚ç”¨äºæœ‰è¿”å›å€¼if __name__=='__main__': print("CPUå†…æ ¸æ•°:&#123;&#125;".format(cpu_count())) # 4 print('å½“å‰æ¯è¿›ç¨‹: &#123;&#125;'.format(os.getpid())) start = time.time() p = Pool(4) results = [] for i in range(5): # p.apply_async(long_time_task, args=(i,)) results.append(p.apply_async(long_time_task, args=(i,))) print('ç­‰å¾…æ‰€æœ‰å­è¿›ç¨‹å®Œæˆã€‚') p.close() p.join() end = time.time() print("æ€»å…±ç”¨æ—¶&#123;&#125;ç§’".format((send - start))) # æŸ¥çœ‹è¿”å›å€¼ for res in results: print(res.get()) apply_async(func, args=(), kwds={}, callback=None)ï¼šå‘è¿›ç¨‹æ± æäº¤éœ€è¦æ‰§è¡Œçš„å‡½æ•°åŠå‚æ•°ï¼Œå„ä¸ªè¿›ç¨‹é‡‡ç”¨éé˜»å¡ï¼ˆå¼‚æ­¥ï¼‰çš„è°ƒç”¨æ–¹å¼ï¼Œå³æ¯ä¸ªå­è¿›ç¨‹åªç®¡è¿è¡Œè‡ªå·±çš„ï¼Œä¸ç®¡å…¶å®ƒè¿›ç¨‹æ˜¯å¦å·²ç»å®Œæˆã€‚ close()ï¼šå…³é—­è¿›ç¨‹æ± ï¼ˆpoolï¼‰ï¼Œä¸å†æ¥å—æ–°çš„ä»»åŠ¡ã€‚ join()ï¼šä¸»è¿›ç¨‹é˜»å¡ç­‰å¾…å­è¿›ç¨‹çš„é€€å‡ºï¼Œ è°ƒç”¨join()ä¹‹å‰å¿…é¡»å…ˆè°ƒç”¨close()æˆ–terminate()æ–¹æ³•ï¼Œä½¿å…¶ä¸å†æ¥å—æ–°çš„Processã€‚ å¤šçº¿ç¨‹threading pythonçš„å¤šçº¿ç¨‹æ˜¯ä¼ªå¤šçº¿ç¨‹ï¼Œå› ä¸ºä¸»è¿›ç¨‹åªæœ‰ä¸€ä¸ªï¼Œæ‰€ä»¥åªç”¨äº†å•æ ¸ï¼Œåªæ˜¯é€šè¿‡ç¢ç‰‡åŒ–è¿›ç¨‹ã€è°ƒåº¦ã€å…¨å±€é”ç­‰æ“ä½œï¼Œcpuåˆ©ç”¨ç‡æå‡äº† æ‰€ä»¥æˆ‘æƒ³å¹¶è¡Œå¤„ç†ç™¾ä¸‡é‡çº§çš„æ•°æ®å…¥åº“æ“ä½œæ—¶ï¼Œå¤šè¿›ç¨‹çš„æ•ˆç‡æ˜æ˜¾é«˜äºå¤šçº¿ç¨‹ ã€é—®é¢˜ã€‘ä»æˆ‘è§‚å¯Ÿä¸Šçœ‹å¤šçº¿ç¨‹åŸºæœ¬å°±æ˜¯ä¸²è¡Œï¼Ÿï¼Ÿ ğŸŒ°threading 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import threadingimport timedef long_time_task(): print('å½“å­çº¿ç¨‹: &#123;&#125;'.format(threading.current_thread().name)) time.sleep(2) print("ç»“æœ: &#123;&#125;".format(8 ** 20))if __name__=='__main__': start = time.time() print('è¿™æ˜¯ä¸»çº¿ç¨‹ï¼š&#123;&#125;'.format(threading.current_thread().name)) for i in range(5): t = threading.Thread(target=long_time_task, args=()) t.setDaemon(True) t.start() t.join() end = time.time() print("æ€»å…±ç”¨æ—¶&#123;&#125;ç§’".format((end - start))) # ç»§æ‰¿&amp;æœ‰è¿”å›å€¼çš„å†™æ³•def long_time_task(i): time.sleep(2) return 8**20class MyThread(threading.Thread): def __init__(self, func, args , name='', ): threading.Thread.__init__(self) self.func = func self.args = args self.name = name self.result = None def run(self): print('å¼€å§‹å­è¿›ç¨‹&#123;&#125;'.format(self.name)) self.result = self.func(self.args[0],) print("ç»“æœ: &#123;&#125;".format(self.result)) print('ç»“æŸå­è¿›ç¨‹&#123;&#125;'.format(self.name)) def get_result(self): threading.Thread.join(self) # ç­‰å¾…çº¿ç¨‹æ‰§è¡Œå®Œæ¯• return self.resultif __name__=='__main__': start = time.time() threads = [] for i in range(1, 3): t = MyThread(long_time_task, (i,), str(i)) threads.append(t) for t in threads: t.start() for t in threads: t.join() end = time.time() print("æ€»å…±ç”¨æ—¶&#123;&#125;ç§’".format((end - start))) joinæ–¹æ³•ï¼šç­‰å¾…æ‰€æœ‰è¿›ç¨‹æ‰§è¡Œå®Œï¼Œä¸»è¿›ç¨‹å†æ‰§è¡Œå®Œ setDaemon(True)ï¼šä¸»çº¿ç¨‹æ‰§è¡Œå®Œå°±é€€å‡º]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IoU]]></title>
    <url>%2F2020%2F08%2F03%2FIoU%2F</url>
    <content type="text"><![CDATA[reference: https://bbs.cvmart.net/articles/1396 IoU IoU = Intersection / Union $Loss_{IoU} = 1 - IoU$ [0,1] æ— æ³•ç›´æ¥ä¼˜åŒ–æ²¡æœ‰é‡å çš„éƒ¨åˆ†ï¼šå¦‚æœä¸¤ä¸ªæ¡†æ²¡æœ‰äº¤é›†ï¼ŒIoU=0ï¼Œæ²¡æœ‰æ¢¯åº¦å›ä¼ ï¼Œæ— æ³•è¿›è¡Œå­¦ä¹ è®­ç»ƒ å°ºåº¦ä¸æ•æ„Ÿ æ— æ³•ç²¾ç¡®çš„åæ˜ ä¸¤è€…çš„é‡åˆè´¨é‡ GIoU(Generalized Intersection over Union) $GIoU = IoU - \frac{|A_c - U|}{|A_c|}$ï¼Œ$A_c$æ˜¯åŒ…å«ä¸¤ä¸ªæ¡†çš„æœ€å°å¤–æ¥æ¡† $Loss_{GIoU} = 1 - GIoU$ GIoUå€¾å‘äºå…ˆå¢å¤§bboxçš„å¤§å°æ¥å¢å¤§ä¸GTçš„äº¤é›†ï¼Œç„¶åé€šè¿‡IoUé¡¹å¼•å¯¼æœ€å¤§åŒ–bboxçš„é‡å åŒºåŸŸ [-1,1] èƒ½å¤Ÿå…³æ³¨åˆ°éé‡åˆåŒºåŸŸ å°ºåº¦ä¸æ•æ„Ÿ ä¸¤ä¸ªæ¡†ä¸ºåŒ…å«å…³ç³»æ—¶ï¼Œé€€åŒ–ä¸ºIoU å¦‚æœä¹‹é—´ç”¨æ¥æ›¿æ¢mseï¼Œå‰æœŸæ”¶æ•›ä¼šæ¯”è¾ƒæ…¢ ä¸€èˆ¬åœ°ï¼ŒGIoU lossä¸èƒ½å¾ˆå¥½åœ°æ”¶æ•›SOTAç®—æ³•ï¼Œåè€Œé€ æˆä¸å¥½çš„ç»“æœ DIoU (Distance-IoU) $DIoU = IoU - \frac{d^2}{c^2}$ï¼Œdæ˜¯ä¸¤ä¸ªä¸­å¿ƒç‚¹é—´çš„æ¬§å¼è·ç¦»ï¼Œcæ˜¯ä¸¤ä¸ªæ¡†çš„æœ€å°å¤–æ¥æ¡†çš„å¯¹è§’çº¿è·ç¦» $Loss_{DIoU} = 1 - DIoU$ * ç›´æ¥æœ€å°åŒ–ä¸¤ä¸ªç›®æ ‡æ¡†çš„è·ç¦»ï¼Œæ”¶æ•›å¿«å¾—å¤š * èƒ½å¤Ÿå…³æ³¨åˆ°éé‡åˆåŒºåŸŸ * å¯¹äºåŒ…å«å…³ç³»çš„ä¸¤ä¸ªæ¡†ï¼Œä»æ—§æœ‰è·ç¦»æŸå¤±ï¼Œä¸ä¼šé€€åŒ–ä¸ºIoU * å¯ä»¥æ›¿æ¢NMSä¸­çš„IoUï¼šåŸå§‹çš„IoUä»…è€ƒè™‘äº†é‡å åŒºåŸŸï¼Œå¯¹åŒ…å«çš„æƒ…å†µæ²¡æœ‰å¾ˆå¥½çš„å¤„ç† $$ score = score\text{ if }IoU - dis(box_{max}, box)&gt;\epsilon \text{, else } 0 $$ * æ²¡æœ‰è€ƒè™‘å½¢çŠ¶ï¼ˆé•¿å®½æ¯”ï¼‰ CIoU (Complete-IoU) $CIoU = IoU - \frac{d^2}{c^2}-av$ï¼Œåœ¨DIoUçš„åŸºç¡€ä¸Šæ–°å¢äº†æƒ©ç½šé¡¹avï¼Œaæ˜¯æƒé‡ç³»æ•°ï¼Œvç”¨æ¥è¯„ä»·é•¿å®½æ¯”ï¼š v = \frac{4}{\pi ^ 2}(arctan\frac{w^{gt}}{h^{gt}}-arctan\frac{w}{h})^2\\ a = \frac{v}{1-IoU+v} $Loss_{CIoU} = 1 - CIoU$ vçš„æ¢¯åº¦ä¸­æœ‰$\frac{1}{w^2+h^2}$ï¼Œé•¿å®½åœ¨[0,1]ä¹‹é—´ï¼Œå¯èƒ½å¾ˆå°ï¼Œä¼šå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸ï¼Œç”¨çš„æ—¶å€™ clampä¸€ä¸‹ä¸Šä¸‹é™ åˆ†æ¯ä¸­çš„$w^2+h^2$æ›¿æ¢æˆ1 \frac{\partial v }{\partial w} = \frac{8}{\pi ^ 2}(arctan\frac{w^{gt}}{h^{gt}}-arctan\frac{w}{h})\frac{h}{w^2+h^2}\\ \frac{\partial v }{\partial w} = \frac{8}{\pi ^ 2}(arctan\frac{w^{gt}}{h^{gt}}-arctan\frac{w}{h})\frac{w}{w^2+h^2}]]></content>
      <tags>
        <tag>ç›®æ ‡æ£€æµ‹ï¼Œloss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YOLOACT]]></title>
    <url>%2F2020%2F07%2F17%2FYOLOACT%2F</url>
    <content type="text"><![CDATA[[YOLACT] Real-time Instance Segmentationï¼š33 FPS/30 mAP [YOLACT++] Better Real-time Instance Segmentationï¼š33.5 FPS/34.1 mAP YOLACT: Real-time Instance Segmentation åŠ¨æœº create a real-time instance segmentation base on fast, one-stage detection model forgoes an explicit localization step (e.g., feature repooling) doesnâ€™t depend on repooling (RoI Pooling) produces very high-quality masks set two parallel subtasks prototypesâ€”â€”conv mask coefficientsâ€”â€”fc ä¹‹åå°†æ¨¡æ¿maskå’Œå®ä¾‹maskç³»æ•°è¿›è¡Œçº¿æ€§ç»„åˆæ¥è·å¾—å®ä¾‹çš„mask â€˜prototypesâ€™: vocabulary fully-convolutional localization is still translation variant Fast NMS è®ºç‚¹ State-of-the-art approaches to instance segmentation like Mask R- CNN and FCIS directly build off of advances in object detection like Faster R-CNNand R-FCN focus primarily on performance over speed these methods â€œre-poolâ€ features in some bounding box region inherently sequential therefore difficult to accelerate One-stage instance segmentation methods generate position sensitive maps still require repooling or other non-trivial computations prototypes related works use prototypes to represent features (Bag of Feature) we use them to assemble masks for instance segmentation we learn prototypes that are specific to each image, rather than global prototypes shared across the entire dataset Bag of Feature BOFå‡è®¾å›¾åƒç›¸å½“äºä¸€ä¸ªæ–‡æœ¬ï¼Œå›¾åƒä¸­çš„ä¸åŒå±€éƒ¨åŒºåŸŸæˆ–ç‰¹å¾å¯ä»¥çœ‹ä½œæ˜¯æ„æˆå›¾åƒçš„è¯æ±‡(codebook) æ‰€æœ‰çš„æ ·æœ¬å…±äº«ä¸€ä»½è¯æ±‡æœ¬ï¼Œé’ˆå¯¹æ¯ä¸ªå›¾åƒï¼Œç»Ÿè®¡æ¯ä¸ªå•è¯çš„é¢‘æ¬¡ï¼Œå³å¯å¾—åˆ°å›¾ç‰‡çš„ç‰¹å¾å‘é‡ æ–¹æ³• parallel tasks The first branch uses an FCN to produce a set of image-sized â€œprototype masksâ€ that do not depend on any one instance. The second adds an extra head to the object detection branch to predict a vector of â€œmask coefficientsâ€ for each anchor that encode an instanceâ€™s rep- resentation in the prototype space. linearly combining Rationale masks are spatially coherentï¼šmashæ˜¯ç©ºé—´ç›¸å…³çš„ï¼Œç›¸é‚»åƒç´ å¾ˆå¯èƒ½æ˜¯ä¸€ç±» å·ç§¯å±‚èƒ½å¤Ÿåˆ©ç”¨åˆ°è¿™ç§ç©ºé—´ç›¸å…³æ€§ï¼Œä½†æ˜¯fcå±‚ä¸èƒ½ è€Œone-stageæ£€æµ‹å™¨çš„æ£€æµ‹å¤´é€šå¸¸æ˜¯fcå±‚ï¼Ÿï¼Ÿ making use of fc layers, which are good at producing semantic vectors and conv layers, which are good at producing spatially coherent masks Prototype åœ¨backbone feature layer P3ä¸Šæ¥ä¸€ä¸ªFCN taking protonet from deeper backbone features produces more robust masks higher resolution prototypes result in both higher quality masks and better performance on smaller objects upsampleåˆ°x4çš„å°ºåº¦to increase performance on small objects headåŒ…å«kä¸ªchannels æ¢¯åº¦å›ä¼ æ¥æºäºæœ€ç»ˆçš„final assembled maskï¼Œä¸æ˜¯å½“å‰è¿™ä¸ªå¤´ unboundedï¼šReLU or no nonlinearity We choose ReLU for more interpretable prototypes Mask Coefficients a third branch in parallel with detection heads nonlinearityï¼šè¦æœ‰æ­£è´Ÿï¼Œæ‰€ä»¥tanh Mask Assembly linear combination + sigmoid: $M=\sigma(PC^T)$ loss cls lossï¼šw=1, å’Œssdä¸€æ ·ï¼Œc+1 softmax box reg lossï¼šw=1.5, å’Œssdä¸€æ ·ï¼Œsmooth-L1 mask lossï¼šw=6.125ï¼Œ BCE crop mask evalï¼šç”¨predict boxå»crop trainï¼šç”¨gt boxå»cropï¼ŒåŒæ—¶è¿˜è¦ç»™mask lossé™¤ä»¥gt boxçš„é¢ç§¯ï¼Œto preserve small objects Emergent Behavior ä¸cropä¹Ÿèƒ½åˆ†å‰²ä¸­å¤§ç›®æ ‡ï¼š YOLACT learns how to localize instances on its own via different activations in its prototypes è€Œä¸æ˜¯é å®šä½ç»“æœ translation variant the consistent rim of padding in modern FCNs like ResNet gives the network the ability to tell how far away from the imageâ€™s edge a pixel isï¼Œæ‰€ä»¥ç”¨ä¸€å¼ çº¯è‰²çš„å›¾èƒ½å¤Ÿçœ‹å‡ºkernelå®é™…highlightçš„æ˜¯å“ªéƒ¨åˆ†ç‰¹å¾ åŒä¸€ç§kernelï¼ŒåŒä¸€ç§äº”è§’æ˜Ÿï¼Œåœ¨ç”»é¢ä¸åŒä½ç½®ï¼Œå¯¹åº”çš„å“åº”å€¼æ˜¯ä¸åŒçš„ï¼Œè¯´æ˜fcnæ˜¯èƒ½å¤Ÿæå–ç‰©ä½“ä½ç½®è¿™æ ·çš„è¯­ä¹‰ä¿¡æ¯çš„ prototypes are compressibleï¼š å¢åŠ æ¨¡ç‰ˆæ•°ç›®åè€Œä¸å¤ªæœ‰æ•ˆï¼Œbecause predicting coefficients is difficultï¼Œ the network has to play a balancing act to produce the right coef- ficients, and adding more prototypes makes this harder, We choose 32 for its mix of performance and speed Network speed as well as feature richness backboneå‚è€ƒRetinaNetï¼ŒResNet-101 + FPN 550x550 inputï¼Œresize å»æ‰P2ï¼Œadd P6&amp;P7 3 anchors per levelï¼Œ[1, 1/2, 2] P3çš„anchorå°ºå¯¸æ˜¯24x24ï¼Œæ¥ä¸‹æ¥æ¯å±‚double the scale æ£€æµ‹å¤´ï¼šshared conv+parallel conv OHEM single GPUï¼šbatch size 8 using ImageNet weightsï¼Œno extra bn layers Fast NMS æ„é€ cxnxnçš„çŸ©é˜µï¼Œcä»£è¡¨æ¯ä¸ªclass ç„¶åææˆä¸Šä¸‰è§’ï¼Œæ±‚column-wise max å†IoU threshold 15.0 ms faster with a performance loss of 0.3 mAP Semantic Segmentation Loss using modules not executed at test time P3ä¸Š1x1 convï¼Œsigmoid and c channels w=1 +0.4 mAP boost YOLACT++: Better Real-time Instance Segmentation]]></content>
      <tags>
        <tag>å®ä¾‹åˆ†å‰²</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cornerNet]]></title>
    <url>%2F2020%2F07%2F17%2FcornerNet%2F</url>
    <content type="text"><![CDATA[CornerNet: Detecting Objects as Paired Keypoints åŠ¨æœº corner formulation top-left corner bottom-right corner anchor-free corner pooling no multi-scale è®ºç‚¹ anchor box drawbacks huge set of anchors boxes to ensure sufficient overlapï¼Œcause huge imbalance hyperparameters and design choices cornerNet detect and group heatmap to predict corners ä»æ•°å­¦è¡¨è¾¾ä¸Šçœ‹ï¼Œå…¨å›¾whä¸ªtl cornerï¼Œwhä¸ªbt cornerï¼Œå¯ä»¥è¡¨è¾¾wwhhä¸ªæ¡† anchor-basedï¼Œå…¨å›¾whä¸ªä¸­å¿ƒç‚¹ï¼Œ9ä¸ªanchor sizeï¼Œåªèƒ½è¡¨è¾¾æœ‰é™çš„æ¡†ï¼Œä¸”å¯èƒ½matchä¸ä¸Š embeddings to group pairs of corners corner pooling better localize corners which are usually out of the foreground modifid hourglass architecture add our novel variant of focal loss æ–¹æ³• two prediction modules heatmaps C channels, C for number of categories binary mask each corner has only one ground-truth positive penalty the neighbored negatives within a radius that still hold high iou (0.3 iou) determine the radius penalty reduction $=e^{-\frac{x^2+y^2}{2\sigma^2}}$ variant focal loss L_{det} = \frac{-1}{N} \sum^C \sum^H \sum^W \begin{cases} (1-p_{i,j})^\alpha log(p_{i,j}), \ \ if y_{ij}=1\\ (1-y_{ij})^\beta (p_{i,j})^\alpha log(1-p_{i,j}), \ \ otherwise \end{cases} $\alpha=2, \beta=4$ N is the number of gts embeddings associative embedding use 1-dimension embedding pull and push loss on gt positives $L_{pull} = \frac{1}{N} \sum^N [(e_{tk}-e_k)^2 + (e_{bk}-e_k)^2]$ $L_{push} = \frac{1}{N(N-1)} \sum_j^N\sum_{k\neq j}^N max(0, \Delta -|e_k-e_j|)$ $e_k$ is the average of $e_{tk}$ and $e{bk}$ $\Delta$ = 1 offsets ä»heatmap resolution remappingåˆ°origin resolutionå­˜åœ¨ç²¾åº¦æŸå¤± o_k = ï¼ˆ\frac{x_k}{n} - \lfloor \frac{x_k}{n} \rfloorï¼Œ \frac{y_k}{n} - \lfloor \frac{y_k}{n} \rfloorï¼‰ greatly affect the IoU of small bounding boxes shared among all categories smooth L1 loss on gt positives $$ L_{off} = \frac{1}{N} \sum^N SmoothL1(o_k, \hat o_k) $$ corner pooling top-left pooling layerï¼š * ä»å½“å‰ç‚¹(i,j)å¼€å§‹ï¼Œ * å‘ä¸‹elementwise maxæ‰€æœ‰feature vecorï¼Œå¾—åˆ°$t_{i,j}$ * å‘å³elementwise maxæ‰€æœ‰feature vecorï¼Œå¾—åˆ°$l_{i,j}$ * æœ€åä¸¤ä¸ªvectorç›¸åŠ  bottom-right cornerï¼šå‘å·¦å‘ä¸Š Hourglass Network hourglass modules series of convolution and max pooling layers series of upsampling and convolution layers skip layers multiple hourglass modules stackedï¼šreprocess the features to capture higher-level information intermediate supervision å¸¸è§„çš„ä¸­ç»§ç›‘ç£ï¼š ä¸‹ä¸€çº§hourglass moduleçš„è¾“å…¥åŒ…æ‹¬ä¸‰ä¸ªéƒ¨åˆ† å‰ä¸€çº§è¾“å…¥ å‰ä¸€çº§è¾“å‡º ä¸­ç»§ç›‘ç£çš„è¾“å‡º æœ¬æ–‡ä½¿ç”¨äº†ä¸­ç»§ç›‘ç£ï¼Œä½†æ˜¯æ²¡æŠŠè¿™ä¸ªç»“æœåŠ å›å» hourglass2 inputï¼š1x1 conv-BN to both input and output of hourglass1 + add + relu Our backbone 2 hourglasses 5 times downsamp with channels [256,384,384,384,512] use stride2 conv instead of max-pooling upsampï¼š2 residual modules + nearest neighbor upsampling skip connection: 2 residual modulesï¼Œadd mid connection: 4 residual modules stem: 7x7 stride2, ch128 + residual stride2, ch256 hourglass2 inputï¼š1x1 conv-BN to both input and output of hourglass1 + add + relu å®éªŒ training details randomly initialized, no pretrained biasï¼šset the biases in the convolution layers that predict the corner heatmaps inputï¼š511x511 outputï¼š128x128 apply PCA to the input image full lossï¼š$L = L_{det} + \alpha L_{pull} + \beta L_{push} + \gamma L_{off}$ é…å¯¹lossï¼š$\alpha=\beta=0.1$ offset lossï¼š$\gamma=1$ batch size = 49 = 4+5x9 test details NMSï¼š3x3 max pooling on heatmaps pickï¼štop100 top-left corners &amp; top100 bottom-right corners filter pairsï¼š L1 distance greater than 0.5 from different categories fusionï¼šcombine the detections from the original and flipped images + soft nms Ablation Study corner pooling is especially helpful for medium and large objects penalty reduction especially benefits medium and large objects CornerNet achieves a much higher AP at 0.9 IoU than other detectorsï¼šæ›´æœ‰èƒ½åŠ›ç”Ÿæˆé«˜è´¨é‡æ¡† error analysisï¼šthe main bottleneck is detecting corners CornerNet-Lite: Efficient Keypoint-Based Object Detection åŠ¨æœº keypoint-based methods detecting and grouping accuary but with processing cost propose CornerNet-Lite CornerNet-Saccadeï¼šattention mechanism CornerNet-Squeezeï¼ša new compact backbone performance è®ºç‚¹ main drawback of cornerNet inference speed reducing the number of scales or the image resolution cause a large accuracy drop two orthogonal directions reduce the number of pixels to processï¼šCornerNet-Saccade reduce the amount of processing per pixelï¼š CornerNet-Saccade downsized attention map select a subset of crops to examine in high resolution for off-lineï¼šAP of 43.2% at 190ms per image CornerNet-Squeeze inspired by squeezeNet and mobileNet 1x1 convs bottleneck layers depth-wise separable convolution for real-timeï¼šAP of 34.4% at 30ms combined?? CornerNet-Squeeze-Saccade turns out slower and less accurate than CornerNet- Squeeze Saccadesï¼šæ‰«è§† to generate interesting crops RCNNç³»åˆ—ï¼šsingle-type &amp; single object AutoFocusï¼šadd a branchè°ƒç”¨faster-RCNNï¼Œthus multi-type &amp; mixed-objectsï¼Œæœ‰single branchæœ‰multi branch CornerNet-Saccadeï¼š single-type &amp; multi object crops can be much smaller than number of objects æ–¹æ³• CornerNet-Saccade step1ï¼šobtain possible locations downsizeï¼štwo scalesï¼Œ255 &amp; 192ï¼Œzero-padding predicts 3 attention maps small objectï¼šlonger side&lt;32 pixels medium objectï¼š32-96 large objectï¼š&gt;96 so that we can control the zoom-in factorï¼šzoom-in more for smaller objects feature mapï¼šdifferent scales from the upsampling layers attention mapï¼š3x3 conv-relu + 1x1 conv-sigmoid process locations where scores &gt; 0.3 step2ï¼šfiner detection zoom-in scalesï¼š4ï¼Œ2ï¼Œ1 for smallã€mediumã€large objects apply CornerNet-Saccade on the ROI 255x255 window centered at the location step3ï¼šNMS soft-nms remove the bounding boxes which touch the crop boundary CornerNet-Saccade uses the same network for attention maps and bounding boxes åœ¨ç¬¬ä¸€æ­¥çš„æ—¶å€™ï¼Œå¯¹ä¸€äº›å¤§ç›®æ ‡å·²ç»æœ‰äº†æ£€æµ‹æ¡† ä¹Ÿè¦zoom-inï¼ŒçŸ«æ­£ä¸€ä¸‹ efficiency regions/croped imageséƒ½æ˜¯processed in batch/parallel resize/cropæ“ä½œåœ¨GPUä¸­å®ç° suppress redundant regions using a NMS-similar policy before prediction new hourglass backbone 3 hourglass moduleï¼Œdepth 54 downsize twice before hourglass modules downsize 3 times in each moduleï¼Œwith channels [384,384,512] one residual in both encoding path &amp; skip connection mid connectionï¼šone residualï¼Œwith channels 512 CornerNet-Squeeze to replace the heavy hourglass104 use fire module to replace residuals downsizes 3 times before hourglass modules downsize 4 times in each module replace the 3x3 conv in prediction head with 1x1 conv replace the nearest neighboor upsampling with 4x4 transpose conv]]></content>
      <tags>
        <tag>ç›®æ ‡æ£€æµ‹ï¼Œanchor-free</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SOLO]]></title>
    <url>%2F2020%2F07%2F17%2FSOLO%2F</url>
    <content type="text"><![CDATA[[SOLO] SOLO: Segmenting Objects by Locationsï¼šå­—èŠ‚ï¼Œç›®å‰ç»å¤§å¤šæ•°æ–¹æ³•å®ä¾‹åˆ†å‰²çš„ç»“æ„éƒ½æ˜¯é—´æ¥å¾—åˆ°â€”â€”æ£€æµ‹æ¡†å†…è¯­ä¹‰åˆ†å‰²ï¼å…¨å›¾è¯­ä¹‰åˆ†å‰²èšç±»ï¼Œä¸»è¦åŸå› æ˜¯formulation issueï¼Œå¾ˆéš¾æŠŠå®ä¾‹åˆ†å‰²å®šä¹‰æˆä¸€ä¸ªç»“æ„åŒ–çš„é—®é¢˜ [SOLOv2] SOLOv2: Dynamic, Faster and Strongerï¼šbest 41.7% AP SOLO: Segmenting Objects by Locations åŠ¨æœº challengingï¼šarbitrary number of instances form the task into a classification-solvable problem direct &amp; end-to-end &amp; one-stage &amp; using mask annotations solely on par accuracy with Mask R-CNN outperforming recent single-shot instance segmenters è®ºç‚¹ formulating Objects in an image belong to a fixed set of semantic categoriesâ€”â€”semantic segmentation can be easily formulated as a dense per-pixel classification problem the number of instances varies existing methods æ£€æµ‹ï¼èšç±»ï¼šstep-wise and indirect ç´¯ç§¯è¯¯å·® core idea in most cases two instances in an image either have different center locations or have different object sizes locationï¼š think image as a divided grid of cells an object instance is assigned to one of the grid cells as its center location category encode center location categories as the channel axis size FPN assign objects of different sizes to different levels of feature maps SOLO converts coordinate regression into classification by discrete quantization One feat of doing so is the avoidance of heuristic coordination normalization and log-transformation typically used in detectorsã€ï¼Ÿï¼Ÿï¼Ÿä¸æ‡‚è¿™å¥è¯æƒ³è¡¨è¾¾å•¥ã€‘ æ–¹æ³• problem formulation divided grids simultaneous task category-aware prediction instance-aware mask generation category prediction predict instance for each gridï¼š$SSC$ grid sizeï¼š$S*S$ number of classesï¼š$C$ based on the assumption that each cell must belong to one individual instance C-dim vec indicates the class probability for each object instance in each grid mask prediction predict instance mask for each positive cellï¼š$HWS^2$ the channel corresponding to the location position sensitiveï¼šå› ä¸ºæ¯ä¸ªgridä¸­åˆ†å‰²çš„maskæ˜¯è¦æ˜ å°„åˆ°å¯¹åº”çš„channelçš„ï¼Œå› æ­¤æˆ‘ä»¬å¸Œæœ›ç‰¹å¾å›¾æ˜¯spatially variant è®©ç‰¹å¾å›¾spatially variantçš„æœ€ç›´æ¥åŠæ³•å°±æ˜¯åŠ ä¸€ç»´spatially variantçš„ä¿¡æ¯ inspired by CoordConvï¼šæ·»åŠ ä¸¤ä¸ªé€šé“ï¼Œnormed_xå’Œnormed_yï¼Œ[-1,1] original feature tensor $HWD$ becomes $HW(D+2)$ final results gather category prediction &amp; mask prediction NMS network backboneï¼šresnet FCNï¼š256-d headsï¼šweights are shared across different levels except for the last 1x1 conv learning positive gridï¼šfalls into a center region maskï¼šmask center $(c_x, c_y)$ï¼Œmask size $(h,w)$ center regionï¼š$(c_x,c_y,\epsilon w, \epsilon h)$ï¼Œset $\epsilon = 0.2$ lossï¼š$L = L_{cate} + \lambda L_{seg}$ cate lossï¼šfocal loss seg lossï¼šdiceï¼Œ$L_{mask} = \frac{1}{N_{pos}}\sum_k 1_{p^_{i,j}&gt;0} dice(m_k, m^_k) $ï¼Œå¸¦æ˜Ÿå·çš„æ˜¯groud truth inference use a confidence threshold of 0.1 to filter out low spacial predictions use a threshold of 0.5 to binary the soft masks select the top 500 scoring masks NMS Only one instance will be activated at each grid and one in- stance may be predicted by multiple adjacent mask channels keep top 100 å®éªŒ grid number é€‚å½“å¢åŠ æœ‰æå‡ï¼Œä¸»è¦æå‡è¿˜æ˜¯åœ¨FPN fpn äº”ä¸ªFPN pyramids å¤§ç‰¹å¾å›¾ï¼Œå°æ„Ÿå—é‡ï¼Œç”¨æ¥åˆ†é…å°ç›®æ ‡ï¼Œgridæ•°é‡è¦å¢å¤§ feature alignment åœ¨åˆ†ç±»branchï¼Œ$HW$ç‰¹å¾å›¾è¦è½¬æ¢æˆ$SS$çš„ç‰¹å¾å›¾ interpolationï¼šbilinear interpolating adaptive-poolï¼šapply a 2D adaptive max-pool region-grid- interpolationï¼šå¯¹æ¯ä¸ªcellï¼Œé‡‡æ ·å¤šä¸ªç‚¹åšåŒçº¿æ€§æ’å€¼ï¼Œç„¶åå–å¹³å‡ is no noticeable performance gap between these variants ï¼ˆå¯èƒ½å› ä¸ºæœ€ç»ˆæ˜¯åˆ†ç±»ä»»åŠ¡ head depth 4-7æœ‰æ¶¨ç‚¹ æ‰€ä»¥æœ¬æ–‡é€‰äº†7 decoupled SOLO mask branché¢„æµ‹çš„channelæ•°æ˜¯$S^2$ï¼Œå…¶ä¸­å¤§éƒ¨åˆ†channelå…¶å®æ˜¯æ²¡æœ‰è´¡çŒ®çš„ï¼Œç©ºå å†…å­˜ prediction is somewhat redundant as in most cases the objects are located sparsely in the image element-wise multiplication å®éªŒä¸‹æ¥ achieves the same performance efficient and equivalent variant SOLOv2: Dynamic, Faster and Stronger åŠ¨æœº take one step further on the mask head dynamically learning the mask head decoupled into mask kernel branch and mask feature branch propose Matrix NMS faster &amp; better results try object detection and panoptic segmentation è®ºç‚¹ SOLO develop pure instance segmentation instance segmentation requires instance-level and pixel-level predictions simultaneously most existing instance segmentation methods build on the top of bounding boxes SOLO develop pure instance segmentation SOLOv2 improve SOLO mask learningï¼šdynamic scheme mask NMSï¼šparallel matrix operationsï¼Œoutperforms Fast NMS Dynamic Convolutions STNï¼šadaptively transform feature maps conditioned on the input Deformable Convolutional Networksï¼šlearn location æ–¹æ³• revisit SOLOv1 redundant mask prediction decouple dynamicï¼šdynamically pick the valid ones from predicted $s^2$ classifiers and perform the convolution SOLOv2 dynamic mask segmentation head mask kernel branch mask feature branch mask kernel branch prediction headsï¼š4 convs + 1 final convï¼Œshared across scale no activation on the output concat normalized coordinates in two additional input channels at start ouputs D-dims kernel weights for each gridï¼še.g. for 3x3 conv with E input channels, outputs $SS9E$ mask feature branch predict instance-aware featureï¼š$F \in R^{HWE}$ unified and high-resolution mask featureï¼šåªè¾“å‡ºä¸€ä¸ªå°ºåº¦çš„ç‰¹å¾å›¾ï¼Œencoded x32 feature with coordinates info we feed normalized pixel coordinates to the deepest FPN level (at 1/32 scale) repeated ã€3x3 conv, group norm, ReLU, 2x bilinear upsamplingã€‘ element-wise sum last layerï¼š1x1 conv, group norm, ReLU instance mask mask feature branch conved by the mask kernel branchï¼šfinal conv $HWS^2$ mask NMS train lossï¼š$L = L_{cate} + \lambda L_{seg}$ cate lossï¼šfocal loss seg lossï¼šdiceï¼Œ$L_{mask} = \frac{1}{N_{pos}}\sum_k 1_{p^_{i,j}&gt;0} dice(m_k, m^_k) $ï¼Œå¸¦æ˜Ÿå·çš„æ˜¯groud truth inference category scoreï¼šfirst use a confidence threshold of 0.1 to filter out predictions with low confidence mask branchï¼šrun convolution based on the filtered category map sigmoid use a threshold of 0.5 to convert predicted soft masks to binary masks Matrix NMS Matrix NMS decremented functions linearï¼š$f(iou_{i,j}=1-iou_{i,j})$ gaussianï¼š$f(iou_{i,j}=exp(-\frac{iou_{i,j}^2}{\sigma})$ the most overlapped prediction for $m_i$ï¼šmax iou $f(iou_{*,i}) = min_{s_k}f(iou_{k,i})$ decay factor $decay_i = min \frac{f(iou_{i,j})}{f(iou_{*,i})}$]]></content>
      <tags>
        <tag>å®ä¾‹åˆ†å‰²</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[polarMask]]></title>
    <url>%2F2020%2F06%2F29%2FpolarMask%2F</url>
    <content type="text"><![CDATA[PolarMask: Single Shot Instance Segmentation with Polar Representation åŠ¨æœº instance segmentation anchor-free single-shot modified on FCOS è®ºç‚¹ two-stage methods FCIS, Mask R-CNN bounding box detection then semantic segmentation within each box single-shot method formulate the task as instance center classification and dense distance regression in a polar coordinate FCOS can be regarded as a special case that the contours has only 4 directions this paper two parallel taskï¼š instance center classification dense distance regression Polar IoU Loss can largely ease the optimization and considerably improve the accuary Polar Centerness improves the original idea of â€œCentrenessâ€ in FCOS, leading to further performance boost æ–¹æ³• architecture back &amp; fpn are the same as FCOS model the instance mask as one center and n rays conclude that mass-center is more advantageous than box center the angle interval is pre-fixed, thus only the length of the rays is to be regressed positive samplesï¼šfalls into 1.5xstrides of the area around the gt mass-centerï¼Œthat is 9-16 pixels around gt grid distance regression å¦‚æœä¸€æ¡å°„çº¿ä¸Šå­˜åœ¨å¤šä¸ªäº¤ç‚¹ï¼Œå–æœ€é•¿çš„ å¦‚æœä¸€æ¡å°„çº¿ä¸Šæ²¡æœ‰äº¤ç‚¹ï¼Œå–æœ€å°å€¼$\epsilon=10^{-6}$ potential issuse of the mask regression branch dense regression task with such as 36 rays, may cause imbalance between regression loss and classification loss n rays are relevant and should be trained as a whole rather than a set of independent valuesâ€”-&gt;iou loss inference multiply center-ness with classification to obtain final confidence scores, conf thresh=0.05 take top-1k predictions per fpn level use the smallest bounding boxes to run NMS, nms thresh=0.5 polar centerness to suppress low quality detected centers $polar\ centerness=\sqrt{\frac{min(\{d_1,d_2, â€¦, d_n\})}{max(\{d_1,d_2, â€¦, d_n\})}}$ $d_{min}$å’Œ$d_{max}$è¶Šæ¥è¿‘ï¼Œè¯´æ˜ä¸­å¿ƒç‚¹è´¨é‡è¶Šå¥½ Experiments show that Polar Centerness improves accuracy especially under stricter localization metrics, such as $AP_{75}$ polar IoU loss polar IoUï¼š$IoU=lim_{N\to\inf}\frac{\sum_{i=1}^N\frac{1}{2} d_{min}^2 \Delta \theta}{\sum_{i=1}^N\frac{1}{2} d_{max}^2 \Delta \theta}$ empirically observe that å»æ‰å¹³æ–¹é¡¹æ•ˆæœæ›´å¥½ï¼š$polar\ IoU=\frac{\sum_{i=1}^n d_{min}}{\sum_{i=1}^n d_{max}}$ polar iou lossï¼šbce of polar IoUï¼Œ$-log(\frac{\sum_{i=1}^n d_{min}}{\sum_{i=1}^n d_{max}})$ advantage differentiable, enable bp regards the regression targets as a whole keep balance with classification loss]]></content>
      <tags>
        <tag>å®ä¾‹åˆ†å‰²ï¼Œæåæ ‡ï¼Œone-stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FCOS]]></title>
    <url>%2F2020%2F06%2F23%2FFCOS%2F</url>
    <content type="text"><![CDATA[FCOS: Fully Convolutional One-Stage Object Detection åŠ¨æœº anchor free proposal free avoids the complicated computation related to anchor boxes calculating overlapping during training avoid all hyper-parameters related to anchor boxes size &amp; shape positiveï¼ignoredï¼negative leverage as many foreground samples as possible è®ºç‚¹ anchor-based detectors detection performance is sensitive to anchor settings encounter difficulties in cases with large shape variations hamper the generalization ability of detectors dense proposeï¼šthe excessive number of negative samples aggravates the imbalance involve complicated computationï¼šsuch as calculating the IoU with gt boxes FCN-based detector predict a 4D vector plus a class category at each spatial location on a level of feature maps do not work well when applied to overlapped bounding boxes with FPN this ambiguity can be largely eliminated anchor-free detector yolov1ï¼šonly the points near the center are usedï¼Œlow recall CornerNetï¼šcomplicated post-processing to match the pairs of corners DenseBoxï¼šdifficulty in handling overlapping bounding boxes this methos use FPN to deal with ambiguity dense predictï¼šuse all points in a ground truth bounding box to predict the bounding box introduce â€œcenter-nessâ€ branch to predict the deviation of a pixel to the center of its corresponding bounding box can be used as a RPN in two-stage detectors and can achieve significantly better performance æ–¹æ³• ground truth boxesï¼Œ$B_i=(x_0, y_0, x_1, y_1, c)$ï¼Œcorners + cls anchor-freeï¼šeach location (x,y)ï¼Œmap into abs input image (xs+[s/2], ys+[s/2]) positive sampleï¼šif a location falls into any ground-truth box ambiguous sampleï¼šlocation falls into multiple gt boxesï¼Œchoose the box with minimal area regression targetï¼šl t r b distanceï¼Œlocation to the four sides cls branch C binary classifiers C-dims vector p focal loss $\frac{1}{N_{pos}} \sum_{x,y}L_{cls}(p_{x,y}, c_{x,y}^*)$ calculate on both positive/negative samples box reg branch 4-dims vector t IOU loss $\frac{1}{N_{pos}} \sum_{x,y}1_{\{c_{x,y}^&gt;0\}}L_{reg}(t_{x,y}, t_{x,y}^)$ calculate on positive samples inference choose the location with p &gt; 0.05 as positive samples two possible issues large stride makes BPR low, which is actually not a problem in FCOS overlaps gt boxes cause ambiguity, which can be greatly resolved with multi-level prediction FPN P3, P4, P5ï¼š1x1 conv from C3, C4, C5, top-down connections P6, P7: stride2 conv from P5, P6 limit the bbox regression for each level $m_i$ï¼šmaximum distance for each level if a locationâ€™s gt bbox satifiesï¼š$max(l^,t^,r^,b^)&gt;m_i$ or $max(l^,t^,r^,b^)&lt;m_{i-1}$ï¼Œit is set as a negative sampleï¼Œnot regress at current level objects with different sizes are assigned to different feature levelsï¼šlargely alleviateä¸€éƒ¨åˆ†box overlappingé—®é¢˜ for other overlapping casesï¼šsimply choose the gt box with minimal area sharing heads between different feature levels to regress different size rangeï¼šuse $exp(s_ix)$ trainable scalar $s_i$ slightly improve center-ness low-quality predicted bounding boxes are produced by locations far away from the center of an object predict the â€œcenter-nessâ€ of a location normalized distance centerness^* = \sqrt {\frac{min(l^*,r^*)}{max(l^*,r^*)}* \frac{min(t^*,b^*)}{max(t^*,b^*)}} sqrt to slow down the decay [0,1] use bce loss when inference center-ness is mutiplied with the class scoreï¼šcan down-weight the scores of bounding boxes far from the center of an object, then filtered out by NMS an alternative of the center-nessï¼šuse of only the central portion of ground-truth bounding box as positive samplesï¼Œå®éªŒè¯æ˜ä¸¤ç§æ–¹æ³•ç»“åˆæ•ˆæœæœ€å¥½ architecture two minor differences from the standard RetinaNet use Group Normalization in the newly added convolutional layers except for the last prediction layers use P5 instead of C5 to produce P6&amp;P7 â€‹]]></content>
      <tags>
        <tag>ç›®æ ‡æ£€æµ‹ï¼Œå…¨å·ç§¯ï¼Œone-stageï¼Œcenternessï¼Œanchor free</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FCIS]]></title>
    <url>%2F2020%2F06%2F22%2FFCIS%2F</url>
    <content type="text"><![CDATA[Fully Convolutional Instance-aware Semantic Segmentation åŠ¨æœº instance segmentationï¼š å®ä¾‹åˆ†å‰²æ¯”èµ·æ£€æµ‹ï¼Œéœ€è¦å¾—åˆ°ç›®æ ‡æ›´ç²¾ç¡®çš„è¾¹ç•Œä¿¡æ¯ æ¯”èµ·è¯­ä¹‰åˆ†å‰²ï¼Œéœ€è¦åŒºåˆ†ä¸åŒçš„ç‰©ä½“ detects and segments simultanously FCN + instance mask proposal è®ºç‚¹ FCNs do not work for the instance-aware semantic segmentation task convolution is translation invariantï¼šæƒå€¼å…±äº«ï¼Œä¸€ä¸ªåƒç´ å€¼å¯¹åº”ä¸€ä¸ªå“åº”å€¼ï¼Œä¸ä½ç½®æ— å…³ instance segmentation operates on region level the same pixel can have different semantics in different regions Certain translation-variant property is required prevalent method step1: an FCN is applied on the whole image to generate shared feature maps step2: a pooling layer warps each region of interest into fixed-size per-ROI feature maps step3: use fc layers to convert the per-ROI feature maps to per-ROI masks the translation-variant property is introduced in the fc layer(s) in the last step drawbacks the ROI pooling step losses spatial details the fc layers over-parametrize the task InstanceFCN position-sensitive score maps sliding windows sub-tasks are separated and the solution is not end-to-end blind to the object categoriesï¼šå‰èƒŒæ™¯åˆ†å‰² In this work extends InstanceFCN end-to-end fully convolutional operates on box proposals instead of sliding windows per-ROI computation does not involve any warping or resizing operations æ–¹æ³• position-sensitive score map FCN predict a single score map predict each pixelâ€™s likelihood score of belonging to each category at instance level the same pixel can be foreground on one object but background on another a single score map per-category is insufficient to distinguish these two cases a fully convolutional solution for instance mask proposal k x k evenly partitioned cells of object thus obtain k x k position-sensitive score maps Each score represents å½“å‰åƒç´ åœ¨å½“å‰ä½ç½®ï¼ˆscore mapåœ¨cellsä¸­çš„ä½ç½®ï¼‰ä¸Šå±äºæŸä¸ªç‰©ä½“å®ä¾‹çš„ä¼¼ç„¶å¾—åˆ† assembling (copy-paste) jointly and simultaneously The same set of score maps are shared for the two sub-tasks For each pixel in a ROI, there are two tasks: detectionï¼šwhether it belongs to an object bounding box segmentationï¼šwhether it is inside an object instanceâ€™s boundary separateï¼štwo 1x1 conv heads fuseï¼šinside and outside high inside score and low outside scoreï¼šdetection+, segmentation+ low inside score and high outside scoreï¼šdetection+, segmentation- low inside score and low outside scoreï¼šdetection-, segmentation- detection score average pooling over all pixelsâ€˜ likelihoods for each class max(detection score) represent the object segmentation softmax(inside, outside) for each pixel to distinguish fgï¼bg All the per-ROI components are implemented through convs local weight sharing propertyï¼ša regularization mechanism without involving any feature warping, resizing or fc layers the per-ROI computation cost is negligible architecture ResNet back produce features with 2048 channels a 1x1 conv reduces the dimension to 1024 x16 output strideï¼šconv5 stride is decreased from 2 to 1, the dilation is increased from 1 to 2 head1ï¼šjoint det conf &amp; segmentation 1x1 convï¼Œgenerates $2k^2(C+1)$ score maps 2 for insideï¼outside $k^2$ for $k^2$ä¸ªposition $(C+1)$ for fgï¼bg head2ï¼šbbox regression 1x1 convï¼Œ$4k^2$ channels RPN to generate ROIs inference 300 ROIs pass through the bbox regression obtaining another 300 ROIs pass through joint head to obtain detection score&amp;fg mask for all categories mask votingï¼šæ¯ä¸ªROI (with max det score) åªåŒ…å«å½“å‰ç±»åˆ«çš„å‰æ™¯ï¼Œè¿˜è¦è¡¥ä¸Šæ¡†å†…å…¶ä»–ç±»åˆ«èƒŒæ™¯ for current ROI, find all the ROIs (from the 600) with IoU scores higher than 0.5 their fg masks are averaged per-pixel and weighted by the classification score training ROI positiveï¼negativeï¼šIoU&gt;0.5 loss softmax detection loss over C+1 categories softmax segmentation loss over the gt fg mask, on positive ROIs bbox regression loss, , on positive ROIs OHEMï¼šamong the 300 proposed ROIs on one image, 128 ROIs with the highest losses are selected to back-propagate their error gradients RPNï¼š 9 anchors sharing feature between FCIS and RPN å®éªŒ metricï¼šmAP FCIS (translation invariant)ï¼š set k=1ï¼Œachieve the worst mAP indicating the position sensitive score map is vital for this method back 50-101ï¼šincrease 101-152ï¼šsaturate tricks * r]]></content>
      <tags>
        <tag>å®ä¾‹åˆ†å‰²ï¼Œå…¨å·ç§¯ï¼Œå¸¦ä½ç½®ä¿¡æ¯çš„scoremap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å¤©æ± è„ŠæŸ±MRI]]></title>
    <url>%2F2020%2F06%2F11%2F%E5%A4%A9%E6%B1%A0%E8%84%8A%E6%9F%B1MRI%2F</url>
    <content type="text"><![CDATA[MRI T1åŠ æƒç›¸ä¸Šé¢å¯†åº¦é«˜çš„éª¨å¤´ä¼šæ¯”è¾ƒäº®(å°±æ˜¯é«˜ä¿¡å·)ï¼Œè¿˜æœ‰è„‚è‚ªå’Œç”²çŠ¶è…ºä¹Ÿæ˜¯é«˜ä¿¡å·ï¼Œæ°´ä»½ä¸€èˆ¬éƒ½æ˜¯æ— ä¿¡å·ï¼Œ T2åŠ æƒç›¸é‡Œæ°´æ˜¯é«˜ä¿¡å·æ‰€ä»¥æ°´æ¯”è¾ƒäº®ï¼Œå› ä¸ºå¾ˆå¤šçš„ç—…å˜æœ‰æ°´è‚¿ï¼Œæ‰€ä»¥T2åŠ æƒç›¸é€šä¿—å¯ä»¥è¯´æ˜¯çœ‹ç—…å˜(æ¯•ç«Ÿæ¯”è¾ƒæ˜æ˜¾)ï¼Œ è§†è§‰ç›´è§‚ä¸Šæ¥çœ‹ï¼ŒT1çœ‹è§£å‰–ï¼ŒT2çœ‹ç—…å˜ â€”â€”æ€ä¹ˆfusionä¸€ä¸ªcaseï¼ˆæ ‡æ³¨åªæœ‰ä¸€å¼ ï¼‰ æ•°æ®é›† T1ã€T2çŸ¢çŠ¶ä½ï¼ŒT2è½´çŠ¶ä½ï¼Œ å…³é”®ç‚¹ï¼šåŸºäºT2çŸ¢çŠ¶ä½çš„ä¸­é—´å¸§ï¼Œ æ ‡æ³¨èŒƒå›´ï¼šä»èƒ¸12ï¼ˆT12ï¼‰è…°1ï¼ˆL1ï¼‰é—´çš„æ¤é—´ç›˜å¼€å§‹ï¼Œåˆ°è…°5ï¼ˆL5ï¼‰éª¶1ï¼ˆS1ï¼‰é—´çš„æ¤é—´ç›˜ç»“æŸ ç±»åˆ«ï¼šæ¤å—æœ‰ç¼–å·ï¼ˆT12åˆ°L5ï¼‰ï¼Œé—´ç›˜é€šè¿‡ä¸Šä¸‹æ¤å—çš„ç¼–å·è¡¨ç¤ºï¼ˆT12-L1åˆ°L5-S1ï¼‰ ç—…ç¶ï¼š * æ¤å—æœ‰ä¸¤ç±»ï¼šæ­£å¸¸V1å’Œé€€è¡Œæ€§ç—…å˜V2ï¼Œ * æ¤é—´ç›˜æœ‰7ç±»ï¼šæ­£å¸¸V1ï¼Œé€€è¡Œæ€§æ”¹å˜V2ï¼Œå¼¥æ¼«æ€§è†¨å‡ºï¼Œéå¯¹ç§°æ€§è†¨å‡ºï¼Œçªå‡ºï¼Œè„±å‡ºï¼Œç–å‡ºV7 jsonç»“æ„ï¼š uidï¼Œdimï¼Œspacingç­‰ä¸€äº›header info annotationï¼š sliceï¼šéš¾é“ä¸æ˜¯T2çŸ¢çŠ¶ä½çš„ä¸­é—´å¸§å—ï¼Ÿ pointï¼šå…³é”®ç‚¹åæ ‡ï¼Œç—…ç¶ç±»åˆ«ï¼Œå…³é”®ç‚¹ç±»åˆ« è¯„ä¼°æŒ‡æ ‡ distance&lt;8mm TPï¼šå¤šä¸ªå‘½ä¸­å–æœ€è¿‘çš„ï¼Œå…¶ä½™å¿½ç•¥ FPï¼šå‡é˜³æ€§ï¼Œdistanceè¶…å‡ºæ‰€æœ‰gtçš„8mmåœˆåœˆï¼è½è¿›åœˆåœˆä½†æ˜¯ç±»åˆ«é”™äº† FNï¼šå‡é˜´æ€§ï¼Œgtç‚¹æ²¡æœ‰è¢«TP precisionï¼šTP/(TP+FP) recallï¼šTP/(TP+FN) AP MAP]]></content>
      <tags>
        <tag>competition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[group normalization]]></title>
    <url>%2F2020%2F06%2F08%2Fgroup-normalization%2F</url>
    <content type="text"><![CDATA[Group Normalization åŠ¨æœº for small batch size do normalization in channel groups batch-independent behaves stably over different batch sizes approach BNâ€™s accuracy è®ºç‚¹ BN requires sufficiently large batch size (e.g. 32) Mask R-CNN frameworks use a batch size of 1 or 2 images because of higher resolution, where BN is â€œfrozenâ€ by transforming to a linear layer synchronized BN ã€BR LN &amp; IN effective for training sequential models or generative models but have limited success in visual recognition GNèƒ½è½¬æ¢æˆLNï¼IN WN normalize the filter weights, instead of operating on features æ–¹æ³• group it is not necessary to think of deep neural network features as unstructured vectors ç¬¬ä¸€å±‚å·ç§¯æ ¸é€šå¸¸å­˜åœ¨ä¸€ç»„å¯¹ç§°çš„filterï¼Œè¿™æ ·å°±èƒ½æ•è·åˆ°ç›¸ä¼¼ç‰¹å¾ è¿™äº›ç‰¹å¾å¯¹åº”çš„channel can be normalized together normalization transform the feature xï¼š$\hat x_i = \frac{1}{\sigma}(x_i-\mu_i)$ the mean and the standard deviationï¼š \mu_i=\frac{1}{m}\sum_{k\in S_i}x_k\\ \sigma_i=\sqrt {\frac{1}{m}\sum_{k\in S_i}(x_k-\mu_i)^2+\epsilon} the set $S_i$ BNï¼š $S_i=\{k|k_C = i_C\}$ pixels sharing the same channel index are normalized together for each channel, BN computes Î¼ and Ïƒ along the (N, H, W) axes LN $S_i=\{k|k_N = i_N\}$ pixels sharing the same batch index (per sample) are normalized together LN computes Î¼ and Ïƒ along the (C,H,W) axes for each sample IN $S_i=\{k|k_N = i_N, k_C=i_C\}$ pixels sharing the same batch index and the same channel index are normalized together LN computes Î¼ and Ïƒ along the (H,W) axes for each sample GN $S_i=\{k|k_N = i_N, [\frac{k_C}{C/G}]=[\frac{i_C}{C/G}]\}$ computes Î¼ and Ïƒ along the (H, W ) axes and along a group of C/G channels linear transform to keep representational ability per channel scale and shiftï¼š$y_i = \gamma \hat x_i + \beta$ relation to LN LN assumes all channels in a layer make â€œsimilar contributionsâ€ which is less valid with the presence of convolutions GN improved representational power over LN to IN IN can only rely on the spatial dimension for computing the mean and variance it misses the opportunity of exploiting the channel dependence ã€QUESTIONã€‘BNä¹Ÿæ²¡è€ƒè™‘é€šé“é—´çš„è”ç³»å•Šï¼Œä½†æ˜¯è®¡ç®—meanå’Œvarianceæ—¶è·¨äº†sample implementation reshape learnable $\gamma \&amp; \beta$ computable mean &amp; var å®éªŒ GNç›¸æ¯”äºBNï¼Œtraining erroræ›´ä½ï¼Œä½†æ˜¯val errorç•¥é«˜äºBN GN is effective for easing optimization loses some regularization ability it is possible that GN combined with a suitable regularizer will improve results é€‰å–ä¸åŒçš„groupæ•°ï¼Œæ‰€æœ‰çš„group&gt;1å‡å¥½äºgroup=1ï¼ˆLNï¼‰ é€‰å–ä¸åŒçš„channelæ•°ï¼ˆCï¼Gï¼‰ï¼Œæ‰€æœ‰çš„channel&gt;1å‡å¥½äºchannel=1ï¼ˆINï¼‰ Object Detection frozenï¼šå› ä¸ºhigher resolutionï¼Œbatch sizeé€šå¸¸è®¾ç½®ä¸º2/GPUï¼Œè¿™æ—¶çš„BN frozenæˆä¸€ä¸ªçº¿æ€§å±‚$y=\gamma(x-\mu)/\sigma+beta$ï¼Œå…¶ä¸­çš„$\mu$å’Œ$sigma$æ˜¯loadäº†pre-trained modelä¸­ä¿å­˜çš„å€¼ï¼Œå¹¶ä¸”frozenæ‰ï¼Œä¸å†æ›´æ–° denote as BN* replace BN* with GN during fine-tuning use a weight decay of 0 for the Î³ and Î² parameters]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ­£åˆ™åŒ–]]></title>
    <url>%2F2020%2F06%2F02%2Fregularization%2F</url>
    <content type="text"><![CDATA[ç»¼è¿° æ­£åˆ™ æ­£åˆ™åŒ–æ˜¯ç”¨æ¥è§£å†³ç¥ç»ç½‘ç»œè¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œé€šè¿‡é™ä½æ¨¡å‹çš„å¤æ‚æ€§å’Œçº¦æŸæƒå€¼ï¼Œè¿«ä½¿ç¥ç»ç½‘ç»œå­¦ä¹ å¯æ³›åŒ–çš„ç‰¹å¾ æ­£åˆ™åŒ–å¯ä»¥å®šä¹‰ä¸ºæˆ‘ä»¬ä¸ºäº†å‡å°‘æ³›åŒ–è¯¯å·®è€Œä¸æ˜¯å‡å°‘è®­ç»ƒè¯¯å·®è€Œå¯¹è®­ç»ƒç®—æ³•æ‰€åšçš„ä»»ä½•æ”¹å˜ å¯¹æƒé‡è¿›è¡Œçº¦æŸ å¯¹ç›®æ ‡å‡½æ•°æ·»åŠ é¢å¤–é¡¹ï¼ˆé—´æ¥çº¦æŸæƒå€¼ï¼‰ï¼šL1 &amp; L2æ­£åˆ™ æ•°æ®å¢å¼º é™ä½ç½‘ç»œå¤æ‚åº¦ï¼šdropoutï¼Œstochastic depth early stopping æˆ‘ä»¬åœ¨å¯¹ç½‘ç»œè¿›è¡Œæ­£åˆ™åŒ–æ—¶ä¸è€ƒè™‘ç½‘ç»œçš„biasï¼šæ­£åˆ™è¡¨è¾¾å¼åªæ˜¯æƒå€¼çš„è¡¨è¾¾å¼ï¼Œä¸åŒ…å«bias biasæ¯”weightå…·æœ‰æ›´å°‘çš„å‚æ•°é‡ å¯¹biasè¿›è¡Œæ­£åˆ™åŒ–å¯èƒ½å¼•å…¥å¤ªå¤šçš„æ–¹å·®ï¼Œå¼•å…¥å¤§é‡çš„æ¬ æ‹Ÿåˆ L1 &amp; L2ï¼š è¦æƒ©ç½šçš„æ˜¯ç¥ç»ç½‘ç»œä¸­æ¯ä¸ªç¥ç»å…ƒçš„æƒé‡å¤§å° L2å…³æ³¨çš„æ˜¯æƒé‡çš„å¹³æ–¹å’Œï¼Œæ˜¯è¦ç½‘ç»œä¸­çš„æƒé‡æ¥è¿‘0ä½†ä¸ç­‰äº0ï¼Œâ€œæƒé‡è¡°å‡â€ \frac{d}{dW}(\frac{\lambda}{2m}W^2) = \frac{\lambda}{m} W L1å…³æ³¨çš„æ˜¯æƒé‡çš„ç»å¯¹å€¼ï¼Œæƒé‡å¯èƒ½è¢«å‹ç¼©æˆ0ï¼Œæƒé‡æ›´æ–°æ—¶æ¯æ¬¡å‡å»çš„æ˜¯ä¸€ä¸ªå¸¸é‡ \frac{d}{dW}(\frac{\lambda}{m}W) = \frac{\lambda}{m} sgn(W) L1ä¼šè¶‹å‘äºäº§ç”Ÿå°‘é‡çš„ç‰¹å¾ï¼Œè€Œå…¶ä»–çš„ç‰¹å¾éƒ½æ˜¯0ï¼Œè€ŒL2ä¼šé€‰æ‹©æ›´å¤šçš„ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾éƒ½ä¼šæ¥è¿‘äº0 dropout æ¯ä¸ªepochè®­ç»ƒçš„æ¨¡å‹éƒ½æ˜¯éšæœºçš„ åœ¨testçš„æ—¶å€™ç›¸å½“äºensembleå¤šä¸ªæ¨¡å‹ æƒé‡å…±äº« æ•°æ®å¢å¼º éšå¼æ­£åˆ™åŒ–ï¼šå…¶å‡ºç°çš„ç›®çš„ä¸æ˜¯ä¸ºäº†æ­£åˆ™åŒ–ï¼Œè€Œæ­£åˆ™åŒ–çš„æ•ˆæœæ˜¯å…¶å‰¯äº§å“ï¼ŒåŒ…æ‹¬early stoppingï¼ŒBNï¼Œéšæœºæ¢¯åº¦ä¸‹é™ dropout &amp; drop connectï¼ˆ[Reference][https://zhuanlan.zhihu.com/p/108024434]ï¼‰ dropoutï¼š 2012å¹´Hintonæå‡ºï¼Œåœ¨æ¨¡å‹è®­ç»ƒæ—¶ä»¥æ¦‚ç‡péšæœºè®©éšå±‚èŠ‚ç‚¹çš„è¾“å‡ºå˜æˆ0ï¼Œæš‚æ—¶è®¤ä¸ºè¿™äº›èŠ‚ç‚¹ä¸æ˜¯ç½‘ç»œç»“æ„çš„ä¸€éƒ¨åˆ†ï¼Œä½†æ˜¯ä¼šæŠŠå®ƒä»¬çš„æƒé‡ä¿ç•™ä¸‹æ¥ï¼ˆä¸æ›´æ–°ï¼‰ã€‚ æ ‡å‡†dropoutç›¸å½“äºåœ¨ä¸€å±‚ç¥ç»å…ƒä¹‹åå†æ·»åŠ ä¸€ä¸ªé¢å¤–çš„å±‚ï¼Œè¿™äº›ç¥ç»å…ƒåœ¨è®­ç»ƒæœŸé—´ä»¥ä¸€å®šçš„æ¦‚ç‡å°†å€¼è®¾ç½®ä¸ºé›¶ï¼Œå¹¶åœ¨æµ‹è¯•æœŸé—´å°†å®ƒä»¬ä¹˜ä»¥pã€‚ drop connectï¼š ä¸æ˜¯éšæœºçš„å°†éšå±‚èŠ‚ç‚¹çš„è¾“å‡ºå˜æˆ0ï¼Œè€Œæ˜¯å°†èŠ‚ç‚¹ä¸­çš„æ¯ä¸ªä¸å…¶ç›¸è¿çš„è¾“å…¥æƒå€¼ä»¥1-pçš„æ¦‚ç‡å˜æˆ0ã€‚ï¼ˆä¸€ä¸ªæ˜¯è¾“å‡ºä¸€ä¸ªæ˜¯è¾“å…¥ï¼‰ è®­ç»ƒé˜¶æ®µï¼Œå¯¹æ¯ä¸ªexampleï¼mini-batch, æ¯ä¸ªepochéƒ½éšæœºsampleä¸€ä¸ªmaskçŸ©é˜µ Dropconnectåœ¨æµ‹è¯•æœŸé—´é‡‡ç”¨äº†ä¸æ ‡å‡†dropoutä¸åŒçš„æ–¹æ³•ã€‚ä½œè€…æå‡ºäº†dropconnectåœ¨æ¯ä¸ªç¥ç»å…ƒå¤„çš„é«˜æ–¯è¿‘ä¼¼ï¼Œç„¶åä»è¿™ä¸ªé«˜æ–¯å‡½æ•°ä¸­æŠ½å–ä¸€ä¸ªæ ·æœ¬å¹¶ä¼ é€’ç»™ç¥ç»å…ƒæ¿€æ´»å‡½æ•°ã€‚è¿™ä½¿å¾—dropconnectåœ¨æµ‹è¯•æ—¶å’Œè®­ç»ƒæ—¶éƒ½æ˜¯ä¸€ç§éšæœºæ–¹æ³•ã€‚ ä¼¯åŠªåˆ©åˆ†å¸ƒï¼š0-1åˆ†å¸ƒ dropout &amp; drop connect é€šå¸¸åªä½œç”¨äºå…¨è¿æ¥å±‚ä¸Šï¼šè¿™ä¿©æ˜¯ç”¨æ¥é˜²æ­¢è¿‡å¤šå‚æ•°å¯¼è‡´è¿‡æ‹Ÿåˆ å·ç§¯å±‚å‚æ•°è´¼å°‘ï¼Œæ‰€ä»¥æ²¡å¿…è¦ï¼Œ é’ˆå¯¹å·ç§¯é€šé“æœ‰spacial dropoutï¼šæŒ‰ç…§channeléšæœºæ‰” dropblockï¼šæ˜¯é’ˆå¯¹å·ç§¯å±‚çš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œç›¸æ¯”è¾ƒäºdropoutçš„random muteï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°removeæ‰éƒ¨åˆ†è¯­ä¹‰ä¿¡æ¯ï¼Œblock size=1çš„æ—¶å€™é€€åŒ–æˆdropout papers [dropout] Improving neural networks by preventing co-adaptation of feature detectorsï¼Œä¸¢èŠ‚ç‚¹ [drop connect] Regularization of neural networks using dropconnectï¼Œä¸¢weight path [Stochastic Depth] Deep Networks with Stochastic Depthï¼Œä¸¢layer [DropBlock] A regularization method for convolutional networks dropå¤§æ³•ä¸€å¥è¯æ±‡æ€» dropoutï¼šå„ç»´åº¦å®Œå…¨éšæœºæ‰” spacial dropoutï¼šæŒ‰ç…§channeléšæœºæ‰” stochastic depthï¼šæŒ‰ç…§res blockéšæœºæ‰” dropblockï¼šåœ¨feature mapä¸ŠæŒ‰ç…§spacialå—éšæœºæ‰” cutoutï¼šåœ¨input mapä¸ŠæŒ‰ç…§spacialå—éšæœºæ‰” dropconnectï¼šæ‰”è¿æ¥ä¸æ‰”ç¥ç»å…ƒ Deep Networks with Stochastic Depth åŠ¨æœº propose a training procedureï¼šstochastic depthï¼Œtrain short and test deep for each mini-batch randomly drop a subset of layers and bypass them with the identity function shortï¼šreduces training time regï¼šimproves the test error can increase the network depth è®ºç‚¹ deeper expressiveness vanishing gradients diminishing feature reuse resnet skip connection whenè¾“å…¥è¾“å‡ºchannelæ•°ä¸matchï¼šredefine id(Â·) as a linear projection to reduce the dimensions dropout Dropout reduces the effect known as â€œco- adaptationâ€ of hidden nodes Dropout loses effectiveness when used in combination with Batch Normalization our approach higher diversity shorter instead of thinner work with Batch Normalization æ–¹æ³• stochastic depth randomly dropping entire ResBlocks $H_l = ReLU(b_l Res_l(H_{l-1}) + id(H_{l-1}))$ survival probabilities $p_l = Pr(b_l=1)$ set uniformly / set following a linear decay rule set $p_0=1, p_L=0.5$ï¼š p_l = 1 - \frac{l}{L}(1-p_L) intuitionï¼šthe earlier layers extract low-level features that will be used by later layers and should therefore be more reliably present Expected network depth $E(L) \approx 3L/4$ approximately 25% of training time could be saved during testing all res path are active each res path is weighted by its survival probability $H_l^{Test} = ReLU(b_l Res_l(H_{l-1}, W_l) + id(H_{l-1}))$ è·Ÿdropoutä¸€æ ·]]></content>
      <tags>
        <tag>æ­£åˆ™åŒ–ï¼Œdropoutï¼Œdropconnect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RetinaNet]]></title>
    <url>%2F2020%2F05%2F30%2FRetinaNet%2F</url>
    <content type="text"><![CDATA[[det] RetinaNet: Focal Loss for Dense Object Detection [det+instance seg] RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free [det+semantic seg] Retina U-Net: Embarrassingly Simple Exploitation of Segmentation Supervision for Medical Object Detection Focal Loss for Dense Object Detection åŠ¨æœº dense prediction(one-stage detector) focal lossï¼šaddress the class imbalance problem RetinaNetï¼šdesign and train a simple dense detector è®ºç‚¹ accuracy trailed two-stageï¼šclassifier is applied to a sparse set of candidate one-stageï¼šdense sampling of possible object locations the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause loss standard cross entropy lossï¼šdown-weights the loss assigned to well-classified examples proposed focal lossï¼šfocuses training on a sparse set of hard examples R-CNNç³»åˆ—two-stage framework proposal-driven the first stage generates a sparse set of candidate object locations the second stage classifies each candidate location as one of the foreground classes or as background class imbalanceï¼šåœ¨stage1å¤§éƒ¨åˆ†èƒŒæ™¯è¢«filter outäº†ï¼Œstage2è®­ç»ƒçš„æ—¶å€™å¼ºåˆ¶å›ºå®šå‰èƒŒæ™¯æ ·æœ¬æ¯”ä¾‹ï¼Œå†åŠ ä¸Šå›°éš¾æ ·æœ¬æŒ–æ˜OHEM fasterï¼šreducing input image resolution and the number of proposals ever fasterï¼šone-stage one-stage detectors One stage detectors are applied over a regular, dense sampling of object locations, scales, and aspect ratios denseï¼šregularly sampling(contrast to selection)ï¼ŒåŸºäºgridä»¥åŠanchorä»¥åŠå¤šå°ºåº¦ the training procedure is still dominated by easily classified background examples class imbalanceï¼šé€šå¸¸å¼•å…¥bootstrappingå’Œhard example miningæ¥ä¼˜åŒ– Object Detectors Classicï¼šsliding-window+classifier based on HOGï¼Œdense predict Two-stageï¼šselective Search+classifier based on CNNï¼Œshared network RPN One-stageï¼šâ€˜anchorsâ€™ introduced by RPNï¼ŒFPN loss Huber lossï¼šdown-weighting the loss of outliers (hard examples) focal lossï¼šdown-weighting inliers (easy examples) æ–¹æ³• focal loss CEï¼š$CE(p_t)=-log(p_t)$ even examples that are easily classified ($p_t&gt;0.5$) incur a loss with non-trivial magnitude summed CE loss over a large number of easy examples can overwhelm the rare class WCEï¼š$WCE(p_t)=-\alpha_t log(p_t)$ balances the importance of positive/negative examples does not differentiate between easy/hard examples FLï¼š$FL(p_t)=-\alpha_t(1-p_t)^\gamma log(p_t)$ as $\gamma$ increases the modulating factor is likewise increased $\gamma=2$ works best in our experiments â€‹ two-stage detectorsé€šå¸¸ä¸ä¼šä½¿ç”¨WCEæˆ–FL cascade stageä¼šè¿‡æ»¤æ‰å¤§éƒ¨åˆ†easy negatives ç¬¬äºŒé˜¶æ®µè®­ç»ƒä¼šåšbiased minibatch sampling Online Hard Example Mining (OHEM) construct minibatches using high-loss examples scored by loss + nms completely discards easy examples RetinaNet composeï¼šbackbone network + two task-specific subnetworks backboneï¼šconvolutional feature map over the entire input image subnet1ï¼šobject classification subnet2ï¼šbounding box regression ResNet-FPN backbone rich, multi-scale feature pyramidï¼ŒäºŒé˜¶æ®µçš„RPNä¹Ÿç”¨äº†FPN each level can be used for detecting objects at a different scale P3 - P7ï¼š8x - 128x downsamp FPN channelsï¼š256 anchors anchor ratiosï¼š{1:2, 1:1, 2:1}ï¼Œé•¿å®½æ¯” anchor scalesï¼š{$2^0$, $2^\frac{1}{3}$, $2^\frac{2}{3}$}ï¼Œå¤§å°ï¼ŒåŒä¸€ä¸ªscaleçš„anchorï¼Œé¢ç§¯ç›¸åŒï¼Œéƒ½æ˜¯size*sizeï¼Œé•¿å®½é€šè¿‡ratioæ±‚å¾— anchor size per levelï¼š[32, 64, 128, 256, 512]ï¼ŒåŸºæœ¬çš„æ­£æ–¹å½¢anchorçš„è¾¹é•¿ total anchors per levelï¼šA=9 KAï¼šeach anchor is assigned a length K one-hot vector of classification targets 4Aï¼šand a 4-vector of box regression targets anchors are assigned to ground-truth object boxes using an intersection-over-union (IoU) threshold of 0.5 anchors are assigned background if their IoU is in [0, 0.4) anchor is unassigned between [0.4, 0.5), which is ignored during training each anchor is assigned to at most one object box for each anchor classification targetsï¼šone-hot vector box regression targetsï¼šeach anchorå’Œå…¶å¯¹åº”çš„gt boxçš„offset rpn offsetï¼šä¸­å¿ƒç‚¹ã€å®½ã€é«˜ $$ t_x = (x - x_a) / w_a\\ t_y = (y - y_a) / h_a\\ t_w = log(w/ w_a)\\ t_h = log(h/ h_a) $$ or omitted if there is no assignment ã€QUESTIONã€‘æ‰€è°“çš„anchor state {-1:ignore, 0:negative, 1:positive} æ˜¯é’ˆå¯¹cls lossæ¥è¯´çš„ï¼Œç›¸å½“äºäººä¸ºä¸¢å¼ƒäº†ä¸€éƒ¨åˆ†åå‘ä¸­ç«‹çš„æ ·æœ¬ï¼Œè¿™å¯¹åˆ†ç±»æ•ˆæœæœ‰æå‡å—ï¼Ÿï¼Ÿ classification subnet for each spatial positionï¼Œfor each anchorï¼Œpredict one among K classesï¼Œone-hot inputï¼šC channels feature map from FPN structureï¼šfour 3x3 conv + ReLUï¼Œeach with C filters headï¼š3x3 conv + sigmoidï¼Œwith KA filters share across levels not share with box regression subnet focal lossï¼š sum over all ï½100k anchors * and normalized by the number of anchors assigned to a ground-truth box * å› ä¸ºæ˜¯sumï¼Œæ‰€ä»¥è¦normailizeï¼Œnormé¡¹ç”¨çš„æ˜¯number of assigned anchorsï¼ˆè¿™æ˜¯åŒ…æ‹¬äº†å‰èƒŒæ™¯ï¼Ÿï¼‰ * vast majority of anchors are **easy negatives** and receive negligible loss values under the focal lossï¼ˆç¡®å®åŒ…å«èƒŒæ™¯æ¡†ï¼‰ * $\alpha$ï¼šIn general $alpha$ should be decreased slightly as $\gamma$ is increased strong effect on negativesï¼šFL can effectively discount the effect of easy negatives, focusing all attention on the hard negative examples box regression subnet class-agnostic bounding box regressor same structureï¼šfour 3x3 conv + ReLUï¼Œeach with C filters * headï¼š4A linear outputs * L1 loss inference keep top 1k predictions per FPN level * all levels are merged and non-maximum suppression with a threshold of 0.5 train initializationï¼š cls head bias initializationï¼Œencourage more foreground prediction at the start of training prevents the large number of background anchors from generating a large, destabilizing loss network design anchors * one-stage detecors use fixed sampling grid to generate position * use multiple â€˜anchorsâ€™ at each spatial position to cover boxes of various scales and aspect ratios * beyond 6-9 anchors did not shown further gains in AP * speed/accuracy trade-off * outperforms all previous methods * bigger resolution bigger AP * Retina-101-600ä¸ResNet101-FRCNNçš„APæŒå¹³ï¼Œä½†æ˜¯æ¯”ä»–å¿« gradientï¼š æ¢¯åº¦æœ‰ç•Œ the derivative is small as soon as $x_t &gt; 0$ &lt;img src=&quot;RetinaNet/gradient.png&quot; width=&quot;70%;&quot; /&gt; â€‹ RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free åŠ¨æœº improve single-shot detectors to the same level as current two-stage techniques improve on RetinaNet integrating instance mask prediction adaptive loss additional hard examples Group Normalization same computational cost as the original RetinaNet but more accurateï¼šåŒæ ·çš„å‚æ•°é‡çº§æ¯”orgin RetinaNetå‡†ï¼Œæ•´ä½“çš„å‚æ•°é‡çº§å¤§äºyolov3ï¼Œaccå¿«è¦æ¥è¿‘äºŒé˜¶æ®µçš„mask RCNNäº† è®ºç‚¹ part of improvements of two-stage detectors is due to architectures like Mask R-CNN that involves multiple prediction heads additional segmentation task had only been added to two-stage detectors in the past two-stage detectors have the cost of resampling(ROI-Align) issueï¼šRPNä¹‹åè¦ç‰¹å¾å¯¹é½ add addtional heads in training keeps the structure of the detector at test time unchanged potential improvement directions dataï¼šOHEM contextï¼šFPN additional taskï¼šsegmentation branch this paperâ€™s contribution add a mask prediction branch propose a new self-adjusting loss function include more of positive samplesâ€”&gt;those with low overlap æ–¹æ³• best matching policy speical caseï¼šoutlier gt boxï¼Œè·Ÿæ‰€æœ‰çš„anchor iouéƒ½ä¸å¤§äº0.5ï¼Œæ°¸è¿œä¸ä¼šè¢«å½“ä½œæ­£æ ·æœ¬ use best matching anchor with any nonzero overlap to replace the threshold self-adjusting Smooth L1 loss bbox regression smooth L1ï¼š L1 loss is used beyond $\beta$ to avoid over-penalizing outliers the choice of control point is heuristic and is usually done by hyper parameter search f(x) = \begin{cases} 0.5 \frac{x^2}{\beta} \text{, if } |x| < \beta \\ |x| - 0.5\beta \text{, otherwise } \end{cases} self-adjusting control point running mean &amp; variance \mu_B = \frac{1}{n}\sum_{i=1}^n |x_i|\\ \sigma_B^2 = \frac{1}{n}\sum_{i=1}^n(|x_i|-\mu_B)^2 minibatch updateï¼šm=0.9 \mu_R = \mu_R * m + \mu_B*(1-m)\\ \sigma_R^2 = \sigma_R^2*m+\sigma_B^2*(1-m) control pointï¼š$[0, \hat \beta]$ clip to avoid unstable \beta = max(0, min(\hat \beta, \mu_R-\sigma_R^2)) mask module detection predictions are treated as mask proposals extract the top N scored predictions distribute the mask proposals to sample features from the appropriate layers k = [k_0 + log_2 \sqrt{wh}/224] $k_0=4$ï¼Œå¦‚æœsizeå°äº224*224ï¼Œproposalä¼šè¢«åˆ†é…ç»™P3ï¼Œå¦‚æœå¤§äº448*448ï¼Œproposalä¼šè¢«åˆ†é…ç»™P5 using more feature layers shows no performance boost architecture r50&amp;r101 backï¼šfreezing all of the Batch Nor- malization layers fpn feature channelï¼š256 classification branch 4 conv layersï¼šconv3x3+reluï¼Œchannel256 headï¼šconv3x3+sigmoidï¼Œchannel n_anchors*n_classes regression branch 4 conv layersï¼šconv3x3+reluï¼Œchannel256 headï¼šconv3x3ï¼Œchannel n_anchors*4 aggregate the boxes to the FPN layers ROI-Align yielding 14x14 resolution features mask head 4 conv layersï¼šconv3x3 a single transposed convolutional layerï¼šconvtranspose2d 2x2ï¼Œto 28*28 resolution prediction headï¼šconv1x1 training min side &amp; max sideï¼š800&amp;1333 limited GPUï¼šreduce the batch sizeï¼Œincreasing the number of training iterations and reducing the learning rate accordingly positive/ignore/negativeï¼š0.5ï¼Œ0.4 focal loss for classification gaussian initialization $\alpha=0.25, \lambda=2.0$ $FL=-\alpha_t(1-p_t)^\lambda log(p_t)$ FL = \left\{ \begin{array}{lr} -\alpha (1-p)^{\gamma}log(p), \ \ y=1\\ -(1-\alpha) p^{\gamma}log(1-p), \ \ y=0\\ \end{array} \right. gammaé¡¹æ§åˆ¶çš„æ˜¯ç®€å•æ ·æœ¬çš„è¡°å‡é€Ÿåº¦ï¼Œalphaé¡¹æ§åˆ¶çš„æ˜¯æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ï¼Œå¯ä»¥é»˜è®¤å€¼ä¸‹æ­£æ ·æœ¬çš„æƒé‡æ˜¯0.25ï¼Œè´Ÿæ ·æœ¬çš„æƒé‡æ˜¯0.75ï¼Œå’Œæƒ³è±¡ä¸­çš„ç»™æ­£æ ·æœ¬æ›´å¤šæƒé‡ä¸ä¸€æ ·ï¼Œå› ä¸ºalphaå’Œgammaæ˜¯è€¦åˆèµ·æ¥ä½œç”¨çš„ï¼Œï¼ˆå¯èƒ½æ£€æµ‹åœºæ™¯ä¸‹å›°éš¾çš„è´Ÿæ ·æœ¬ç›¸æ¯”äºæ­£æ ·æœ¬æ›´å°‘ï¼ŸèƒŒæ™¯å°±æ˜¯æ¯”å‰æ™¯å¥½å­¦ï¼Ÿä¸ç¡®å®šä¸ç¡®å®šã€‚ã€‚ã€‚ï¼‰ self-adjusting L1 loss for box regression limit running paramsï¼š[0, 0.11] mask loss top-100 predicted boxes + ground truth boxes inference box confidence threshold 0.05 nms threshold 0.4 use top-50 boxes for mask prediction Retina U-Net: Embarrassingly Simple Exploitation of Segmentation Supervision for Medical Object Detection åŠ¨æœº localization pixel-level predict ad-hoc heuristics when mapping back to object-level scores semantic segmentation auxiliary task overall one-stage leveraging available supervision signals è®ºç‚¹ monitoring pixel-wise predictions are clinically required medical annotations is commonly performed in pixel- wise full semantic supervision fully exploiting the available semantic segmentation signal results in significant performance gains one-stage explicit scale variance enforced by the resampling operation in two-stage detectors is not helpful in the medical domain two-stage methods predict proposal-based segmentations mask loss is only evaluated on cropped proposalï¼šno context gradients ROI-Alignï¼šnot suggested in medical image depends on the results of region proposalï¼šserial vs parallel gradients of the mask loss do not flow through the entire model æ–¹æ³• model backï¼š ResNet50 fpnï¼š shift p3-p6 to p2-p5 change sigmoid to softmax 3d head channelsï¼š64 anchor sizeï¼š$\{P_2: 4^2, P_3: 8^2,, P_4: 16^2,, P_5: 32^2\}$ 3d z-scaleï¼š{1ï¼Œ2ï¼Œ4ï¼Œ8}ï¼Œè€ƒè™‘åˆ°zæ–¹å‘çš„low resolution segmentation supervision p0 &amp; p1 with skip connections without detection heads segmentation loss calculates on p0 logits dice + ce h weighted box clustering patch crop tiling strategies &amp; model ensembling causes multi predictions per location nmsé€‰äº†ä¸€ç±»ä¸­scoreæœ€å¤§çš„boxï¼Œç„¶åæŠ‘åˆ¶æ‰€æœ‰ä¸å®ƒåŒç±»çš„IoUå¤§äºä¸€å®šé˜ˆå€¼çš„box weighted boxä½œç”¨äºè¿™ä¸€ç±»æ‰€æœ‰çš„boxï¼Œè®¡ç®—ä¸€ä¸ªèåˆçš„ç»“æœ coordinates confidenceï¼š$o_c = \frac{\sum c_i s_i w_i}{\sum s_i w_i}$ score confidenceï¼š$o_s = \frac{\sum s_i w_i}{\sum w_i + n_{missing * \overline w}}$ $w_i$ï¼š$w=f a p$ overlap factor fï¼šä¸highest scoring boxçš„overlap area factor aï¼šhigher weights to larger boxesï¼Œç»éªŒ patch center factor pï¼šç›¸å¯¹äºpatch centerçš„æ­£æ€åˆ†å¸ƒ score confidenceçš„åˆ†æ¯ä¸Šæœ‰ä¸€ä¸ªdown-weighté¡¹$n_{missing}$ï¼šåŸºäºprior knowledgeé¢„æœŸpredictionçš„æ€»æ•°å¾—åˆ° è®ºæ–‡ç»™çš„ä¾‹å­è®©æˆ‘æ„Ÿè§‰å¥½æ¯”nmsçš„ç‚¹ ä¸€ä¸ªclusteré‡Œé¢ä¸€ç±»æœ€ç»ˆå°±ç•™ä¸‹ä¸€ä¸ªæ¡†ï¼šè§£å†³nmsä¸€ç±»å¤§æ¡†åŒ…å°æ¡†çš„æƒ…å†µ è¿™ä¸ªlocationä¸Špredictionæ˜æ˜¾å°‘äºprior knowledgeçš„ç±»åˆ«confidenceä¼šè¢«æ˜¾è‘—æ‹‰ä½ï¼šè§£å†³ä¸€ä¸ªä½ç½®å‡ºç°å¤§æ¦‚ç‡å‡é˜³æ¡†çš„æƒ…å†µ]]></content>
      <tags>
        <tag>ç›®æ ‡æ£€æµ‹ï¼Œfocallossï¼Œå®ä¾‹åˆ†å‰²ï¼Œè‡ªé€‚åº”smoothL1</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DCGAN]]></title>
    <url>%2F2020%2F05%2F27%2FDCGAN%2F</url>
    <content type="text"><![CDATA[UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS åŠ¨æœº unsupervised learning learns a hierarchy of representations from object parts to scenes used for novel tasks è®ºç‚¹ GAN Learning reusable feature representations from large unlabeled datasets generator and discriminator networks can be later used as feature extractors for supervised tasks unstable to train we propose a set of constraints on the architectural topology making it stable to train use the trained discriminators for image classification tasks visualize the filters show that the generators have interesting vector arithmetic properties unsupervised representation learning clustering, hierarchical clustering auto-encoders learn good feature representations generative image models samples often suffer from being blurry, being noisy and incomprehensible further use for supervised tasks æ–¹æ³• architecture all convolutional netï¼šæ²¡æœ‰æ± åŒ–ï¼Œç”¨stride conv eliminating fully connected layersï¼š generatorï¼šè¾“å…¥æ˜¯ä¸€ä¸ªå‘é‡ï¼Œreshapeä»¥åæ¥çš„å…¨æ˜¯å·ç§¯å±‚ discriminatorï¼šæœ€åä¸€å±‚å·ç§¯å‡ºæ¥ç›´æ¥flatten Batch Normalization generatorè¾“å‡ºå±‚ &amp; discriminatorè¾“å…¥å±‚ä¸åŠ  resulted in sample oscillation and model instability ReLU generatorè¾“å‡ºå±‚ç”¨Tanh discriminatorç”¨leakyReLU train image preprocessï¼šrescale to [-1,1] LeakyReLU(0.2) lrï¼š2e-4 momentum term $\beta 1$ï¼š0.5, default 0.9 å®éªŒ evaluate apply them as a feature extractor on supervised datasets evaluate the performance of linear models on top of these features model use the discriminatorâ€™s convolutional features from all layers maxpooling to 4x4 grids flattened and concatenated to form a 28672 dimensional vector regularized linear L2-SVM ç›¸æ¯”ä¹‹ä¸‹ï¼šthe discriminator has many less feature maps, but larger total feature vector size visualizing walking in the latent space åœ¨vector Zä¸Šå·®å€¼ï¼Œç”Ÿæˆå›¾åƒå¯ä»¥è§‚å¯Ÿåˆ°smooth transitions visualize the discriminator feature ç‰¹å¾å›¾å¯è§†åŒ–ï¼Œèƒ½è§‚å¯Ÿåˆ°åºŠç»“æ„ manipulate the generator representation generator learns specific object representations for major scene components use logistic regression to find feature maps related with window, drop the spatial locations on feature-maps most result forgets to draw windows in the bedrooms, replacing them with other objects vector arithmetic averaging the Z vector for three examplars semantically obeyed the arithmetic]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[densenet]]></title>
    <url>%2F2020%2F05%2F27%2Fdensenet%2F</url>
    <content type="text"><![CDATA[åŠ¨æœº embrace shorter connections the feature-maps of all preceding layers are used as inputs advantages alleviate vanishing-gradient encourage feature reuse reduce the number of parameters è®ºç‚¹ Dense each layer obtains additional inputs from all preceding lay- ers and passes on its own feature-maps to all subsequent layers feature reuse combine features by concatenatingï¼šthe summation in ResNet may impede the information flow in the network information preservation id shortcut/additive identity transformations fewer params DenseNet layers are very narrow add only a small set of feature-maps to the â€œcollective knowledgeâ€ gradients flow each layer has direct access to the gradients from the loss function have regularizing effect æ–¹æ³• architecture dense blocks concat BN-ReLU-3x3 conv $x_l = H_l([x_0, x_1, â€¦, x_{l-1}])$ transition layers change the size of feature-maps BN-1x1 conv-2x2 avg pooling growth rate k $H_l$ produces feature- maps narrowï¼še.g., k = 12 One can view the feature-maps as the global state of the network The growth rate regulates how much new information each layer contributes to the global state bottleneck â€”- DenseNet-B in dense block stage 1x1 conv reduce dimension first number of channelsï¼š4k compression â€”- DenseNet-C in transition stage reduce the number of feature-maps number of channelsï¼š$\theta k$ structure configurations 1st conv channelsï¼šç¬¬ä¸€å±‚å·ç§¯é€šé“æ•° number of dense blocks Lï¼šdense blocké‡Œé¢çš„layeræ•° kï¼šgrowth rate Bï¼šbottleneck 4k Cï¼šcompression 0.5k è®¨è®º concat replace sumï¼š seemingly small modification lead to substantially different behaviors of the two network architectures feature reuseï¼šfeature can be accessed anywhere parameter efficientï¼šåŒæ ·å‚æ•°é‡ï¼Œtest accæ›´é«˜ï¼ŒåŒæ ·accï¼Œå‚æ•°é‡æ›´å°‘ deep supervisionï¼šclassifiers attached to every hidden layer weight assign All layers spread their weights over multi inputs (include transition layers) least weight are assigned to the transition layer, indicating that transition layers contain many redundant features, thus can be compressed overall there seems to be concentration towards final feature-maps, suggesting that more high-level features are produced late in the network]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GANomaly]]></title>
    <url>%2F2020%2F05%2F25%2FGANomaly%2F</url>
    <content type="text"><![CDATA[GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training åŠ¨æœº Anomaly detection highly biased towards one class (normal) insufficient sample size of the other class (abnormal) semi-supervised learning detecting the unknown/unseen anomaly case trained on normal samples tested on normal and abnormal samples encoder-decoder-encoder minimizing the distance between the images and the latent vectors a larger distance metric è®ºç‚¹ supervised approaches heavily depend on large, labeled datasets Generative Adversarial Networks (GAN) have emerged as a leading methodology across both unsupervised and semi-supervised problems reconstruction-based anomaly techniques Overall prior work strongly supports the hypothesis that the use of autoencoders and GAN æ–¹æ³• GAN unsupervised to generate realistic images compete generator tries to generate an image, decoder- alike network, map input to latent space discriminator decides whether the generated image is a real or a fake, classical classification architecture, reading an input image, and determining its validity Adversarial Auto-Encoders (AAE) encoder + decoder reconstruction: maps the input to latent space and remaps back to input data space train autoencoders with adversarial setting inverse mapping with the additional use of an encoder, a vanilla GAN network is capable of learning inverse mapping model learns both the normal data distribution and minimizes the output anomaly score two encoder, one decoder, a discriminator encoder convolutional layers followed by batch-norm and leaky ReLU() activation compress to a vector z decoder convolutional transpose layers, ReLU() activation and batch-norm a tanh layer at the end 2nd encoder with the same architectural but different parametrization discriminator DCGAN discriminator Adversarial Loss ä¸æ˜¯åŸºäºGANçš„traditional 0/1 ouput è€Œæ˜¯é€‰äº†ä¸€ä¸ªä¸­é—´å±‚ï¼Œè®¡ç®—realï¼fake(reconstructed)çš„L2 distance Contextual Loss L1 yields less blurry results than L2 è®¡ç®—è¾“å…¥å›¾åƒå’Œé‡å»ºå›¾åƒçš„L1 distance Encoder Loss an additional encoder loss to minimize the distance of the bottleneck features è®¡ç®—ä¸¤ä¸ªé«˜ç»´å‘é‡çš„L2 distance åœ¨æµ‹è¯•çš„æ—¶å€™ç”¨å®ƒæ¥scoring the abnormality]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[resnets]]></title>
    <url>%2F2020%2F05%2F23%2Fresnets%2F</url>
    <content type="text"><![CDATA[overview papers [resnet] ResNet: Deep Residual Learning for Image Recognition [resnext] ResNext: Aggregated Residual Transformations for Deep Neural Networks [resnest] ResNeSt: Split-Attention Networks [revisiting resnets] Revisiting ResNets: Improved Training and Scaling Strategies ResNext: Aggregated Residual Transformations for Deep Neural Networks åŠ¨æœº new network architecture new building blocks with the same topology propose cardinality increasing cardinality is able to improve classification accuracy is more effective than going deeper or wider classification task è®ºç‚¹ VGG &amp; ResNetsï¼š stacking building blocks of the same topology deeper reduces the free choices of hyper-parameters Inception models split-transform-merge strategy splitï¼š1x1conv spliting into a few lower-dimensional embeddings transformï¼ša set of specialized filters mergeï¼šconcat approach the representational power of large and dense layers, but at a considerably lower computational complexity modules are customized stage-by-stage our architecture adopts VGG/ResNetsâ€™ repeating layers adopts Inceptionâ€˜s split-transform-merge strategy aggregated by summation cardinalityï¼šthe size of the set of transformationsï¼ˆsplit pathæ•°ï¼‰ å¤šäº†1x1 convçš„è®¡ç®—é‡ å°‘äº†3x3 convçš„è®¡ç®—é‡ è¦ç´  Multi-branch convolutional blocks Grouped convolutionsï¼šé€šé“å¯¹é½ï¼Œç¨€ç–è¿æ¥ Compressing convolutional networks Ensembling æ–¹æ³• architecture a template module if producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes) when the spatial map is downsampled by a factor of 2, the width of the blocks is multiplied by a factor of 2 grouped convolutionsï¼šç¬¬ä¸€ä¸ª1x1å’Œ3x3convçš„widthè¦æ ¹æ®Cè¿›è¡Œsplit equivalent blocks BN after each conv ReLU after each BN except the last of block ReLU after add r Model Capacity improve accuracy when maintaining the model complexity and number of parameters adjust the width of bottleneck, the according C to maintain capacityï¼šC=1çš„æ—¶å€™é€€åŒ–æˆResNet block ResNeSt: Split-Attention Networks åŠ¨æœº propose a modular Split-Attention block enables attention across feature-map groups preserve the overall ResNet structure for downstream applications such as object detection and semantic segmentation prove improvement on detection &amp; segmentation tasks è®ºç‚¹ ResNet simple and modular design limited receptive-field size and lack of cross-channel interaction image classification networks have focused more on group or depth-wise convolution do not transfer well to other tasks isolated representations cannot capture cross-channel relationships a versatile backbone improving performance across multiple tasks at the same time a network with cross-channel representations is desirable a Split-Attention block divides the feature-map into several groups (along the channel dimension) finer-grained subgroups or splits weighted combination featuremap attention mechanismï¼šNiNâ€™s 1x1 conv Multi-pathï¼šGoogleNet channel-attention mechanismï¼šSE-Net ç»“æ„ä¸Šï¼Œå…¨å±€ä¸Šçœ‹ï¼Œæ¨¡ä»¿ResNextï¼Œå¼•å…¥cardinalityå’Œgroup convï¼Œå±€éƒ¨ä¸Šçœ‹ï¼Œæ¯ä¸ªgroupå†…éƒ¨ç»§ç»­åˆ†ç»„ï¼Œç„¶åæ¨¡ä»¿SK-Netï¼Œèåˆå¤šä¸ªåˆ†æ”¯çš„split-attentionï¼Œå¤§groupä¹‹é—´concatï¼Œè€Œä¸æ˜¯ResNextçš„addï¼Œå†ç»1x1 convè°ƒæ•´ç»´åº¦ï¼Œadd id path æ–¹æ³• Split-Attention block enables feature-map attention across different feature-map groups within a blockï¼šcontrolled by cardinality within a cardinal groupï¼šintroduce a new radix hyperparameter R indicating the number of splits split-attention å¤šä¸ªin-group branchçš„inputè¾“å…¥è¿›æ¥ fusionï¼šå…ˆåšelement-wise summation channel-wise global contextual informationï¼šåšglobal average pooling é™ç»´ï¼šDense-BN-ReLU å„åˆ†æ”¯Dense(the attention weight function)ï¼šå­¦ä¹ å„è‡ªçš„é‡è¦æ€§æƒé‡ channel-wise soft attentionï¼šå¯¹å…¨éƒ¨çš„denseåšsoftmax åŠ æƒï¼šåŸå§‹çš„å„åˆ†æ”¯inputä¸åŠ æƒçš„denseåšä¹˜æ³• å’Œï¼šåŠ æƒçš„å„åˆ†æ”¯add r=1ï¼šé€€åŒ–æˆSE-blockaverage pooling shortcut connection for blocks with a strided convolution or combined convolution-with-pooling can be applied to the id concat average pooling downsampling for dense prediction tasksï¼šit becomes essential to preserve spatial information former work tend to use strided 3x3 conv we use an average pooling layer with 3x3 kernel 2x2 average pooling applied to strided shortcut connection before 1x1 conv Revisiting ResNets: Improved Training and Scaling Strategies åŠ¨æœº disentangle the three aspects model architecture training methodology scaling strategies improve ResNets to SOTA design a family of ResNet architectures, ResNet-RS use improved training and scaling strategies and combine minor architecture changes åœ¨ImageNetä¸Šæ‰“è´¥efficientNet åœ¨åŠç›‘ç£ä¸Šæ‰“è´¥efficientNet-noisystudent è®ºç‚¹ ImageNetä¸Šæ¦œå¤§æ³• Architecture äººå·¥ç³»åˆ—ï¼šAlexNetï¼ŒVGGï¼ŒResNetï¼ŒInceptionï¼ŒResNeXt NASç³»åˆ—ï¼šNasNet-Aï¼ŒAmoebaNet-Aï¼ŒEfficientNet Training and Regularization Methods regularization methods dropoutï¼Œlabel smoothingï¼Œstochastic depthï¼Œdropblockï¼Œdata augmentation significantly improve generalization when training more epochs training learning rate schedules Scaling Strategies model dimensionï¼šwidthï¼Œdepthï¼Œresolution efficientNetæå‡ºçš„å‡è¡¡å¢é•¿ï¼Œåœ¨æœ¬æ–‡ä¸­shows sub-optimal for both resnet and efficientNet Additional Training Data pretraining on larger dataset semi-supervised the performance of a vision model architectureï¼šmost research focus on training methods and scaling strategyï¼šless publicized but critical unfairï¼šä½¿ç”¨modern training methodçš„æ–°æ¶æ„ä¸ä½¿ç”¨dated methodsçš„è€ç½‘ç»œç›´æ¥å¯¹æ¯” we focus on the impact of training methods and scaling strategies training methodsï¼š We survey the modern training and regularization techniques å‘ç°å¼•å…¥å…¶ä»–æ­£åˆ™æ–¹æ³•çš„æ—¶å€™é™ä½ä¸€ç‚¹weight decayæœ‰å¥½å¤„ scaling strategiesï¼š We offer new perspectives and practical advice on scaling å¯èƒ½å‡ºç°è¿‡æ‹Ÿåˆçš„æ—¶å€™å°±åŠ depthï¼Œå¦åˆ™å…ˆåŠ å®½ resolutionæ…¢ç‚¹å¢é•¿ï¼Œmore slowly than prior works ä»accå›¾å¯ä»¥çœ‹åˆ°ï¼šæˆ‘ä»¬çš„scaling strategiesä¸ç½‘ç»œç»“æ„çš„lightweight changeæ­£äº¤ï¼Œæ˜¯additiveçš„ re-scaled ResNets, ResNet-RS ä»…improve training &amp; scaling strategyå°±èƒ½å¤§å¹…åº¦æ¶¨ç‚¹ combine minor architectural changesè¿›ä¸€æ­¥æ¶¨ç‚¹ æ–¹æ³• architecture use ResNet with two widely used architecture changes ResNet-D stemçš„7x7convæ¢æˆ3ä¸ª3x3conv stemçš„maxpoolingå»æ‰ï¼Œæ¯ä¸ªstageçš„é¦–ä¸ª3x3convè´Ÿè´£stride2 residual pathä¸Šå‰ä¸¤ä¸ªå·ç§¯çš„strideäº’æ¢ï¼ˆåœ¨3x3ä¸Šä¸‹é‡‡æ ·ï¼‰ id pathä¸Šçš„1x1 s2convæ›¿æ¢æˆ2x2 s2çš„avg pooling+1x1conv SE in bottleneck use se-ratio of 0.25 training methods match the efficientNet setup train for 350 epochs use cosine learning rate instead of exponential decay RandAugment instead of AutoAugment use Momentum optimizer instead of RMSProp regularization weight decay label smoothing dropout stochastic depth data augmentation we use RandAugment EfficientNet use AutoAugment which slightly outperforms RandAugment hyperï¼š droprate increase the regularization as the model size increase to limit overfitting label smoothing = 0.1 weight decay = 4e-5 improved training methods additive study æ€»ä½“ä¸Šçœ‹éƒ½æ˜¯additiveçš„ increase training epochsåœ¨æ·»åŠ regularization methodsçš„å‰æä¸‹æ‰ä¸hurtï¼Œå¦åˆ™ä¼šoverfitting dropoutåœ¨ä¸é™ä½weight decayçš„æƒ…å†µä¸‹ä¼šhurt weight decay å°‘é‡/æ²¡æœ‰regularization methodsçš„æƒ…å†µä¸‹ï¼šå¤§weight decayé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œ1e-4 å¤š/å¼ºregularization methodsçš„æƒ…å†µä¸‹ï¼šé€‚å½“å‡å°weight decayèƒ½æ¶¨ç‚¹ï¼Œ4e-5 improved scaling strategies search space width multiplierï¼š[0.25, 0.5, 1.0, 1.5, 2.0] depthï¼š[26, 50, 101, 200, 300, 350, 400] resolutionï¼š[128, 160, 224, 320, 448] increase regularization as model size increase observe 10/100/350 epoch regime we found that the best scaling strategies depends on training regime strategy1ï¼šscale depth Depth scaling outperforms width scaling for longer epoch regimes width scaling is preferable for shorter epoch regimes scaling widthå¯èƒ½ä¼šå¼•èµ·overfittingï¼Œæœ‰æ—¶å€™ä¼šhurt performance depth scalingå¼•å…¥çš„å‚æ•°é‡ä¹Ÿæ¯”widthå° strategy2ï¼šslow resolution scaling efficientNets/resNeSt lead to very large images our experimentsï¼šå¤§å¯ä¸å¿… å®éªŒ]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Receptive Field]]></title>
    <url>%2F2020%2F05%2F18%2FReceptive-Field%2F</url>
    <content type="text"><![CDATA[ç»¼è¿° æ„Ÿå—é‡ é™¤äº†å·ç§¯å’Œæ± åŒ–ï¼Œå…¶ä»–å±‚å¹¶ä¸å½±å“æ„Ÿå—é‡å¤§å° æ„Ÿå—é‡ä¸å·ç§¯æ ¸å°ºå¯¸kernel_sizeå’Œæ­¥é•¿strideæœ‰å…³ é€’å½’è®¡ç®—ï¼š N\_RF = kernel\_size + (cur\_RF-1)*stride å…¶ä¸­$cur_RF$æ˜¯å½“å‰å±‚ï¼ˆstart from 1ï¼‰ï¼Œ$kernel_size$ã€$stride$æ˜¯å½“å‰å±‚å‚æ•°ï¼Œ$N_RF$æ˜¯ä¸Šä¸€å±‚çš„æ„Ÿå—é‡ã€‚ æ„Ÿå—é‡è®¡ç®—å™¨ https://fomoro.com/research/article/receptive-field-calculator Understanding the Effective Receptive Field in Deep Convolutional Neural Networks åŠ¨æœº effective receptive field the effect of nonlinear activations, dropout, sub-sampling and skip connections on it è®ºç‚¹ it is critical for each output pixel to have a big receptive field, such that no important information is left out when making the prediction deeper networkï¼šincrease the receptive field size linearly Sub-samplingï¼šincreases the receptive field size multiplicatively it is easy to see that pixels at the center of a receptive field have a much larger impact on an outputï¼šå‰å‘ä¼ æ’­çš„æ—¶å€™ï¼Œä¸­é—´ä½ç½®çš„åƒç´ ç‚¹æœ‰æ›´å¤šæ¡pathé€šå‘output æ–¹æ³•çœ‹ä¸æ‡‚ç›´æ¥çœ‹ç»“è®º dropout does not change the Gaussian ERF shape Subsampling and dilated convolutions turn out to be effective ways to increase receptive field size quickly Skip-connections make ERFs smaller ERFs are Gaussian distributed uniformlyå’Œéšæœºåˆå§‹åŒ–éƒ½æ˜¯perfect Gaus- sian shapes åŠ ä¸Šéçº¿æ€§æ¿€æ´»å‡½æ•°ä»¥åæ˜¯near Gaussian shapes with different nonlinearities $\sqrt n$ absolute growth and $1/\sqrt n$ relative shrinkageï¼šRFæ˜¯éšç€layerçº¿æ€§å¢é•¿çš„ï¼ŒERFåœ¨logä¸Š0.56çš„æ–œç‡ï¼Œçº¦ç­‰äº$\sqrt n$ Subsampling &amp; dilated convolution increases receptive field The reference baseline is a convnet with 15 dense convolution layers Subsamplingï¼šreplace 3 of the 15 convolutional layers with stride-2 convolution dilatedï¼šreplace them with dilated convolution with factor 2,4 and 8ï¼Œrectangular ERF shape evolves during training as the networks learns, the ERF gets bigger, and at the end of training is significantly larger than the initial ERF classification 32*32 cifar 10 theoretical receptive field of our network is actually 74 Ã— 74 segmentation CamVid dataset the theoretical receptive field of the top convolutional layer units is quite big at 505 Ã— 505 å®é™…çš„ERFéƒ½å¾ˆå°ï¼Œéƒ½æ²¡åˆ°åŸå›¾å¤§å° increase the effective receptive field New Initializationï¼š makes the weights at the center of the convolution kernel to have a smaller scale, and the weights on the outside to be larger 30% speed-up of training å…¶ä»–æ•ˆæœä¸æ˜æ˜¾ Architecturalchanges sparsely connect each unit to a larger area dilated convolution or even not grid-like g]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SqueezeNet]]></title>
    <url>%2F2020%2F05%2F18%2FSqueezeNet%2F</url>
    <content type="text"><![CDATA[SQUEEZENET: ALEXNET-LEVEL ACCURACY WITH 50X FEWER PARAMETERS AND &lt;0.5MB MODEL SIZE åŠ¨æœº Smaller CNN achieve AlexNet-level accuracy model compression è®ºç‚¹ model compression SVD sparse matrix quantization (to 8 bits or less) CNN microarchitecture extensively 3x3 filters 1x1 filters higher level building blocks bypass connections automated designing approaches this paper eschew automated approaches propose and evaluate the SqueezeNet architecture with and without model compression explore the impact of design choices æ–¹æ³• architectural design strategy Replace 3x3 filters with 1x1 filters Decrease the number of input channels to 3x3 filters ï¼ˆsqueezeï¼‰ Downsample late in the network so that convolution layers have large activation mapsï¼šlarge activation maps (due to delayed downsampling) can lead to higher classification accuracy the fire module squeezeï¼š1x1 convs expandï¼šmix of 1x1 and 3x3 convs, same padding relu concatenate the SqueezeNet a standalone convolution layer (conv1) followed by 8 Fire modules (fire2-9) ending with a final conv layer (conv10) stride2 max-pooling after layers conv1, fire4, fire8, and conv10 dropout with a ratio of 50% is applied after the fire9 module GAP understand the impact each Fire module has three dimensional hyperparameters, to simplifyï¼š define $base_e$ï¼šthe number of expand filters in the first Fire module for layer iï¼š$e_i=base_e + (incr_e*[\frac{i}{freq}])$ expand ratio $pct_{3x3}$ï¼šthe percentage of 3x3 filters in expand layers squeeze ratio $SR$ï¼šthe number of filters in the squeeze layerï¼the number of filters in the expnad layer normal settingï¼š$base_e=128, incre_e=128, pct_{3x3}=0.5, freq=2, SR=0.125$ SR increasing SR leads to higher accuracy and larger model size Accuracy plateaus at 86.0% with SR=0.75 further increasing provides no improvement pct increasing pct leads to higher accuracy and larger model size Accuracy plateaus at 85.6% with pct=50% further increasing provides no improvement bypass Vanilla simple bypassï¼šwhen in &amp; out channels have the same dimensions complex bypassï¼šincludes a 1x1 convolution layer alleviate the representational bottleneck introduced by squeeze layers both yielded accuracy improvements simple bypass enabled higher accuracy]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hausdorff Distance]]></title>
    <url>%2F2020%2F05%2F14%2FHausdorff-Distance%2F</url>
    <content type="text"><![CDATA[Reducing the Hausdorff Distance in Medical Image Segmentation with Convolutional Neural Networks åŠ¨æœº novel loss function to reduce HD directly propose three methods 2D&amp;3Dï¼Œultra &amp; MR &amp; CT lead to approximately 18 âˆ’ 45% reduction in HD without degrading other segmentation performance criteria è®ºç‚¹ HD is one of the most informative and useful criteria because it is an indicator of the largest segmentation error current segmentation algorithms rarely aim at minimizing or reducing HD directly HD is determined solely by the largest error instead of the overall segmentation performance HDâ€˜s sensitivity to noise and outliers â€”&gt; modified version the optimization diffculty thus we propose an â€œHD- inspiredâ€ loss function æ–¹æ³• denotations probabilityï¼š$q$ binary maskï¼š$\bar p$ã€$\bar q$ boundaryï¼š$\delta p$ã€$\delta q$ single hdï¼š$hd(\bar p, \bar q)$ã€$hd(\bar q, \bar p)$ based on distance transforms distance map $d_p$ï¼šdefine the distance map as the unsigned distance to the boundary $\delta p$ DT_X[i,j] = min_{[k,l]\in X}d([i,j], [k,l]) è·ç¦»åœºå®šä¹‰ä¸ºï¼šæ¯ä¸ªç‚¹åˆ°ç›®æ ‡åŒºåŸŸ(X)çš„è·ç¦»çš„æœ€å°å€¼ HD based on DTï¼š hd_{DT}(\delta p, \delta q) = max((\bar p \triangle \bar q)\circ d_p)\\ \bar p \triangle \bar q = |\bar p - \bar q| finally haveï¼š HD_{DT}(\delta p, \delta q) = max(hd_{DT}(\delta p, \delta q), hd_{DT}(\delta q, \delta p)) modified loss version of HDï¼š Loss_{DT}(q,p) = \frac{1}{|\Omega|}\sum_{\Omega}((p-q)^2\circ(d_p^{\alpha}+d_q^{\alpha})) penalizely focus on areas instead of single point $\alpha$ determines how strongly we penalize larger errors use possibility instead of thresholded value use $(p-q)^2$ instead of $|p-q|$ correlations $HD_{DT}$ï¼šPearson correlation coefficient above 0.99 $Loss_{DT}$ï¼šPearson correlation coefficient above 0.93 drawback high computational cost especially in 3D $q$ changes along with training process thus $d_q$ changes while $d_p$ remains modified one-sided HD (OS)ï¼š Loss_{DT-OS}(q,p) = \frac{1}{|\Omega|}\sum_{\Omega}((p-q)^2\circ(d_p^{\alpha})) HD using Morphological Operations morphological erosionï¼š S \ominus B = \{z\in \Omega | B(z) \subseteq S\} è…èš€æ“ä½œå®šä¹‰ä¸ºï¼šåœ¨åŸå§‹äºŒå€¼åŒ–å›¾çš„å‰æ™¯åŒºåŸŸï¼Œä»¥æ¯ä¸ªåƒç´ ä¸ºä¸­å¿ƒç‚¹ï¼Œrun structure element block Bï¼Œå¦‚æœBå®Œå…¨åœ¨åŸå›¾å†…ï¼Œåˆ™å½“å‰ä¸­å¿ƒç‚¹åœ¨è…èš€åä¹Ÿæ˜¯å‰æ™¯ã€‚ HD based on erosionï¼š HD_{ER}(\delta p, \delta q)=2r^*\\ where\ r^* = min_r \{(\bar p \triangle \bar q) \ominus B_r = \varnothing\} $HD_{ER}$ is a lower bound of the true value can be computed more efficiently using convolutional operations modifid loss versionï¼š Loss_{ER}(q,p) = \frac{1}{|\Omega|}\sum_k \sum_{\Omega}((p-q)^2 \ominus_k B)k^{\alpha} k successive erosions cross-shaped kernel whose elements sum to one followed by a soft thresholding at 0.50 correlations $HD_{ER}$ï¼šPearson correlation coefficient above 0.91 $Loss_{ER}$ï¼šPearson correlation coefficient above 0.83 HD using circular-shaped convolutional kernel circular-shaped kernel HD based on circular-shaped kernelï¼š hd_{CV}(\delta p, \delta q)=max(r_1, r_2)\\ where \ r_1=max_r (max_{\Omega}f_h(\bar p ^C * B_r)\circ(\bar q \backslash \bar p))\\ where \ r_2=max_r (max_{\Omega}f_h(\bar p * B_r)\circ(\bar p \backslash \bar q))\\ $\bar p^C$ï¼šcomplement è¡¥é›† $f_h$ï¼šhard thresholding setting all values below 1 to zero modified loss versionï¼š Loss_{CV}(q,p)=\frac{1}{|\Omega|}\sum_{r\in R}r^{\alpha}\sum_{\Omega}[f_s(Br*\bar p^C)\circ f_{\bar q\backslash \bar p} + f_s(B_r * \bar p) \circ f_{\bar p \backslash \bar q}\\ +f_s(Br*\bar q^C)\circ f_{\bar p\backslash \bar q} + f_s(B_r * \bar q) \circ f_{\bar q \backslash \bar p}] soft thresholding f_{\bar p\backslash \bar q} = (p-q)^2*p correlations $HD_{CV}$ï¼šPearson correlation coefficient above 0.99 $Loss_{CV}$ï¼šPearson correlation coefficient above 0.88 computationï¼š kernel size $HD_{ER}$ is computed using small fixed convolutional kernels (of size 3) $Loss_{CV}$ require applying filters of increasing size(we use a maximum kernel radius of 18 pixels in 2D and 9 voxels in 3D) steps choose R based on the expected range of segmentation errors set R = {3, 6, . . . 18} for 2D images and R = {3,6,9} for 3D training standard Unet augment our HD-based loss term with a DSC loss term for more stable training reweight both loss after every epoch d]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SE block]]></title>
    <url>%2F2020%2F04%2F30%2FSE-block%2F</url>
    <content type="text"><![CDATA[ç»¼è¿°å›¾åƒç‰¹å¾çš„æå–èƒ½åŠ›æ˜¯CNNçš„æ ¸å¿ƒèƒ½åŠ›ï¼Œè€ŒSE blockå¯ä»¥èµ·åˆ°ä¸ºCNNæ ¡å‡†é‡‡æ ·çš„ä½œç”¨ã€‚ æ ¹æ®æ„Ÿå—é‡ç†è®ºï¼Œç‰¹å¾çŸ©é˜µä¸»è¦æ¥è‡ªäºæ ·æœ¬çš„ä¸­å¤®åŒºåŸŸï¼Œå¤„åœ¨è¾¹ç¼˜ä½ç½®çš„é…’ç“¶çš„å›¾åƒç‰¹å¾å¾ˆå¤§æ¦‚ç‡ä¼šè¢«poolingå±‚æŠ›å¼ƒæ‰ã€‚è€ŒSE blockçš„åŠ å…¥å°±å¯ä»¥é€šè¿‡æ¥è°ƒæ•´ç‰¹å¾çŸ©é˜µï¼Œå¢å¼ºé…’ç“¶ç‰¹å¾çš„æ¯”é‡ï¼Œæé«˜å®ƒçš„è¯†åˆ«æ¦‚ç‡ã€‚ [SE-Net] Squeeze-and-Excitation Networks [SC-SE] Concurrent Spatial and Channel â€˜Squeeze &amp; Excitationâ€™ in Fully Convolutional Networks [CMPE-SE] Competitive Inner-Imaging Squeeze and Excitation for Residual Network SENet: Squeeze-and-Excitation Networks åŠ¨æœº prior research has investigated the spatial component to achieve more powerful representations we focus on the channel relationship instead SE-blockï¼šadaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels enhancing the representational power in a computationally efficient manner è®ºç‚¹ stronger networkï¼š deeper NiN-like bocks cross-channel correlations in prior work mapped as new combinations of features through 1x1 conv concentrated on the objective of reducing model and computational complexity In contrast, we found this mechanism can ease the learning process and significantly enhance the representational power of the network Attention Attention can be interpreted as a means of biasing the allocation of available computational resources towards the most informative components Some works provide interesting studies into the combined use of spatial and channel attention æ–¹æ³• SE-block The channel relationships modelled by convolution are inherently implicit and local we would like to provide it with access to global information squeezeï¼šusing global average pooling excitationï¼šnonlinear &amp; non-mutually-exclusive using sigmoid bottleneckï¼ša dimensionality-reduction layer $W_1$ with reduction ratio $r$ and ReLU and a dimensionality-increasing layer $W_2$ $s = F_{ex}(z,W) = \sigma (W_2 \delta(W_1 z))$ integration insert after the non-linearity following each convolution inceptionï¼štake the transformation $F_{tr}$ to be an entire Inception module residualï¼štake the transformation $F_{tr}$ to be the non-identity branch of a residual module model and computational complexity ResNet50 vs. SE-ResNet50ï¼š0.26% relative increase GFLOPs approaching ResNet10â€™s accuracy the additional parameters result solely from the two FC layers, among which the final stage FC claims the majority due to being performed across the greatest number of channels the costly final stage of SE blocks could be removed at only a small cost in performance ablations FC removing the biases of the FC layers in the excitation facilitates the modelling of channel dependencies reduction ratio performance is robust to a range of reduction ratios In practice, using an identical ratio throughout a network may not be optimal due to the distinct roles performed by different layers squeeze global average pooling vs. global max poolingï¼šaverage pooling slightly better excitation Sigmoid vs. ReLU vs. tanhï¼š tanhï¼šslightly worse ReLUï¼šdramatically worse stages each stages brings benefits combination make even better integration strategy fairly robust to their location, provided that they are applied prior to branch aggregation inside the residual unitï¼šfewer channels, fewer parameters, comparable accuracy primitive understanding squeeze the use of global information has a significant influence on the model performance excitation the distribution across different classes is very similar at the earlier layers (general features) the value of each channel becomes much more class-specific at greater depth SE_5_2 exhibits an interesting tendency towards a saturated state in which most of the activations are close to one SE_5_3 exhibits a similar pattern emerges over different classes, up to a modest change in scale suggesting that SE_5_2 and SE_5_3 are less important than previous blocks in providing recalibration to the network (thus can be removed) APPENDIX åœ¨ImageNetä¸ŠSOTAçš„æ¨¡å‹æ˜¯SENet-154ï¼Œtop1-erræ˜¯18.68ï¼Œè¢«æ ‡è®°åœ¨äº†efficientNetè®ºæ–‡çš„æŠ˜çº¿å›¾ä¸Š SE-ResNeXt-152ï¼ˆ64x4dï¼‰ input=(224,224)ï¼štop1-erræ˜¯18.68 input=320/299ï¼štop1-erræ˜¯17.28 further difference each bottleneck building blockçš„ç¬¬ä¸€ä¸ª1x1 convsçš„é€šé“æ•°å‡åŠ stemçš„ç¬¬ä¸€ä¸ª7x7convæ¢æˆäº†3ä¸ªè¿ç»­çš„3x3 conv 1x1çš„s2 convæ¢æˆäº†3x3çš„s2 conv fcä¹‹å‰æ·»åŠ dropout layer label smoothing æœ€åå‡ ä¸ªtraining epochå°†BNå±‚çš„å‚æ•°å†»ä½ï¼Œä¿è¯è®­ç»ƒå’Œæµ‹è¯•çš„å‚æ•°ä¸€è‡´ 64 GPUsï¼Œbatch size=2048ï¼ˆ32 per GPUï¼‰ initial lr=1.0 SC-SE: Concurrent Spatial and Channel â€˜Squeeze &amp; Excitationâ€™ in Fully Convolutional Networks åŠ¨æœº image segmentation task ä¸Šé¢SE-Netæå‡ºæ¥ä¸»è¦æ˜¯é’ˆå¯¹åˆ†ç±» three variants of SE modules squeezing spatially and exciting channel-wise (cSE) squeezing channel-wise and exciting spatially (sSE) concurrent spatial and channel squeeze &amp; excitation (scSE) integrate within three different state-of-the- art F-CNNs (DenseNet, SD-Net, U-Net) è®ºç‚¹ F-CNNs have become the tool of choice for many image segmentation tasks coreï¼šconvolutions that capturing local spatial pattern along all input channels jointly SE block factors out the spatial dependency by global average pooling to learn a channel specific descriptor (later refered to as cSE /channel-SE) while for image segmentation, we hypothesize that the pixel-wise spatial information is more informative thus we propose sSE(spatial SE) and scSE(spatial and channel SE) can be seamlessly integrated by placing after every encoder and decoder block æ–¹æ³• cSE GAPï¼šembeds the global spatial information into a vector FC-ReLU-FC-Sigmoidï¼šadaptively learns the importance recalibrate sSE 1x1 convï¼šgenerating a projection tensor representing the linearly combined representation for all channels C for a spatial location (i,j) Sigmoidï¼šrescale recalibrate scSE by element-wise addition encourages the network to learn more meaningful feature maps â€”â€”â€”- relevant both spatially and channel-wise å®éªŒ F-CNN architecturesï¼š 4 encoder blocks, one bottleneck layer, 4 decoder blocks and a classification layer class imbalanceï¼šmedian frequency balancing ce dice cmpï¼šscSE &gt; sSE &gt; cSE &gt; vanilla å°åŒºåŸŸç±»åˆ«çš„åˆ†å‰²ï¼Œè§‚å¯Ÿåˆ°ä½¿ç”¨cSEå¯èƒ½ä¼šå·®äºvanillaï¼š might have got overlooked by only exciting the channels å®šæ€§åˆ†æï¼š ä¸€äº›under segmentedçš„åœ°æ–¹ï¼ŒscSE improves with the inclusion ä¸€äº›over segmentedçš„åœ°æ–¹ï¼ŒscSE rectified the result Competitive Inner-Imaging Squeeze and Excitation for Residual Network åŠ¨æœº for residual network the residual architecture has been proved to be diverse and redundant model the competition between residual and identity mappings make the identity flow to control the complement of the residual feature maps è®ºç‚¹ For analysis of ResNet, with the increase in depth, the residual network exhibits a certain amount of redundancy with the CMPE-SE mechanism, it makes residual mappings tend to provide more efficient supplementary for identity mappings æ–¹æ³• ä¸»è¦æå‡ºäº†ä¸‰ç§å˜ä½“ï¼š ç¬¬ä¸€ä¸ªå˜ä½“ï¼š ä¸¤ä¸ªåˆ†æ”¯idå’Œresåˆ†åˆ«GAPå‡ºä¸€ä¸ªvectorï¼Œç„¶åfc reduct by ratioï¼Œç„¶åconcatï¼Œç„¶åchannel back Implicitly, we can believe that the winning of the identity channels in this competition results in less weights of the residual channels ç¬¬äºŒä¸ªå˜ä½“ï¼š ä¸¤ç§æ–¹æ¡ˆ 2x1 convsï¼šå¯¹ä¸Šä¸‹ç›¸åº”ä½ç½®çš„å…ƒç´ æ±‚avg 1x1 convsï¼šå¯¹å…¨éƒ¨å…ƒç´ æ±‚avgï¼Œç„¶åflatten ç¬¬ä¸‰ä¸ªå˜ä½“ï¼š ä¸¤è¾¹çš„channel-wise vectorå èµ·æ¥ï¼Œç„¶åreshapeæˆçŸ©é˜µå½¢å¼ï¼Œç„¶å3x3 convï¼Œç„¶åflatten æ¯”è¾ƒæ‰¯ï¼Œä¸æµªè´¹æ—¶é—´åˆ†æäº†ã€‚]]></content>
      <tags>
        <tag>papers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cv2&numpy&tobeadded]]></title>
    <url>%2F2020%2F04%2F19%2Fcv2-numpy-tobeadded%2F</url>
    <content type="text"><![CDATA[çŸ©é˜µä¹˜æ³• np.dot(A,B)ï¼šçœŸæ­£çš„çŸ©é˜µä¹˜æ³• np.multiply(A,B) &amp; npé‡è½½çš„*ï¼šelement-wise productï¼ŒçŸ©é˜µä¸­å¯¹åº”å…ƒç´ ç›¸ä¹˜ cvçš„A.dot(B) &amp; cvé‡è½½çš„*ï¼šçœŸæ­£çš„çŸ©é˜µä¹˜æ³• cvçš„A.mul(B) ï¼šelement-wise productï¼ŒçŸ©é˜µä¸­å¯¹åº”å…ƒç´ ç›¸ä¹˜ å›¾åƒæ—‹è½¬ é€šè¿‡ä»¿å°„çŸ©é˜µcv2.getRotationMatrix2Då’Œä»¿å°„å˜æ¢å‡½æ•°cv2.warpAffineæ¥å®ç° srcï¼šè¾“å…¥å›¾åƒ Mï¼šå˜æ¢çŸ©é˜µ dsizeï¼šè¾“å‡ºå›¾åƒçš„å¤§å°ï¼ˆåŸºäºå›¾åƒåŸç‚¹è£å‰ªï¼‰ flagsï¼šæ’å€¼æ–¹æ³• borderModeï¼šè¾¹ç•Œåƒç´ æ¨¡å¼ borderValueï¼šè¾¹ç•Œå¡«å……å€¼ï¼Œé»˜è®¤ä¸º0 cv2.getRotationMatrix2D(center, angle, scale)ï¼šè¿”å›ä¸€ä¸ª2x3çš„å˜æ¢çŸ©é˜µ centerï¼šæ—‹è½¬ä¸­å¿ƒ angleï¼šæ—‹è½¬è§’åº¦ï¼Œæ­£å€¼æ˜¯é€†æ—¶é’ˆæ—‹è½¬ scaleï¼šç¼©æ”¾å› å­ cv2.warpAffine(src, M, dsize[, dst[, flags[, borderMode[, borderValue]]]]))ï¼šè¿”å›å˜æ¢åçš„å›¾åƒ srcï¼šè¾“å…¥å›¾åƒ Mï¼šå˜æ¢çŸ©é˜µ dsizeï¼šè¾“å‡ºå›¾åƒçš„å¤§å°ï¼ˆåŸºäºå›¾åƒåŸç‚¹è£å‰ªï¼‰ flagsï¼šæ’å€¼æ–¹æ³• borderModeï¼šè¾¹ç•Œåƒç´ æ¨¡å¼ borderValueï¼šè¾¹ç•Œå¡«å……å€¼ï¼Œé»˜è®¤ä¸º0 12345678910111213141516171819def rotate_img(angle, img, interpolation=cv2.INTER_LINEAR, points=[]): h, w = img.shape rotataMat = cv2.getRotationMatrix2D((w/2, h/2), math.degrees(angle), 1) # rotate_img1: è¾“å‡ºå›¾åƒå°ºå¯¸ä¸å˜ï¼Œè¶…å‡ºåŸå›¾åƒéƒ¨åˆ†è¢«cutæ‰ rotate_img1 = cv2.warpAffine(img, rotataMat, dsize=(w, h), flags=interpolation, borderMode=cv2.BORDER_CONSTANT, borderValue=0) # rotate_img2: è¾“å‡ºå›¾åƒå°ºå¯¸å˜å¤§ï¼Œä¿ç•™è¶…å‡ºåŸå›¾åƒéƒ¨åˆ†ï¼Œæ–°çš„åæ ‡åŸç‚¹ä¿è¯æ—‹è½¬ä¸­å¿ƒä»æ—§ä½äºå›¾åƒä¸­å¿ƒ new_h = int(w*math.fabs(math.sin(angle)) + h*math.fabs(math.cos(angle))) new_w = int(h*math.fabs(math.sin(angle)) + w*math.fabs(math.cos(angle))) rotataMat[0, 2] += (new_w - w) / 2 rotataMat[1, 2] += (new_h - h) / 2 rotate_img2 = cv2.warpAffine(img, rotataMat, dsize=(new_w, new_h), flags=interpolation, borderMode=cv2.BORDER_CONSTANT, borderValue=0) # åæ ‡ç‚¹çš„å˜æ¢ rotated_points = [] for point in points: point = rotataMat.dot([[point[0]], [point[1]], [1]]) rotated_points.append((int(point[0]), int(point[1]))) return rotate_img2, rotated_points ä½¿ç”¨tipsï¼š å¦‚æœä¸ä¿®æ”¹ä»¿å°„å˜æ¢çŸ©é˜µçš„å¹³ç§»å‚æ•°ï¼Œåæ ‡åŸç‚¹çš„ä½ç½®ä¸å‘ç”Ÿæ”¹å˜ dsizeæŒ‡å®šçš„è¾“å‡ºå›¾åƒæ˜¯ä»åŸç‚¹ä½ç½®å¼€å§‹è£å‰ª åæ ‡ç‚¹çš„å˜æ¢æ»¡è¶³å…¬å¼ï¼š dst(x,y) = src(M_{11}x+M_{12}y+M_{13}, M_{21}x+M_{22}y+M_{23}) np.meshgrid(*xi,**kwargs) è¿™ä¸ªå‡½æ•°ç¥ä»–å¦ˆå‘ï¼Œä½œç”¨æ˜¯Return coordinate matrices from coordinate vectors. Make N-D coordinate arrays for vectorized evaluations of N-D scalar/vector fields over N-D grids, given one-dimensional coordinate arrays x1, x2,â€¦, xn. ä½†æ˜¯å°è¯•ä¸€ä¸‹ä¼šå‘ç°ï¼š 12345x = np.arange(0,10,1)y = np.arange(0,20,1)z = np.arange(0,30,1)x, y, z= np.meshgrid(x, y, z)print(x.shape) # (20, 10, 30) xyè½´åæ ‡æ˜¯åè¿‡æ¥çš„ï¼Œè¿™æ˜¯å› ä¸ºoptional argsé‡Œé¢æœ‰ä¸€ä¸ªindexingï¼š indexing : {â€˜xyâ€™, â€˜ijâ€™}, Cartesian (â€˜xyâ€™, default) or matrix (â€˜ijâ€™) indexing of output. æˆ‘ä»¬æƒ³è¦å¾—åˆ°çš„åæ ‡ç³»å’Œè¾“å…¥çš„è½´ä¸€ä¸€å¯¹åº”ï¼Œå¾—æŒ‡å®šå‚æ•°indexing=&#39;ij&#39; 12345x = np.arange(0,10,1)y = np.arange(0,20,1)z = np.arange(0,30,1)x, y, z= np.meshgrid(x, y, z, indexing='ij')print(x.shape) # (10, 20, 30) è¿˜æœ‰ä¸€ä¸ªå‚æ•°sparseï¼Œå› ä¸ºæ¯æ ¹è½´çš„åæ ‡éƒ½æ˜¯å¤åˆ¶çš„ï¼Œæ‰€ä»¥å¯ä»¥ç¨€ç–å­˜å‚¨ï¼Œæ­¤æ—¶å‡½æ•°è¿”å›å€¼å˜åŒ–ï¼š sparse : bool, If True a sparse grid is returned in order to conserve memory. Default is False. 12345678910x = np.arange(0,10,1)y = np.arange(0,20,1)xx, yy = np.meshgrid(x, y)print(xx) # a 20x10 listxx, yy = np.meshgrid(x, y, sparse=True)print(xx) # a 1*10 listprint(yy) # a 20*1 list# æ‰€ä»¥æ•´ä½“ä¸Šè¿˜æ˜¯ä¸ª20*10çš„çŸ©é˜µ äºŒç»´å¯è§†åŒ–ï¼š 12345import matplotlib.pyplot as pltz = xx**2 + yy**2 # xxå’Œyyæ—¢å¯ä»¥æ˜¯dense convervationä¹Ÿå¯ä»¥æ˜¯sparse convervationh = plt.contourf(x,y,z)plt.show() np.tile(A,reps) è¿™ä¸ªå‡½æ•°æŒºæœ‰ç”¨çš„ï¼ŒæŠŠæ•°ç»„æ²¿ç€æŒ‡å®šç»´åº¦å¤åˆ¶ï¼Œæ¯”stackã€concatå•¥çš„éƒ½ä¼˜é›…ï¼Œèƒ½è‡ªåŠ¨åˆ›å»ºæ–°çš„ç»´åº¦ Aï¼šarray_like, The input array. repsï¼šarray_like, The number of repetitions of A along each axis. np.reshape(a, newshape, order=â€™Câ€™) è¿™ä¸ªå‡½æ•°è´¼å¸¸ç”¨ï¼Œä½†æ˜¯ä¸€èˆ¬ç”¨äºäºŒç»´çš„æ—¶å€™æ²¡è€ƒè™‘é‡ç»„é¡ºåºè¿™ä»¶äº‹ order: {â€˜Câ€™, â€˜Fâ€™, â€˜Aâ€™}, optionalï¼Œç®€å•ç†è§£ï¼Œreshapeçš„é€šç”¨å®ç°æ–¹å¼æ˜¯å…ˆå°†çœŸä¸ªarrayæ‹‰ç›´ï¼Œç„¶åä¾æ¬¡å–æ•°æ®å¡«å…¥æŒ‡å®šç»´åº¦ï¼ŒCæ˜¯ä»æœ€é‡Œé¢çš„ç»´åº¦å¼€å§‹æ‹‰ç›´&amp;æ„é€ ï¼ŒFæ˜¯ä»æœ€å¤–é¢çš„ç»´åº¦å¼€å§‹æ‹‰ç›´&amp;æ„é€ ï¼ŒA for auto 123456789101112a = np.arange(6)array([0, 1, 2, 3, 4, 5])# C-like index orderingnp.reshape(a, (2, 3))array([[0, 1, 2], [3, 4, 5]])# Fortran-like index orderingnp.reshape(a, (2, 3), order='F')array([[0, 4, 3], [2, 1, 5]]) tfå’Œkerasé‡Œé¢ä¹Ÿæœ‰reshapeï¼Œæ˜¯æ²¡æœ‰orderå‚æ•°çš„ï¼Œé»˜è®¤æ˜¯â€™Câ€™]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MobileNets]]></title>
    <url>%2F2020%2F04%2F16%2FMobileNets%2F</url>
    <content type="text"><![CDATA[preview åŠ¨æœº è®¡ç®—åŠ›æœ‰é™ æ¨¡å‹å‹ç¼©ï¼ä½¿ç”¨å°æ¨¡å‹ æ·±åº¦å¯åˆ†ç¦»å·ç§¯ Depthwise Separable Convolution å°†æ ‡å‡†å·ç§¯æ‹†åˆ†ä¸ºä¸¤ä¸ªæ“ä½œï¼šæ·±åº¦å·ç§¯(depthwise convolution) å’Œé€ç‚¹å·ç§¯(pointwise convolution) æ ‡å‡†å·ç§¯ï¼šå‚æ•°é‡k*k*input_channel*output_channel æ·±åº¦å·ç§¯(depthwise convolution) ï¼šé’ˆå¯¹æ¯ä¸ªè¾“å…¥é€šé“é‡‡ç”¨ä¸åŒçš„å·ç§¯æ ¸ï¼Œå‚æ•°é‡k*k*input_channel é€ç‚¹å·ç§¯(pointwise convolution)ï¼šå°±æ˜¯æ™®é€šçš„å·ç§¯ï¼Œåªä¸è¿‡å…¶é‡‡ç”¨1x1çš„å·ç§¯æ ¸ï¼Œå‚æ•°é‡1*1*input_channel*output_channel with BN and ReLUï¼š DWæ²¡æœ‰æ”¹å˜é€šé“æ•°çš„èƒ½åŠ›ï¼Œå¦‚æœè¾“å…¥å±‚çš„é€šé“æ•°å¾ˆå°‘ï¼ŒDWä¹Ÿåªèƒ½åœ¨ä½ç»´ç©ºé—´æç‰¹å¾ï¼Œå› æ­¤V2æå‡ºå…ˆå¯¹åŸå§‹è¾“å…¥åšexpansionï¼Œç”¨ä¸€ä¸ªéçº¿æ€§PWå‡ç»´ï¼Œç„¶åDWï¼Œç„¶åå†ä½¿ç”¨ä¸€ä¸ªPWé™ç»´ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç¬¬äºŒä¸ªPWä¸ä½¿ç”¨éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œå› ä¸ºä½œè€…è®¤ä¸ºï¼Œreluä½œç”¨åœ¨ä½ç»´ç©ºé—´ä¸Šä¼šå¯¼è‡´ä¿¡æ¯æŸå¤±ã€‚ è¿›ä¸€æ­¥ç¼©å‡è®¡ç®—é‡ é€šé“æ•°ç¼©å‡ï¼šå®½åº¦å› å­ alpha åˆ†è¾¨ç‡ç¼©å‡ï¼šåˆ†è¾¨ç‡å› å­rho papers [V1] MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applicationsï¼Œä¸»è¦è´¡çŒ®Depthwise Separable Convolution [V2] MobileNetV2: Inverted Residuals and Linear Bottlenecksï¼Œä¸»è¦è´¡çŒ®inverted residual with linear bottleneck [V3] Searching for MobileNetV3ï¼Œæ¨¡å‹ç»“æ„å‡çº§(inverted-res-block + SE-block)ï¼Œé€šè¿‡NASï¼Œè€Œéæ‰‹åŠ¨è®¾è®¡ MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications åŠ¨æœº efficient modelsï¼šuses depthwise separable convolutions and two simple global hyper-parameters resource and accuracy tradeoffs a class of network architectures that allows a model developer to specifically choose a small network that matches the resource restrictions (latency, size) for their application è®ºç‚¹ï¼š the general trend has been to make deeper and more complicated networks in order to achieve higher accuracy not efficient on computationally limited platform building small and efficient neural networksï¼šeither compressing pretrained networks or training small networks directly Many papers on small networks focus only on size but do not consider speed speed &amp; size ä¸å®Œå…¨å¯¹ç­‰ sizeï¼šdepthwise separable convolutions, bottleneck approaches, compressing pretrained networks, distillation æ–¹æ³• depthwise separable convolutions a form of factorized convolutionsï¼ša standard conv splits into 2 layers factorize the filtering and combination steps of standard conv drastically reducing computation and model size to $\frac{1}{N} + \frac{1}{D_k^2}$ use both batchnorm and ReLU nonlinearities for both layers MobileNet uses 3 Ã— 3 depthwise separable convolutions which bring between 8 to 9 times less computation MobileNet the first layer is a full convolution, the rest depthwise separable convolutions down sampling is handled with strided convolution all layers are followed by a BN and ReLU nonlinearity a final average pooling reduces the spatial resolution to 1 before the fully connected layer. the final fully connected layer has no nonlinearity and feeds into a softmax layer for classification training so few parameters RMSprop less regularization and data augmentation techniques because small models have less trouble with overfitting it was important to put very little or no weight decay (l2 regularization) do not use side heads or label smoothing or image distortions Width Multiplier: Thinner Models thin a network uniformly at each layer the input channels $M$ and output channels $N$ becomes $\alpha M$ and $\alpha N$ $\alpha=1$ï¼šbaseline MobileNet $\alpha&lt;1$ï¼šreduced MobileNet reduce the parameters roughly by $\alpha^2$ Resolution Multiplier: Reduced Representation apply this to the input image the input resolution of the network is typically 224, 192, 160 or 128 $\rho=1$ï¼šbaseline MobileNet $\rho&lt;1$ï¼šreduced MobileNet reduce the parameters roughly by $\rho^2$ ç»“è®º using depthwise separable convolutions compared to full convolutions only reduces accuracy by 1% on ImageNet but saving tremendously on mult-adds and parameters at similar computation and number of parameters, thinner MobileNets is 3% better than making them shallower trade-offs based on the two hyper-parameters MobileNetV2: Inverted Residuals and Linear Bottlenecks åŠ¨æœº a new mobile architecture based on an inverted residual structure remove non-linearities in the narrow layers in order to maintain representational power prove on multiple tasks object detectionï¼šSSDLite semantic segmentationï¼šMobile DeepLabv3 æ–¹æ³• Depthwise Separable Convolutions replace a full convolutional opera- tor with a factorized version depthwise convolution, it performs lightweight filtering per input channel pointwise convolution, computing linear combinations of the input channels Linear Bottlenecks ReLU results in information loss in lower dimension space expansion ratioï¼šif we have lots of channels, information might still be preserved in the other channels linearï¼šbottleneckä¸Šé¢ä¸åŒ…å«éçº¿æ€§æ¿€æ´»å•å…ƒ Inverted residuals bottlenecks actually contain all the necessary information expansion layer acts merely as an implementation detail that accompanies a non-linear transformation parameter countï¼š basic building block is a bottleneck depth-separable convolution with residuals * interpretation * provides a natural separation between the input/output * expansionï¼šcapacity * layer transformationï¼šexpressiveness * MobileNetV2 model architecture * initial filtersï¼š32 * ReLU6ï¼šuse ReLU6 as the non-linearity because of its robustness when used with low-precision computation * use constant expansion rate between 5 and 10 except the 1stï¼šsmaller network inclines smaller and larger larger &lt;img src=&quot;MobileNets/MobileNetV2.png&quot; width=&quot;40%&quot; /&gt; * comparison with other architectures &lt;img src=&quot;MobileNets/cmpV2.png&quot; width=&quot;40%&quot; /&gt; å®éªŒ Object Detection evaluate the performance as feature extractors replace all the regular convolutions with separable convolutions in SSD prediction layersï¼šbackboneæ²¡æœ‰æ”¹åŠ¨ï¼Œåªæ›¿æ¢å¤´éƒ¨çš„å·ç§¯ï¼Œé™ä½è®¡ç®—é‡ achieves competitive accuracy with significantly fewer parameters and smaller computational complexity Semantic Segmentation build DeepLabv3 heads on top of the second last feature map of MobileNetV2 DeepLabv3 heads are computationally expensive and removing the ASPP module significantly reduces the MAdds ablation inverted residual connectionsï¼šshortcut connecting bottleneck perform better than shortcuts connecting the expanded layers åœ¨å°‘é€šé“çš„ç‰¹å¾ä¸Šè¿›è¡ŒçŸ­è¿æ¥ linear bottlenecksï¼šlinear bottlenecks improve performance, providing support that non-linearity destroys information in low-dimensional space Searching for MobileNetV3 åŠ¨æœº automated search algorithms and network design work together classification &amp; detection &amp; segmentation a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP) new efficient versions of nonlinearities è®ºç‚¹ reducing the number of parameters the number of operations (MAdds) inference latency related work SqueezeNetï¼š1x1 convolutions MobileNetV1ï¼šseparable convolution MobileNetV2ï¼šinverted residuals ShuffleNetï¼šgroup convolutions CondenseNetï¼šgroup convolutions ShiftNetï¼šshift operation MnasNetï¼šMobileNetV2+SE-blockï¼Œattention modules are placed after the depthwise filters in the expansion æ–¹æ³• base blocks combination of ideas from [MobileNetV1, MobileNetV2, MnasNet] inverted-res-block + SE-block swish nonlinearity hard sigmoid Network Search use platform-aware NAS to search for the global network structures use the NetAdapt algorithm to search per layer for the number of filters Network Improvements redesign the computionally-expensive layers at the beginning and the end of the network the last block of MobileNetV2â€™s inverted bottleneck structure move this layer past the final average poolingï¼šç§»åŠ¨åˆ°GAPåé¢å»ï¼Œä½œç”¨åœ¨1x1çš„featuremapä¸Šinstead of 7x7ï¼Œæ›²çº¿æ•‘å›½ a new nonlinearity, h-swish the initial set of filters are also expensiveï¼šusually start with 32 filters in a full 3x3 convolution to build initial filter banks for edge detection reduce the number of filters to 16 and use the hard swish nonlinearity swish\ [x]=x*\sigma(x)\\ h\_swish\ [x]=x\frac{ReLU6(x+3)}{6} most of the benefits swish are realized by using them only in the deeper layersï¼šåªåœ¨ååŠæ®µç½‘ç»œä¸­ç”¨ SE-block ratioï¼šall to fixed to be 1/4 of the number of channels in expansion layer MobileNetV3 architecture å®éªŒ Detection use MobileNetV3 as replacement for the backbone feature extractor in SSDLiteï¼šæ”¹åšbackboneäº† reduce the channel counts of C4&amp;C5â€™s blockï¼šå› ä¸ºMobileNetV3åŸæœ¬æ˜¯è¢«ç”¨æ¥è¾“å‡º1000ç±»çš„ï¼Œtransferåˆ°90ç±»çš„cocoæ•°æ®é›†ä¸Šæœ‰äº›redundant Segmentation as network backbone compare two segmentation heads R-ASPPï¼šreduced design of the Atrous Spatial Pyramid Pooling module with only two branches Lite R-ASPPï¼šç±»SE-blockçš„è®¾è®¡ï¼Œå¤§å·ç§¯æ ¸ï¼Œå¤§æ­¥é•¿]]></content>
      <tags>
        <tag>papers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[verseg]]></title>
    <url>%2F2020%2F04%2F15%2Fverseg%2F</url>
    <content type="text"><![CDATA[challenge Large Scale Vertebrae Segmentation Challenge task1:Vertebra Labellingï¼Œå…³é”®ç‚¹æ£€æµ‹ task2:Vertebra Segmentationï¼Œå¤šç±»åˆ«åˆ†å‰² data variationï¼šæ•°æ®affineè½´ä¸ç»Ÿä¸€ï¼Œå°ºå¯¸ä¸ç»Ÿä¸€ï¼Œæ‰«æèŒƒå›´ä¸ç»Ÿä¸€ï¼ŒFOVåŒºåŸŸä¸ç»Ÿä¸€ niiçš„ä¸¤å¤§è§£æå·¥å…·ï¼šnibabelåº“load dataçš„xyzé¡ºåºä¸axcodeçš„é¡ºåºä¸€è‡´ï¼Œe.g.[â€˜Râ€™,â€™Aâ€™,â€™Sâ€™]çš„orientationä¼šå¾—åˆ°xyzçš„arrayï¼Œè€Œsitkçš„è¯»å–åˆšå¥½åè¿‡æ¥ï¼Œsitkçš„arrä¼šæ˜¯zyxã€‚æˆ‘ä»¬ä¹‹å‰åœ¨å°†dicomå†™å…¥niiæ—¶ï¼Œä¼šæŒ‡å®šä¸€ä¸ªä¸ä¸ºnp.eye(4)çš„affineï¼Œå°±æ˜¯ä¸ºäº†transposeè¿™ä¸‰ä¸ªè½´ã€‚ model team paper \ ä¸‰é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µï¼Œdue to large variation FOV of the datasetï¼Œç²—åˆ†å‰²å®šä½è„ŠæŸ±ä½ç½®ï¼Œç¬¬äºŒé˜¶æ®µï¼Œhigher resolutionå¤šç±»åˆ«å…³é”®ç‚¹å®šä½centerï¼Œè·å¾—each located vertebraï¼Œç¬¬ä¸‰é˜¶æ®µï¼ŒäºŒç±»åˆ†å‰²for each located vertebraã€‚ keywordsï¼š1. uniform voxel spacingï¼šä¸è¦éšæ„resizeï¼Œtodo: trilinear interpï¼›2. on-the-fly data augmentationï¼šusing SimpleITK ç¬¬ä¸€é˜¶æ®µï¼šSpine Localization Unet regress the Gaussian heatmap of spinal centerline L2-loss uniform voxel spacing of 8mm input shapeï¼š[64,64,128]ï¼Œpadï¼Ÿ ç¬¬äºŒé˜¶æ®µï¼šVertebrae Localization SpatialConfiguration-Net regress each located vertebraâ€˜s heatmap in individual channel resamplingï¼šbi/tricubic interpolation normï¼šmaxmin on the whole dataset uniform voxel spacing of 2mm input shapeï¼š[96,96,128]ï¼Œz-axis random cropï¼Œxy-plane use ROI from stage1 ç¬¬ä¸‰é˜¶æ®µï¼šVertebrae Segmentation Unet binary segment the mask of each vertebrae sigmoid ce-loss uniform voxel spacing of 1mm input shapeï¼š[128,128,96]ï¼Œcrop origin image &amp; heatmap image based on centroids reference paper\ æ ¸å¿ƒè´¡çŒ®ï¼š1.MIPï¼šcombines the information across reformationsï¼Œ3D to 2Dï¼Œ2. åŸºäºåˆ¤åˆ«å™¨çš„è®­ç»ƒæœºåˆ¶ï¼šencodes local spine structure as an anatomical priorï¼ŒåŠ å›ºæ¤å—é—´ç±»åˆ«&amp;ä½ç½®çš„spacial information MIPï¼š localisation and identification rely on a large context large receptive field in full-body scans where spine is not spatially centred or is obstructed by the ribcage, such cases are handled with a pre-processing stage detecting the occluded spine adversarial learningï¼š FCNç”¨äºåˆ†å‰² AEç”¨äºè¯„ä¼°åˆ†å‰²çš„å¥½å do not â€˜pre-trainâ€™ it (the AE) lossï¼šan anatomically-inspired supervision instead of the usual binary adversarial supervision (vanilla GAN) å…ˆè¯´FCNâ€”â€”Btrfly Network å»ºæ¨¡æˆå›å½’é—®é¢˜ï¼Œæ¯ä¸ªå…³é”®ç‚¹å¯¹åº”ä¸€ä¸ªé€šé“çš„é«˜æ–¯heatmapï¼ŒèƒŒæ™¯channelä¸º$1-max_i (y_i)$ åŒè¾“å…¥åŒè¾“å‡ºï¼ˆsagittal &amp; coronalï¼‰ ä¸¤ä¸ªè§†è§’çš„feature mapåœ¨ç½‘ç»œæ·±å±‚åšäº†èåˆï¼Œto learn their inter-dependency Batch- normalisation is used after every convolution layer, along with 20% dropout in the fused layers of Btrfly lossï¼šl2 distance + weighted ce L_{sag} = ||Y_{sag} - \hat{Y}_{sag}||^2 + \omega CE(softmax(Y_{sag}, softmax(\hat{Y}_{sag})) $\omega$ is the median frequency weighing map, boosting the learning of less frequent classes(ECB) å†è¯´åˆ¤åˆ«å™¨â€”â€”Energy-based adversary for encoding prior fully-convolutionalï¼šits predictions across voxels are independent of each other owing to the spatial invariance of convolutions to impose the anatomical prior of the spineâ€™s shape onto the Btrfly net look at $\hat{Y}_{sag}$ and $\hat{Y}_{cor}$ as a 3D volume and employ a 3D AE with a receptive field covering a part of the spine $\hat{Y}_{sag}$ consists of Gaussiansï¼šless informative than an image, avoid using max-pooling by resorting to average pooling employ spatially dilated convolution kernels mission of AEï¼špredict the l2 distance of input and its reconstruction, it learns to discriminate by predicting a low E for real annotations, while G learns to generate annotations that would trick D L = D(Y_x) + max(0, m-D(Y_g))\\ L_G = D(Y_g) + L_{fcn} inferenceï¼š The values below a threshold (T) are ignored in order to remove noisy predictions ç”¨å¤–ç§¯ï¼Œ$\hat{Y}=\hat{Y}_{sag}\otimes\hat{Y}_{cor}$ æ¯ä¸ªchannelçš„æœ€å¤§å€¼ä½œä¸ºcentroids experiments ã€IMPORTANTã€‘10 MIPs are obtained from one 3D scan per view, each time randomly choosing half the slices of interest å¯¹äºæ¯ä¸ªè§†è§’ï¼Œæ¯æ¬¡éšæœºæŠ½å–ä¸€åŠæ•°ç›®çš„sliceç”¨äºè®¡ç®—MIP similar local appearanceï¼š strong spatial configurationï¼šå‡¡æ˜¯æ¶‰åŠåˆ°æ¤å—-wiseçš„ä¿¡æ¯ï¼Œä»å…¨å±€ä¿¡æ¯å…¥æ‰‹]]></content>
      <tags>
        <tag>challenge</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GoogLeNetç³»åˆ—]]></title>
    <url>%2F2020%2F04%2F13%2FGoogLeNet%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[ç»¼è¿° papers [V1] Going Deeper with Convolutions, 6.67% test error [V2] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, 4.8% test error [V3] Rethinking the Inception Architecture for Computer Vision, 3.5% test error [V4] Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, 3.08% test error [Xception] Xception: Deep Learning with Depthwise Separable Convolutions [EfficientNet] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks [EfficientDet] EfficientDet: Scalable and Efficient Object Detection [EfficientNetV2] EfficientNetV2: Smaller Models and Faster Training å¤§ä½“æ€è·¯ inception V1ï¼šæ‰“ç ´ä¼ ç»Ÿçš„conv blockï¼Œè®¾è®¡äº†Inception blockï¼Œå°†1*1ã€3*3ã€5*5çš„å·ç§¯ç»“æœconcatï¼Œå¢åŠ ç½‘ç»œå®½åº¦ inception V2ï¼šåŠ å…¥äº†BNå±‚ï¼Œå‡å°‘Internal Covariate Shiftï¼Œç”¨ä¸¤ä¸ª3*3æ›¿ä»£5*5ï¼Œé™ä½å‚æ•°é‡ inception V3ï¼šæå‡ºåˆ†è§£Factorizationï¼Œ7*7æ”¹æˆ7*1å’Œ1*7ï¼Œå‚æ•°å‡å°‘åŠ é€Ÿè®¡ç®—ï¼Œå¢åŠ ç½‘ç»œæ·±åº¦å’Œéçº¿æ€§ inception V4ï¼šç»“åˆResidual Connection Xceptionï¼šé’ˆå¯¹inception V3çš„åˆ†è§£ç»“æ„çš„æ”¹è¿›ï¼Œä½¿ç”¨å¯åˆ†ç¦»å·ç§¯ EfficientNetï¼šä¸»è¦ç ”ç©¶model scalingï¼Œé’ˆå¯¹ç½‘ç»œæ·±åº¦ã€å®½åº¦ã€å›¾åƒåˆ†è¾¨ç‡ï¼Œæœ‰æ•ˆåœ°æ‰©å±•CNN EfficientDetï¼šå°†EfficientNetä»åˆ†ç±»ä»»åŠ¡æ‰©å±•åˆ°ç›®æ ‡æ£€æµ‹ä»»åŠ¡ review review0122ï¼šconv-BNå±‚åˆå¹¶è¿ç®— referenceï¼šhttps://nenadmarkus.com/p/fusing-batchnorm-and-conv/ freezed BNå¯ä»¥çœ‹æˆ1x1çš„å·ç§¯è¿ç®— ä¸¤ä¸ªçº¿æ€§è¿ç®—æ˜¯å¯ä»¥åˆå¹¶çš„ given $W_{conv} \in R^{CC_{prev}kk}$ï¼Œ$b_{conv} \in R^C $ï¼Œ$W_{bn}\in R^{CC}$ï¼Œ$b_{bn}\in R^C$ F = W_{bn} * (W_{conv} * F_{prev} + b_{conv}) + b_{bn} V1: Going deeper with convolutions åŠ¨æœº improved utilization of the computing resources increasing the depth and width of the network while keeping the computational budget è®ºç‚¹ the recent trend has been to increase the number of layers and layer size, while using dropout to address the problem of overfitting major bottleneckï¼šlarge networkï¼Œlarge number of paramsï¼Œlimited datasetï¼Œoverfitting methods use filters of different sizes in order to handle multiple scales NiN use 1x1 convolutional layers to easily integrate in the current CNN pipelines we use 1x1 convs with a dual purpose of dimension reduction æ–¹æ³• Architectural 1x1 conv+ReLU for compute reductions an alternative parallel pooling path since pooling operations have been essential for the success overall architecture : ç»†èŠ‚ï¼š rectified linear activation mean subtraction a move from fully connected layers to average pooling improves acc the use of dropout remained essential adding auxiliary classifiers(on 4c&amp;4d) with a discount weight 5x5 avg pool, stride 3 1x1 conv+relu, 128 filters 1024 fc+relu 70% dropout 1000 fc+softmax asynchronous stochastic gradient descent with 0.9 momentum fixed learning rate schedule (de- creasing the learning rate by 4% every 8 epochs photometric distortions useful to combat overfitting random interpolation methods for resizing V2: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift åŠ¨æœº use much higher learning rates be less careful about initialization also acts as a regularizer, eliminating the need for Dropout è®ºç‚¹ SGDï¼šoptimizes the parameters $\theta$ of the network, so as to minimize the loss \theta = argmin_{\theta} \frac{1}{N}\sum_{i=1}^N loss(x_i, \theta) æ¢¯åº¦æ›´æ–°ï¼š\theta_{next-timestep} = \theta - \alpha \frac{\partial[\frac{1}{N}\sum_Nloss(\theta)]}{\partial \theta}ï¼Œ$x_i$ is the full set batch approximationï¼šuse $\frac{1}{m} \sum_M \frac{\partial loss(\theta)}{\partial \theta}$ï¼Œ$x_i$ is the mini-batch set quality improves as the batch size increases computation over a batch is much more efficient than m computations for individual examples the learning rate and the initial values require careful tuning Internal covariate shift the input distribution of the layers changes consider a gradient descent step aboveï¼Œ$x$çš„æ•°æ®åˆ†å¸ƒæ”¹å˜äº†ï¼Œ$\theta$å°±è¦ç›¸åº”åœ° readjust to compensate for the change in the distribution of x activation å¯¹äºç¥ç»å…ƒ$z = sigmoid(Wx+b)$ï¼Œå‰é¢å±‚çš„å‚æ•°å˜åŒ–ï¼Œå¾ˆå®¹æ˜“å¯¼è‡´å½“å‰ç¥ç»å…ƒçš„å“åº”å€¼ä¸åœ¨æœ‰æ•ˆæ´»åŠ¨åŒºé—´ï¼Œä»è€Œå¯¼è‡´è¿‡äº†å½“å‰æ¿€æ´»å‡½æ•°ä»¥åæ¢¯åº¦æ¶ˆå¤±ï¼Œslow down the convergence In practice, using ReLU &amp; careful initialization &amp; small learning rates å¦‚æœæˆ‘ä»¬èƒ½ä½¿å¾—the distribution of nonlinearity inputs remains more stable as the network trainsï¼Œå°±ä¸ä¼šå‡ºç°ç¥ç»å…ƒé¥±å’Œçš„é—®é¢˜ whitening å¯¹training setçš„é¢„å¤„ç†ï¼šlinearly transformed to have zero means and unit variances, and decorrelated ä½¿å¾—è¾“å…¥æ•°æ®çš„åˆ†å¸ƒä¿æŒç¨³å®šï¼Œnormal distribution åŒæ—¶å»é™¤äº†æ•°æ®é—´çš„ç›¸å…³æ€§ batch normalization fixes the means and variances of layer inputs reducing the dependence of gradients on the scale of the parameters or of their initial values makes it possible to use saturating nonlinearities * full whitening of each layer is costly * so we normalize each layer independently, full set--&gt; mini-batch * standard normal distributionå¹¶ä¸æ˜¯æ¯ä¸ªç¥ç»å…ƒæ‰€éœ€çš„ï¼ˆå¦‚identity transformï¼‰ï¼šintroduce, for each activation $x(k)$ , a pair of parameters $\gamma(k)$, $\beta(k)$, which scale and shift the normalized value to maintain the representation ability of the neuron â€‹ * for convolutional networks * we add the BN transform immediately before the nonlinearity, $z = g(Wx+b)$ to $z = g(BN(Wx))$ * since we normalize $Wx+b$, the bias b can be ignored * obey the convolutional propertyâ€”â€”different elements of the same feature map, at different locations, are normalized in the same way * We learn a pair of parameters $\gamma(k)$ and $\beta(k)$ **per feature map**, rather than per activation * properties * back-propagation through a layer is unaffected by the scale of its parameters * Moreover, larger weights lead to smaller gradients, thus stabilizing the parameter growth * regularizes the modelï¼šå› ä¸ºç½‘ç»œä¸­mini-batchçš„æ•°æ®ä¹‹é—´æ˜¯æœ‰äº’ç›¸å½±å“çš„è€Œéindependentçš„ æ–¹æ³• batch normalization full whitening of each layer is costly so we normalize each layer independently, full setâ€”&gt; mini-batch standard normal distributionå¹¶ä¸æ˜¯æ¯ä¸ªç¥ç»å…ƒæ‰€éœ€çš„ï¼ˆå¦‚identity transformï¼‰ï¼šintroduce, for each activation $x(k)$ , a pair of parameters $\gamma(k)$, $\beta(k)$, which scale and shift the normalized value to maintain the representation ability of the neuron bpï¼š inferenceé˜¶æ®µï¼š é¦–å…ˆä¸¤ä¸ªå¯å­¦ä¹ å‚æ•°$\gamma$å’Œ$\beta$æ˜¯å®šä¸‹æ¥çš„ è€Œå‡å€¼å’Œæ–¹å·®ä¸å†é€šè¿‡è¾“å…¥æ•°æ®æ¥è®¡ç®—ï¼Œè€Œæ˜¯è½½å…¥è®­ç»ƒè¿‡ç¨‹ä¸­ç»´æŠ¤çš„å‚æ•°ï¼ˆmoving averagesï¼‰ â€‹ for convolutional networks we add the BN transform immediately before the nonlinearity, $z = g(Wx+b)$ to $z = g(BN(Wx))$ since we normalize $Wx+b$, the bias b can be ignored obey the convolutional propertyâ€”â€”different elements of the same feature map, at different locations, are normalized in the same way We learn a pair of parameters $\gamma(k)$ and $\beta(k)$ per feature map, rather than per activation properties back-propagation through a layer is unaffected by the scale of its parameters Moreover, larger weights lead to smaller gradients, thus stabilizing the parameter growth regularizes the modelï¼šå› ä¸ºç½‘ç»œä¸­mini-batchçš„æ•°æ®ä¹‹é—´æ˜¯æœ‰äº’ç›¸å½±å“çš„è€Œéindependentçš„ V3: Rethinking the Inception Architecture for Computer Vision åŠ¨æœº go deeper and widerï¼š enough labeled data computational efficiency parameter count to scale up networks utilizing the added computation as efficiently give general design principles and optimization ideas factorized convolutions aggressive regularization è®ºç‚¹ GoogleNet does not provide a clear description about the contributing factors that lead to the various design æ–¹æ³• General Design Principles Avoid representational bottlenecksï¼šç‰¹å¾å›¾å°ºå¯¸åº”è¯¥gently decreaseï¼Œresolutionçš„ä¸‹é™å¿…é¡»ä¼´éšç€channelæ•°çš„ä¸Šå‡ï¼Œé¿å…ä½¿ç”¨max poolingå±‚è¿›è¡Œä¸‹é‡‡æ ·ï¼Œå› ä¸ºè¿™æ ·å¯¼è‡´ä¿¡æ¯æŸå¤±è¾ƒå¤§ Higher dimensional representations are easier to process locally within a network. Increasing the activa- tions per tile in a convolutional network allows for more disentangled features. The resulting networks will train fasterï¼šå‰åŠå¥æ‡‚äº†ï¼Œhigh-resoçš„ç‰¹å¾å›¾focusåœ¨å±€éƒ¨ä¿¡æ¯ï¼ŒååŠå¥ä¸æ‡‚ï¼Œæ ¹æ®ä¸Šä¸€ç¯‡paperï¼Œç”¨äº†batch normä»¥åï¼Œscale upç¥ç»å…ƒä¸å½±å“bpï¼ŒåŒæ—¶ä¼šlead to smaller gradientsï¼Œä¸ºå•¥èƒ½åŠ é€Ÿï¼Ÿ Spatial aggregation can be done over lower dimensional embeddingsï¼šadjacent unitä¹‹é—´æœ‰strong correlationï¼Œæ‰€ä»¥å¯ä»¥reduce the dimension of the input representation before the spatial aggregationï¼Œä¸ä¼šæœ‰å¤ªå¤§çš„ä¿¡æ¯æŸå¤±ï¼Œå¹¶ä¸”promotes faster learning The computational budget should therefore be distributed in a balanced way between the depth and width of the network. Factorizing Convolutions Filter Size into smaller convolutions å¤§filteréƒ½å¯ä»¥æ‹†è§£æˆå¤šä¸ª3x3 å•çº¯å»ç­‰ä»·çº¿æ€§åˆ†è§£å¯ä»¥ä¸ä½¿ç”¨éçº¿æ€§activationï¼Œä½†æ˜¯æˆ‘ä»¬ä½¿ç”¨äº†batch normï¼ˆincrease variatyï¼‰ï¼Œæ‰€ä»¥è§‚å¯Ÿåˆ°ä½¿ç”¨ReLUä»¥åæ‹Ÿåˆæ•ˆæœæ›´å¥½ into Asymmetric Convolutions n*nçš„filteræ‹†è§£æˆ1*nå’Œn*1 this factorization does not work well on early layers, but gives very good results on medium grid-sizes (ranges between 12 and 20, using 1x7 and 7x1 Utility of Auxiliary Classifiers did not result in improved convergence early in the trainingï¼šè®­ç»ƒå¼€å§‹é˜¶æ®µæ²¡å•¥ç”¨ï¼Œå¿«æ”¶æ•›æ—¶å€™æœ‰ç‚¹ç‚¹accæå‡ removal of the lower auxiliary branch did not have any adverse effect on the final qualityï¼šæ‹¿æ‰å¯¹æœ€ç»ˆç»“æœæ²¡å½±å“ æ‰€ä»¥æœ€åˆçš„è®¾æƒ³ï¼ˆhelp evolving the low-level featuresï¼‰ æ˜¯é”™çš„ï¼Œä»…ä»…act as regularizerï¼Œauxiliary headé‡Œé¢åŠ ä¸Šbatch normä¼šä½¿å¾—æœ€ç»ˆç»“æœbetter Efficient Grid Size Reductionä¸‹é‡‡æ ·æ¨¡å—ä¸å†ä½¿ç”¨maxpooling dxdxk feature map expand to (d/2)x(d/2)x2kï¼š 1x1x2k convï¼Œstride2 poolï¼škxdxdx2k computation 1x1x2k stride2 convï¼škx(d/2)x(d/2)x2k computationï¼Œè®¡ç®—é‡ä¸‹é™ï¼Œä½†æ˜¯è¿åprinciple1 parallel stride P and C blocksï¼škx(d/2)x(d/2)xk computationï¼Œç¬¦åˆprinciple1:reduces the grid-size while expands the filter banks Inception-v3 å¼€å¤´çš„7x7convå·²ç»æ¢æˆäº†å¤šä¸ª3x3 ä¸­é—´å±‚featuremapé™ç»´åˆ°17x17çš„æ—¶å€™ï¼Œå¼€å§‹ç”¨Asymmetric Factorization block åˆ°8x8çš„æ—¶å€™ï¼Œåšäº†expanding the filter bank outputs Label Smoothing ï¼ˆhttps://zhuanlan.zhihu.com/p/116466239ï¼‰ used the uniform distribution $u(k)=1/K$ q(k) = (1-\epsilon)\delta(k) + \frac{\epsilon}{K} å¯¹äºsoftmaxå…¬å¼ï¼š$p(k)=\frac{exp(y_k)}{\sum exp(y_i)}$ï¼Œè¿™ä¸ªlossè®­ç»ƒçš„ç»“æœå°±æ˜¯$y_k$æ— é™è¶‹è¿‘äº1ï¼Œå…¶ä»–$y_i$æ— é™è¶‹è¿‘äº0ï¼Œ äº¤å‰ç†µlossï¼š$ce=\sum -y_{gt}log(y_k)$ï¼ŒåŠ äº†label smoothingä»¥åï¼Œlossä¸Šå¢åŠ äº†é˜´æ€§æ ·æœ¬çš„regularizationï¼Œæ­£è´Ÿæ ·æœ¬çš„æœ€ä¼˜è§£è¢«é™å®šåœ¨æœ‰é™å€¼ï¼Œé€šè¿‡æŠ‘åˆ¶æ­£è´Ÿæ ·æœ¬è¾“å‡ºå·®å€¼ï¼Œä½¿å¾—ç½‘ç»œæœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚ V4: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning åŠ¨æœº residualï¼šwhether there are any benefit in combining the Inception architecture with residual connections inceptionV4ï¼šsimplify the inception blocks è®ºç‚¹ residual connections seems to improve the training speed greatlyï¼šä½†æ˜¯æ²¡æœ‰ä¹Ÿèƒ½è®­ç»ƒæ·±å±‚ç½‘ç»œ made uniform choices for the Inception blocks for each grid size Inception-A for 35x35 Inception-B for 17x17 Inception-C for 8x8 for residual versions use cheaper Inception blocks for residual versionsï¼šç®€åŒ–moduleï¼Œå› ä¸ºidentityéƒ¨åˆ†ï¼ˆç›´æ¥ç›¸è¿çš„çº¿ï¼‰æœ¬èº«åŒ…å«ä¸°å¯Œçš„ç‰¹å¾ä¿¡æ¯ æ²¡æœ‰ä½¿ç”¨pooling replace the filter concatenation stage of the Inception architecture with residual connectionsï¼šåŸæ¥blocké‡Œé¢çš„concatenationä¸»ä½“æ”¾åœ¨æ®‹å·®pathä¸­ Each Inception block is followed by filter-expansion layer (1 Ã— 1 convolution without activation) to match the depth of the input for additionï¼šç›¸åŠ ä¹‹å‰ä¿è¯channelæ•°ä¸€è‡´ used batch-normalization only on top of the traditional layers, but not on top of the summationsï¼šæµªè´¹å†…å­˜ number of filters exceeded 1000 causes instabilities scaling down the residuals before adding by factors between 0.1 and 0.3ï¼šæ®‹å·®é€šé“å“åº”å€¼ä¸è¦å¤ªå¤§ blocks V4 ABCï¼š Res ABCï¼š Xception: Deep Learning with Depthwise Separable Convolutions åŠ¨æœº Inception modules have been replaced with depthwise separable convolutions significantly outperforms Inception V3 on a larger dataset due to more efficient use of model parameters è®ºç‚¹ early LeNet-style models simple stacks of convolutions for feature extraction and max-pooling operations for spatial sub-sampling increasingly deeper complex blocks Inception modules inspired by NiN be capable of learning richer repre- sentations with less parameters The Inception hypothesis a single convolution kernel is tasked with simultaneously mapping cross-channel correlations and spatial correlations while Inception factors it into a series of operations that independently look at cross-channel correlations(1x1 convs) and at spatial correlations(3x3/5x5 convs) suggesting that cross-channel correlations and spatial correlations are sufficiently decoupled that it is preferable not to map them jointly inception blockå…ˆç”¨1x1çš„convå°†åŸè¾“å‡ºæ˜ å°„åˆ°3-4ä¸ªlower spaceï¼ˆcross-channel correlationsï¼‰ï¼Œç„¶ååœ¨è¿™äº›å°çš„3d spacesä¸Šåšregular convï¼ˆmaps all correlations ï¼‰â€”â€”è¿›ä¸€æ­¥å‡è®¾ï¼Œå½»åº•è§£è€¦ï¼Œç¬¬äºŒæ­¥åªåšspatial correlations main differences between â€œextreme â€ Inception and depthwise separable convolution order of the operationsï¼š1x1 first or latter non-linearityï¼šdepthwise separable convolutions are usually implemented without non-linearitiesã€QUESTIONï¼šè¿™å’ŒMobileNeté‡Œé¢è¯´çš„ä¸ä¸€æ ·å•Šï¼ŒMé‡Œé¢çš„depthwiseä¹Ÿæ˜¯æ¯å±‚éƒ½å¸¦äº†BNå’ŒReLUçš„ã€‘ è¦ç´  Convolutional neural networks The Inception family Depthwise separable convolutions Residual connections æ–¹æ³• architecture a linear stack of depthwise separable convolution layers with residual connections all conv are followed by BN kerasçš„separableConvå’ŒdepthwiseConvï¼šå‰è€…ç”±åè€…åŠ ä¸Šä¸€ä¸ªpointwiseConvç»„æˆï¼Œæœ€åæœ‰activationï¼Œä¸­é—´æ²¡æœ‰ cmp Xception and Inception V3 have nearly the same number of parameters marginally better on ImageNet much larger performance increasement on JFT Residual connections are clearly essential in helping with convergence, both in terms of speed and final classification performance. Effect of intermediate activationï¼šthe absence of any non-linearity leads to both faster convergence and better final performance EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks åŠ¨æœº common senseï¼šscaled up the network for better accuracy we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance propose a new scaling methodï¼šusing compound coefficient to uniformly scale all dimensions of depth/width/resolution on MobileNets and ResNet a new baseline network family EfficientNets much better accuracy and efficiency è®ºç‚¹ previous work scale up one of the three dimensions depthï¼šmore layers widthï¼šmore channels image resolutionï¼šhigher resolution arbitrary scaling requires tedious manual tuning and often yields sub-optimal accuracy and efficiency uniformly scalingï¼šOur empirical study shows that it is critical to balance all dimensions of network width/depth/resolution, and surprisingly such balance can be achieved by simply scaling each of them with constant ratio. neural architecture searchï¼šbecomes increasingly popular in designing efficient mobile-size ConvNets æ–¹æ³• problem formulation ConvNetsï¼š$N = \bigodot_{i=1â€¦s} F_i^{L_i}(X_{})$, s for stage, L for repeat times, F for function simplify the design problem fixing $F_i$ all layers must be scaled uniformly with constant ratio an optimization problemï¼šd for depth coefficients, w for width coefficients, r for resolution coefficients max_{d,w,r} \ \ \ Accuracy(N(d,w,r))\\ s.t. \ \ \ N(d,w,r) = \bigodot_{i=1...s} F_i^{d*L_i}(X_{}) observation 1 Scaling up any dimension of network (width, depth, or resolution) improves accuracy, but the accuracy gain diminishes for bigger models. å‡†ç¡®ç‡éƒ½ä¼šæå‡ï¼Œæœ€ç»ˆéƒ½ä¼šé¥±å’Œ depthï¼šdeeper ConvNet can capture richer and more complex features widthï¼šwider networks tend to be able to capture more fine-grained features and are easier to train ï¼ˆcommonly used for small size modelsï¼‰ä½†æ˜¯æ·±åº¦å’Œå®½åº¦æœ€å¥½åŒ¹é…ï¼Œä¸€å‘³åŠ å®½shallow networkä¼šè¾ƒéš¾æå–é«˜çº§ç‰¹å¾ resolutionï¼šhigher resolution input can potentially capture more fine-grained patterns observation 2 compound scalingï¼šit is critical to balance all dimensions of network width, depth, and resolution different scaling dimensions are not independent è¾“å…¥æ›´é«˜çš„resolutionï¼Œå°±éœ€è¦æ›´æ·±çš„ç½‘ç»œï¼Œä»¥è·å–æ›´å¤§çš„æ„Ÿå—é‡ï¼ŒåŒæ—¶è¿˜éœ€è¦æ›´å®½çš„ç½‘ç»œï¼Œä»¥æ•è·æ›´å¤šçš„ç»†ç²’åº¦ç‰¹å¾ compound coefficient $\phi$ï¼š d = \alpha ^ \phi\\ w = \beta ^ \phi\\ r = \gamma ^ \phi\\ s.t. \alpha * \beta^2 * \gamma^2 \approx 2, \ \alpha\geq1,\ \beta\geq1,\ \gamma\geq1 $\alpha, \beta, \gamma$ are constants determined by a small grid search, controling the assign among the 3 dimensions [d,w,r] $\phi$ controls how many more resources are available for model scaling the total FLOPS will approximately increase by $2^\phi$ efficientNet architecture having a good baseline network is also critical thus we developed a new mobile-size baseline called EfficientNet by leveraging a multi-objective neural architecture search that optimizes both accuracy and FLOPS compound scalingï¼šfix $\phi=1$ and grid search $\alpha, \beta, \gamma$, fix $\alpha, \beta, \gamma$ and use different $\phi$ å®éªŒ on MobileNets and ResNets compared to other single-dimension scaling methods compound scaling method improves the accuracy on all on EfficientNet model with compound scaling tends to focus on more relevant regions with more object details while other models are either lack of object details or unable to capture all objects in the images implementing details RMSProp: decay=0.9, momentum(rho)=0.9ï¼Œtpuä¸Šä½¿ç”¨lars BN: momentum=0.99 weight decay = 1e-5 lr: initial=0.256, decays by 0.97 every 2.4 epochs SiLU activation AutoAugment Stochastic depth: survive_prob = 0.8 dropout rate: 0.2 to 0.5 for B0 to B7 EfficientDet: Scalable and Efficient Object Detection åŠ¨æœº model efficiency for object detectionï¼šbased on one-stage detector ç‰¹å¾èåˆï¼špropose a weighted bi-directional feature pyramid network (BiFPN) ç½‘ç»œrescaleï¼šuniformly scales the resolution, depth, and width for all backbone achieve better accuracy with much fewer parameters and FLOPs also test on Pascal VOC 2012 semantic segmentation è®ºç‚¹ previous work tends to achieve better efficiency by sacrificing accuracy previous work fuse feature at different resolutions by simply summing up without distinction EfficientNet backboneï¼šcombine EfficientNet backbones with our propose BiFPN scale upï¼šjointly scales up the resolution/depth/width for all backbone, feature network, box/class prediction network Existing object detectors two-stageï¼šhave a region-of-interest proposal step one-stageï¼šhave not, use predefined anchors æ–¹æ³• BiFPNï¼šefficient bidirectional cross-scale connec- tions and weighted feature fusion FPNï¼šlimitæ˜¯åªæœ‰top-bottomä¸€æ¡information flow PANetï¼šåŠ ä¸Šäº†ä¸€æ¡bottom-up pathï¼Œbetter accuracyä½†æ˜¯more parameters and computations NAS-FPNï¼šåŸºäºç½‘ç»œæœç´¢å‡ºçš„ç»“æ„ï¼Œirregular and difficult to interpret or modify BiFPN remove those nodes that only have one input edgeï¼šåªæœ‰ä¸€æ¡è¾“å…¥çš„èŠ‚ç‚¹ï¼Œæ²¡åšåˆ°ä¿¡æ¯èåˆ add an extra edge from the original input to output node if they are at the same levelï¼šfuse more features without adding much cost repeat blocks Weighted Feature Fusion since different input features are at different resolutions, they usually contribute to the output feature unequally learnable weight that can be a scalar (per-feature), a vector (per-channel), or a multi-dimensional tensor (per-pixel) weight normalization Softmax-basedï¼š$O=\sum_i \frac{e^{w_i}}{\sum_j e^{w_j}}*I_i$ Fast normalizedï¼š$O=\sum_i \frac{w_i}{\epsilon + \sum_j w_j}*I_i$ï¼ŒRelu is applied after each $w_i$ to keep non-negative P_6^{td} = Conv(\frac{w_1P_6^{in}+w_2Resize(P_7^{in})}{\epsilon+w_1+w_2})\\ P_6^{out} = Conv(\frac{w_1P_6^{in}+w_2P_6^{td}+w_3Resize(P_5^{out})}{\epsilon+w_1+w_2+w_3}) EfficientDet ImageNet-pretrained Effi- cientNets as the backbone BiFPN serves as the feature network the fused features(level 3-7) are fed to a class and box network respectively compound scaling backboneï¼šreuse the same width/depth scaling coefficients of EfficientNet-B0 to B6 feature networkï¼š depth(layers)ï¼š$D=3+\phi$ width(channes)ï¼š$W=64 \cdot (1.35^{\phi}) $ box/class prediction networkï¼š depthï¼š$D=3+[\phi/3]$ widthï¼šsame as FPN resolution use feature 3-7ï¼šmust be dividable by $2^7$ $R=512+128*\phi$ EfficientDet-D0 ($\phi=0$) to D7 ($\phi=7$) å®éªŒ for object detection train Learning rate is linearly increased from 0 to 0.16 in the first training epoch and then annealed down employ commonly-used focal loss 3x3 anchors compare low-accuracy regimeï¼šä½ç²¾åº¦ä¸‹ï¼ŒEfficientDet-D0å’ŒyoloV3å·®ä¸å¤š ä¸­ç­‰ç²¾åº¦ï¼ŒEfficientDet-D1å’ŒMask-RCNNå·®ä¸å¤š EfficientDet-D7 achieves a new state-of-the-art for semantic segmentation modify keep feature level {P2,P3,â€¦,P7} in BiFPN but only use P2 for the final per-pixel classification set the channel size to 128 for BiFPN and 256 for classification head Both BiFPN and classification head are repeated by 3 times compare å’Œdeeplabv3æ¯”çš„ï¼ŒCOCOæ•°æ®é›† better accuracy and fewer FLOPs ablation study backbone improves accuracy v.s. resnet50 BiFPN improves accuracy v.s. FPN BiFPN achieves similar accuracy as repeated FPN+PANet BiFPN + weghting achieves the best accuracy Normalizedï¼šsoftmaxå’Œfastç‰ˆæœ¬æ•ˆæœå·®ä¸å¤šï¼Œæ¯ä¸ªèŠ‚ç‚¹çš„weightåœ¨è®­ç»ƒå¼€å§‹è¿…é€Ÿå˜åŒ–ï¼ˆsuggesting different features contribute to the feature fusion unequallyï¼‰ Compound Scalingï¼šè¿™ä¸ªæ¯”å…¶ä»–åªæé«˜ä¸€ä¸ªæŒ‡æ ‡çš„æ•ˆæœå¥½å°±ä¸ç”¨è¯´äº† è¶…å‚ï¼š efficientNetå’ŒefficientDetçš„resolutionæ˜¯ä¸ä¸€æ ·çš„ï¼Œå› ä¸ºæ£€æµ‹è¿˜æœ‰neckå’Œheadï¼Œå±‚æ•°æ›´æ·±ï¼Œæ‰€ä»¥resolutionæ›´å¤§ EfficientNetV2: Smaller Models and Faster Training åŠ¨æœº faster training speed and better parameter efficiency use a new op: Fused-MBConv propose progressive learningï¼šadaptively adjuts regularization &amp; image size æ–¹æ³• review of EfficientNet large image size large memory usageï¼Œsmall batch sizeï¼Œlong training time thus propose increasing image size gradually in V2 extensive depthwise conv often cannot fully utilize modern accelerators thus introduce Fused-MBConv in V2ï¼šWhen applied in early stage 1-3, Fused-MBConv can improve training speed with a small overhead on parameters and FLOPs equally scaling up proved sub-optimal in nfnets since the stages are not equally contributed to the efficiency &amp; accuracy thus in V2 use a non-uniform scaling strategyï¼šgradually add more layers to later stages(s5 &amp; s6) restrict the max image size EfficientNet V2 Architecture basic ConvBlock use fused-MBConv in the early layers use MBConv in the latter layers expansion ratios use smaller expansion ratios å› ä¸ºåŒæ ·çš„é€šé“æ•°ï¼Œfused-MBæ¯”MBçš„å‚æ•°é‡å¤§ kernel size å…¨å›¾3x3ï¼Œæ²¡æœ‰5x5äº† add more layers to compensate the reduced receptive field last stride 1 stage effv1æ˜¯7ä¸ªstage effv2æœ‰6ä¸ªstage scaling policy compound scalingï¼šRã€Wã€Dä¸€èµ·scale ä½†æ˜¯é™åˆ¶äº†æœ€å¤§inference image size=480ï¼ˆtrain=384ï¼‰ gradually add more layers to later stages (s5 &amp; s6) progressive learning large models require stronger regularization larger image size leads to more computations with larger capacityï¼Œthus also needs stronger regularization training process in the early training epochs, we train the network with smaller images and weak regularization gradually increase image size but also making learning more difficult by adding stronger regularization adaptive params image size dropout rate randAug magnitude mixup alpha ç»™å®šæœ€å¤§æœ€å°å€¼ï¼Œstage Nï¼Œä½¿ç”¨linear interpolation train&amp;test details RMSProp optimizer with decay 0.9 and momentum 0.9 batch norm momentum 0.99 weight decay 1e-5 trained for 350 epochs with total batch size 4096 Learning rate is first warmed up from 0 to 0.256, and then decayed by 0.97 every 2.4 epochs exponential moving average with 0.9999 decay rate stochastic depth with 0.8 survival probability 4 stages (87 epochs per stage)ï¼šearly stage with weak regularization &amp; later stronger maximum image size for training is about 20% smaller than inference &amp; no further finetuning]]></content>
      <tags>
        <tag>papers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FCN]]></title>
    <url>%2F2020%2F03%2F28%2FFCN%2F</url>
    <content type="text"><![CDATA[FCN: Fully Convolutional Networks for Semantic Segmentation åŠ¨æœº take input of arbitrary size pixelwise prediction (semantic segmentation) efficient inference and learning end-to-end with superwised-pretraining è®ºç‚¹ fully connected layers brings heavy computation patchwise/proposals training with less efficiency (ä¸ºäº†å¯¹ä¸€ä¸ªåƒç´ åˆ†ç±»ï¼Œè¦æ‰£å®ƒå‘¨å›´çš„patchï¼Œä¸€å¼ å›¾çš„å­˜å‚¨å®¹é‡ä¸Šå‡åˆ°k*kå€ï¼Œè€Œä¸”ç›¸é‚»patché‡å çš„éƒ¨åˆ†å¼•å…¥å¤§é‡é‡å¤è®¡ç®—ï¼ŒåŒæ—¶æ„Ÿå—é‡å¤ªå°ï¼Œæ²¡æ³•æœ‰æ•ˆåˆ©ç”¨å…¨å±€ä¿¡æ¯) fully convolutional structure are used to get a feature extractor which yield a localized, fixed-length feature Semantic segmentation faces an inherent tension between semantics and location: global information resolves what while local information resolves where. Deep feature hierarchies jointly encode location and semantics in a local-to-global pyramid. other semantic works (RCNN) are not end-to-end è¦ç´  æŠŠå…¨è¿æ¥å±‚æ¢æˆ1*1å·ç§¯ï¼Œç”¨äºæå–ç‰¹å¾ï¼Œå½¢æˆçƒ­ç‚¹å›¾ åå·ç§¯å°†å°å°ºå¯¸çš„çƒ­ç‚¹å›¾ä¸Šé‡‡æ ·åˆ°åŸå°ºå¯¸çš„è¯­ä¹‰åˆ†å‰²å›¾åƒ a novel â€œskipâ€ architecture to combine deep, coarse, semantic information and shallow, fine, appearance information æ–¹æ³• fully convolutional network receptive fields: Locations in higher layers correspond to the locations in the image they are path-connected to typical recognition nets: fixed-input patches the fully connected layers can be viewed as convolutions with kernels that cover their entire input regions our structure: arbitrary-input the computation is saved by computing the overlapping regions of those patches only once output size corresponds to the input(H/16, W/16) heatmap: the (H/16 * W/16) high-dims feature-map corresponds to the 1000 classes coarse predictions to dense OverFeat introduced å¯¹äºé«˜ç»´ç‰¹å¾å›¾ä¸Šä¸€ä¸ªå…ƒç´ ï¼Œå¯¹åº”äº†åŸå›¾æ„Ÿå—é‡ä¸€ç‰‡åŒºåŸŸï¼Œå°†reception fieldä¸­cä½å¡«ä¸Šè¿™ä¸ªå…ƒç´ çš„å€¼ ç§»åŠ¨åŸå›¾ï¼Œç›¸åº”çš„æ„Ÿå—é‡å¯¹åº”çš„å›¾ç‰‡ä¹Ÿå‘ç”Ÿäº†ç§»åŠ¨ï¼Œé«˜ç»´ç‰¹å¾å›¾çš„è¾“å‡ºå˜äº†ï¼Œcä½å˜äº† ç§»åŠ¨èŒƒå›´stride*strideï¼Œå°±ä¼šå¾—åˆ°åŸå›¾å°ºå¯¸çš„è¾“å‡ºäº† upsampling simplest: bilinear interpolation in-network upsampling: backwards convolution (deconvolution) with an output stride of f A stack of deconvolution layers and activation functions can even learn a nonlinear upsampling factor: FCNé‡Œé¢inputsizeå’Œoutputsizeä¹‹é—´å­˜åœ¨çº¿æ€§å…³ç³»ï¼Œå°±æ˜¯æ‰€æœ‰å·ç§¯poolingå±‚çš„ç´¯ç§¯é‡‡æ ·æ­¥é•¿ä¹˜ç§¯ kernelsizeï¼š$2 * factor - factor \% 2$ strideï¼š$factor$ paddingï¼š$ceil((factor - 1) / 2.)$ è¿™å—çš„è®¡ç®—æœ‰ç‚¹ç»•ï¼Œ$stride=factor$æ¯”è¾ƒå¥½ç¡®å®šï¼Œè¿™æ˜¯å°†ç‰¹å¾å›¾æ¢å¤çš„åŸå›¾å°ºå¯¸è¦rescaleçš„å°ºåº¦ã€‚ç„¶ååœ¨è¾“å…¥çš„ç›¸é‚»å…ƒç´ ä¹‹é—´æ’å…¥s-1ä¸ª0å…ƒç´ ï¼ŒåŸå›¾å°ºå¯¸å˜ä¸º$(s-1)(input_size-1)+input_size = sinput_size + (s-1)$ï¼Œä¸ºäº†å¾—åˆ°$output_size=s*input_size$è¾“å‡ºï¼Œå†è‡³å°‘$padding=[(s-1)/2]_{ceil}$ï¼Œç„¶åæ ¹æ®ï¼š (s-1) * (in-1) + in + 2p -k + 1 = outæœ‰ï¼š 2p-k+2 = såœ¨kerasé‡Œé¢å¯ä»¥è°ƒç”¨åº“å‡½æ•°Conv2DTransposeæ¥å®ç°ï¼š 123456789101112131415161718192021x = Input(shape=(64,64,16))y = Conv2DTranspose(filters=16, kernel_size=20, strides=8, padding='same')(x)model = Model(x, y)model.summary()# input: (None, 64, 64, 16) output: (None, 512, 512, 16) params: 102,416x = Input(shape=(32,32,16))y = Conv2DTranspose(filters=16, kernel_size=48, strides=16, padding='same')(x)# input: (None, 32, 32, 16) output: (None, 512, 512, 16) params: 589,840x = Input(shape=(16,16,16))y = Conv2DTranspose(filters=16, kernel_size=80, strides=32, padding='same')(x)# input: (None, 16, 16, 16) output: (None, 512, 512, 16) params: 1,638,416# å‚æ•°å‚è€ƒï¼šorig unetçš„totalå‚æ•°é‡ä¸º36,605,042# å„çº§transposeçš„å‚æ•°é‡ä¸ºï¼š# (None, 16, 16, 512) 4,194,816# (None, 32, 32, 512) 4,194,816# (None, 64, 64, 256) 1,048,832# (None, 128, 128, 128) 262,272# (None, 256, 256, 32) 16,416 å¯ä»¥çœ‹åˆ°kernel_sizeå˜å¤§ï¼Œå¯¹å‚æ•°é‡çš„å½±å“æå¤§ã€‚ï¼ˆkernel_sizeè®¾ç½®çš„å°äº†ï¼Œåªèƒ½æå–åˆ°å•ä¸ªå…ƒç´ ï¼Œæˆ‘è§‰å¾—kernel_sizeè‡³å°‘è¦å¤§äºstrideï¼‰ Segmentation Architecture use pre-trained model convert all fully connected layers to convolutions append a 1*1 conv with channel dimension(including background) to predict scores followed by a deconvolution layer to upsample the coarse outputs to dense outputs skips the 32 pixel stride at the final prediction layer limits the scale of detail in the upsampled output é€å±‚upsamplingï¼Œèåˆå‰å‡ å±‚çš„feature mapï¼Œelement-wise add finer layers: â€œAs they see fewer pixels, the finer scale predictions should need fewer layers.â€ è¿™æ˜¯é’ˆå¯¹å‰é¢çš„å·ç§¯ç½‘ç»œæ¥è¯´ï¼Œéšç€ç½‘ç»œåŠ æ·±ï¼Œç‰¹å¾å›¾ä¸Šçš„æ„Ÿå—é‡å˜å¤§ï¼Œå°±éœ€è¦æ›´å¤šçš„channelæ¥è®°å½•æ›´å¤šçš„ä½çº§ç‰¹å¾ç»„åˆ add a 1*1 conv on top of pool4 (zero-initialized) adding a 2x upsampling layer on top of conv7 (We initialize this 2xupsampling to bilinear interpolation, but allow the parameters to be learned) sum the above two stride16 predictions (â€œMax fusion made learning difficult due to gradient switchingâ€) 16x upsampled back to the image åšåˆ°ç¬¬ä¸‰è¡Œå†å¾€ä¸‹ï¼Œç»“æœåˆä¼šå˜å·®ï¼Œæ‰€ä»¥åšåˆ°è¿™é‡Œå°±åœä¸‹ æ€»ç»“ åœ¨å‡é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œåˆ†é˜¶æ®µå¢å¤§æ¯”ä¸€æ­¥åˆ°ä½æ•ˆæœæ›´å¥½ åœ¨å‡é‡‡æ ·çš„æ¯ä¸ªé˜¶æ®µï¼Œä½¿ç”¨é™é‡‡æ ·å¯¹åº”å±‚çš„ç‰¹å¾è¿›è¡Œè¾…åŠ© 8å€ä¸Šé‡‡æ ·è™½ç„¶æ¯”32å€çš„æ•ˆæœå¥½äº†å¾ˆå¤šï¼Œä½†æ˜¯ç»“æœè¿˜æ˜¯æ¯”è¾ƒæ¨¡ç³Šå’Œå¹³æ»‘ï¼Œå¯¹å›¾åƒä¸­çš„ç»†èŠ‚ä¸æ•æ„Ÿï¼Œè®¸å¤šç ”ç©¶è€…é‡‡ç”¨MRFç®—æ³•æˆ–CRFç®—æ³•å¯¹FCNçš„è¾“å‡ºç»“æœåšè¿›ä¸€æ­¥ä¼˜åŒ– x8ä¸ºå•¥å¥½äºx32ï¼š1. x32çš„ç‰¹å¾å›¾æ„Ÿå—é‡è¿‡å¤§ï¼Œå¯¹å°ç‰©ä½“ä¸æ•æ„Ÿ 2. x32çš„æ”¾å¤§æ¯”ä¾‹é€ æˆçš„å¤±çœŸæ›´å¤§ unetçš„åŒºåˆ«ï¼š unetæ²¡ç”¨imagenetçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå› ä¸ºæ˜¯åŒ»å­¦å›¾åƒ unetåœ¨è¿›è¡Œæµ…å±‚ç‰¹å¾èåˆçš„æ—¶å€™ç”¨äº†concatè€Œéelement-wise add é€å±‚ä¸Šé‡‡æ ·ï¼Œx2 vs. x8/x32 orig unetæ²¡ç”¨padï¼Œè¾“å‡ºå°äºè¾“å…¥ï¼ŒFCNåˆ™pad+crop æ•°æ®å¢å¼ºï¼ŒFCNæ²¡ç”¨è¿™äº›â€˜machineryâ€™ï¼ŒåŒ»å­¦å›¾åƒéœ€è¦å¼ºaugmentation åŠ æƒloss â€‹]]></content>
      <tags>
        <tag>papers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cs231n-RNN-review]]></title>
    <url>%2F2020%2F03%2F15%2Fcs231n-RNN-review%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[attentionç³»åˆ—]]></title>
    <url>%2F2020%2F03%2F13%2Fattention%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[0. ç»¼è¿° attentionçš„æ–¹å¼åˆ†ä¸ºä¸¤ç§ï¼ˆReferenceï¼‰ å­¦ä¹ æƒé‡åˆ†å¸ƒ éƒ¨åˆ†åŠ æƒï¼ˆhard attentionï¼‰ï¼å…¨éƒ¨åŠ æƒï¼ˆsoft attentionï¼‰ åŸå›¾ä¸ŠåŠ æƒï¼ç‰¹å¾å›¾ä¸ŠåŠ æƒ ç©ºé—´å°ºåº¦åŠ æƒï¼channelå°ºåº¦åŠ æƒï¼æ—¶é—´åŸŸåŠ æƒï¼æ··åˆåŸŸåŠ æƒ CAMç³»åˆ—ã€SE-blockç³»åˆ—ï¼šèŠ±å¼åŠ æƒï¼Œå­¦ä¹ æƒé‡ï¼Œnon-localçš„æ¨¡å—ï¼Œä½œç”¨äºæŸä¸ªç»´åº¦ ä»»åŠ¡åˆ†è§£ è®¾è®¡ä¸åŒçš„ç½‘ç»œç»“æ„ï¼ˆæˆ–åˆ†æ”¯ï¼‰ä¸“æ³¨äºä¸åŒçš„å­ä»»åŠ¡ï¼Œ é‡æ–°åˆ†é…ç½‘ç»œçš„å­¦ä¹ èƒ½åŠ›ï¼Œä»è€Œé™ä½åŸå§‹ä»»åŠ¡çš„éš¾åº¦ï¼Œä½¿ç½‘ç»œæ›´åŠ å®¹æ˜“è®­ç»ƒ STNã€deformable convï¼šæ·»åŠ æ˜¾å¼çš„æ¨¡å—è´Ÿè´£å­¦ä¹ å½¢å˜/receptive fieldçš„å˜åŒ–ï¼Œlocalæ¨¡å—ï¼Œapply by pixel local / non-local localæ¨¡å—çš„ç»“æœæ˜¯pixel-specificçš„ non-localæ¨¡å—çš„ç»“æœæ˜¯å…¨å±€å…±åŒè®¡ç®—çš„çš„ åŸºäºæƒé‡çš„attentionï¼ˆReferenceï¼‰ æ³¨æ„åŠ›æœºåˆ¶é€šå¸¸ç”±ä¸€ä¸ªè¿æ¥åœ¨åŸç¥ç»ç½‘ç»œä¹‹åçš„é¢å¤–çš„ç¥ç»ç½‘ç»œå®ç° æ•´ä¸ªæ¨¡å‹ä»ç„¶æ˜¯ç«¯å¯¹ç«¯çš„ï¼Œå› æ­¤æ³¨æ„åŠ›æ¨¡å—èƒ½å¤Ÿå’ŒåŸæ¨¡å‹ä¸€èµ·åŒæ­¥è®­ç»ƒ å¯¹äºsoft attentionï¼Œæ³¨æ„åŠ›æ¨¡å—å¯¹å…¶è¾“å…¥æ˜¯å¯å¾®çš„ï¼Œæ‰€ä»¥æ•´ä¸ªæ¨¡å‹ä»å¯ç”¨æ¢¯åº¦æ–¹æ³•æ¥ä¼˜åŒ– è€Œhard attentionè¦ç¦»æ•£åœ°é€‰æ‹©å…¶è¾“å…¥çš„ä¸€éƒ¨åˆ†ï¼Œè¿™æ ·æ•´ä¸ªç³»ç»Ÿå¯¹äºè¾“å…¥ä¸å†æ˜¯å¯å¾®çš„ papers [STN] Spatial Transformer Networks [deformable conv] Deformable Convolutional Networks [CBAM] CBAM: Convolutional Block Attention Module [SE-Net] Squeeze-and-Excitation Networks [SE-blockçš„ä¸€ç³»åˆ—å˜ä½“] SC-SEï¼ˆfor segmentationï¼‰ã€CMPE-SEï¼ˆå¤æ‚åˆæ²¡ç”¨ï¼‰ [SK-Net] Selective Kernel Networksï¼šæ˜¯attension moduleï¼Œä½†æ˜¯ä¸»è¦æ”¹è¿›ç‚¹åœ¨receptive fieldï¼Œtrickå¤§æ‚çƒ© [GC-Net] GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond CBAM: Convolutional Block Attention Module åŠ¨æœº attention module lightweight and general improvements in classification and detection è®ºç‚¹ deeperï¼š can obtain richer representation increased widthï¼šcan outperform an extremely deep network cardinalityï¼šresults in stronger representation power than depth and width attentionï¼šimproves the representation of interests humans exploit a sequence of partial glimpses and selectively focus on salient parts Residual Attention Networkï¼šcomputes 3d attention map we decompose the process that learns channel attention and spatial attention separately SE-blockï¼šuse global average-pooled features we suggest to use max-pooled features as well æ–¹æ³• sequentially infers a 1D channel attention map and a 2D spatial attention map broadcast and element-wise multiplication Channel attention module focuses on â€˜whatâ€™ is meaningful squeeze the spatial dimension use both average-pooled and max-pooled features simultaneously both descriptors are then forwarded to a shared MLP to reduce dimension ã€QUESTIONã€‘çœ‹è®ºæ–‡MLPæ˜¯çº¿æ€§çš„å—ï¼Œæ²¡å†™æ¿€æ´»å‡½æ•° then use element-wise summation sigmoid function Spatial attention module focuses on â€˜whereâ€™ apply average-pooling and max-pooling along the channel axis and concatenate 7x7 conv sigmoid function Arrangement of attention modules in a parallel or sequential manner we found sequential better than parallel we found channel-first order slightly better than the spatial-first integration apply CBAM on the convolution outputs in each block in residual path before the add operation å®éªŒ Ablation studies Channel attentionï¼šä¸¤ä¸ªpooling pathéƒ½æœ‰æ•ˆï¼Œä¸€èµ·ç”¨æœ€å¥½ Spatial attentionï¼š1x1convç›´æ¥squeezeä¹Ÿè¡Œï¼Œavg+maxæ›´å¥½ï¼Œ7x7convç•¥å¥½äº3x3conv arrangementï¼šå‰é¢è¯´äº†ï¼Œæ¯”SEçš„å•spacial squeezeå¥½ï¼Œchannelåœ¨å‰å¥½äºåœ¨åï¼Œä¸²è¡Œå¥½äºå¹¶è¡Œ Classification resultsï¼šoutperform baselines and SE Network Visualization cover the target object regions better the target class scores also increase accordingly Object Detection results apply to detectorsï¼šright before every classifier apply to backbone SK-Net: Selective Kernel Networks åŠ¨æœº ç”Ÿç‰©çš„ç¥ç»å…ƒçš„æ„Ÿå—é‡æ˜¯éšç€åˆºæ¿€å˜åŒ–è€Œå˜åŒ–çš„ propose a selective kernel unit adaptively adjust the RF multiple branches with different kernel sizes guided fusion å¤§æ‚çƒ©ï¼šmulti-branch&amp;kernelï¼Œgroup convï¼Œdilated convï¼Œattention mechanism SKNet by stacking multiple SK units åœ¨åˆ†ç±»ä»»åŠ¡ä¸ŠéªŒè¯ è®ºç‚¹ multi-scale aggregation inception blockå°±æœ‰äº† but linear aggregation approach may be insufficient multi-branch network two-branchï¼šä»¥resnetä¸ºä»£è¡¨ï¼Œä¸»è¦æ˜¯ä¸ºäº†easier to train multi-branchï¼šä»¥inceptionä¸ºä»£è¡¨ï¼Œä¸»è¦ä¸ºäº†å¾—åˆ°multifarious features grouped/depthwise/dilated conv grouped convï¼šreduce computationï¼Œæå‡ç²¾åº¦ depthwise convï¼šreduce computationï¼Œç‰ºç‰²ç²¾åº¦ dilated convï¼šenlarge RFï¼Œæ¯”dense large kernelèŠ‚çœå‚æ•°é‡ attention mechanism åŠ æƒç³»åˆ—ï¼š SENet&amp;CBAMï¼š ç›¸æ¯”ä¹‹ä¸‹SKNetå¤šäº†adaptive RF åŠ¨æ€å·ç§¯ç³»åˆ—ï¼š STNä¸å¥½è®­ç»ƒï¼Œè®­å¥½ä»¥åå˜æ¢å°±å®šæ­»äº† deformable convèƒ½å¤Ÿåœ¨inferenceçš„æ—¶å€™ä¹ŸåŠ¨æ€çš„å˜åŒ–å˜æ¢ï¼Œä½†æ˜¯æ²¡æœ‰multi-scaleå’Œnonlinear aggregation thus we propose SK convolution multi-kernelsï¼šå¤§sizeçš„conv kernelæ˜¯ç”¨äº†dilated conv nonlinear aggregation computationally lightweight could successfully embedded into small models workflow split fuse select main difference from inception less customized adaptive selection instead of equally addition æ–¹æ³• selective kernel convolution split multi-branch with different kernel size grouped/depthwise conv + BN + ReLU 5x5 kernel can be further replaced with dilated conv fuse to learn the control of information flow from different branches element-wise summation global average pooling fc-BN-ReLUï¼šreduce dimensionï¼Œat least 32 select channel-wise weighting factor A &amp; B &amp; moreï¼šA+B + more = 1 fc-softmax åœ¨2åˆ†æ”¯çš„æƒ…å†µä¸‹ï¼Œä¸€ä¸ªæƒé‡çŸ©é˜µAå°±å¤Ÿäº†ï¼ŒBæ˜¯å†—ä½™çš„ï¼Œå› ä¸ºå¯ä»¥é—´æ¥ç®—å‡ºæ¥ reweighting network start from resnext repeated SK unitsï¼šç±»ä¼¼bottleneck 1x1 conv SK conv 1x1 conv hyperparams number of branches M=2 group number G=32ï¼šcardinality of each path reduction ratio r=16ï¼šfuse operatorä¸­dim-reductionçš„å‚æ•° åµŒå…¥åˆ°è½»é‡çš„ç½‘ç»œç»“æ„ MobileNet/shuffleNet æŠŠå…¶ä¸­çš„3x3 depthwiseå·ç§¯æ›¿æ¢æˆSK conv å®éªŒ æ¯”sortçš„resnetã€densenetã€resnextç²¾åº¦éƒ½è¦å¥½ GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond åŠ¨æœº Non-Local Network (NLNet) capture long-range dependencies obtain query-specific global context but we found global contexts are almost the same for different query positions we produce query-independent formulation smiliar structure as SE-Net aims at global context modeling è®ºç‚¹ Capturing long-range dependency mainly by sdeeply stacking conv layersï¼šinefficient non-local network via self-attention mechanism computes the pairwise relations between the query position then aggregate ä½†æ˜¯ä¸åŒä½ç½®queryå¾—åˆ°çš„attention mapåŸºæœ¬ä¸€è‡´ we simply the non-local block query-independent maintain acc &amp; save computation our proposed GC-block unifies both the NL block and the SE block three steps global context modelingï¼š feature transform moduleï¼šcapture channel-wise interdependency fusion moduleï¼šmerge into the original features å¤šç§ä»»åŠ¡ä¸Šå‡æœ‰æ¶¨ç‚¹ ä½†éƒ½æ˜¯åœ¨è·Ÿresnet50å¯¹æ¯” revisit NLNet non-local block $f(x_i, x_j)$ï¼š encodes the relationship between position i &amp; j è®¡ç®—æ–¹å¼æœ‰Gaussianã€Embedded Gaussianã€Dot productã€Concat different instantiations achieve comparable performance $C(x)$ï¼šnorm factor $x_i + \sum^{N_p} F(x_j)$ï¼šaggregates a specific global feature on $x_i$ widely-used Embedded Gaussianï¼š åµŒå…¥æ–¹å¼ï¼š Mask R-CNN with FPN and Res50 only add one non-local block right before the last residual block of res4 observations &amp; inspirations distances among inputs show that input features are discriminated outputs &amp; attention maps are almost the sameï¼šglobal context after training is actually independent of query position inspirations simplify the Non-local block no need of query-specific æ–¹æ³• simplifying form of NL blockï¼šSNL æ±‚ä¸€ä¸ªcommonçš„global featureï¼Œshareç»™å…¨å›¾æ¯ä¸ªposition è¿›ä¸€æ­¥ç®€åŒ–ï¼šæŠŠ$x_j$çš„1x1 convæåˆ°å‰é¢ï¼ŒFLOPså¤§å¤§å‡å°‘ï¼Œå› ä¸ºfeature scaleä»HWå˜æˆäº†1x1 the SNL block achieves comparable performance to the NL block with significantly lower FLOPs global context modeling SNLå¯ä»¥æŠ½è±¡æˆä¸‰éƒ¨åˆ†ï¼š global attention poolingï¼šé€šè¿‡$W_k$ &amp; softmaxè·å–attention weightsï¼Œç„¶åè¿›è¡Œglobal pooling feature transformï¼š1x1 conv feature aggregationï¼šbroadcast element-wise add SE-blockä¹Ÿå¯ä»¥åˆ†è§£æˆç±»ä¼¼çš„æŠ½è±¡ global attention poolingï¼šç”¨äº†ç®€å•çš„global average pooling feature transformï¼šç”¨äº†squeeze &amp; exciteçš„fc-relu-fc-sigmoid feature aggregationï¼šbroadcast element-wise multiplication Global Context Block integrate the benefits of both SNL global attention poolingï¼šeffective modeling on long-range dependency SE bottleneck transformï¼šlight computationï¼ˆåªè¦ratioå¤§äº2å°±ä¼šèŠ‚çœå‚æ•°é‡å’Œè®¡ç®—é‡ï¼‰ ç‰¹åˆ«åœ°ï¼Œåœ¨SE transformçš„squeeze layerä¸Šï¼ŒåˆåŠ äº†BN ease optimization benefit generalization fusionï¼šadd åµŒå…¥æ–¹å¼ï¼š GC-ResNet50 add GC-block to all layers (c3+c4+c5) in resnet50 with se ratio of 16 relationship to SE-block é¦–å…ˆæ˜¯fusion method reflects different goals SEåŸºäºå…¨å±€ä¿¡æ¯rescales the channelsï¼Œé—´æ¥ä½¿ç”¨ GCç›´æ¥ä½¿ç”¨ï¼Œå°†long-range dependencyåŠ åœ¨æ¯ä¸ªpositionä¸Š å…¶æ¬¡æ˜¯norm layer ease optimization æœ€åæ˜¯global attention pooling SEçš„GAPæ˜¯a special case weighting factors shows superior]]></content>
      <tags>
        <tag>æ³¨æ„åŠ›æœºåˆ¶</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deeplabç³»åˆ—]]></title>
    <url>%2F2020%2F02%2F24%2FDeeplab%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[ç»¼è¿° papers deeplabV1: SEMANTIC IMAGE SEGMENTATION WITH DEEP CONVOLUTIONAL NETS AND FULLY CONNECTED CRFSï¼Œä¸»è¦è´¡çŒ®æå‡ºäº†ç©ºæ´å·ç§¯ï¼Œä½¿å¾—feature extractioné˜¶æ®µè¾“å‡ºçš„ç‰¹å¾å›¾ç»´æŒè¾ƒé«˜çš„resolution deeplabV2: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFsï¼Œä¸»è¦è´¡çŒ®æ˜¯å¤šå°ºåº¦ASPPç»“æ„ deeplabV3: Rethinking Atrous Convolution for Semantic Image Segmentationï¼Œæå‡ºäº†åŸºäºResNetçš„ä¸²è¡Œ&amp;å¹¶è¡Œä¸¤ç§ç»“æ„ï¼Œç»†èŠ‚ä¸Šæåˆ°äº†multi-gridï¼Œæ”¹è¿›äº†ASPPæ¨¡å— deeplabV3+: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation åˆ†å‰²ç»“æœæ¯”è¾ƒç²—ç³™çš„åŸå›  æ± åŒ–ï¼šå°†å…¨å›¾æŠ½è±¡åŒ–ï¼Œé™ä½åˆ†è¾¨ç‡ï¼Œä¼šä¸¢å¤±ç»†èŠ‚ä¿¡æ¯ï¼Œå¹³ç§»ä¸å˜æ€§ï¼Œä½¿å¾—è¾¹ç•Œä¿¡æ¯ä¸æ¸…æ™° æ²¡æœ‰åˆ©ç”¨æ ‡ç­¾ä¹‹é—´çš„æ¦‚ç‡å…³ç³»ï¼šCNNç¼ºå°‘å¯¹ç©ºé—´ã€è¾¹ç¼˜ä¿¡æ¯ç­‰çº¦æŸ å¯¹æ­¤ï¼ŒdeeplabV1å¼•å…¥äº† ç©ºæ´å·ç§¯ï¼šVGGä¸­æå‡ºçš„å¤šä¸ªå°å·ç§¯æ ¸ä»£æ›¿å¤§å·ç§¯æ ¸çš„æ–¹æ³•ï¼Œåªèƒ½ä½¿æ„Ÿå—é‡çº¿æ€§å¢é•¿ï¼Œè€Œå¤šä¸ªç©ºæ´å·ç§¯ä¸²è”ï¼Œå¯ä»¥å®ç°æŒ‡æ•°å¢é•¿ã€‚ å…¨è¿æ¥æ¡ä»¶éšæœºåœºCRFï¼šä½œä¸ºstage2ï¼Œæé«˜æ¨¡å‹æ•è·ç»†èŠ‚çš„èƒ½åŠ›ï¼Œæå‡è¾¹ç•Œåˆ†å‰²ç²¾åº¦ å¤§å°ç‰©ä½“åŒæ—¶åˆ†å‰² deeplabV2å¼•å…¥ å¤šå°ºåº¦ASPP(Atrous Spatial Pyramid Pooling)ï¼šå¹¶è¡Œçš„é‡‡ç”¨å¤šä¸ªé‡‡æ ·ç‡çš„ç©ºæ´å·ç§¯æå–ç‰¹å¾ï¼Œå†è¿›è¡Œç‰¹å¾èåˆ backbone model changeï¼šVGG16æ”¹ä¸ºResNet ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡ è¿›ä¸€æ­¥æ”¹è¿›æ¨¡å‹æ¶æ„ deeplabV3å¼•å…¥ ASPPåµŒå…¥ResNetåå‡ ä¸ªblock å»æ‰äº†CRF ä½¿ç”¨åŸå§‹çš„Conv/poolæ“ä½œï¼Œå¾—åˆ°çš„low resolution score mapï¼Œpool strideä¼šä½¿å¾—è¿‡ç¨‹ä¸­ä¸¢å¼ƒä¸€éƒ¨åˆ†ä¿¡æ¯ï¼Œä¸Šé‡‡æ ·ä¼šå¾—åˆ°è¾ƒå¤§çš„å¤±çœŸå›¾åƒï¼Œä½¿ç”¨ç©ºæ´å·ç§¯ï¼Œä¿ç•™ç‰¹å¾å›¾ä¸Šçš„å…¨éƒ¨ä¿¡æ¯ï¼ŒåŒæ—¶keep resolutionï¼Œå‡å°‘äº†ä¿¡æ¯æŸå¤± DeeplabV3çš„ASPPç›¸æ¯”è¾ƒäºV2ï¼Œå¢åŠ äº†ä¸€æ¡1x1 conv pathå’Œä¸€æ¡image pooling pathï¼ŒåŠ GAPè¿™æ¡pathæ˜¯å› ä¸ºï¼Œå®éªŒä¸­å‘ç°ï¼Œéšç€rateçš„å¢å¤§ï¼Œæœ‰æ•ˆçš„weightæ•°ç›®å¼€å§‹å‡å°‘ï¼ˆéƒ¨åˆ†è¶…å‡ºè¾¹ç•Œæ— æ³•æœ‰æ•ˆæ•æ‰è¿œè·ç¦»ä¿¡æ¯ï¼‰ï¼Œå› æ­¤åˆ©ç”¨global average poolingæå–äº†image-levelçš„ç‰¹å¾å¹¶ä¸ASPPçš„ç‰¹å¾å¹¶åœ¨ä¸€èµ·ï¼Œæ¥è¡¥å……å› ä¸ºdilationä¸¢å¤±çš„ä¿¡æ¯ ç©ºæ´å·ç§¯çš„pathï¼ŒV2æ˜¯æ¯æ¡pathåˆ†åˆ«ç©ºæ´å·ç§¯ç„¶åæ¥ä¸¤ä¸ª1x1convï¼ˆæ²¡æœ‰BNï¼‰ï¼ŒV3æ˜¯ç©ºæ´å·ç§¯å’ŒBatchNormalizationç»„åˆ fusionæ–¹å¼ï¼ŒV2æ˜¯sum fusionï¼ŒV3æ˜¯æ‰€æœ‰path concatç„¶å1x1 convï¼Œå¾—åˆ°æœ€ç»ˆscore map DeeplabV3çš„ä¸²è¡Œç‰ˆæœ¬ï¼Œâ€œIn order to maintain original image size, convolutions are replaced with strous convolutions with rates that differ from each other with factor 2â€ï¼Œpptä¸Šè¯´åé¢å‡ ä¸ªblockå¤åˆ¶äº†block4ï¼Œæ¯ä¸ªblocké‡Œé¢ä¸‰å±‚convï¼Œå…¶ä¸­æœ€åä¸€å±‚conv stride2ï¼Œç„¶åä¸ºäº†maintain output sizeï¼Œç©ºæ´rate*2ï¼Œè¿™ä¸ªä¸å¤ªç†è§£ã€‚ multi-grid methodï¼šå¯¹æ¯ä¸ªblocké‡Œé¢çš„ä¸‰å±‚å·ç§¯é‡‡ç”¨ä¸åŒç©ºæ´ç‡ï¼Œunit rateï¼ˆe.g.(1,2,4)ï¼‰ * rate ï¼ˆe.g. 2ï¼‰ deeplabV1: SEMANTIC IMAGE SEGMENTATION WITH DEEP CONVOLUTIONAL NETS AND FULLY CONNECTED CRFS åŠ¨æœº brings together methods from Deep Convolutional Neural Networks and probabilistic graphical models poor localization property of deep networks combine a fully connected Conditional Random Field (CRF) be able to localize segment boundaries beyond previous accuracies speed: atrous accuracy: simplicity: cascade modules è®ºç‚¹ DCNN learns hierarchical abstractions of data, which is desirable for high-level vision tasks (classification) but it hampers low-level tasks, such as pose estimation and semantic segmentation, where we want precise localization, rather than abstraction of spatial details two technical hurdles in DCNNs when applying to image labeling tasks pooling, loss of resolution: we employ the â€˜atrousâ€™ (with holes) for efficient dense computation spacial invariance: we use the fully connected pairwise CRF to capture fine edge details Our approach treats every pixel as a CRF node exploits long-range dependencies and uses CRF inference to directly optimize a DCNN-driven cost function æ–¹æ³• structure fully convolutional VGG-16 keep the first 3 subsampling blocks for a target stride of 8 use hole algorithm conv filters for the last two blocks keep the pooling layers for the purpose of fine-tuingï¼Œchange strides from 2 to 1 for dense map(h/8), the first fully convolutional 7*7*4096 is computational, thus change to 4*4 / 3*3 convs further computation decreasement: reduce the fc channels from 4096 to 1024 train labelï¼šground truth subsampled by 8 loss functionï¼šcross-entropy test x8ï¼šsimply bilinear interpolation fcnï¼šstride32 forces them to use learned upsampling layers, significantly increasing the complexity and training time CRF short-rangeï¼šused to smooth noisy fully connected modelï¼što recover detailed local structure rather than further smooth it energy function: E(x) = \sum_{i}\theta_i(x_i) + \sum_{ij}\theta_{ij}(x_i, x_j)\\ \theta_i(x_i) = -logP(x_i)\\ $P(x_i)$ is the bi-linear interpolated probability output of DCNN. \theta_{ij}(x_i, x_j) = \mu(x_i, x_j)\sum_{m=1}^K \omega_m k^m (f_i,f_j)\\ \mu(x_i, x_j) = \begin{cases} 1& \text{if }x_i \neq x_j\\ 0& \text{otherwise} \end{cases} $k^m(f_i, f_j)$ is the Gaussian kernel depends on features (involving pixel positions &amp; pixel color intensities) multi-scale prediction to increase the boundary localization accuracy we attach to the input image and the output of each of the first four max pooling layers a two-layer MLP (first layer: 128 3x3 convolutional filters, second layer: 128 1x1 convolutional filters) the feature maps above is concatenated to the main networkâ€™s last layer feature map the new outputs is enhanced by 128*5=640 channels we only adjust the newly added weights introducing these extra direct connections from fine-resolution layers improves localization performance, yet the effect is not as dramatic as the one obtained with the fully-connected CRF ç©ºæ´å·ç§¯dilated convolution ç©ºæ´å·ç§¯ç›¸æ¯”è¾ƒäºæ­£å¸¸å·ç§¯ï¼Œå¤šäº†ä¸€ä¸ª hyper-parameterâ€”â€”dilation rateï¼ŒæŒ‡çš„æ˜¯kernelçš„é—´éš”æ•°é‡(æ­£å¸¸çš„convolution dilatation rateæ˜¯1) fcnï¼šå…ˆpoolingå†upsamplingï¼Œè¿‡ç¨‹ä¸­æœ‰ä¿¡æ¯æŸå¤±ï¼Œèƒ½ä¸èƒ½è®¾è®¡ä¸€ç§æ–°çš„æ“ä½œï¼Œä¸é€šè¿‡poolingä¹Ÿèƒ½æœ‰è¾ƒå¤§çš„æ„Ÿå—é‡çœ‹åˆ°æ›´å¤šçš„ä¿¡æ¯å‘¢ï¼Ÿ å¦‚å›¾(b)çš„2-dilated convï¼Œkernel sizeåªæœ‰3x3ï¼Œä½†æ˜¯è¿™ä¸ªå·ç§¯çš„æ„Ÿå—é‡å·²ç»å¢å¤§åˆ°äº†7x7ï¼ˆå‡è®¾å‰ä¸€å±‚æ˜¯3x3çš„1-dilated convï¼‰ å¦‚å›¾(c)çš„4-dilated convï¼Œkernel sizeåªæœ‰3x3ï¼Œä½†æ˜¯è¿™ä¸ªå·ç§¯çš„æ„Ÿå—é‡å·²ç»å¢å¤§åˆ°äº†15x15ï¼ˆå‡è®¾å‰ä¸¤å±‚æ˜¯3x3çš„1-dilated convå’Œ3x3çš„2-dilated convï¼‰ è€Œä¼ ç»Ÿçš„ä¸‰ä¸ª3x3çš„1-dilated convå †å ï¼Œåªèƒ½è¾¾åˆ°7x7çš„æ„Ÿå—é‡ dilatedä½¿å¾—åœ¨ä¸åšpoolingæŸå¤±ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼ŒåŠ å¤§äº†æ„Ÿå—é‡ï¼Œè®©æ¯ä¸ªå·ç§¯è¾“å‡ºéƒ½åŒ…å«è¾ƒå¤§èŒƒå›´çš„ä¿¡æ¯ The Gridding Effectï¼šå¦‚ä¸‹å›¾ï¼Œå¤šæ¬¡å åŠ 3x3çš„2-dilated convï¼Œä¼šå‘ç°æˆ‘ä»¬å°†æ„¿è¾“å…¥ç¦»æ•£åŒ–äº†ã€‚å› æ­¤å åŠ å·ç§¯çš„ dilation rate ä¸èƒ½æœ‰å¤§äº1çš„å…¬çº¦æ•°ã€‚ Long-ranged informationï¼šå¢å¤§dilation rateå¯¹å¤§ç‰©ä½“æœ‰æ•ˆæœï¼Œå¯¹å°ç‰©ä½“å¯èƒ½æœ‰å¼Šæ— åˆ© HDC(Hybrid Dilated Convolution)è®¾è®¡ç»“æ„ å åŠ å·ç§¯çš„ dilation rate ä¸èƒ½æœ‰å¤§äº1çš„å…¬çº¦æ•°ï¼Œå¦‚[2,4,6] å°† dilation rate è®¾è®¡æˆé”¯é½¿çŠ¶ç»“æ„ï¼Œä¾‹å¦‚ [1, 2, 5, 1, 2, 5] å¾ªç¯ç»“æ„ï¼Œé”¯é½¿çŠ¶èƒ½å¤ŸåŒæ—¶æ»¡è¶³å°ç‰©ä½“å¤§ç‰©ä½“çš„åˆ†å‰²è¦æ±‚(å° dilation rate æ¥å…³å¿ƒè¿‘è·ç¦»ä¿¡æ¯ï¼Œå¤§ dilation rate æ¥å…³å¿ƒè¿œè·ç¦»ä¿¡æ¯) æ»¡è¶³$M_i = max [M_{i+1}-2r_i, M_{i+1}-2(M_{i+1}-r_i), r_i]$ï¼Œ$M_i$æ˜¯ç¬¬iå±‚æœ€å¤§dilation rate ä¸€ä¸ªå¯è¡Œæ–¹æ¡ˆ[1,2,5]ï¼š deeplabV2: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs åŠ¨æœº atrous convolutionï¼šcontrol the resolution atrous spatial pyramid pooling (ASPP) ï¼šmultiple sampling rates fully connected Conditional Random Field (CRF) è®ºç‚¹ three challenges in the application of DCNNs to semantic image segmentation reduced feature resolutionï¼šmax-pooling and downsampling (â€˜stridingâ€™) â€”&gt; atrous convolution existence of objects at multiple scalesï¼šmulti input scale â€”&gt; ASPP reduced localization accuracy due to DCNN invarianceï¼šskip-layers â€”&gt; CRF improvements compared to its first version better segment objects at multiple scales ResNet replaces VGG16 a more comprehensive experimental evaluation on models &amp; dataset related works jointly learning of the DCNN and CRF to form an end-to-end trainable feed-forward network while in our work still a 2 stage process use a series of atrous convolutional layers with increasing rates to aggregate multiscale context while in our structure using parallel instead of serial æ–¹æ³• atrous convolution åœ¨ä¸‹é‡‡æ ·ä»¥åçš„ç‰¹å¾å›¾ä¸Šï¼Œè¿è¡Œæ™®é€šå·ç§¯ï¼Œç›¸å½“äºåœ¨åŸå›¾ä¸Šè¿è¡Œä¸Šé‡‡æ ·çš„filter 1-Dç¤ºæ„å›¾ä¸Šå¯ä»¥çœ‹å‡ºï¼Œä¸¤è€…æ„Ÿå—é‡ç›¸åŒ åŒæ—¶èƒ½ä¿æŒhigh resolution while both the number of filter parameters and the number of operations per position stay constant æŠŠbackboneä¸­ä¸‹é‡‡æ ·çš„å±‚(pooling/conv)ä¸­çš„strideæ”¹æˆ1ï¼Œç„¶åå°†æ¥ä¸‹æ¥çš„convå±‚éƒ½æ”¹æˆ2-dilated convï¼šcould allow us to compute feature responses at the original image resolution efficiency/accuracy trade-offï¼šusing atrous convolution to increase the resolution by a factor of 4 followed by fast bilinear interpolation by a factor of 8 to the original image resolution Bilinear interpolation is sufficient in this setting because the class score maps are quite smooth unlike FCN Atrous convolution offers easily control of the field-of-view and finds the best trade-off between accurate localization (small field-of-view) and context assimilation (large field-of-view)ï¼šå¤§æ„Ÿå—é‡ï¼ŒæŠ½è±¡èåˆä¸Šä¸‹æ–‡ï¼Œå¤§æ„Ÿå—é‡ï¼Œlow-levelå±€éƒ¨ä¿¡æ¯å‡†ç¡® å®ç°ï¼šï¼ˆ1ï¼‰æ ¹æ®å®šä¹‰ï¼Œç»™filterä¸Šé‡‡æ ·ï¼Œæ’0ï¼›ï¼ˆ2ï¼‰ç»™feature mapä¸‹é‡‡æ ·å¾—åˆ°k*kä¸ªreduced resolution mapsï¼Œç„¶årun orgin convï¼Œç»„åˆä½ç§»ç»“æœ ASPP multi input scaleï¼š run parallel DCNN branches that share the same parameters fuse by taking at each position the maximum response across scales computing spatial pyramid pooling run multiple parallel filters with different rates multi-scale features are further processed in separate branchesï¼šfc7&amp;fc8 fuseï¼šsum fusion CRFï¼škeep the same as V1 deeplabV3: Rethinking Atrous Convolution for Semantic Image Segmentation åŠ¨æœº for segmenting objects at multiple scales employ atrous convolution in cascade or in parallel with multiple atrous rates augment ASPP with image-level features encoding global context and further boost performance without DenseCRF è®ºç‚¹ our proposed module consists of atrous convolution with various rates and batch normalization layers modules in cascade or in parallelï¼šwhen applying a 3*3 atrous convolution with an extremely large rate, it fails to capture long range information due to image boundary effects æ–¹æ³• Atrous Convolution for each location $i$ on the output $y$ and a filter $w$, an $r$-rate atrous convolution is applied over the input feature map $x$ï¼š y[i] = \sum_k x[i+rk]w[k] in cascade duplicate several copies of the last ResNet block (block4) extra block5, block6, block7 as replicas of block4 multi-rates ASPP we include batch normalization within ASPP as the sampling rate becomes larger, the number of valid filter weights becomes smaller (beyond boundary) to incorporate global context informationï¼šwe adopt image-level features by GAP on the last feature map of the model GAP â€”&gt; 1*1*256 conv â€”&gt; BN â€”&gt; bilinearly upsample fusion: concatenated + 1*1 conv segï¼šfinal 1*1*n_classes conv training details large crop size required to make sure the large atrous rates effective upsample the output: it is important to keep the groundtruths intact and instead upsample the final logits ç»“è®º output stride=8 å¥½è¿‡16ï¼Œä½†æ˜¯è¿ç®—é€Ÿåº¦æ…¢äº†å‡ å€ deeplabV3+: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation åŠ¨æœº spatial pyramid pooling module captures rich contextual information encode-decoder structure captures sharp object boundaries combine the above two methods propose a simple yet effective decoder module explore Xception backbone è®ºç‚¹ even though rich semantic information is encoded through ASPP, detailed information related to object boundaries is missing due to striding operations atrous convolution could alleviate but suffer the computational balance while encoder-decoder models lend themselves to faster computation (since no features are dilated) in the encoder path and gradually recover sharp object boundaries in the decoder path æ‰€è°“encoder-decoder structureï¼Œå°±æ˜¯é€šè¿‡encoderå’Œdecoderä¹‹é—´çš„çŸ­è¿æ¥æ¥å°†ä¸åŒå°ºåº¦çš„ç‰¹å¾é›†æˆèµ·æ¥ï¼Œå¢åŠ è¿™æ ·çš„shortcutï¼ŒåŒæ—¶å¢å¤§ç½‘ç»œçš„ä¸‹é‡‡æ ·ç‡ï¼ˆencoder pathä¸Šä¸ä½¿ç”¨ç©ºæ´å·ç§¯ï¼Œå› æ­¤ä¸ºäº†è¾¾åˆ°åŒæ ·çš„æ„Ÿå—é‡ï¼Œå¾—å¢åŠ poolingï¼Œç„¶åä¿ç•™æœ€åº•ç«¯çš„ASPP blockï¼‰ï¼Œæ—¢å‡å°‘äº†è®¡ç®—ï¼Œåˆenrichäº†local borderè¿™ç§ç»†èŠ‚ç‰¹å¾ applying the atrous separable convolution to both the ASPP and decoder modulesï¼šæœ€ååˆå¼•å…¥å¯åˆ†ç¦»å·ç§¯ï¼Œè¿›ä¸€æ­¥æå‡è®¡ç®—æ•ˆç‡ æ–¹æ³• atrous separable convolution significantly reduces the computation complexity while maintaining similar (or better) performance DeepLabv3 as encoder output_stride=16/8ï¼šremove the striding of the last 1/2 blocks atrous convolutionï¼šapply atrous convolution to the blocks without striding ASPPï¼šrun 1x1 conv in the end to set the output channel to 256 proposed decoder naive decoderï¼šbilinearly upsampled by 16 proposedï¼šfirst bilinearly upsampled by 4, then concatenated with the corresponding low-level features low-level featuresï¼š apply 1x1 conv on the low-level features to reduce the number of channels to avoid outweigh the importance the last feature map in res2x residual block before striding combined featuresï¼šapply 3x3 conv(2 layers, 256 channels) to obtain sharper segmentation results more shortcutï¼šobserved no significant improvement modified Xception backbone deeper all the max pooling operations are replaced with depthwise separable convolutions with striding DWconv-BN-ReLU-PWconv-BN-ReLU å®éªŒ decoder effect on border f Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation]]></content>
      <tags>
        <tag>papers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RCNNç³»åˆ—]]></title>
    <url>%2F2020%2F01%2F08%2FRCNN%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[ç»¼è¿° papers [R-CNN] R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation [SPP] SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition [Fast R-CNN] Fast R-CNN: Fast Region-based Convolutional Network [Faster R-CNN] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks [Mask R-CNN] Mask R-CNN [FPN] FPN: Feature Pyramid Networks for Object Detection [Cascade R-CNN] Cascade R-CNN: Delving into High Quality Object Detection R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation åŠ¨æœº localizing objects with a deep network and training a high-capacity model with only a small quantity of annotated detection data apply CNN to region proposals: R-CNN represents â€˜Regions with CNN featuresâ€™ supervised pre-training è®ºç‚¹ model as a regression problem: not fare well in practice build a sliding-window detector: have to maintain high spatial resolution what we do: our method gener- ates around 2000 category-independent region proposals for the input image, extracts a fixed-length feature vector from each proposal using a CNN, and then classifies each region with category-specific linear SVMs conventional solution to training a large CNN is â€˜using unsupervised pre-training, followed by supervised fine-tuningâ€™ what we do: â€˜supervised pre-training on a large auxiliary dataset (ILSVRC), followed by domain specific fine-tuning on a small dataset (PASCAL)â€™ we also demonstrate: a simple bounding box regression method significantly reduces mislocalizations R-CNN operates on regions: it is natural to extend it to the task of semantic segmentation è¦ç´  category-independent region proposals a large convolutional neural network that extracts a fixed-length feature vector from each region a set of class-specific linear SVMs æ–¹æ³• Region proposals: we use selective search Feature extraction: we use Krizhevsky CNN, 227*227 RGB input, 5 convs, 2 fcs, 4096 output we first dilate the tight bounding box (padding=16) then warp the bounding box to the required size (å„å‘å¼‚æ€§ç¼©æ”¾) Test-time detection: we score each extracted feature vector using the SVM trained for each class we apply a greedy non-maximum suppression (for each class independently) å¯¹ç•™ä¸‹çš„è¿™äº›æ¡†è¿›è¡Œcannyè¾¹ç¼˜æ£€æµ‹ï¼Œå°±å¯ä»¥å¾—åˆ°bounding-box (then B-BoxRegression) Supervised pre-training: pre-trained the CNN on a large auxiliary dataset (ILSVRC 2012) with image-level annotations Domain-specific fine-tuning: continue SGD training of the CNN using only warped region proposals from VOC replace the 1000-way classification layer with a randomly initialized 21-way layer (20 VOC classes plus background) class label: all region proposals with â‰¥ 0.5 IoU overlap with a ground-truth box as positives, else negatives 1/10th of the initial pre-training rate uniformly sample 32 positive windows (over all classes) and 96 background windows to construct a mini-batch of size 128 Object category classifiers: considering a binary classifier for a specific class class label: take IoU overlap threshold &lt;0.3 as negatives, take only regions tightly enclosing the object as positives take the ground-truth bounding boxes for each class as positives unexplained: the positive and negative examples are defined differently in CNN fine-tuning versus SVM training CNNå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œéœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œæ‰€ä»¥åœ¨CNNè®­ç»ƒé˜¶æ®µæˆ‘ä»¬å¯¹Bounding boxçš„ä½ç½®é™åˆ¶æ¡ä»¶é™åˆ¶çš„æ¯”è¾ƒæ¾(IOUåªè¦å¤§äº0.5éƒ½è¢«æ ‡æ³¨ä¸ºæ­£æ ·æœ¬)ï¼Œsvmé€‚ç”¨äºå°‘æ ·æœ¬è®­ç»ƒï¼Œæ‰€ä»¥å¯¹äºè®­ç»ƒæ ·æœ¬æ•°æ®çš„IOUè¦æ±‚æ¯”è¾ƒä¸¥æ ¼ï¼Œæˆ‘ä»¬åªæœ‰å½“bounding boxæŠŠæ•´ä¸ªç‰©ä½“éƒ½åŒ…å«è¿›å»äº†ï¼Œæˆ‘ä»¬æ‰æŠŠå®ƒæ ‡æ³¨ä¸ºç‰©ä½“ç±»åˆ«ã€‚ itâ€™s necessary to train detection classifiers rather than simply use outputs of the fine-tuned CNN ä¸Šä¸€ä¸ªå›ç­”å…¶å®åŒæ—¶ä¹Ÿè§£é‡Šäº†CNNçš„headå·²ç»æ˜¯ä¸€ä¸ªåˆ†ç±»å™¨äº†ï¼Œè¿˜è¦ç”¨SVMåˆ†ç±»ï¼šæŒ‰ç…§ä¸Šè¿°æ­£è´Ÿæ ·æœ¬å®šä¹‰ï¼ŒCNN softmaxçš„è¾“å‡ºæ¯”é‡‡ç”¨svmç²¾åº¦ä½ã€‚ åˆ†æ learned features: compute the unitsâ€™ activations on a large set of held-out region proposals sort from the highest to low perform non-maximum suppression display the top-scoring regions Ablation studies: without fine-tuning: features from fc7 generalize worse than features from fc6, indicating that most of the CNNâ€™s representational power comes from its convolutional layers with fine-tuning: The boost from fine-tuning is much larger for fc6 and fc7 than for pool5, suggests that pool features learned from ImageNet are general and that most of the improvement is gained from learning domain-specific non-linear classifiers on top of them Detection error analysis: more of our errors result from poor localization rather than confusion CNN features are much more discriminative than HOG Loose localization likely results from our use of bottom-up region proposals and the positional invariance learned from pre-training the CNN for whole-image classification(ç²—æš´çš„IOUåˆ¤å®šå‰èƒŒæ™¯ï¼ŒäºŒå€¼åŒ–labelï¼Œæ— æ³•ä½“ç°å®šä½å¥½åå·®å¼‚) Bounding box regressionï¼š a linear regression model use the pool5 features for a selective search region proposal as input è¾“å‡ºä¸ºxyæ–¹å‘çš„ç¼©æ”¾å’Œå¹³ç§» è®­ç»ƒæ ·æœ¬ï¼šåˆ¤å®šä¸ºæœ¬ç±»çš„å€™é€‰æ¡†ä¸­å’ŒçœŸå€¼é‡å é¢ç§¯å¤§äº0.6çš„å€™é€‰æ¡† Semantic segmentationï¼š three strategies for computing features: â€˜full â€˜ ignores the regionâ€™s shape, two regions with different shape might have very similar bounding boxes(ä¿¡æ¯ä¸å……åˆ†) â€˜fg â€˜ slightly outperforms full, indicating that the masked region shape provides a stronger signal â€˜full+fg â€˜ achieves the best, indicating that the context provided by the full features is highly informative even given the fg features(å½¢çŠ¶å’Œcontextä¿¡æ¯éƒ½é‡è¦) SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition åŠ¨æœºï¼š propose a new pooling strategy, â€œspatial pyramid poolingâ€ can generate a fixed-length representation regardless of image size/scale also robust to object deformations è®ºç‚¹ï¼š existing CNNs require a fixed-size input reduce accuracy for sub-images of an arbitrary size/scale (need cropping/warping) cropped region lost content, while warped content generates unwanted distortion overlooks the issues involving scales convolutional layers do not require a fixed image size, whle the fully-connected layers need to have fixed- size/length input by their definition by introducing the SPP layer between the last convolutional layer and the first fully-connected layer pools the features and generates fixed- length outputs Spatial pyramid pooling partitions the image into divisions from finer to coarser levels, and aggregates local features in them generates fixed- length output uses multi-level spatial bins(robust to object deformations ) can run at variable scales also allows varying sizes or scales during training: train the network with different input size at different epoch increases scale-invariance reduces over-fitting in object detection run the convolutional layers only once on the entire image then extract features by SPP-net on the feature maps speedup accuracy æ–¹æ³•ï¼š Convolutional Layers and Feature Maps the outputs of the convolutional layers are known as feature maps feature maps involve not only the strength of the responses(the strength of activation), but also their spatial positions(the reception field) The Spatial Pyramid Pooling Layer it can maintain spatial information by pooling in local spatial bins the spatial bins have sizes proportional to the image size(k-level: 1*1, 2*2, â€¦, k*k) we can resize the input image to any scale, which is important for the accuracy the coarsest pyramid level has a single bin that covers the entire image, which is in fact a â€œglobal poolingâ€ operation for a feature map of $aÃ—a$, with a pyramid level of $nÃ—n$ bins: the\ window\ size:\ win = ceiling(a/n)\\ the\ stride:\ str = floor(a/n) Training the Network Single-size training: fixed-size input (224Ã—224) cropped from images, cropping for data augmentation Multi-size training: rather than cropping, we resize the aforementioned 224Ã—224 region to 180Ã—180, then we train two fixed-size networks that share parameters by altenate epoch åˆ†æ 50 bins vs. 30 bins: the gain of multi-level pooling is not simply due to more parameters, it is because the multi-level pooling is robust to the variance in object deformations and spatial layout multi-size vs. single-size: multi results are more or less better than the single-size version full vs. crop: shows the importance of maintaining the complete content SPP-NET FOR OBJECT DETECTION We extract the feature maps from the entire image only once we apply the spatial pyramid pooling on each candidate window of the feature maps These representations are provided to the fully-connected layers of the network SVM samples: We use the ground-truth windows to generate the positive samples, use the samples with IOU&lt;30% as the negative samples multi-scale feature extraction: We resize the image at {480, 576, 688, 864, 1200}, and compute the feature maps of conv5 for each scale. we choose a single scale s âˆˆ S such that the scaled candidate window has a number of pixels closest to 224Ã—224. And we use the corresponding feature map to compute the feature for this window this is roughly equivalent to resizing the window to 224Ã—224 fine-tuning: Since our features are pooled from the conv5 feature maps from windows of any sizes for simplicity we only fine-tune the fully-connected layers Mapping a Window to Feature Maps** we project the corner point of a window onto a pixel in the feature maps, such that this corner point in the image domain is closest to the center of the receptive field of that feature map pixel. â€‹ ç¡®å®šåŸå›¾ä¸Šçš„ä¸¤ä¸ªè§’ç‚¹ï¼ˆå·¦ä¸Šè§’å’Œå³ä¸‹è§’ï¼‰ï¼Œæ˜ å°„åˆ° feature mapä¸Šçš„ä¸¤ä¸ªå¯¹åº”ç‚¹ï¼Œä½¿å¾—æ˜ å°„ç‚¹$(x^{â€˜}, y^{â€˜})$åœ¨åŸå§‹å›¾ä¸Šæ„Ÿå—é‡ï¼ˆä¸Šå›¾ç»¿è‰²æ¡†ï¼‰çš„ä¸­å¿ƒç‚¹ä¸$(x,y)$å°½å¯èƒ½æ¥è¿‘ã€‚ Fast R-CNN: Fast Region-based Convolutional Network åŠ¨æœº improve training and testing speed increase detection accuracy è®ºç‚¹ current approaches train models in multi-stage pipelines that are slow and inelegant R-CNN &amp; SPPnet: CNN+SVM+bounding-box regression disk storage: features are written to disk SPPnet: can only fine-tuning the fc layers, limits the accuracy of very deep networks task complexity: numerous candidate proposals rough localization proposals must be refined We propose: a single-stage training algorithm multi-task: jointly learns to classify object proposals and refine their spatial locations è¦ç´  input: an entire image and a set of object proposals convs a region of interest (RoI) pooling layer: extracts a fixed-length feature vector from the feature map fcs that finally branch into two sibling output layers multi-outputs: one produces softmax probability over K+1 classes one outputs four bounding-box regression offsets per class æ–¹æ³• RoI pooling an RoI is a rectangular window inside a conv feature map, which can be defined by (r, c, h, w) the RoI pooling layer converts the features inside any valid RoI into a small feature map with a fixed size H Ã— W it is a special case of SPPnet when there is only one pyramid level (pooling window size = h/H * w/W) Initializing from pre-trained networks the last max pooling layer is replaced by a RoI pooling layer the last fully connected layer and softmax is replaced by the wo sibling layers + respective head (softmax &amp; regressor) modified to take two inputs Fine-tuning for detection why SPPnet is unable to update weights below the spatial pyramid pooling layer: åŸæ–‡æåˆ°feature vectoræ¥æºäºä¸åŒå°ºå¯¸çš„å›¾åƒâ€”â€”ä¸æ˜¯ä¸»è¦åŸå›  feature vectoråœ¨åŸå›¾ä¸Šçš„æ„Ÿå—é‡é€šå¸¸å¾ˆå¤§ï¼ˆæ¥è¿‘å…¨å›¾ï¼‰â€”â€”forward passçš„è®¡ç®—é‡å°±å¾ˆå¤§ ä¸åŒçš„å›¾ç‰‡forward passçš„è®¡ç®—ç»“æœä¸èƒ½å¤ç”¨ï¼ˆwhen each training sample (i.e. RoI) comes from a different image, which is exactly how R-CNN and SPPnet networks are trainedï¼‰ We propose: takes advantage of feature sharing mini-batches are sampled hierarchically: N images and R/N RoIs from each image RoIs from the same image share computation and memory in the forward and backward passes jointly optimize the two tasks each RoI is labeled with a ground-truth class $u$ and a ground-truth bounding-box regression target $v$ the network outputs are K+1 probability $p=(p_0,â€¦p_k)$ and K b-box regression offsets $t^k=(t_x^k, t_y^k, t_w^k,t_h^k)$ L(p, u, t^u, v) = L_{cls}(p,u) + \lambda[u>0]L_{loc}(t^u,v)\\ $L_{cls}$: L_{cls}(p,u) = -log p_u\\ $L_{loc}$: L_{loc}(t^u, v) = \sum_{i \in \{x,y,w,h\}}smooth_{L_1}(t^u_i - v_i)\\ smooth_{L_1}(x) = \begin{cases} 0.5x^2\ \ \ \ \ \ \ \ \ \ \ if |x|]]></content>
      <tags>
        <tag>ç›®æ ‡æ£€æµ‹ï¼Œtwo-stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN Visualizationç³»åˆ—]]></title>
    <url>%2F2020%2F01%2F03%2FCNN-Visualization%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[1. Visualizing and Understanding Convolutional Networks åŠ¨æœº give insight into the internal operation and behavior of the complex models then one can design better models reveal which parts of the scene in image are important for classification explore the generalization ability of the model to other datasets è®ºç‚¹ most visualizing methods limited to the 1st layer where projections to pixel space are possible Our approach propose a method that could projects high level feature maps to the pixel space * some methods give some insight into invariances basing on a simple quadratic approximation * Our approach, by contrast, provides a non-parametric view of invariance * some methods associate patches that responsible for strong activations at higher layers * In our approach they are not just crops of input images, but rather top-down projections that reveal structures æ–¹æ³• 3.1 Deconvnet: use deconvnet to project the feature activations back to the input pixel space To examine a given convnet activation, we set all other activations in the layer to zero and pass the feature maps as input to the attached deconvnet layer Then successively (i) unpool, (ii) rectify and (iii) filter to reconstruct the activity of the layer beneath until the input pixel space is reached ã€Unpoolingã€‘using switches ã€Rectificationã€‘the convnet uses relu to ensure always positive, same for back projection ã€Filteringã€‘transposed conv Due to unpooling, the reconstruction obtained from a single activation resembles a small piece of the original input image 3.2 CNN model 3.3 visualization among layers for each layer, we take the top9 strongest activation across the validation data calculate the back projection separately alongside we provide the corresponding image patches 3.4 visualization during training randomly choose several strongest activation of a given feature map lower layers converge fast, higher layers conversely 3.5 visualizing the Feature Invariance 5 sample images being translated, rotated and scaled by varying degrees Small transformations have a dramatic effect in the first layer of the model(c2 &amp; c3å¯¹æ¯”) the network is stable to translations and scalings, but not invariant to rotation 3.6 architecture selection old architecture(stride4, filterSize11)ï¼šThe first layer filters are a mix of extremely high and low frequency information, with little coverage of the mid frequencies. The 2nd layer visualization shows aliasing artifacts caused by the large stride 4 used in the 1st layer convolutions. (è¿™ç‚¹å¯ä»¥å‚è€ƒä¹‹å‰vnetä¸­æåˆ°çš„ï¼Œdeconvå¯¼è‡´çš„æ£‹ç›˜æ ¼ä¼ªå½±ï¼Œå¤§strideä¼šæ›´æ˜æ˜¾) smaller stride &amp; smaller filter(stride2, filterSize7)ï¼šmore coverage of mid frequencies, no aliasing, no dead feature 3.7 å¯¹äºç‰©ä½“çš„å…³é”®éƒ¨åˆ†é®æŒ¡ä¹‹åä¼šæå¤§çš„å½±å“åˆ†ç±»ç»“æœ ç¬¬äºŒä¸ªå’Œç¬¬ä¸‰ä¸ªä¾‹å­ä¸­åˆ†åˆ«æ˜¯æ–‡å­—å’Œäººè„¸çš„å“åº”æ›´é«˜ï¼Œä½†æ˜¯å´ä¸æ˜¯å…³é”®éƒ¨åˆ†ã€‚ ç†è§£ 4.1 æ€»çš„æ¥è¯´ï¼Œç½‘ç»œå­¦ä¹ åˆ°çš„ç‰¹å¾ï¼Œæ˜¯å…·æœ‰è¾¨åˆ«æ€§çš„ç‰¹å¾ï¼Œé€šè¿‡å¯è§†åŒ–å°±å¯ä»¥çœ‹åˆ°æˆ‘ä»¬æå–åˆ°çš„ç‰¹å¾å¿½è§†äº†èƒŒæ™¯ï¼Œè€Œæ˜¯æŠŠå…³é”®çš„ä¿¡æ¯ç»™æå–å‡ºæ¥äº†ã€‚ä»layer 1ã€layer 2å­¦ä¹ åˆ°çš„ç‰¹å¾åŸºæœ¬ä¸Šæ˜¯é¢œè‰²ã€è¾¹ç¼˜ç­‰ä½å±‚ç‰¹å¾ï¼›layer 3åˆ™å¼€å§‹ç¨å¾®å˜å¾—å¤æ‚ï¼Œå­¦ä¹ åˆ°çš„æ˜¯çº¹ç†ç‰¹å¾ï¼Œæ¯”å¦‚ä¸Šé¢çš„ä¸€äº›ç½‘æ ¼çº¹ç†ï¼›layer 4å­¦ä¹ åˆ°çš„åˆ™æ˜¯è¾ƒå¤šçš„ç±»åˆ«ä¿¡æ¯ï¼Œæ¯”å¦‚ç‹—å¤´ï¼›layer 5å¯¹åº”ç€æ›´å¼ºçš„ä¸å˜æ€§ï¼Œå¯ä»¥åŒ…å«ç‰©ä½“çš„æ•´ä½“ä¿¡æ¯ã€‚ã€‚ 4.2 åœ¨ç½‘ç»œè¿­ä»£çš„è¿‡ç¨‹ä¸­ï¼Œç‰¹å¾å›¾å‡ºç°äº†sudden jumpsã€‚ä½å±‚åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­åŸºæœ¬æ²¡å•¥å˜åŒ–ï¼Œæ¯”è¾ƒå®¹æ˜“æ”¶æ•›ï¼Œé«˜å±‚çš„ç‰¹å¾å­¦ä¹ åˆ™å˜åŒ–å¾ˆå¤§ã€‚è¿™è§£é‡Šäº†ä½å±‚ç½‘ç»œçš„ä»è®­ç»ƒå¼€å§‹ï¼ŒåŸºæœ¬ä¸Šæ²¡æœ‰å¤ªå¤§çš„å˜åŒ–ï¼Œå› ä¸ºæ¢¯åº¦å¼¥æ•£ã€‚é«˜å±‚ç½‘ç»œåˆšå¼€å§‹å‡ æ¬¡çš„è¿­ä»£ï¼Œå˜åŒ–ä¸æ˜¯å¾ˆå¤§ï¼Œä½†æ˜¯åˆ°äº†40~50çš„è¿­ä»£çš„æ—¶å€™ï¼Œå˜åŒ–å¾ˆå¤§ï¼Œå› æ­¤æˆ‘ä»¬ä»¥ååœ¨è®­ç»ƒç½‘ç»œçš„æ—¶å€™ï¼Œä¸è¦ç€æ€¥çœ‹ç»“æœï¼Œçœ‹ç»“æœéœ€è¦ä¿è¯ç½‘ç»œæ”¶æ•›ã€‚ 4.3 å›¾åƒçš„å¹³ç§»ã€ç¼©æ”¾ã€æ—‹è½¬ï¼Œå¯ä»¥çœ‹å‡ºç¬¬ä¸€å±‚ä¸­å¯¹äºå›¾åƒå˜åŒ–éå¸¸æ•æ„Ÿï¼Œç¬¬7å±‚å°±æ¥è¿‘äºçº¿æ€§å˜åŒ–ã€‚ 2. Striving for Simplicity: The All Convolutional Net åŠ¨æœº traditional pipeline: alternating convolution and max-pooling layers followed by a small number of fully connected layers questioning the necessity of different components in the pipeline, max-pooling layer to be specified to analyze the network we introduce a new variant of the â€œdeconvolution approachâ€ for visualizing features è®ºç‚¹ two major improving directions based on traditional pipeline using more complex activation functions building multiple conv modules we study the most simple architecture we could conceive a homogeneous network solely consisting of convolutional layers without the need for complicated activation functions, any response normalization or max-pooling reaches state of the art performance æ–¹æ³• replace the pooling layers with standard convolutional layers with stride two the spatial dimensionality reduction performed by pooling makes covering larger parts of the input in higher layers possible which is crucial for achieving good performance with CNNs make use of small convolutional layers greatly reduce the number of parameters in a network and thus serve as a form of regularization if the topmost convolutional layer covers a portion of the image large enough to recognize its content then fully connected layers can also be replaced by simple 1-by-1 convolutions the overall architecture consists only of convolutional layers with rectified linear non-linearities and an averaging + softmax layer to produce predictions Strided-CNN-C: pooling is removed and the preceded conv stride is increase ConvPool-CNN-C: a dense conv is placed, to show the effect of increasing parameters All-CNN-C: max-pooling is replaced by conv when pooling is replaced by an additional convolution layer with stride 2, performance stabilizes and even improves small 3 Ã— 3 convolutions stacked after each other seem to be enough to achieve the best performance guided backpropagation the paper above proposed â€˜deconvnetâ€™, which we observe that it does not always work well without max-pooling layers For higher layers of our network the method of Zeiler and Fergus fails to produce sharp, recognizable image structure Our architecture does not include max-pooling, thus we can â€™deconvolveâ€™ without switches, i.e. not conditioning on an input image In order to obtain a reconstruction conditioned on an input image from our network without pooling layers we to combine the simple backward pass and the deconvnet Interestingly, the very first layer of the network does not learn the usual Gabor filters, but higher layers do 3. Cam: Learning Deep Features for Discriminative Localization åŠ¨æœº we found that CNNs actually behave as object detectors despite no supervision on the location this ability is lost when fully-connected layers are used for classification we found that the advantages of global average pooling layers are beyond simply acting as a regularizer it makes it easily to localize the discriminative image regions despite not being trained for them è®ºç‚¹ 2.1 Weakly-supervised object localization previous methods are not trained end-to-end and require multiple forward passes Our approach is trained end-to-end and can localize objects in a single forward pass 2.2 Visualizing CNNs previous methods only analyze the convolutional layers, ignoring the fully connected thereby painting an incomplete picture of the full story we are able to understand our network from the beginning to the end æ–¹æ³• 3.1 Class Activation Mapping A class activation map for a particular category indicates the discriminative image regions used by the network to identify that category the network architecture: convsâ€”-gapâ€”-fc+softmax we can identify the importance of the image regions by projecting back the weights of the output layer on to the convolutional feature maps by simply upsampling the class activation map to the size of the input image we can identify the image regions most relevant to the particular category 3.2 Weakly-supervised Object Localization our technique does not adversely impact the classification performance when learning to localize we found that the localization ability of the networks improved when the last convolutional layer before GAP had a higher spatial resolution, thus we removed several convolutional layers from the origin networks overall we find that the classification performance is largely preserved for our GAP networks compared with the origin fc structure our CAM approach significantly outperforms the backpropagation approach on generating bounding box low mapping resolution prevents the network from obtaining accurate localizations 3.3 Visualizing Class-Specific Units the convolutional units of various layers of CNNs act as visual concept detec- tors, identifying low-level concepts like textures or mate- rials, to high-level concepts like objects or scenes Deeper into the network, the units become increasingly discriminative given the fully-connected layers in many networks, it can be difficult to identify the importance of different units for identifying different categories 4. Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization5. Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks6. ç»¼è¿° GAP é¦–å…ˆå›é¡¾ä¸€ä¸‹GAPï¼ŒNiNä¸­æå‡ºäº†GAPï¼Œä¸»è¦ä¸ºäº†è§£å†³å…¨è¿æ¥å±‚å‚æ•°è¿‡å¤šï¼Œä¸æ˜“è®­ç»ƒä¸”å®¹æ˜“è¿‡æ‹Ÿåˆç­‰é—®é¢˜ã€‚ å¯¹å¤§å¤šæ•°åˆ†ç±»ä»»åŠ¡æ¥è¯´ä¸ä¼šå› ä¸ºåšäº†gapè®©ç‰¹å¾å˜å°‘è€Œè®©æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚å› ä¸ºGAPå±‚æ˜¯ä¸€ä¸ªéçº¿æ€§æ“ä½œå±‚ï¼Œè¿™Cä¸ªç‰¹å¾ç›¸å½“äºæ˜¯ä»kxkxCç»è¿‡éçº¿æ€§å˜åŒ–é€‰æ‹©å‡ºæ¥çš„å¼ºç‰¹å¾ã€‚ heatmap step1. å›¾åƒç»è¿‡å·ç§¯ç½‘ç»œåæœ€åå¾—åˆ°çš„ç‰¹å¾å›¾ï¼Œåœ¨å…¨è¿æ¥å±‚åˆ†ç±»çš„æƒé‡ï¼ˆ$w_{k,n}$ï¼‰è‚¯å®šä¸åŒï¼Œ step2. åˆ©ç”¨åå‘ä¼ æ’­æ±‚å‡ºæ¯å¼ ç‰¹å¾å›¾çš„æƒé‡ï¼Œ step3. ç”¨æ¯å¼ ç‰¹å¾å›¾ä¹˜ä»¥æƒé‡å¾—åˆ°å¸¦æƒé‡çš„ç‰¹å¾å›¾ï¼Œåœ¨ç¬¬ä¸‰ç»´æ±‚å‡å€¼ï¼Œreluæ¿€æ´»ï¼Œå½’ä¸€åŒ–å¤„ç† reluåªä¿ç•™wxå¤§äº0çš„å€¼â€”â€”æˆ‘ä»¬æ­£å“åº”æ˜¯å¯¹å½“å‰ç±»åˆ«æœ‰ç”¨çš„ç‰¹å¾ï¼Œè´Ÿå“åº”ä¼šæ‹‰ä½$\sum wx$ï¼Œå³ä¼šé™ä½å½“å‰ç±»åˆ«çš„ç½®ä¿¡åº¦ å¦‚æœæ²¡æœ‰reluï¼Œå®šä½å›¾è°±æ˜¾ç¤ºçš„ä¸ä»…ä»…æ˜¯æŸä¸€ç±»çš„ç‰¹å¾ã€‚è€Œæ˜¯æ‰€æœ‰ç±»åˆ«çš„ç‰¹å¾ã€‚ step4. å°†ç‰¹å¾å›¾resizeåˆ°åŸå›¾å°ºå¯¸ï¼Œä¾¿äºå åŠ æ˜¾ç¤º CAM CAMè¦æ±‚å¿…é¡»ä½¿ç”¨GAPå±‚ï¼Œ CAMé€‰æ‹©softmaxå±‚å€¼æœ€å¤§çš„èŠ‚ç‚¹åå‘ä¼ æ’­ï¼Œæ±‚GAPå±‚çš„æ¢¯åº¦ä½œä¸ºç‰¹å¾å›¾çš„æƒé‡ï¼Œæ¯ä¸ªGAPçš„èŠ‚ç‚¹å¯¹åº”ä¸€å¼ ç‰¹å¾å›¾ã€‚ Grad-CAM Grad-CAMä¸éœ€è¦é™åˆ¶æ¨¡å‹ç»“æ„ï¼Œ Grad-CAMé€‰æ‹©softmaxå±‚å€¼æœ€å¤§çš„èŠ‚ç‚¹åå‘ä¼ æ’­ï¼Œå¯¹æœ€åä¸€å±‚å·ç§¯å±‚æ±‚æ¢¯åº¦ï¼Œç”¨æ¯å¼ ç‰¹å¾å›¾çš„æ¢¯åº¦çš„å‡å€¼ä½œä¸ºè¯¥ç‰¹å¾å›¾çš„æƒé‡ã€‚]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NiN: network in network]]></title>
    <url>%2F2019%2F12%2F25%2FNiN-network-in-network%2F</url>
    <content type="text"><![CDATA[Network In Network åŠ¨æœº enhance model discriminability(è·å¾—æ›´å¥½çš„ç‰¹å¾æè¿°)ï¼špropose mlpconv less prone to overfittingï¼špropose global average pooling è®ºç‚¹ comparison 1: conventional CNN uses linear filter, which implicitly makes the assumption that the latent concepts are linearly separable. traditional CNN is stacking [linear filters+nonlinear activation/linear+maxpooling+nonlinear]ï¼šè¿™é‡Œå¼•å‡ºäº†ä¸€ä¸ªæ¿€æ´»å‡½æ•°å’Œæ± åŒ–å±‚å…ˆåé¡ºåºçš„é—®é¢˜ï¼Œå¯¹äºavg_poollingï¼Œä¸¤ç§æ“ä½œå¾—åˆ°çš„ç»“æœæ˜¯ä¸ä¸€æ ·çš„ï¼Œå…ˆæ¥æ¿€æ´»å‡½æ•°ä¼šä¸¢å¤±éƒ¨åˆ†ä¿¡æ¯ï¼Œæ‰€ä»¥åº”è¯¥å…ˆæ± åŒ–å†æ¿€æ´»ï¼Œå¯¹äºMAX_poolingï¼Œä¸¤ç§æ“ä½œç»“æœä¸€æ ·ï¼Œä½†æ˜¯å…ˆæ± åŒ–ä¸‹é‡‡æ ·ï¼Œå¯ä»¥å‡å°‘æ¿€æ´»å‡½æ•°çš„è®¡ç®—é‡ï¼Œæ€»ç»“å°±æ˜¯å…ˆæ± åŒ–å†æ¿€æ´»ã€‚ä½†æ˜¯å¥½å¤šç½‘ç»œå®é™…å®ç°ä¸Šéƒ½æ˜¯reluç´§è·Ÿç€convï¼Œåé¢æ¥poolingï¼Œè¿™æ ·æ¯”è¾ƒinterpretableâ€”â€”cross feature map pooling mlpconv layer can be regarded as a highly nonlinear function(filter-fc-activation-fc-activation-fc-activationâ€¦) comparison 2: maxout network imposes the prior that instances of a latent concept lie within a convex set in the input spaceã€QUESTION HEREã€‘ mlpconv layer is a universal function approximator instead of a convex function approximator comparison 3: fully connected layers are prone to overfitting and heavily depend on dropout regularization global average pooling is more meaningful and interpretable, moreover it itself is a structural regularizerã€QUESTION HEREã€‘ æ–¹æ³• use mlpconv layer to replace conventional GLM(linear filters) use global average pooling to replace traditional fully connected layers the overall structure is a stack of mlpconv layers, on top of which lie the global average pooling and the objective cost layer Sub-sampling layers can be added in between the mlpconv as in CNN dropout is applied on the outputs of all but the last mlpconv layers for regularization another regularizer applied is weight decay ç»†èŠ‚ preprocessingï¼šglobal contrast normalization and ZCA whitening augmentationï¼štranslation and horizontal flipping GAP for conventional CNNï¼šCNN+FC+DROPOUT &lt; CNN+GAP &lt; CNN+FC gap is effective as a regularizer slightly worse than the dropout regularizer result for some reason confidence maps explicitly enforce feature maps in the last mlpconv layer of NIN to be confidence maps of the categories by means of global average poolingï¼šNiNå°†GAPçš„è¾“å‡ºç›´æ¥ä½œä¸ºoutput layerï¼Œå› æ­¤æ¯ä¸€ä¸ªç±»åˆ«å¯¹åº”çš„feature mapå¯ä»¥è¿‘ä¼¼è®¤ä¸ºæ˜¯ confidence mapã€‚ the strongest activations appear roughly at the same region of the object in the original imageï¼šç‰¹å¾å›¾ä¸Šé«˜å“åº”åŒºåŸŸåŸºæœ¬ä¸åŸå›¾ä¸Šç›®æ ‡åŒºåŸŸå¯¹åº”ã€‚ this motivates the possibility of performing object detection via NIN architectureï¼šå®é™…ä¸­å¤šå±‚æ„ŸçŸ¥å™¨ä½¿ç”¨1x1convæ¥å®ç°ï¼Œå¢åŠ çš„å¤šå±‚æ„ŸçŸ¥å™¨ç›¸å½“äºæ˜¯ä¸€ä¸ªå«å‚çš„æ± åŒ–å±‚ï¼Œé€šè¿‡å¯¹å¤šä¸ªç‰¹å¾å›¾è¿›è¡Œå«å‚æ± åŒ–ï¼Œå†ä¼ é€’åˆ°ä¸‹ä¸€å±‚ç»§ç»­å«å‚æ± åŒ–ï¼Œè¿™ç§çº§è”çš„è·¨é€šé“çš„å«å‚æ± åŒ–è®©ç½‘ç»œæœ‰äº†æ›´å¤æ‚çš„è¡¨å¾èƒ½åŠ›ã€‚ æ€»ç»“ mlpconvï¼šstronger local reception unit gapï¼šregularizer &amp; bring confidence maps]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[unet & vnet]]></title>
    <url>%2F2019%2F12%2F05%2Funet-vnet%2F</url>
    <content type="text"><![CDATA[U-NET: Convolutional Networks for Biomedical Image Segmentation åŠ¨æœºï¼š train from very few images outperforms more precisely on segmentation tasks fast è¦ç´ ï¼š ç¼–ç ï¼ša contracting path to capture context è§£ç ï¼ša symmetric expanding path that enables precise localization å®ç°ï¼špooling operators &amp; upsampling operators è®ºç‚¹ï¼š when we talk about deep convolutional networksï¼š larger and deeper millions of parameters millions of training samples representative methodï¼šrun a sliding-window and predict a pixel label based on itsâ€˜ patch drawbacksï¼š calculating redundancy of overlapping patches big patchï¼šmore max-pooling layers that reduce the localization accuracy small patchï¼šless involvement of context metioned but not further explainedï¼šcascade structure æ–¹æ³•ï¼š In order to localize, high resolution features from the contracting path are combined with the upsampled output. A successive convolution layer can then learn to assemble a more precise output based on this information. ç†è§£ï¼šæ·±å±‚ç‰¹å¾å±‚æ„Ÿå—é‡è¾ƒå¤§ï¼Œå¸¦æœ‰å…¨å±€ä¿¡æ¯ï¼Œå°†å…¶ä¸Šé‡‡æ ·ç”¨äºæä¾›localization informationï¼Œè€Œæ¨ªå‘addè¿‡æ¥ç‰¹å¾å±‚å¸¦æœ‰å±€éƒ¨ç‰¹å¾ä¿¡æ¯ã€‚ä¸¤ä¸ª3*3çš„conv blockç”¨äºå°†ä¸¤ç±»ä¿¡æ¯æ•´åˆï¼Œè¾“å‡ºæ›´ç²¾ç¡®çš„è¡¨è¾¾ã€‚ In the upsampling part we have also a large number of feature channels, which allow the network to propagate context information to higher resolution layers. ç†è§£ï¼šåº”è¯¥æ˜¯å­—é¢æ„æ€å§ï¼Œä¸ºä¸Šé‡‡æ ·çš„å·ç§¯å±‚ä¿ç•™æ›´å¤šçš„ç‰¹å¾é€šé“ï¼Œå°±ç›¸å½“äºä¿ç•™äº†æ›´å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ we use excessive data augmentation. ç»†èŠ‚ï¼š contracting pathï¼š typical CNNï¼šblocks of [2 3*3 unpadded convs+ReLU+2*2 stride2 maxpooling] At each downsampling step we double the number of feature channels expansive pathï¼š upsamplingï¼š 2*2 up-conv that half the channels concatenation the corresponding cropped feature map from the contracting path 2 [3x3 conv+ReLU] final layerï¼šuse a 1*1 conv to map the feature vectors to class vectors trainï¼š prefer larger input size to larger batch size sgd with 0.99 momentum so that the previously seen samples dominate the optimization lossï¼šsoftmax &amp; cross entropy unbalanced weightï¼š pre-compute the weight map base on the frequency of pixels for a certain class add the weight for a certain element to force the learning emphasisï¼še.g. the small separation borders initializationï¼šGaussian distribution data augmentationï¼š deformations â€œDrop-out layers at the end of the contracting path perform further implicit data augmentationâ€ metricsï¼šâ€œwarping errorâ€, the â€œRand errorâ€ and the â€œpixel errorâ€ for EM segmentation challenge and average IOU for ISBI cell tracking challenge predictionï¼š æŒ‰ç…§è®ºæ–‡çš„æ¨¡å‹ç»“æ„ï¼Œè¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦æ˜¯ä¸ä¸€æ ·çš„â€”â€”åœ¨valid paddingçš„è¿‡ç¨‹ä¸­æœ‰è¾¹ç¼˜ä¿¡æ¯æŸå¤±ã€‚ é‚£ä¹ˆå¦‚æœæˆ‘ä»¬æƒ³è¦é¢„æµ‹é»„æ¡†å†…çš„åˆ†å‰²ç»“æœï¼Œéœ€è¦è¾“å…¥ä¸€å¼ æ›´å¤§çš„å›¾ï¼ˆè“æ¡†ï¼‰ä½œä¸ºè¾“å…¥ï¼Œåœ¨å›¾ç‰‡è¾¹ç¼˜çš„æ—¶å€™ï¼Œæˆ‘ä»¬é€šè¿‡é•œåƒçš„æ–¹å¼è¡¥å…¨ã€‚ å› æœå…³ç³»ï¼š é¦–å…ˆå› ä¸ºå†…å­˜é™åˆ¶ï¼Œè¾“å…¥çš„ä¸æ˜¯æ•´å¼ å›¾ï¼Œæ˜¯å›¾ç‰‡patchï¼Œ ä¸ºäº†ä¿ç•™ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä½¿å¾—é¢„æµ‹æ›´å‡†ç¡®ï¼Œæˆ‘ä»¬ç»™å›¾ç‰‡patchæ·»åŠ ä¸€åœˆborderçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå®é™…æ„Ÿå…´è¶£çš„æ˜¯é»„æ¡†åŒºåŸŸï¼‰ åœ¨è®­ç»ƒæ—¶ï¼Œä¸ºäº†é¿å…é‡å å¼•å…¥çš„è®¡ç®—ï¼Œå·ç§¯å±‚ä½¿ç”¨äº†valid padding å› æ­¤åœ¨ç½‘ç»œçš„è¾“å‡ºå±‚ï¼Œè¾“å‡ºå°ºå¯¸æ‰æ˜¯æˆ‘ä»¬çœŸæ­£å…³æ³¨çš„éƒ¨åˆ† å¦‚æœè®­ç»ƒæ ·æœ¬å°ºå¯¸ä¸é‚£ä¹ˆhugeï¼Œå®Œå…¨å¯ä»¥å…¨å›¾è¾“å…¥ï¼Œç„¶åä½¿ç”¨same paddingï¼Œç›´æ¥é¢„æµ‹å…¨å›¾mask æ€»ç»“ï¼š train from very few images â€”-&gt; data augmentation fast â€”-&gt; full convolution layers precise â€”-&gt; global? V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation åŠ¨æœº entire 3D volume imbalance between the number of foreground and background voxelsï¼šdice coefficient limited dataï¼šapply random non-linear transformations and histogram matching fast and accurate è®ºç‚¹ï¼š early approaches based on patches local context challenging modailities efficiency issues fully convolutional networks 2D so far imbalance issueï¼šthe anatomy of interest occupies only a very small region of the scan thus predictions are strongly biased towards the background. re-weighting dice coefficient claims to be better that above è¦ç´ ï¼š a compression path a decompression path æ–¹æ³•ï¼š compressionï¼š add residualèƒ½å¤ŸåŠ é€Ÿæ”¶æ•› resolution is reduced by [2*2*2 conv with stride 2]ç›¸æ¯”äºmaxpoolingèŠ‚çœäº†bpæ‰€éœ€switch mapçš„memoryæ¶ˆè€— double the number of feature maps as we reduce their resolution PReLU decompressionï¼š horizontal connectionsï¼š1) gather fine grained detail that would be otherwise lost in the compression path 2) improve the convergence time residual convï¼šblocks of [5*5*5 conv with stride 1] æå–ç‰¹å¾ç»§ç»­å¢å¤§æ„Ÿå—é‡ up-convï¼šexpands the spatial support of the lower resolution feature maps last layerï¼šrun [1*1*1conv with 2 channel+softmax] to obtain the voxelwise probabilistic segmentations of the foreground and background dice coefficientï¼š [0,1] which we aim to maximiseï¼Œassume $p_i$ã€$g_i$ belong to two binary volumes D = \frac{2\sum_i^N p_i g_i}{\sum_i^N p_i^2 + \sum_i^N g_i^2} trainï¼š input fix size 128 Ã— 128 Ã— 64 voxels and a spatial resolution of 1 Ã— 1 Ã— 1.5 millimeters each mini-batch contains 2 volumes online augmentationï¼š randomly deformation vary the intensity distributionï¼šéšæœºé€‰å–æ ·æœ¬çš„ç°åº¦åˆ†å¸ƒä½œä¸ºå½“å‰è®­ç»ƒæ ·æœ¬çš„ç°åº¦åˆ†å¸ƒ used a momentum of 0.99 and a initial learning rate of 0.0001 which decreases by one order of magnitude every 25K iterations metricsï¼š Dice coefficient Hausdorff distance of the predicted delineation to the ground truth annotation the score obtained on the challenge dice loss &amp; focal loss CE &amp; BCE CEï¼šcategorical_crossentropyï¼Œé’ˆå¯¹æ‰€æœ‰ç±»åˆ«è®¡ç®—ï¼Œç±»åˆ«é—´äº’æ–¥ CE(x) = -\sum_{i=1}^{n\_class}y_i log f_i(x) $x$æ˜¯è¾“å…¥æ ·æœ¬ï¼Œ$y_i$æ˜¯ç¬¬$i$ä¸ªç±»åˆ«å¯¹åº”çš„çœŸå®æ ‡ç­¾ï¼Œ$f_i(x)$æ˜¯å¯¹åº”çš„æ¨¡å‹è¾“å‡ºå€¼ã€‚ å¯¹åˆ†ç±»é—®é¢˜ï¼Œ$y_i$æ˜¯one-hotï¼Œ$f_i(x)$æ˜¯ä¸ªä¸€ç»´å‘é‡ã€‚æœ€ç»ˆå¾—åˆ°ä¸€ä¸ªæ•°å€¼ã€‚ BCEï¼šbinary_crossentropyï¼Œé’ˆå¯¹æ¯ä¸ªç±»åˆ«è®¡ç®— BCE(x)_i = - [y_i log f_i(x) + (1-y_i)log(1-f_i(x))] $i$æ˜¯ç±»åˆ«ç¼–å·ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªç»´åº¦ä¸º$n_class$çš„å‘é‡ã€‚ å†æ±‚ç±»å‡å€¼å¾—åˆ°ä¸€ä¸ªæ•°å€¼ä½œä¸ºå•ä¸ªæ ·æœ¬çš„lossã€‚ BCE(x) = \frac{\sum_{i=1}^{n\_class}BCE_i(x)}{n\_class} batch lossï¼šå¯¹batchä¸­æ‰€æœ‰æ ·æœ¬çš„lossæ±‚å‡å€¼ã€‚ ä»å…¬å¼ä¸Šçœ‹ï¼ŒCEçš„è¾“å‡ºé€šå¸¸æ˜¯ç»è¿‡äº†softmaxï¼Œsoftmaxçš„æŸä¸€ä¸ªè¾“å‡ºå¢å¤§ï¼Œå¿…ç„¶å¯¼è‡´å…¶å®ƒç±»åˆ«çš„è¾“å‡ºå‡å°ï¼Œå› æ­¤åœ¨è®¡ç®—lossçš„æ—¶å€™å…³æ³¨æ­£ç¡®ç±»åˆ«çš„é¢„æµ‹å€¼æ˜¯å¦è¢«æ‹‰é«˜å³å¯ã€‚ä½¿ç”¨BCEçš„åœºæ™¯é€šå¸¸æ˜¯ä½¿ç”¨sigmoidï¼Œç±»åˆ«é—´ä¸ä¼šäº’ç›¸å‹åˆ¶ï¼Œå› æ­¤æ—¢è¦è€ƒè™‘æ‰€å±ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡å¤Ÿé«˜ï¼Œä¹Ÿè¦è€ƒè™‘ä¸æ‰€å±ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡è¶³å¤Ÿä½ï¼ˆè¿™ä¸€é¡¹åœ¨softmaxä¸­è¢«å®ç°äº†æ•…CEä¸éœ€è¦è¿™ä¸€é¡¹ï¼‰ã€‚ åœºæ™¯ï¼š äºŒåˆ†ç±»ï¼šåªæœ‰ä¸€ä¸ªè¾“å‡ºèŠ‚ç‚¹ï¼Œ$f(x) \in (0,1)$ï¼Œåº”è¯¥ä½¿ç”¨sigmoid+BCEä½œä¸ºæœ€åçš„è¾“å‡ºå±‚é…ç½®ã€‚ å•æ ‡ç­¾å¤šåˆ†ç±»ï¼šåº”è¯¥ä½¿ç”¨softmax+CEçš„æ–¹æ¡ˆï¼ŒBCEä¹ŸåŒæ ·é€‚ç”¨ã€‚ å¤šæ ‡ç­¾å¤šåˆ†ç±»ï¼šmulti-labelæ¯ä¸ªæ ‡ç­¾çš„è¾“å‡ºæ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œå› æ­¤å¸¸ç”¨é…ç½®æ˜¯sigmoid+BCEã€‚ å¯¹åˆ†å‰²åœºæ™¯æ¥è¯´ï¼Œè¾“å‡ºçš„æ¯ä¸€ä¸ªchannelå¯¹åº”ä¸€ä¸ªç±»åˆ«çš„é¢„æµ‹mapï¼Œå¯ä»¥çœ‹æˆæ˜¯å¤šä¸ªchannelé—´çš„å•æ ‡ç­¾å¤šåˆ†ç±»ï¼ˆsoftmax+CEï¼‰ï¼Œä¹Ÿå¯ä»¥çœ‹æˆæ˜¯æ¯ä¸ªç‹¬ç«‹é€šé“ç±»åˆ«mapçš„äºŒåˆ†ç±»ï¼ˆsigmoid+BCEï¼‰ã€‚unetè®ºæ–‡ç”¨äº†weightedçš„softmax+CEã€‚vnetè®ºæ–‡ç”¨äº†dice_lossã€‚ re-weighting(WCE) åŸºäºCE&amp;BCEï¼Œç»™äº†æ ·æœ¬ä¸åŒçš„æƒé‡ã€‚ unetè®ºæ–‡ä¸­æåˆ°äº†åŸºäºpixel frequencyä¸ºä¸åŒçš„ç±»åˆ«åˆ›å»ºäº†weight mapã€‚ ä¸€ç§å®ç°ï¼šåŸºäºæ¯ä¸ªç±»åˆ«çš„weight mapï¼Œåœ¨å®ç°CEçš„æ—¶å€™æ”¹æˆåŠ æƒå¹³å‡å³å¯ã€‚ å¦ä¸€ç§å®ç°ï¼šåŸºäºæ¯ä¸ªæ ·æœ¬çš„weight mapï¼Œä½œä¸ºç½‘ç»œçš„é™„åŠ è¾“å…¥ï¼Œåœ¨å®ç°CEçš„æ—¶å€™ä¹˜åœ¨loss mapä¸Šã€‚ focal loss æå‡ºæ˜¯åœ¨ç›®æ ‡æ£€æµ‹é¢†åŸŸï¼Œç”¨äºè§£å†³æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ä¸¥é‡å¤±è°ƒçš„é—®é¢˜ã€‚ ä¹Ÿæ˜¯ä¸€ç§åŠ æƒï¼Œä½†æ˜¯ç›¸æ¯”è¾ƒäºre-weightingï¼Œå›°éš¾æ ·æœ¬çš„æƒé‡ç”±ç½‘ç»œè‡ªè¡Œæ¨æ–­å‡ºï¼Œé€šè¿‡æ·»åŠ $(\alpha)$å’Œ$(-)^\lambda$è¿™ä¸€åŠ æƒé¡¹ï¼š focal\_loss(x)_i = -[\alpha y_i (1-p_i)^\lambda log (p_i)+(1-\alpha)(1-y_i)p_i^\lambda log(1-p_i)] å¯¹äºç±»åˆ«é—´ä¸å‡è¡¡çš„æƒ…å†µï¼ˆé€šå¸¸è´Ÿæ ·æœ¬è¿œè¿œå¤šäºæ­£æ ·æœ¬ï¼‰ï¼Œ$(\alpha)$é¡¹ç”¨äºå¹³è¡¡æ­£è´Ÿæ ·æœ¬æƒé‡ã€‚ å¯¹äºç±»å†…å›°éš¾æ ·æœ¬çš„æŒ–æ˜ï¼Œ$(-)^\lambda$é¡¹ç”¨äºè°ƒæ•´ç®€å•æ ·æœ¬å’Œå›°éš¾æ ·æœ¬çš„æƒé‡ï¼Œé¢„æµ‹æ¦‚ç‡æ›´æ¥è¿‘çœŸå®labelçš„æ ·æœ¬ï¼ˆç®€å•æ ·æœ¬ï¼‰çš„æƒé‡ä¼šè¡°å‡æ›´å¿«ï¼Œé¢„æµ‹æ¦‚ç‡æ¯”è¾ƒä¸å‡†ç¡®çš„æ ·æœ¬ï¼ˆè‹¦éš¾æ ·æœ¬ï¼‰çš„æƒé‡åˆ™æ›´é«˜äº›ã€‚ ç”±äºåˆ†å‰²ç½‘ç»œçš„è¾“å‡ºçš„å•é€šé“ï¼å¤šé€šé“çš„å›¾ç‰‡ï¼Œç›´æ¥ä½¿ç”¨focal lossä¼šå¯¼è‡´losså€¼å¾ˆå¤§ã€‚ â€‹ 1. é€šå¸¸ä¸å…¶ä»–lossåŠ æƒç»„åˆä½¿ç”¨ â€‹ 2. sumå¯ä»¥æ”¹æˆmean â€‹ 3.ä¸å»ºè®®åœ¨è®­ç»ƒåˆæœŸå°±åŠ å…¥ï¼Œå¯åœ¨è®­ç»ƒåæœŸç”¨äºä¼˜åŒ–æ¨¡å‹ â€‹ 4. å…¬å¼ä¸­å«logè®¡ç®—ï¼Œå¯èƒ½å¯¼è‡´nanï¼Œè¦å¯¹logä¸­çš„å…ƒç´ clip 12345678910111213def focal_loss(y_true, y_pred): gamma = 2. alpha = 0.25 # score = alpha * y_true * K.pow(1 - y_pred, gamma) * K.log(y_pred) + # this works when y_true==1 # (1 - alpha) * (1 - y_true) * K.pow(y_pred, gamma) * K.log(1 - y_pred) # this works when y_true==0 pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred)) pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred)) # avoid nan pt_1 = K.clip(pt_1, 1e-3, .999) pt_0 = K.clip(pt_0, 1e-3, .999) score = -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - \ K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0)) return score dice loss diceå®šä¹‰ä¸¤ä¸ªmaskçš„ç›¸ä¼¼ç¨‹åº¦ï¼š dice = \frac{2 * A \bigcap B}{|A|+|B|} = \frac{2 * TP}{2*TP + FN + FP} åˆ†å­æ˜¯TPâ€”â€”åªå…³æ³¨å‰æ™¯ åˆ†æ¯å¯ä»¥æ˜¯$|A|$ï¼ˆé€ä¸ªå…ƒç´ ç›¸åŠ ï¼‰ï¼Œä¹Ÿå¯ä»¥æ˜¯å¹³æ–¹å½¢å¼$|A|^2$ æ¢¯åº¦ï¼šâ€œä½¿ç”¨dice lossæœ‰æ—¶ä¼šä¸å¯ä¿¡ï¼ŒåŸå› æ˜¯å¯¹äºsoftmaxæˆ–log losså…¶æ¢¯åº¦ç®€è¨€ä¹‹æ˜¯p-t ï¼Œtä¸ºç›®æ ‡å€¼ï¼Œpä¸ºé¢„æµ‹å€¼ã€‚è€Œdice loss ä¸º 2t2 / (p+t)2 å¦‚æœpï¼Œtè¿‡å°ä¼šå¯¼è‡´æ¢¯åº¦å˜åŒ–å‰§çƒˆï¼Œå¯¼è‡´è®­ç»ƒå›°éš¾ã€‚â€ ã€è¯¦ç»†è§£é‡Šä¸‹ã€‘äº¤å‰ç†µlossï¼š$L=-(1-|t-p|)log(1-|t-p|)$ï¼Œæ±‚å¯¼å¾—åˆ°$\frac{\partial L}{\partial p}=-log(1-|t-p|)$ï¼Œå…¶å®å°±å¯ä»¥ç®€åŒ–çœ‹ä½œ$t-p$ï¼Œå¾ˆæ˜¾ç„¶è¿™ä¸ªæ¢¯åº¦æ˜¯æœ‰ç•Œçš„ï¼Œå› æ­¤ä½¿ç”¨äº¤å‰ç†µlossçš„ä¼˜åŒ–è¿‡ç¨‹æ¯”è¾ƒç¨³å®šã€‚è€Œdice lossçš„ä¸¤ç§å½¢å¼ï¼ˆä¸å¹³æ–¹&amp;å¹³æ–¹ï¼‰ï¼š$L=\frac{2pt}{p+t}\ or\ L=\frac{2pt}{p^2+t^2}$ï¼Œæ±‚å¯¼ä»¥ååˆ†åˆ«æ˜¯$\frac{\partial L}{\partial p} = \frac{t^2+2pt}{(p+t)^2} \ or\ \frac{3tp^2+t^3}{(p^2+t^2)^2}$è®¡ç®—ç»“æœæ¯”è¾ƒå¤æ‚ï¼Œptéƒ½å¾ˆå°çš„æƒ…å†µä¸‹ï¼Œæ¢¯åº¦å€¼å¯èƒ½å¾ˆå¤§ï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œlossæ›²çº¿æ··ä¹±ã€‚ vnetè®ºæ–‡ä¸­çš„å®šä¹‰åœ¨åˆ†æ¯ä¸Šç¨æœ‰ä¸åŒï¼ˆsee belowï¼‰ã€‚smoothingçš„å¥½å¤„ï¼š é¿å…åˆ†å­é™¤0 å‡å°‘è¿‡æ‹Ÿåˆ 12345678def dice_coef(y_true, y_pred): smooth = 1. intersection = K.sum(y_true * y_pred, axis=[1,2,3]) union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3]) return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0) def dice_coef_loss(y_true, y_pred): 1 - dice_coef(y_true, y_pred, smooth=1) iou loss dice lossè¡ç”Ÿï¼Œintersection over unionï¼š iou = \frac{A \bigcap B}{A \bigcup B} åˆ†æ¯ä¸Šæ¯”diceå°‘äº†ä¸€ä¸ªintersectionã€‚ â€œIOU lossçš„ç¼ºç‚¹åŒDICE lossï¼Œè®­ç»ƒæ›²çº¿å¯èƒ½å¹¶ä¸å¯ä¿¡ï¼Œè®­ç»ƒçš„è¿‡ç¨‹ä¹Ÿå¯èƒ½å¹¶ä¸ç¨³å®šï¼Œæœ‰æ—¶ä¸å¦‚ä½¿ç”¨softmax lossç­‰çš„æ›²çº¿æœ‰ç›´è§‚æ€§ï¼Œé€šå¸¸è€Œè¨€softmax losså¾—åˆ°çš„lossä¸‹é™æ›²çº¿è¾ƒä¸ºå¹³æ»‘ã€‚â€ boundary loss dice losså’Œiou lossæ˜¯åŸºäºåŒºåŸŸé¢ç§¯åŒ¹é…åº¦å»å­¦ä¹ ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨è¾¹ç•ŒåŒ¹é…åº¦å»ç›‘ç£ç½‘ç»œçš„å­¦ä¹ ã€‚ åªå¯¹è¾¹ç•Œä¸Šçš„åƒç´ è¿›è¡Œè¯„ä¼°ï¼Œå’ŒGTçš„è¾¹ç•Œå»åˆåˆ™ä¸º0ï¼Œä¸å»åˆçš„ç‚¹ï¼Œæ ¹æ®å…¶è·ç¦»è¾¹ç•Œçš„è·ç¦»è¯„ä¼°å®ƒçš„Lossã€‚ Hausdorff distance ç”¨äºåº¦é‡ä¸¤ä¸ªç‚¹é›†ä¹‹é—´çš„ç›¸ä¼¼ç¨‹åº¦ï¼Œdenote ç‚¹é›†$A\{a_1, a_2, â€¦, a_p\}$ï¼Œç‚¹é›†$B\{b_1, b_2, â€¦, b_p\}$ï¼š HD(A, B) = max\{hd(A,B), hd(B,A)\}\\ hd(A,B) = max_{a \in A} min_{b in B} ||a-b||\\ hd(B,A) = max_{b \in B} min_{a in A} ||b-a|| å…¶ä¸­HD(A,B)æ˜¯Hausdorff distanceçš„åŸºæœ¬å½¢å¼ï¼Œç§°ä¸ºåŒå‘è·ç¦» hd(A,B)æè¿°çš„æ˜¯å•å‘è·ç¦»ï¼Œé¦–å…ˆæ‰¾åˆ°ç‚¹é›†Aä¸­æ¯ä¸ªç‚¹åœ¨ç‚¹é›†Bä¸­è·ç¦»æœ€è¿‘çš„ç‚¹ä½œä¸ºåŒ¹é…ç‚¹ï¼Œç„¶åè®¡ç®—è¿™äº›a-b-pairçš„è·ç¦»çš„æœ€å¤§å€¼ã€‚ HD(A,B)å–å•å‘è·ç¦»ä¸­çš„æœ€å¤§å€¼ï¼Œæè¿°äº†ä¸¤ä¸ªç‚¹é›†åˆçš„æœ€å¤§ä¸åŒ¹é…ç¨‹åº¦ã€‚ mix loss BCE + dice lossï¼šåœ¨æ•°æ®è¾ƒä¸ºå¹³è¡¡çš„æƒ…å†µä¸‹æœ‰æ”¹å–„ä½œç”¨ï¼Œä½†æ˜¯åœ¨æ•°æ®æåº¦ä¸å‡è¡¡çš„æƒ…å†µä¸‹ï¼Œäº¤å‰ç†µæŸå¤±ä¼šåœ¨å‡ ä¸ªè®­ç»ƒä¹‹åè¿œå°äºDice æŸå¤±ï¼Œæ•ˆæœä¼šæŸå¤±ã€‚ focal loss + dice lossï¼šæ•°é‡çº§é—®é¢˜ MSE å…³é”®ç‚¹æ£€æµ‹æœ‰æ—¶å€™ä¹Ÿä¼šé‡‡ç”¨åˆ†å‰²æ¡†æ¶ï¼Œè¿™æ—¶å€™ground truthæ˜¯é«˜æ–¯mapï¼Œdiceæ˜¯é’ˆå¯¹äºŒå€¼åŒ–maskçš„ï¼Œè¿™æ—¶å€™è¿˜å¯ä»¥ç”¨MSEã€‚ ohnm online hard negative mining å›°éš¾æ ·æœ¬æŒ–æ˜ Tversky loss ä¸€ç§åŠ æƒçš„dice lossï¼Œdice lossä¼šå¹³ç­‰çš„æƒè¡¡FPï¼ˆç²¾åº¦ï¼Œå‡é˜³ï¼‰å’ŒFNï¼ˆå¬å›ï¼Œå‡é˜´ï¼‰ï¼Œä½†æ˜¯åŒ»å­¦å›¾åƒä¸­ç—…ç¶æ•°ç›®è¿œå°‘äºèƒŒæ™¯æ•°é‡ï¼Œå¾ˆå¯èƒ½å¯¼è‡´è®­ç»ƒç»“æœåå‘é«˜ç²¾åº¦ä½†æ˜¯ä½å¬å›ç‡ï¼ŒTversky lossæ§åˆ¶lossæ›´åå‘FNï¼š loss = 1-\frac{|PG|}{|PG|+\alpha|P\backslash G|+\beta|G\backslash P|}1234567891011def tversky_loss(y_true, y_pred): y_true_pos = K.flatten(y_true) y_pred_pos = K.flatten(y_pred) # TP true_pos = K.sum(y_true_pos * y_pred_pos) # FN false_neg = K.sum(y_true_pos * (1-y_pred_pos)) # FP false_pos = K.sum((1-y_true_pos) * y_pred_pos) alpha = 0.7 return 1 - (true_pos + K.epsilon())/(true_pos + alpha * false_neg + (1-alpha) * false_pos + K.epsilon()) Lovasz hinge &amp; Lovasz-Softmax loss IOU lossè¡ç”Ÿï¼Œjaccard lossåªé€‚ç”¨äºç¦»æ•£æƒ…å†µï¼Œè€Œç½‘ç»œé¢„æµ‹æ˜¯è¿ç»­å€¼ï¼Œå¦‚æœä¸ä½¿ç”¨æŸä¸ªè¶…å‚å°†ç¥ç»å…ƒè¾“å‡ºäºŒå€¼åŒ–ï¼Œå°±ä¸å¯å¯¼ã€‚blabla ä¸æ˜¯å¾ˆæ‡‚ç›´æ¥ç”¨å§ï¼šhttps://github.com/bermanmaxim/LovaszSoftmax ä¸€äº›è¡¥å…… æ”¹è¿›ï¼š dropoutã€batch normalizationï¼šä»è®ºæ–‡ä¸Šçœ‹ï¼Œunetåªåœ¨æœ€æ·±å±‚å·ç§¯å±‚åé¢æ·»åŠ äº†dropout layerï¼ŒBNæœªè¡¨ï¼Œè€Œcommon senseç”¨æ¯ä¸€ä¸ªconvå±‚åé¢æ¥BNå±‚èƒ½å¤Ÿæ›¿æ¢æ‰dropoutå¹¶èƒ½è·å¾—æ€§èƒ½æå‡çš„ã€‚ UpSampling2Dã€Conv2DTransposeï¼šunetä½¿ç”¨äº†ä¸Šé‡‡æ ·ï¼Œvnetä½¿ç”¨äº†deconvï¼Œä½†æ˜¯â€œDeConv will produce image with checkerboard effect, which can be revised by upsample and convâ€(Reference)ã€‚ valid paddingã€same paddingï¼šunetè®ºæ–‡ä½¿ç”¨å›¾åƒpatchä½œä¸ºè¾“å…¥ï¼Œç‰¹å¾æå–æ—¶ä½¿ç”¨valid paddingï¼ŒæŸå¤±è¾¹ç¼˜ä¿¡æ¯ã€‚ network blocksï¼šunetç”¨çš„conv blockæ˜¯ä¸¤ä¸ªä¸€ç»„çš„3*3convï¼Œvnetç¨å¾®ä¸åŒä¸€ç‚¹ï¼Œå¯ä»¥å°è¯•çš„blockæœ‰ResNetï¼ResNextã€DenseNetã€DeepLabç­‰ã€‚ pretrained encoderï¼šfeature extraction pathä½¿ç”¨ä¸€äº›ç°æœ‰çš„backboneï¼Œå¯ä»¥åŠ è½½é¢„è®­ç»ƒæƒé‡(Reference)ï¼ŒåŠ é€Ÿè®­ç»ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚ åŠ å…¥SEæ¨¡å—(Reference)ï¼šå¯¹æ¯ä¸ªé€šé“çš„ç‰¹å¾åŠ æƒ attention mechanismsï¼š å¼•ç”¨nn-Unetä¸»è¦ç»“æ„æ”¹è¿›åˆé›†ï¼šâ€œJust to provide some prominent examples: variations of encoder-decoder style architectures with skip connections, first introduced by the U-Net [12], include the introduction of residual connections [9], dense connections [6], at- tention mechanisms [10], additional loss layers [5], feature recalibration [13], and others [11]. è¡ç”Ÿï¼š TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation nnU-Net: Breaking the Spell on Successful Medical Image Segmentation TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation åŠ¨æœºï¼š neural network initialized with pre-trained weights usually shows better performance than those trained from scratch on a small dataset. ä¿ç•™encoder-decoderçš„ç»“æ„ï¼ŒåŒæ—¶å……åˆ†åˆ©ç”¨è¿ç§»å­¦ä¹ çš„ä¼˜åŠ¿ è®ºç‚¹ï¼š load pretrained weights ç”¨huge datasetåšé¢„è®­ç»ƒ æ–¹æ³•ï¼š ç”¨vgg11æ›¿æ¢åŸå§‹çš„encoderï¼Œå¹¶load pre-trained weights on ImageNetï¼š æœ€æ·±å±‚è¾“å…¥(maxpooling5)ï¼šuse a single conv of 512 channels that serves as a bottleneck central part of the network upsamplingæ¢æˆäº†convTranspose loss functionï¼šIOU + BCEï¼š L = BCE - log(IOU) inferenceï¼šchoose a threshold 0.3, all pixel values below which are set to be zero ç»“è®ºï¼š converge faster better IOU nnU-Net: Breaking the Spell on Successful Medical Image Segmentation åŠ¨æœº many proposed methods fail to generalize: å¯¹äºåˆ†å‰²ä»»åŠ¡ï¼Œä»unetå‡ºæ¥ä¹‹åçš„å‡ å¹´é‡Œï¼Œåœ¨ç½‘ç»œç»“æ„ä¸Šå·²ç»æ²¡æœ‰å¤šå°‘çš„çªç ´äº†ï¼Œç»“æ„ä¿®æ”¹è¶Šå¤šï¼Œåè€Œè¶Šå®¹æ˜“è¿‡æ‹Ÿåˆ relies on just a simple U-Net architecture embedded in a robust training scheme automate necessary adaptations such as preprocessing, the exact patch size, batch size, and inference settings based on the properties of a given dataset: æ›´å¤šçš„æå‡å…¶å®åœ¨äºç†è§£æ•°æ®ï¼Œé’ˆå¯¹æ•°æ®é‡‡ç”¨é€‚å½“çš„é¢„å¤„ç†å’Œè®­ç»ƒæ–¹æ³•å’ŒæŠ€å·§ è®ºç‚¹ the diversity and individual peculiarities of imaging datasets make it difficult to generalize prominent modifications focus on architectural modifications, merely brushing over all the other hyperparameters we propose: ä½¿ç”¨åŸºç¡€ç‰ˆunetï¼šnnUNetï¼ˆno-new-Netï¼‰ a formalism for automatic adaptation to new datasets automatically designs and executes a network training pipeline without any manual fine-tuning è¦ç´  a segmentation task: $f_{\theta}(X) = \hat Y$, in this paper we seek for a $g(X,Y)=\theta$. First we distinguish two type of hyperparameters: static paramsï¼šin this case the network architecture and a robust training scheme dynamic paramsï¼šthose that need to be changed in dependence of $X$ and $Y$ Second we define gâ€”â€”a set of heuristics rules covering the entire process of the task: é¢„å¤„ç†ï¼šresamplingå’Œnormalization è®­ç»ƒï¼šlossï¼Œoptimizerè®¾ç½®ã€æ•°æ®å¢å¹¿ æ¨ç†ï¼špatch-basedç­–ç•¥ã€test-time-augmentationsé›†æˆå’Œæ¨¡å‹é›†æˆç­‰ åå¤„ç†ï¼šå¢å¼ºå•è¿é€šåŸŸç­‰ æ–¹æ³• Preprocessing Image Normalizationï¼š CTï¼š$normed_intensity = (intensity - fg_mean) / fg_standard_deviation$, $fg$ for $[0.05,0.95]$ foreground intensity not CTï¼š$normed_intensity = (intensity - mean) / standard_deviation $ Voxel Spacingï¼š for each axis chooses the median as the target spacing image resampled with third order spline interpolation z-axis using nearest neighbor interpolation if â€˜anisotropic spacingâ€™ occurs mask resampled with third order spline interpolation Training Procedure Network Architectureï¼š 3 independent modelï¼ša 2D U-Net, a 3D U-Net and a cascade of two 3D U-Net padded convolutionsï¼što achieve identical output and input shapes instance normalizationï¼šâ€œBNé€‚ç”¨äºåˆ¤åˆ«æ¨¡å‹ï¼Œæ¯”å¦‚å›¾ç‰‡åˆ†ç±»æ¨¡å‹ã€‚å› ä¸ºBNæ³¨é‡å¯¹æ¯ä¸ªbatchè¿›è¡Œå½’ä¸€åŒ–ï¼Œä»è€Œä¿è¯æ•°æ®åˆ†å¸ƒçš„ä¸€è‡´æ€§ï¼Œè€Œåˆ¤åˆ«æ¨¡å‹çš„ç»“æœæ­£æ˜¯å–å†³äºæ•°æ®æ•´ä½“åˆ†å¸ƒã€‚ä½†æ˜¯BNå¯¹batchsizeçš„å¤§å°æ¯”è¾ƒæ•æ„Ÿï¼Œç”±äºæ¯æ¬¡è®¡ç®—å‡å€¼å’Œæ–¹å·®æ˜¯åœ¨ä¸€ä¸ªbatchä¸Šï¼Œæ‰€ä»¥å¦‚æœbatchsizeå¤ªå°ï¼Œåˆ™è®¡ç®—çš„å‡å€¼ã€æ–¹å·®ä¸è¶³ä»¥ä»£è¡¨æ•´ä¸ªæ•°æ®åˆ†å¸ƒï¼›INé€‚ç”¨äºç”Ÿæˆæ¨¡å‹ï¼Œæ¯”å¦‚å›¾ç‰‡é£æ ¼è¿ç§»ã€‚å› ä¸ºå›¾ç‰‡ç”Ÿæˆçš„ç»“æœä¸»è¦ä¾èµ–äºæŸä¸ªå›¾åƒå®ä¾‹ï¼Œæ‰€ä»¥å¯¹æ•´ä¸ªbatchå½’ä¸€åŒ–ä¸é€‚åˆå›¾åƒé£æ ¼åŒ–ï¼Œåœ¨é£æ ¼è¿ç§»ä¸­ä½¿ç”¨Instance Normalizationä¸ä»…å¯ä»¥åŠ é€Ÿæ¨¡å‹æ”¶æ•›ï¼Œå¹¶ä¸”å¯ä»¥ä¿æŒæ¯ä¸ªå›¾åƒå®ä¾‹ä¹‹é—´çš„ç‹¬ç«‹ã€‚â€ Leaky ReLUs Network Hyperparametersï¼š sets the batch size, patch size and number of pooling operations for each axis based on the memory consumption large patch sizes are favored over large batch sizes pooling along each axis is done until the voxel size=4 start num of filters=30, double after each pooling If the selected patch size covers less than 25% of the voxels, train the 3D U-Net cascade on a downsampled version of the training data to keep sufficient context Network Training: five-fold cross-validation One epoch is defined as processing 250 batches loss = dice loss + cross-entropy loss Adam(lr=3e-4, decay=3e-5) lrReduce: EMA(train_loss), 30 epoch, factor=0.2 earlyStop: earning rate drops below 10 6 or 1000 epochs are exceeded data augmentation: elastic deformations, random scaling and random rotations as well as gamma augmentation($g(x,y)=f(x,y)^{gamma}$) keep transformations in 2D-plane if â€˜anisotropic spacingâ€™ occurs Inference sliding window with half the patch size: this increases the weight of the predictions close to the center relative to the borders ensemble: U-Net configurations (2D, 3D and cascade) furthermore uses the five models (five-fold cross-validation) Ablation studies 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation åŠ¨æœº learns from sparsely/full annotated volumetric images (user annotates some slices) provides a dense 3D segmentation è¦ç´  3D operations avoid bottlenecks and use batch normalization for faster convergence on-the-fly elastic deformation train from scratch è®ºç‚¹ neighboring slices show almost the same information many biomedical applications generalizes reasonably well because medical images comprises repetitive structures thus we suggest dense-volume-segmentation-network that only requires some annotated 2D slices for training scenarios manual annotated ä¸€éƒ¨åˆ†sliceï¼Œç„¶åè®­ç»ƒç½‘ç»œå®ç°dense seg ç”¨ä¸€éƒ¨åˆ† sparsely annotatedçš„datasetä½œä¸ºtraining setï¼Œç„¶åè®­ç»ƒçš„ç½‘ç»œå®ç°åœ¨æ–°çš„æ•°æ®é›†ä¸Šdense seg æ–¹æ³• Network Architecture compressionï¼š2*3x3x3 convs(+BN)+relu+2x2x2 maxpooling decompressionï¼š2x2x2 upconv+2*3x3x3 convs+relu headï¼š1x1x1 conv concat shortcut connections ã€QUESTIONã€‘avoid bottlenecks by doubling the number of channels already before max pooling ä¸ªäººç†è§£è¿™ä¸ªdouble channelæ˜¯åœ¨è·ŸåŸå§‹çš„unetç»“æ„å¯¹æ¯”ï¼ŒåŸå§‹unetæ¯ä¸ªstageçš„ä¸¤ä¸ªconvçš„filter numæ˜¯ä¸€æ ·çš„ï¼Œç„¶åè¿›è¡Œmax poolingä¼šæŸå¤±éƒ¨åˆ†ä¿¡æ¯ï¼Œä½†æ˜¯åˆ†å‰²ä»»åŠ¡æœ¬èº«æ˜¯ä¸ªdense predictionï¼Œæ‰€ä»¥å¢å¤§channelæ¥å‡å°‘ä¿¡æ¯æŸå¤± ä½†æ˜¯ä¸ç†è§£ä»€ä¹ˆå«â€œavoid bottlenecksâ€ åŸæ–‡è¯´æ˜¯å‚è€ƒäº†ã€ŠRethinking the inception architecture for computer visionã€‹å¤§åé¼é¼çš„inception V3 å¯èƒ½å¯¹åº”çš„æ˜¯â€œ1. Avoid representational bottlenecks, especially early in the network.â€ï¼Œä»è¾“å…¥åˆ°è¾“å‡ºï¼Œè¦é€æ¸å‡å°‘feature mapçš„å°ºå¯¸ï¼ŒåŒæ—¶è¦é€æ¸å¢åŠ feature mapçš„æ•°é‡ã€‚ inputï¼š132x132x116 voxel tile outputï¼š44x44x28 BNï¼šbefore each ReLU weighted softmax loss functionï¼šsetting the weights of unlabeled pixels to zero makes it possible to learn from only the labelled ones and, hence, to generalize to the whole volumeï¼ˆæ˜¯ä¸æ˜¯random set the loss zeros of some samplesæ€»èƒ½è®©ç½‘ç»œæ›´å¥½çš„generalizeï¼Ÿï¼‰ Data manually annotated some orthogonal xy, xz, and yz slices annotation slices were sampled uniformly ran on down-sampled versions of the original resolution by factor of two labelsï¼š0: â€œinside the tubuleâ€; 1: â€œtubuleâ€; 2: â€œbackgroundâ€, and 3: â€œunlabeledâ€. Training rotation, scaling and gray value augmentation a smooth dense deformationï¼šrandom vector, normal distribution, B-spline interpolation weighted cross-entropy lossï¼šincrease weights â€œinside the tubuleâ€, reduce weights â€œbackgroundâ€, set zero â€œunlabeledâ€ 2.5D-UNet: Automatic Segmentation of Vestibular Schwannoma from T2-Weighted MRI by Deep Spatial Attention with Hardness-Weighted Loss ä¸“ä¸šæœ¯è¯­ Vestibular Schwannoma(VS) tumorsï¼šå‰åº­ç¥ç»é˜ç˜¤ through-plane resolutionï¼šå±‚åš isotropic resolutionï¼šå„å‘åŒæ€§ anisotropic resolutionsï¼šå„å‘å¼‚æ€§ åŠ¨æœº tumorçš„ç²¾ç¡®è‡ªåŠ¨åˆ†å‰² challenge low contrastï¼šhardness-weighted Dice loss functio small target regionï¼šattention module low through-plane resolutionï¼š2.5D è®ºç‚¹ segment small structures from large image contexts coarse-to-fine attention map Dice loss our method end-to-end supervision on the learning of attention map voxel-level hardness- weighted Dice loss function CNN 2D CNNs ignore inter-slice correlation 3D CNNs most applied to images with isotropic resolution requiring upsampling to balance the physical receptive field (in terms of mm rather than voxels)ï¼šmemory rise our method high in-plane resolution &amp; low through-plane resolution 2.5D CNN combining 2D and 3D convolutions use inter-slice features more efficient than 3D CNNs æ•°æ® T2-weighted MR images of 245 patients with VS tumor high in-plane resolution around 0.4 mmÃ—0.4 mmï¼Œ512x512 slice thickness and inter-slice spacing 1.5 mmï¼Œslice number 19 to 118 cropped cube sizeï¼š100 mmÃ—50 mmÃ—50 mm æ–¹æ³• architecture five levelsï¼šL1ã€L2 use 2Dï¼ŒL3ã€L4ã€L5 use 3D After the first two max-pooling layers that downsample the feature maps only in 2D, the feature maps in L3 and the followings have a near- isotropic 3D resolution. start channelsï¼š16 conv blockï¼šconv-BN-pReLU add a spatial attention module to each level of the decoder spatial attention module A spatial attention map can be seen as a single-channel image of attention coefficient inputï¼šfeature map with channel $N_l$ conv1+ReLUï¼š channel $N_l/2$ conv2+Sigmoidï¼šchannel 1ï¼Œoutputs the attention map multiplied the feature map with the attention map a residual connection explicit supervision multi-scale attention loss $L_{attention} = \frac{1}{L} \sum_{L} l(A_l, G_l^f)$ $A_l$æ˜¯æ¯ä¸€å±‚çš„attention mapï¼Œ$G_l^f$æ˜¯æ¯ä¸€å±‚æ˜¯å‰æ™¯ground truth average-poolåˆ°å½“å‰resolutionçš„mask Voxel-Level Hardness-Weighted Dice Loss automatic hard voxel weightingï¼š$w_i = \lambda * abs(p_i - g_i) + (1-\lambda)$ $\lambda \in [0,1]$ï¼Œcontrols the degree of hard voxel weighting hardness-weighted Dice loss (HDL) ï¼š l(P,G) = 1.0 - \frac{1}{C}\sum_{C} \frac{2\sum_i w_i p_i g_i + \epsilon}{\sum_i w_i (p_i + g_i) + \epsilon} total lossï¼š L = \frac{1}{L} \sum_{L} l(A_l, G_l^f) + l(P,G) Combining analysis of multi-parametric MR images into a convolutional neural network: Precise target delineation for vestibular schwannoma treatment planningåªæœ‰æ‘˜è¦å’Œä¸€å¹…å›¾ multi-parametric MR imagesï¼šT1Wã€T2Wã€T1C two-pathway U-Net model kernel 3 Ã— 3 Ã— 1 and 1 Ã— 1 Ã— 3 respectively to extract the in-plane and through-plane features of the anisotropic MR images ç»“è®º The proposed two-pathway U-Net model outperformed the single-pathway U-Net model when segmenting VS using anisotropic MR images. multi-inputsï¼ˆT1ã€T2ï¼‰outperforms single-inputs]]></content>
      <tags>
        <tag>paper, è¯­ä¹‰åˆ†å‰²</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yoloç³»åˆ—]]></title>
    <url>%2F2019%2F11%2F28%2Fyolo%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[ç»¼è¿° [yolov1] Yolov1: You Only Look Once: Unified, Real-Time Object Detection [yolov2] Yolov2: YOLO9000: Better, Faster, Stronger [yolov3] Yolov3: An Incremental Improvement [yolov4] YOLOv4: Optimal Speed and Accuracy of Object Detection [poly-yolo] POLY-YOLO: HIGHER SPEED, MORE PRECISE DETECTION AND INSTANCE SEGMENTATION FOR YOLOV3 [scaled-yolov4] Scaled-YOLOv4: Scaling Cross Stage Partial Network 0. review review0121ï¼šå…³äºyolo loss ä¹‹å‰çœ‹kerasç‰ˆçš„yolo lossï¼ŒåŒ…å«åˆ†ç±»çš„bceï¼Œå›å½’çš„l2/mseï¼Œä»¥åŠconfidenceçš„å›å½’lossï¼Œå…¶ä¸­conf lossè¢«å»ºæ¨¡æˆå•çº¯çš„0-1åˆ†ç±»é—®é¢˜ï¼Œç”¨bceæ¥å®ç°ã€‚ äº‹å®ä¸ŠåŸç‰ˆçš„yolo lossä¸­ï¼Œobjectnessæ˜¯iouï¼ˆpredå’Œgtçš„iouï¼‰ï¼Œä»æ„ä¹‰ä¸Šï¼Œä¸ä»…æŒ‡ç¤ºå½“å‰æ ¼å­æœ‰æ— ç›®æ ‡ï¼Œè¿˜å¯¹å½“å‰çš„box predictionåšäº†è¯„ä¼° å›ä¼ æ¢¯åº¦ ä¸å›ä¼ æ¢¯åº¦ iouæ˜¯é€šè¿‡xywhè®¡ç®—çš„ï¼Œscaled_yolov4ä¸­æŠŠè¿™ä¸ªæ¢¯åº¦æˆªæ–­ï¼Œåªä½œä¸ºä¸€ä¸ªå€¼ï¼Œå¯¹confidenceè¿›è¡Œæ¢¯åº¦å›ä¼ ï¼Œ æ¢¯åº¦ä¸æˆªæ–­ä¹Ÿæ²¡æœ‰é—®é¢˜ï¼Œç›¸å½“äºå¯¹xywhå†å›ä¼ ä¸€ä¸ªiouçš„loss 1. Yolov1: You Only Look Once: Unified, Real-Time Object Detection åŠ¨æœº: end-to-end: 2 stages â€”-&gt; 1 stage real-time è®ºç‚¹ï¼š past methods: complex pipelines, hard to optimize(trained separately) DPM use a sliding window and a classifier to evaluate an object at various locations R-CNN use region proposal and run classifier on the proposed boxes, then post-processing in this paper: you only look once at an image rebuild the framework as a single regression problem: single stands for you donâ€™t have to run classifiers on each patch straight from image pixels to bounding box coordinates and class probabilities: straight stands for you obtain the bounding box and the classification results side by side, comparing to the previous serial pipeline advantagesï¼š fast &amp; twice the mean average precision of other real-time systems CNN sees the entire image thus encodes contextual information generalize better disadvantage: accuracy: â€œ it struggles to precisely localize some objects, especially small onesâ€ ç»†èŠ‚ï¼š gridï¼š Our system divides the input image into an S Ã— S grid. If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object. predictionï¼š Each grid cell predicts B bounding boxes, confidence scores for these boxes , and C conditional class probabilities for each grid that is an S*S*(B*5+C) tensor We normalize the bounding box width and height by the image width and height so that they fall between 0 and 1. We parametrize the bounding box x and y coordinates to be offsets of a particular grid cell so they are also bounded between 0 and 1. at test timeï¼š We obtain the class-specific confidence for individual box by multiply the class probability and box confidenceï¼š Pr(Class_i | Object) * Pr(Object)* IOU^{truth}_{pred} = Pr(Class_i)* IOU^{truth}_{pred} networkï¼š the convolutional layers extract features from the image while the fully connected layers predict the probabilities and coordinates trainingï¼š activationï¼šuse a linear activation function for the final layer and leaky rectified linear activation all the other layers optimizationï¼šuse sum-squared error, however it does not perfectly align with the goal of maximizing average precision â€‹ * weights equally the localization error and classification errorï¼š$\lambda_{coord}$ â€‹ * weights equally the grid cells containing and not-containing objectsï¼š$\lambda_{noobj}$ â€‹ * weights equally the large boxes and small boxesï¼šsquare roots the h&amp;w insteand of the straight h&amp;w lossï¼špick the box predictor has the highest current IOU with the ground truth per grid cell avoid overfittingï¼šdropout &amp; data augmentation â€‹ * use dropout after the first connected layer, â€‹ * introduce random scaling and translations of up to 20% of the original image size for data augmentation â€‹ * randomly adjust the exposure and saturation of the image by up to a factor of 1.5 in the HSV color space for data augmentation inferenceï¼š multiple detectionsï¼šsome objects locates near the border of multiple cells and can be well localized by multiple cells. Non-maximal suppression is proved critical, adding 2- 3% in mAP. Limitationsï¼š strong spatial constraintsï¼šdecided by the settings of bounding boxes softmax classificationï¼šcan only have one class for each grid â€œThis spatial constraint lim- its the number of nearby objects that our model can pre- dict. Our model struggles with small objects that appear in groups, such as flocks of birds. â€œ â€œ It struggles to generalize to objects in new or unusual aspect ratios or configurations. â€œ coarse bounding box predictionï¼šthe architecture has multiple downsampling layers the loss function treats errors the same in small bounding boxes versus large bounding boxesï¼š The same error has much greater effect on a small boxâ€™s IOU than a big box. â€œOur main source of error is incorrect localizations. â€œ Comparisonï¼š mAP among real-time detectors and Less Than Real-Time detectorsï¼šless mAP than fast-rcnn but much faster error analysis between yolo and fast-rcnnï¼šgreater localization error and less background false-positive combination analysisï¼š[fast-rcnn+yolo] defeats [fast-rcnn+fast-rcnn] since YOLO makes different kinds of mistakes with fast-rcnn generalizabilityï¼šRCNN degrades more because the Selective Search is tuned for natural images, change of dataset makes the proposals get worse. YOLO degrades less because it models the size and shape of objects, change of dataset varies less at object level but more at pixel level. 2. Yolov2: YOLO9000: Better, Faster, Stronger åŠ¨æœºï¼š run at varying sizesï¼šoffering an easy tradeoff between speed and accuracy recognize a wide variety of objects ï¼šjointly train on object detection and classification, so that the model can predict objects that arenâ€™t labelled in detection data better performance but still fast è®ºç‚¹ï¼š Current object detection datasets are limited compared to classification datasets leverage the classification data to expand the scope of current detection system joint training algorithm making the object detectors working on both detection and classification data Better performance often hinges on larger networks or ensembling multiple models. However we want a more accurate detector that is still fast YOLOv1â€™s shortcomings more localization errors low recall è¦ç´ ï¼š better faster backbone stronger uses labeled detection images to learn to precisely localize objects uses classification images to increase its vocabulary and robustness æ–¹æ³•ï¼š betterï¼š batch normalizationï¼šconvergence &amp; regularization add batch normalization on all of the convolutional layers remove dropout from the model high resolution classifierï¼špretrain a hi-res classifier first fine tune the classification network at the full 448 Ã— 448 resolution for 10 epochs on ImageNet then fine tune the resulting network on detection convolutional with anchor boxesï¼š YOLOv1é€šè¿‡ç½‘ç»œæœ€åçš„å…¨è¿æ¥å±‚ï¼Œç›´æ¥é¢„æµ‹æ¯ä¸ªgridä¸Šbounding boxçš„åæ ‡ è€ŒRPNåŸºäºå…ˆéªŒæ¡†ï¼Œä½¿ç”¨æœ€åä¸€å±‚å·ç§¯å±‚ï¼Œåœ¨ç‰¹å¾å›¾çš„å„ä½ç½®é¢„æµ‹bounding boxçš„offsetå’Œconfidence â€œPredicting offsets instead of coordinates simplifies the problem and makes it easier for the network to learnâ€ YOLOv2å»æ‰äº†å…¨è¿æ¥å±‚ï¼Œä¹Ÿä½¿ç”¨anchor boxæ¥å›å½’bounding box eliminate one pooling layer to make the network output have higher resolution shrink the network input to 416416 to obtain an odd number so that there is a *single center cell in the feature map predict class and objectness for every anchor box(offset prediction) instead of nothing(direct location&amp;scale prediction) dimension clusteringï¼š what we want are priors that lead to good IOU scores, thus comes the distance metricï¼š d(box, centroid) = 1 - IOU(box, centroid) direct location predictionï¼š YOLOv1 encounter model instability issue for predicting the (x, y) locations for the box RPN also takes a long time to stabilize by predicting a (tx, ty) and obtain the (x, y) center coordinates indirectly because this formulation is unconstrained so any anchor box can end up at any point in the imageï¼š x = t_x * w_a - x_a\\ y = t_y * h_a - y_a å­¦ä¹ RPNï¼šå›å½’ä¸€ä¸ªç›¸å¯¹é‡ï¼Œæ¯”ç›²çŒœå›å½’ä¸€ä¸ªç»å¯¹locationï¼ˆYOLOv1ï¼‰æ›´å¥½å­¦ä¹  å­¦ä¹ YOLOv1ï¼šåŸºäºcellçš„é¢„æµ‹ï¼Œå°†bounding boxé™å®šåœ¨æœ‰é™åŒºåŸŸï¼Œä¸æ˜¯å…¨å›¾é£ï¼ˆRPNï¼‰ YOLOv2å¯¹æ¯ä¸ªcellï¼ŒåŸºäº5ä¸ªprior anchor sizeï¼Œé¢„æµ‹5ä¸ªbounding boxï¼Œæ¯ä¸ªbounding boxå…·æœ‰5ç»´ï¼š b_x = \sigma(t_x) + c_x\\ b_y = \sigma(t_y) + c_y\\ b_w = p_w e^{t_w}\\ b_h = p_h e^{t_h}\\ Pr(object)*IOU(b,object) = \sigma(t_o) $t_x\ \&amp;\ t_y$ç”¨äºå›å½’bounding boxçš„ä½ç½®ï¼Œé€šè¿‡sigmoidæ¿€æ´»å‡½æ•°è¢«é™å®šåœ¨0-1ï¼Œé€šè¿‡ä¸Šå¼èƒ½å¤Ÿé—´æ¥å¾—åˆ°bounding boxçš„å½’ä¸€åŒ–ä½ç½®ï¼ˆç›¸å¯¹åŸå›¾ï¼‰ $t_w\ \&amp;\ t_h$ç”¨äºå›å½’bounding boxçš„å°ºåº¦ï¼Œè¾“å‡ºåº”è¯¥ä¸æ˜¯0-1é™å®šï¼Œ$p_w\ \&amp;\ p_h$æ˜¯å…ˆéªŒæ¡†çš„å½’ä¸€åŒ–å°ºåº¦ï¼Œé€šè¿‡ä¸Šå¼èƒ½å¤Ÿé—´æ¥å¾—åˆ°bounding boxçš„å½’ä¸€åŒ–å°ºåº¦ï¼ˆç›¸å¯¹åŸå›¾ï¼‰ $t_o$ç”¨äºå›å½’objectnessï¼Œé€šè¿‡sigmoidé™å®šåœ¨0-1ä¹‹é—´ï¼Œå› ä¸º$Pr(object)\ \&amp;\ IOU(b,object)$éƒ½æ˜¯0-1ä¹‹é—´çš„å€¼ï¼ŒIOUé€šè¿‡å‰é¢å››ä¸ªå€¼èƒ½å¤Ÿæ±‚è§£ï¼Œè¿›è€Œå¯ä»¥è§£è€¦objectness fine-grained featuresï¼š motiveï¼šå°ç‰©ä½“çš„æ£€æµ‹ä¾èµ–æ›´åŠ ç»†ç²’åº¦çš„ç‰¹å¾ cascadeï¼šFaster R-CNN and SSD both run their proposal networks at various feature maps in the network to get a range of resolutions ã€QUESTIONã€‘YOLOv2 simply adds a passthrough layer from an earlier layer at 26 Ã— 26 resolutionï¼š latter featuremap â€”-&gt; upsampling concatenate with early featuremap the detector runs on top of this expanded feature map predicts a $NN(3*(4+1+80))$ tensor for each scale multi-scale trainingï¼š æ¨¡å‹æœ¬èº«ä¸é™å®šè¾“å…¥å°ºå¯¸ï¼šmodel only uses convolutional and pooling layers thus it can be resized on the fly forces the network to learn to predict well across a variety of input dimensions the same network can predict detections at different resolutions lossï¼šcited from the latter yolov3 paper use sum of squared error loss for box coordinate(x,y,w,h)ï¼šthen the gradient is $y_{true} - y_{pred}$ use logistic regression for objectness scoreï¼šwhich should be 1 if the bounding box prior overlaps a ground truth object by more than any other bounding box prior if a bounding box prior is not assigned to a ground truth object it incurs no loss(coordinate&amp;objectness) use binary cross-entropy loss for multilabel classification fasterï¼š darknet-19ï¼š YOLOv1ä¸­è®¨è®ºè¿‡æ¢VGG-16å’ŒYOLOv1ä½¿ç”¨çš„backboneå¯¹æ¯”ï¼Œå‰è€…æœ‰mapæå‡ï¼Œä½†æ˜¯è€—æ—¶ã€‚ YOLOv2çš„æ–°backboneï¼Œå‚æ•°æ›´å°‘ï¼Œè€Œä¸”ç›¸å¯¹äºVGG16åœ¨ImageNetä¸Šç²¾åº¦æ›´é«˜ã€‚ training for classificationï¼š * first train on ImageNet using 224*224 * then fine-tuning on 448*448 training for detectionï¼š remove the last convolutional layer add on three 3 Ã— 3 convolutional layers with 1024 filters each followed by a final 1Ã—1 convolutional layer with the number of outputs we need for detection add a passthrough from the final 3 Ã— 3 Ã— 512 layer to the second to last convolutional layer so that our model can use fine grain features. strongerï¼š jointly trainingï¼šä»¥åå†å¡«å‘ æ„é€ æ ‡ç­¾æ ‘ classification sampleç”¨cls lossï¼Œdetection sampleç”¨detect loss é¢„æµ‹æ­£ç¡®çš„classification sampleç»™ä¸€ä¸ª.3 IOUçš„å‡è®¾å€¼ç”¨äºè®¡ç®—objectness loss 3. Yolov3: An Incremental Improvement åŠ¨æœºï¼š nothing like super interesting, just a bunch of small changes that make it better æ–¹æ³•ï¼š bounding box predictionï¼š use anchor boxes and predicts offsets for each bounding box use sum of squared error loss for training predicts the objectness score for each bounding box using logistic regression one ground truth coresponds to one best box and one loss class predictionï¼š use binary cross-entropy loss for multilabel classification ã€NEWã€‘prediction across scalesï¼š the detectorï¼ša few more convolutional layers following the feature map, the last of which predicts a 3-d(for 3 priors) tensor encoding bounding box, objectness, and class predictions expanded feature mapï¼šupsampling the deeper feature map by 2X and concatenating with the former features â€œWith the new multi-scale predictions, YOLOv3 has better perfomance on small objects and comparatively worse performance on medium and larger size objects â€œ ã€NEWã€‘feature extractorï¼š darknet-53 ! trainingï¼šcommon skills 4. ä¸€äº›è¡¥å…… metricsï¼šmAP æœ€æ—©ç”±PASCAL VOCæå‡ºï¼Œè¾“å‡ºç»“æœæ˜¯ä¸€ä¸ªranked listï¼Œæ¯ä¸€é¡¹åŒ…å«æ¡†ã€confidenceã€classï¼Œ yolov3æåˆ°äº†ä¸€ä¸ªâ€œCOCOs weird average mean AP metric â€ IoUï¼šé¢„æµ‹æ¡†ä¸ground truthçš„äº¤å¹¶æ¯”ï¼Œä¹Ÿè¢«ç§°ä¸ºJaccardæŒ‡æ•°ï¼Œæˆ‘ä»¬é€šå¸¸ç”¨å…¶æ¥åˆ¤æ–­æ¯ä¸ªæ£€æµ‹çš„æ­£ç¡®æ€§ã€‚PASCAL VOCæ•°æ®é›†ç”¨0.5ä¸ºé˜ˆå€¼æ¥åˆ¤å®šé¢„æµ‹æ¡†æ˜¯True Positiveè¿˜æ˜¯False Positiveï¼ŒCOCOæ•°æ®é›†åˆ™å»ºè®®å¯¹ä¸åŒçš„IoUé˜ˆå€¼è¿›è¡Œè®¡ç®—ã€‚ ç½®ä¿¡åº¦ï¼šé€šè¿‡æ”¹å˜ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œæˆ‘ä»¬å¯ä»¥æ”¹å˜ä¸€ä¸ªé¢„æµ‹æ¡†æ˜¯Positiveè¿˜æ˜¯ Negativeã€‚ precision &amp; recallï¼šprecision = TP ï¼(TP + FP)ï¼Œrecall = TPï¼(TP + FN)ã€‚å›¾ç‰‡ä¸­æˆ‘ä»¬æ²¡æœ‰é¢„æµ‹åˆ°çš„æ¯ä¸ªéƒ¨åˆ†éƒ½æ˜¯Negativeï¼Œå› æ­¤è®¡ç®—True Negativesæ¯”è¾ƒéš¾åŠã€‚ä½†æ˜¯æˆ‘ä»¬åªéœ€è¦è®¡ç®—False Negativesï¼Œå³æˆ‘ä»¬æ¨¡å‹æ‰€æ¼æ£€çš„ç‰©ä½“ã€‚ APï¼šä¸åŒçš„ç½®ä¿¡åº¦ä¸‹ä¼šå¾—åˆ°ä¸åŒçš„precision-recallã€‚ä¸ºäº†å¾—åˆ°precision-recallæ›²çº¿ï¼Œé¦–å…ˆå¯¹æ¨¡å‹é¢„æµ‹ç»“æœè¿›è¡Œæ’åºï¼ŒæŒ‰ç…§å„ä¸ªé¢„æµ‹å€¼ç½®ä¿¡åº¦é™åºæ’åˆ—ã€‚ç»™å®šä¸åŒçš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œå°±æœ‰ä¸åŒçš„ranked outputï¼ŒRecallå’ŒPrecisionä»…åœ¨é«˜äºè¯¥rankå€¼çš„é¢„æµ‹ç»“æœä¸­è®¡ç®—ã€‚è¿™é‡Œå…±é€‰æ‹©11ä¸ªä¸åŒçš„recallï¼ˆ[0, 0.1, â€¦, 0.9, 1.0]ï¼‰ï¼Œé‚£ä¹ˆAPå°±å®šä¹‰ä¸ºåœ¨è¿™11ä¸ªrecallä¸‹precisionçš„å¹³å‡å€¼ï¼Œå…¶å¯ä»¥è¡¨å¾æ•´ä¸ªprecision-recallæ›²çº¿ï¼ˆæ›²çº¿ä¸‹é¢ç§¯ï¼‰ã€‚ç»™å®šrecallä¸‹çš„precisionè®¡ç®—ï¼Œæ˜¯é€šè¿‡ä¸€ç§æ’å€¼çš„æ–¹å¼ï¼š AP = \frac{1}{11}\sum_{r\in\{0,0.1,...,1.0\}}p_{interp}(r) \\ p_{interp}(r) = max_{\tilde r: \tilde r > r} p(\tilde r) mAPï¼šæ­¤åº¦é‡æŒ‡æ ‡åœ¨ä¿¡æ¯æ£€ç´¢å’Œç›®æ ‡æ£€æµ‹é¢†åŸŸæœ‰ä¸åŒçš„è®¡ç®—æ–¹å¼ã€‚å¯¹äºç›®æ ‡æ£€æµ‹ï¼Œå¯¹äºå„ä¸ªç±»åˆ«ï¼Œåˆ†åˆ«æŒ‰ç…§ä¸Šè¿°æ–¹å¼è®¡ç®—APï¼Œå–æ‰€æœ‰ç±»åˆ«çš„APå¹³å‡å€¼å°±æ˜¯mAPã€‚ evalï¼š yolo_headè¾“å‡ºï¼šbox_xyæ˜¯boxçš„ä¸­å¿ƒåæ ‡ï¼Œ(0~1)ç›¸å¯¹å€¼ï¼›box_whæ˜¯boxçš„å®½é«˜ï¼Œ(0~1)ç›¸å¯¹å€¼ï¼›box_confidenceæ˜¯æ¡†ä¸­ç‰©ä½“ç½®ä¿¡åº¦ï¼›box_class_probsæ˜¯ç±»åˆ«ç½®ä¿¡åº¦ï¼› yolo_correct_boxeså‡½æ•°ï¼šèƒ½å¤Ÿå°†boxä¸­å¿ƒçš„ç›¸å¯¹ä¿¡æ¯è½¬æ¢æˆ[y_min,x_min,y_max,x_max]çš„ç»å¯¹å€¼ yolo_boxes_and_scoreså‡½æ•°ï¼šè¾“å‡ºç½‘ç»œé¢„æµ‹çš„æ‰€æœ‰box yolo_evalå‡½æ•°ï¼šåŸºäºscore_thresholdã€max_boxesä¸¤é¡¹è¿‡æ»¤ï¼Œç±»å†…NMSï¼Œå¾—åˆ°æœ€ç»ˆè¾“å‡º 4. YOLOv4: Optimal Speed and Accuracy of Object Detection åŠ¨æœº Practical testing the tricks of improving CNN some features work for certain problems/dataset exclusively applicable to the majority of models, tasks, and datasets only increase the training cost [bag-of-freebies] only increase the inference cost by a small amount but can significantly improve the accuracy [bag-of-specials] Optimal Speed and Accuracy è®ºç‚¹ headï¼š predict classes and bounding boxes one-stage head YOLO, SSD, RetinaNet anchor-freeï¼šCenterNet, CornerNet, FCOS two-stage head R-CNN series anchor-freeï¼šRepPoints neckï¼š collect feature maps from different stages FPN, PAN, BiFPN, NAS-FPN backboneï¼š pre-trained on ImageNet VGG, ResNet, ResNeXt, DenseNet Bag of freebies data augmentation pixel-wise adjustments photometric distortionsï¼šbrightness, contrast, hue, saturation, and noise geometric distortionsï¼šrandom scaling, cropping, flipping, and rotating object-wise cutï¼š to imageï¼šCutOut to featuremapsï¼šDropOut, DropConnect, DropBlock addï¼šMixUp, CutMix, GAN data imbalance for classification two-stageï¼šhard example mining one-stageï¼šfocal loss, soft label bounding box regression MSE-regressionï¼štreat [x,y,w,h] as independent variables IoU lossï¼šconsider the integrity &amp; scale invariant Bag of specials enlarging receptive fieldï¼šimproved SPP, ASPP, RFB introducing attention mechanism channel-wise attentionï¼šSE, increase the inference time by about 10% point-wise attentionï¼šSpatial Attention Module (SAM), does not affect the speed of inference strengthening feature integration channel-wise levelï¼šSFAM point-wise levelï¼šASFF scale-wise levelï¼šBiFPN activation functionï¼šA good activation function can make the gradient more efficiently propagated post-processingï¼šå„ç§NMS æ–¹æ³• choose a backbone â€”- CSPDarknet53 Higher input network size (resolution) â€“ for detecting multiple small-sized objects More conv layers â€“ for a higher receptive field to cover the increased size of input network More parameters â€“ for greater capacity of a model to detect multiple objects of different sizes in a single image add the SPP block over the CSPDarknet53 significantly increases the receptive field separates out the most significant context features causes almost no re- duction of the network operation speed use PANet as the method of parameter aggregation Modified PAN replace shortcut connection of PAN to concatenation use YOLOv3 (anchor based) head Mosaic data augmentation mixes 4 training images allows detection of objects outside their normal context reduces the need for a large mini-batch size Self-Adversarial Training (SAT) data augmentation 1st stage alters images 2nd stage train on the modified images CmBNï¼ša CBN modified version modified SAMï¼šfrom spatial-wise attention to point- wise attention è¿™é‡Œçš„SAM for ã€ŠAn Empirical Study of Spatial Attention Mechanisms in Deep Networksã€‹ ï¼Œç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ è¿˜æœ‰ä¸€ç¯‡SAMæ˜¯ã€ŠSharpness-Aware Minimization for Efficiently Improving Generalizationã€‹ï¼Œgoogleçš„é”åº¦æ„ŸçŸ¥æœ€å°åŒ–ï¼Œç”¨æ¥æå‡æ¨¡å‹æ³›åŒ–æ€§èƒ½ï¼Œæ³¨æ„åŒºåˆ† å®éªŒ Influence of different features on Classifier training Bluringå’ŒSwishæ²¡æœ‰æå‡ Influence of different features on Detector training IoU threshold, CmBN, Cosine annealing sheduler, CIOUæœ‰æå‡ POLY-YOLO: HIGHER SPEED, MORE PRECISE DETECTION AND INSTANCE SEGMENTATION FOR YOLOV3 åŠ¨æœº yoloV3â€™s weakness rewritten labels inefficient distribution of anchors light backboneï¼š stairstep upsampling single scale output to extend instance segmentation detect size-independent polygons defined on a polar grid real-time processing è®ºç‚¹ yolov3 real-time low precision cmp with RetinaNet, EfficientDet low precision of the detection of big boxes rewriting of labels by each-other due to the coarse resolution this paper solutionï¼š è§£å†³yoloç²¾åº¦é—®é¢˜ï¼špropose a brand-new feature decoder with a single ouput tensor that goes to head with higher resolution å¤šå°ºåº¦ç‰¹å¾èåˆï¼šutilize stairstep upscaling å®ä¾‹åˆ†å‰²ï¼šbounding polygon within a poly grid instance segmentation two-stageï¼šmask-rcnn one-stageï¼š top-downï¼šsegmenting this object within a bounding box bottom-upï¼šstart with clustering pixels direct methodsï¼šæ—¢ä¸éœ€è¦bounding boxä¹Ÿä¸éœ€è¦clustered pixelsï¼ŒPolarMask cmp with PolarMask size-independentï¼šå°ºåº¦ï¼Œå¤§å°ç›®æ ‡éƒ½èƒ½æ£€æµ‹ dynamic number of verticesï¼šå¤šè¾¹å½¢å®šç‚¹å¯å˜ yolov3 issues rewriting of labelsï¼š ä¸¤ä¸ªç›®æ ‡å¦‚æœè½åœ¨åŒä¸€ä¸ªæ ¼å­é‡Œï¼Œåœ¨ä¸€ä¸ªå°ºåº¦ä¸Šground truth labelåªä¼šä¿ç•™ä¸€ä¸ªbox å¯¹è¶Šå°çš„ç‰¹å¾å›¾ï¼Œgridè¶Šå¤§ï¼Œè¿™ä¸ªé—®é¢˜è¶Šä¸¥é‡ imbalanced distribution of anchors across output scales anchorå¦‚æœé€‰çš„ä¸åˆç†ï¼Œä¼šå¯¼è‡´ç‰¹å¾å›¾å°ºåº¦å’Œanchorå°ºåº¦ä¸åŒ¹é… most of the boxes will be captured by the middle output layer and the two other layers will be underused å¦‚ä¸Šé¢è½¦çš„caseï¼Œå¤§å¤šæ•°è½¦çš„æ¡†å¾ˆå°ï¼Œèšç±»å‡ºçš„ç»™level0å’Œlevel1çš„anchor shapeè¿˜æ˜¯å¾ˆå°ï¼Œä½†æ˜¯level0æ˜¯ç¨€ç–grid ä¸€æ–¹é¢ï¼Œgrid shapeå’Œanchor shapeä¸åŒ¹é… ä¸€æ–¹é¢ï¼Œlabel rewritené—®é¢˜ä¼šå‡çº§ åè¿‡æ¥ï¼Œå¦‚æœdense gridä¸Šé¢„æµ‹å¤§ç›®æ ‡ï¼Œä¼šå—åˆ°æ„Ÿå—é‡çš„åˆ¶çº¦ ä¸€ç§è§£å†³æ–¹æ¡ˆæ˜¯åŸºäºæ„Ÿå—é‡é¦–å…ˆå¯¹gt boxåˆ†æˆä¸‰ç»„ï¼Œç„¶ååˆ†åˆ«èšç±»ï¼Œç„¶å9é€‰1 yolov3åŸæ–‡ï¼šYOLOv3 has relatively high $AP_{small}$ performance. However, it has comparatively worse performance on medium and larger size objects. More investigation is needed to get to the bottom of this. å°ç›®æ ‡performanceæ›´å¥½ï¼Œå¤§ç›®æ ‡worseï¼Œä¸»è¦æ˜¯å°±æ˜¯å› ä¸ºcoarse gridä¸Šå­˜åœ¨label rewritené—®é¢˜ï¼Œå­˜åœ¨éƒ¨åˆ†gt boxè¢«æŠ‘åˆ¶æ‰äº†ã€‚ æ–¹æ³• architecture single output higher resolutionï¼šstride4 handle all the anchors at once cross-scale fusion hypercolumn techniqueï¼šadd operation stairstep interpolationï¼šx2 x2 â€¦ SE-blocks reduced the number of convolutional filters to 75% in the feature extraction phase bounding polygons extend the box tupleï¼š$b_i=\{b_i^{x^1},b_i^{y^1},b_i^{x^2},b_i^{y^2},V_i\}$ The center of a bounding box is used as the origin polygon tupleï¼š$v_{i,j}=\{\alpha_{i,j},\beta_{i,j},\gamma_{i,j}\}$ polar coordinateï¼šdistance &amp; oriented angleï¼Œç›¸å¯¹è·ç¦»ï¼ˆç›¸å¯¹anchor boxçš„å¯¹è§’çº¿ï¼‰ï¼Œç›¸å¯¹è§’åº¦ï¼ˆnormåˆ°[0,1]ï¼‰ polar cellï¼šä¸€å®šè§’åº¦çš„æ‰‡å½¢åŒºåŸŸ å†…ï¼Œå¦‚æœsectorå†…æ²¡æœ‰å®šç‚¹ï¼Œconf=0 general shapeï¼š ä¸åŒå°ºåº¦ï¼Œå½¢çŠ¶ç›¸åŒçš„objectï¼Œåœ¨polar coordä¸‹è¡¨ç¤ºæ˜¯ä¸€æ ·çš„ distance*anchor boxçš„å¯¹è§’çº¿ï¼Œè½¬æ¢æˆç»å¯¹å°ºåº¦ bounding boxçš„ä¸¤ä¸ªå¯¹è§’é¢„æµ‹ï¼Œè´Ÿè´£å°ºåº¦ä¼°è®¡ï¼Œpolygonåªè´Ÿè´£é¢„æµ‹å½¢çŠ¶ sharing values should make the learning easier mix loss outputï¼ša*(4+1+3*n_vmax) box center lossï¼šbce box wh lossï¼šl2 loss conf lossï¼šbce with ignore mask cls lossï¼šbce polygon lossï¼š$\gamma(log(\frac{\alpha}{anchor^d})-\hat a)^2 + \gammabce(\beta,\hat{beta})+bce(\gamma, \hat \gamma)$ auxiliary task learningï¼š ä»»åŠ¡é—´ç›¸äº’boost converge faster Scaled-YOLOv4: Scaling Cross Stage Partial Network åŠ¨æœº model scaling method redesign yolov4 and propose yolov4-CSP develop scaled yolov4 yolov4-tiny yolov4-large æ²¡ä»€ä¹ˆæŠ€æœ¯ç»†èŠ‚ï¼Œå°±æ˜¯ç½‘ç»œç»“æ„å¤§æ›´æ–° è®ºç‚¹ common technique changes depth &amp; width of the backbone recently there are NAS model scaling input sizeã€widthã€depthå¯¹ç½‘ç»œè®¡ç®—é‡å‘ˆç°square, linear, and square increase æ”¹æˆCSPç‰ˆæœ¬ä»¥åï¼Œèƒ½å¤Ÿå‡å°‘å‚æ•°é‡ã€è®¡ç®—é‡ï¼Œæé«˜accï¼Œç¼©çŸ­inference time æ£€æµ‹çš„å‡†ç¡®æ€§é«˜åº¦ä¾èµ–reception fieldï¼ŒRFéšç€depthçº¿æ€§å¢é•¿ï¼Œéšç€strideå€æ•°å¢é•¿ï¼Œæ‰€ä»¥ä¸€èˆ¬å…ˆç»„åˆè°ƒèŠ‚input sizeå’Œstageï¼Œç„¶åå†æ ¹æ®ç®—åŠ›è°ƒæ•´depthå’Œwidth æ–¹æ³• backboneï¼šCSPDarknet53 neckï¼šCSP-PANï¼Œå‡å°‘40%è®¡ç®—é‡ï¼ŒSPP yoloV4-tiny yoloV4-largeï¼šP456]]></content>
      <tags>
        <tag>ç›®æ ‡æ£€æµ‹ï¼Œone-stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[triplet-center-lossè®ºæ–‡]]></title>
    <url>%2F2019%2F11%2F13%2Ftriplet-center-loss%E8%AE%BA%E6%96%87%2F</url>
    <content type="text"><![CDATA[0. before readingç»“åˆï¼š triplet lossï¼šè€ƒè™‘ç±»é—´å…³ç³»ï¼Œä½†è®¡ç®—å¤æ‚åº¦é«˜ï¼Œå›°éš¾æ ·æœ¬éš¾æŒ–æ˜ center lossï¼šè€ƒè™‘ç±»å†…å…³ç³» TCLï¼šåŒæ—¶å¢åŠ ç±»å†…æ•°æ®çš„ç´§å®åº¦ï¼ˆcompactnessï¼‰å’Œç±»é—´çš„åˆ†ç¦»åº¦ï¼ˆseparabilityï¼‰ ä¸‰å…ƒç»„åªè€ƒè™‘æ ·æœ¬ã€æ‰€å±ç±»ä¸­å¿ƒã€æœ€è¿‘é‚»ç±»çš„ä¸­å¿ƒã€‚é¿å…äº†å»ºç«‹tripletsçš„å¤æ‚åº¦å’Œmining hard samplesçš„éš¾åº¦ã€‚ titleï¼šTriplet-Center Loss for Multi-View 3D Object Retrieval åŠ¨æœºï¼šdeep metric learning the learned features using softmax loss are not discriminative enough in nature although samples of the two classes are separated by the decision boundary elaborately, there exists significant intra-class variations QUESTION1ï¼šso what? how does this affect the current task? åŠ¨æœºæè¿°ä¸å……åˆ†ã€‚ QUESTION2ï¼šåœ¨äºŒç»´å¹³é¢ä¸Šoverlapä¸ä»£è¡¨åœ¨é«˜ç»´ç©ºé—´ä¸­overlapï¼Œè¿™ç§illustrationç©¶ç«Ÿæ˜¯å¦æœ‰æ„ä¹‰ã€‚ ANSWER for aboveï¼šé«˜ç»´ç©ºé—´å¯åˆ†ï¼ŒæŠ•å½±åˆ°äºŒç»´å¹³é¢ä¸ä¸€å®šå¯åˆ†ï¼Œä½†æ˜¯åè¿‡æ¥ï¼ŒäºŒç»´å¹³é¢ä¸Šé«˜åº¦å¯åˆ†ï¼Œæ˜ å°„ä¼šé«˜ç»´ç©ºé—´æ•°æ®ä»æ—§æ˜¯é«˜åº¦å¯åˆ†çš„ã€‚åªèƒ½è¯´ï¼Œåè€…èƒ½å¤Ÿç¡®ä¿ä¸åŒç±»åˆ«æ•°æ®ç¦»æ•£æ€§æ›´å¥½ï¼Œä¸èƒ½è¯´æ˜å‰è€…æ•°æ®ç¦»æ•£æ€§ä¸å¥½ï¼ˆå¦‚æœå®šä¹‰äº†é«˜ç»´è·ç¦»ï¼Œä¹Ÿå¯ä»¥è¯´æ˜ï¼‰ã€‚ åº”ç”¨åœºæ™¯ï¼š3D object retrieval è¦ç´ ï¼š learns a center for each class requires that the distances between samples and centers from the same class are smaller than those from different classes, in this way the samples are pulled closer to the corresponding center and meanwhile pushed away from the different centers both the inter-class separability and the intra-class variations are considered è®ºç‚¹ï¼š Compared with triplet loss, TCL avoids the complex construction of triplets and hard sample mining mechanism. Compared with center loss, TCL not only considers to reduce the intra-class variations. QUESTIONï¼šwhat about the comparison with [softmax loss + center loss]? ANSWER for aboveï¼šcenter-loss is actually representing for the joint loss [softmax loss + center loss]. â€˜â€™Since the class centers are updated at each iteration based on a mini-batch instead of the whole dataset, which can be very unstable, it has to be under the joint supervision of softmax loss during training. â€˜â€™ æœ¬æ–‡åšæ³•ï¼š the proposed TCL is used as the supervision loss the softmax loss could be also combined in as an addition ç»†èŠ‚ï¼š TCLï¼š L_{tc} = \sum_{i=1}^Mmax(D(f_i, c_{y^i}) + m - min_{j\neq y^i}D(f_i, c_j), 0) å‰åŠéƒ¨åˆ†æ˜¯center-lossï¼Œç±»å†…æ¬§å‡ é‡Œå¾—è·ç¦»ï¼ŒååŠéƒ¨åˆ†æ˜¯æ¯ä¸ªæ ·æœ¬å’Œä¸å…¶æœ€è¿‘çš„negative centerä¹‹é—´çš„è·ç¦»ã€‚ â€˜Unlike center loss, TCL can be used independently from softmax loss. Howeverâ€¦ â€˜ ä½œè€…è§£é‡Šè¯´ï¼Œå› ä¸ºcenter layeræ˜¯éšæœºåˆå§‹åŒ–å‡ºæ¥çš„ï¼Œè€Œä¸”æ˜¯batch updatingï¼Œå› æ­¤å¼€å§‹é˜¶æ®µä¼šæ¯”è¾ƒtrickyï¼Œâ€™while softmax loss could serve as a good guider for seeking better class centers â€˜ è°ƒå‚ä¸­æåˆ°â€™m is fixed to 5â€™ï¼Œè¯´æ˜æœ¬æ–‡å¯¹feature vectoræ²¡æœ‰åšnormalizationï¼ˆç›¸æ¯”ä¹‹ä¸‹facenetåšäº†å½’ä¸€åŒ–ï¼Œé™å®šæ‰€æœ‰embeddingåˆ†å¸ƒåœ¨é«˜ç»´çƒé¢ä¸Šï¼‰ã€‚ è¡¡é‡æŒ‡æ ‡ï¼šAUCå’ŒMAPï¼Œè¿™æ˜¯ä¸€ä¸ªretrievalä»»åŠ¡ï¼Œæœ€ç»ˆéœ€è¦çš„æ˜¯embeddingï¼Œç»™å®šQueryï¼Œå¬å›top matchesã€‚ reviewsï¼š ä¸ªäººç†è§£ï¼š softmaxåˆ†ç±»å™¨æ—¨åœ¨æ•°æ®å¯åˆ†ï¼Œå¯¹äºåˆ†ç±»è¾¹ç•Œã€feature vectorçš„ç©ºé—´æ„ä¹‰ä¸å­˜åœ¨ä¸€ä¸ªå…·è±¡çš„æè¿°ã€‚deep metric learningèƒ½å¤Ÿå¼•å…¥è¿™ç§å…·è±¡çš„ã€å›¾åƒå­¦çš„æ„ä¹‰ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šï¼Œæ¢è®¨distanceã€centeræ‰æœ‰æ„ä¹‰ã€‚ å°±å°é—­ç±»æ•°æ®ï¼ˆç±»åˆ«æœ‰é™ä¸”å·²çŸ¥ï¼‰åˆ†ç±»æ¥è®²ï¼Œåˆ†ç±»è¾¹ç•Œæœ‰æ— å›¾åƒå­¦æè¿°å…¶å®æ„ä¹‰ä¸å¤§ã€‚å·²çŸ¥çš„æ•°æ®åˆ†å¸ƒå°½å¯èƒ½discriminativeçš„ä¸»è¦æ„ä¹‰æ˜¯é’ˆå¯¹æœªçŸ¥ç±»åˆ«ï¼Œæˆ‘ä»¬å¸Œæœ›ç»™åˆ°æ¨¡å‹ä¸€ä¸ªæœªçŸ¥æ•°æ®æ—¶ï¼Œå®ƒèƒ½å¤Ÿæ£€æµ‹å‡ºæ¥ï¼Œè€Œä¸æ˜¯åˆ’å…¥æŸä¸ªå·²çŸ¥ç±»ï¼ˆsoftmaxï¼‰ã€‚ TCLçš„æœ€å¤§è´¡çŒ®åº”è¯¥æ˜¯æƒ³åˆ°ç”¨centeræ›¿ä»£æ ·æœ¬æ¥è¿›è¡Œmetric judgementï¼Œæ”¹å–„triplet-losså¤æ‚è®¡ç®—é‡è¿™ä¸€é—®é¢˜ï¼Œåè€…å®é™…è®­èµ·æ¥å¤ªéš¾äº†ï¼Œæ²¡æœ‰æ„Ÿæƒ…çš„GPUåå™¬æœºå™¨ã€‚ XXXï¼š èƒ½å¤Ÿå¼•å…¥è¿™ç§å…·è±¡çš„ã€å›¾åƒå­¦çš„æ„ä¹‰ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æ¢è®¨distanceã€centeræ‰æœ‰æ„ä¹‰ã€‚ â€‹]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dicomReader]]></title>
    <url>%2F2019%2F11%2F11%2FdicomReader%2F</url>
    <content type="text"><![CDATA[read a dcm file 1234import SimpleITK as sitkimage = sitk.ReadImage(dcm_file)image_arr = sitk.GetArrayFromImage(image) read a dcm series 12345678910series_IDs = sitk.ImageSeriesReader.GetGDCMSeriesIDs(series_path)nb_series = len(series_IDs)print(nb_series)# é»˜è®¤è·å–ç¬¬ä¸€ä¸ªåºåˆ—çš„æ‰€æœ‰åˆ‡ç‰‡è·¯å¾„dicom_names = sitk.ImageSeriesReader.GetGDCMSeriesFileNames(file_path)series_reader = sitk.ImageSeriesReader()series_reader.SetFileNames(dicom_names)image3D = series_reader.Execute() read a dcm case 123456series_IDs = sitk.ImageSeriesReader.GetGDCMSeriesIDs(case_path)for series_id in series_IDs: dicom_names = sitk.ImageSeriesReader.GetGDCMSeriesFileNames(case_path, series_id) series_reader = sitk.ImageSeriesReader() series_reader.SetFileNames(dicom_names) image3D = series_reader.Execute() read tag 12# é¦–å…ˆå¾—åˆ°imageå¯¹è±¡Image_type = image.GetMetaData("0008|0008") if image.HasMetaData("0008|0008") else 'Nan' å‘ç°ä¸€ç§åºåˆ—ï¼Œæ¯å¼ å›¾çš„å°ºå¯¸ä¸åŒï¼Œè¿™æ ·æ‰§è¡Œseries_readerçš„æ—¶å€™ä¼šæŠ¥é”™ï¼Œå› ä¸ºseries_readerä¼šä¾ç…§ç¬¬ä¸€å±‚çš„å›¾åƒå°ºå¯¸ç”³è¯·ç©ºé—´ï¼Œæ‰€ä»¥è¦ä¹ˆå¼‚å¸¸è¦ä¹ˆé€å¼ è¯»ã€‚ reference: http://itk-users.7.n7.nabble.com/ITK-users-Reader-InvalidRequestedRegionError-td38608.html]]></content>
      <tags>
        <tag>SimpleITK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++ tricks in engineering]]></title>
    <url>%2F2019%2F11%2F06%2Fc-tricks-in-engineering%2F</url>
    <content type="text"><![CDATA[æ•°ç»„ä¼ å‚ å·¥ç¨‹åŒ–è¢«å‘äº†å¥½å¤šå›ï¼ C/C++ ä¼ é€’æ•°ç»„ï¼Œè™½ç„¶ä¼ é€’çš„æ˜¯é¦–åœ°å€åœ°å€ï¼Œä½†æ˜¯å‚æ•°åˆ°äº†å‡½æ•°å†…å°±æˆäº†æ™®é€šæŒ‡é’ˆã€‚ æ‰€ä»¥è¯•å›¾åœ¨è°ƒç”¨å‡½æ•°ä¸­æ±‚å–æ‰€ä¼ é€’æ•°ç»„çš„é•¿åº¦æ˜¯è¡Œä¸é€šçš„ã€‚ vectorä¼ å‚ ä¼ å€¼â€”&gt;æ‹·è´æ„é€ ï¼Œä¼ å¼•ç”¨ï¼æŒ‡é’ˆâ€”&gt;ä¸å‘ç”Ÿæ‹·è´æ„é€ ã€‚ å®é™…å·¥ç¨‹åŒ–ä¸­é‡åˆ°çš„é—®é¢˜æ˜¯ï¼Œæ„å»ºäº†ä¸€ä¸ªvector\ imgså¯¹è±¡ï¼Œä¼ å…¥å‡½æ•°ä»¥åï¼Œåœ¨å‡½æ•°å†…éƒ¨åˆ›å»ºç©ºé—´cv::Mat imgï¼Œç„¶åå°†img pushè¿›vectorã€‚åœ¨å‡½æ•°å¤–è¯»å–è¯¥vectorçš„æ—¶å€™å‘ç°å…¶å†…éƒ¨æ²¡å€¼ã€‚ è¦ç‚¹ï¼š1. è¦ä¼ å¼•ç”¨ï¼Œ2. push cloneï¼šimgs.push_back(img) å¦å¤–ï¼Œvectorå¯ä»¥ä½œä¸ºå‡½æ•°è¿”å›å€¼ã€‚]]></content>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å›¾åƒç®—æ³•ç»¼è¿°]]></title>
    <url>%2F2019%2F10%2F31%2F%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[ç±»åˆ« æŒ‰ç…§ä»»åŠ¡ç±»å‹ï¼šåº¦é‡å­¦ä¹ ï¼ˆmetric learningï¼‰å’Œæè¿°å­å­¦ä¹ ï¼ˆimage descriptor learningï¼‰ æŒ‰ç…§ç½‘ç»œç»“æ„ï¼špairwiseçš„siameseç»“æ„ã€tripletçš„three branchç»“æ„ã€ä»¥åŠå¼•å…¥å°ºåº¦ä¿¡æ¯çš„central-surroundç»“æ„ æŒ‰ç…§ç½‘ç»œè¾“å‡ºï¼šç‰¹å¾å‘é‡ï¼ˆfeature embeddingï¼‰å’Œå•ä¸ªæ¦‚ç‡å€¼ï¼ˆpairwise similarityï¼‰ æŒ‰ç…§æŸå¤±å‡½æ•°ï¼šå¯¹æ¯”æŸå¤±å‡½æ•°ã€äº¤å‰ç†µæŸå¤±å‡½æ•°ã€triplet lossã€hinge lossç­‰ï¼Œæ­¤å¤–æŸå¤±å‡½æ•°å¯ä»¥å¸¦æœ‰éšå¼çš„å›°éš¾æ ·æœ¬æŒ–æ˜ï¼Œä¾‹å¦‚pn-netä¸­çš„softpnç­‰ï¼Œä¹Ÿå¯ä»¥æ˜¯æ˜¾ç¤ºçš„å›°éš¾æŒ–æ˜ã€‚ Plainç½‘ç»œ ä¸»è¦æ˜¯è¯´AlexNetï¼VGG-Netï¼Œåè€…æ›´å¸¸ç”¨ä¸€äº›ã€‚ Plainç½‘ç»œçš„è®¾è®¡ä¸»è¦éµå¾ªä»¥ä¸‹å‡ ä¸ªå‡†åˆ™ï¼š ï¼ˆ1ï¼‰è¾“å‡ºç‰¹å¾å›¾å°ºå¯¸ç›¸åŒçš„å±‚ä½¿ç”¨ç›¸åŒæ•°é‡çš„æ»¤æ³¢å™¨ã€‚ ï¼ˆ2ï¼‰å¦‚æœç‰¹å¾å›¾å°ºå¯¸å‡åŠï¼Œé‚£ä¹ˆæ»¤æ³¢å™¨æ•°é‡å°±åŠ å€ï¼Œä»è€Œä¿è¯æ¯å±‚çš„æ—¶é—´å¤æ‚åº¦ç›¸åŒï¼ˆè¿™æ˜¯ä¸ºå•¥ï¼Ÿï¼Ÿï¼‰ã€‚ åè¯ æ„Ÿå—é‡ï¼šå·ç§¯ç¥ç»ç½‘ç»œæ¯ä¸€å±‚è¾“å‡ºçš„ç‰¹å¾å›¾ä¸Šçš„åƒç´ ç‚¹åœ¨åŸå§‹å›¾åƒä¸Šæ˜ å°„åŒºåŸŸçš„å¤§å°ã€‚é€šä¿—çš„è¯´ï¼Œå°±æ˜¯è¾“å…¥å›¾åƒå¯¹è¿™ä¸€å±‚è¾“å‡ºçš„ç¥ç»å…ƒçš„å½±å“æœ‰å¤šå¤§ã€‚ æ„Ÿå—é‡è®¡ç®—ï¼šç”±å½“å‰å±‚å‘å‰æ¨ï¼Œéœ€è¦çš„å‚æ•°æ˜¯kernel sizeå’Œstrideã€‚ N\_RF = kernel\_size + (cur\_RF-1)*stride å…¶ä¸­$cur_RF$æ˜¯å½“å‰å±‚ï¼ˆstart from 1ï¼‰ï¼Œ$N_RF$ã€$kernel_size$ã€$stride$æ˜¯ä¸Šä¸€å±‚å‚æ•°ã€‚ æœ‰æ•ˆæ„Ÿå—é‡ï¼šå¹¶ä¸æ˜¯æ„Ÿå—é‡å†…æ‰€æœ‰åƒç´ å¯¹è¾“å‡ºå‘é‡çš„è´¡çŒ®ç›¸åŒï¼Œåœ¨å¾ˆå¤šæƒ…å†µä¸‹æ„Ÿå—é‡åŒºåŸŸå†…åƒç´ çš„å½±å“åˆ†å¸ƒæ˜¯é«˜æ–¯ï¼Œæœ‰æ•ˆæ„Ÿå—é‡ä»…å ç†è®ºæ„Ÿå—é‡çš„ä¸€éƒ¨åˆ†ï¼Œä¸”é«˜æ–¯åˆ†å¸ƒä»ä¸­å¿ƒåˆ°è¾¹ç¼˜å¿«é€Ÿè¡°å‡ã€‚ æ„Ÿå—é‡å¤§å°ï¼š å°æ„Ÿå—é‡ï¼šlocalï¼Œä½ç½®ä¿¡æ¯æ›´å‡†ç¡® å¤§æ„Ÿå—é‡ï¼šglobalï¼Œè¯­ä¹‰ä¿¡æ¯æ›´ä¸°å¯Œ inception moduleï¼šä¸‹å›¾ä¸ºå…¶ä¸­ä¸€ç§ã€‚ æ„ä¹‰ï¼šå¢åŠ ç½‘ç»œæ·±åº¦å’Œå®½åº¦çš„åŒæ—¶ï¼Œå‡å°‘å‚æ•°ã€‚ç»“æ„ä¸­åµŒå…¥äº†å¤šå°ºåº¦ä¿¡æ¯ï¼Œé›†æˆäº†å¤šç§ä¸åŒæ„Ÿå—é‡ä¸Šçš„ç‰¹å¾ã€‚ building blockï¼šå·¦è¾¹è¿™ç§ï¼Œçº¢è‰²æ¡†æ¡†é‡Œé¢æ˜¯ä¸€ä¸ªblockã€‚ å‡ ä¸ªç›¸åŒçš„building blockå †å ä¸ºä¸€å±‚convã€‚åœ¨ç¬¬ä¸€ä¸ªbuilding Blockå—ä¸­ï¼Œè¾“å‡ºç‰¹å¾å›¾çš„å°ºå¯¸ä¸‹é™ä¸€åŠï¼ˆç¬¬ä¸€ä¸ªå·ç§¯stride=2ï¼‰ï¼Œå‰©ä½™çš„building Blockå—è¾“å…¥è¾“å‡ºå°ºå¯¸æ˜¯ä¸€æ ·çš„ã€‚ bottleneckï¼šå³è¾¹è¿™ç§ï¼Œè“è‰²æ¡†æ¡†blockã€‚å­—é¢æ„æ€ï¼Œç“¶é¢ˆï¼Œå½¢å®¹è¾“å…¥è¾“å‡ºç»´åº¦å·®è·è¾ƒå¤§ã€‚ ç¬¬ä¸€ä¸ª1*1è´Ÿè´£é™ä½ç»´åº¦ï¼Œç¬¬äºŒä¸ª1*1è´Ÿè´£æ¢å¤ç»´åº¦ï¼Œ3*3å±‚å°±å¤„åœ¨ä¸€ä¸ªè¾“å…¥ï¼è¾“å‡ºç»´åº¦è¾ƒå°çš„ç“¶é¢ˆã€‚ å·¦å³ä¸¤ç§ç»“æ„æ—¶é—´å¤æ‚åº¦ç›¸ä¼¼ã€‚ &lt;img src=&quot;å›¾åƒç®—æ³•ç»¼è¿°/block.png&quot; width=&quot;30%;&quot; /&gt; &lt;img src=&quot;å›¾åƒç®—æ³•ç»¼è¿°/ImageNet.png&quot; width=&quot;110%;&quot; /&gt; top-1å’Œtop-5ï¼štop-1å°±æ˜¯é¢„æµ‹æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«ï¼Œtop-5åˆ™å–æœ€åé¢„æµ‹æ¦‚ç‡çš„å‰äº”ä¸ªï¼Œåªè¦å…¶ä¸­åŒ…å«æ­£ç¡®ç±»åˆ«åˆ™è®¤ä¸ºé¢„æµ‹æ­£ç¡®ã€‚ ä½¿ç”¨top-5ä¸»è¦æ˜¯å› ä¸ºImageNetä¸­å¾ˆå¤šå›¾ç‰‡ä¸­å…¶å®æ˜¯åŒ…å«å¤šä¸ªç‰©ä½“çš„ã€‚ accuracyã€error rateã€F1-scoreã€sensitivityã€specificityã€precisionã€recall accuracyï¼šæ€»ä½“å‡†ç¡®ç‡ precisionï¼šä»ç»“æœè§’åº¦ï¼Œå•ä¸€ç±»åˆ«å‡†ç¡®ç‡ recallï¼šä»è¾“å…¥è§’åº¦ï¼Œé¢„æµ‹ç±»åˆ«çœŸå®ä¸º1çš„å‡†ç¡®ç‡ P-Ræ›²çº¿ï¼šé€‰ç”¨ä¸åŒé˜ˆå€¼ï¼Œprecision-recallå›´æˆçš„æ›²çº¿ APï¼šå¹³å‡ç²¾åº¦ï¼ŒP-Ræ›²çº¿å›´ä½çš„é¢ç§¯ F1-scoreï¼šå¯¹äºæŸä¸ªåˆ†ç±»ï¼Œç»¼åˆäº†Precisionå’ŒRecallçš„ä¸€ä¸ªåˆ¤æ–­æŒ‡æ ‡ï¼Œå› ä¸ºé€‰ç”¨ä¸åŒé˜ˆå€¼ï¼Œprecision-recallä¼šéšä¹‹å˜åŒ–ï¼ŒF1-scoreç”¨äºé€‰å‡ºæœ€ä½³é˜ˆå€¼ã€‚ sensitivityï¼š=recall specificityï¼šé¢„æµ‹ç±»åˆ«çœŸå®ä¸º0çš„å‡†ç¡®ç‡ referenceï¼šhttps://zhuanlan.zhihu.com/p/33273532 trade-offï¼š FLOPSï¼šæ¯ç§’æµ®ç‚¹è¿ç®—æ¬¡æ•°æ˜¯æ¯ç§’æ‰€æ‰§è¡Œçš„æµ®ç‚¹è¿ç®—æ¬¡æ•°çš„ç®€ç§°ï¼Œè¢«ç”¨æ¥ä¼°ç®—ç”µè„‘æ•ˆèƒ½ã€‚ ROCã€AUCã€MAPï¼š ROCï¼šTPRå’ŒFPRå›´æˆçš„æ›²çº¿ AUCï¼šROCå›´ä½çš„é¢ç§¯ mAPï¼šæ‰€æœ‰ç±»åˆ«APçš„å¹³å‡å€¼ æ¢¯åº¦å¼¥æ•£ï¼š â€œåº•å±‚å…ˆæ”¶æ•›ã€é«˜å±‚å†æ”¶æ•›â€ï¼š ç‰¹å¾å›¾ï¼šå·ç§¯å±‚é€šè¿‡çº¿æ€§æ»¤æ³¢å™¨è¿›è¡Œçº¿æ€§å·ç§¯è¿ç®—ï¼Œç„¶åå†æ¥ä¸ªéçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œæœ€ç»ˆç”Ÿæˆç‰¹å¾å›¾ã€‚ TTA test time augmentationï¼šæµ‹è¯•æ—¶å¢å¼ºï¼Œä¸ºåŸå§‹å›¾åƒé€ å‡ºå¤šä¸ªä¸åŒç‰ˆæœ¬ï¼ŒåŒ…æ‹¬ä¸åŒåŒºåŸŸè£å‰ªå’Œæ›´æ”¹ç¼©æ”¾ç¨‹åº¦ç­‰ï¼Œå¹¶å°†å®ƒä»¬è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼›ç„¶åå¯¹å¤šä¸ªç‰ˆæœ¬è¿›è¡Œè®¡ç®—å¾—åˆ°å¹³å‡è¾“å‡ºï¼Œä½œä¸ºå›¾åƒçš„æœ€ç»ˆè¾“å‡ºåˆ†æ•°ã€‚ pooling mode: full modeï¼šä»filterå’Œimageåˆšå¼€å§‹ç›¸äº¤å¼€å§‹å·ç§¯ same modeï¼šå½“filterçš„ä¸­å¿ƒå’Œimageçš„è§’é‡åˆæ—¶å¼€å§‹å·ç§¯ï¼Œå¦‚æœstride=1ï¼Œé‚£ä¹ˆè¾“å…¥è¾“å‡ºå°ºå¯¸ç›¸åŒ valid modeï¼šå½“filterå®Œå…¨åœ¨imageé‡Œé¢æ—¶å¼€å§‹å·ç§¯ ç©ºé—´ä¸å˜æ€§ï¼š å¹³ç§»ä¸å˜æ€§ï¼šä¸ç®¡è¾“å…¥å¦‚ä½•å¹³ç§»ï¼Œç³»ç»Ÿäº§ç”Ÿå®Œå…¨ç›¸åŒçš„å“åº”ï¼Œæ¯”å¦‚å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œå›¾åƒä¸­çš„ç›®æ ‡ä¸ç®¡è¢«ç§»åŠ¨åˆ°å›¾ç‰‡çš„å“ªä¸ªä½ç½®ï¼Œå¾—åˆ°çš„ç»“æœï¼ˆæ ‡ç­¾ï¼‰åº”è¯¥æ˜¯ç›¸åŒçš„ å¹³ç§»åŒå˜æ€§ï¼ˆtranslation equivarianceï¼‰ï¼šç³»ç»Ÿåœ¨ä¸åŒä½ç½®çš„å·¥ä½œåŸç†ç›¸åŒï¼Œä½†å®ƒçš„å“åº”éšç€ç›®æ ‡ä½ç½®çš„å˜åŒ–è€Œå˜åŒ–ï¼Œæ¯”å¦‚å®ä¾‹åˆ†å‰²ä»»åŠ¡ï¼Œç›®æ ‡å¦‚æœè¢«å¹³ç§»äº†ï¼Œé‚£ä¹ˆè¾“å‡ºçš„å®ä¾‹æ©ç ä¹Ÿç›¸åº”å˜åŒ– å±€éƒ¨è¿æ¥ï¼šæ¯ä¸ªç¥ç»å…ƒæ²¡æœ‰å¿…è¦å¯¹å…¨å±€å›¾åƒè¿›è¡Œæ„ŸçŸ¥ï¼Œåªéœ€è¦å¯¹å±€éƒ¨è¿›è¡Œæ„ŸçŸ¥ï¼Œç„¶ååœ¨æ›´é«˜å±‚å°†å±€éƒ¨çš„ä¿¡æ¯ç»¼åˆèµ·æ¥å°±å¾—åˆ°äº†å…¨å±€çš„ä¿¡æ¯ æƒå€¼å…±äº«ï¼šå¯¹äºè¿™ä¸ªå›¾åƒä¸Šçš„æ‰€æœ‰ä½ç½®ï¼Œæˆ‘ä»¬éƒ½èƒ½ä½¿ç”¨åŒæ ·çš„å­¦ä¹ ç‰¹å¾ æ± åŒ–ï¼šé€šè¿‡æ¶ˆé™¤éæå¤§å€¼ï¼Œé™ä½äº†ä¸Šå±‚çš„è®¡ç®—å¤æ‚åº¦ã€‚æœ€å¤§æ± åŒ–è¿”å›æ„Ÿå—é‡ä¸­çš„æœ€å¤§å€¼ï¼Œå¦‚æœæœ€å¤§å€¼è¢«ç§»åŠ¨äº†ï¼Œä½†æ˜¯ä»ç„¶åœ¨è¿™ä¸ªæ„Ÿå—é‡ä¸­ï¼Œé‚£ä¹ˆæ± åŒ–å±‚ä¹Ÿä»ç„¶ä¼šè¾“å‡ºç›¸åŒçš„æœ€å¤§å€¼ã€‚ å·ç§¯å’Œæ± åŒ–è¿™ä¸¤ç§æ“ä½œå…±åŒæä¾›äº†ä¸€äº›å¹³ç§»ä¸å˜æ€§ï¼Œå³ä½¿å›¾åƒè¢«å¹³ç§»ï¼Œå·ç§¯ä¿è¯ä»ç„¶èƒ½æ£€æµ‹åˆ°å®ƒçš„ç‰¹å¾ï¼Œæ± åŒ–åˆ™å°½å¯èƒ½åœ°ä¿æŒä¸€è‡´çš„è¡¨è¾¾ã€‚ åŒç†ï¼Œæ‰€è°“çš„CNNçš„å°ºåº¦ã€æ—‹è½¬ä¸å˜æ€§ï¼Œä¹Ÿæ˜¯ç”±äºpoolingæ“ä½œï¼Œå¼•å…¥çš„å¾®å°å½¢å˜çš„é²æ£’æ€§ã€‚ æ¨¡å‹å¤§å°ä¸å‚æ•°é‡ï¼šfloat32æ˜¯4ä¸ªå­—èŠ‚ï¼Œå› æ­¤æ¨¡å‹å¤§å°å­—èŠ‚æ•°=å‚æ•°é‡Ã—4 è®­ç»ƒæŠ€å·§ è¿ç§»å­¦ä¹ ï¼šå½“æ•°æ®é›†å¤ªå°ï¼Œæ— æ³•ç”¨æ¥è®­ç»ƒä¸€ä¸ªè¶³å¤Ÿå¥½çš„ç¥ç»ç½‘ç»œï¼Œå¯ä»¥é€‰æ‹©fine-tuneä¸€äº›é¢„è®­ç»ƒç½‘ç»œã€‚ä½¿ç”¨æ—¶ä¿®æ”¹æœ€åå‡ å±‚ï¼Œé™ä½å­¦ä¹ ç‡ã€‚ kerasä¸­ä¸€äº›é¢„è®­ç»ƒæƒé‡ä¸‹è½½åœ°å€ï¼šhttps://github.com/fchollet/deep-learning-models/releases/ K-foldäº¤å‰éªŒè¯ï¼š æˆ‘ä»¬ä¸èƒ½å°†å…¨éƒ¨æ•°æ®é›†ç”¨äºè®­ç»ƒâ€”â€”è¿™æ ·å°±æ²¡æœ‰æ•°æ®æ¥æµ‹è¯•æ¨¡å‹æ€§èƒ½äº† å°†æ•°æ®é›†åˆ†å‰²ä¸ºtraining set å’Œ test setï¼Œè¡¡é‡ç»“æœå–å†³äºæ•°æ®é›†åˆ’åˆ†ï¼Œtraining setå’Œå…¨é›†ä¹‹é—´å­˜åœ¨biasï¼Œä¸åŒtestä¸‹ç»“æœvarietyå¾ˆå¤§ äº¤å‰éªŒè¯Cross-Validationï¼š æç«¯æƒ…å†µLOOCVï¼šå…¨é›†Nï¼Œæ¯æ¬¡å–ä¸€ä¸ªåštestï¼Œå…¶ä»–åštrainï¼Œé‡å¤Næ¬¡ï¼Œå¾—åˆ°Nä¸ªæ¨¡å‹ï¼Œå¹¶è®¡ç®—Nä¸ªteståšå¹³å‡ K-foldï¼šå…¨é›†åˆ‡åˆ†æˆkä»½ï¼Œæ¯æ¬¡å–ä¸€ä¸ªåštestï¼Œå…¶ä»–åštrainï¼Œé‡å¤kæ¬¡ï½ å®éªŒæ˜¾ç¤ºLOOCVå’Œ10-foldCVçš„ç»“æœå¾ˆç›¸è¿‘ï¼Œåè€…è®¡ç®—æˆæœ¬æ˜æ˜¾å‡å° Bias-Variance Trade-Offï¼šKè¶Šå¤§ï¼Œtrain setè¶Šæ¥è¿‘å…¨é›†ï¼Œbiasè¶Šå°ï¼Œä½†æ˜¯æ¯ä¸ªtrain setä¹‹é—´ç›¸å…³æ€§è¶Šå¤§ï¼Œè€Œè¿™ç§å¤§ç›¸å…³æ€§ä¼šå¯¼è‡´æœ€ç»ˆçš„test errorå…·æœ‰æ›´å¤§çš„Variance åˆ†å‰² å®ä¾‹åˆ†å‰²&amp;è¯­ä¹‰åˆ†å‰² instance segmentationï¼šæ ‡è®°å®ä¾‹å’Œè¯­ä¹‰, ä¸ä»…è¦åˆ†å‰²å‡ºäººè¿™ä¸ªç±», è€Œä¸”è¦åˆ†å‰²å‡ºè¿™ä¸ªäººæ˜¯è°, ä¹Ÿå°±æ˜¯å…·ä½“çš„å®ä¾‹ semantic segmentationï¼šåªæ ‡è®°è¯­ä¹‰, ä¹Ÿå°±æ˜¯è¯´åªåˆ†å‰²å‡ºäººè¿™ä¸ªç±»æ¥]]></content>
      <tags>
        <tag>CV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Segmentation]]></title>
    <url>%2F2019%2F08%2F22%2FSegmentation%2F</url>
    <content type="text"><![CDATA[idea: CTå›¾ä¸€èˆ¬æ˜¯å•é€šé“ç°åº¦å›¾åƒï¼Œå‡å¦‚æˆ‘å°†128å¼ CTå›¾å †å åœ¨ä¸€èµ·ï¼ˆå³128é€šé“çš„å›¾åƒï¼‰ï¼Œç„¶åç”¨2Då·ç§¯ï¼ˆä¼šè€ƒè™‘é€šé“æ•°128ï¼‰ï¼Œè¿™æ ·å’Œç›´æ¥ç”¨3Då·ç§¯ä¼šæœ‰ç»“æœä¸Šçš„å·®åˆ«å—ï¼Ÿ 3dç½‘ç»œå¯ä»¥ç»“åˆå›¾åƒå±‚é—´ä¿¡æ¯ï¼Œèƒ½å¤Ÿä¿è¯éš”å±‚å›¾åƒMaskä¹‹é—´çš„ä¸€ä¸ªå˜åŒ–è¿ç»­æ€§ï¼Œæ•ˆæœä¼šæ¯”2då¥½ã€‚å±‚é—´è·å¤§çš„å›¾åƒï¼Œåœ¨é¢„å¤„ç†ä¸­ä¼šæœ‰æ’å€¼ã€‚ 3dç½‘ç»œå› ä¸ºæ˜¾å­˜çš„é™åˆ¶ï¼Œä¸€ç§å¤„ç†æ–¹å¼æ˜¯è£æˆ3d patchä½œä¸ºè¾“å…¥ï¼Œå¯¼è‡´å…¶æ„Ÿå—é‡æœ‰é™ï¼Œé€šå¸¸åªèƒ½ä¸“æ³¨äºç»†èŠ‚å’Œå±€éƒ¨ç‰¹å¾ï¼Œé€‚åˆä½œä¸ºç¬¬äºŒçº§ç½‘ç»œç”¨äºå¯¹ç»†èŠ‚åšç²¾ä¼˜åŒ–ã€‚ä¸€ç§å¤„ç†æ–¹å¼æ˜¯é™é‡‡æ ·ï¼Œåˆ†å‰²ç²¾åº¦ä¸‹é™ã€‚ 2.5Dç½‘ç»œã€‚]]></content>
  </entry>
  <entry>
    <title><![CDATA[keras note]]></title>
    <url>%2F2019%2F08%2F14%2Fkeras-note%2F</url>
    <content type="text"><![CDATA[1. keras Lambdaè‡ªå®šä¹‰å±‚å®˜æ–¹æ–‡æ¡£ï¼šå°†ä»»æ„è¡¨è¾¾å¼(function)å°è£…ä¸º Layer å¯¹è±¡ã€‚1keras.layers.Lambda(function, output_shape=None, mask=None, arguments=None) function: éœ€è¦å°è£…çš„å‡½æ•°ã€‚ å°†è¾“å…¥å¼ é‡ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°ã€‚ output_shape: é¢„æœŸçš„å‡½æ•°è¾“å‡ºå°ºå¯¸ã€‚(ä½¿ç”¨ TensorFlow æ—¶ï¼Œå¯è‡ªåŠ¨æ¨ç†å¾—åˆ°) arguments: å¯é€‰çš„éœ€è¦ä¼ é€’ç»™å‡½æ•°çš„å…³é”®å­—å‚æ•°ã€‚ä»¥å­—å…¸å½¢å¼ä¼ å…¥ã€‚ å‡ ä¸ªæ —å­ï¼š 1.1 æœ€ç®€ï¼šä½¿ç”¨åŒ¿åå‡½æ•°123model.add(Lambda(lambda x: x ** 2))x = Lambda(lambda image: K.image.resize_images(image, (target_size, target_size)))(inpt) å…¶ä¸­ï¼Œlambdaæ˜¯pythonçš„åŒ¿åå‡½æ•°ï¼Œåé¢çš„[xx: xxxx]ç”¨æ¥æè¿°å‡½æ•°çš„è¡¨è¾¾å½¢å¼ï¼Œlambda xx: xxxxæ•´ä½“ä½œä¸ºLambdaå‡½æ•°çš„functionå‚æ•°ã€‚ 1.2 ä¸­çº§ï¼šé€šè¿‡å­—å…¸ä¼ å‚ï¼Œå°è£…è‡ªå®šä¹‰å‡½æ•°ï¼Œå®ç°æ•°æ®åˆ‡ç‰‡123456789101112131415from keras.layers import Input, Lambda, Dense, Activation, Reshape, concatenatefrom keras.utils import plot_modelfrom keras.models import Modeldef slice(x, index): return x[:, :, index]a = Input(shape=(4,2))x1 = Lambda(slice,output_shape=(4,1),arguments=&#123;'index':0&#125;)(a)x2 = Lambda(slice,output_shape=(4,1),arguments=&#123;'index':1&#125;)(a)x1 = Reshape((4,1,1))(x1)x2 = Reshape((4,1,1))(x2)output = concatenate([x1,x2])model = Model(a, output)plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True) æ¨¡å‹ç»“æ„å›¾å¦‚ä¸‹ï¼š 1.3 é«˜çº§ï¼šè‡ªå®šä¹‰æŸå¤±å‡½æ•° step 1. æŠŠy_trueå®šä¹‰ä¸ºä¸€ä¸ªè¾“å…¥ step 2. æŠŠlosså†™æˆä¸€ä¸ªå±‚ï¼Œä½œä¸ºç½‘ç»œçš„æœ€ç»ˆè¾“å‡º step 3. åœ¨compileçš„æ—¶å€™ï¼Œå°†lossè®¾ç½®ä¸ºy_pred 123456789101112# yolov3 train.py create_model:model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss', arguments=&#123;'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5&#125;)( [*model_body.output, *y_true])model = Model([model_body.input, *y_true], model_loss)model.compile(optimizer=Adam(lr=1e-3), loss=&#123;'yolo_loss': lambda y_true, y_pred: y_pred&#125;)# yolov3 model.py yolo_lossdef yolo_loss(args, anchors, num_classes, ignore_thresh=.5, print_loss=False): ... return loss 2. keras è‡ªå®šä¹‰lossè¡¥å……1.3: ä¹Ÿå¯ä»¥ä¸å®šä¹‰ä¸ºç½‘ç»œå±‚ï¼Œç›´æ¥è°ƒç”¨è‡ªå®šä¹‰losså‡½æ•°å‚æ•°ï¼š y_true: çœŸå®æ ‡ç­¾ï¼ŒTheano/Tensorflow å¼ é‡ã€‚ y_pred: é¢„æµ‹å€¼ã€‚å’Œ y_true ç›¸åŒå°ºå¯¸çš„ Theano/TensorFlow å¼ é‡ã€‚1234def mycrossentropy(y_true, y_pred, e=0.1): return (1-e)*K.categorical_crossentropy(y_pred,y_true) + e*K.categorical_crossentropy(y_pred, K.ones_like(y_pred)/num_classes)model.compile(optimizer='rmsprop',loss=mycrossentropy, metrics=['accuracy']) å¸¦å‚æ•°çš„è‡ªå®šä¹‰lossï¼š æœ‰æ—¶å€™æˆ‘ä»¬è®¡ç®—lossçš„æ—¶å€™ä¸åªè¦ç”¨åˆ°y_trueå’Œy_predï¼Œè¿˜æƒ³å¼•å…¥ä¸€äº›å‚æ•°ï¼Œä½†æ˜¯kerasé™å®šæ„é€ losså‡½æ•°æ—¶åªèƒ½æ¥æ”¶(y_true, y_pred)ä¸¤ä¸ªå‚æ•°ï¼Œå¦‚ä½•ä¼˜é›…çš„ä¼ å…¥å‚æ•°ï¼Ÿ ä¼˜é›…çš„è§£å†³æ–¹æ¡ˆå¦‚ä¸‹ï¼š 1234567891011# build modelmodel = my_model()# define loss funcmodel_loss = dice_loss(smooth=1e-5, thresh=0.5)model.compile(loss=model_loss)# å®ç°def dice_loss(smooth, thresh): def dice(y_true, y_pred): return 1-dice_coef(y_true, y_pred, smooth, thresh) return dice 3. keras è‡ªå®šä¹‰metricsmodel.compileé‡Œé¢é™¤äº†lossè¿˜æœ‰ä¸€ä¸ªmetricsï¼Œç”¨äºæ¨¡å‹æ€§èƒ½è¯„ä¼°å‚æ•°ï¼š y_true: çœŸå®æ ‡ç­¾ï¼ŒTheano/Tensorflow å¼ é‡ã€‚ y_pred: é¢„æµ‹å€¼ã€‚å’Œ y_true ç›¸åŒå°ºå¯¸çš„ Theano/TensorFlow å¼ é‡ã€‚12345678910111213141516def precision(y_true, y_pred): # Calculates the precision true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1))) precision = true_positives / (predicted_positives + K.epsilon()) return precisiondef recall(y_true, y_pred): # Calculates the recall true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) possible_positives = K.sum(K.round(K.clip(y_true, 0, 1))) recall = true_positives / (possible_positives + K.epsilon()) return recallmodel.compile(optimizer='rmsprop',loss=mycrossentropy, metrics=['accuracy', recall, precision]) 4. keras è‡ªå®šä¹‰Layeræºä»£ç ï¼šhttps://github.com/keras-team/keras/blob/master/keras/engine/base_layer.py è‡ªå®šä¹‰layerç»§æ‰¿kerasçš„Layerç±»ï¼Œéœ€è¦å®ç°ä¸‰ä¸ªæ–¹æ³•ï¼š build(input_shape)ï¼šå®šä¹‰æƒé‡ï¼Œè°ƒç”¨add_weightæ¥åˆ›å»ºå±‚çš„æƒé‡çŸ©é˜µï¼Œå…¶ä¸­æœ‰å‚æ•°trainableå£°æ˜è¯¥å‚æ•°çš„æƒé‡æ˜¯å¦ä¸ºå¯è®­ç»ƒæƒé‡ï¼Œè‹¥trainable==Trueï¼Œä¼šæ‰§è¡Œself._trainable_weights.append(weight)å°†è¯¥æƒé‡åŠ å…¥åˆ°å¯è®­ç»ƒæƒé‡çš„lstä¸­ã€‚ call(x)ï¼šç¼–å†™å±‚é€»è¾‘ compute_output_shape(input_shape)ï¼šå®šä¹‰å¼ é‡å½¢çŠ¶çš„å˜åŒ–é€»è¾‘ get_configï¼šè¿”å›ä¸€ä¸ªå­—å…¸ï¼Œè·å–å½“å‰å±‚çš„å‚æ•°ä¿¡æ¯ çœ‹äº†kerasä¸€äº›å±‚çš„å®ç°ï¼Œkerasä¸­å±‚ï¼ˆå¦‚convã€depthwise convï¼‰çš„callå‡½æ•°åŸºæœ¬éƒ½æ˜¯é€šè¿‡è°ƒç”¨tf.backendä¸­çš„æ–¹æ³•æ¥å®ç° 4.1 æ —å­ï¼šCenterLossLayer 123456789101112131415161718192021222324252627282930class CenterLossLayer(Layer): def __init__(self, alpha=0.5, **kwargs): # alphaï¼šcenter updateçš„å­¦ä¹ ç‡ super().__init__(**kwargs) self.alpha = alpha def build(self, input_shape): # add_weightï¼šä¸ºè¯¥å±‚åˆ›å»ºä¸€ä¸ªå¯è®­ç»ƒï¼ä¸å¯è®­ç»ƒçš„æƒé‡ self.centers = self.add_weight(name='centers', shape=(10, 2), initializer='uniform', trainable=False) # ä¸€å®šè¦åœ¨æœ€åè°ƒç”¨å®ƒ super().build(input_shape) def call(self, x, mask=None): # x[0] is Nx2, x[1] is Nx10 onehot, self.centers is 10x2 delta_centers = K.dot(K.transpose(x[1]), (K.dot(x[1], self.centers) - x[0])) # 10x2 center_counts = K.sum(K.transpose(x[1]), axis=1, keepdims=True) + 1 # 10x1 delta_centers /= center_counts new_centers = self.centers - self.alpha * delta_centers # add_updateï¼šæ›´æ–°å±‚å†…å‚æ•°(buildä¸­å®šä¹‰çš„å‚æ•°)çš„æ–¹æ³• self.add_update((self.centers, new_centers), x) self.result = x[0] - K.dot(x[1], self.centers) self.result = K.sum(self.result ** 2, axis=1, keepdims=True) #/ K.dot(x[1], center_counts) return self.result # Nx1 def compute_output_shape(self, input_shape): return K.int_shape(self.result) æœ‰ä¸€äº›è‡ªå®šä¹‰å±‚ï¼Œæœ‰æ—¶å€™ä¼šä¸å®ç°compute_output_shapeå’Œget_config åœ¨callæ–¹æ³•ä¸­ï¼Œè¾“å‡ºtensorå¦‚æœå‘ç”Ÿäº†shapeçš„å˜åŒ–ï¼Œkeras layeræ˜¯ä¸ä¼šè‡ªåŠ¨æ¨å¯¼å‡ºè¾“å‡ºshapeçš„ï¼Œæ‰€ä»¥è¦æ˜¾ç¤ºçš„è‡ªå®šä¹‰compute_output_shape ä¸ç®¡å®šä¸å®šä¹‰get_configæ–¹æ³•ï¼Œéƒ½å¯ä»¥ä½¿ç”¨load_weightsæ–¹æ³•åŠ è½½ä¿å­˜çš„æƒé‡ ä½†æ˜¯å¦‚æœè¦ä½¿ç”¨load_modelæ–¹æ³•è½½å…¥åŒ…å«è‡ªå®šä¹‰å±‚çš„modelï¼Œå¿…é¡»è¦æ˜¾ç¤ºè‡ªå®šä¹‰get_configæ–¹æ³•ï¼Œå¦åˆ™keras æ— æ³•è·çŸ¥ Linear çš„é…ç½®å‚æ•°ï¼ åœ¨ __init__ çš„æœ€ååŠ ä¸Š **kwargs å‚æ•°ï¼Œå¹¶ç”¨ **kwargs å‚æ•°åˆå§‹åŒ–çˆ¶ç±»ã€‚ å®ç°ä¸Šè¿°çš„ get_config æ–¹æ³•ï¼Œè¿”å›è‡ªå®šä¹‰çš„å‚æ•°é…ç½®å’Œé»˜è®¤çš„å‚æ•°é…ç½® 4.2 è¡¥å……1.3 &amp; 2: è‡ªå®šä¹‰æŸå¤±å‡½æ•°é™¤äº†å¯ä»¥ç”¨Lambdaå±‚ï¼Œä¹Ÿå¯ä»¥å®šä¹‰Layerå±‚ï¼Œè¿™æ˜¯ä¸ªæ²¡æœ‰æƒé‡çš„è‡ªå®šä¹‰Layerã€‚ 123456789101112131415161718# å®˜æ–¹ç¤ºä¾‹ï¼šCustom loss layerclass CustomVariationalLayer(Layer): def __init__(self, **kwargs): self.is_placeholder = True super(CustomVariationalLayer, self).__init__(**kwargs) def vae_loss(self, x, x_decoded_mean): xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)#Square Loss kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)# KL-Divergence Loss return K.mean(xent_loss + kl_loss) def call(self, inputs): x = inputs[0] x_decoded_mean = inputs[1] loss = self.vae_loss(x, x_decoded_mean) self.add_loss(loss, inputs=inputs) # We won't actually use the output. return x 4.3 è¡¥å…… callæ–¹æ³•çš„å®Œæ•´å‚æ•°ï¼šcall(self, inputs, args, *kwargs) å…¶ä¸­inputså°±æ˜¯å±‚è¾“å…¥ï¼Œtensor/tensors é™¤æ­¤ä¹‹å¤–è¿˜æœ‰ä¸¤ä¸ªreserved keyword argumentsï¼štraining&amp;maskï¼Œä¸€ä¸ªç”¨äºbn/dropoutè¿™ç§train/testè®¡ç®—æœ‰åŒºåˆ«çš„flagï¼Œä¸€ä¸ªç”¨äºRNNlayersçº¦æŸæ—¶åºç›¸å…³å…³ç³» argså’Œ*kwargsæ˜¯é¢„ç•™ä¸ºäº†ä»¥åæ‰©å±•æ›´å¤šè¾“å…¥å‚æ•°çš„ 5. keras Generatoræœ¬è´¨ä¸Šå°±æ˜¯pythonçš„ç”Ÿæˆå™¨ï¼Œæ¯æ¬¡è¿”å›ä¸€ä¸ªbatchçš„æ ·æœ¬åŠæ ‡ç­¾è‡ªå®šä¹‰generatorçš„æ—¶å€™è¦å†™æˆæ­»å¾ªç¯ï¼ˆwhile trueï¼‰ï¼Œå› ä¸ºmodel.fit_generator()åœ¨ä½¿ç”¨åœ¨ä¸ªå‡½æ•°çš„æ—¶å€™ï¼Œå¹¶ä¸ä¼šåœ¨æ¯ä¸€ä¸ªepochä¹‹åé‡æ–°è°ƒç”¨ï¼Œé‚£ä¹ˆå¦‚æœè¿™æ—¶å€™generatorè‡ªå·±ç»“æŸäº†å°±ä¼šæœ‰é—®é¢˜ã€‚æ —å­æ˜¯æˆ‘ä¸ºmixupå†™çš„generatorï¼šæ²¡æœ‰æ˜¾ç¤ºçš„while Trueæ˜¯å› ä¸ºåˆ›å»ºkerasè‡ªå¸¦çš„generatorçš„æ—¶å€™å·²ç»æ˜¯æ­»å¾ªç¯äº†ï¼ˆforæ°¸ä¸è·³å‡ºï¼‰123456789101112131415161718192021222324252627def Datagen_mixup(data_path, img_size, batch_size, is_train=True, mix_prop=0.8, alpha=1.0): if is_train: datagen = ImageDataGenerator() else: datagen = ImageDataGenerator() # using kerasåº“å‡½æ•° generator = datagen.flow_from_directory(data_path, target_size=(img_size, img_size), batch_size=batch_size, color_mode="grayscale", shuffle=True) for x,y in generator: # a batch of &lt;img, label&gt; if alpha &gt; 0: lam = np.random.beta(alpha, alpha) else: lam = 1 idx = [i for i in range(x.shape[0])] random.shuffle(idx) mixed_x = lam*x + (1-lam)*x[idx] mixed_y = lam*y + (1-lam)*y[idx] n_origin = int(batch_size * mix_prop) gen_x = np.vstack(x[:n_origin], mixed_x[:(batch_size-n_origin)]) gen_y = np.vstack(y[:n_origin], mixed_y[:(batch_size-n_origin)]) yield gen_x, gen_y ã€å¤šè¿›ç¨‹ã€‘fit_generatorä¸­æœ‰ä¸€ä¸ªå‚æ•°use_multiprocessingï¼Œé»˜è®¤è®¾ç½®ä¸ºfalseï¼Œå› ä¸ºâ€˜using a generator with use_multiprocessing=True and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequenceâ€™ classâ€™ å¦‚æœè®¾ç½®å¤šè¿›ç¨‹use_multiprocessingï¼Œä»£ç ä¼šæŠŠä½ çš„æ•°æ®å¤åˆ¶å‡ ä»½ï¼Œåˆ†ç»™ä¸åŒçš„workersè¿›è¡Œè¾“å…¥ï¼Œè¿™æ˜¾ç„¶ä¸æ˜¯æˆ‘ä»¬å¸Œæœ›çš„ï¼Œæˆ‘ä»¬å¸Œæœ›ä¸€ä»½æ•°æ®ç›´æ¥å¹³å‡åˆ†ç»™å¤šä¸ªworkerså¸®å¿™è¾“å…¥ï¼Œè¿™æ ·æ‰æ˜¯æœ€å¿«çš„ã€‚è€ŒSequenceæ•°æ®ç±»èƒ½å®Œç¾è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ keras.utils.Sequence()ï¼š æ¯ä¸€ä¸ª Sequence å¿…é¡»å®ç° __getitem__ å’Œ __len__ æ–¹æ³• __getitem__ æ–¹æ³•åº”è¯¥èŒƒå›´ä¸€ä¸ªå®Œæ•´çš„æ‰¹æ¬¡ å¦‚æœä½ æƒ³åœ¨è¿­ä»£ä¹‹é—´ä¿®æ”¹ä½ çš„æ•°æ®é›†ï¼Œä½ å¯ä»¥å®ç° on_epoch_endï¼ˆä¼šåœ¨æ¯ä¸ªè¿­ä»£ä¹‹é—´è¢«éšå¼è°ƒç”¨ï¼‰\ githubä¸Šæœ‰issueåæ˜ on_epoch_endä¸ä¼šæ²¡è°ƒç”¨ï¼Œè§£å†³æ–¹æ¡ˆï¼šåœ¨__len__æ–¹æ³•ä¸­æ˜¾ç¤ºè‡ªè¡Œè°ƒç”¨ ç›´æ¥çœ‹æ —å­ï¼š 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import kerasimport mathimport osimport cv2import numpy as npfrom keras.applications import ResNet50from keras.optimizers import SGDclass DataGenerator(keras.utils.Sequence): def __init__(self, data, batch_size=1, shuffle=True): self.batch_size = batch_size self.data = data self.indexes = np.arange(len(self.data)) self.shuffle = shuffle def __len__(self): # è®¡ç®—æ¯ä¸€ä¸ªepochçš„è¿­ä»£æ¬¡æ•° return math.ceil(len(self.data) / float(self.batch_size)) def __getitem__(self, index): # ç”Ÿæˆæ¯ä¸ªbatchæ•°æ® batch_indices = self.indexes[index*self.batch_size:(index+1)*self.batch_size] batch_data = [self.data[k] for k in batch_indices] x_batch, y_batch = self.data_generation(batch_data) return x_batch, y_batch def on_epoch_end(self): if self.shuffle == True: np.random.shuffle(self.indexes) def data_generation(self, batch_data): images = [] labels = [] # ç”Ÿæˆæ•°æ® for i, data in enumerate(batch_data): image = cv2.imread(data, 0) image = cv2.resize(image, dsize=(64,64), interpolation=cv2.INTER_LINEAR) if np.max(image)&gt;1: image = image / 255. image = np.expand_dims(image, axis=-1) images.append(image) if 'd0' in data: labels.append([1,0]) else: labels.append([0,1]) return np.array(images), np.array(labels)if __name__ == '__main__': # data data_dir = "/Users/amber/dataset/mnist" data_lst = [] for file in os.listdir(data_dir+"/d0")[:]: data_lst.append(os.path.join(data_dir, "d0", file)) for file in os.listdir(data_dir+"/d1")[:]: data_lst.append(os.path.join(data_dir, "d1", file)) training_generator = DataGenerator(data_lst, batch_size=128) # model model = ResNet50(input_shape=(64,64,1),weights=None, classes=2) model.compile(optimizer=SGD(1e-3), loss='categorical_crossentropy', metrics=['accuracy']) model.fit_generator(training_generator, epochs=50,max_queue_size=200,workers=2) ç»éªŒå€¼ï¼š workersï¼š2/3 max_queue_sizeï¼šé»˜è®¤10ï¼Œå…·ä½“åŸºäºGPUå¤„äºç©ºé—²çŠ¶æ€é€‚é‡è°ƒèŠ‚ ã€é™„åŠ ã€‘å®éªŒä¸­è¿˜å‘ç°ä¸€ä¸ªé—®é¢˜ï¼Œæœ€å¼€å§‹å®šä¹‰äº†ä¸€ä¸ªsequential modelï¼Œç„¶ååœ¨è°ƒç”¨fit_generatorä¸€ç›´æŠ¥é”™ï¼šmodel not compileï¼Œä½†æ˜¯æ˜¾ç„¶modelæ˜¯compileè¿‡äº†çš„ï¼Œç½‘ä¸ŠæŸ¥åˆ°çš„è§£é‡Šï¼šâ€˜Sequential model works with model.fit but not with model.fit_generatorâ€™ 6. å¤šGPUå¤šGPUè¿è¡Œåˆ†ä¸ºä¸¤ç§æƒ…å†µï¼š * æ•°æ®å¹¶è¡Œ * è®¾å¤‡å¹¶è¡Œ 6.1 æ•°æ®å¹¶è¡Œ æ•°æ®å¹¶è¡Œå°†ç›®æ ‡æ¨¡å‹åœ¨å¤šä¸ªGPUä¸Šå„å¤åˆ¶ä¸€ä»½ï¼Œä½¿ç”¨æ¯ä¸ªå¤åˆ¶å“å¤„ç†æ•°æ®é›†çš„ä¸åŒéƒ¨åˆ†ã€‚ ä¸€ä¸ªæ —å­ï¼šå†™tripleNetæ¨¡å‹æ—¶ï¼Œå–äº†batch=4ï¼Œæ€»å…±15ç±»ï¼Œé‚£ä¹ˆä¸‰å…ƒç»„æ€»å…±æœ‰$(4/2)^2*15=60$ä¸ªï¼Œè®­ç»ƒç”¨äº†224çš„å›¾åƒï¼Œå•å¼ GPUå†…å­˜ä¼šæº¢å‡ºï¼Œå› æ­¤éœ€è¦å•æœºå¤šå¡æ•°æ®å¹¶è¡Œã€‚ â€‹ step1. åœ¨æ¨¡å‹å®šä¹‰ä¸­ï¼Œç”¨multi_gpu_modelå°ä¸€å±‚ï¼Œéœ€è¦åœ¨model.compileä¹‹å‰ã€‚ 12345678910111213141516171819202122232425262728from keras.util import multi_gpu_modeldef triple_model(input_shape=(512,512,1), n_classes=10, multi_gpu=False): anchor_input = Input(shape=input_shape) positive_input = Input(shape=input_shape) negative_input = Input(shape=input_shape) sharedCNN = base_model(input_shape) encoded_anchor = sharedCNN(anchor_input) encoded_positive = sharedCNN(positive_input) encoded_negative = sharedCNN(negative_input) # class branch x = Dense(n_classses, activation='softmax')(encoded_anchor) # distance branch encoded_anchor = Activation('sigmoid')(encoded_anchor) encoded_positive = Activation('sigmoid')(encoded_positive) encoded_negative = Activation('sigmoid')(encoded_negative) merged = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=-1, name='tripleLossLayer') model = Model(inputs=[anchor_input,positive_input,negative_input], outputs=[x, merged]) if multi_gpu: model = multi_gpu_model(model, GPU_COUNT) model.compile(optimizer=SGD, loss=[cls_loss, triplet_loss], metrics=[cls_acc]) return model â€‹ step2. åœ¨å®šä¹‰checkpointæ—¶ï¼Œè¦ç”¨ParallelModelCheckpointå°ä¸€å±‚ï¼Œåˆå§‹åŒ–å‚æ•°çš„modelè¦ä¼ åŸå§‹æ¨¡å‹ã€‚ 1234567891011121314151617181920212223from keras.callbacks import ModelCheckpointclass ParallelModelCheckpoint(ModelCheckpoint): def __init__(self,single_model,multi_model, filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1): self.single_model = single_model self.multi_model = multi_model super(ParallelModelCheckpoint,self).__init__(filepath, monitor, verbose,save_best_only, save_weights_only,mode, period) def set_model(self, model): self.single_model.optimizer = self.multi_model.optimizer super(ParallelModelCheckpoint,self).set_model(self.single_model) def on_epoch_end(self, epoch, logs=None): # save optimizer weights self.single_model.optimizer = self.multi_model.optimizer super(ParallelModelCheckpoint, self).on_epoch_end(epoch, logs)model = triple_model(multi_gpu=True)single_model = triple_model(multi_gpu=False)filepath = "./tripleNet_&#123;epoch:02d&#125;_val_loss_&#123;val_loss:.3f&#125;.h5"check_point = ParallelModelCheckpoint(single_model, filepath) â€‹ step3. åœ¨ä¿å­˜æƒé‡æ—¶ï¼Œé€šè¿‡cpuæ¨¡å‹æ¥ä¿å­˜ã€‚ 1234567# å®ä¾‹åŒ–åŸºç¡€æ¨¡å‹ï¼Œè¿™æ ·å®šä¹‰æ¨¡å‹æƒé‡ä¼šå­˜å‚¨åœ¨CPUå†…å­˜ä¸­with tf.device('/cpu:0'): model = Resnet50(input_shape=(512,512,3), classes=4, weights=None) parallel_model = multi_gpu_model(model, GPU_COUNT)parallel_model.fit(x,y, epochs=20, batch_size=32)model.save('model.h5') â€‹ ã€attentionã€‘åŒç†ï¼Œåœ¨loadæƒé‡æ—¶ï¼Œä¹Ÿæ˜¯loadå•æ¨¡å‹çš„æƒé‡ï¼Œå†è°ƒç”¨multi_gpu_modelå°†æ¨¡å‹å¤åˆ¶åˆ°å¤šä¸ªGPUä¸Šï¼š 12345model = Model(inputs=[anchor_input,positive_input,negative_input], outputs=[x, merged])if multi_gpu: if os.path.exists(weight_pt): model.load_weights(weight_pt) model = multi_gpu_model(model, GPU_COUNT) ã€ATTENTIONã€‘å®éªŒä¸­å‘ç°ä¸€ä¸ªé—®é¢˜ï¼šåœ¨æœ‰äº›caseä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†è‡ªå®šä¹‰lossä½œä¸ºç½‘ç»œçš„è¾“å‡ºï¼Œæ­¤æ—¶ç½‘ç»œçš„è¾“å‡ºæ˜¯ä¸ªæ ‡é‡ï¼Œä½†æ˜¯åœ¨è°ƒç”¨multi_gpu_modelè¿™ä¸ªæ–¹æ³•æ—¶ï¼Œå…·ä½“å®ç°åœ¨multi_gpu_utils.pyä¸­ï¼Œæœ€åä¸€ä¸ªæ­¥éª¤è¦mergeå‡ ä¸ªdeviceçš„è¾“å‡ºï¼Œé€šè¿‡axis=0çš„concatå®ç°ï¼Œç½‘ç»œè¾“å‡ºæ˜¯æ ‡é‡çš„è¯å°±ä¼šæŠ¥é”™â€”â€”list assignment index out of rangeã€‚ å°è¯•çš„è§£å†³æ–¹æ¡ˆæ˜¯æ”¹æˆç›¸åŠ ï¼š 1234567# Merge outputs under expected scope.with tf.device('/cpu:0' if cpu_merge else '/gpu:%d' % target_gpu_ids[0]): merged = [] for name, outputs in zip(output_names, all_outputs): merged.append(Lambda(lambda x: K.sum(x))(outputs)) # merged.append(concatenate(outputs, axis=0, name=name)) return Model(model.inputs, merged) ã€ATTENTION++ã€‘ç½‘ç»œçš„è¾“å‡ºä¸èƒ½æ˜¯æ ‡é‡ï¼ï¼æ°¸è¿œä¼šéšè—ä¿ç•™ä¸€ä¸ªbatch dimï¼Œä¹‹å‰æ˜¯å†™é”™äº†ï¼ï¼ model lossæ˜¯ä¸€ä¸ªæ ‡é‡ ä½œä¸ºè¾“å‡ºå±‚çš„lossæ˜¯ä¿ç•™batch dimçš„ï¼ï¼ 6.2 è®¾å¤‡å¹¶è¡Œ è®¾å¤‡å¹¶è¡Œé€‚ç”¨äºå¤šåˆ†æ”¯ç»“æ„ï¼Œä¸€ä¸ªåˆ†æ”¯ç”¨ä¸€ä¸ªGPUã€‚é€šè¿‡ä½¿ç”¨TensorFlow device scopeså®ç°ã€‚ æ —å­ï¼š 12345678910111213141516# Model where a shared LSTM is used to encode two different sequences in parallelinput_a = keras.Input(shape=(140, 256))input_b = keras.Input(shape=(140, 256))shared_lstm = keras.layers.LSTM(64)# Process the first sequence on one GPUwith tf.device_scope('/gpu:0'): encoded_a = shared_lstm(tweet_a)# Process the next sequence on another GPUwith tf.device_scope('/gpu:1'): encoded_b = shared_lstm(tweet_b)# Concatenate results on CPUwith tf.device_scope('/cpu:0'): merged_vector = keras.layers.concatenate([encoded_a, encoded_b],axis=-1) 7. åº“å‡½æ•°è®²è§£7.1 BatchNormalization(axis=-1) ç”¨äºåœ¨æ¯ä¸ªbatchä¸Šå°†å‰ä¸€å±‚çš„æ¿€æ´»å€¼é‡æ–°è§„èŒƒåŒ–ï¼Œå³ä½¿å¾—å…¶è¾“å‡ºæ•°æ®çš„å‡å€¼æ¥è¿‘0ï¼Œå…¶æ ‡å‡†å·®æ¥è¿‘1ã€‚ å¸¸ç”¨å‚æ•°axisï¼šæŒ‡å®šè¦è§„èŒƒåŒ–çš„è½´ï¼Œé€šå¸¸ä¸ºç‰¹å¾è½´ï¼Œå¦‚åœ¨â€œchannels_firstâ€çš„data formatä¸‹ï¼Œaxis=1ï¼Œåä¹‹axis=-1ã€‚ 7.2 LSTM å‚æ•°ï¼š unitsï¼šè¾“å‡ºç»´åº¦ï¼ˆæœ€åä¸€ç»´ï¼‰ï¼Œæ ‡å‡†è¾“å…¥NxTxDï¼ŒN for batchï¼ŒT for time-stepï¼ŒD for vector-dimensionã€‚ activationï¼šæ¿€æ´»å‡½æ•° recurrent_activationï¼šç”¨äºå¾ªç¯æ—¶é—´æ­¥çš„æ¿€æ´»å‡½æ•° dropoutï¼šåœ¨ 0 å’Œ 1 ä¹‹é—´çš„æµ®ç‚¹æ•°ã€‚ å•å…ƒçš„ä¸¢å¼ƒæ¯”ä¾‹ï¼Œç”¨äºè¾“å…¥çš„çº¿æ€§è½¬æ¢ recurrent_dropoutï¼šåœ¨ 0 å’Œ 1 ä¹‹é—´çš„æµ®ç‚¹æ•°ã€‚ å•å…ƒçš„ä¸¢å¼ƒæ¯”ä¾‹ï¼Œç”¨äºå¾ªç¯å±‚çŠ¶æ€çš„çº¿æ€§è½¬æ¢ return_sequences: å¸ƒå°”å€¼ï¼Œé»˜è®¤Falseã€‚æ˜¯è¿”å›è¾“å‡ºåºåˆ—ä¸­çš„æœ€åä¸€ä¸ªè¾“å‡ºï¼Œè¿˜æ˜¯å…¨éƒ¨åºåˆ—çš„è¾“å‡ºã€‚å³many-to-oneè¿˜æ˜¯many-to-manyï¼Œç®€å•æ¥è®²ï¼Œå½“æˆ‘ä»¬éœ€è¦æ—¶åºè¾“å‡ºï¼ˆmany-to-manyï¼‰çš„æ—¶å€™ï¼Œå°±set Trueã€‚ return_state: å¸ƒå°”å€¼ï¼Œé»˜è®¤Falseã€‚é™¤äº†è¾“å‡ºä¹‹å¤–æ˜¯å¦è¿”å›æœ€åä¸€ä¸ªçŠ¶æ€ï¼ˆcellå€¼ï¼‰ 1234567891011121314151617181920# return_sequencesinputs1 = Input(tensor=(1ï¼Œ3, 1))lstm1 = LSTM(1, return_sequences=True)(inputs1)'''è¾“å‡ºç»“æœä¸º[[[-0.02243521][-0.06210149][-0.11457888]]]è¡¨ç¤ºæ¯ä¸ªtime-stepï¼ŒLSTM cellçš„è¾“å‡º'''# return_statelstm1, state_h, state_c = LSTM(1, return_state=True)(inputs1)'''è¾“å‡ºç»“æœä¸º[array([[ 0.10951342]], dtype=float32), array([[ 0.10951342]], dtype=float32), array([[ 0.24143776]], dtype=float32)] listä¸­ä¾æ¬¡ä¸ºç½‘ç»œè¾“å‡ºï¼Œæœ€åä¸€ä¸ªtime-stepçš„LSTM cellçš„è¾“å‡ºå€¼å’Œcellå€¼''' 7.2.5 TimeDistributed é¡ºä¾¿å†è¯´ä¸‹TimeDistributedï¼Œå½“æˆ‘ä»¬ä½¿ç”¨many-to-manyæ¨¡å‹ï¼Œæœ€åä¸€å±‚LSTMçš„è¾“å‡ºç»´åº¦ä¸ºkï¼Œè€Œæˆ‘ä»¬æƒ³è¦çš„æœ€ç»ˆè¾“å‡ºç»´åº¦ä¸ºnï¼Œé‚£ä¹ˆå°±éœ€è¦å¼•å…¥Denseå±‚ï¼Œå¯¹äºæ—¶åºæ¨¡å‹ï¼Œæˆ‘ä»¬è¦å¯¹æ¯ä¸€ä¸ªtime-stepå¼•å…¥denseå±‚ï¼Œè¿™å®è´¨ä¸Šæ˜¯å¤šä¸ªDenseæ“ä½œï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥ç”¨TimeDistributedæ¥åŒ…è£¹Denseå±‚æ¥å®ç°ã€‚ 123model = Sequential()model.add(LSTM(3, input_shape=(length, 1), return_sequences=True))model.add(TimeDistributed(Dense(1))) å®˜æ–¹æ–‡æ¡£ï¼šè¿™ä¸ªå°è£…å™¨å°†ä¸€ä¸ªå±‚åº”ç”¨äºè¾“å…¥çš„æ¯ä¸ªæ—¶é—´ç‰‡ã€‚ å½“è¯¥å±‚ä½œä¸ºç¬¬ä¸€å±‚æ—¶ï¼Œåº”æ˜¾å¼è¯´æ˜input_shape TimeDistributedå¯ä»¥åº”ç”¨äºä»»æ„å±‚ï¼Œå¦‚Conv3Dï¼š 1234567891011121314151617# ä¾‹å¦‚æˆ‘çš„crnn modeldef crnn(input_shape, cnn, n_classes=24): inpt = Input(input_shape) x = TimeDistributed(cnn, input_shape=input_shape)(inpt) x = LSTM(128, return_sequences=True)(x) x = LSTM(256, return_sequences=True)(x) x = TimeDistributed(Dense(n_classes))(x) model = Model(inpt, x) model.summary() return model crnn_model = crnn((24,128,128,128,2), cnn_model) 7.3 Embedding ç”¨äºå°†ç¨€ç–ç¼–ç æ˜ å°„ä¸ºå›ºå®šå°ºå¯¸çš„å¯†é›†è¡¨ç¤ºã€‚ è¾“å…¥å½¢å¦‚ï¼ˆsamplesï¼Œsequence_lengthï¼‰çš„2Då¼ é‡ï¼Œè¾“å‡ºå½¢å¦‚(samples, sequence_length, output_dim)çš„3Då¼ é‡ã€‚ å‚æ•°ï¼š input_dimï¼šå­—å…¸é•¿åº¦ï¼Œå³è¾“å…¥æ•°æ®æœ€å¤§ä¸‹æ ‡+1 output_dimï¼š input_lengthï¼š æ —å­ï¼š 123456789101112# centerloss branchlambda_c = 1input_ = Input(shape=(1,))centers = Embedding(10,2)(input_) # (None, 1, 2)# è¿™é‡Œçš„è¾“å…¥æ˜¯0-9çš„æšä¸¾ï¼ˆdim=10ï¼‰ï¼Œç„¶åæ˜ å°„æˆä¸€ä¸ªç°‡å¿ƒintra_loss = Lambda(lambda x:K.sum(K.square(x[0]-x[1][:,0]),1,keepdims=True))([out1,centers])model_center_loss = Model([inputs,input_],[out2,intra_loss])model_center_loss.compile(optimizer="sgd", loss=["categorical_crossentropy",lambda y_true,y_pred:y_pred], loss_weights=[1,lambda_c/2.], metrics=["acc"])model_center_loss.summary() 7.4 plot_model 12from keras.utils import plot_modelplot_model(model, to_file=&apos;model.png&apos;, show_shapes=False, show_layer_names=True) 7.5 K.function è·å–æ¨¡å‹æŸå±‚çš„è¾“å‡ºï¼Œä¸€ç§æ–¹æ³•æ˜¯åˆ›å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹ï¼Œä½¿å®ƒçš„è¾“å‡ºæ˜¯ç›®æ ‡å±‚ï¼Œç„¶åè°ƒç”¨predictã€‚ 123456model = ... # the original modelnew_model = Model(input=model.input, output=model.get_layer('my_layer').output)intermediate_output = new_model.predict(input_dataï¼‰ ä¹Ÿå¯ä»¥åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥å®ç°ï¼škeras.backend.function(inputs, outputs, updates=None) 123456# è¿™æ˜¯å†™center-lossæ—¶å†™çš„æ —å­ï¼šfunc = K.function(inputs=[model.input[0]], outputs=[model.get_layer('out1').output]) # model.input[0]: one input of the multi-input modeltest_features = func([x_test])[0] 7.6 K.gradients(y,x) æ±‚yå…³äºxçš„å¯¼æ•°ï¼Œyå’Œxå¯ä»¥æ˜¯å¼ é‡ï¼å¼ é‡åˆ—è¡¨ã€‚è¿”å›å¼ é‡åˆ—è¡¨ï¼Œåˆ—è¡¨é•¿åº¦åŒxåˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­å…ƒç´ shapeåŒxåˆ—è¡¨ä¸­å…ƒç´ ã€‚ å¯¹äº$y=[y_1, y_2], x=[x_1, x_2, x_3]$ï¼Œæœ‰è¿”å›å€¼$[grad_1, grad_2, grad_3]$ï¼ŒçœŸå®çš„è®¡ç®—è¿‡ç¨‹ä¸ºï¼š grad_1 = \frac{\partial y_1}{\partial x_1} + \frac{\partial y_2}{\partial x_1} \\ grad_2 = \frac{\partial y_1}{\partial x_2} + \frac{\partial y_2}{\partial x_2} \\ grad_3 = \frac{\partial y_1}{\partial x_3} + \frac{\partial y_2}{\partial x_3}7.7 ModelCheckpointã€ReduceLROnPlateauã€EarlyStoppingã€LearningRateSchedulerã€Tensorboard æ¨¡å‹æ£€æŸ¥ç‚¹ModelCheckpoint 12ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1) filepathå¯ä»¥ç”± epoch çš„å€¼å’Œ logs çš„é”®æ¥å¡«å……ï¼Œå¦‚weights.{epoch:02d}-{val_loss:.2f}.hdf5ã€‚ moniterï¼šè¢«ç›‘æµ‹çš„æ•°æ® modeï¼šåœ¨ auto æ¨¡å¼ä¸­ï¼Œæ–¹å‘ä¼šè‡ªåŠ¨ä»è¢«ç›‘æµ‹çš„æ•°æ®çš„åå­—(ä¸é è°±ğŸ¤·â€â™€ï¸)ä¸­åˆ¤æ–­å‡ºæ¥ã€‚ å­¦ä¹ ç‡è¡°å‡ReduceLROnPlateau å­¦ä¹ ç‡çš„æ–¹æ¡ˆç›¸å¯¹ç®€å•ï¼Œè¦ä¹ˆåœ¨éªŒè¯é›†çš„æŸå¤±æˆ–å‡†ç¡®ç‡å¼€å§‹ç¨³å®šæ—¶è°ƒä½å­¦ä¹ ç‡ï¼Œè¦ä¹ˆåœ¨å›ºå®šé—´éš”ä¸Šè°ƒä½å­¦ä¹ ç‡ã€‚ 1ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0) å½“å­¦ä¹ åœæ­¢æ—¶ï¼Œæ¨¡å‹æ€»æ˜¯ä¼šå—ç›Šäºé™ä½ 2-10 å€çš„å­¦ä¹ é€Ÿç‡ã€‚ moniterï¼šè¢«ç›‘æµ‹çš„æ•°æ® factorï¼šæ–°çš„å­¦ä¹ é€Ÿç‡ = å­¦ä¹ é€Ÿç‡ * factor patienceï¼šè¢«ç›‘æµ‹æ•°æ®æ²¡æœ‰è¿›æ­¥çš„è®­ç»ƒè½®æ•°ï¼Œåœ¨è¿™ä¹‹åè®­ç»ƒé€Ÿç‡ä¼šè¢«é™ä½ã€‚ æ›´å¤æ‚çš„å­¦ä¹ ç‡å˜åŒ–æ¨¡å¼å®šä¹‰LearningRateScheduler å‰ææ˜¯åªéœ€è¦ç”¨åˆ°é»˜è®¤å‚æ•°æ˜¯epoch 12345678910111213141516# é¦–å…ˆå®šä¹‰ä¸€ä¸ªå˜åŒ–æ¨¡å¼def warmup_scheduler(epoch, mode='power_decay'): lr_base = 1e-5 lr_stable = 1e-4 lr = lr_base * math.pow(10, epoch) if lr&gt;lr_stable: return lr_stable else: return lr # ç„¶åè°ƒç”¨LearningRateScheduleræ–¹æ³•wrapperè¿™ä¸ªschedulerscheduler = LearningRateScheduler(warmup_scheduler)# åœ¨ä½¿ç”¨çš„æ—¶å€™æ”¾åœ¨callbacksçš„listé‡Œé¢ï¼Œåœ¨æ¯ä¸ªepochç»“æŸè§¦å‘callbacks = [checkpoint, reduce_lr, scheduler, early_stopping] æ›´æ›´å¤æ‚çš„å­¦ä¹ ç‡å˜åŒ–æ¨¡å¼å®šä¹‰å¯ä»¥ç›´æ¥ç»§æ‰¿Callbackï¼šhttps://kexue.fm/archives/5765 å½“æˆ‘ä»¬éœ€è¦ä¼ å…¥æ›´ä¸°å¯Œçš„è‡ªå®šä¹‰å‚æ•°/éœ€è¦è¿›è¡Œby stepçš„å‚æ•°æ›´æ–°ç­‰ï¼Œå¯ä»¥ç›´æ¥ç»§æ‰¿Callbackï¼Œè¿›è¡Œæ›´è‡ªç”±çš„è‡ªå®šä¹‰ 123456789101112131415161718192021222324252627282930313233# ä»¥ä½™å¼¦é€€ç«ç®—æ³•ä¸ºä¾‹ï¼šclass CosineAnnealingScheduler(Callback): """Cosine annealing scheduler. """ def __init__(self, epochs, scale=1.6, shift=0, verbose=0): super(CosineAnnealingScheduler, self).__init__() self.epochs = epochs self.scale = scale self.shift = shift self.verbose = verbose def on_epoch_begin(self, epoch, logs=None): if epoch&lt;=6: # linearly increase from 0 to 1.6 in first 5 epochs lr = 1.6 / 5 * (epoch+1) else: # cosine annealing lr = self.shift + self.scale * (1 + math.cos(math.pi * (epoch+1-5) / self.epochs)) / 2 K.set_value(self.model.optimizer.lr, lr) if self.verbose &gt; 0: print('\nEpoch %05d: CosineAnnealingScheduler setting learning rate to %s.' % (epoch+1, lr)) def on_epoch_end(self, epoch, logs=None): logs = logs or &#123;&#125; logs['lr'] = K.get_value(self.model.optimizer.lr) # è°ƒç”¨lrscheduler = CosineAnnealingScheduler(epochs=2, verbose=1)callbacks = [checkpoint, lrscheduler]model.fit(..., callbacks=callbacks) æœ‰ä¸€äº›è®¡ç®—æŒ‡æ ‡ï¼Œä¸å¥½å†™æˆå¼ é‡å½¢å¼ï¼Œä¹Ÿå¯ä»¥æ”¾åœ¨Callbackå™¨é‡Œé¢ï¼Œæƒ³å’‹å†™å°±å’‹å†™ è¯´ç™½äº†å°±æ˜¯on_epoch_endé‡Œé¢çš„æ•°æ®æ˜¯arrayï¼Œè€Œä¸æ˜¯tensorï¼Œæ¯”è¾ƒå¥½å†™ 12345678910111213141516171819202122232425262728from keras.callbacks import Callback# å®šä¹‰Callbackå™¨ï¼Œè®¡ç®—éªŒè¯é›†çš„accï¼Œå¹¶ä¿å­˜æœ€ä¼˜æ¨¡å‹class Evaluate(Callback): def __init__(self): self.accs = [] self.highest = 0. def on_epoch_end(self, epoch, logs=None): ###### è‡ªç”±å‘æŒ¥åŒºåŸŸ pred = model.predict(x_test) acc = np.mean(pred.argmax(axis=1) == y_test) ######## self.accs.append(acc) if acc &gt;= self.highest: # ä¿å­˜æœ€ä¼˜æ¨¡å‹æƒé‡ self.highest = acc model.save_weights('best_model.weights') print('acc: %s, highest: %s' % (acc, self.highest))evaluator = Evaluate()model.fit(x_train, y_train, epochs=10, callbacks=[evaluator]) Callbackç±»å…±æ”¯æŒå…­ç§åœ¨ä¸åŒé˜¶æ®µçš„æ‰§è¡Œå‡½æ•°ï¼š on_epoch_beginï¼šwarmup on_epoch_endï¼šmetrics on_batch_begin on_batch_end on_train_begin on_train_end æå‰åœæ­¢è®­ç»ƒEarlyStopping 1EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False) moniterï¼šè¢«ç›‘æµ‹çš„æ•°æ® patienceï¼šè¢«ç›‘æµ‹æ•°æ®æ²¡æœ‰è¿›æ­¥çš„è®­ç»ƒè½®æ•°ï¼Œåœ¨è¿™ä¹‹åè®­ç»ƒé€Ÿç‡ä¼šè¢«é™ä½ã€‚ min_deltaï¼šåœ¨è¢«ç›‘æµ‹çš„æ•°æ®ä¸­è¢«è®¤ä¸ºæ˜¯æå‡çš„æœ€å°å˜åŒ–ï¼Œå°äº min_delta çš„ç»å¯¹å˜åŒ–ä¼šè¢«è®¤ä¸ºæ²¡æœ‰æå‡ã€‚ baseline: è¦ç›‘æ§çš„æ•°é‡çš„åŸºå‡†å€¼ã€‚ ä»¥ä¸Šè¿™å››ä¸ªéƒ½æ˜¯ç»§æ‰¿è‡ªkeras.callbacks() å¯è§†åŒ–å·¥å…·TensorBoard è¿™ä¸ªå›è°ƒå‡½æ•°ä¸º Tensorboard ç¼–å†™ä¸€ä¸ªæ—¥å¿—ï¼Œ è¿™æ ·ä½ å¯ä»¥å¯è§†åŒ–æµ‹è¯•å’Œè®­ç»ƒçš„æ ‡å‡†è¯„ä¼°çš„åŠ¨æ€å›¾åƒï¼Œ ä¹Ÿå¯ä»¥å¯è§†åŒ–æ¨¡å‹ä¸­ä¸åŒå±‚çš„æ¿€æ´»å€¼ç›´æ–¹å›¾ã€‚ 1TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch') å®é™…ä½¿ç”¨æ—¶å…³æ³¨ç¬¬ä¸€ä¸ªå‚æ•°log_dirå°±å¥½ï¼ŒæŸ¥çœ‹æ—¶é€šè¿‡å‘½ä»¤è¡Œå¯åŠ¨ï¼š 1tensorboard --logdir=/full_path_to_your_logs è¿™å‡ ä¸ªå›è°ƒå‡½æ•°ï¼Œé€šé€šåœ¨è®­ç»ƒæ—¶ï¼ˆmodel.fit / fit_generatorï¼‰æ”¾åœ¨callbackså…³é”®å­—é‡Œé¢ã€‚ 7.8 åå·ç§¯ Conv2DTranspose ä¸‰ä¸ªæ ¸å¿ƒçš„å‚æ•°filtesã€kernel_sizeã€stridesã€padding=â€™validâ€™ filtesï¼šè¾“å‡ºé€šé“æ•° stridesï¼šæ­¥é•¿ kernel_sizeï¼šä¸€èˆ¬éœ€è¦é€šè¿‡ä¸Šé¢ä¸¤é¡¹è®¡ç®—å¾—åˆ° åå·ç§¯è¿ç®—å’Œæ­£å‘å·ç§¯è¿ç®—ä¿æŒä¸€è‡´ï¼Œå³ï¼š (output\_shape - kernel\_size) / stride + 1 = input\_shape1234# fcn example: current feature map x (,32,32,32), input_shape (512,512,2), output_shape (,512,512,1)strides = 2kernel_size = input_shape[0] - (x.get_shape().as_list()[1] - 1)*stridesy = Conv2DTranspose(1, kernel_size, padding='valid', strides=strides) 7.9 K.shape &amp; K.int_shape &amp; tensor._keras_shape tensor._keras_shapeç­‰ä»·äºK.int_shapeï¼šå¼ é‡çš„shapeï¼Œè¿”å›å€¼æ˜¯ä¸ªtuple K.shapeï¼šè¿”å›å€¼æ˜¯ä¸ªtensorï¼Œtensoræ˜¯ä¸ªä¸€ç»´å‘é‡ï¼Œå…¶ä¸­æ¯ä¸€ä¸ªå…ƒç´ å¯ä»¥ç”¨[i]æ¥è®¿é—®ï¼Œæ˜¯ä¸ªæ ‡é‡tensor ä¸¤ä¸ªæ–¹æ³•çš„ä¸»è¦åŒºåˆ«æ˜¯ï¼šå‰è€…è¿”å›å€¼æ˜¯ä¸ªå¸¸é‡ï¼Œåªèƒ½è¡¨å¾è¯­å¥æ‰§è¡Œæ—¶åˆ»ï¼ˆå¦‚æ„å»ºå›¾ï¼‰tensorçš„çŠ¶æ€ï¼Œåè€…è¿”å›å€¼æ˜¯ä¸ªå˜é‡ï¼Œwrapperçš„æ–¹æ³•å¯ä»¥çœ‹æˆä¸€ä¸ªèŠ‚ç‚¹ï¼Œåœ¨graphçš„ä½œç”¨åŸŸå†…å§‹ç»ˆæœ‰æ•ˆï¼Œåœ¨æ„å»ºå›¾çš„æ—¶å€™å¯ä»¥æ˜¯Noneï¼Œåœ¨å®é™…æµå…¥æ•°æ®æµçš„æ—¶å€™ä¼ å€¼å°±è¡Œï¼Œå¦‚batch_sizeï¼ï¼ï¼ 1234567891011import keras.backend as Kfrom keras.layers import Inputx = Input((22,22,1))print(K.shape(x))# Tensor("Shape:0", shape=(4,), dtype=int32)print(K.shape(x)[0])# Tensor("strided_slice:0", shape=(), dtype=int32)print(K.int_shape(x))# (None, 22, 22, 1) 7.10 binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0) from_logitsï¼šlogitsè¡¨ç¤ºç½‘ç»œçš„ç›´æ¥è¾“å‡ºâ€”â€”æ²¡ç»è¿‡sigmoidæˆ–è€…softmaxçš„æ¦‚ç‡åŒ–ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è®¤ä¸ºy_predæ˜¯å·²ç»å¤„ç†è¿‡çš„æ¦‚ç‡åˆ†å¸ƒ 8. è¡ç”Ÿï¼šä¸€äº›tfå‡½æ•°8.1 tf.where(condition, x=None, y=None,name=None) ä¸¤ç§ç”¨æ³•ï¼š å¦‚æœxï¼Œyä¸ºç©ºï¼Œè¿”å›å€¼æ˜¯æ»¡è¶³conditionå…ƒç´ çš„ç´¢å¼•ï¼Œæ¯ä¸ªç´¢å¼•å ä¸€è¡Œã€‚ å¦‚æœxï¼Œyä¸ä¸ºç©ºï¼Œé‚£ä¹ˆconditionã€xã€y å’Œè¿”å›å€¼ç›¸åŒç»´åº¦ï¼Œconditionä¸ºTrueçš„ä½ç½®æ›¿æ¢xä¸­å¯¹åº”å…ƒç´ ï¼Œconditionä¸ºFalseçš„ä½ç½®æ›¿æ¢yä¸­å¯¹åº”å…ƒç´ ã€‚ å…³äºç´¢å¼•indicesï¼š conditionçš„shapeçš„dimï¼Œå°±æ˜¯æ¯ä¸€è¡Œç´¢å¼•vectorçš„shapeï¼Œä¾‹ï¼š 1234567891011import tensorflow as tfimport numpy as npcondition1 = np.array([[True,False,False],[False,True,True]])print(condition1.shape) # (2,3)with tf.compat.v1.Session() as sess: print(sess.run(tf.where(condition1)))# [[0 0]# [1 1]# [1 2]]# conditionæ˜¯2x3çš„arrï¼Œä¹Ÿå°±æ˜¯dim=2ï¼Œé‚£ä¹ˆç´¢å¼•vectorçš„shapeå°±æ˜¯2ï¼Œçºµè½´çš„shapeæ˜¯æ»¡è¶³condçš„æ•°é‡ ç´¢å¼•é€šå¸¸ä¸tf.gatherå’Œtf.gather_ndæ­é…ä½¿ç”¨ï¼š tf.gather(params,indices,axis=0.name=None)ï¼štf.gatheråªèƒ½æ¥å—1-Dçš„ç´¢å¼•ï¼Œaxisç”¨æ¥æŒ‡å®šè½´ï¼Œä¸€ä¸ªç´¢å¼•å–å›å¯¹åº”ç»´åº¦çš„ä¸€ä¸ªå‘é‡ tf.gather_nd(params,indices)ï¼štf.gather_ndå¯ä»¥æ¥å—å¤šç»´çš„ç´¢å¼•ï¼Œå¦‚æœç´¢å¼•çš„dimå°äºparamsçš„dimï¼Œåˆ™ä»axis=0å¼€å§‹ç´¢å¼•ï¼Œåé¢çš„å–å…¨éƒ¨ã€‚ 8.2 tf.Print() ç›¸å½“äºä¸€ä¸ªèŠ‚ç‚¹ï¼Œå®šä¹‰äº†æ•°æ®çš„æµå…¥å’Œæµå‡ºã€‚ ä¸€ä¸ªerrorï¼šåœ¨æ¨¡å‹å®šä¹‰ä¸­ï¼Œç›´æ¥è°ƒç”¨ï¼š 1234intra_distance = tf.Print(intra_distance, [intra_distance], message='Debug info: ', summarize=10) ä¼šæŠ¥é”™ï¼šAttributeError: â€˜Tensorâ€™ object has no attribute â€˜_keras_historyâ€™ å‚è€ƒï¼šhttps://stackoverflow.com/questions/56096399/creating-model-throws-attributeerror-tensor-object-has-no-attribute-keras You cannot use backend functions directly in Keras tensors, every operation in these tensors must be a layer. You need to wrap each custom operation in a Lambda layer and provide the appropriate inputs to the layer. ä¹‹å‰ä¸€ç›´æ²¡æ³¨æ„åˆ°è¿™ä¸ªé—®é¢˜ï¼Œå‡¡æ˜¯è°ƒç”¨äº†tf.XXXçš„operationï¼Œéƒ½è¦wrapperåœ¨Lambdaå±‚é‡Œã€‚ æ”¹å†™ï¼š 123456789101112131415# wrapper functiondef debug(args): intra_distance, min_inter_distance = args intra_distance = tf.Print(intra_distance, [intra_distance], message='Debug info: ', summarize=10) min_inter_distance = tf.Print(min_inter_distance, [min_inter_distance], message='Debug info: ', summarize=10) return [intra_distance, min_inter_distance]# æ¨¡å‹å†…intra_distance, min_inter_distance = Lambda(debug)([intra_distance, min_inter_distance]) ã€å¤¹å¸¦ç§è´§ã€‘tf.PrintåŒæ—¶ä¹Ÿå¯ä»¥æ‰“å°wrapper functionå†…çš„ä¸­é—´å˜é‡ï¼Œéƒ½æ”¾åœ¨åˆ—è¡¨é‡Œé¢å°±å¯ä»¥äº†ã€‚ 8.3 tf.while_loop(cond, body, init_value) tensorflowä¸­å®ç°å¾ªç¯çš„è¯­å¥ ç»ˆæ­¢æ¡ä»¶condï¼šæ˜¯ä¸€ä¸ªå‡½æ•° å¾ªç¯ä½“bodyï¼šæ˜¯ä¸€ä¸ªå‡½æ•° init_valueï¼šæ˜¯ä¸€ä¸ªlistï¼Œä¿å­˜å¾ªç¯ç›¸å…³å‚æ•° condã€bodyçš„å‚æ•°æ˜¯è¦ä¸init_valueåˆ—è¡¨ä¸­å˜é‡ä¸€ä¸€å¯¹åº”çš„ bodyè¿”å›å€¼çš„æ ¼å¼è¦ä¸init_valueå˜é‡ä¸€è‡´ï¼ˆtensorå½¢çŠ¶ä¿æŒä¸å˜ï¼‰ è‹¥éè¦å˜æ€ä¹ˆåŠï¼ˆæœ‰æ—¶å€™æˆ‘ä»¬å¸Œæœ›åœ¨while_loopçš„è¿‡ç¨‹ä¸­ï¼Œç»´æŠ¤ä¸€ä¸ªlistï¼‰ï¼ŸåŠ¨æ€æ•°ç»„TensorArrayï¼é«˜çº§å‚æ•°shape_invariants 8.3.1 åŠ¨æ€æ•°ç»„ 12345# å®šä¹‰b_boxes = tf.TensorArray(K.dtype(boxes), size=1, dynamic_size=True, clear_after_read=False)# å†™å…¥æŒ‡å®šä½ç½®b_boxes = b_boxes.write(b, boxes_) â€‹ tensor arrayå˜é‡ä¸­ä¸€ä¸ªä½ç½®åªèƒ½å†™å…¥ä¸€æ¬¡ 8.3.2 shape_invariants â€‹ reference stackoverflow 12345678910i = tf.constant(0)l = tf.Variable([])def body(i, l): temp = tf.gather(array,i) l = tf.concat([l, [temp]], 0) return i+1, lindex, list_vals = tf.while_loop(cond, body, [i, l], shape_invariants=[i.get_shape(), tf.TensorShape([None])]) â€‹ åœ¨while_loopä¸­æ˜¾ç¤ºåœ°æŒ‡å®šå‚æ•°çš„shapeï¼Œä¸Šé¢çš„ä¾‹å­ç”¨äº†tf.TensorShape([None])ä»¤å…¶è‡ªåŠ¨æ¨æ–­ï¼Œè€Œä¸æ˜¯å›ºå®šæ£€æŸ¥ï¼Œå› æ­¤å¯ä»¥è§£å†³å˜åŒ–é•¿åº¦åˆ—è¡¨ã€‚ ä¸€ä¸ªå®Œæ•´çš„æ —å­ï¼šç¬¬ä¸€æ¬¡è§while_loopï¼Œåœ¨yolo_lossé‡Œé¢ åŸºäºbatchç»´åº¦åšéå† loopç»“æŸåå°†åŠ¨æ€æ•°æ®stackèµ·æ¥ï¼Œé‡è·batch dim 12345678910111213# Find ignore mask, iterate over each of batch.# extract the elements on the mask which has iou &lt; ignore_threshignore_mask = tf.TensorArray(K.dtype(y_true[0]), size=1, dynamic_size=True) # åŠ¨æ€sizeæ•°ç»„object_mask_bool = K.cast(object_mask, 'bool')def loop_body(b, ignore_mask): true_box = tf.boolean_mask(y_true[l][b,...,0:4], object_mask_bool[b,...,0]) # (H,W,3,5) iou = box_iou(pred_box[b], true_box) # (H,W,3,1) best_iou = K.max(iou, axis=-1) ignore_mask = ignore_mask.write(b, K.cast(best_iou&lt;ignore_thresh, K.dtype(true_box))) return b+1, ignore_mask_, ignore_mask = K.control_flow_ops.while_loop(lambda b,*args: b&lt;m, loop_body, [0, ignore_mask])ignore_mask = ignore_mask.stack()ignore_mask = K.expand_dims(ignore_mask, -1) # ï¼ˆN,H,W,3,1ï¼‰ 8.4 tf.image.non_max_suppression() éæœ€å¤§å€¼æŠ‘åˆ¶ï¼šè´ªå©ªç®—æ³•ï¼ŒæŒ‰scoresç”±å¤§åˆ°å°æ’åºï¼Œé€‰å®šç¬¬ä¸€ä¸ªï¼Œä¾æ¬¡å¯¹ä¹‹åçš„æ¡†æ±‚iouï¼Œåˆ é™¤é‚£äº›å’Œé€‰å®šæ¡†iouå¤§äºé˜ˆå€¼çš„boxã€‚ 12345# è¿”å›æ˜¯è¢«é€‰ä¸­è¾¹æ¡†åœ¨å‚æ•°boxesä¸­çš„ä¸‹æ ‡ä½ç½®selected_indices=tf.image.non_max_suppression(boxes, scores, max_output_size, iou_threshold=0.5, name=None)# æ ¹æ®indicesè·å–è¾¹æ¡†selected_boxes=tf.gather(boxes,selected_indices) boxesï¼š2-Dçš„floatç±»å‹çš„ï¼Œå¤§å°ä¸º[num_boxes,4]çš„å¼ é‡ scoresï¼š1-Dçš„floatç±»å‹çš„ï¼Œå¤§å°ä¸º[num_boxes]ï¼Œå¯¹åº”çš„æ¯ä¸€ä¸ªboxçš„ä¸€ä¸ªscore max_output_sizeï¼šæ ‡é‡æ•´æ•°Tensorï¼Œè¾“å‡ºæ¡†çš„æœ€å¤§æ•°é‡ iou_thresholdï¼šæµ®ç‚¹æ•°ï¼ŒIOUé˜ˆå€¼ selected_indicesï¼š1-Dçš„æ•´æ•°å¼ é‡ï¼Œå¤§å°ä¸º[M]ï¼Œç•™ä¸‹æ¥çš„è¾¹æ¡†ä¸‹æ ‡ï¼ŒMå°äºç­‰äºmax_output_size ã€æ‹“å±•ã€‘è¿˜æœ‰Multi-class version of NMSâ€”â€”tf.multiclass_non_max_suppression() 8.5 é™åˆ¶GPUç”¨é‡ linuxä¸‹æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µï¼Œ1ç§’åˆ·æ–°ä¸€æ¬¡ï¼š 1watch -n 1 nvidia-smi æŒ‡å®šæ˜¾å¡å· 12import osos.environ["CUDA_VISIBLE_DEVICES"] = "2" é™åˆ¶GPUç”¨é‡ 1234567891011121314import tensorflow as tfimport keras.backend as K# è®¾ç½®ç™¾åˆ†æ¯”config = tf.ConfigProto()config.gpu_options.per_process_gpu_memory_fraction = 0.3session = tf.Session(config=config)K.set_session(session)# è®¾ç½®åŠ¨æ€ç”³è¯·config = tf.ConfigProto() config.gpu_options.allow_growth=True #ä¸å…¨éƒ¨å æ»¡æ˜¾å­˜, æŒ‰éœ€åˆ†é…session = tf.Session(config=config)K.set_session(session) 8.6 tf.boolean_mask() tf.boolean_mask(tensor,mask,name=â€™boolean_maskâ€™,axis=None) å…¶ä¸­ï¼Œtensoræ˜¯Nç»´åº¦çš„ï¼Œmaskæ˜¯Kç»´åº¦çš„ï¼Œ$K \leq N$ axisè¡¨ç¤ºmaskçš„èµ·å§‹ç»´åº¦ï¼Œè¢«maskçš„ç»´åº¦åªä¿ç•™maskä¸ºTrueçš„æ•°æ®ï¼ŒåŒæ—¶è¿™éƒ¨åˆ†æ•°æ®flattenæˆä¸€ç»´ï¼Œæœ€ç»ˆtensorçš„ç»´åº¦æ˜¯N-K+1 æ —å­ï¼šyolov3é‡Œé¢ï¼ŒæŠŠç‰¹å¾å›¾ä¸Šæœ‰objectçš„gridæå–å‡ºæ¥ï¼š 123# y_trues: [b,h,w,a,4]# conf_gt: [b,h,w,a,1]true_box = tf.boolean_mask(y_trues[i][b,...,0:4], conf_gt[b,...,0]) 9. kerasè‡ªå®šä¹‰ä¼˜åŒ–å™¨optimizer9.1 å…³äºæ¢¯åº¦çš„ä¼˜åŒ–å™¨å…¬å…±å‚æ•°ï¼Œç”¨äºæ¢¯åº¦è£å‰ª clipnormï¼šå¯¹æ‰€æœ‰æ¢¯åº¦è¿›è¡Œdownscaleï¼Œä½¿å¾—æ¢¯åº¦vectorä¸­l2èŒƒæ•°æœ€å¤§ä¸º1ï¼ˆg * 1 / max(1, l2_norm)ï¼‰ clipvalueï¼šå¯¹ç»å¯¹å€¼è¿›è¡Œä¸Šä¸‹é™æˆªæ–­ 9.2 kerasçš„Optimizierå¯¹è±¡ kerasçš„å®˜æ–¹ä»£ç æœ‰optimizier_v1å’Œoptimizier_v2ä¸¤ç‰ˆï¼Œåˆ†åˆ«é¢å‘tf1å’Œtf2ï¼Œv1çš„çœ‹èµ·æ¥ç®€æ´ä¸€äº› self.updates &amp; self.weights self.updatesï¼šstores the variables that will be updated with every batch that is processed by the model in training ç”¨æ¥ä¿å­˜ä¸æ¨¡å‹è®­ç»ƒç›¸å…³çš„å‚æ•°ï¼ˆiterationsã€paramsã€momentsã€accumulatorsï¼Œetcï¼‰ symbolic graph variableï¼Œé€šè¿‡K.update_addæ–¹æ³•è¯´æ˜å›¾çš„operation self.weightsï¼šthe functions that save and load optimizers will save and load this property ç”¨æ¥ä¿å­˜ä¸ä¼˜åŒ–å™¨ç›¸å…³çš„å‚æ•° model.save()æ–¹æ³•ä¸­æ¶‰åŠinclude_optimizer=Falseï¼Œå†³å®šä¼˜åŒ–å™¨çš„ä¿å­˜å’Œé‡è½½ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class Optimizer(object): # - æŠ½è±¡ç±»ï¼Œæ‰€æœ‰çœŸå®çš„ä¼˜åŒ–å™¨ç»§æ‰¿è‡ªOptimizerå¯¹è±¡ # - æä¾›ä¸¤ä¸ªç”¨äºæ¢¯åº¦æˆªæ–­çš„å…¬å…±å‚æ•° def __init__(self, **kwargs): allowed_kwargs = &#123;'clipnorm', 'clipvalue'&#125; for k in kwargs: if k not in allowed_kwargs: raise TypeError('Unexpected keyword argument passed to optimizer: ' + str(k)) # checks that clipnorm &gt;= 0 and clipvalue &gt;= 0 if kwargs[k] &lt; 0: raise ValueError('Expected &#123;&#125; &gt;= 0, received: &#123;&#125;'.format(k, kwargs[k])) self.__dict__.update(kwargs) self.updates = [] # è®¡ç®—æ›´æ–°çš„å‚æ•° self.weights = [] # ä¼˜åŒ–å™¨å¸¦æ¥çš„æƒé‡ï¼Œåœ¨get_updatesä»¥åæ‰æœ‰å…ƒç´ ï¼Œåœ¨ä¿å­˜æ¨¡å‹æ—¶ä¼šè¢«ä¿å­˜ # Set this to False, indicating `apply_gradients` does not take the # `experimental_aggregate_gradients` argument. _HAS_AGGREGATE_GRAD = False def _create_all_weights(self, params): # å£°æ˜é™¤äº†gradsä»¥å¤–ç”¨äºæ¢¯åº¦æ›´æ–°çš„å‚æ•°ï¼Œåˆ›å»ºå†…å­˜ç©ºé—´ï¼Œåœ¨get_updatesæ–¹æ³•ä¸­ä½¿ç”¨ raise NotImplementedError def get_updates(self, loss, params): # å®šä¹‰æ¢¯åº¦æ›´æ–°çš„è®¡ç®—æ–¹æ³•, æ›´æ–°self.updates raise NotImplementedError def get_config(self): # configé‡Œé¢æ˜¯ä¼˜åŒ–å™¨ç›¸å…³çš„å‚æ•°ï¼Œé»˜è®¤åªæœ‰ä¸¤ä¸ªæ¢¯åº¦æˆªæ–­çš„å‚æ•°ï¼Œéœ€è¦æ ¹æ®å®é™…ä¼˜åŒ–å™¨æ·»åŠ ï¼ˆlrã€decay ...ï¼‰ config = &#123;&#125; if hasattr(self, 'clipnorm'): config['clipnorm'] = self.clipnorm if hasattr(self, 'clipvalue'): config['clipvalue'] = self.clipvalue return config def get_gradients(self, loss, params): # è®¡ç®—æ¢¯åº¦å€¼ï¼Œå¹¶åœ¨æœ‰å¿…è¦æ—¶è¿›è¡Œæ¢¯åº¦æˆªæ–­ grads = K.gradients(loss, params) if any(g is None for g in grads): raise ValueError('An operation has `None` for gradient. ' 'Please make sure that all of your ops have a ' 'gradient defined (i.e. are differentiable). ' 'Common ops without gradient: ' 'K.argmax, K.round, K.eval.') if hasattr(self, 'clipnorm'): grads = [tf.clip_by_norm(g, self.clipnorm) for g in grads] if hasattr(self, 'clipvalue'): grads = [ tf.clip_by_value(g, -self.clipvalue, self.clipvalue) for g in grads ] return grads def set_weights(self, weights): # ç»™optimizerçš„weightsç”¨ä¸€ç³»åˆ—np arrayèµ‹å€¼ # æ²¡çœ‹åˆ°æœ‰è°ƒç”¨ï¼Œçœç•¥codeï¼š K.batch_set_value() def get_weights(self): # è·å–weightsçš„np arrayå€¼ # æ²¡çœ‹åˆ°æœ‰è°ƒç”¨ï¼Œçœç•¥codeï¼š K.batch_get_value() @classmethod def from_config(cls, config): return cls(**config) 9.3 å®ä¾‹åŒ–ä¸€ä¸ªä¼˜åŒ–å™¨ based on keras.Optimizerå¯¹è±¡ ä¸»è¦éœ€è¦é‡å†™get_updateså’Œget_configæ–¹æ³• get_updatesç”¨æ¥å®šä¹‰æ¢¯åº¦æ›´æ–°çš„è®¡ç®—æ–¹æ³• get_configç”¨æ¥å®šä¹‰å®ä¾‹ç”¨åˆ°çš„å‚æ•° ä»¥SoftSGDä¸ºä¾‹ï¼š æ¯éš”ä¸€å®šçš„batchæ‰æ›´æ–°ä¸€æ¬¡å‚æ•°ï¼Œä¸æ›´æ–°æ¢¯åº¦çš„stepæ¢¯åº¦ä¸æ¸…ç©ºï¼Œæ‰§è¡Œç´¯åŠ ï¼Œä»è€Œå®ç°batchsizeçš„å˜ç›¸æ‰©å¤§ å»ºè®®æ­é…é—´éš”æ›´æ–°å‚æ•°çš„BNå±‚æ¥ä½¿ç”¨ï¼Œå¦åˆ™BNè¿˜æ˜¯åŸºäºå°batchsizeæ¥æ›´æ–°å‡å€¼å’Œæ–¹å·® 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class SoftSGD(Optimizer): # [new arg] steps_per_update: how many batch to update gradient def __init__(self, lr=0.01, momentum=0., decay=0., nesterov=False, steps_per_update=2, **kwargs): super(SoftSGD, self).__init__(**kwargs) with K.name_scope(self.__class__.__name__): self.iterations = K.variable(0, dtype='int64', name='iterations') self.lr = K.variable(lr, name='lr') self.steps_per_update = steps_per_update # å¤šå°‘batchæ‰æ›´æ–°ä¸€æ¬¡ self.momentum = K.variable(momentum, name='momentum') self.decay = K.variable(decay, name='decay') self.initial_decay = decay self.nesterov = nesterov def get_updates(self, loss, params): # learning rate decay lr = self.lr if self.initial_decay &gt; 0: lr = lr * (1. / (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay)))) shapes = [K.int_shape(p) for p in params] sum_grads = [K.zeros(shape) for shape in shapes] # å¹³å‡æ¢¯åº¦ï¼Œç”¨æ¥æ¢¯åº¦ä¸‹é™ grads = self.get_gradients(loss, params) # å½“å‰batchæ¢¯åº¦ self.updates = [K.update_add(self.iterations, 1)] self.weights = [self.iterations] + sum_grads for p, g, sg in zip(params, grads, sum_grads): # momentum æ¢¯åº¦ä¸‹é™ v = self.momentum * sg / float(self.steps_per_update) - lr * g # velocity if self.nesterov: new_p = p + self.momentum * v - lr * sg / float(self.steps_per_update) else: new_p = p + v # å¦‚æœæœ‰çº¦æŸï¼Œå¯¹å‚æ•°åŠ ä¸Šçº¦æŸ if getattr(p, 'constraint', None) is not None: new_p = p.constraint(new_p) # æ»¡è¶³æ¡ä»¶æ‰æ›´æ–°å‚æ•° cond = K.equal(self.iterations % self.steps_per_update, 0) self.updates.append(K.switch(cond, K.update(p, new_p), p)) self.updates.append(K.switch(cond, K.update(sg, g), K.update(sg, sg + g))) return self.updates def get_config(self): config = &#123;'lr': float(K.get_value(self.lr)), 'steps_per_update': self.steps_per_update, 'momentum': float(K.get_value(self.momentum)), 'decay': float(K.get_value(self.decay)), 'nesterov': self.nesterov &#125; base_config = super(SoftSGD, self).get_config() return dict(list(base_config.items()) + list(config.items())) 10. kerasè‡ªå®šä¹‰æ¿€æ´»å‡½æ•°activation10.1 å®šä¹‰æ¿€æ´»å‡½æ•° 123def gelu(x): cdf = 0.5 * (1.0 + tf.erf(x / tf.sqrt(2.0))) return x*cdf 10.2 ä½¿ç”¨è‡ªå®šä¹‰æ¿€æ´»å‡½æ•° ä½¿ç”¨Activationæ–¹æ³• 1x = Activation(gelu)(x) ä¸èƒ½æ•´åˆè¿›å¸¦æœ‰activationå‚æ•°çš„å±‚ï¼ˆå¦‚Conv2Dï¼‰ï¼Œå› ä¸ºConvåŸºç±»çš„get_config()æ–¹æ³•ä»keras.activationsé‡Œé¢è¯»å–ç›¸åº”çš„æ¿€æ´»å‡½æ•°ï¼Œå…¶ä¸­å¸¦å‚æ•°çš„æ¿€æ´»å‡½æ•°å¦‚PReLUï¼ˆAdvanced activationsï¼‰ã€ä»¥åŠè‡ªå®šä¹‰çš„æ¿€æ´»å‡½æ•°éƒ½ä¸åœ¨è¿™ä¸ªå­—å…¸ä¸­ï¼Œå¦åˆ™ä¼šæŠ¥é”™ï¼š AttributeError: â€˜Activationâ€™ object has no attribute â€˜nameâ€˜ 10.3 checkpoint issue ç½‘ä¸Šè¿˜æœ‰å¦ä¸€ç§å†™æ³•ï¼š 1234567891011from keras.layers import Activationfrom keras.utils.generic_utils import get_custom_objectsdef gelu(x): cdf = 0.5 * (1.0 + tf.erf(x / tf.sqrt(2.0))) return x*cdf get_custom_objects().update(&#123;'gelu': Activation(gelu)&#125;)# åé¢å¯ä»¥é€šè¿‡åå­—è°ƒç”¨æ¿€æ´»å‡½æ•°x = Activation('gelu')(x) è¿™ç§å†™æ³•åœ¨ä½¿ç”¨ModelCheckpointsæ–¹æ³•ä¿å­˜æƒé‡æ—¶ä¼šæŠ¥é”™ï¼š AttributeError: â€˜Activationâ€™ object has no attribute â€˜nameâ€˜ çœ‹logå‘ç°å½“ä½¿ç”¨åå­—ä»£è¡¨æ¿€æ´»å±‚çš„æ—¶å€™ï¼Œåœ¨ä¿å­˜æ¨¡å‹çš„æ—¶å€™ï¼Œåˆä¼šæœ‰ä¸€ä¸ªget_config()å‡½æ•°ä»keras.activationsä¸­æŸ¥è¡¨ 11. kerasè‡ªå®šä¹‰æ­£åˆ™åŒ–å™¨regularizers11.1 ä½¿ç”¨å°è£…å¥½çš„regularizers kerasçš„æ­£åˆ™åŒ–å™¨æ²¡æœ‰globalçš„ä¸€é”®æ·»åŠ æ–¹æ³•ï¼Œè¦layer-wiseä¸ºæ¯ä¸€å±‚æ·»åŠ  kerasçš„å±‚share 3 commonå‚æ•°æ¥å£ï¼š kernel_regularizer bias_regularizer activity_regularizer å¯é€‰ç”¨çš„æ­£åˆ™åŒ–å™¨ keras.regularizers.l1(0.01) keras.regularizers.l2(0.01) keras.regularizers.l1_l2(l1=0.01, l2=0.01) ä½¿ç”¨ 12345678layer = tf.keras.layers.Dense(5, kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l1(0.01), activity_regularizer=tf.keras.regularizers.l2(0.01))tensor = tf.ones(shape=(5, 5)) * 2.0out = layer(tensor)# The kernel regularization term is 0.25# The activity regularization term (after dividing by the batch size) is 5print(tf.math.reduce_sum(layer.losses)) # 5.25 (= 5 + 0.25) 11.2 custom regularizer ä¸€èˆ¬ä¸ä¼šè‡ªå®šä¹‰è¿™ä¸ªä¸œè¥¿ï¼Œç¡¬è¦customçš„è¯ï¼Œä¸¤ç§æ–¹å¼ ç®€å•ç‰ˆï¼Œæ¥å£å‚æ•°æ˜¯weight_matrixï¼Œæ— é¢å¤–å‚æ•°ï¼Œå±‚ç›´æ¥è°ƒç”¨ 1234def my_regularizer(x): return 1e-3 * tf.reduce_sum(tf.square(x))layer = tf.keras.layers.Dense(5, kernel_initializer='ones', kernel_regularizer=my_regularizer) å­ç±»ç»§æ‰¿ç‰ˆï¼Œå¯ä»¥åŠ é¢å¤–å‚æ•°ï¼Œéœ€è¦è¡¥å……get_configæ–¹æ³•ï¼Œæ”¯æŒè¯»å†™æƒé‡æ—¶çš„ä¸²è¡ŒåŒ– 123456789101112class MyRegularizer(regularizers.Regularizer): def __init__(self, strength): self.strength = strength def __call__(self, x): return self.strength * tf.reduce_sum(tf.square(x)) def get_config(self): return &#123;'strength': self.strength&#125;layer = tf.keras.layers.Dense(5, kernel_initializer='ones', kernel_regularizer=MyRegularizer(0.01)) 11.3 å¼ºè¡Œglobal æ¯å±‚åŠ èµ·æ¥å¤ªçƒ¦äº†ï¼Œæ‰¹é‡åŠ çš„å®è´¨ä¹Ÿæ˜¯é€å±‚åŠ ï¼Œåªä¸è¿‡å†™æˆå¾ªç¯ æ ¸å¿ƒæ˜¯layerçš„add_lossæ–¹æ³• 12345678model = keras.applications.ResNet50(include_top=True, weights='imagenet')alpha = 0.00002 # weight decay coefficientfor layer in model.layers: if isinstance(layer, keras.layers.Conv2D) or isinstance(layer, keras.layers.Dense): layer.add_loss(lambda: keras.regularizers.l2(alpha)(layer.kernel)) if hasattr(layer, 'bias_regularizer') and layer.use_bias: layer.add_loss(lambda: keras.regularizers.l2(alpha)(layer.bias)) 12. kerasæŸ¥çœ‹æ¢¯åº¦&amp;æƒé‡12.1 easiest way æŸ¥çœ‹æ¢¯åº¦æœ€ç®€å•çš„æ–¹æ³•ï¼šé€šè¿‡K.gradientsæ–¹æ³•å®šä¹‰ä¸€ä¸ªæ±‚æ¢¯åº¦çš„funcï¼Œç„¶åç»™å®šè¾“å…¥ï¼Œå¾—åˆ°æ¢¯åº¦ï¼ˆCAMå°±æ˜¯è¿™ä¹ˆå¹²çš„ï¼‰ æŸ¥çœ‹æƒé‡æœ€ç®€å•çš„æ–¹æ³•ï¼šå­˜åœ¨h5æ–‡ä»¶ï¼Œç„¶åèŠ±å¼h5pyè§£æ 12.2 dig deeper ä¸€ä¸ªæ€è·¯ï¼šå°†æ¢¯åº¦ä¿å­˜åœ¨optimizerçš„self.weightsä¸­ï¼Œå¹¶åœ¨model.saveå¾—åˆ°çš„æ¨¡å‹ä¸­è§£æ 13. keraså®ç°æƒé‡æ»‘åŠ¨å¹³å‡13.1 why EMA on weights [reference1][https://www.jiqizhixin.com/articles/2019-05-07-18]ï¼šæƒé‡æ»‘åŠ¨å¹³å‡æ˜¯æä¾›è®­ç»ƒç¨³å®šæ€§çš„æœ‰æ•ˆæ–¹æ³•ï¼Œè¦ä¹ˆåœ¨ä¼˜åŒ–å™¨é‡Œé¢å®ç°ï¼Œè¦ä¹ˆå¤–åµŒåœ¨è®­ç»ƒä»£ç é‡Œ [reference2][https://cloud.tencent.com/developer/article/1636781]ï¼šè¿™é‡Œé¢ä¸¾çš„ä¾‹å­å¾ˆæ¸…æ™°äº†ï¼Œå°±æ˜¯ä¸ºäº†æƒé‡æ¯ä¸ªstepå‰åå˜åŒ–ä¸å¤§ æƒé‡EMAçš„è®¡ç®—æ–¹å¼æœ‰ç‚¹ç±»ä¼¼äºBNçš„running mean&amp;varï¼š åœ¨è®­ç»ƒé˜¶æ®µï¼šå®ƒä¸æ”¹å˜æ¯ä¸ªtraining stepçš„ä¼˜åŒ–æ–¹å‘ï¼Œè€Œæ˜¯ä»initial weightså¼€å§‹ï¼Œå¦å¤–ç»´æŠ¤ä¸€ç»„shadow weightsï¼Œç”¨æ¯æ¬¡çš„updating weightsæ¥è¿›è¡Œæ»‘åŠ¨æ›´æ–° åœ¨inferenceé˜¶æ®µï¼Œæˆ‘ä»¬è¦ç”¨shadow weightsæ¥æ›¿æ¢å½“å‰æƒé‡æ–‡ä»¶ä¿å­˜çš„weightsï¼ˆcurrent stepä¸‹è®¡ç®—çš„æ–°æƒé‡ï¼‰ å¦‚æœè¦ç»§ç»­è®­ç»ƒï¼Œè¦å°†æ›¿æ¢çš„æƒé‡åœ¨æ¢å›æ¥ï¼Œå› ä¸ºã€EMAä¸å½±å“æ¨¡å‹çš„ä¼˜åŒ–è½¨è¿¹ã€‘ 13.2 who uses EMA å¾ˆå¤šGANçš„è®ºæ–‡éƒ½ç”¨äº†EMAï¼Œ è¿˜æœ‰NLPé˜…è¯»ç†è§£æ¨¡å‹QANetï¼Œ è¿˜æœ‰Googleçš„efficientNetã€resnet_rs 13.3 how to implement outside 12345678910111213141516171819202122232425262728293031class ExponentialMovingAverage: """å¯¹æ¨¡å‹æƒé‡è¿›è¡ŒæŒ‡æ•°æ»‘åŠ¨å¹³å‡ã€‚ ç”¨æ³•ï¼šåœ¨model.compileä¹‹åã€ç¬¬ä¸€æ¬¡è®­ç»ƒä¹‹å‰ä½¿ç”¨ï¼› å…ˆåˆå§‹åŒ–å¯¹è±¡ï¼Œç„¶åæ‰§è¡Œinjectæ–¹æ³•ã€‚ """ def __init__(self, model, momentum=0.9999): self.momentum = momentum self.model = model self.ema_weights = [K.zeros(K.shape(w)) for w in model.weights] def inject(self): """æ·»åŠ æ›´æ–°ç®—å­åˆ°model.metrics_updatesã€‚ """ self.initialize() for w1, w2 in zip(self.ema_weights, self.model.weights): op = K.moving_average_update(w1, w2, self.momentum) self.model.metrics_updates.append(op) def initialize(self): """ema_weightsåˆå§‹åŒ–è·ŸåŸæ¨¡å‹åˆå§‹åŒ–ä¸€è‡´ã€‚ """ self.old_weights = K.batch_get_value(self.model.weights) K.batch_set_value(zip(self.ema_weights, self.old_weights)) def apply_ema_weights(self): """å¤‡ä»½åŸæ¨¡å‹æƒé‡ï¼Œç„¶åå°†å¹³å‡æƒé‡åº”ç”¨åˆ°æ¨¡å‹ä¸Šå»ã€‚ """ self.old_weights = K.batch_get_value(self.model.weights) ema_weights = K.batch_get_value(self.ema_weights) K.batch_set_value(zip(self.model.weights, ema_weights)) def reset_old_weights(self): """æ¢å¤æ¨¡å‹åˆ°æ—§æƒé‡ã€‚ """ K.batch_set_value(zip(self.model.weights, self.old_weights)) then train 1234EMAer = ExponentialMovingAverage(model) # åœ¨æ¨¡å‹compileä¹‹åæ‰§è¡ŒEMAer.inject() # åœ¨æ¨¡å‹compileä¹‹åæ‰§è¡Œmodel.fit(x_train, y_train) # è®­ç»ƒæ¨¡å‹ then inference 12345MAer.apply_ema_weights() # å°†EMAçš„æƒé‡åº”ç”¨åˆ°æ¨¡å‹ä¸­model.predict(x_test) # è¿›è¡Œé¢„æµ‹ã€éªŒè¯ã€ä¿å­˜ç­‰æ“ä½œEMAer.reset_old_weights() # ç»§ç»­è®­ç»ƒä¹‹å‰ï¼Œè¦æ¢å¤æ¨¡å‹æ—§æƒé‡ã€‚è¿˜æ˜¯é‚£å¥è¯ï¼ŒEMAä¸å½±å“æ¨¡å‹çš„ä¼˜åŒ–è½¨è¿¹ã€‚model.fit(x_train, y_train) # ç»§ç»­è®­ç»ƒ 14. kerasçš„Modelç±»ç»§æ‰¿14.1 å®šä¹‰æ¨¡å‹çš„æ–¹å¼ Sequentialï¼šæœ€ç®€å•ï¼Œä½†æ˜¯ä¸èƒ½è¡¨ç¤ºå¤æ‚æ‹“æ‰‘ç»“æ„ å‡½æ•°å¼ APIï¼šå’ŒSequentialç”¨æ³•åŸºæœ¬ä¸€è‡´ï¼Œè¾“å…¥å¼ é‡å’Œè¾“å‡ºå¼ é‡ç”¨äºå®šä¹‰ tf.keras.Modelå®ä¾‹ æ¨¡å‹å­ç±»åŒ–ï¼šå¼•å…¥äº Keras 2.2.0 kerasæºä»£ç å®šä¹‰åœ¨ï¼šhttps://github.com/keras-team/keras/blob/master/keras/engine/training.py 14.2 æ¨¡å‹å­ç±»åŒ–overview æ—¢å¯ä»¥ç”¨æ¥å®šä¹‰ä¸€ä¸ªmodelï¼Œä¹Ÿå¯ä»¥ç”¨æ¥å®šä¹‰ä¸€ä¸ªå¤æ‚çš„ç½‘ç»œå±‚ï¼Œä¸ºå®ç°å¤æ‚æ¨¡å‹æä¾›æ›´å¤§çš„çµæ´»æ€§ æœ‰ç‚¹ç±»ä¼¼äºtorchçš„è¯­æ³• ç½‘ç»œå±‚å®šä¹‰åœ¨ __init__(self, ...) ä¸­ï¼šè·Ÿtorchè¯­æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºå±‚ä¸èƒ½å¤ç”¨ï¼ŒtorchåŒä¸€ä¸ªå±‚åœ¨forwardä¸­æ¯è°ƒç”¨ä¸€æ¬¡èƒ½å¤Ÿåˆ›å»ºä¸€ä¸ªå®ä¾‹ï¼Œkerasæ¯ä¸ªå±‚åº”è¯¥æ˜¯åœ¨initä¸­å£°æ˜å¹¶åˆ›å»ºï¼Œæ‰€ä»¥ä¸èƒ½å¤ç”¨ å‰å‘ä¼ æ’­åœ¨ call(self, inputs) ä¸­ï¼Œè¿™é‡Œé¢ä¹Ÿå¯ä»¥æ·»åŠ loss compute_output_shapeè®¡ç®—æ¨¡å‹è¾“å‡ºçš„å½¢çŠ¶ å’Œkerasè‡ªå®šä¹‰å±‚çš„è¯­æ³•ä¹Ÿå¾ˆç›¸ä¼¼ build(input_shape)ï¼šä¸»è¦åŒºåˆ«å°±åœ¨äºbuildï¼Œå› ä¸ºè‡ªå®šä¹‰å±‚æœ‰buildï¼Œæ˜¾å¼å£°æ˜äº†æ•°æ®æµçš„shapeï¼Œèƒ½å¤Ÿæ„é€ å‡ºé™æ€å›¾ call(x)ï¼š compute_output_shape(input_shape)ï¼š ã€ä»¥ä¸‹æ–¹æ³•å’Œå±æ€§ä¸é€‚ç”¨äºç±»ç»§æ‰¿æ¨¡å‹ã€‘ï¼Œæ‰€ä»¥è¿˜æ˜¯æ¨èä¼˜å…ˆä½¿ç”¨å‡½æ•°å¼ API model.inputs &amp; model.outputs model.to_yaml() &amp; model.to_json() model.get_config() &amp; model.save()ï¼šï¼ï¼ï¼åªèƒ½save_weightsï¼ï¼ï¼ 14.3 æ —å­ 12345678910111213141516171819202122232425262728293031import kerasimport numpy as npclass SimpleMLP(keras.Model): def __init__(self, num_classes=10): super(SimpleMLP, self).__init__(name='mlp') self.num_classes = num_classes self.dense1 = keras.layers.Dense(32, activation='relu') self.dense2 = keras.layers.Dense(num_classes, activation='softmax') self.dp = keras.layers.Dropout(0.5) self.bn = keras.layers.BatchNormalization(axis=-1) def call(self, inputs, training=None, mask=None): x = self.dense1(inputs) x = self.dp(x) x = self.bn(x, training=training) return self.dense2(x) def compute_output_shape(self, input_shape): batch, dim = input_shape return (batch, self.num_classes)model = SimpleMLP()model.compile('adam', loss='categorical_crossentropy')x = np.random.uniform(0,1,(32,100))y = np.random.randint(0, 2, (32,10))model.fit(x, y)model.summary() å¯ä»¥çœ‹åˆ°ï¼Œç±»ç»§æ‰¿æ¨¡å‹æ˜¯æ²¡æœ‰æŒ‡æ˜input_shapeçš„ï¼Œæ‰€ä»¥ä¹Ÿå°±ä¸å­˜åœ¨é™æ€å›¾ï¼Œè¦åœ¨æœ‰çœŸæ­£æ•°æ®æµä»¥åï¼Œmodelæ‰è¢«buildï¼Œæ‰èƒ½å¤Ÿè°ƒç”¨summayæ–¹æ³•ï¼ŒæŸ¥çœ‹å›¾ç»“æ„ ç¬¬äºŒä¸ªæ˜¯ï¼Œcallæ–¹æ³•çš„é»˜è®¤å‚æ•°ï¼šdef call(self, inputs, training=None, mask=None)ï¼Œ å­ç±»ç»§æ‰¿æ¨¡å‹ä¸æ”¯æŒæ˜¾å¼çš„å¤šè¾“å…¥å®šä¹‰ï¼Œæ‰€æœ‰çš„è¾“å…¥æ„æˆinputs éœ€è¦æ‰‹å·¥ç®¡ç†trainingå‚æ•°ï¼Œbn/dropoutç­‰åœ¨train/inference modeä¸‹è®¡ç®—ä¸ä¸€æ ·çš„æƒ…å†µï¼Œè¦æ˜¾å¼ä¼ å…¥trainingå‚æ•° maskåœ¨æ„å»ºAttentionæœºåˆ¶æˆ–è€…åºåˆ—æ¨¡å‹æ—¶ä¼šä½¿ç”¨åˆ°ï¼Œå¦‚æœprevious layerç”Ÿæˆäº†æ©ç ï¼ˆembeddingçš„mask_zeroå‚æ•°ä¸ºTrueï¼‰ï¼Œå‰ä¸¤ç§æ„å»ºæ¨¡å‹çš„æ–¹æ³•ä¸­ï¼Œmaskä¼šè‡ªåŠ¨ä¼ å…¥å½“å‰å±‚çš„callæ–¹æ³•ä¸­ 15. low-level training &amp; evaluation loops15.1 kerasçš„Modelç±»æä¾›äº†build-inçš„train/evalæ–¹æ³• * fit() * evaluate() * predict() * reference: https://keras.io/api/models/model_training_apis/ * reference: https://keras.io/guides/training_with_built_in_methods/ 15.2 å¦‚æœä½ æƒ³ä¿®æ”¹æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œä½†ä»æ—§é€šè¿‡fit()æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼ŒModelç±»ä¸­æä¾›äº†train_step()å¯ä»¥ç»§æ‰¿å’Œé‡è½½ reference: https://keras.io/guides/customizing_what_happens_in_fit/ Modelç±»ä¸­æœ‰ä¸€ä¸ªtrain_step()æ–¹æ³•ï¼Œfitæ¯ä¸ªbatchçš„æ—¶å€™éƒ½ä¼šè°ƒç”¨ä¸€æ¬¡ åœ¨é‡å†™è¿™ä¸ªtrain_step()æ–¹æ³•æ—¶ ä¼ å…¥å‚æ•°dataï¼šå–å†³äºfit()æ–¹æ³•ä¼ å…¥çš„å‚æ•°å½¢å¼ï¼Œtuple(x,y) / tf.data.Dataset forward passï¼šself(model) è®¡ç®—lossï¼šself.compiled_loss è®¡ç®—æ¢¯åº¦ï¼štf.GradientTape() æ›´æ–°æƒé‡ï¼šself.optimizer æ›´æ–°metricsï¼šself.compiled_metrics è¿”å›å€¼a dictionary mapping metric names æ —å­ğŸŒ° 12345678910111213141516171819202122232425262728293031323334class CustomModel(keras.Model): def train_step(self, data): # Unpack the data x, y = data with tf.GradientTape() as tape: y_pred = self(x, training=True) # Forward pass # Compute the loss value # (the loss function is configured in `compile()`) loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses) # Compute gradients trainable_vars = self.trainable_variables gradients = tape.gradient(loss, trainable_vars) # Update weights self.optimizer.apply_gradients(zip(gradients, trainable_vars)) # Update metrics (includes the metric that tracks the loss) self.compiled_metrics.update_state(y, y_pred) # Return a dict mapping metric names to current value return &#123;m.name: m.result() for m in self.metrics&#125;import numpy as np# Construct and compile an instance of CustomModelinputs = keras.Input(shape=(32,))outputs = keras.layers.Dense(1)(inputs)model = CustomModel(inputs, outputs)model.compile(optimizer="adam", loss="mse", metrics=["mae"])# Just use `fit` as usualx = np.random.random((1000, 32))y = np.random.random((1000, 1))model.fit(x, y, epochs=3) * æ³¨æ„åˆ°è¿™é‡Œé¢æˆ‘ä»¬è°ƒç”¨äº†self.compiled_losså’Œself.compiled_metricsï¼Œè¿™å°±æ˜¯åœ¨è°ƒç”¨compile()æ–¹æ³•çš„æ—¶å€™ä¼ å…¥çš„losså’Œmetricså‚æ•° get lower losså’Œmetricsä¹Ÿå¯ä»¥ä¸ä¼ ï¼Œç›´æ¥åœ¨CustomModelé‡Œé¢å£°æ˜å’Œå®šä¹‰ å£°æ˜ï¼šé‡è½½metrics()æ–¹æ³•ï¼Œåˆ›å»ºmetric instancesï¼Œç”¨äºè®¡ç®—losså’Œmetricsï¼ŒæŠŠä»–ä»¬æ”¾åœ¨è¿™é‡Œæ¨¡å‹ä¼šåœ¨fit()/evaluate()æ–¹æ³•çš„æ¯ä¸ªepochèµ·å§‹é˜¶æ®µè°ƒç”¨reset_states()æ–¹æ³•ï¼Œç¡®ä¿losså’Œmetricsçš„stateséƒ½æ˜¯per epochçš„ï¼Œè€Œä¸æ˜¯avg from the beginning æ›´æ–°ï¼šè°ƒç”¨update_state()æ–¹æ³•æ›´æ–°ä»–ä»¬çš„çŠ¶æ€å‚æ•°ï¼Œè°ƒç”¨result()æ–¹æ³•æ‹¿åˆ°ä»–ä»¬çš„current value 12345678910111213141516171819202122232425262728293031323334353637383940414243loss_tracker = keras.metrics.Mean(name="loss")mae_metric = keras.metrics.MeanAbsoluteError(name="mae")class CustomModel(keras.Model): def train_step(self, data): x, y = data with tf.GradientTape() as tape: y_pred = self(x, training=True) # Forward pass # Compute our own loss loss = keras.losses.mean_squared_error(y, y_pred) # Compute gradients trainable_vars = self.trainable_variables gradients = tape.gradient(loss, trainable_vars) # Update weights self.optimizer.apply_gradients(zip(gradients, trainable_vars)) # Compute our own metrics loss_tracker.update_state(loss) mae_metric.update_state(y, y_pred) return &#123;"loss": loss_tracker.result(), "mae": mae_metric.result()&#125; @property def metrics(self): # We list our `Metric` objects here so that `reset_states()` can be called automatically per epoch return [loss_tracker, mae_metric]# Construct an instance of CustomModelinputs = keras.Input(shape=(32,))outputs = keras.layers.Dense(1)(inputs)model = CustomModel(inputs, outputs)# We don't passs a loss or metrics here.model.compile(optimizer="adam")# Just use `fit` as usual -- you can use callbacks, etc.x = np.random.random((1000, 32))y = np.random.random((1000, 1))model.fit(x, y, epochs=5) ç›¸å¯¹åº”åœ°ï¼Œä¹Ÿå¯ä»¥å®šåˆ¶model.evaluate()çš„è®¡ç®—è¿‡ç¨‹â€”â€”override test_step()æ–¹æ³• ä¼ å…¥å‚æ•°dataï¼šå–å†³äºfit()æ–¹æ³•ä¼ å…¥çš„å‚æ•°å½¢å¼ï¼Œtuple(x,y) / tf.data.Dataset forward passï¼šself(model) è®¡ç®—lossï¼šself.compiled_loss è®¡ç®—metricsï¼šself.compiled_metrics è¿”å›å€¼a dictionary mapping metric names 12345678910111213141516171819202122232425class CustomModel(keras.Model): def test_step(self, data): # Unpack the data x, y = data # Compute predictions y_pred = self(x, training=False) # Updates the metrics tracking the loss self.compiled_loss(y, y_pred, regularization_losses=self.losses) # Update the metrics. self.compiled_metrics.update_state(y, y_pred) # Return a dict mapping metric names to current value. # Note that it will include the loss (tracked in self.metrics). return &#123;m.name: m.result() for m in self.metrics&#125;# Construct an instance of CustomModelinputs = keras.Input(shape=(32,))outputs = keras.layers.Dense(1)(inputs)model = CustomModel(inputs, outputs)model.compile(loss="mse", metrics=["mae"])# Evaluate with our custom test_stepx = np.random.random((1000, 32))y = np.random.random((1000, 1))model.evaluate(x, y) 15.3 å®ç°å®Œæ•´çš„train loops reference: https://keras.io/guides/writing_a_training_loop_from_scratch/ a train loop a for loopï¼šiter for each epoch a for loopï¼šiter over the dataset * open a `GradientTape()` scopeï¼štensorflowçš„æ¢¯åº¦APIï¼Œç”¨äºç»™å®šlossè®¡ç®—æ¢¯åº¦ * Inside this scopeï¼šforward passï¼Œcompute loss * Outside the scopeï¼šretrieve the gradients * use optimizer to update the gradientsï¼š`optimizer.apply_gradients`ï¼Œä½¿ç”¨è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦æ¥æ›´æ–°å¯¹åº”çš„variable æ —å­ğŸŒ° 12345678910111213141516171819202122232425262728293031323334353637383940# modelmodel = keras.Model(inputs=inputs, outputs=outputs)optimizer = keras.optimizers.SGD(learning_rate=1e-3)loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)# datatrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)# iter epochsepochs = 2for epoch in range(epochs): print("\nStart of epoch %d" % (epoch,)) # iter batches for step, (x_batch_train, y_batch_train) in enumerate(train_dataset): # Open a GradientTape to record the operations run # during the forward pass, which enables auto-differentiation. with tf.GradientTape() as tape: # Run the forward pass of the layer. logits = model(x_batch_train, training=True) # Logits for this minibatch # Compute the loss value for this minibatch. loss_value = loss_fn(y_batch_train, logits) # automatically retrieve the gradients of the trainable variables with respect to the loss grads = tape.gradient(loss_value, model.trainable_weights) # Run one step of gradient descent by updating the value of the variables to minimize the loss. optimizer.apply_gradients(zip(grads, model.trainable_weights)) # Log every 200 batches. if step % 200 == 0: print( "Training loss (for one batch) at step %d: %.4f" % (step, float(loss_value)) ) print("Seen so far: %s samples" % ((step + 1) * 64))]]></content>
      <tags>
        <tag>keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencvåº“å‡½æ•°]]></title>
    <url>%2F2019%2F08%2F11%2Fopencv%E5%BA%93%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[1. imshowæœ‰æ—¶å€™imshowçš„å›¾ç‰‡ä¼šæ˜¾ç¤ºçš„å’ŒåŸå›¾ä¸ä¸€æ ·ï¼Œè¦æŸ¥çœ‹readè¿›æ¥çš„æ•°æ®æ ¼å¼ï¼Œimshowä¼šæ ¹æ®è¯»å…¥çš„æ•°æ®æ ¼å¼è‡ªåŠ¨è¿›è¡Œå½’ä¸€åŒ–ï¼Œæ˜ å°„åˆ°0-255ã€‚ å¦‚æœimageæ˜¯é»˜è®¤çš„8-bit unsignedï¼ˆ0-255ï¼‰ï¼Œä¸åšå¤„ç†ã€‚ å¦‚æœimageæ˜¯16-bit unsignedï¼ˆ0-65535ï¼‰æˆ–è€…32-bit integerï¼ˆï¼Ÿï¼Ÿè´¼å¤§ï¼‰ï¼Œåƒç´ å€¼é™¤ä»¥256ï¼Œ[0,255*256]å½’ä¸€åŒ–åˆ°[0ï¼Œ255]ã€‚ å¦‚æœimageæ˜¯32-bit floatï¼Œåƒç´ å€¼ä¹˜ä»¥255ï¼Œ[0,1]å½’ä¸€åŒ–åˆ°[0ï¼Œ255]ã€‚ 2. imwriteé€šå¸¸imwriteæŠŠæ‰€æœ‰æ•°æ®éƒ½å¼ºåˆ¶è½¬æ¢æˆucharï¼ˆ0-255ï¼‰ã€‚]]></content>
  </entry>
  <entry>
    <title><![CDATA[beautifulsoup saving file]]></title>
    <url>%2F2019%2F06%2F11%2Fbeautifulsoup-saving-file%2F</url>
    <content type="text"><![CDATA[æœ€è¿‘ç”¨bs4å¤„ç†xmlæ–‡ä»¶ï¼Œé‡åˆ°äº†ä¸€ä¸ªåœ¨çˆ¬è™«æ—¶å€™ä»æœªæ€è€ƒè¿‡çš„é—®é¢˜â€”â€” ä¿®æ­£ä»xmlæ–‡ä»¶ä¸­è§£æå‡ºçš„æ–‡ä»¶æ ‘ï¼Œå¹¶å°†changesä¿å­˜åˆ°åŸæ¥çš„xmlæ–‡ä»¶ä¸­ã€‚ æˆ‘ä¸€ç›´åœ¨beautifulsoupçš„æ‰‹å†Œä¸­å»å¯»æ‰¾åº“å‡½æ•°ï¼Œå®é™…åªéœ€è¦ç®€å•çš„æ–‡ä»¶è¯»å†™æ“ä½œï¼š 123456789from bs4 import BeautifulSoupsoup = BeautifulSoup(open('test.xml'), 'xml')add = BeautifulSoup("&lt;a&gt;Foo&lt;/a&gt;", 'xml')soup.orderlist.append(add)print(soup.prettify())f = open('test.xml', 'w')f.write(str(soup))f.close() é™„ä¸€ä¸ªç®€å•xmlæ–‡ä»¶ç”¨æ¥å®éªŒï¼š 1234567891011121314&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;orderlist&gt;&lt;order&gt;&lt;customer&gt;å§“å1&lt;/customer&gt;&lt;phone&gt;ç”µè¯1&lt;/phone&gt;&lt;address&gt;åœ°å€1&lt;/address&gt;&lt;count&gt;ç‚¹é¤æ¬¡æ•°1&lt;/count&gt;&lt;/order&gt;&lt;order&gt;&lt;customer&gt;å§“å2&lt;/customer&gt;&lt;phone&gt;ç”µè¯2&lt;/phone&gt;&lt;address&gt;åœ°å€2&lt;/address&gt;&lt;count&gt;ç‚¹é¤æ¬¡æ•°2&lt;/count&gt;&lt;/order&gt;]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh visualization]]></title>
    <url>%2F2018%2F12%2F29%2Fssh-visualization%2F</url>
    <content type="text"><![CDATA[ssh boocax@192.168.1.100 å¯†ç ï¼šrobot123 echo $ROS_MASTER_URI æŸ¥çœ‹ç«¯å£å·11311 å°è½¦ç«¯ï¼š export ROS_MASTER_URI=http://192.168.1.100:11311 export ROS_IP=192.168.1.100 è™šæ‹Ÿæœºç«¯ï¼š export ROS_MASTER_URI=http://192.168.1.100:11311 export ROS_IP=172.16.128.142 # navè¿œç¨‹å¼€å¯ä¸‰ä¸ªç»ˆç«¯ï¼ˆä»£ç é‡æ„ä»¥å‰ï¼‰ï¼š move_base: roslaunch teleop_twist_joy real_nav.launch mapserver: rosrun map_server map_server catkin_ws2/src/patrol/map/p1.yaml amcl: roslaunch patrol real_loc.launch æœ¬åœ°å¯è§†åŒ–ï¼šrvizï¼rqt_graph / rosservice call /rostopic pub å…¨å±€å®šä½ï¼š 1rosservice call /global_localization "&#123;&#125;" è®¾ç½®å¯¼èˆªç›®æ ‡ç‚¹ï¼š 12345// ç›¸å¯¹base_linkåæ ‡ç³»rostopic pub -1 /navigation_simple/goal geometry_msgs/PoseStamped '&#123; header: &#123; frame_id: "base_link" &#125;, pose: &#123; position: &#123; x: 0.5, y: 0.0, z: 0 &#125;, orientation: &#123; x: 0, y: 0, z: 0, w: 1 &#125; &#125; &#125;'// ç›¸å¯¹mapåæ ‡ç³»rostopic pub -1 /navigation_simple/goal geometry_msgs/PoseStamped '&#123; header: &#123; frame_id: "map" &#125;, pose: &#123; position: &#123; x: 5, y: 0.0, z: 0 &#125;, orientation: &#123; x: 0, y: 0, z: 0, w: 1 &#125; &#125; &#125;' æ³¨æ„-1ï¼Œå¦åˆ™å¾ªç¯å‘å¸ƒã€‚ # å¾€å›å¤‡ä»½ï¼š 1scp -r boocax@192.168.1.100:/home/boocax/catkin_ws2019 bkp/]]></content>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Occupancy Grid Map]]></title>
    <url>%2F2018%2F12%2F04%2FOccupancy-Grid-Map%2F</url>
    <content type="text"><![CDATA[to be completedâ€¦ Inverse Sensor Model Incremental Updating]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[object tracking]]></title>
    <url>%2F2018%2F12%2F03%2Fobject-tracking%2F</url>
    <content type="text"><![CDATA[èŒƒå›´é™å®šï¼šè¿‡æ»¤æ‰è¾ƒè¿œèŒƒå›´çš„ç‚¹äº‘æ•°æ® èšç±»ï¼šK-Meansï¼æ¬§å¼èšç±»ï¼Œå› ä¸ºå‰è€…éœ€è¦è®¾å®šKï¼Œæ•…ä½¿ç”¨åè€…ã€‚ D(p_i, p_{i+1}) = \sqrt{r_i^2 + r_{i+1}^2 - 2r_ir_{i+1}cos(\varphi_{i+1} - \varphi_i)}å¦‚æœè¿ç»­æ‰«æç‚¹ä¹‹é—´çš„è·ç¦»å°äºä¸€ä¸ªé˜ˆå€¼$D_t$ï¼Œé‚£ä¹ˆè¿™ä¸¤ä¸ªç‚¹è¢«è®¤ä¸ºå±äºåŒä¸€ä¸ªå¯¹è±¡ã€‚è¿™ä¸ªé˜ˆå€¼æ˜¯æ ¹æ®å½“å‰å‚è€ƒç‚¹çš„è·ç¦»åŠ¨æ€è°ƒæ•´çš„ã€‚ D_t = D_0 + a*r_i*sin(\Delta \varphi) è¿åŠ¨ç›®æ ‡ç‰¹å¾æå–ï¼šï¼ˆä¸­å¿ƒåæ ‡ï¼Œé•¿ï¼å®½ï¼åŠå¾„ï¼Œåå°„å¼ºåº¦ï¼‰ ç”±ä¸Šä¸€æ—¶åˆ»çš„ä½ç½®é€Ÿåº¦è®¾ç½®ROIï¼š åŸºäºå±€éƒ¨åŒ¹é…ï¼šé€šè¿‡ç›¸ä¼¼åº¦è®¡ç®—é€‰å–å“åº”å€¼æœ€é«˜çš„ç›®æ ‡ åŸºäºåˆ†ç±»å™¨ï¼šåŠ¨æ€ç›®æ ‡å·²çŸ¥ï¼ˆäººè…¿ï¼‰ï¼Œé‡‡é›†æ­£è´Ÿæ ·æœ¬ï¼Œæ„é€ åˆ†ç±»å™¨ï¼Œ å¡å°”æ›¼æ»¤æ³¢ï¼š]]></content>
      <tags>
        <tag>extensions for slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlibçš„colormap]]></title>
    <url>%2F2018%2F11%2F29%2Fmatplotlib%E7%9A%84colormap%2F</url>
    <content type="text"><![CDATA[ç”¨pltçš„imshowç”»å›¾ï¼Œæ€»æ˜¯æ‰¾ä¸åˆ°å¿ƒä»ªçš„colorbarï¼Œå¯ä»¥è‡ªå®šä¹‰ï¼š åœ¨åŸæœ‰cmapåŸºç¡€ä¸Šè‡ªå®šä¹‰ï¼š 12345colorbar = plt.get_cmap('Greys')(range(180))cm = LinearSegmentedColormap.from_list(name="grey_cm", colors=colorbar)plt.register_cmap(cmap=cm)plt.imshow(map2d.data, cmap='grey_cm') defineä¸€ä¸ªæ–°çš„cmapï¼š 123456def colormap(): colors = ['#FFFFFF', '#9ff113', '#5fbb44', '#f5f329', '#e50b32'] return colors.ListedColormap(colors, 'my_cmap')my_cmap = colormap()plt.imshow(map2d.data, cmap=my_cmap)]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mpv-video-cutter]]></title>
    <url>%2F2018%2F11%2F26%2Fmpv-video-cutter%2F</url>
    <content type="text"><![CDATA[mpvçš„å°æ’ä»¶ï¼Œèƒ½å¤Ÿä¸€é”®ï¼ˆä¸‰é”®ï¼‰å‰ªè¾‘ã€‚ å·¥ç¨‹åœ°å€ï¼šhttps://github.com/rushmj/mpv-video-cutter step1ï¼šæŠŠc_concat.shå’Œcutter.luaä¸¤ä¸ªæ–‡ä»¶å¤åˆ¶åˆ°~/.config/mpv/scripts/ç›®å½•ä¸‹ã€‚ step2ï¼šç»™c_concat.shè„šæœ¬æ·»åŠ æ‰§è¡Œæƒé™ã€‚ step3ï¼šç”¨å‘½ä»¤è¡Œæ‰“å¼€æ–‡ä»¶ï¼Œc-c-oåœ¨åŸç›®å½•ä¸‹ç”Ÿæˆå‰ªè¾‘æ–‡ä»¶ã€‚]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[problems with ROS]]></title>
    <url>%2F2018%2F11%2F22%2Fproblems-with-ROS%2F</url>
    <content type="text"><![CDATA[[WARN] Detected jump back in time of 5.51266s. Clearing TF buffer. æ‰‹åŠ¨å»ºå›¾çš„æ—¶å€™ï¼Œæ—¶ä¸æ—¶çš„å°±è·³å‡ºæ¥è¿™ä¸ªï¼Œç„¶åå°è½¦è·³å˜åˆ°åˆå§‹ä½ç½®ï¼Œè€Œä¸”è¿˜æ˜¯æ ¹æ®TF bufferå›æº¯å›å»çš„ï¼ŒçœŸé«˜çº§ã€‚ã€‚ã€‚ æ’æŸ¥åŸå› å‘ç°ç«Ÿç„¶æ˜¯å¿˜è®°è¿è¡Œroscoreäº†ï¼Œmmpã€‚ [rosrun] Couldn&#39;t find executable named patrol.py below /home/carrol/catkin_ws/src/patrol åŸå› å¦‚æç¤ºï¼Œpythonæ˜¯è„šæœ¬æ‰§è¡Œï¼Œè¦æ·»åŠ å¯æ‰§è¡Œæƒé™ã€‚ error: â€˜arrayâ€™ is not a member of â€˜stdâ€™ ç¼–è¯‘å¯¼èˆªåŒ…æ—¶åå¤å‡ºç°è¿™ä¸ªé”™è¯¯ï¼Œå› ä¸ºcmakeç‰ˆæœ¬æ¯”è¾ƒä½ï¼ˆ2.8ï¼‰ï¼Œä¸ä¼šè‡ªåŠ¨æ‰¾c++11ï¼Œè§£å†³åŠæ³•åœ¨å¯¹åº”packageçš„cmakeæ–‡ä»¶ä¸­æ·»åŠ c++å£°æ˜ï¼šadd_definitions(-std=c++11) åŒæ ·çš„é”™è¯¯catkin_makeæ—¶é‡å¤å‡ºç°ï¼Œæˆ‘è¿˜ä»¥ä¸ºé—®é¢˜æ²¡è§£å†³ï¼š åˆ é™¤buildæ–‡ä»¶å¤¹ä¸­å¯¹åº”åŒ…ï¼Œå†è¿›è¡Œcatkin_makeã€‚å¦‚æœåˆ é™¤äº†æŸä¸ªåŒ…ï¼Œè¿˜è¦åˆ é™¤develæ–‡ä»¶å¤¹å†ç¼–è¯‘ã€‚ cmake warning conflicts with Anacondaï¼š ç¼–è¯‘åˆ°æœ€åä¼šå¡æ­»ï¼Œé”™è¯¯å…·ä½“å•¥æ„æ€æˆ‘ä¹Ÿæ²¡å¼„æ˜ç™½ï¼Œç²—æš´è§£å†³äº†ï¼Œå°†ç³»ç»Ÿç¯å¢ƒå˜é‡é‡Œé¢çš„anaconda pathæš‚æ—¶å±è”½ï¼Œé¦–å…ˆæŸ¥çœ‹ç¯å¢ƒå˜é‡ï¼šecho $PATHï¼Œç„¶åè¿”å›ç»“æœï¼š /home/[username]/anaconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games ç„¶ååœ¨å½“å‰å‘½ä»¤è¡Œæ‰§è¡Œï¼šexport PATH=&quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games&quot; c++: internal compiler error: Killed (program cc1plus) è™šæ‹Ÿæœºå†…å­˜ä¸è¶³ã€‚ undefined error with CONSOLE_BRIDGE_logError/CONSOLE_BRIDGE_logWarn å®‰è£…å¹¶ç¼–è¯‘console_bridgeåŒ…ï¼Œæ³¨æ„build instructionsï¼š 12345git clone git://github.com/ros/console_bridge.gitcd console_bridgecmake .makesudo make install there are no arguments to â€˜logDebugâ€™ that depend on a template parameter, so a declaration of â€˜logDebugâ€™ must be available [-fpermissive] å‚è€ƒï¼ˆReferenceï¼‰ï¼Œè¿˜æ˜¯ä¸Šé¢çš„é—®é¢˜ï¼Œ console_bridgeçš„APIå˜äº†ï¼Œå°†logDebugæ”¹æˆCONSOLE_BRIDGE_logDebugå°±è¡Œäº†ã€‚ running environmentç›¸å…³åŒ…çš„ç¼ºå¤±å’Œå®‰è£…ï¼š åœ¨å®˜ç½‘æŸ¥æ‰¾ç›¸å…³åŒ…å’Œä¾èµ–ï¼Œç„¶åæ‰§è¡Œï¼š 12345# installsudo dpkg -i è½¯ä»¶åŒ…å.deb# uninstallsudo apt-get remove è½¯ä»¶åŒ…åç§°]]></content>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[amcl]]></title>
    <url>%2F2018%2F11%2F16%2Famcl%2F</url>
    <content type="text"><![CDATA[æ»¤æ³¢ï¼šæœºå™¨äººä»å·²çŸ¥ç‚¹$x_0$å¼€å§‹è¿åŠ¨ï¼Œé‡Œç¨‹è®¡è¯¯å·®é€æ¸ç´¯ç§¯ï¼Œä½ç½®ä¸ç¡®å®šæ€§å°†è¶Šæ¥è¶Šå¤§ï¼ˆ$x_1, x_2$ï¼‰ã€‚å› æ­¤éœ€è¦å€ŸåŠ©å¤–éƒ¨ç¯å¢ƒä¿¡æ¯å¯¹è‡ªå·±è¿›è¡Œå®šä½ï¼Œäºæ˜¯å¼•å…¥æµ‹é‡å€¼$d$ï¼Œè®¡ç®—å‡ºå½“å‰ä½ç½®$x_2^{â€˜}$ï¼Œå†ç»“åˆé¢„æµ‹å€¼$x_2$ï¼Œå¾—åˆ°ä¸€ä¸ªçŸ«æ­£ä½ç½®$x_2^{â€˜â€™}$ï¼Œä½¿å…¶ä¸ç¡®å®šæ€§é™åˆ°æœ€å°ã€‚ è´å¶æ–¯æ»¤æ³¢ï¼š$p(x|z) = \frac{p(z|x)p(x)}{p(z)}$å…ˆéªŒï¼š$p(x_t|u_t, x_{t-1})$ï¼Œé€šè¿‡é¢„æµ‹æ–¹ç¨‹å¾—åˆ° ä¼¼ç„¶ï¼š$p(z_t| x_t)$ï¼Œé€šè¿‡æµ‹é‡æ–¹ç¨‹å¾—åˆ° åéªŒï¼š$p(x_t|z_t)$ï¼Œé€šè¿‡è´å¶æ–¯æ–¹ç¨‹å¾—åˆ° å¯¹äºä¸€èˆ¬çš„éçº¿æ€§ã€éé«˜æ–¯ç³»ç»Ÿï¼Œå¾ˆéš¾é€šè¿‡ä¸Šè¿°æ–¹æ³•å¾—åˆ°åéªŒæ¦‚ç‡çš„è§£æè§£ã€‚ è’™ç‰¹å¡æ´›é‡‡æ ·ï¼šå‡è®¾èƒ½ä»ä¸€ä¸ªç›®æ ‡åˆ†å¸ƒ$p(x)$è·å¾—ä¸€ç³»åˆ—æ ·æœ¬$x_1, x2, â€¦, x_N$ï¼Œé‚£ä¹ˆå°±èƒ½åˆ©ç”¨è¿™äº›æ ·æœ¬å»ä¼°è®¡è¿™ä¸ªåˆ†å¸ƒçš„æŸäº›å‡½æ•°çš„æœŸæœ›å€¼ã€‚ E(f(x)) = \int_a^{b}f(x)p(x)dx \approx\frac{f(x_1) + f(x_2) + ... + f(x_N)}{N}è’™ç‰¹å¡æ´›é‡‡æ ·çš„æ ¸å¿ƒæ€æƒ³å°±æ˜¯ç”¨å‡å€¼æ¥ä»£æ›¿ç§¯åˆ†ã€‚ å‡è®¾å¯ä»¥ä»åéªŒæ¦‚ç‡ä¸­é‡‡æ ·åˆ°Nä¸ªæ ·æœ¬ï¼Œé‚£ä¹ˆåéªŒæ¦‚ç‡å¯ä»¥è¡¨ç¤ºä¸ºï¼š \hat p(x_t|z_{1:t}) = \frac{1}{N} \sum_{i=1}^{N}\delta(x_n - x_n^{i}) \approx p(x_t|z_{1:t})ç²’å­æ»¤æ³¢ï¼š \begin{align} E(f(x)) & \approx \int f(x_n) \hat p(x_t|z_{1:t}) dx \nonumber\\ & = \frac{1}{N}\sum_N f(x_n) \delta(x_n - x_n^{i})\nonumber\\ & = \frac{1}{N}\sum_N f(x_n^{i}) \nonumber \end{align}ç”¨é‡‡æ ·ç²’å­ï¼ˆæœä»åéªŒæ¦‚ç‡ï¼‰çš„çŠ¶æ€å€¼ç›´æ¥å¹³å‡ä½œä¸ºæœŸæœ›å€¼ï¼Œè¿™å°±æ˜¯ç²’å­æ»¤æ³¢ã€‚ MCLï¼šè’™ç‰¹å¡æ´›å®šä½ï¼ç²’å­æ»¤æ³¢å®šä½ Randomly generate a bunch of particles Predict next state of the particles Update the weighting of the particles based on the measurement. Resampleï¼šDiscard highly improbable particle and replace them with copies of the more probable particles. This leads to a new particle set with uniform importance weights, but with an increased number of particles near the three likely places. Compute the weighted mean and covariance of the set of particles to get a state estimate. æƒå€¼é€€åŒ–ï¼šå¦‚æœä»»ç”±ç²’å­æƒå€¼å¢é•¿ï¼Œåªæœ‰å°‘æ•°ç²’å­çš„æƒå€¼è¾ƒå¤§ï¼Œå…¶ä½™ç²’å­çš„æƒå€¼å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œå˜æˆæ— æ•ˆç²’å­ï¼Œå› æ­¤éœ€è¦å¼•å…¥é‡é‡‡æ ·ã€‚é‡‡ç”¨$N_{eff}$è¡¡é‡ç²’å­æƒå€¼çš„é€€åŒ–ç¨‹åº¦ã€‚ N_{eff} \approx \hat{N_{eff}} = \frac{1}{\sum_N (w_k^{i})^2}ç²’å­å¤šæ ·æ€§ï¼šé€šå¸¸æˆ‘ä»¬ä¼šèˆå¼ƒæƒå€¼è¾ƒå°çš„ç²’å­ï¼Œä»£ä¹‹ä»¥æƒå€¼è¾ƒå¤§çš„ç²’å­ã€‚è¿™æ ·ä¼šå¯¼è‡´æƒå€¼å°çš„ç²’å­é€æ¸ç»ç§ï¼Œç²’å­ç¾¤å¤šæ ·æ€§å‡å¼±ï¼Œä»è€Œä¸è¶³ä»¥è¿‘ä¼¼è¡¨å¾åéªŒå¯†åº¦ã€‚ é‡è¦æ€§é‡‡æ ·å®é™…ä¸ŠåéªŒæ¦‚ç‡å¹¶ä¸çŸ¥é“ï¼Œè°ˆä½•é‡‡æ ·ï¼ˆ$x_n^i$ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥ä»ä¸€ä¸ªå·²çŸ¥çš„åˆ†å¸ƒ$q(x|z)$é‡Œæ¥é‡‡æ ·ï¼Œé—´æ¥å¾—åˆ°æ»¤æ³¢å€¼ã€‚ \begin{align} E(f(x_k))& = \int f(x) \frac{p(x|z)}{q(x|z)} q(x|z) dx \nonumber\\ & = \frac{E_{q(x|z)}W_k(x_k)f(x_k)}{E_{q(x|z)}W_k(x_k)}\nonumber\\ & \approx \frac{\frac{1}{N} \sum_{i=1}^N W_k({x_k^{i})f(x_k^{i})}}{\frac{1}{N} \sum_{i=1}^N W_k({x_k^{i}})}\nonumber\\ & = \sum_N \hat W_k(x_k^i)f(x_k^i) \nonumber \end{align}ç›¸æ¯”è¾ƒäºåŸå§‹çš„å‡å€¼è¡¨ç¤ºï¼Œå˜æˆäº†åŠ æƒå¹³å‡å€¼ã€‚ä¸åŒç²’å­æ‹¥æœ‰äº†ä¸åŒçš„æƒé‡ã€‚ \hat W_k(x_k^i) = \frac{W_k(x_k^i)}{\sum_N W_k(x_k^i)}\\ W_k (x_k) \propto \frac{p(x_k|z_{1:k})}{q(x_k|z_{1:k})}å·²çŸ¥çš„$q$åˆ†å¸ƒå«åšé‡è¦æ€§æ¦‚ç‡å¯†åº¦å‡½æ•°ã€‚ é€’æ¨ç®—æ³•ï¼šåºè´¯é‡è¦æ€§é‡‡æ · \{x_k^i, w_k^i\} = SIS(\{x_{k-1}, w_{k-1}\})_{i=1}^N, y_k)é¦–å…ˆå‡è®¾é‡è¦æ€§åˆ†å¸ƒ$q(x|z)$æ»¡è¶³ï¼š q(x_k | x_{0:k-1}, y_{1:k}) = q(x_k|x_{k-1}, y_k)å³åªå’Œå‰ä¸€æ—¶åˆ»çš„çŠ¶æ€$x_{k-1}$å’Œæµ‹é‡$y_k$æœ‰å…³ã€‚äºæ˜¯æœ‰ï¼š w_k^i \approx w_{k-1}^i \frac{p(y_k|x_k^i)p(x_k^i|x_{k-1}^i)}{q(x_k^i|x_{k-1}^i, y_k)}ä¼ªä»£ç ï¼š 1234567For i=1:N (1)é‡‡æ ·ï¼šå¼1 (2)æƒå€¼æ›´æ–°ï¼šå¼2End Foræƒå€¼å½’ä¸€åŒ–åŠ æƒå¹³å‡å¾—åˆ°ç²’å­æ»¤æ³¢å€¼ï¼Œä¹Ÿå°±æ˜¯å½“å‰çŠ¶æ€çš„ä¼°è®¡å€¼é‡é‡‡æ · é‡é‡‡æ ·æ—¢ç„¶æƒé‡å°çš„é‚£äº›ç²’å­ä¸èµ·ä½œç”¨äº†ï¼Œé‚£å°±ä¸è¦äº†ã€‚ä¸ºäº†ä¿æŒç²’å­æ•°ç›®ä¸å˜ï¼Œå°±è¦è¡¥å……æ–°ç²’å­ï¼Œæœ€ç®€å•çš„åŠæ³•å°±æ˜¯å¤åˆ¶æƒé‡å¤§çš„ç²’å­ã€‚ç”¨$x_k^i$è¡¨ç¤ºkæ—¶åˆ»çš„ç²’å­ï¼Œ$x_k^j$è¡¨ç¤ºé‡é‡‡æ ·ä»¥åçš„ç²’å­ï¼Œé‚£ä¹ˆï¼š \tilde p (x_k|y_{1:k}) = \sum_N \frac{1}{N}\delta(x_k - x_k^j) = \sum_N \frac{n_i}{N}\delta(x_k - x_i^j)æ€»çš„æ¥è¯´ï¼Œæ–°ç²’å­æŒ‰ç…§æƒé‡æ¯”ä¾‹æ¥è¡¥å……ï¼Œç®—æ³•æµç¨‹ä¸ºï¼š 12345678è®¡ç®—æ¦‚ç‡ç´¯ç§¯å’Œwcum(N)ç”¨[0,1]ä¹‹é—´çš„å‡åŒ€åˆ†å¸ƒéšæœºé‡‡æ ·Nä¸ªå€¼u(N)for i in 1:N: k = 1 while u(i)&lt;wcum(k): k += 1 end while resample(i) = k SIRæ»¤æ³¢å™¨ï¼ˆSampling Importance Resampling Filter ï¼‰é€‰å–ç‰¹å®šçš„é‡è¦æ€§æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼š q(x_k^i|x_{k-1}^i, y_k) = p(x_k^i|x_{k-1}^i)äºæ˜¯æƒé‡æ›´æ–°å…¬å¼å¯ä»¥ç®€åŒ–ï¼š w_k^i \propto w_{k-1}^i \frac{p(z|x)p(x|x_{k-1})}{q(x|x_{k-1})}ç”±äºé‡é‡‡æ ·ä»¥åï¼Œç²’å­åˆ†å¸ƒæ›´æ–°ï¼Œæƒå€¼ç»Ÿä¸€ä¸º$\frac{1}{N}$ï¼Œäºæ˜¯æƒé‡æ›´æ–°å…¬å¼è¿›ä¸€æ­¥ç®€åŒ–ï¼š w_k^i \propto p(z_k|x_k^i)æ ¹æ®æµ‹é‡æ–¹ç¨‹å¯çŸ¥ï¼Œä¸Šé¢è¿™ä¸ªæ¦‚ç‡å°±æ˜¯ä»¥çœŸå®æµ‹é‡å€¼ä¸ºå‡å€¼ï¼Œä»¥å™ªå£°æ–¹å·®ä¸ºæ–¹å·®çš„é«˜æ–¯åˆ†å¸ƒã€‚ æ­¤ç®—æ³•ä¸­çš„é‡‡æ ·ï¼Œå¹¶æ²¡æœ‰åŠ å…¥æµ‹é‡$z_k$ï¼Œåªå‡­å…ˆéªŒçŸ¥è¯†$p(x_k|x_{k-1})$ï¼Œè™½ç„¶ç®€å•æ˜“ç”¨ï¼Œä½†æ˜¯å­˜åœ¨æ•ˆç‡ä¸é«˜å’Œå¯¹å¥‡å¼‚ç‚¹(outliers)æ•æ„Ÿçš„é—®é¢˜ã€‚ AMCLMCLç®—æ³•èƒ½å¤Ÿç”¨äºå…¨å±€å®šä½ï¼Œä½†æ˜¯æ— æ³•ä»æœºå™¨äººç»‘æ¶æˆ–å…¨å±€å®šä½å¤±è´¥ä¸­æ¢å¤è¿‡æ¥ï¼Œå› ä¸ºéšç€ä½ç½®è¢«è·å–ï¼Œå…¶ä»–åœ°æ–¹çš„ä¸æ­£ç¡®ç²’å­ä¼šé€æ¸æ¶ˆå¤±ã€‚ç¨³å®šçŠ¶æ€ä¸‹ï¼Œç²’å­åªâ€œç”Ÿå­˜â€åœ¨ä¸€ä¸ªå•ä¸€çš„å§¿æ€é™„è¿‘ï¼Œå¦‚æœè¿™ä¸ªå§¿æ€æ°å¥½ä¸æ­£ç¡®ï¼ˆåœ¨é‡é‡‡æ ·æ­¥éª¤ä¸­å¯èƒ½æ„å¤–çš„ä¸¢å¼ƒæ‰€æœ‰æ­£ç¡®ä½å§¿é™„è¿‘çš„ç²’å­ï¼‰ï¼Œç®—æ³•å°±æ— æ³•æ¢å¤ã€‚ AMCLå°±æ˜¯ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼šç»“åˆäº†è‡ªé€‚åº”ï¼ˆAugmented_MCLï¼‰å’Œåº“å°”è´å…‹-è±ä¸å‹’æ•£åº¦é‡‡æ ·ï¼ˆKLD_Sampling_MCLï¼‰ Augmented_MCLï¼šåœ¨æœºå™¨äººé­åˆ°ç»‘æ¶çš„æ—¶å€™ï¼Œå®ƒä¼šåœ¨å‘ç°ç²’å­ä»¬çš„å¹³å‡åˆ†æ•°çªç„¶é™ä½äº†ï¼Œè¿™æ„å‘³ç€æ­£ç¡®çš„ç²’å­åœ¨æŸæ¬¡è¿­ä»£ä¸­è¢«æŠ›å¼ƒäº†ï¼Œæ­¤æ—¶ä¼šéšæœºçš„å…¨å±€æ³¨å…¥ç²’å­ï¼ˆinjection of random particlesï¼‰ã€‚ KLD_Sampling_MCLï¼šåŠ¨æ€è°ƒæ•´ç²’å­æ•°ï¼Œå½“æœºå™¨äººå®šä½å·®ä¸å¤šå¾—åˆ°äº†çš„æ—¶å€™ï¼Œç²’å­éƒ½é›†ä¸­åœ¨ä¸€å—äº†ï¼Œå°±æ²¡å¿…è¦ç»´æŒè¿™ä¹ˆå¤šçš„ç²’å­äº†â€”â€”åœ¨æ …æ ¼åœ°å›¾ä¸­ï¼Œçœ‹ç²’å­å äº†å¤šå°‘æ …æ ¼ã€‚å å¾—å¤šï¼Œè¯´æ˜ç²’å­å¾ˆåˆ†æ•£ï¼Œåœ¨æ¯æ¬¡è¿­ä»£é‡é‡‡æ ·çš„æ—¶å€™ï¼Œå…è®¸ç²’å­æ•°é‡çš„ä¸Šé™é«˜ä¸€äº›ã€‚å å¾—å°‘ï¼Œè¯´æ˜ç²’å­éƒ½å·²ç»é›†ä¸­äº†ï¼Œé‚£å°±å°†ä¸Šé™è®¾ä½ã€‚ ç®—æ³•æµç¨‹ä¸Šçœ‹ï¼Œaugmented_MCLç®—æ³•æœ€æ˜¾è‘—çš„åŒºåˆ«å°±æ˜¯å¼•å…¥äº†å››ä¸ªå‚æ•°ç”¨äºå¤±æ•ˆæ¢å¤ï¼š $w_{slow}$ï¼šé•¿æœŸä¼¼ç„¶å¹³å‡ä¼°è®¡ $w_{fast}$ï¼šçŸ­æœŸä¼¼ç„¶å¹³å‡ä¼°è®¡ $\alpha_{slow}$ï¼šé•¿æœŸæŒ‡æ•°æ»¤æ³¢å™¨è¡°å‡ç‡ $\alpha_{fast}$ï¼šçŸ­æœŸæŒ‡æ•°æ»¤æ³¢å™¨è¡°å‡ç‡ å¤±æ•ˆæ¢å¤çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šæµ‹é‡ä¼¼ç„¶çš„ä¸€ä¸ªçªç„¶è¡°å‡ï¼ˆçŸ­æœŸä¼¼ç„¶åŠ£äºé•¿æœŸä¼¼ç„¶ï¼‰è±¡å¾ç€ç²’å­è´¨é‡çš„ä¸‹é™ï¼Œè¿™å°†å¼•èµ·éšæœºé‡‡æ ·æ•°ç›®çš„å¢åŠ ã€‚ $w_{avg}$è®¡ç®—äº†ç²’å­çš„å¹³å‡æƒé‡ï¼Œå½“ç²’å­è´¨é‡ä¸‹é™æ—¶ï¼Œå¹³å‡æƒé‡éšä¹‹ä¸‹é™ï¼Œ$w_{slow}ã€w_{fast}$ä¹Ÿä¼šéšä¹‹ä¸‹é™ï¼Œä½†æ˜¯æ˜¾ç„¶$w_{fast}$ä¸‹é™çš„é€Ÿåº¦è¦å¿«äº$w_{slow}$â€”â€”è¿™ç”±è¡°å‡ç‡å†³å®šï¼Œå› æ­¤éšæœºæ¦‚ç‡$p = 1 - \frac{w_{fast}}{w_{slow}}$ä¼šå¢å¤§ï¼Œéšæœºç²’å­æ•°ç›®å¢åŠ ã€‚è€Œå½“ç²’å­è´¨é‡æé«˜æ—¶ï¼Œç²’å­çŸ­æœŸæƒé‡è¦å¥½äºé•¿æœŸï¼Œéšæœºæ¦‚ç‡å°äº0ï¼Œä¸ç”Ÿæˆéšæœºç²’å­ã€‚ ROS amclå‚æ•°è§£æAugmented_MCLï¼š &lt;param name=&quot;recovery_alpha_slow&quot; value=&quot;0.0&quot;/&gt;ï¼šé»˜è®¤0ï¼ˆMCLï¼‰ï¼Œæˆ‘çš„0.001ã€‚ &lt;param name=&quot;recovery_alpha_fast&quot; value=&quot;0.0&quot;/&gt;ï¼šé»˜è®¤0ï¼ˆMCLï¼‰ï¼Œæˆ‘çš„0.8ã€‚ åœ¨rvizé‡Œé€šè¿‡2D Pose EstimateæŒ‰é’®ç§»åŠ¨æœºå™¨äººæ¥è§¦å‘ï¼Œæœºå™¨äººä½ç½®çªå˜åè¦è¿‡ä¸€ä¼šå„¿æ‰æ³¨å…¥éšæœºç²’å­ï¼Œå› ä¸ºæ¦‚ç‡æ˜¯æ¸å˜çš„ã€‚ KLDï¼š &lt;param name=&quot;kld_z&quot; value=&quot;0.99&quot;/&gt;ï¼š KLDé‡‡æ ·ä»¥æ¦‚ç‡$1-\deltaï¼ˆkld_zï¼‰$ç¡®å®šæ ·æœ¬æ•°ã€‚ &lt;param name=&quot;kld_err&quot; value=&quot;0.05&quot;/&gt;ï¼š çœŸå®çš„åéªŒä¸åŸºäºé‡‡æ ·çš„è¿‘ä¼¼ä¹‹é—´çš„è¯¯å·®ã€‚ åŠ¨æ€éšœç¢ç‰©ï¼šç¯å¢ƒä¸­çš„åŠ¨æ€ç‰©ä½“æ€»æ˜¯ä¼šè·å¾—æ¯”é™æ€éšœç¢ç‰©æ›´çŸ­çš„è¯»æ•°ï¼Œå› æ­¤å¯ä»¥æ ¹æ®è¿™æ ·çš„ä¸å¯¹ç§°æ€§å»é™¤å¼‚å¸¸å€¼ã€‚ é™æ€éšœç¢ç‰©åº”è¯¥æœä»ç¨³å®šçš„é«˜æ–¯åˆ†å¸ƒï¼Œä»¥è·ç¦»ä¼ æ„Ÿå™¨çš„çœŸå®è·ç¦»ä¸ºå‡å€¼ã€‚ æ‰«æåˆ°åŠ¨æ€ç›®æ ‡çš„beamåˆ™æœä»è¡°å‡åˆ†å¸ƒï¼Œ$-\eta e ^{-\lambda z}$ã€‚ laser_model_typeï¼šä½¿ç”¨beam modelæ—¶ä¼šç”¨åˆ°å››ä¸ªæ··åˆæƒé‡å‚æ•°z_hitï¼Œz_shortï¼Œz_maxå’Œz_randï¼Œä½¿ç”¨likelihood_field modelæ—¶ä½¿ç”¨ä¸¤ä¸ªz_hitå’Œz_randã€‚ laser_z_hitï¼šdefault=0.95ï¼Œä»¥çœŸå®å€¼ä¸ºå‡å€¼çš„å™ªå£°é«˜æ–¯åˆ†å¸ƒ laser_z_randï¼šdefualt=0.05ï¼Œéšæœºæµ‹é‡æƒé‡ï¼Œå‡åŒ€åˆ†å¸ƒ laser_z_shortï¼šdefault=0.1ï¼Œæ„å¤–å¯¹è±¡æƒé‡ï¼Œè¡°å‡åˆ†å¸ƒ laser_z_maxï¼šdefault=0.05ï¼Œæµ‹é‡å¤±è´¥æƒé‡ï¼Œ0/1åˆ†å¸ƒ åˆå§‹ä½å§¿ï¼š å¯ä»¥åœ¨rvizé‡Œé€šè¿‡2D Pose EstimateæŒ‰é’®è®¾å®šï¼ˆrvizä¼šå‘å¸ƒinitialPoseè¯é¢˜ï¼‰ã€‚ æˆ–è€…å†™åœ¨launchæ–‡ä»¶ä¸­ï¼š 123456&lt;param name="initial_pose_x" value="0.0"/&gt;&lt;param name="initial_pose_y" value="0.0"/&gt;&lt;param name="initial_pose_a" value="0.0"/&gt; &lt;param name="initial_cov_xx" value="0.25"/&gt;&lt;param name="initial_cov_yy" value="0.25"/&gt;&lt;param name="initial_cov_aa" value="(pi/12)*(pi/12)"/&gt; è°ƒç”¨å…¨å±€å®šä½æœåŠ¡ï¼š 1rosservice call /global_localization "&#123;&#125;" ä½å§¿éšæœºåˆå§‹åŒ–ï¼Œç²’å­æ´’æ»¡åœ°å›¾ï¼š transform_toleranceï¼š é»˜è®¤æ˜¯0.1secondsï¼Œå®˜æ–¹å®šä¹‰æ˜¯Time with which to post-date the transform that is published, to indicate that this transform is valid into the future. tfå˜æ¢å‘å¸ƒæ¨è¿Ÿçš„æ—¶é—´ï¼Œæ„æ€æ˜¯tfå˜æ¢åœ¨æœªæ¥è¿™æ®µæ—¶é—´å†…æ˜¯å¯ç”¨çš„ã€‚ ã€å­˜ç–‘ã€‘æˆ‘ä¸ªäººç†è§£tfçš„æ›´æ–°é¢‘ç‡åº”è¯¥è¶Šå¿«è¶Šå‡†ç¡®ï¼Œlaunchæ–‡ä»¶ä¸­æœ€å¼€å§‹è®¾å®šä¸º0.5ï¼Œä½†æ˜¯å®é™…ä¸Šæœºå™¨äººç§»åŠ¨é€Ÿåº¦è°ƒå¿«æ—¶ï¼Œä¼šæŠ¥é”™Costmap2DROS transform timeout...Could not get robot pose, cancelling reconfigurationï¼Œç„¶åæˆ‘è°ƒæ•´ä¸º1.5å°±ä¸æŠ¥é”™äº†ã€‚ ç›®å‰è®¾å®šä¸º1.0ï¼Œä»¿çœŸé‡Œè§‚æµ‹ä¸å‡ºå·®å¼‚ã€‚]]></content>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[recovery behavior]]></title>
    <url>%2F2018%2F11%2F16%2Frecovery-behavior%2F</url>
    <content type="text"><![CDATA[navigation stackçš„move_baseåŒ…ä¸­ä¸€ä¸ªæ’ä»¶ã€‚DWAçš„é€Ÿåº¦ç©ºé—´ä¸­å¦‚æœæ²¡æœ‰å¯è¡Œçš„é‡‡æ ·ç‚¹ï¼Œé‚£ä¹ˆæœºå™¨äººget stuckï¼Œè§¦å‘recoveryè¡Œä¸ºã€‚ recoveryè¡Œä¸ºçš„å®è´¨æ˜¯clear out spaceâ€”â€”è¯•å›¾ææ¸…æ¥šè‡ªå·±çš„å¤„å¢ƒï¼š é¦–å…ˆæœºå™¨äººä¼šæ¸…æ‰«åœ°å›¾â€”â€”conservative reset ç„¶ååŸåœ°æ—‹è½¬360åº¦ï¼Œåˆ·æ–°å¤„å¢ƒâ€”â€”clearing rotation å¦‚æœè¿˜æ˜¯å¯¼èˆªå¤±è´¥ï¼Œæœºå™¨äººä¼šæ›´åŠ æ¿€è¿›çš„æ¸…æ‰«åœ°å›¾â€”â€”aggressive reset ç„¶ååŸåœ°æ—‹è½¬360åº¦ï¼Œåˆ·æ–°å¤„å¢ƒâ€”â€”clearing rotation å¦‚æœä»æ—§å¤±è´¥â€”â€”mission impossible æºä»£ç åœ¨move_base.cppé‡Œé¢ï¼Œç»§æ‰¿äº†nav_coreçš„æ¥å£ï¼Œè®¾ç½®åœ¨move_base_params.yamlé…ç½®æ–‡ä»¶ä¸­ã€‚ nav_coreçš„recovery_behavior.hå°è£…äº†RecoveryBehaviorç±»ã€‚ move_baseä¸­åˆ›å»ºäº†åä¸ºâ€clear_costmap_recovery/ClearCostmapRecoveryâ€ã€â€rotate_recovery/RotateRecoveryâ€ã€â€clear_costmap_recovery/ClearCostmapRecoveryâ€çš„é»˜è®¤å¯¹è±¡ã€‚ move_baseçš„ä¸»ç¨‹åºæ˜¯ä¸€ä¸ªçŠ¶æ€æœºï¼Œcase CLEARINGå°±è°ƒç”¨RecoveryBehaviorçš„runBehavior()ã€‚]]></content>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dynamic window approach]]></title>
    <url>%2F2018%2F11%2F15%2Fdynamic-window-approach%2F</url>
    <content type="text"><![CDATA[åŠ¨æ€çª—å£ï¼š çª—å£æ¡†çš„æ˜¯é€Ÿåº¦ç©ºé—´çš„é‡‡æ ·ç‚¹$(v_t, w_t)$ï¼Œä¸€å¯¹$(v_t, w_t)$å°±ä»£è¡¨ä¸€æ®µè½¨è¿¹ï¼Œè½¨è¿¹é€šè¿‡æœºå™¨äººåº•ç›˜çš„è¿åŠ¨å­¦å»ºæ¨¡å¾—åˆ°ã€‚ 12345678910// base_local_plannerçš„simple_trajectory_generator.cpp// ç›´çº¿æ¨¡å‹double dt = (current_time - last_time).toSec();double delta_x = (vx * cos(theta) - vy * sin(theta)) * dt;double delta_y = (vx * sin(theta) + vy * cos(theta)) * dt;double delta_th = vth * dt;x += delta_x;y += delta_y;theta += delta_th; çª—å£çš„é€‰æ‹©ï¼š é€Ÿåº¦é™åˆ¶ (V, W) = \{v \in[v_{min}, v_{max}], w \in [w_{min}, w_{max}] \} åŠ é€Ÿåº¦é™åˆ¶ (V, W) = \left\{ \begin{array} & v \in[v_c - \dot{v}*\Delta t, v_c + \dot{v}*\Delta t], \\ w \in [w_c - \dot{w}*\Delta t, w_c + \dot{w}*\Delta t] \end{array} \right\} éšœç¢ç‰©åˆ¶åŠ¨é™åˆ¶ (V, W) = \left\{ v \leq \sqrt{2*dist(v,w)*\dot{v}}, w \leq \sqrt{2*dist(v,w)*\dot{w}} \right\}$dist(v,w)$è¡¨ç¤ºé‡‡æ ·ç‚¹$(v, w)$å¯¹åº”è½¨è¿¹ä¸Šç¦»éšœç¢ç‰©æœ€è¿‘çš„è·ç¦»ã€‚ ç¡®å®šçª—å£åè¿›è¡Œé‡‡æ ·ï¼Œå¯ä»¥å¾—åˆ°ä¸€ç³»åˆ—è½¨è¿¹ï¼š è½¨è¿¹çš„é€‰æ‹©ï¼š åŸå§‹è®ºæ–‡ä¸­é‡‡ç”¨è¯„ä»·å‡½æ•°ï¼š G(v,w) = \sigma [\alpha * heading(v, w) + \beta * dist(v,w) + \gamma * velocity(v,w)] æ–¹ä½è§’è¯„ä»·å‡½æ•°ï¼šé‡‡ç”¨å½“å‰é‡‡æ ·ç‚¹è®¾å®šä¸‹ï¼Œè¾¾åˆ°æ¨¡æ‹Ÿè½¨è¿¹æœ«ç«¯æ—¶æœºå™¨äººçš„æœå‘è§’ä¸ç›®æ ‡ç‚¹æœå‘è§’çš„å·®è·ã€‚ ç©ºéš™è¯„ä»·ï¼šå½“å‰æ¨¡æ‹Ÿè½¨è¿¹ä¸Šä¸æœ€è¿‘éšœç¢ç‰©ä¹‹é—´çš„è·ç¦»ã€‚ é€Ÿåº¦è¯„ä»·ï¼šé‡‡æ ·ç‚¹é€Ÿåº¦ä¸æœ€å¤§é€Ÿåº¦çš„å·®è·ã€‚ ä¸Šè¿°è¯„ä»·å‡½æ•°è¦è¿›è¡Œå½’ä¸€åŒ–ã€‚ ç®—æ³•æµç¨‹ï¼š 123456789101112131415161718BEGIN DWA(robotPose,robotGoal,robotModel) laserscan = readScanner() allowable_v = generateWindow(robotV, robotModel) allowable_w = generateWindow(robotW, robotModel) for each v in allowable_v for each w in allowable_w dist = find_dist(v,w,laserscan,robotModel) breakDist = calculateBreakingDistance(v) if (dist &gt; breakDist) //can stop in time heading = hDiff(robotPose,goalPose, v,w) clearance = (dist-breakDist)/(dmax - breakDist) cost = costFunction(heading,clearance, abs(desired_v - v)) if (cost &gt; optimal) best_v = v best_w = w optimal = cost set robot trajectory to best_v, best_wEND]]></content>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A-star algorithm]]></title>
    <url>%2F2018%2F11%2F14%2FA-star-algorithm%2F</url>
    <content type="text"><![CDATA[æœ‰ç›®çš„æ€§åœ°å¯»æ‰¾æœ€ä½³è·¯å¾„ï¼Œé¦–å…ˆå®šä¹‰ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œè¡¨ç¤ºèŠ‚ç‚¹æ¶ˆè€—ï¼š f = g+h$g$è¡¨ç¤ºèµ·ç‚¹åˆ°å½“å‰èŠ‚ç‚¹çš„å·²çŸ¥æ¶ˆè€— $h$è¡¨ç¤ºå¯¹å½“å‰èŠ‚ç‚¹åˆ°ç»ˆç‚¹æ¶ˆè€—çš„çŒœæµ‹ï¼Œä¼°ä»·å‡½æ•°æœ‰å¤šç§å½¢å¼â€”â€”å¯å‘å¼æ¢ç´¢çš„æ ¸å¿ƒ ç®—æ³•æµç¨‹ï¼š 123456789101112131415161718192021åˆå§‹åŒ–openListåˆå§‹åŒ–closeListå°†èµ·ç‚¹æ”¾å…¥openListwhile openListéç©ºï¼š æ‰¾åˆ°å¼€å¯åˆ—è¡¨ä¸Šfæœ€å°çš„èŠ‚ç‚¹ï¼Œè®°ä¸ºq æ‰¾åˆ°qå‘¨å›´çš„å­èŠ‚ç‚¹pï¼Œè®°å…¶çˆ¶èŠ‚ç‚¹ä¸ºq for æ¯ä¸€ä¸ªå­èŠ‚ç‚¹pï¼š if pæ˜¯ç»ˆç‚¹ï¼š ç®—æ³•ç»ˆæ­¢ p.g = q.g + qp p.h = h(p, terminate) p.f = p.g + p.h if på·²ç»åœ¨å¼€å¯åˆ—è¡¨ä¸­ä¸”ä¿å­˜çš„få€¼å°äºå½“å‰è®¡ç®—å€¼||påœ¨å…³é—­åˆ—è¡¨ä¸­ï¼š è·³è¿‡è¯¥èŠ‚ç‚¹ elseï¼š if på·²ç»åœ¨å¼€å¯åˆ—è¡¨ä¸­ï¼š ä¿®æ”¹è¯¥èŠ‚ç‚¹çš„ä¿¡æ¯ï¼ˆçˆ¶èŠ‚ç‚¹ã€fghï¼‰ elseï¼š å°†è¯¥èŠ‚ç‚¹åŠ å…¥openList å°†qæ”¾å…¥closeList ç®—æ³•æ€§èƒ½åœ¨ç»†èŠ‚ä¸Šçš„ä¼˜åŒ–ï¼šhttp://theory.stanford.edu/~amitp/GameProgramming/ åºè¨€ï¼šè·¯å¾„æœç´¢ç®—æ³•çš„å‰ä¸–ä»Šç”Ÿ Dijkstraç®—æ³•ï¼šä»åˆå§‹èŠ‚ç‚¹å¼€å§‹å‘å¤–æ‰©å±•ï¼Œç›´åˆ°åˆ°è¾¾ç›®æ ‡èŠ‚ç‚¹ã€‚ç®—æ³•ä¿è¯èƒ½æ‰¾åˆ°ä»åˆå§‹ç‚¹åˆ°ç›®æ ‡ç‚¹çš„æœ€çŸ­è·¯å¾„ã€‚ æœ€ä½³ä¼˜å…ˆæœç´¢BFSç®—æ³•ï¼šç®—æ³•èƒ½å¤Ÿè¯„ä¼°ä»»æ„èŠ‚ç‚¹åˆ°ç›®æ ‡ç‚¹çš„ä»£ä»·ï¼Œå¹¶ä¼˜å…ˆé€‰æ‹©ç¦»ç›®æ ‡æœ€è¿‘çš„ç»“ç‚¹ã€‚å¯å‘å¼ç®—æ³•ï¼Œæ¯”Dijkstraç®—æ³•è¿è¡Œå¿«å¾—å¤šï¼Œä½†æ˜¯ä¸èƒ½ä¿è¯è·¯å¾„æœ€çŸ­ã€‚ å¦‚ä¸‹é¢è¿™ç§æƒ…å†µï¼š å› ä¸ºBFSæ˜¯åŸºäºè´ªå¿ƒç­–ç•¥çš„ï¼Œå®ƒåªå…³æ³¨åˆ°å°½å¯èƒ½å‘ç€ç›®æ ‡ç‚¹ç§»åŠ¨ï¼Œè€Œä¸è€ƒè™‘å·²èŠ±è´¹çš„ä»£ä»·ã€‚Dijkstraç®—æ³•åˆ™æ­£ç›¸åï¼Œå®ƒä¼šç¡®ä¿æ¯ä¸€æ­¥éƒ½æ˜¯æœ€ä¼˜çš„ï¼Œä½†æ˜¯ä¸ºæ­¤è¦éå†å‘¨å›´å…¨éƒ¨çš„èŠ‚ç‚¹ã€‚ A*ç®—æ³•ï¼šå°†ä¸¤ç§è·¯å¾„æœç´¢ç®—æ³•çš„æ€æƒ³ç»“åˆèµ·æ¥ï¼Œè€ƒè™‘ä¸¤ä¸ªæç«¯åŠå…¶ä¸­é—´çš„æƒ…å†µï¼š å¦‚æœ$h(n)$æ˜¯0ï¼Œåªæœ‰$g(n)$èµ·ä½œç”¨ï¼Œé‚£ä¹ˆç®—æ³•æ¼”å˜ä¸ºDijkstraç®—æ³•ã€‚ å¦‚æœ$h(n)$èƒ½å¤Ÿå§‹ç»ˆæ»¡è¶³â€œæ¯”å½“å‰èŠ‚ç‚¹ç§»åŠ¨åˆ°ç›®æ ‡èŠ‚ç‚¹çš„å®é™…ä»£ä»·å°â€ï¼Œé‚£ä¹ˆç®—æ³•ä¿è¯èƒ½å¤Ÿæ‰¾åˆ°æœ€çŸ­è·¯å¾„ã€‚ï¼ˆ$h(n)$è¶Šå°ï¼Œç®—æ³•æ‰©å±•çš„èŠ‚ç‚¹æ•°å°±è¶Šå¤šï¼‰ å¦‚æœ$h(n)$èƒ½å¤Ÿç²¾ç¡®ç­‰äºâ€œå½“å‰èŠ‚ç‚¹ç§»åŠ¨åˆ°ç›®æ ‡èŠ‚ç‚¹çš„å®é™…ä»£ä»·â€ï¼Œé‚£ä¹ˆç®—æ³•å°†ä¼šä»…ä»…æ‰©å±•æœ€ä¼˜è·¯å¾„ã€‚è€Œä¸æ‰©å±•å…¶ä»–èŠ‚ç‚¹ï¼Œç®—æ³•è¿è¡Œéå¸¸å¿«ã€‚ å¦‚æœ$h(n)$æœ‰æ—¶ä¼šâ€œæ¯”å½“å‰èŠ‚ç‚¹ç§»åŠ¨åˆ°ç›®æ ‡èŠ‚ç‚¹çš„å®é™…ä»£ä»·å¤§â€ï¼Œé‚£ä¹ˆæ­¤æ—¶ç®—æ³•ä¸èƒ½ä¿è¯æœ€çŸ­è·¯å¾„äº†ã€‚ å¦‚æœ$g(n)$æ¯”$h(n)$å°çš„å¤šï¼Œåªæœ‰$h(n)$èµ·ä½œç”¨ï¼Œé‚£ä¹ˆç®—æ³•æ¼”å˜ä¸ºBFSç®—æ³•ã€‚ ä¼°ä»·å‡½æ•°Heuristic function $h(a, b)$ ä¼°ä»·å‡½æ•°çš„é€‰æ‹©å¯ä»¥followä»¥ä¸‹çš„instructionsï¼š square grid that allows 4 directionsï¼šuse Manhattan distance (L1) dx = abs(a.x - b.x)\\ dy = abs(a.y - b.y)\\ dis = D*(dx+dy) square grid that allows 8 directionsï¼šuse Diagonal distance (Lâˆ) dx = abs(a.x - b.x)\\ dy = abs(a.y - b.y)\\ dis = D*(dx+dy) + (D2 - 2*D)*min(dx, dy)å½“$D = D2 =1$æ—¶ï¼Œ$dis = dx + dy -min(dx, dy) = max(dx, dy)$ï¼Œè¿™ä¸ªè·ç¦»ç§°ä¸ºåˆ‡æ¯”é›ªå¤«è·ç¦»ã€‚ å½“$D = 1, D2 =\sqrt 2$æ—¶ï¼Œè¿™ä¸ªè·ç¦»ç§°ä¸ºoctile distanceã€‚ square grid that allows any direcitonsï¼šuse Euclidean distance (L2) dx = abs(a.x - b.x)\\ dy = abs(a.y - b.y)\\ dis = D*\sqrt{dx*dx + dy*dy} If A* is finding paths on the grid but you are allowing movement not on the grid, you may want to consider other representations of the map â€‹ æ¬§å‡ é‡Œå¾—è·ç¦»å¹¶ä¸é€‚ç”¨äºæ …æ ¼åœ°å›¾ï¼Œå› ä¸ºè¿™ä¼šå¯¼è‡´ä»£ä»·å‡½æ•°gå’Œä¼°ä»·å‡½æ•°çš„ä¸åŒ¹é…ï¼ˆä»£ä»·å‡½æ•°å¹¶ä¸æ˜¯è¿ç»­çš„ï¼‰ã€‚ ç”±äºæ¬§å‡ é‡Œå¾—è·ç¦»å¼•å…¥äº†å¼€æ ¹å·è®¡ç®—ï¼Œä¸€äº›ç®—æ³•ä¼šç›´æ¥ç”¨$dis = D(dxdx + dydy)$æ¥ä»£æ›¿ï¼Œ*ä¸å»ºè®®ï¼ï¼Œä¼šå¼•å…¥å°ºåº¦é—®é¢˜ï¼Œ$f = g + h$ï¼Œå…¶ä¸­ä»£ä»·å‡½æ•°ä¼šé€æ¸å¢é•¿ï¼Œä¼°ä»·å‡½æ•°åˆ™é€æ¸å‡å°ï¼Œå¹³æ–¹ä¼šå¯¼è‡´ä¸¤ä¸ªå‡½æ•°çš„å˜åŒ–é€Ÿç‡ä¸matchï¼Œä½¿å¾—ä¼°ä»·å‡½æ•°çš„æƒé‡è¿‡å¤§ï¼Œå¯¼è‡´BFSã€‚ codeï¼š 12345678910111213public function manhattanHeuristic(a:Object, b:Object):Number &#123; return graph.distance(a, b) + simpleCost(a, b) - 1;&#125;public function simpleCost(a:Object, b:Object):Number &#123; var c:Number = costs[graph.nodeToString(b)]; if (isNaN(c)) &#123; return 1; &#125; else &#123; return c; &#125;&#125;// simpleCosté™å®šä¸ºå°äºç­‰äº1çš„æ•° æ­¤æ—¶$h(a,b) = dis(a,b)+c-1 \leq h^{*}(a,b)$ï¼Œæ­¤æ—¶èƒ½å¤Ÿæ‰¾åˆ°æœ€ä¼˜è§£ã€‚]]></content>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[branch and bound]]></title>
    <url>%2F2018%2F11%2F13%2Fbranch-and-bound%2F</url>
    <content type="text"><![CDATA[ä¸€ä¸ªæ —å­ï¼šæ•´æ•°è§„åˆ’é—®é¢˜æ¬²æ±‚$max\ z = 5x_1 + 8x_2$ \left\{ \begin{align} & x_1 + x_2 \leq 6 \nonumber\\ & 5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\ & x_1,x_2ä¸ºæ•´æ•° \nonumber \end{align} \right.æ ¹æ®æ–¹ç¨‹ç»„å¯ä»¥ç»˜åˆ¶ä¸‹å›¾ï¼š äºæ˜¯å¯ä»¥å¾—åˆ°å®æ•°ç©ºé—´ä¸Šçš„æœ€ä¼˜è§£ï¼š$x_1 = 2.25, x_2 = 3.75, z_0 = 41.25$ã€‚â€”â€”æ¾å¼›é—®é¢˜ ç”±äºå­˜åœ¨æ•´æ•°é™å®šæ¡ä»¶ï¼š æœ€ä¼˜è§£$0 \leq z^{*} \leq 41$ï¼Œä¸”å¿…ä¸ºæ•´æ•° x_2çš„æœ€ä¼˜è§£ä¸åœ¨3å’Œ4ä¹‹é—´ï¼Œå› ä¸ºé™å®šä¸ºæ•´æ•° ä¸€ã€åˆ†æ”¯ äºæ˜¯é—®é¢˜å¯ä»¥æ‹†åˆ†ä¸ºï¼š$max\ z = 5x_1 + 8x_2$ p1\left\{ \begin{align} & x_1 + x_2 \leq 6 \nonumber\\ & 5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\ & x_2\leq 3 \nonumber \end{align} \right.\\ p2\left\{ \begin{align} & x_1 + x_2 \leq 6 \nonumber\\ & 5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\ & x_2\geq 4 \nonumber \end{align} \right.\\é—®é¢˜æ‹†åˆ†çš„å®è´¨æ˜¯å°†$x_2$åœ¨3å’Œ4ä¹‹é—´çš„å°æ•°éƒ¨åˆ†åˆ’æ‰äº†ï¼Œå°†å¯è¡ŒåŸŸæ‹†åˆ†æˆ$x_2 \leq 3$ å’Œ$x_3 \geq 4$ï¼Œä½†æ˜¯æ²¡æœ‰æ’é™¤ä»»ä½•æ•´æ•°å¯è¡Œè§£ã€‚â€”â€”åˆ†æ”¯ äºŒã€å®šç•Œ å­é—®é¢˜$p1$çš„æœ€ä¼˜è§£ä¸ºï¼š$x_1 = 3, x_2=3, z^{*}=39$ å­é—®é¢˜$p2$çš„æœ€ä¼˜è§£ä¸ºï¼š$x_1 = 1.8, x_2=4, z^{*}=41$ ä¹Ÿå°±æ˜¯è¯´ï¼Œå­é—®é¢˜$p1$çš„æ•´ä¸ªå‚æ•°ç©ºé—´ä¸Šï¼Œèƒ½å¤Ÿå–å¾—çš„æœ€ä¼˜è§£ä¸º39ï¼Œå­é—®é¢˜$p2$ä¸Šåˆ™ä¸º41ï¼Œæ˜¾ç„¶æœ€ä¼˜è§£åº”è¯¥ä½äºå­é—®é¢˜$p2$æ‰€åœ¨çš„å‚æ•°ç©ºé—´ä¸­ï¼Œä¸”$39\leq z^{} \leq41$ã€‚â€”â€”*å®šç•Œ ä¸‰ã€è¿­ä»£ å¯¹$p2$å‚æ•°ç©ºé—´å†åˆ†æ”¯ï¼Œå‚æ•°$x_1$å¯ä»¥æ‹†åˆ†ä¸º$x_1 \leq 1$å’Œ$x_1 \geq 2$ï¼š p3\left\{ \begin{align} & x_1 + x_2 \leq 6 \nonumber\\ & 5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\ & x_1\leq 1 \nonumber\\ & x_2\geq 4 \nonumber \end{align} \right.\\ p4\left\{ \begin{align} & x_1 + x_2 \leq 6 \nonumber\\ & 5x_1 + 9x_2 \leq 45 \nonumber\\ & x_1\geq0, x_2\geq0 \nonumber\\ & x_1\geq 2 \nonumber \\ & x_2\geq 4 \nonumber \end{align} \right.\\å››ã€æ€»ç»“ åˆ†æ”¯å®šç•Œç®—æ³•çš„æ€»ä½“æµç¨‹å¦‚ä¸‹ï¼š å…ˆæ±‚è§£ç›¸åº”çš„æ¾å¼›é—®é¢˜ï¼Œå¾—åˆ°æœ€ä¼˜è§£ï¼Œæ£€æŸ¥å…¶æ˜¯å¦ç¬¦åˆåŸé—®é¢˜çº¦æŸï¼Œè‹¥ç¬¦åˆåˆ™ä¸ºæœ€ä¼˜è§£ï¼Œå¦åˆ™è¿›è¡Œä¸‹ä¸€æ­¥ã€‚ å®šç•Œï¼Œå–å„åˆ†æ”¯ä¸­ç›®æ ‡å‡½æ•°æœ€å¤§çš„ä½œä¸ºä¸Šç•Œ$U_z$ï¼Œå–å…¶ä½™åˆ†æ”¯ä¸­ç›®æ ‡å‡½æ•°ä¸­æœ€å¤§çš„ä½œä¸ºä¸‹ç•Œ$L_z$ã€‚$L_z \leq z^{*} \leq U_z$ã€‚ åˆ†æ”¯ï¼Œå¦åˆ™é€‰æ‹©ä¸€ä¸ªä¸ç¬¦åˆåŸé—®é¢˜æ¡ä»¶çš„å˜é‡ï¼Œæ„å»ºå­é—®é¢˜ã€‚ å¯¹å„åˆ†æ”¯ï¼Œæœ‰åºåœ°ï¼Œè¿›è¡Œæ­¥éª¤1ã€‚ åœ¨æ±‚è§£å¯¹åº”çš„æ¾å¼›é—®é¢˜æ—¶ï¼Œé€šå¸¸ä¼šæœ‰ä»¥ä¸‹å‡ ç§æƒ…å†µï¼š æ¾å¼›é—®é¢˜æ²¡æœ‰å¯è¡Œè§£ï¼Œé‚£ä¹ˆåŸé—®é¢˜ä¹Ÿæ²¡æœ‰å¯è¡Œè§£ã€‚ æ¾å¼›é—®é¢˜çš„æœ€ä¼˜è§£ä¹Ÿæ»¡è¶³åŸé—®é¢˜çº¦æŸï¼Œé‚£ä¹ˆè¯¥è§£ä¹Ÿæ˜¯åŸé—®é¢˜çš„æœ€ä¼˜è§£ï¼Œç®—æ³•ç»ˆæ­¢ã€‚ æ¾å¼›é—®é¢˜çš„æœ€ä¼˜è§£å°äºç°æœ‰ä¸‹ç•Œï¼Œé‚£ä¹ˆåº”è¯¥å¯¹è¯¥å­é—®é¢˜è¿›è¡Œå‰ªæã€‚ äº”ã€DFS å¯¹ä¸€é¢—æœç´¢æ ‘ï¼Œä¸ç”¨è®¡ç®—æ¯ä¸€å±‚å…¨éƒ¨èŠ‚ç‚¹çš„scoreï¼ˆBFSï¼‰ï¼Œæˆ‘ä»¬ä¼šç»´æŠ¤ä¸€ä¸ªä¼˜å…ˆé˜Ÿåˆ—ï¼Œå…¶ä¸­æŒ‰ç…§scoreçš„å¤§å°å­˜æ”¾èŠ‚ç‚¹ï¼Œç„¶åé€‰æ‹©scoreæœ€å¤§çš„èŠ‚ç‚¹ï¼ˆthe most promising childï¼‰è¿›è¡Œåˆ†æ”¯ã€‚]]></content>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[trifles with arduino]]></title>
    <url>%2F2018%2F10%2F30%2Ftrifles-with-arduino%2F</url>
    <content type="text"><![CDATA[ç³»ç»Ÿæ€»ä½“çš„é€šä¿¡æ¶æ„å¦‚ä¸‹ï¼š åº•ç›˜é©±åŠ¨æ¿Arduinoè´Ÿè´£æ¥æ”¶ä¸Šå±‚çš„è¿åŠ¨æ§åˆ¶æŒ‡ä»¤ï¼Œå¹¶é©±åŠ¨ç”µæœºï¼Œä¸¤å—æ¿å­é€šè¿‡ä¸²å£è¿›è¡Œé€šä¿¡ã€‚ ROSæä¾›äº†ä¸€ä¸ªros_arduino_bridgeåŠŸèƒ½åŒ…é›†ï¼Œå®ƒåŒ…æ‹¬äº†Arduinoåº“ï¼ˆROSArduinoBridgeï¼‰ä»¥åŠä¸€ç³»åˆ—ç”¨æ¥æ§åˆ¶Arduino-based robotçš„ROSåŠŸèƒ½åŒ…ï¼Œè¿™ä¸ªåŠŸèƒ½åŒ…å¯ä»¥å®ç°è¯»å–Twistæ§åˆ¶ä¿¡æ¯ï¼Œä»¥åŠå‘å¸ƒé‡Œç¨‹è®¡ä¿¡æ¯ç­‰ä»»åŠ¡ï¼Œå°è£…äº†Raspberry Piå’ŒArduinoä¹‹é—´çš„åº•å±‚é€šä¿¡ã€‚ Arduinoåº“ï¼ˆROSArduinoBridgeï¼‰ä½äºros_arduino_firmware/src/libraries/è·¯å¾„ä¸‹ï¼Œé‡Œé¢æ˜¯ä¸€äº›arduinoè„šæœ¬å’Œå¤´æ–‡ä»¶ï¼Œå°†è¿™ä¸ªæ–‡ä»¶å¤¹å¤åˆ¶åˆ°æˆ‘ä»¬Arduino IDEçš„SKETCHBOOK_PATHä¸‹ï¼Œç„¶ååœ¨Arduino IDEä¸­å°±å¯ä»¥ç›´æ¥æ‰“å¼€è¿™ä¸ªsketché¡¹ç›®ã€‚ ROSArduinoBridgeæ–‡ä»¶ä¸‹æ˜¯ä¸€äº›é…ç½®é€‰é¡¹ï¼Œå¦å¤–commands.hæ–‡ä»¶ä¸­ç»™å‡ºäº†ä¸€äº›å¯ç”¨çš„ä¸²å£æ§åˆ¶æŒ‡ä»¤ï¼Œå¦‚ç”µæœºæ§åˆ¶æŒ‡ä»¤ï¼š 1m 20 20 // move the robot forward at 20 encoder ticks per second]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[skid-steer drive]]></title>
    <url>%2F2018%2F10%2F29%2Fskid-steer-drive%2F</url>
    <content type="text"><![CDATA[å®éªŒä¸­ä½¿ç”¨äº†ä¸¤ç§ç±»å‹çš„åº•ç›˜ï¼ŒåŸºäºå·®é€Ÿé©±åŠ¨çš„2WDåº•ç›˜å’ŒåŸºäºæ»‘åŠ¨è½¬å‘çš„4WDåº•ç›˜ã€‚ä¸¤ç§é©±åŠ¨æ–¹å¼åŸç†ç›¸ä¼¼ï¼Œä¹Ÿæœ‰å…¶æ˜¾è‘—çš„åŒºåˆ«ã€‚ ç›¸åŒç‚¹ï¼š ä¸¤ç§åº•ç›˜éƒ½æ²¡æœ‰æ˜¾ç¤ºçš„è½¬åŠ¨æœºåˆ¶ï¼Œé‡‡ç”¨å·®é€Ÿé©±åŠ¨çš„æ–¹å¼é€šè¿‡ä»¥ä¸åŒçš„æ–¹å‘æˆ–é€Ÿåº¦é©±åŠ¨ä¸¤è¾¹è½®å­æ¥å®ç°æ–¹å‘æ§åˆ¶ã€‚ å·®é€Ÿé©±åŠ¨çš„è¿åŠ¨å½¢å¼é€šå¸¸æœ‰ä¸€ä¸‹å‡ ç§ç±»å‹ï¼š ç¬¬ä¸€ç§æ˜¯åŸåœ°æ—‹è½¬ï¼Œå·¦å³è½®çš„é€Ÿåº¦å¤§å°ç›¸ç­‰ï¼Œæ–¹å‘ç›¸åï¼Œè¿™æ ·ç›¸å½“äºç»•ç€åº•ç›˜çš„å½¢å¿ƒåŸåœ°æ‰“è½¬ã€‚ ç¬¬äºŒç§æ˜¯æ²¿ç€æŸä¸ªæ–¹å‘ç›´çº¿è¡Œèµ°ï¼Œæ­¤æ—¶å·¦å³è½®é€Ÿåº¦ç›¸åŒã€‚ ç¬¬ä¸‰ç§æ˜¯æ²¿ç€æŸæ¡æ›²çº¿å‰è¡Œæˆ–åé€€ï¼Œæ­¤æ—¶å·¦å³è½®é€Ÿåº¦æ–¹å‘ç›¸åŒï¼Œå¤§å°ä¸åŒã€‚ ç¬¬å››ç§æ˜¯æ—‹è½¬è½¬å¼¯ï¼Œæ­¤æ—¶å·¦å³è½®é€Ÿåº¦æ–¹å‘ç›¸åã€‚ ä¸¤ç§æœºæ„å…±åŒçš„ä¼˜åŠ¿æ˜¯ï¼šæ²¡æœ‰æ˜¾ç¤ºçš„è½¬å‘æœºæ„ï¼Œæå¤§åœ°ç®€åŒ–äº†è¿åŠ¨å­¦æ¨¡å‹ã€‚ è€Œä¸¤ç§æœºæ„å…±åŒçš„ç¼ºç‚¹æ˜¯ï¼šç”±äºä¸¤ä¾§çš„è½®å­æ˜¯ç”±ç‹¬ç«‹ç”µæœºåˆ†åˆ«é©±åŠ¨çš„ï¼Œç›´çº¿è¿åŠ¨è¦æ±‚ä¸¤ä¾§çš„è½®å­ä»¥ç›¸åŒé€Ÿåº¦è½¬åŠ¨ï¼Œè¿™å°†å¾ˆéš¾å®Œæˆã€‚ ä¸åŒç‚¹ï¼š å·®é€Ÿé©±åŠ¨åº•ç›˜é€šå¸¸æ˜¯ç”±ä¸€ä¸ªä¸¤è½®ç³»ç»Ÿï¼Œæ¯ä¸ªè½®å­éƒ½å¸¦æœ‰ç‹¬ç«‹çš„æ‰§è¡Œæœºæ„ï¼ˆç›´æµç”µæœºï¼‰ï¼Œä»¥åŠä¸€ä¸ªæ— é©±åŠ¨è½®ï¼ˆå¯ä»¥æ˜¯è„šè½®æˆ–è€…ä¸‡å‘æ»šç ï¼‰ç»„æˆï¼Œæœºå™¨äººçš„è¿åŠ¨çŸ¢é‡æ˜¯æ¯ä¸ªç‹¬ç«‹è½¦è½®è¿åŠ¨çš„æ€»å’Œã€‚ æ»‘åŠ¨è½¬å‘åº•ç›˜é€šå¸¸è¢«ç”¨åœ¨å±¥å¸¦è½¦ä¸Šï¼Œæ¯”å¦‚å¦å…‹å’Œæ¨åœŸæœºï¼Œä¹Ÿè¢«ç”¨äºæŸäº›å››è½®å…­è½®æœºæ„ä¸Šï¼Œç›¸æ¯”è¾ƒäºä¸¤è½®å·®é€Ÿåº•ç›˜ï¼Œæ»‘åŠ¨è½¬å‘çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼š ä¼˜åŠ¿ï¼šæ»‘åŠ¨è½¬å‘ä½¿ç”¨äº†ä¸¤ä¸ªé¢å¤–çš„é©±åŠ¨è½®ä»£æ›¿äº†å·®é€Ÿé©±åŠ¨çš„è„šè½®ï¼Œå¢å¤§äº†ç‰µå¼•åŠ›ã€‚ åŠ£åŠ¿ï¼šå¼•å…¥äº†æ»‘åŠ¨ï¼Œåœ¨å¯¹é‡Œç¨‹è®¡è¦æ±‚é«˜çš„åœºæ™¯ä¸­ï¼Œæ»‘åŠ¨æ˜¯ä¸€ä¸ªè‡´å‘½çš„ç¼ºé™·ï¼Œå› ä¸ºè¿™ä¼šå¯¹ç¼–ç å™¨é€ æˆè´Ÿé¢å½±å“ï¼Œæ»‘åŠ¨çš„è½®å­ä¸ä¼šè·Ÿè¸ªæœºå™¨äººçš„ç¡®åˆ‡è¿åŠ¨ã€‚ è¿åŠ¨å­¦åˆ†æï¼š å¯¹äºå·®é€Ÿé©±åŠ¨æœºæ„ï¼Œç§»åŠ¨æœºå™¨äººèˆªå‘è§’å˜åŒ–äº†å¤šå°‘è§’åº¦ï¼Œå®ƒå°±ç»•å…¶è¿åŠ¨è½¨è¿¹çš„åœ†å¿ƒæ—‹è½¬äº†å¤šå°‘è§’åº¦ã€‚è¿™å¥è¯å¾ˆå¥½éªŒè¯ï¼Œæˆ‘ä»¬è®©æœºå™¨äººåšåœ†å‘¨è¿åŠ¨ï¼Œä»èµ·ç‚¹å‡ºå‘ç»•åœ†å¿ƒä¸€åœˆå›åˆ°èµ·ç‚¹å¤„ï¼Œåœ¨è¿™è¿‡ç¨‹ä¸­æœºå™¨äººç´¯è®¡çš„èˆªå‘è§’ä¸º360åº¦ï¼ŒåŒæ—¶å®ƒä¹Ÿç¡®å®ç»•è½¨è¿¹åœ†å¿ƒè¿åŠ¨äº†360åº¦ã€‚ æœºå™¨äººçš„é€Ÿåº¦æ˜¯æŒ‡ä¸¤ä¸ªç›¸é‚»çš„æ§åˆ¶æ—¶åˆ»ä¹‹é—´çš„é€Ÿåº¦ï¼Œå› æ­¤å°è½¦çš„è¡Œé©¶è½¨è¿¹å¯ä»¥åˆ†è§£ä¸ºè¿ç»­çš„åœ†å¼§ç‰‡æ®µï¼Œå¯¹äºæ¯ä¸€æ®µåœ†å¼§ï¼Œæ ¹æ®é˜¿å…‹æ›¼è½¬å‘å‡ ä½•åŸç†ï¼Œåœ¨å°è½¦è½¬å‘æ—¶ï¼Œä¸ºä¿è¯è¡Œé©¶ç¨³å®šæ€§ï¼Œä¸¤ä¾§è½®èƒéƒ½è¿‘ä¼¼å›´ç»•ä¸€ä¸ªä¸­å¿ƒç‚¹æ—‹è½¬ã€‚å³æ•´ä¸ªå°è½¦åº•ç›˜éƒ½å›´ç»•ä¸€ä¸ªä¸­å¿ƒç‚¹æ—‹è½¬ï¼Œå·²çŸ¥å°è½¦ä¸­å¿ƒçš„çº¿é€Ÿåº¦ï¼ˆä¸Šå±‚ç®—æ³•ç»™å®šï¼‰ï¼Œæ­¤æ—¶å°è½¦åº•ç›˜çš„è¿åŠ¨å­¦æ¨¡å‹å¦‚ä¸‹å›¾ï¼š å‚æ•°è¯´æ˜ï¼š $\alpha_1$æ˜¯å°è½¦å‰å·¦è½®å’Œåå·¦è½®çš„è½¬è§’ã€‚ $\alpha_2$æ˜¯å°è½¦å‰å³è½®å’Œåå³è½®çš„è½¬è§’ã€‚ $2L$æ˜¯å·¦å³è½®è·ç¦»ã€‚ $2K$æ˜¯å‰åè½®è·ç¦»ã€‚ $w$æ˜¯å°è½¦è½¬è½´çš„è§’é€Ÿåº¦ã€‚ $v$æ˜¯å°è½¦å‡ ä½•ä¸­å¿ƒçš„çº¿é€Ÿåº¦ã€‚ $v1, v2, v3, v4$æ˜¯å››ä¸ªè½¦è½®çš„é€Ÿåº¦ã€‚ $i$æ˜¯ç”µæœºçš„å‡é€Ÿæ¯”ã€‚ $r$æ˜¯è½¦è½®åŠå¾„ã€‚ é¦–å…ˆå¯ä»¥å¾—åˆ°å„è½¦è½®é€Ÿåº¦å’Œè§’é€Ÿåº¦çš„å…³ç³»ï¼š V_1 = w * R_1 = w * \frac{K}{sin\alpha_1}\\ V_2 = w * R_2 = w * \frac{K}{sin\alpha_2}\\ V_3 = V_1 = w * \frac{K}{sin\alpha_1}\\ V_4 = V_2 = w * \frac{K}{sin\alpha_2}\\å…¶ä¸­è½¦è½®æ²¿ç€è½¬åŠ¨æ–¹å‘ï¼ˆ$y$æ–¹å‘ï¼‰çš„é€Ÿåº¦ç”±ç”µæœºæä¾›ï¼Œåˆ‡å‘é€Ÿåº¦ç”±åœ°é¢æ‘©æ“¦æä¾›ï¼Œè½¦è½®æ²¿ç€$y$æ–¹å‘çš„é€Ÿåº¦ä¸ºï¼š R =\frac{v}{w}\\ V_{1y} = V_1 * cos\alpha_1 = w * \frac{K}{tan \alpha_1} = w(R-L)\\ V_{2y} = V_2 * cos\alpha_2 = w * \frac{K}{tan \alpha_2} = w(R+L)\\ V_{3y} = V_{1y} = w(R-L)\\ V_{4y} = V_{2y} = w(R+L)\\é‚£ä¹ˆç”µæœºçš„è§’é€Ÿåº¦ä¸ºï¼š w_n= \frac{V_{ny}*i}{r}, n = 1,2,3,4\\ç›¸åº”ç”µæœºçš„è½¬é€Ÿï¼ˆby rpmï¼‰ä¸ºï¼š n = \frac{w_n*60}{2\pi}æ•´ç†æˆçŸ©é˜µè¡¨è¾¾å¼ä¸ºï¼š \begin{bmatrix} w_1\\ w_2\\ w_3\\ w_4 \end{bmatrix}= \begin{bmatrix} 1 & - L\\ 1 & L\\ 1 & - L\\ 1 & L \end{bmatrix} \begin{bmatrix} v\\ w \end{bmatrix}è¯¥è¡¨è¾¾å¼åæ˜ äº†æœºå™¨äººå…³é”®ç‚¹é€Ÿåº¦ä¸ä¸»åŠ¨è½®è½¬é€Ÿä¹‹é—´çš„å…³ç³»ã€‚ç»™å®šå°è½¦åº•ç›˜ç”µæœºè½¬é€Ÿå°±å¯ä»¥æ±‚å‡ºæœºå™¨äººå…³é”®ç‚¹çš„é€Ÿåº¦ï¼Œå¹¶ç”±æ­¤å¾—åˆ°æœºå™¨äººä¸Šä»»æ„ä¸€ç‚¹çš„é€Ÿåº¦ï¼ˆå¦‚æ¿€å…‰é›·è¾¾çš„å®‰è£…ä½ç½®çš„é€Ÿåº¦ï¼‰ï¼Œä¸Šå±‚ç®—æ³•ç»™å‡ºçš„å…³é”®ç‚¹é€Ÿåº¦æ§åˆ¶ä¿¡å·ä¹Ÿå¯ä»¥ç”±æ­¤è½¬åŒ–æˆç”µæœºçš„æ§åˆ¶é‡ã€‚ é‡Œç¨‹è®¡æ¨¡å‹ ï¼ æœºå™¨äººå®šä½æ–¹æ³• åæ ‡å˜æ¢æ¨¡å‹ï¼š åœ¨ä¸€ä¸ªè¾ƒçŸ­çš„æ—¶é—´é—´éš”$\Delta t$å†…ï¼Œå‡å®šæœºå™¨äººå·¦å³è½®çš„ç§»åŠ¨è·ç¦»åˆ†åˆ«æ˜¯$\Delta l$å’Œ$\Delta r$ï¼Œé‚£ä¹ˆåœ¨æœºå™¨äººåæ ‡ç³»ä¸‹ï¼šæœºå™¨äººä¸­å¿ƒæ²¿ç€æœºå™¨äººåæ ‡ç³»çš„$x$è½´æ–¹å‘å‰è¿›çš„è·ç¦»ä¸º$\Delta u = (\Delta l + \Delta r)/2$ï¼Œ$y$è½´æ–¹å‘å‰è¿›çš„è·ç¦»ä¸º$\Delta v = 0$ï¼Œè½¬è¿‡çš„è§’åº¦ä¸º$\Delta \varphi = (\Delta l - \Delta r)/b$ã€‚æœºå™¨äººåæ ‡ç³»åˆ°ä¸–ç•Œåæ ‡ç³»çš„æ—‹è½¬å˜æ¢çŸ©é˜µä¸º$R(\phi)$ã€‚ é‚£ä¹ˆè½¬æ¢åˆ°ä¸–ç•Œåæ ‡ç³»ä¸‹æœºå™¨äººçš„è¿åŠ¨å¢é‡ä¸ºï¼š \begin{bmatrix} \Delta x\\ \Delta y\\ \Delta \phi \end{bmatrix} = \begin{bmatrix} R(\phi) & 0\\ 0 & 1 \end{bmatrix} \begin{bmatrix} \Delta u\\ \Delta v\\ \Delta \varphi \end{bmatrix} = \begin{bmatrix} cos\phi & sin\phi & 0\\ -sin\phi & cos\phi & 0\\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} (\Delta l + \Delta r)/2\\ 0\\ (\Delta l - \Delta r)/b \end{bmatrix}ä¸–ç•Œåæ ‡ç³»ä¸‹æœºå™¨äººä½å§¿æ›´æ–°ä¸ºï¼š \begin{bmatrix} x_t\\ y_t\\ \phi_t \end{bmatrix} = \begin{bmatrix} cos\phi_{t-1}(\Delta l + \Delta r)/2\\ sin\phi_{t-1}(\Delta l + \Delta r)/2\\ (\Delta l - \Delta r)/b \end{bmatrix} + \begin{bmatrix} x_{t-1}\\ y_{t-1}\\ \phi_{t-1} \end{bmatrix}beside fromæµ‹é‡è¯¯å·®ï¼Œåˆ©ç”¨åæ ‡å˜æ¢æ¨¡å‹å»æ¨ç®—é‡Œç¨‹è®¡ä¿¡æ¯æ˜¯å¼•å…¥äº†æ¨¡å‹è¯¯å·®çš„â€”â€”åœ¨æ—¶é—´é—´éš”$\Delta t$å†…ï¼Œä¸ºäº†ç®€åŒ–è®¡ç®—ï¼Œæœºå™¨äººåæ ‡ç³»ç›¸å¯¹ä¸–ç•Œåæ ‡ç³»çš„æ—‹è½¬å˜æ¢çŸ©é˜µè¢«å‡å®šä¸ºèµ·å§‹å€¼$R(\phi)$ã€‚åœ¨è½¬å‘è¿åŠ¨æ¯”è¾ƒå¤šçš„æƒ…å†µä¸‹ï¼Œé‡Œç¨‹è®¡ä¿¡æ¯ä¼šè¿…é€Ÿæ¶åŒ–ã€‚ åœ†å¼§æ¨¡å‹ï¼š å°†æå°æ—¶é—´é—´éš”å†…å°è½¦è¿åŠ¨çš„è½¨è¿¹çœ‹ä½œæ˜¯ä¸€æ®µåœ†å¼§ï¼Œé‚£ä¹ˆå°±å¯ä»¥ç¡®å®šè¯¥æ—¶åˆ»çš„è½¬åŠ¨ä¸­å¿ƒ$C_{t-1}$ï¼ŒåŠå†…ä¾§è½®çš„è½¬åŠ¨åŠå¾„ä¸º$R_{t-1}$ï¼Œæ ¹æ®å‡ ä½•å…³ç³»ï¼š \left\{ \begin{align} &\Delta l = (b + R)\Delta \varphi\\ &\Delta r = R \Delta \varphi \end{align} \right.è§£å¾—ï¼š \left\{ \begin{align} &R = \frac{b\Delta r}{\Delta l - \Delta r}\\ &\Delta \varphi = \frac{\Delta l - \Delta r }{b} \end{align} \right.ç”±ä¸‰è§’ç›¸ä¼¼å¾—ï¼š \frac{Rsin(\Delta \varphi/2)}{D/2} = \frac{R}{R + b/2}è§£å¾—å¼¦$D$çš„é•¿åº¦ä¸ºï¼š D = [b(\Delta l + \Delta r)/(\Delta l-\Delta r)]sin[(\Delta l - \Delta r)/2b]å¼¦$D$ä¸ä¸–ç•Œåæ ‡ç³»$x$è½´æ­£å‘çš„å¤¹è§’ä¸º$\theta = \phi - \Delta \varphi/2$ï¼Œé‚£ä¹ˆæœºå™¨äººåœ¨ä¸–ç•Œåæ ‡ç³»ä¸‹çš„ä½å§¿å¢é‡ä¸ºï¼š \left\{ \begin{align} &\Delta x_{t-1} = D_{t-1} cos\theta_{t-1}\\ &\Delta y_{t-1} = D_{t-1} sin\theta_{t-1}\\ &\Delta \varphi = (\Delta l - \Delta r )/b \end{align} \right.ä½¿ç”¨åœ†å¼§æ¨¡å‹å¯¹é‡Œç¨‹è®¡å¢é‡è¿›è¡Œæ¨ç®—ï¼Œå®Œå…¨ä¾ç…§å‡ ä½•å…³ç³»æ¥è®¡ç®—ï¼Œè®¡ç®—è¿‡ç¨‹ä¸­æ²¡æœ‰è¿‘ä¼¼ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ§åˆ¶è¯¯å·®ç´¯ç§¯ã€‚ æ¦‚ç‡æ¨¡å‹ï¼š ä¸æ˜¯å•çº¯çš„åŸºäºé‡Œç¨‹è®¡çš„ä¼°è®¡ï¼Œè€Œæ˜¯ç»“åˆå…¶ä»–ä¼ æ„Ÿå™¨çš„æµ‹é‡å€¼å¯¹é‡Œç¨‹è®¡è¿›è¡ŒçŸ«æ­£ï¼Œè¯¦è§æ»¤æ³¢ç®—æ³•ã€‚ scan matchæ¨¡å‹ï¼š åŒæ ·ä¹Ÿä¸æ˜¯å•çº¯çš„åŸºäºé‡Œç¨‹è®¡çš„ä¼°è®¡ï¼Œè¯¦è§karto scanMatchã€‚ä¸æ»¤æ³¢çš„åŒºåˆ«åœ¨äºï¼Œè¿”å›çš„ä¸æ˜¯æ¦‚ç‡åˆ†å¸ƒï¼Œè€Œæ˜¯ä¸€ä¸ªä»£è¡¨æœ€ä½³ä¼°è®¡çš„å€¼ã€‚]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scan matcher]]></title>
    <url>%2F2018%2F10%2F26%2Fscan-matcher%2F</url>
    <content type="text"><![CDATA[ç›®å‰å¼€æºç®—æ³•ä¸­é‡‡å–çš„scanMatchingæ–¹æ³•ä¸»è¦æ˜¯ä»¥ä¸‹å››ç§ï¼š Gmappingï¼šICPï¼ˆsimple Gradient Descentï¼‰ Hectorï¼šGuass-Newtonï¼ˆmulti-resolution mapï¼‰ kartoï¼šReal-time CSMï¼ˆmulti-resolution + ä¸‰ç»´çª—å£éå†å¯»ä¼˜ï¼‰ cartographerï¼šFast CSMï¼ˆmulti-resolution + branch and boundï¼‰ scanMatcherä¸»è¦æ¶‰åŠä¸¤ä¸ªè¯„ä»·å‡½æ•°ï¼Œä¸€ä¸ªscoreç”¨äºä¼˜åŒ–è°ƒæ•´ç²’å­poseä½œä¸ºå‚è€ƒï¼Œä¸€ä¸ªlikelihoodAndScoreç”¨äºæ›´æ–°ç²’å­æƒé‡ï¼š s(x, z, m) = \sum_i s(x, z^i, m)\\ s(x, z^i, m) = e^{d^2/ \sigma}ç²’å­æƒé‡æ ¹æ®åœ°å›¾çš„åŒ¹é…åº¦æ›´æ–°ï¼š laser\ frame \to map\ frame: \hat{z}^i = x \oplus z^i\\ map\ cell: (x,\ y)^T\\ d^2 = (\hat{z}^i - (x,y)^T)^T (\hat{z}^i - (x,y)^T)]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[navigation stack]]></title>
    <url>%2F2018%2F10%2F23%2Fnavigation-stack%2F</url>
    <content type="text"><![CDATA[part0ç°å®ä¸­æƒ³è¦ç§»åŠ¨å¹³å°ç§»åŠ¨åˆ°æŒ‡å®šåœ°ç‚¹ï¼Œæœºå™¨äººè¿åŠ¨æ§åˆ¶ç³»ç»Ÿæ¶æ„åŒ…æ‹¬äº†å¦‚ä¸‹å‡ ä¸ªå±‚æ¬¡ï¼š æœ€åº•å±‚æ˜¯æœºå™¨äººçš„åº•ç›˜æ§åˆ¶éƒ¨åˆ†ï¼Œé©±åŠ¨å™¨æ¥æ”¶çš„æ˜¯æœºå™¨äººçš„æœŸæœ›é€Ÿåº¦ï¼ˆTwistï¼‰ï¼Œå°†é€Ÿåº¦è§£ç®—ä¸ºå·¦å³è½®çš„æœŸæœ›é€Ÿåº¦ï¼Œå¹¶æ ¹æ®æœŸæœ›é€Ÿåº¦å¯¹å·¦å³è½®åˆ†åˆ«è¿›è¡ŒPIDé©±æ§é€Ÿï¼Œè¾“å‡ºç”µæœºçš„è½¬é€Ÿã€‚ è¿™éƒ¨åˆ†ROSç¤¾åŒºå·²ç»æœ‰é’ˆå¯¹Arduinoå°è£…å¥½çš„Packageâ€”â€”rosserial_arduinoã€‚ ä¸­é—´å±‚æ˜¯é€šä¿¡å±‚ï¼Œç”µè„‘ç«¯å‘å¸ƒé€Ÿåº¦æŒ‡ä»¤ç»™å¹³å°ï¼ŒåŒæ—¶æ¥æ”¶å¹³å°å‘å¸ƒçš„å½“å‰é€Ÿåº¦ï¼Œç„¶åå‘å¸ƒ/odom topicï¼Œè®©å…¶ä»–èŠ‚ç‚¹è®¢é˜…ã€‚ æœ€ä¸Šå±‚æ˜¯å†³ç­–å±‚ï¼Œä¹Ÿå°±æ˜¯å¯¼èˆªè§„åˆ’å±‚ï¼Œgoalã€localizationï¼ˆmatching&amp;é‡Œç¨‹è®¡ï¼‰ã€path plannerä»¥åŠæœ€ç»ˆè¾“å‡ºé€Ÿåº¦æŒ‡ä»¤ï¼Œè¿™ä¸€éƒ¨åˆ†éƒ½åœ¨navigation stacké‡Œé¢ã€‚ part1 packagesnavigation stackæ˜¯ROSæä¾›çš„å¯¼èˆªæ–¹æ¡ˆï¼Œå†…éƒ¨é›†æˆäº†å¾ˆå¤šä¸ªpackageï¼Œæ¨¡å—ä¹‹é—´å®Œå…¨è§£è€¦ï¼Œå¯ä»¥ä¸ªæ€§åŒ–é€‰æ‹©æä¾›çš„æ–¹æ³•ã€‚ å®˜ç½‘ä¸Šç»™å‡ºäº†å¯¼èˆªæ ˆå®è§‚çš„ç»“æ„æè¿°ï¼š move_baseä¸­ä¸»è¦åŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼Œglobal_planã€local_planä»¥åŠrecovery behaviorã€‚å¯¹åº”çš„æ’ä»¶æœ‰ï¼š global_planï¼šglobal_plannerï¼ˆå®ç°äº†dijkstraå’ŒA*ç®—æ³•ï¼‰ï¼Œcarrot_plannerï¼Œnavfn local_planï¼šbase_local_plannerï¼ˆå®ç°äº†Trajectory Rolloutå’ŒDWAç®—æ³•ï¼‰ï¼Œdwa_local_planner recoveryï¼šclear_costmap_recoveryï¼Œmove_slow_and_clearï¼Œrotate_recovery nav_coreæ˜¯ä¸€ä¸ªæ¥å£æ’ä»¶ï¼ŒåŒ…å«äº†ä»¥ä¸Šæ’ä»¶åŸºç±»çš„å¤´æ–‡ä»¶ï¼Œmove_baseä¸­çš„æ–¹æ³•éƒ½æ˜¯åœ¨å…¶è§„åˆ™ä¸Šæ‰©å±•çš„ã€‚ ä¸¤ä¸ªç°è‰²çš„æ’ä»¶map_serverå’Œamclè¡¨ç¤ºå¯é€‰å¯ä¸é€‰ï¼š å¯ä»¥ä½¿ç”¨meta packageæä¾›çš„map_serverèŠ‚ç‚¹æ¥è¿›è¡Œä»£ä»·åœ°å›¾ç®¡ç†ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–èŠ‚ç‚¹ï¼ˆä¾‹å¦‚ç›´æ¥ä½¿ç”¨gmappingçš„è¾“å‡ºï¼‰ã€‚ å¯ä»¥ä½¿ç”¨meta packageæä¾›çš„amclèŠ‚ç‚¹æ¥è¿›è¡Œè‡ªå®šä½ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–ç®—æ³•åŒ…ï¼ˆä¾‹å¦‚ROSé‡Œé¢è¿˜æœ‰ä¸€ä¸ªrobot_pose_ekfèŠ‚ç‚¹ï¼‰ã€‚ costmap_2då°†ä¸åŒä¼ æ„Ÿå™¨çš„è¾“å…¥å¤„ç†æˆç»Ÿä¸€çš„æ …æ ¼åœ°å›¾æ ¼å¼ã€‚ä»¥å±‚çš„æ¦‚å¿µæ¥ç»„ç»‡å›¾å±‚ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®éœ€è¦è‡ªå·±é…ç½®ï¼ˆé€šè¿‡Social Costmap Layerã€Range Sensor Layerç­‰å¼€æºæ’ä»¶ï¼‰ï¼Œé»˜è®¤çš„å±‚æœ‰ï¼š static_layerï¼šé™æ€åœ°å›¾å±‚ï¼Œï¼ˆé€šè¿‡è®¢é˜…map_serverçš„/mapä¸»é¢˜ï¼‰æ¥ç”Ÿæˆã€‚ obstacle_layerï¼šéšœç¢åœ°å›¾å±‚ï¼Œæ ¹æ®åŠ¨æ€çš„ä¼ æ„Ÿå™¨ä¿¡æ¯æ¥ç”Ÿæˆã€‚ inflation_layerï¼šè†¨èƒ€å±‚ï¼Œå°†å‰ä¸¤ä¸ªå›¾å±‚çš„ä¿¡æ¯ç»¼åˆè¿›è¡Œç¼“å†²åŒºæ‰©å±•ã€‚ voxel_gridæ˜¯ä¸‰ç»´ä»£ä»·åœ°å›¾ã€‚ fake_localizationç”¨æ¥åšå®šä½ä»¿çœŸï¼Œå†…å«/base_pose_ground_truthè¯é¢˜ã€‚ part2 params move_base_params.yamlï¼š planner_frequencyï¼šå…¨å±€è§„åˆ’çš„æ‰§è¡Œé¢‘ç‡ï¼Œå¦‚æœè®¾ç½®ä¸º0.0åˆ™å…¨å±€è§„åˆ’å™¨ä»…åœ¨æ¥å—åˆ°æ–°ç›®æ ‡ç‚¹æˆ–è€…å±€éƒ¨è§„åˆ’å™¨æŠ¥å‘Šè·¯å¾„å µå¡æ—¶æ‰ä¼šé‡æ–°æ‰§è¡Œã€‚ global_planner_params.yamlï¼š default_toleranceï¼šå½“è®¾ç½®çš„ç›®çš„åœ°è¢«å æ®æ—¶ï¼Œä»¥è¯¥å‚æ•°ä¸ºåŠå¾„çš„èŒƒå›´å†…é€‰å–æœ€è¿‘çš„ç‚¹ä½œä¸ºæ–°ç›®æ ‡ç‚¹ã€‚ dwa_local_planner_params.yamlï¼š latch_xy_goal_toleranceï¼šå¦‚æœè®¾ç½®ä¸ºtrueï¼Œè¾¾åˆ°xy_goal_toleranceä»¥å†…æœºå™¨äººå°±ä¼šåŸåœ°æ—‹è½¬ï¼Œå³ä½¿ä¼šè½¬å‡ºå®¹é”™åœˆå¤–ã€‚ sim_granularityï¼šé—´éš”å°ºå¯¸ï¼Œè½¨è¿¹ä¸Šé‡‡æ ·ç‚¹æ­¥é•¿ã€‚ scaling_speedï¼šå¯åŠ¨æœºå™¨äººåº•ç›˜çš„é€Ÿåº¦ã€‚ global_costmap_params.yamlï¼š raytrace_rangeï¼šå®æ—¶æ¸…é™¤ä»£ä»·åœ°å›¾ä¸Šéšœç¢ç‰©çš„æœ€å¤§èŒƒå›´ï¼Œæ¸…é™¤çš„æ˜¯obstacle_layerçš„æ•°æ®ã€‚ part3 topics move_base &amp; move_base_simpleï¼š 12345678ros::NodeHandle action_nh("move_base");action_goal_pub_ = action_nh.advertise&lt;move_base_msgs::MoveBaseActionGoal&gt;("goal", 1);//we'll provide a mechanism for some people to send goals as PoseStamped messages over a topic//they won't get any useful information back about its status, but this is useful for tools//like nav_view and rvizros::NodeHandle simple_nh("move_base_simple");goal_sub_ = simple_nh.subscribe&lt;geometry_msgs::PoseStamped&gt;("goal", 1, boost::bind(&amp;MoveBase::goalCB, this, _1)); ä¹‹å‰æŸ¥çœ‹èŠ‚ç‚¹å›¾çš„æ—¶å€™å‘ç°è¿™ä¸¤ä¸ªèŠ‚ç‚¹éƒ½æä¾›goalï¼Œä¸€ç›´æ²¡æƒ³é€šä¸¤è€…çš„å…³ç³»ï¼Œå‘ç°ä»£ç æ³¨é‡Šé‡Œé¢æœ‰ï¼Œmove_baseç»§æ‰¿äº†actionlibï¼Œæœ‰çŠ¶æ€åé¦ˆï¼ˆè¯¦è§wiki 1.1.2 ActionAPIï¼‰ï¼Œmove_base_simpleå°±æ˜¯ä¸€ä¸ªpublisherï¼ˆtopicå¯ä»¥æ¥è‡ªrvizï¼cmd lineï¼‰ã€‚ /result è®°å½•äº†Goal reached /feedback è®°å½•äº†æ¯ä¸ªæ—¶åˆ»æœºå™¨äººçš„ä½å§¿ /status è®°å½•äº†ä»»åŠ¡è¿›ç¨‹ï¼ˆgoal acceptedã€failedã€abortingï¼‰ /cancel æ²¡echoå‡ºä¿¡æ¯ï¼Œåº”è¯¥ä¸ä¸Šå±‚å¯¹æ¥ ã€å®šç‚¹å·¡èˆªã€‘å¦å¤–ï¼Œå®šç‚¹å·¡èˆªçš„æ—¶å€™å°†global_pathçš„bufferè®¾ç½®ä¸ºnå°±å¯ä»¥æ˜¾ç¤ºå¤šæ¡è·¯å¾„äº†ã€‚ DWAPlannerçš„global_plan &amp; local_planï¼š local_planå°±æ˜¯DWAç®—æ³•æ¯ä¸ªæ—¶åˆ»è®¡ç®—çš„æœ€ä¼˜é¢„æœŸè·¯å¾„ã€‚global_planæ˜¯æ•´ä¸ªå±€éƒ¨ä»£ä»·åœ°å›¾ä¸Šçš„è·¯å¾„ï¼Œå®ƒæ˜¯å…¨å±€è·¯å¾„çš„cropï¼Œå› ä¸ºå±€éƒ¨åŠ¨æ€ç¯å¢ƒä¸ä¼šå½±å“å…¨å±€è·¯å¾„ï¼Œæˆ‘ä»¬åªç ”ç©¶è½åœ¨localmapä»¥å†…è¿™ä¸€æ®µè·¯å¾„æ˜¯å¦éœ€è¦çŸ«æ­£ã€‚]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Autolabor]]></title>
    <url>%2F2018%2F10%2F15%2Fautolabor%2F</url>
    <content type="text"><![CDATA[ä»Šå¤©å‘ç°äº†ä¸€ä¸ªæ”¯æŒäºŒæ¬¡å¼€å‘çš„å¼€æºæ–¹æ¡ˆï¼Œè¯´ç™½äº†å°±æ˜¯æŠŠkartoã€acmlã€navigation stackç­‰å‡ ä¸ªROSå¼€æºåŒ…æ•´åˆçš„æ¯”è¾ƒæ¼‚äº®ï¼Œä»£ç ç»“æ„å€¼å¾—å€Ÿé‰´ã€‚ æ‰§è¡Œkeyboard_controlä¹‹å‰è¦å…ˆæ‰§è¡Œè„šæœ¬ï¼Œæ·»åŠ é”®ç›˜ã€‚ è€ASåŒ…é‡Œé¢æ²¡æœ‰keyboard controlï¼Œå¯ä»¥æ‰§è¡Œteleop_twist_keyboardåŒ…ã€‚autolabor_fakeæ˜¯è™šæ‹Ÿå°è½¦çš„driverï¼Œå»ºæ¨¡äº†ç”µæœºã€odomç›¸å…³ä¿¡æ¯ï¼Œè®¢é˜…cmd_velä¿¡æ¯ã€‚æ§åˆ¶çœŸå®å°è½¦æ—¶è¿™ä¸ªèŠ‚ç‚¹è¦æ›¿æ¢ã€‚ baseæ˜¯å¯¹æœºå™¨äººåº•ç›˜çš„ä»¿çœŸï¼Œlaunchæ–‡ä»¶çš„é»˜è®¤Fixed Frameæ˜¯base_linkï¼Œæƒ³è¦æ§åˆ¶å°è½¦è¿åŠ¨å¯ä»¥å°†Frameåˆ‡æˆreal_mapæˆ–odomã€‚ todolist: å‘½ä»¤è¡Œé‡Œé¢æ§åˆ¶é‡çš„æ˜¾ç¤ºä¸å¤ªå¥½çœ‹ï¼Œå¯ä»¥å°è¯•åœ¨æºæ–‡ä»¶é‡Œé¢ä¼˜åŒ–ï¼Œæ·»åŠ äº¤äº’æç¤ºã€‚ stageæ˜¯å¯¹åœºæ™¯çš„ä»¿çœŸï¼Œåœºæ™¯ç”±map_serverè¯»å–ï¼Œlaunchä»¥åå°±èƒ½æŸ¥çœ‹å½“å‰åœºæ™¯åœ°å›¾ã€‚ rostopicé‡Œé¢æœ‰ä¸€ä¸ªinitialposeä¿¡æ¯ï¼Œç”±rvizå‘å¸ƒã€‚rostopic listé‡Œé¢å¥½å¤štopicéƒ½æ˜¯rvizå‘å¸ƒçš„ï¼Œåœ¨displaysæ ç›®é‡Œé¢å–æ¶ˆå‹¾é€‰å°±ä¸ä¼šå‘å¸ƒäº†ã€‚ objectæ˜¯å¯¹éšœç¢ç‰©çš„ä»¿çœŸï¼Œè°ƒç”¨stageï¼Œæ·»åŠ interactivemarkerï¼Œç„¶åé€‰æ‹©Interactå·¥å…·ï¼Œç†è®ºä¸Šåœ°å›¾ä¸Šåº”è¯¥å‡ºç°éšœç¢ç‰©ï¼Œä½†æ˜¯æˆ‘æ²¡æ‰¾åˆ°ã€‚ã€‚ã€‚çŠ¶æ€æ˜¾ç¤ºwaiting for tf infoã€‚ ä¿®æ­£ï¼šæ·»åŠ çš„markerè¦åœ¨topicé‡Œé¢é€‰æ‹©ï¼Œä¸è¦åœ¨typeæ ä¸‹ã€‚ç„¶ååœ°å›¾ä¸Šæ”¾å¥½éšœç¢ç‰©ä»¥åè¦å³é”®applyã€‚ lidaræ˜¯å¯¹é›·è¾¾ç‚¹äº‘çš„ä»¿çœŸï¼Œlaunchä¸­ç»™äº†ä¸€ä¸ªlidarå’Œmapçš„é™æ€tfï¼Œå®é™…ä½¿ç”¨ä¸­åº”è¯¥ç»™lidarå’Œbase_linkçš„ã€‚ todolist: mapå’Œreal_mapç»™çš„æœ‰ç‚¹æ··ä¹±ï¼Œæ˜å¤©ä¼šç»Ÿä¸€ä¸€ä¸‹ã€‚ è€ASåŒ…create_mapä»¿çœŸè¿‡ç¨‹ä¸­ï¼Œç”±äºåœºæ™¯æä¾›mapserverå’Œslamç®—æ³•åŒæ—¶publishäº†/mapè¿™ä¸ªtopicï¼Œè¦è¿›è¡ŒåŒºåˆ†ï¼Œåœ¨launchæ–‡ä»¶é‡Œé¢å¯¹å…¶ä¸­ä¸€ä¸ªè¿›è¡Œremapï¼š 123&lt;node pkg="map_server" type="map_server" name="map_server" args="$(find simulation_launch)/map/MG_map.yaml"&gt; &lt;remap from="/map" to="real_map" /&gt;&lt;/node&gt; è¿™æ—¶rostopicé‡Œé¢å°±ä¼šå‡ºç°real_mapè¿™ä¸ªè¯é¢˜ï¼Œä¸¤ä¸ªåœ°å›¾èƒ½å¤ŸåŒæ—¶æ˜¾ç¤ºã€‚ ä»£ç è§£æé¦–å…ˆæ˜¯simulationåŒ…ï¼š autolabor_descriptionæ²¡å•¥å¥½è¯´çš„ï¼Œurdfæ–‡ä»¶é‡Œé¢å®šä¹‰äº†ä¸€ä¸ªrobotï¼Œæ•´ä¸ªæœºå™¨äººè¢«æ¸²æŸ“æˆäº†ä¸€ä¸ªbase_linkï¼Œæ²¡æœ‰å­èŠ‚ç‚¹ï¼Œæ‡’ã€‚ autolabor_fakeåŒ…æ˜¯åº•ç›˜é©±åŠ¨ï¼Œæä¾›äº†ä¸€ä¸ªautolabor_fake_nodeèŠ‚ç‚¹ï¼Œå…¶è®¢é˜…ç±»å‹ä¸ºgeometry_msgs/Twistçš„è¯é¢˜cmd_velï¼Œä¿¡æ¯æ¥æºå¯ä»¥æ˜¯joystickï¼keyboardï¼ˆtele_op_xxxï¼‰ï¼cmd lineï¼ˆrostopic pub xxxï¼‰ã€‚å…¶å‘å¸ƒç±»å‹ä¸ºnav_msgs/Odometryçš„è¯é¢˜odomã€‚åŒæ—¶è¯¥èŠ‚ç‚¹è¿˜ä¼šå°†odom frameåˆ°base_link frameçš„transformä¿¡æ¯æä¾›ç»™tf nodeï¼Œç”¨æ¥tf_broadcastã€‚ lidar_simulationåŒ…è‡ªèº«æä¾›äº†ä¸¤ä¸ªèŠ‚ç‚¹lidar_simulationå’Œobstacle_simulationã€‚ 3.1 LidarSimulation::getPoseè¿™ä¸ªå‡½æ•°ä¸­æœ‰ä¸€æ®µä»£ç å¼€å§‹æ¯”è¾ƒå›°æƒ‘ï¼š 1234567// ROS_INFO("roll and pitch and yaw and esp :%lf %lf %lf %lf", roll, pitch, yaw, esp); //esp=0.000001,r&amp;p=0.000000if (pow(roll,2) + pow(pitch,2) &gt; esp)&#123; start_angle = yaw + max_angle_; reverse = -1.0;&#125;else&#123; // default situation: start_angle = yaw + min_angle_; reverse = 1.0; é€šå¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éƒ½ä½¿ç”¨å³æ‰‹åæ ‡ç³»ï¼ŒäºŒç»´å¹³é¢ä¸‹ï¼Œglobal_frame_åˆ°lidar_frame_çš„åæ ‡å˜æ¢transformæ¬§æ‹‰è§’å½¢å¼ä¸‹çš„rå’Œpè§’åº”è¯¥å§‹ç»ˆæ˜¯$0.0$ï¼Œyawä»£è¡¨äº†æ¿€å…‰é›·è¾¾$x$è½´çš„å˜æ¢ï¼ŒåŠ ä¸Šmin_angle_å°±åˆ‡æ¢æˆäº†æ¿€å…‰å…‰æŸçš„åˆå§‹å‘å°„è§’åº¦start_angleã€‚å¦‚æœåæ ‡ç³»å®šä¹‰åäº†ï¼Œr&amp;på°±åº”è¯¥æœ‰å€¼ï¼Œè¿™æ—¶å› ä¸ºåæ ‡è½´å®šä¹‰åè¿‡æ¥äº†ï¼Œæ¿€å…‰å…‰æŸçš„åˆå§‹å‘å°„è§’åº¦å°±å˜æˆäº†ä»æ­£æ–¹å‘ä¸Šçš„max_angle_å¼€å§‹çš„ã€‚ 3.2 LidarSimulation::updateMapè¿™ä¸ªå‡½æ•°å€¼å¾—æ³¨æ„ï¼Œè¿™æ˜¯ä¸€ä¸ªservice clientï¼Œç”¨æ¥è°ƒç”¨åœ°å›¾æ›´æ–°ï¼Œåœ¨å½“å‰åŠŸèƒ½åŒ…çš„é»˜è®¤launchæ–‡ä»¶ä¸­ï¼ŒåªåŠ è½½äº†ä¸€æ¬¡åœ°å›¾ï¼Œæ²¡æœ‰ä½“ç°å‡ºå®ƒçš„ä½œç”¨ã€‚å½“æ‰§è¡Œå»ºå›¾ä»»åŠ¡æ—¶ï¼Œå› ä¸ºmap frameå’Œodom frameä¼šä¸æ–­è¿›è¡ŒçŸ«æ­£ï¼Œå»ºå›¾åŒ…å°±ä¼šcallè¿™ä¸ªrequestæ¥å®æ—¶æ›´æ–°åœ°å›¾ã€‚ 3.3 è¯¥åŠŸèƒ½åŒ…ä¸‹è¿˜è‡ªå®šä¹‰äº†ä¸€ä¸ªobstacle serviceï¼Œæä¾›obstacle_simulationèŠ‚ç‚¹æ¥æ›´æ–°éšœç¢ç‰©ä¿¡æ¯ã€‚è¿™é‡Œçš„éšœç¢ç‰©æ˜¯æŒ‡æ‰‹åŠ¨æ·»åŠ çš„éšœç¢ç‰©ï¼ˆinteractiveMarkerï¼‰ï¼Œlaunchæ–‡ä»¶ä¸­å¯ä»¥å®šä¹‰å…¶å½¢çŠ¶é¡¶ç‚¹ã€‚ â€‹ å…¶ä¸­çš„ObstacleSimulation::pnpolyå‡½æ•°ç”¨æ¥åˆ¤æ–­æŸç‚¹æ˜¯å¦è½åœ¨å¤šè¾¹å½¢å†…ï¼Œä¹‹å‰åˆ·ç®—æ³•æ—¶æœ‰è€ƒè™‘è¿‡è¿™ä¸ªé—®é¢˜ï¼Œè¿™é‡Œç»™å‡ºçš„è§£æ³•ä¸çŸ¥é“æ˜¯ä¸æ˜¯æœ€ä¼˜çš„ï¼Œjust for recordï¼š 1234567891011bool ObstacleSimulation::pnpoly(geometry_msgs::Polygon&amp; footprint, float&amp; x, float&amp; y)&#123; int i,j; bool c = false; for (i=0, j=footprint.points.size()-1; i&lt;footprint.points.size(); j = i++)&#123; if ( ( (footprint.points.at(i).y &gt; y) != (footprint.points.at(j).y &gt; y) ) &amp;&amp; (x &lt; (footprint.points.at(j).x-footprint.points.at(i).x) * (y-footprint.points.at(i).y) / (footprint.points.at(j).y - footprint.points.at(i).y) + footprint.points.at(i).x) )&#123; c = !c; &#125; &#125; return c;&#125; è¿™é‡Œé¢çš„forï¼Œå¾ªç¯æ¡ä»¶éå†çš„æ˜¯å¤šè¾¹å½¢çš„æ¯ä¸€æ¡è¾¹ï¼Œå¦‚$(3,0), (0,1), (1,2), (2, 3)$è¿™æ ·ã€‚åˆ¤å®šçš„æ˜¯ç»™å®šç‚¹æ˜¯å¦è½åœ¨ç»™å®šè¾¹çš„å†…ä¾§ï¼Œè¿™é‡Œæ‰€è°“çš„å†…ä¾§æ˜¯ä»¥ç»™å®šè¾¹çš„èµ·å§‹èŠ‚ç‚¹ä¸ºåŸç‚¹ï¼Œç»™å®šçº¿æ®µçš„é¡ºæ—¶é’ˆæ–¹å‘ã€‚ 3.4 obstacleçš„å…·ä½“æ“ä½œå®šä¹‰åœ¨static_mapä¸­ï¼Œè¿™é‡Œé¢å‡ºç°äº†ä¸–ç•Œåæ ‡ç³»Worldï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ …æ ¼åœ°å›¾çš„åŸç‚¹åœ¨åœ°å›¾çš„ä¸€è§’ï¼Œæ …æ ¼çš„ä½ç½®ç”¨æ•´å‹æ¥è¡¨ç¤ºï¼Œè€Œä¸–ç•Œåæ ‡ç³»ä¸­æ …æ ¼çš„ä½ç½®ç”±å…¶ä¸­å¿ƒæ¥è¡¨ç¤ºï¼Œä¸¤è€…ç›¸å·®$0.5$ä¸ªresolutionã€‚lidar_simulationé‡Œé¢åˆ›å»ºäº†ä¸€ä¸ªstatic_mapå¯¹è±¡map_ï¼Œä»¥åŠå›è°ƒå‡½æ•°LidarSimulation::obstacleHandleServerã€‚ 3.5 lidar_simulationåŠŸèƒ½åŒ…ä¸­çš„è¿™ä¸¤ä¸ªèŠ‚ç‚¹ï¼šlidar_simulationæ˜¯mapçº§çš„ï¼Œobstacle_simulationæ˜¯obstacleçº§çš„ã€‚ æ¥ä¸‹æ¥çœ‹simulation_launchåŒ…ï¼š è¿™ä¸ªåŒ…é‡Œé¢æ²¡æœ‰æºä»£ç ï¼Œåªæä¾›äº†å‡ ä¸ªlaunchæ–‡ä»¶ï¼Œç”¨æ¥ä»¿çœŸå‡ ç§ä¸åŒçš„æƒ…å†µï¼š sim_move_simulation.launchå°±æ˜¯ç®€å•çš„åº•ç›˜æ§åˆ¶ï¼Œæ§åˆ¶å°è½¦åœ¨ç»™å®šåœ°å›¾ä¸Šè¿åŠ¨ï¼ŒåŒæ—¶å¯è§†åŒ–é›·è¾¾ç‚¹äº‘ä¿¡æ¯ã€‚ create_map_simulation.launchç”¨æ¥å»ºå›¾ï¼Œåœ¨åº•ç›˜æ§åˆ¶çš„åŸºç¡€ä¸Šï¼Œå¯åŠ¨äº†å»ºå›¾åŠŸèƒ½åŒ…ã€‚å‘å¸ƒé»˜è®¤åå­—ä¸º/mapçš„topicï¼Œå‘½ä»¤è¡Œæ‰§è¡Œmap_saverä¿å­˜ã€‚ move_base_simulation.launchç”¨æ¥å¯¼èˆªï¼Œåœ¨åº•ç›˜æ§åˆ¶çš„åŸºç¡€ä¸Šï¼Œå¯åŠ¨äº†å¯¼èˆªå¥—ä»¶acml&amp;move_baseï¼Œè¿™æ—¶å°è½¦çš„åº•ç›˜æ§åˆ¶èŠ‚ç‚¹autolabor_fake_nodeè®¢é˜…çš„cmd_velä¿¡æ¯ä¸å†æ¥è‡ªteleop_keyboardï¼Œè€Œæ˜¯æ¥è‡ªmove_baseçš„è§„åˆ’ç»“æœã€‚ æœ€åæ˜¯move_base_simè¿™ä¸ªåŠŸèƒ½åŒ…ï¼š æ˜¯ç”¨ä½œçœŸå®åº•ç›˜æ§åˆ¶çš„ï¼ˆç›®æµ‹å°±æ˜¯å¯¹ROSå¼€æºçš„move_baseåŒ…çš„äºŒæ¬¡å°è£…ï¼Œè²Œä¼¼åˆ äº†ä¸€äº›ä¸ç”¨çš„æ’ä»¶ï¼‰ï¼Œå…ˆskipï¼Œæ¥ä¸‹æ¥æˆ‘ä¼šç›´æ¥è§£æROSå¯¼èˆªå¥—ä»¶ã€‚]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sublimeæ— æ³•å®‰è£…æ’ä»¶]]></title>
    <url>%2F2018%2F10%2F12%2Fsublime%E6%97%A0%E6%B3%95%E5%AE%89%E8%A3%85%E6%8F%92%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Package Controlçš„é…ç½®æ–‡ä»¶ä¸­æ·»åŠ ï¼š 123456789101112131415161718"downloader_precedence":&#123; "linux": [ "curl", "urllib", "wget" ], "osx": [ "curl", "urllib" ], "windows": [ "wininet" ]&#125;, OSXå’Œubuntuä¸‹äº²æµ‹å‡æœ‰æ•ˆã€‚]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VMwareæ‰©å®¹]]></title>
    <url>%2F2018%2F10%2F10%2FVMware%E6%89%A9%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[ä¹‹å‰åˆ›å»ºè™šæ‹Ÿæœºæ—¶ä¸çŸ¥é“éœ€è¦å¤šå¤§ç©ºé—´ï¼Œç»™äº†20Gï¼Œè£…äº†ROSç›¸å…³å¥—ä»¶ä¹‹åç£ç›˜ä½¿ç”¨ç‡åˆ°äº†90%ã€‚ ç„¶ååœ¨ç¡¬ç›˜è®¾ç½®é‡Œé¢è°ƒæ•´åˆ°äº†60Gï¼Œæˆ‘ä»¥ä¸ºè¿™æ ·å°±å¯ä»¥äº†ã€‚ã€‚ã€‚ã€‚ too naiveã€‚ã€‚ã€‚è¿˜æ˜¯è¦æ‰‹åŠ¨é…ç½®ä¸€ä¸‹ï¼Œéœ€è¦å®‰è£…å·¥å…·Gpartedï¼š 1sudo apt-get install gparted ç„¶åå…ˆæŠ¹æ‰extendedå’Œswapä¸¤ä¸ªåˆ†åŒºï¼Œç„¶åå°±å¯ä»¥resizeä¸»åˆ†åŒºäº†ï¼Œç„¶ååœ¨é‡æ–°åˆ›å»ºé‚£ä¸¤ä¸ªåˆ†åŒºå°±å¥½äº†ã€‚ Attentionï¼šç£ç›˜åªèƒ½æ‰©å±•ï¼Œä¸èƒ½å˜å°ï¼Œå› æ­¤å»ºè®®é€æ¸æ‰©å±•ã€‚]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSM, Correlative Scan Matching]]></title>
    <url>%2F2018%2F10%2F08%2FCSM%2F</url>
    <content type="text"><![CDATA[SLAMå‰ç«¯ä¸»è¦è§£å†³ä¸¤ä¸ªé—®é¢˜ï¼Œä¸€æ˜¯ä»ä¸åŒæ—¶åˆ»çš„ä¼ æ„Ÿå™¨è¾“å…¥ä¸­è¯†åˆ«å‡ºåŒä¸€ä¸ªåœ°å›¾ç‰¹å¾ï¼ŒäºŒæ˜¯è®¡ç®—æ¯ä¸ªå½“å‰æ—¶åˆ»æœºå™¨äººç›¸å¯¹è¯¥ç‰¹å¾çš„ä½å§¿ã€‚ vSLAMèƒ½å¤Ÿè½»æ¾è§£å†³å‰è€…ï¼ŒLidarSLAMè§£å†³åè€…æ— å‹åŠ›ã€‚æœ¬æ–‡è®¨è®ºæ¿€å…‰SLAMå‰ç«¯é—®é¢˜1â€”â€”ç‰¹å¾ç‚¹åŒ¹é…ã€‚ç›®å‰ä¸¤å¤§å¸¸ç”¨æ€è·¯ï¼šscan2scanâ€”â€”ICPï¼Œscan2mapâ€”â€”CSMã€‚ basicçš„CSMç®—æ³•æ€è·¯å¦‚ä¸‹ï¼šä»å‰ä¸€å¸§æœºå™¨äººä½å§¿å¼€å§‹ï¼Œå¯»æ‰¾æœ€ä¼˜åˆšæ€§å˜æ¢ï¼Œä½¿é›·è¾¾ç‚¹åœ¨æ …æ ¼åœ°å›¾ä¸­çš„ä½ç½®å°½é‡å¯¹åº”äºå æ®åº¦ä¸º1çš„æ …æ ¼ã€‚ ä¸ºäº†ä¿æŒå®šä½ä¿¡æ¯åŸæœ‰çš„æ•°ä½ç²¾åº¦ï¼Œä½¿ç”¨åŒçº¿æ€§æ’å€¼æ–¹æ³•æ¥è·å–é›·è¾¾ç‚¹çš„åœ°å›¾å€¼(å æ®åº¦)ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å…¶æ‰€åœ¨æ …æ ¼çš„åœ°å›¾å€¼æ¥ç›´æ¥å¯¹åº”ï¼š $Pm$æ˜¯é›·è¾¾ç‚¹ï¼Œ$P_{00}, P_{01}, P_{10}, P_{11}$æ˜¯é›·è¾¾ç‚¹é‚»è¿‘çš„å››ä¸ªæ …æ ¼ä¸­å¿ƒç‚¹ã€‚äºæ˜¯å¾—åˆ°åœ°å›¾å€¼ï¼š \begin{align} M(P_m)& \approx \frac{y-y_0}{y_1-y_0}M(I_0) +\frac{y_1-y}{y_1-y_0}M(I_1)\\ & =\frac{y-y_0}{y_1-y_0} \left ( \frac{x-x_0}{x_1-x_0}M(P_{11}) + \frac{x_1-x}{x_1-x_0}M(P_{01})\right)\\ & +\frac{y_1-y}{y_1-y_0}\left ( \frac{x-x_0}{x_1-x_0}M(P_{10}) + \frac{x_1-x}{x_1-x_0}M(P_{00})\right) \end{align}é›·è¾¾ç‚¹æ‰€åœ¨ä½ç½®çš„åœ°å›¾å€¼å˜åŒ–æ¢¯åº¦ï¼š \triangledown M(P_m) = \left [\frac{\delta M(P_m)}{\delta x}, \frac{\delta M(P_m)}{\delta y} \right]\\ \frac{\delta M(P_m)}{\delta x} =\frac{y-y_0}{(y_1-y_0)(x_1-x_0)}(M(P_{11})- M(P_{01})) +\frac{y_1-y}{(y_1-y_0)(x_1-x_0)}(M(P_{10})- M(P_{00}))\\ \frac{\delta M(P_m)}{\delta y} =\frac{x-x_0}{(y_1-y_0)(x_1-x_0)}(M(P_{11})- M(P_{10})) +\frac{x_1-x}{(y_1-y_0)(x_1-x_0)}(M(P_{01})- M(P_{00}))\\è®°å½“å‰æ—¶åˆ»çš„å·²æœ‰åœ°å›¾$M$ï¼Œå½“å‰å¸§å…±è¾“å…¥$n$ä¸ªé›·è¾¾ç‚¹$S_1, â€¦, S_n$ï¼Œå…¶å¯¹åº”ä½ç½®çš„å æ®åº¦ä¸º$M(S_k)$ï¼Œæœ€ä¼˜å˜æ¢å®šä¹‰ä¸º$\xi = (\Delta x, \Delta y, \psi)^T$ï¼Œåˆ™æœ€ä¼˜é—®é¢˜çš„æœ€å°äºŒä¹˜æè¿°ä¸ºï¼š \xi = argmin_{\xi} \sum_{k=1}^n[1-M(S_k(\xi))]^2scan2mapçš„é²æ£’æ€§æ›´å¼ºï¼Œä½†æ˜¯å®æ—¶æ€§ä¸Šæ‰“äº†æŠ˜æ‰£ã€‚å¯¹æ­¤ä¸»è¦æœ‰ä¸¤ä¸ªæ”¹è¿›æªæ–½ï¼šä¸€æ˜¯å±€éƒ¨æœç´¢ï¼ŒäºŒæ˜¯åˆ†è¾¨ç‡é‡‘å­—å¡”ã€‚ ä¸€ã€å±€éƒ¨æœç´¢ å®é™…è®¡ç®—ä¸­ä¼šé€‰å®šä¸€ä¸ªæœç´¢åŒºé—´ï¼Œé€šå¸¸åªåœ¨ä¸Šä¸€æ—¶åˆ»åœ°å›¾ä½ç½®çš„é™„è¿‘ï¼Œå¯¹å…¶ä¸­åŒ…å«çš„å…¨éƒ¨å¯èƒ½ä½å§¿è¿›è¡Œè¯„åˆ†ã€‚ ä¸Šå¼ï¼ˆæœ€å°äºŒä¹˜è¡¨è¾¾å¼ï¼‰åªåŒ…å«äº†é›·è¾¾ç«¯ç‚¹ï¼Œè€ƒè™‘åˆ°ä¼ æ„Ÿå™¨çš„å™ªç‚¹å½±å“ï¼Œå±€éƒ¨æå€¼å½±å“å¤§ã€‚ åŸºäºæ¨¡ç‰ˆçš„åŒ¹é…ï¼šé›·è¾¾ç«¯ç‚¹åŠå°„çº¿æ‰€åœ¨æ …æ ¼æ„æˆçš„å¤šè¾¹å½¢åŒºåŸŸï¼Œä»¥æ­¤ä½œä¸ºå±€éƒ¨åœ°å›¾ï¼Œè¿›è¡Œmap2mapåŒ¹é…ã€‚å‡å°‘å±€éƒ¨æå€¼çš„å½±å“ï¼Œæé«˜è®¡ç®—ä»£ä»·ï¼ŒåŒæ—¶è€ƒè™‘åˆ°åŠ¨æ€ç›®æ ‡ï¼Œä¼šå¼•å…¥æ–°çš„å±€éƒ¨æå€¼ã€‚ LMç®—æ³•ï¼šè¿­ä»£çš„æ–¹å¼æ±‚è§£æœ€å°äºŒä¹˜çš„æœ€ä¼˜è§£ã€‚ åˆ†æ”¯ç•Œå®šç®—æ³•ï¼šåŸºäºå¹¿åº¦ä¼˜å…ˆæœç´¢çš„ç®—æ³•ï¼Œé€šè¿‡å¯¹è§£ç©ºé—´æœç´¢æ ‘çš„åˆ†æ”¯è¿›è¡Œæ‰©å±•å’Œå‰ªæï¼Œä¸æ–­è°ƒæ•´æœç´¢æ–¹å‘ï¼ŒåŠ å¿«æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£çš„é€Ÿåº¦ã€‚ç•Œå®šæ ¸å¿ƒï¼šè‹¥å½“å‰åˆ†æ”¯çš„ä¸‹ç•Œ$C_{branch}$å°äºè§£ç©ºé—´ä¸Šç•Œ$C_{HB}$ï¼Œåˆ™è¿›è¡Œæ‹“å±•ï¼Œå¦åˆ™è¿›è¡Œè£å‰ªï¼Œç›´è‡³åˆ°è¾¾å¶å­ç»“ç‚¹ï¼Œå³æ‰¾åˆ°æœ€å°ä»£ä»·è§£ã€‚ äºŒã€å¤šåˆ†è¾¨ç‡é‡‘å­—å¡” ä¸¤å¸§é›·è¾¾ç‚¹äº‘çš„ç›¸ä¼¼åŒºåŸŸå¹¶ä¸ä¼šå½±å“åŒ¹é…çš„æœ€ç»ˆç»“æœï¼Œä½†ä¼šå‚ä¸è®¡ç®—ï¼Œå¯¼è‡´æœç´¢æ•ˆç‡é™ä½ï¼Œéœ€è¦æ›´å¤šçš„è¿­ä»£æ¬¡æ•°è¾¾åˆ°æ”¶æ•›ã€‚ å½“åœ°å›¾åˆ†è¾¨ç‡è¾ƒä½æ—¶ï¼Œéƒ¨åˆ†åœ°å›¾ä¿¡æ¯ä¼šè¢«å¿½ç•¥ï¼Œè¿™ç§é«˜ã€ä½åˆ†è¾¨ç‡ä¸‹çš„å·®å¼‚ï¼Œæœ‰åŠ©äºå¯¹åœ°å›¾ä¸­çš„ç›¸ä¼¼åœºæ™¯è¿›è¡ŒåŒºåˆ†ã€‚å®é™…ä½¿ç”¨ä¸­ï¼Œé¦–å…ˆå°†åˆå§‹ä½å§¿å¯¹åº”çš„é›·è¾¾ç‚¹äº‘ä¸æœ€ä¸Šå±‚ï¼ˆç²—åˆ†è¾¨ç‡ï¼‰çš„åœ°å›¾è¿›è¡ŒåŒ¹é…ï¼Œè®¡ç®—å‡ºå½“å‰åˆ†è¾¨ç‡ä¸‹çš„ä½å§¿ï¼Œå¹¶ä½œä¸ºåˆå§‹å€¼è¿›å…¥æ¬¡ä¸€çº§åœ°å›¾è¿›è¡ŒåŒ¹é…ï¼Œä»¥æ­¤ç±»æ¨ã€‚]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode95-äºŒå‰æœç´¢æ ‘]]></title>
    <url>%2F2018%2F08%2F16%2Fleetcode95-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%2F</url>
    <content type="text"><![CDATA[96å’Œ95éƒ½æ˜¯äºŒå‰æœç´¢æ ‘ï¼Œå…ˆåšçš„96ï¼Œæ±‚æ ‘çš„ç»“æ„æœ‰å‡ ç§ï¼Œæ²¡æ³¨æ„ç»“ç‚¹å¤§å°å…³ç³»ï¼Œç”¨åŠ¨æ€è§„åˆ’æ¥åšï¼Œ$dp[i] = dp[0]*dp[i-1] + â€¦ + dp[i-1]*dp[0]$ã€‚æ³¨æ„é€’å½’è°ƒç”¨çš„æ—¶é—´å¤æ‚åº¦ï¼Œè‡ªåº•å‘ä¸Šæ¥ç®—ï¼š 1234567891011def numTrees(self, n): res = [1, 1] if n &lt; 2: return res[n] res += [0]*(n-1) for i in range(2, n+1): for j in range(i): res[i] += res[j]*res[i-1-j] return res[n] 95è¦åˆ—å‡ºç»“æ„äº†ï¼Œæ‰å‘ç°ä»€ä¹ˆæ˜¯äºŒå‰æœç´¢æ ‘æ¥ç€â€”â€”å·¦å­æ ‘çš„ç»“ç‚¹å€¼å‡å°äºæ ¹èŠ‚ç‚¹ï¼Œå³å­æ ‘çš„ç»“ç‚¹å€¼å‡å¤§äºæ ¹èŠ‚ç‚¹ã€‚æŒ‰ç…§æƒ¯ä¾‹ï¼Œæ±‚æ•°é‡ç”¨DPï¼Œæ±‚æšä¸¾åˆ™ç”¨DFSï¼š éå†æ¯ä¸€ä¸ªæ•°å­—$i$ä½œä¸ºæ ¹èŠ‚ç‚¹ï¼Œé‚£ä¹ˆ$[1, 2, â€¦, i-1]$æ„æˆå…¶å·¦å­æ ‘ï¼Œ$[i+1, i+2, â€¦, n]$æ„æˆå…¶å³å­æ ‘ã€‚ 123456789101112131415161718192021def generateTrees(self, n): if n == 0: return [] return self.dfs(1,n)def dfs(self, b, e): if b &gt; e: return [None] res = [] for i in range(b, e+1): # set as the root node leftTrees = self.dfs(b, i-1) rightTrees = self.dfs(i+1, e) for left in leftTrees: for right in rightTrees: root = TreeNode(i) root.left = left root.right = right res.append(root) return res]]></content>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[çŠ¶æ€ä¼°è®¡]]></title>
    <url>%2F2018%2F06%2F30%2F%E7%8A%B6%E6%80%81%E4%BC%B0%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[1. æ¦‚è¿° ç»å…¸çš„SLAMæ¨¡å‹ç”±ä¸€ä¸ªè¿åŠ¨æ–¹ç¨‹å’Œä¸€ä¸ªè§‚æµ‹æ–¹ç¨‹æ„æˆï¼š \left\{ \begin{split} & x_k = f(x_{k-1}, u_k) + w_k\\ & z_{k,j} = h(y_i, x_k) + v_{kj} \end{split} \right.å…¶ä¸­ï¼Œ$x_k$è¡¨ç¤ºç›¸æœºçš„ä½å§¿ï¼Œå¯ä»¥ç”¨å˜æ¢çŸ©é˜µæˆ–æä»£æ•°æ¥è¡¨ç¤ºï¼Œ$y_i$è¡¨ç¤ºè·¯æ ‡ï¼Œä¹Ÿå°±æ˜¯å›¾åƒä¸­çš„ç‰¹å¾ç‚¹ï¼Œ$w_k$å’Œ$v_{kj}$è¡¨ç¤ºå™ªå£°é¡¹ï¼Œå‡è®¾æ»¡è¶³é›¶å‡å€¼çš„é«˜æ–¯åˆ†å¸ƒã€‚ æˆ‘ä»¬å¸Œæœ›é€šè¿‡å¸¦å™ªå£°çš„è§‚æµ‹æ•°æ®$z$å’Œè¾“å…¥æ•°æ®$u$æ¨æ–­ä½å§¿$x$å’Œåœ°å›¾$y$çš„æ¦‚ç‡åˆ†å¸ƒã€‚ä¸»è¦é‡‡ç”¨ä¸¤å¤§ç±»æ–¹æ³•ï¼Œä¸€ç±»æ˜¯æ»¤æ³¢æ–¹æ³•ï¼šåŸºäºå½“å‰çŠ¶æ€æ¥ä¼°è®¡ä¸‹ä¸€çŠ¶æ€ï¼Œå¿½ç•¥å†å²ï¼›ä¸€ç±»æ˜¯éçº¿æ€§ä¼˜åŒ–æ–¹æ³•ï¼Œä½¿ç”¨æ‰€æœ‰æ—¶åˆ»çš„æ•°æ®ä¼°è®¡æ–°çŠ¶æ€çš„æœ€ä¼˜åˆ†å¸ƒã€‚ æ»¤æ³¢æ–¹æ³•ä¸»è¦åˆ†ä¸ºæ‰©å±•å¡å°”æ›¼æ»¤æ³¢å’Œç²’å­æ»¤æ³¢ä¸¤å¤§ç±»ã€‚ éçº¿æ€§ä¼˜åŒ–æ ¹æ®å®ç°ç»†èŠ‚çš„ä¸åŒä¸»è¦åˆ†ä¸ºæ»‘åŠ¨çª—å£æ³•å’ŒPose Graphæ³•ã€‚ 2. éçº¿æ€§ä¼˜åŒ– éçº¿æ€§ä¼˜åŒ–åŸºäºå†å²ï¼ŒåŒæ—¶ä¹Ÿä½œç”¨äºå†å²ï¼Œå› æ­¤æŠŠæ‰€æœ‰å¾…ä¼°è®¡çš„å˜é‡æ”¾åœ¨ä¸€ä¸ªçŠ¶æ€å˜é‡ä¸­ï¼š x = \{x_1, ..., x_N, y_1, ..., y_M\}åœ¨å·²çŸ¥è§‚æµ‹æ•°æ®$z$å’Œè¾“å…¥æ•°æ®$u$çš„æ¡ä»¶ä¸‹ï¼Œå¯¹æœºå™¨äººçš„çŠ¶æ€ä¼°è®¡ï¼š P(x | z,u)å…ˆå¿½ç•¥æµ‹é‡è¿åŠ¨çš„ä¼ æ„Ÿå™¨ï¼Œä»…è€ƒè™‘æµ‹é‡æ–¹ç¨‹ï¼Œæ ¹æ®è´å¶æ–¯æ³•åˆ™ï¼š P(x|z) = \frac{P(z|x)P(x)}{P(z)} \varpropto P(z|x)P(x) å…ˆéªŒæ¦‚ç‡$P(x)$ï¼šå…ˆéªŒçš„æ¦‚å¿µæœ€å¥½ç†è§£ï¼Œå°±æ˜¯ä¸€ä¸ªäº‹ä»¶çš„æ¦‚ç‡åˆ†å¸ƒã€‚ ä¼¼ç„¶æ¦‚ç‡$P(z|x)$ï¼šå·²çŸ¥äº‹ä»¶çš„æ¦‚ç‡åˆ†å¸ƒï¼Œäº‹ä»¶ä¸­æŸçŠ¶æ€çš„æ¦‚ç‡ã€‚ åéªŒæ¦‚ç‡$P(x|z)$ï¼šåœ¨ç»™å®šæ•°æ®æ¡ä»¶ä¸‹ï¼Œä¸ç¡®å®šæ€§çš„æ¡ä»¶åˆ†å¸ƒã€‚ $Posterior \varpropto Likelihood * Prior$ æ±‚è§£åéªŒåˆ†å¸ƒæ¯”è¾ƒå›°éš¾ï¼Œä½†æ˜¯æ±‚ä¸€ä¸ªçŠ¶æ€æœ€ä¼˜ä¼°è®¡ï¼ˆä½¿å¾—åéªŒæ¦‚ç‡æœ€å¤§åŒ–ï¼‰æ˜¯å¯è¡Œçš„ï¼š x^*_{MAP} = argmaxP(x|z) = argmaxP(z|x)P(x) = argmaxP(z|x)å› ä¸ºå…ˆéªŒæ¦‚ç‡ä¸çŸ¥é“ï¼Œæ‰€ä»¥é—®é¢˜ç›´æ¥è½¬æˆä¸ºæ±‚è§£æå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œé—®é¢˜ä¸­çš„æœªçŸ¥æ•°æ˜¯$x$ï¼Œç›´è§‚æ„ä¹‰å°±æ˜¯ï¼šå¯»æ‰¾ä¸€ä¸ªæœ€ä¼˜çš„çŠ¶æ€åˆ†å¸ƒï¼Œä½¿å…¶æœ€å¯èƒ½äº§ç”Ÿå½“å‰è§‚æµ‹åˆ°çš„æ•°æ®ã€‚ å‡è®¾äº†å™ªå£°é¡¹$v_{kj} \thicksim N(0, Q_{k,j})$ï¼Œæ‰€ä»¥æå¤§ä¼¼ç„¶æ¦‚ç‡ä¹Ÿæœä»ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼š P(z_{kj}|x_k, y_j) = N(h (y_j, x_i), Q)æ±‚é«˜æ–¯åˆ†å¸ƒçš„æœ€å€¼é€šå¸¸å–è´Ÿå¯¹æ•°å¤„ç†ï¼Œæœ€å¤§åŒ–å˜æˆæ±‚æœ€å°åŒ–ï¼š P(x) = \frac{1}{\sqrt{(2\pi)^Ndet(\Sigma)}} exp\bigg(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\bigg)\\ -ln(P(x) ) = \frac{1}{2}ln\big((2\pi)^Ndet(\Sigma)\big) + \frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)å¯¹æ•°ç¬¬ä¸€é¡¹ä¸$x$æ— å…³ï¼Œç¬¬äºŒé¡¹ç­‰ä»·äºå™ªå£°çš„å¹³æ–¹é¡¹ã€‚å› æ­¤å¯ä»¥å¾—åˆ°ä¼˜åŒ–é—®é¢˜çš„ç›®æ ‡å‡½æ•°ï¼š e_{k} = x_k - f(x_{k-1}, u_k)\\ e_{kj} = z_{kj} - h(x_k, y_j)\\ J(x) = \Sigma e_k^TR^{-1}_ke_{k} + \Sigma e_{kj}^TQ^{-1}_ke_{kj}\\ x* = argmin_x J(x)ä»¥ä¸Šçš„æœ€å°äºŒä¹˜é—®é¢˜å¯ä»¥é‡‡ç”¨å„ç§å„æ ·çš„æ¢¯åº¦ä¸‹é™æ³•æ±‚è§£æœ€ä¼˜è§£ï¼ˆå‚è€ƒå›¾ä¼˜åŒ–ï¼‰ã€‚ Bundle Ajustment ä¼˜åŒ–é—®é¢˜æœ€ç»ˆå¯ä»¥è¡¨ç¤ºæˆ$H\Delta x = g$çš„å½¢å¼ï¼Œå…¶å¯¹è§’çº¿ä¸Šçš„ä¸¤ä¸ªçŸ©é˜µä¸ºç¨€ç–çŸ©é˜µï¼Œä¸”å³ä¸‹è§’çš„çŸ©é˜µç»´åº¦å¾€å¾€è¿œå¤§äºå·¦ä¸Šè§’ï¼ˆå› ä¸ºç‰¹å¾ç‚¹çš„æ•°ç›®è¿œå¤§äºä½å§¿èŠ‚ç‚¹ï¼‰ï¼š H = \begin{bmatrix} B &E\\ E^T & C \end{bmatrix}ä¸€ä¸ªæœ‰æ•ˆçš„æ±‚è§£æ–¹å¼ç§°ä¸ºSchuræ¶ˆå…ƒï¼Œä¹Ÿå«è¾¹ç¼˜åŒ–Marginalizationï¼Œä¸»è¦æ€è·¯å¦‚ä¸‹ï¼šé¦–å…ˆæ±‚è§£$C$çŸ©é˜µçš„é€†çŸ©é˜µï¼Œç„¶åå¯¹$H$çŸ©é˜µè¿›è¡Œæ¶ˆå…ƒï¼Œç›®æ ‡æ˜¯æ¶ˆå»å…¶å³ä¸Šè§’çš„$E$çŸ©é˜µï¼Œè¿™æ ·å°±èƒ½å¤Ÿå…ˆç‹¬ç«‹æ±‚è§£ç›¸æœºå‚æ•°$\Delta x_c$ï¼Œå†åˆ©ç”¨æ±‚å¾—çš„è§£æ¥æ±‚landmarkså‚æ•°$\Delta x_p$ï¼š \begin{bmatrix} I &-EC^{-1}\\ 0 & I \end{bmatrix} \begin{bmatrix} B &E\\ E^T & C \end{bmatrix} \begin{bmatrix} \Delta x_c \\ \Delta x_p \end{bmatrix} = \begin{bmatrix} I &-EC^{-1}\\ 0 & I \end{bmatrix} \begin{bmatrix} v \\ w \end{bmatrix} \\ \begin{bmatrix} B - EC^{-1}E^T & 0\\ E^T & C \end{bmatrix}\begin{bmatrix} \Delta x_c \\ \Delta x_p \end{bmatrix} = \begin{bmatrix} v - EC^{-1}w\\ w \end{bmatrix}å› æ­¤å¯ä»¥è§£å¾—$\Delta x_c$ï¼š \begin{bmatrix} B - EC^{-1}E^T \end{bmatrix} \Delta x_c = v - EC^{-1}w è¿™ä¸ªçŸ©é˜µç§°ä¸ºæ¶ˆå…ƒä¹‹åçš„$S$çŸ©é˜µï¼Œå®ƒçš„ç»´åº¦ä¸ç›¸æœºå‚æ•°çš„ç»´åº¦ä¸€è‡´ $S$çŸ©é˜µçš„æ„ä¹‰æ˜¯ä¸¤ä¸ªç›¸æœºå˜é‡ä¹‹é—´æ˜¯å¦å­˜åœ¨ç€å…±åŒè§‚æµ‹ç‚¹ $S$çŸ©é˜µçš„ç¨€ç–æ€§ç”±å®é™…æ•°æ®æƒ…å†µå†³å®šï¼Œå› æ­¤åªèƒ½é€šè¿‡æ™®é€šçš„çŸ©é˜µåˆ†è§£çš„æ–¹å¼æ¥æ±‚è§£ æ ¸å‡½æ•° å½“è¯¯å·®å¾ˆå¤§æ—¶ï¼ŒäºŒèŒƒæ•°å¢é•¿çš„å¾ˆå¿«ï¼Œä¸ºäº†é˜²æ­¢å…¶è¿‡å¤§æ©ç›–æ‰å…¶ä»–çš„è¾¹ï¼Œå¯ä»¥å°†å…¶æ›¿æ¢æˆå¢é•¿æ²¡é‚£ä¹ˆå¿«çš„å‡½æ•°ï¼Œä½¿å¾—æ•´ä¸ªä¼˜åŒ–ç»“æœæ›´ä¸ºç¨³å¥ï¼Œå› æ­¤åˆå«é²æ£’æ ¸å‡½æ•°ï¼Œå¸¸ç”¨çš„æ ¸æœ‰Huberæ ¸ã€Cauchyæ ¸ã€Tukeyæ ¸ç­‰ï¼ŒHuberæ ¸çš„å®šä¹‰å¦‚ä¸‹ï¼š H(e) = \left\{ \begin{split} & \frac{1}{2} e ^2, \ \ \ |e| \leq \delta\\ & \delta(|e| - \frac{1}{2}\delta), \ \ else \end{split} \right. 3. å¡å°”æ›¼æ»¤æ³¢æ»¤æ³¢æ€è·¯åŸºäºä¸€ä¸ªé‡è¦çš„å‡è®¾ï¼šä¸€é˜¶é©¬å°”å¯å¤«æ€§â€”â€”kæ—¶åˆ»çŠ¶æ€åªä¸k-1æ—¶åˆ»çŠ¶æ€æœ‰å…³ï¼Œæ•´ç†æˆä¸¤ä¸ªè¦ç´ å¦‚ä¸‹ï¼š $x_{k-1}$ contains the whole history $x_k = f(x_{k-1}, u_k, z_k)$ åœ¨è¿™é‡Œæˆ‘ä»¬åªéœ€è¦ç»´æŠ¤ä¸€ä¸ªçŠ¶æ€é‡$x_k$ï¼Œå¹¶å¯¹å®ƒä¸æ–­è¿›è¡Œè¿­ä»£æ›´æ–°ï¼Œmoreoverï¼Œå¦‚æœçŠ¶æ€é‡æœä»é«˜æ–¯åˆ†å¸ƒï¼Œæˆ‘ä»¬åªéœ€è¦ç»´æŠ¤çŠ¶æ€é‡çš„å‡å€¼å’Œæ–¹å·®å³å¯ï¼ˆè¿›ä¸€æ­¥ç®€åŒ–ï¼‰ã€‚ é¦–å…ˆè€ƒè™‘ä¸€ä¸ªçº¿æ€§ç³»ç»Ÿï¼š \left\{ \begin{split} & x_k = A_k x_{k-1}+u_k + w_k\\ & z_k = C_k x_k + v_k \end{split} \right.\\ w_k \thicksim N(0, R), v_k \thicksim N(0, Q)å¡å°”æ›¼æ»¤æ³¢å™¨çš„ç¬¬ä¸€æ­¥é¢„æµ‹ï¼Œé€šè¿‡è¿åŠ¨æ–¹ç¨‹ç¡®å®š$x_k$çš„å…ˆéªŒåˆ†å¸ƒï¼Œæ³¨æ„ç”¨ä¸åŒçš„ä¸Šæ ‡åŒºåˆ†ä¸åŒçš„æ¦‚ç‡åˆ†å¸ƒï¼šå°–å¸½å­$\hat x_k$è¡¨ç¤ºåéªŒï¼Œå¹³å¸½å­$\bar x_k$è¡¨ç¤ºå…ˆéªŒï¼š P(\bar x_k) = N(\bar x_k, \bar P_k)\\ \bar x_k = A_k \hat x_{k-1} + u_k\\ \bar P_k = A_k\hat P_{k-1}A_k^T+Rç¬¬äºŒæ­¥ä¸ºè§‚æµ‹ï¼Œé€šè¿‡åˆ†æå®é™…è§‚æµ‹å€¼ï¼Œè®¡ç®—åœ¨æŸçŠ¶æ€ä¸‹åº”è¯¥äº§ç”Ÿæ€æ ·çš„åˆ†å¸ƒï¼š P(z_k|x_k) = N(C_kx_k, Q)ç¬¬ä¸‰æ­¥ä¸ºæ›´æ–°ï¼Œæ ¹æ®ç¬¬ä¸€èŠ‚ä¸­çš„è´å¶æ–¯æ³•åˆ™ï¼Œå¾—åˆ°$x_k$çš„åéªŒåˆ†å¸ƒï¼š (\hat x_k, \hat P_k) = N(C_kx_k, Q)N(\bar x_k, \bar P_k)\\ K = \bar P_k C_k^T(C_k\bar P_k C_k^T + Q_k)^{-1}\\ \hat x_k = \bar x_k + K(z_k-C_k\bar x_k)\\ \hat P_k = (I - KC_k)\bar P_kæ•´ä½“çš„æµç¨‹å›¾å¦‚ä¸‹ï¼š å…·ä½“è¿‡ç¨‹æœ¬èŠ‚ä¸­ä¸åšå±•å¼€ï¼Œè¯¦æƒ…å¯ä»¥å‚è€ƒå¡å°”æ›¼æ»¤æ³¢ã€‚é«˜æ–¯åˆ†å¸ƒç»è¿‡çº¿æ€§å˜æ¢ä»ç„¶æœä»é«˜æ–¯åˆ†å¸ƒï¼Œå› æ­¤æ•´ä¸ªè¿‡ç¨‹æ²¡æœ‰å‘ç”Ÿä»»ä½•çš„è¿‘ä¼¼ï¼Œå› æ­¤å¯ä»¥è¯´å¡å°”æ›¼æ»¤æ³¢å™¨æ„æˆäº†çº¿æ€§ç³»ç»Ÿçš„æœ€ä¼˜æ— åä¼°è®¡ã€‚ ä¸‹é¢è€ƒè™‘éçº¿æ€§ç³»ç»Ÿï¼š SLAMä¸­ä¸ç®¡æ˜¯ä¸‰ç»´è¿˜æ˜¯å¹³é¢åˆšä½“è¿åŠ¨ï¼Œå› ä¸ºéƒ½å¼•å…¥äº†æ—‹è½¬ï¼Œå› æ­¤å…¶è¿åŠ¨æ–¹ç¨‹å’Œè§‚æµ‹æ–¹ç¨‹éƒ½æ˜¯éçº¿æ€§å‡½æ•°ã€‚ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œç»è¿‡éçº¿æ€§å˜æ¢ï¼Œé€šå¸¸å°±ä¸å†æœä»é«˜æ–¯åˆ†å¸ƒï¼Œå› æ­¤å¯¹äºéçº¿æ€§ç³»ç»Ÿï¼Œå¿…é¡»é‡‡å–ä¸€å®šçš„è¿‘ä¼¼ï¼Œå°†ä¸€ä¸ªéé«˜æ–¯åˆ†å¸ƒè¿‘ä¼¼æˆé«˜æ–¯åˆ†å¸ƒã€‚ é€šå¸¸çš„åšæ³•æ˜¯ï¼Œå°†kæ—¶åˆ»çš„è¿åŠ¨æ–¹ç¨‹å’Œè§‚æµ‹æ–¹ç¨‹åœ¨$\hat x_{k-1}$ï¼Œ$\hat P_{k-1}$å¤„åšä¸€é˜¶æ³°å‹’å±•å¼€ï¼Œå¾—åˆ°ä¸¤ä¸ªé›…å¯æ¯”çŸ©é˜µï¼š F = \frac{\partial f}{\partial x_{k-1}}\bigg|_{\hat x_{k-1}}\\ H = \frac{\partial h}{\partial x_k}\bigg|_{\hat x_k}ä¸­é—´é‡å¡å°”æ›¼å¢ç›Š$K_k$ï¼š \bar P_k = F\hat P_{k-1}F^T + R_k\\ K_k = \bar P_k H^T(H \bar P_k H^T + Q_k)^{-1}åéªŒæ¦‚ç‡ï¼š \hat x_k = \bar x_k + K_k(z_k - h(\bar x_k))\\ \hat P_k = (I - K_k H)\bar P_k å¯¹äºSLAMè¿™ç§éçº¿æ€§çš„æƒ…å†µï¼ŒEKFç»™å‡ºçš„æ˜¯å•æ¬¡çº¿æ€§è¿‘ä¼¼ä¸‹çš„æœ€å¤§åéªŒä¼°è®¡ï¼ˆMAPï¼‰ã€‚ 4. EKF VS Graph-Optimization é©¬å°”å¯å¤«æ€§æŠ›å¼ƒäº†æ›´ä¹…ä¹‹å‰çš„çŠ¶æ€ï¼Œä¼˜åŒ–æ–¹æ³•åˆ™è¿ç”¨äº†æ›´å¤šçš„ä¿¡æ¯ã€‚ éçº¿æ€§è¯¯å·®ï¼šä¸¤ç§æ–¹æ³•éƒ½ä½¿ç”¨äº†çº¿æ€§åŒ–è¿‘ä¼¼ï¼ŒEKFåªåœ¨$x_{k-1}$å¤„åšäº†ä¸€æ¬¡çº¿æ€§åŒ–ï¼Œå›¾ä¼˜åŒ–æ³•åˆ™åœ¨æ¯ä¸€æ¬¡è¿­ä»£æ›´æ–°æ—¶éƒ½å¯¹æ–°çš„çŠ¶æ€ç‚¹åšæ³°å‹’å±•å¼€ï¼Œå…¶çº¿æ€§åŒ–çš„æ¨¡å‹æ›´æ¥è¿‘åŸå§‹éçº¿æ€§æ¨¡å‹ã€‚ å­˜å‚¨ï¼šEKFç»´æŠ¤çš„æ˜¯çŠ¶æ€çš„å‡å€¼å’Œæ–¹å·®ï¼Œå­˜å‚¨é‡ä¸çŠ¶æ€ç»´åº¦æˆå¹³æ–¹å¢é•¿ï¼Œå›¾ä¼˜åŒ–å­˜å‚¨çš„æ˜¯æ¯ä¸ªçŠ¶æ€ç‚¹çš„ä½å§¿ï¼Œå­˜å‚¨çº¿æ€§å¢é•¿ã€‚]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä¼˜åŒ–åº“ï¼šCeres & g2o]]></title>
    <url>%2F2018%2F06%2F28%2F%E4%BC%98%E5%8C%96%E5%BA%93%EF%BC%9Ag2o-Ceres%2F</url>
    <content type="text"><![CDATA[Ceresä½¿ç”¨Ceresæ±‚è§£éçº¿æ€§ä¼˜åŒ–é—®é¢˜ï¼Œä¸»è¦åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼š ç¬¬ä¸€éƒ¨åˆ†ï¼šæ„å»ºä»£ä»·å‡½æ•°Cost_Functor 12345678// å®šä¹‰ä¸€ä¸ªå®ä¾‹åŒ–æ—¶æ‰çŸ¥é“çš„ç±»å‹Ttemplate &lt;typename T&gt;// è¿ç®—ç¬¦()çš„é‡è½½ï¼Œç”¨æ¥å¾—åˆ°æ®‹å·®fibool operator()(const T* const x, T* residual) const &#123; residual[0] = T(10.0) - x[0]; return true; &#125; ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºæœ€å°äºŒä¹˜é—®é¢˜problem 12345Problem problem;// ä½¿ç”¨è‡ªåŠ¨æ±‚å¯¼ï¼Œç¬¬ä¸€ä¸ª1æ˜¯è¾“å‡ºç»´åº¦ï¼ˆæ®‹å·®é¡¹ï¼‰ï¼Œç¬¬äºŒä¸ª1æ˜¯è¾“å…¥ç»´åº¦ï¼ˆä¼˜åŒ–é¡¹ï¼‰CostFunction* cost_function = new AutoDiffCostFunction&lt;CostFunctor, 1, 1&gt;(new CostFunctor);// æ·»åŠ è¯¯å·®é¡¹ï¼ŒNULLè¡¨ç¤ºä¸ä½¿ç”¨æ ¸å‡½æ•°ï¼Œxæ˜¯ä¼˜åŒ–é¡¹problem.AddResidualBlock(cost_function, NULL, &amp;x); ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ±‚è§£å™¨å‚æ•°é…ç½®Solver 1234567Solver::Options options;options.linear_solver_type = ceres::DENSE_QR; //é…ç½®å¢é‡æ–¹ç¨‹çš„è§£æ³•ï¼Œç¨ å¯†çš„QRåˆ†è§£options.minimizer_progress_to_stdout = true;//è¾“å‡ºåˆ°coutSolver::Summary summary;//ä¼˜åŒ–ä¿¡æ¯Solve(options, &amp;problem, &amp;summary);//æ±‚è§£cout &lt;&lt; summary.BriefReport() &lt;&lt; "\n";//è¾“å‡ºä¼˜åŒ–çš„ç®€è¦ä¿¡æ¯ ä½¿ç”¨æ ¸å‡½æ•°ï¼šæ•°æ®ä¸­å¾€å¾€å­˜åœ¨ç¦»ç¾¤ç‚¹ï¼Œç¦»ç¾¤ç‚¹ä¼šå¯¹å¯»ä¼˜ç»“æœé€ æˆå½±å“ï¼Œè¿™æ—¶å¯ä»¥ä½¿ç”¨ä¸€äº›æŸå¤±æ ¸å‡½æ•°æ¥å¯¹ç¦»ç¾¤ç‚¹çš„å½±å“åŠ ä»¥æ¶ˆé™¤ï¼ŒCeresåº“ä¸­æä¾›çš„æ ¸å‡½æ•°ä¸»è¦æœ‰ï¼šTrivialLoss ã€HuberLossã€ SoftLOneLoss ã€ CauchyLossã€‚ 12// ä½¿ç”¨æ ¸å‡½æ•°problem.AddResidualBlock(cost_function, new CauchyLoss(0.5, &amp;x); g2oç”¨g2oä¼˜åŒ–åº“æ¥è¿›è¡Œä¼˜åŒ–çš„æ­¥éª¤å¦‚ä¸‹ï¼š å®šä¹‰èŠ‚ç‚¹å’Œè¾¹çš„ç±»å‹ï¼Œé€šå¸¸åœ¨é»˜è®¤çš„åŸºç¡€ç±»å‹ä¸Šåšä¿®æ”¹ å®šä¹‰é¡¶ç‚¹ï¼Œé¡¶ç‚¹çš„åŸºç±»ä¸ºg2o::BaseVertex&lt;ä¼˜åŒ–å˜é‡ç»´åº¦ï¼Œæ•°æ®ç±»å‹&gt; 1class CurveFittingVertex: public g2o::BaseVertex&lt;3, Eigen::Vector3d&gt; é¡¶ç‚¹çš„æ›´æ–°å‡½æ•°oplusImplï¼šå®šä¹‰å¢é‡åŠ æ³•ï¼Œå› ä¸ºä¼˜åŒ–å˜é‡å’Œå¢é‡ä¹‹é—´å¹¶ä¸ä¸€å®šæ˜¯çº¿æ€§å åŠ çš„å…³ç³»ï¼Œå¦‚ä½å§¿å˜æ¢ã€‚ å®šä¹‰è¾¹ï¼Œ æœ¬ä¾‹ä¸­çš„è¾¹ä¸ºä¸€å…ƒè¾¹ï¼ŒåŸºç±»ä¸ºg2o::BaseUnaryEdge&lt;è§‚æµ‹å€¼ç»´åº¦ï¼Œæ•°æ®ç±»å‹ï¼Œè¿æ¥é¡¶ç‚¹ç±»å‹&gt; 1class CurveFittingEdge: public g2o::BaseUnaryEdge&lt;1, double , CurveFittingVertex&gt; è¯¯å·®é¡¹è®¡ç®—å‡½æ•°computeErrorï¼šè®¡ç®—é¢„æµ‹å€¼å’Œè§‚æµ‹å€¼çš„è¯¯å·®ã€‚ä¼°è®¡å€¼æ˜¯åŸºäºå½“å‰å¯¹ä¼˜åŒ–å˜é‡çš„estimateè®¡ç®—å‡ºçš„ï¼Œè§‚æµ‹å€¼æ˜¯ç›´æ¥è·å–çš„ï¼Œå¦‚æœ¬ä¾‹ä¸­çš„yå€¼ã€‚ æ„å»ºå›¾æ¨¡å‹ 123456789101112131415// vertexg2o::VertexSE3Expmap* pose = new g2o::VertexSE3Expmap(); // camera posepose-&gt;setId( index );pose-&gt;setEstimate( expression );optimizer.addVertex ( pose );// edgeg2o::EdgeProjectXYZ2UV* edge = new g2o::EdgeProjectXYZ2UV();edge-&gt;setId ( index );edge-&gt;setVertex ( 0, point );edge-&gt;setVertex ( 1, pose );edge-&gt;setMeasurement ( Eigen::Vector2d ( p.x, p.y ) ); // å¯¼å…¥è§‚æµ‹å€¼edge-&gt;setParameterId ( 0,0 );edge-&gt;setInformation ( Eigen::Matrix2d::Identity() ); // è®¾ç½®ä¿¡æ¯çŸ©é˜µoptimizer.addEdge ( edge ); ä¿¡æ¯çŸ©é˜µedge-&gt;setInformation(ä¿¡æ¯çŸ©é˜µ)ï¼šå› ä¸ºæœ€ç»ˆçš„ä¼˜åŒ–å‡½æ•°æ˜¯$\sum e_i^T \Sigma^{-1}e_i$ï¼Œæ˜¯è¯¯å·®é¡¹å’Œä¿¡æ¯çŸ©é˜µä¹˜ç§¯çš„å½¢å¼ã€‚ ä¼˜åŒ–å™¨é…ç½® çŸ©é˜µå—Block ä¼˜åŒ–ç®—æ³•solver å›¾æ¨¡å‹optimizer æ‰§è¡Œä¼˜åŒ–]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç›´æ¥æ³•]]></title>
    <url>%2F2018%2F06%2F16%2F%E7%9B%B4%E6%8E%A5%E6%B3%95%2F</url>
    <content type="text"><![CDATA[å‰é¢è¯´å®Œäº† PnPï¼Œè¶çƒ­æ‰“é“æ›´æ–°ç›´æ¥æ³•ï¼Œå› ä¸ºä¸¤è€…çš„æ€è·¯åŸºæœ¬ä¸€è‡´ï¼Œä¸»è¦çš„å·®åˆ«åœ¨äºPnPä¸­åˆ©ç”¨çš„æ˜¯ç‰¹å¾ç‚¹çš„é‡æŠ•å½±è¯¯å·®â€”â€”åŒ¹é…ç‚¹åœ¨queryå¸§åƒç´ å¹³é¢ä¸Šçš„å®é™…ä½ç½®å’Œä¼°è®¡ä½ç½®çš„è¯¯å·®ï¼Œç›´æ¥æ³•ä¸æå–ç‰¹å¾ç‚¹ï¼Œè€Œæ˜¯é‡‡ç”¨åƒç´ äº®åº¦è¯¯å·®ã€‚ 1. ç›´æ¥æ³•çš„æ¨å¯¼ä»¥ç¬¬ä¸€ä¸ªç›¸æœºä¸ºå‚è€ƒç³»ï¼Œç¬¬äºŒä¸ªç›¸æœºçš„è¿åŠ¨å‚æ•°ä¸º$R, t, \xi$ï¼Œå¯¹æŸä¸ªç©ºé—´ç‚¹$P$ï¼š p_1 = \begin{bmatrix} u_1\\ v_1\\ 1 \end{bmatrix} = \frac{1}{Z_1}KP\\ p_2 = \begin{bmatrix} u_2\\ v_2\\ 1 \end{bmatrix} = \frac{1}{Z_2}K(RP+t) = \frac{1}{Z_2}Kexp(\xi^{\wedge})Pä¸¤ä¸ªåƒç´ ç‚¹çš„äº®åº¦è¯¯å·®ï¼š e = I_1(p_1) - I_2(p_2)ç›®æ ‡å‡½æ•°ï¼š min_{\xi} J(\xi) = \sum_{i=1}^N||e_i^Te_i||^2_2è¯¯å·®å‡½æ•°å…³äºä¼˜åŒ–å˜é‡çš„å¯¼æ•°ï¼š \begin{split} e(\xi + \delta \xi)& = I_1\big(\frac{1}{Z_1}KP\big) - I_2\big(\frac{1}{Z_2}K(exp(\delta \xi^{\wedge})exp(\xi^{\wedge})P\big)\\ & \approx I_1\big(\frac{1}{Z_1}KP\big) - I_2\big(\frac{1}{Z_2}K(1+\delta \xi^{\wedge})exp(\xi^{\wedge})P\big)\\ & = I_1\big(\frac{1}{Z_1}KP\big) - I_2\big(\frac{1}{Z_2}Kexp(\xi^{\wedge})P\big) - I_2\big(\frac{1}{Z_2}K\delta \xi^{\wedge} exp(\xi^{\wedge})P \big)\\ & = e(\xi) - I_2\big(\frac{1}{Z_2}K\delta \xi^{\wedge} exp(\xi^{\wedge})P \big) \end{split}ä¸Šé¢çš„æ‰°åŠ¨ç›¸å…³é¡¹ä¸­ï¼Œè®°ï¼š q = \delta \xi^{\wedge} exp(\xi^{\wedge})P\\ u = \frac{1}{Z_2}Kq\\è¯¯å·®å‡½æ•°çº¿æ€§åŒ–ï¼š e(\xi + \delta \xi) = e(\xi) - I_2(u)\\ \therefore \frac{e(\xi + \delta \xi)}{\partial \delta \xi} = \frac{-I_2(u)}{\partial \delta \xi}\\ e(\xi + \delta \xi) =e(\xi) - (\frac{\partial I_2}{\partial u}\frac{\partial u}{\partial q}\frac{\partial q}{\partial \delta \xi})\delta \xi$q$è¡¨ç¤ºæ‰°åŠ¨åˆ†é‡åœ¨ç¬¬äºŒç›¸æœºåæ ‡ç³»ä¸‹çš„åæ ‡ï¼ˆå›é¡¾å…³äº$R$çš„å¾®åˆ†æ–¹ç¨‹ï¼š$\dot R(t) = \phi_0^{\wedge}R(t)$ï¼‰ï¼Œå› æ­¤$u$çš„æ„ä¹‰ä¸ºåƒç´ åæ ‡ï¼Œ$\frac{\partial I_2}{\partial u}$çš„ç‰©ç†æ„ä¹‰ä¸ºåƒç´ æ¢¯åº¦ï¼Œ$\frac{\partial u}{\partial q}$çš„ç‰©ç†æ„ä¹‰ä¸ºåƒç´ åæ ‡å…³äºä¸‰ç»´ç‚¹çš„å¯¼æ•°ï¼ˆå‚è€ƒé’ˆå­”ç›¸æœºæ¨¡å‹ï¼‰ï¼Œ$\frac{\partial q}{\partial \delta \xi}$çš„ç‰©ç†æ„ä¹‰ä¸ºä¸‰ç»´ç‚¹å…³äºæ‰°åŠ¨çš„å¯¼æ•°ï¼ˆå‚è€ƒæä»£æ•°ï¼‰ã€‚ 2. ç›´æ¥æ³•åˆ†ç±»æ ¹æ®Pçš„æ¥æºï¼Œç›´æ¥æ³•åˆ†ä¸ºä¸‰ç±»ï¼š Pæ¥è‡ªäºç¨€ç–å…³é”®ç‚¹â€”â€”ç¨€ç–ç›´æ¥æ³• Pæ¥è‡ªéƒ¨åˆ†åƒç´ ï¼Œåªä½¿ç”¨å¸¦æœ‰æ¢¯åº¦çš„åƒç´ ç‚¹â€”â€”åŠç¨ å¯†ç›´æ¥æ³• Pä¸ºæ‰€æœ‰åƒç´ â€”â€”ç¨ å¯†ç›´æ¥æ³• 3. ä»£ç å®ç°ä¸»è¦å…³æ³¨Edgeç±»é‡Œé¢é‡å®šä¹‰çš„å¢é‡æ›´æ–°å‡½æ•°linearizeOplus()é‡Œé¢JacobiançŸ©é˜µçš„å†™æ³•ã€‚ \begin{split} J & = -\frac{\partial I_2}{\partial u}\frac{\partial u}{\partial q}\frac{\partial q}{\partial \delta \xi}\\ & = -\frac{\partial I_2}{\partial u}\frac{\partial u}{\partial \delta \xi}\\ \end{split} å‰ä¸€é¡¹æ˜¯$u$å¤„çš„åƒç´ æ¢¯åº¦ï¼Œä½¿ç”¨æ•°å€¼å¯¼æ•°ï¼š 1234// jacobian from I to u (1*2)Eigen::Matrix&lt;double, 1, 2&gt; jacobian_pixel_uv;jacobian_pixel_uv ( 0,0 ) = ( getPixelValue ( u+1,v )-getPixelValue ( u-1,v ) ) /2;jacobian_pixel_uv ( 0,1 ) = ( getPixelValue ( u,v+1 )-getPixelValue ( u,v-1 ) ) /2; getPixelValueè¿™ä¸ªå‡½æ•°æ¶‰åŠåˆ°ä¸€ä¸ªåŒçº¿æ€§æ’å€¼ï¼Œå› ä¸ºä¸Šé¢çš„äºŒç»´åæ ‡uvæ˜¯é€šè¿‡ç›¸æœºæŠ•å½±å˜æ¢å¾—åˆ°çš„ï¼Œæ˜¯æµ®ç‚¹å½¢å¼ï¼Œè€Œåƒç´ å€¼æ˜¯ç¦»æ•£çš„æ•´æ•°å€¼ï¼Œä¸ºäº†æ›´ç²¾ç»†åœ°è¡¨ç¤ºåƒç´ äº®åº¦ï¼Œè¦å¯¹å›¾åƒè¿›è¡Œè¿›è¡Œæ’å€¼ã€‚ çº¿æ€§æ’å€¼ï¼šå·²çŸ¥æ•°æ®$(x_0, y_0)$å’Œ$(x_1, y_1)$ï¼Œè¦è®¡ç®—$[x_0, x_1]$åŒºé—´å†…ä»»ä¸€$x$å¯¹åº”çš„$y$å€¼ï¼š \frac{y - y_0}{x-x_0} = \frac{y_1-y_0}{x_1-x_0}\\ \therefore y = \frac{x_1 -x}{x_1 - x_0}y_0 + \frac{x-x_0}{x_1-x_0}y_1 åŒçº¿æ€§æ’å€¼ï¼šæœ¬è´¨ä¸Šå°±æ˜¯åœ¨ä¸¤ä¸ªæ–¹å‘ä¸Šåšçº¿æ€§æ’å€¼ï¼š é¦–å…ˆæ˜¯xæ–¹å‘ï¼š f(R_1) = \frac{x_2 - x}{x_2-x_1}f(Q_{11}) + \frac{x - x_1}{x_2-x_1}f(Q_{21}), \ where\ R_1 = (x, y_1)\\ f(R_2) = \frac{x_2 - x}{x_2-x_1}f(Q_{12}) + \frac{x - x_1}{x_2-x_1}f(Q_{22}), \ where\ R_2 = (x, y_2)ç„¶åyæ–¹å‘ï¼š f(P) = \frac{y_2 - y}{y_2 - y_1}f(R_1) + \frac{y-y_1}{y_2-y_1}f(R_2)ç»¼åˆèµ·æ¥å°±æ˜¯ï¼š f(x,y) = \frac{f(Q_{11})}{(x_2-x_1)(y_2-y_1)}(x_2-x)(y_2-y) + \frac{f(Q_{21})}{(x_2-x_1)(y_2-y_1)}(x-x_1)(y_2-y) \\ + \frac{f(Q_{12})}{(x_2-x_1)(y_2-y_1)}(x_2-x)(y-y_1) + \frac{f(Q_{22})}{(x_2-x_1)(y_2-y_1)}(x-x_1)(y-y_1) 1234567891011inline float getPixelValue ( float x, float y )&#123; uchar* data = &amp; image_-&gt;data[ int ( y ) * image_-&gt;step + int ( x ) ]; float xx = x - floor ( x ); float yy = y - floor ( y ); return float ( ( 1-xx ) * ( 1-yy ) * data[0] + xx* ( 1-yy ) * data[1] + ( 1-xx ) *yy*data[ image_-&gt;step ] + xx*yy*data[image_-&gt;step+1] ); åä¸¤é¡¹éƒ½æ˜¯ä¸ç›¸æœºå‚æ•°å’Œä¸‰ç»´ç‚¹åæ ‡æœ‰å…³ï¼Œå¯ä»¥åˆå¹¶ï¼ŒåŒæ—¶æ³¨æ„g2oä¸­å¯¹SE3çš„å®šä¹‰å¹³ç§»å’Œæ—‹è½¬å’Œæœ¬æ–‡è®¾å®šæ˜¯åè¿‡æ¥çš„ã€‚ \xi = \begin{bmatrix} \rho\\ \phi \end{bmatrix}\\ \frac{\partial u}{\partial \delta \xi}= \begin{bmatrix} \frac{f_x}{Z} & 0 & -\frac{f_xX}{Z^2} & |& -\frac{f_xXY}{Z^2} & f_x + \frac{f_xX^2}{Z^2} & -\frac{f_xY}{Z}\\ 0 & \frac{f_y}{Z} & -\frac{f_yY}{Z^2}& | & - f_y - \frac{f_xY^2}{Z^2} & \frac{f_yXY}{Z^2} & \frac{f_yX}{Z}\\ \end{bmatrix}12345678910111213141516// jacobian from u to xi (2*6)Eigen::Matrix&lt;double, 2, 6&gt; jacobian_uv_ksai;// xi = [\phi, \pho]jacobian_uv_ksai ( 0,0 ) = - x*y*invz_2 *fx_;jacobian_uv_ksai ( 0,1 ) = ( 1+ ( x*x*invz_2 ) ) *fx_;jacobian_uv_ksai ( 0,2 ) = - y*invz *fx_;jacobian_uv_ksai ( 0,3 ) = invz *fx_;jacobian_uv_ksai ( 0,4 ) = 0;jacobian_uv_ksai ( 0,5 ) = -x*invz_2 *fx_;jacobian_uv_ksai ( 1,0 ) = - ( 1+y*y*invz_2 ) *fy_;jacobian_uv_ksai ( 1,1 ) = x*y*invz_2 *fy_;jacobian_uv_ksai ( 1,2 ) = x*invz *fy_;jacobian_uv_ksai ( 1,3 ) = 0;jacobian_uv_ksai ( 1,4 ) = invz *fy_;jacobian_uv_ksai ( 1,5 ) = -y*invz_2 *fy_;]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenCVå®šåˆ¶æºç ç¼–è¯‘]]></title>
    <url>%2F2018%2F06%2F12%2FOpenCV%E5%AE%9A%E5%88%B6%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[1. cmakeé€‰é¡¹ æµ‹è¯•å•å…ƒå¯ä»¥å…³æ‰ï¼šBUILD_DOCSï¼ŒBUILD_EXAMPLESï¼ŒBUILD_XXX_TESTSï¼ŒBUILD_opencv_ts(ä¸€äº›å•å…ƒæµ‹è¯•ä»£ç )ï¼ŒBUILD_PACKAGE (CPACK_BINARY_XXXï¼ŒCPACK_SOURCE_XXX)ï¼ŒINSTALL_XXX å‡å°‘å¼•å…¥ä½“ç§¯ï¼šæ‰“å¼€ä¸–ç•Œæ¨¡å—å¼€å…³BUILD_opencv_world(æš‚æ—¶æ²¡å¼€ï¼Œå› ä¸ºç¼–è¯‘ä¹‹åå‘ç°æ‰¾ä¸åˆ°è¦å¼•ç”¨çš„å¤´æ–‡ä»¶äº†)ï¼Œæ‰“å¼€BUILD_SHARED_LIBS å…³æ‰éŸ³è§†é¢‘å¤„ç†ç›¸å…³æ¨¡å—ï¼šBUILD_opencv_videoï¼ŒBUILD_opencv_videoioï¼ŒBUILD_opencv_videostabï¼ŒWITH_1394ï¼ŒWITH_GSTREAMER_XXX å…³é—­GPUç›¸å…³æ¨¡å—ï¼šWITH_OPENCLï¼ŒWITH_CUDA æ‰“å¼€TBBæ¨¡å—ï¼šéšå¼çš„å¹¶è¡Œè®¡ç®—ç¨‹åºï¼Œåº•å±‚ä¾èµ–äºæ“ä½œç³»ç»Ÿçš„å¤šçº¿ç¨‹åº“ï¼ŒBUILD_TBB æ‰“å¼€vizæ¨¡å—ï¼šWITH_VTKï¼ŒBUILD_opencv_viz æš‚æ—¶æ²¡å¼€å¯Javaç›¸å…³æ¨¡å—ï¼šantï¼Œå°±æ²¡brewè¿‡è¿™ä¸ªåŒ… ä»¥ä¸Šreference from åšå®¢1ï¼Œåšå®¢2 2. extra modules with opencv3.0, SURF/SIFT and some other things have been moved to a seperate opencv_contrib repo. ä¸€éƒ¨åˆ†æ¨¡å—è¢«ç‹¬ç«‹åˆ°äº†opencv_contribè¿™ä¸ªåŒ…ï¼Œé¦–å…ˆcloneåˆ°æœ¬åœ°ï¼Œç„¶ååœ¨cmakeé€‰é¡¹é‡Œé¢æ‰¾åˆ°OPENCV_EXTRA_MODULES_PATHï¼Œå¡«å¥½ã€‚ 2. å…¶ä»–è¯´æ˜å¦å¤–ä¹‹å‰brew installçš„opencvåŒ…ä¸€å®šè¦å¸è½½æ‰ï¼Œä¸è¦ä¹±linkï¼Œå¦åˆ™INCLUDEå’ŒLIBSçš„è·¯å¾„éƒ½ä¼šå‡ºé—®é¢˜ï¼Œæ‰‹åŠ¨ä¿®æ”¹cmakeæ–‡ä»¶ä¸è¦å¤ªé…¸çˆ½ã€‚]]></content>
      <tags>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode 81 æœç´¢æ—‹è½¬æ•°ç»„2]]></title>
    <url>%2F2018%2F06%2F10%2Fleetcode-81-%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%842%2F</url>
    <content type="text"><![CDATA[å®ådissè¿™é“é¢˜ï¼Œä¸€çœ‹æ˜¯æ—‹è½¬æ’åºæ•°ç»„ç›´æ¥åšäº†ï¼Œç„¶åæ‰å‘ç°æµ‹è¯•æ ·ä¾‹é‡Œé¢å‡ºç°äº†å·¦å³è¾¹ç•Œé‡åˆçš„æƒ…å†µï¼Œç„¶åä»”ç»†å†å®¡é¢˜æ‰å‘ç°è¿™è¡Œå°å­—ï¼š ä¸åŒ…å«é‡å¤å…ƒç´ çš„æƒ…å†µä¸‹ä»£ç å®ç°å¦‚ä¸‹ï¼š 12345678910111213141516171819202122232425class Solution: def search(self, nums, target): size = len(nums) start = 0 end = size - 1 while start &lt;= end: mid = (start + end) // 2 if nums[mid] == target: return True if nums[mid] &lt;= nums[end]: if target &lt; nums[mid] or target &gt; nums[end]: end = mid - 1 else: start = mid + 1 else: if target &gt; nums[mid] or target &lt; nums[start]: start = mid + 1 else: end = mid - 1 return False å› ä¸ºæ•°ç»„ä¸­ç°åœ¨å­˜åœ¨é‡å¤çš„å…ƒç´ ï¼Œå› æ­¤æœ‰ä¸€ä¸ªç‰¹æ®Šçš„æƒ…å†µï¼šå·¦å³è¾¹ç•Œå€¼ç›¸åŒï¼Œå¹¶ä¸”nums[mid]å€¼ä¸è¾¹ç•Œå€¼ä¹Ÿç›¸åŒï¼Œè¿™æ—¶nums[mid]å¯èƒ½ä½äºä¸¤æ®µæ•°ç»„çš„ä»»æ„ä¸€è¾¹ã€‚å› æ­¤è¦ç‹¬ç«‹è®¨è®ºä¸€ä¸‹è¿™ä¸ªæƒ…å†µï¼š 1234567891011121314151617181920212223242526272829class Solution: def search(self, nums, target): size = len(nums) start = 0 end = size - 1 while start &lt;= end: mid = (start + end) // 2 if nums[mid] == target: return True if nums[mid] &lt; nums[end]: if target &lt; nums[mid] or target &gt; nums[end]: end = mid - 1 else: start = mid + 1 elif nums[mid] &gt; nums[end]: if target &gt; nums[mid] or target &lt; nums[start]: start = mid + 1 else: end = mid - 1 # nums[mid] = nums[end]çš„æƒ…å†µ else: end -= 1 return False æµ‹è¯•ç”¨æ—¶50msï¼Œå› ä¸ºè¾¹ç•Œé‡å¤çš„å¾ªç¯æ²¡æœ‰æœ‰æ•ˆåœ°äºŒåˆ†æ•°ç»„ï¼Œä½†æ˜¯æ€è·¯è´¼ç®€å•å•Šã€‚]]></content>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PnP, Perspective-n-Point]]></title>
    <url>%2F2018%2F06%2F07%2FPnP-Perspective-n-Point%2F</url>
    <content type="text"><![CDATA[1. æ¦‚è¿° PnPæ˜¯æ±‚è§£3Dåˆ°2Dç‚¹å¯¹è¿åŠ¨çš„æ–¹æ³•ï¼Œå®ƒæè¿°äº†å½“çŸ¥é“nä¸ª3Dç©ºé—´ç‚¹åŠå…¶æŠ•å½±ä½ç½®æ—¶ï¼Œå¦‚ä½•ä¼°è®¡ç›¸æœºçš„ä½å§¿å˜æ¢ã€‚æœ€å°‘åªéœ€è¦3ä¸ªç‚¹å¯¹å°±å¯ä»¥ä¼°è®¡ç›¸æœºçš„è¿åŠ¨ã€‚ è¯¥æ–¹æ³•ä½¿ç”¨çš„æ¡ä»¶æ˜¯ï¼Œå‚è€ƒç‚¹ä¸ºä¸–ç•Œåæ ‡ç³»ä¸‹çš„ç‰¹å¾ç‚¹ï¼Œå…¶ç©ºé—´ä½ç½®å·²çŸ¥ï¼Œå¹¶ä¸”çŸ¥é“queryå¸§ä¸­å¯¹åº”å‚è€ƒç‚¹çš„åƒç´ åæ ‡ã€‚ PnPé—®é¢˜çš„æ±‚è§£æ–¹æ³•æœ‰å¾ˆå¤šç§ï¼š ç›´æ¥çº¿æ€§å˜æ¢ P3P éçº¿æ€§ä¼˜åŒ–æ–¹æ³•â€”â€”Bundle Ajustment æœ¬èŠ‚ç€çœ¼äºBAæ±‚è§£æ–¹æ³•ï¼Œå…¶ä»–æ–¹æ³•æš‚æ—¶ä¸åšå±•å¼€ã€‚ 2. Bundle Ajustmentåˆ©ç”¨ä¼˜åŒ–æ±‚è§£çš„æ€è·¯æ˜¯ï¼šæœ€å°åŒ–é‡æŠ•å½±è¯¯å·®â€”â€”æœŸæœ›è®¡ç®—queryå¸§çš„ç›¸æœºä½å§¿$R, t$ï¼Œå®ƒçš„æä»£æ•°ä¸º$\xi$ï¼Œç©ºé—´ç‰¹å¾ç‚¹çš„åæ ‡ä¸º$P_i = [X_i, Y_i, Z_i]^T$ï¼Œå…¶åœ¨queryå¸§ä¸Šçš„åƒç´ åæ ‡ä¸º$u_i = [u_i, v_i]^T$ï¼Œé‚£ä¹ˆç†è®ºä¸Šï¼š s_i u_i = K exp(\xi^{\wedge})P_iæ„å»ºæˆæœ€å°äºŒä¹˜é—®é¢˜å°±æ˜¯ï¼šå¯»æ‰¾æœ€ä¼˜çš„ç›¸æœºä½å§¿$\xi$ï¼Œä½¿å¾—è¯¯å·®æœ€å°åŒ–ï¼š \xi^* = argmin_{\xi} \frac{1}{2}\sum_{i=1}^{n}||u_i -\frac{1}{s_i}Kexp(\xi^{\wedge})P_i||^2_2åœ¨ä½¿ç”¨ä¼˜åŒ–åº“æ¥æ±‚è§£ä¹‹å‰ï¼Œè¿˜æœ‰ä¸€ä¸ªé—®é¢˜â€”â€”æ¯ä¸ªè¯¯å·®é¡¹$e_i = u_i -\frac{1}{s_i}Kexp(\xi^{\wedge})P_i$çš„å¯¼æ•°$J_i$ã€‚ å›å¿†å›¾ä¼˜åŒ–ä¸­è®²è¿‡çš„ï¼Œä¼˜åŒ–é—®é¢˜æœ€ç»ˆè½¬åŒ–æˆä¸ºçŸ©é˜µçš„çº¿æ€§æ±‚è§£$H\Delta x = g$ï¼Œå…¶ä¸­çŸ©é˜µ$H$æ˜¯ç”±å•ä¸ªè¯¯å·®é¡¹ä¸€é˜¶å±•å¼€$e(x+\Delta x) = e(x) + J\Delta x$ä¸­çš„é›…å¯æ¯”çŸ©é˜µ$J_i$ æ„æˆçš„ç¨€ç–å¯¹ç§°é˜µã€‚ è¯¯å·®é¡¹æ˜¯ä¸€ä¸ªäºŒç»´å‘é‡ï¼ˆåƒç´ åæ ‡å·®ï¼‰ï¼Œä¼˜åŒ–å˜é‡æ˜¯ä¸€ä¸ªå…­ç»´å‘é‡ï¼ˆç©ºé—´ä½å§¿æä»£æ•°ï¼‰ï¼Œå› æ­¤$J$æ˜¯ä¸€ä¸ª2*6çš„çŸ©é˜µã€‚ \frac{\partial e}{\partial \delta \xi} = \lim_{\delta \xi \rightarrow0} \frac{\partial e}{\partial P^{'}}\frac{\partial P^{'}}{\partial \delta \xi}å…¶ä¸­$P^{â€˜}$æ˜¯ç‰¹å¾ç‚¹è½¬æ¢åˆ°ç›¸æœºåæ ‡ç³»ä¸‹çš„ç©ºé—´åæ ‡ï¼š su = KP^{'}\\ u = f_x \frac{X^{'}}{Z^{'}} + c_x\\ v = f_y \frac{X^{'}}{Z^{'}} + c_yå› æ­¤è¯¯å·®é¡¹å¯¼æ•°çš„ç¬¬ä¸€é¡¹ä¸ºï¼š \frac{\partial e}{\partial P^{'}} = - \begin{bmatrix} \frac{\partial u}{\partial X^{'}} & \frac{\partial u}{\partial Y^{'}} & \frac{\partial u}{\partial Z^{'}}\\ \frac{\partial v}{\partial X^{'}} & \frac{\partial v}{\partial Y^{'}} & \frac{\partial v}{\partial Z^{'}} \end{bmatrix} =- \begin{bmatrix} \frac{f_x}{Z^{'}} & 0 & -\frac{f_xX^{'}}{Z^{'2}}\\ 0 & \frac{f_y}{Z^{'}} & -\frac{f_yY^{'}}{Z^{'2}}\\ \end{bmatrix}è¯¯å·®é¡¹çš„ç¬¬äºŒé¡¹ä¸ºå˜æ¢åçš„ç‚¹å…³äºæä»£æ•°çš„å¯¼æ•°ï¼Œå‚è€ƒæä»£æ•°èŠ‚ï¼š \frac{\partial TP}{\partial \delta \xi} = \begin{bmatrix} I & -P^{'\wedge}\\ 0^T & 0^T \end{bmatrix}å…¶ä¸­$P^{â€˜\wedge}$æ˜¯$P^{â€˜}$çš„åå¯¹ç§°é˜µï¼š P^{'\wedge} = \begin{bmatrix} 0 & -Z^{'} & Y^{'}\\ Z^{'} & 0 & -X^{'}\\ -Y^{'} & X^{'} & 0 \end{bmatrix}å› æ­¤å¾—åˆ°å®Œæ•´çš„é›…å¯æ¯”çŸ©é˜µï¼š \frac{\partial e}{\partial \delta \xi} =\frac{\partial e}{\partial P^{'}} \begin{bmatrix} I & P^{'\wedge} \end{bmatrix}=- \begin{bmatrix} \frac{f_x}{Z^{'}} & 0 & -\frac{f_xX^{'}}{Z^{'2}} & |& -\frac{f_xX^{'}Y^{'}}{Z^{'2}} & f_x + \frac{f_xX^{'2}}{Z^{'2}} & -\frac{f_xY^{'}}{Z^{'}}\\ 0 & \frac{f_y}{Z^{'}} & -\frac{f_yY^{'}}{Z^{'2}}& | & - f_y - \frac{f_xY^{'2}}{Z^{'2}} & \frac{f_yX^{'}Y^{'}}{Z^{'2}} & \frac{f_yX^{'}}{Z^{'}}\\ \end{bmatrix}é™¤äº†ä¼˜åŒ–ç›¸æœºä½å§¿ä»¥å¤–ï¼Œè¿˜å¯ä»¥åŒæ—¶ä¼˜åŒ–ç‰¹å¾ç‚¹çš„ç©ºé—´ä½ç½®$P$ï¼š \frac{\partial e}{\partial P} = \frac{\partial e}{\partial P^{'}} \frac{\partial P^{'}}{\partial P}å…¶ä¸­çš„ç¬¬äºŒé¡¹ä¸ºï¼š P^{'} = exp(\xi^{\wedge})P = RP +t\\ \therefore \frac{\partial P^{'}}{\partial P} = R^T3. ä¼˜åŒ–åº“ä½¿ç”¨æ„å»ºå›¾æ¨¡å‹ï¼š ä¼˜åŒ–å˜é‡1ï¼šèŠ‚ç‚¹1ï¼šqueryç›¸æœºä½å§¿ï¼ˆå…­ç»´å‘é‡æä»£æ•°ï¼‰ ä¼˜åŒ–å˜é‡2ï¼šèŠ‚ç‚¹2ï¼šç‰¹å¾ç‚¹ç©ºé—´ä½ç½®ï¼ˆä¸‰ç»´å‘é‡åæ ‡æè¿°ï¼‰ é¢„æµ‹å€¼ï¼šè¾¹nï¼šæ ¹æ®å½“å‰estimateçš„ä¼˜åŒ–é‡ï¼ŒæŠ•å½±åˆ°æŠ•å½±å¹³é¢çš„åƒç´ åæ ‡$z_i = h(\xi, P_i)$ è§‚æµ‹å€¼ï¼šèƒ½å¤Ÿç›´æ¥è¯»å‡ºçš„ï¼Œqueryå¸§ä¸Šå¯¹åº”ç‰¹å¾ç‚¹çš„å®é™…æŠ•å½±åæ ‡$u_i$ g2oä¸­å·²ç»æä¾›äº†ç›¸è¿‘çš„åŸºç±»ï¼ˆåœ¨g2o/types/sba/types_six_dof_expmap.hä¸­ï¼‰ï¼šæä»£æ•°ä½å§¿èŠ‚ç‚¹VertexSE3Expmapã€ç©ºé—´ç‚¹ä½ç½®èŠ‚ç‚¹VertexSBAPointXYZã€æŠ•å½±æ–¹ç¨‹è¾¹EdgeProjectXYZ2UVã€‚ è¾¹ç±»é‡Œé¢è¦å…³æ³¨linearizeOpluså‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°æè¿°çš„æ˜¯éçº¿æ€§å‡½æ•°è¿›è¡Œçº¿æ€§åŒ–çš„è¿‡ç¨‹ä¸­ï¼Œæ±‚å¯¼çš„è§£æè§£ï¼ˆå½“ç„¶ä¹Ÿå¯ä»¥ä½¿ç”¨æ•°å€¼è§£ï¼‰ï¼Œæœ€åå‡½æ•°ç»™å‡ºçš„æ˜¯æ¯ä¸ªèŠ‚ç‚¹çš„å¯¼æ•°çŸ©é˜µï¼ˆ$\frac{\partial e}{\partial \delta \xi} $å’Œ$\frac{\partial e}{\partial P_i}$ï¼‰ ã€‚è¿™ç‚¹æ˜¯Ceresåº“å’Œg2oåº“çš„ä¸€ç‚¹ä¸»è¦å·®åˆ«ï¼šCereséƒ½æ˜¯ä½¿ç”¨è‡ªåŠ¨çš„æ•°å€¼å¯¼æ•°ï¼Œg2oè¦è‡ªå·±æ±‚å¯¼ã€‚ 1234567891011void EdgeProjectXYZ2UV::linearizeOplus()&#123; VertexSE3Expmap * vj = static_cast&lt;VertexSE3Expmap*&gt;(_vertices[1]); VertexSBAPointXYZ* vi = static_cast&lt;VertexSBAPointXYZ*&gt;(_vertices[0]); ... ... _jacobianOplusXi = -1./z * tmp * T.rotation().toRotationMatrix(); _jacobianOplusXj(0,0) = x*y/z^2 * cam-&gt;focal_length; ...&#125; 4. OpenCVå†…ç½®å‡½æ•°basicï¼š 12345void cv::solvePnP ( pts3d, pts2d, K, Mat(), r, t, false );Mat r, t, R;cv::solvePnP ( pts3d, pts2d, K, Mat(), r, t, false );cv::Rodrigues ( r, R ); // æ—‹è½¬å‘é‡å’Œæ—‹è½¬çŸ©é˜µçš„è½¬æ¢ advancedï¼š 1234567891011121314bool cv::solvePnPRansac( InputArray objectPoints, // 3Dç©ºé—´åæ ‡ vector&lt;cv::Point3f&gt; pts3d InputArray imagePoints, // 2Dåƒç´ åæ ‡ vector&lt;cv::Point2f&gt; pts2d InputArray cameraMatrix, // ç›¸æœºå†…éƒ¨å‚æ•°çŸ©é˜µ K InputArray distCoeffs, // ç•¸å˜ç³»æ•°å‘é‡ cv::Mat() OutputArray rvec, // æ—‹è½¬å‘é‡ OutputArray tvec, // å¹³ç§»å‘é‡ bool useExtrinsicGuess = false, // If true, the function uses the provided rvec and tvec values as initial int iterationsCount = 100, // Number of iterations float reprojectionError = 8.0,// é‡æŠ•å½±è¯¯å·®æœ€å¤§å€¼ double confidence = 0.99, OutputArray inliers = noArray(), // Output vector that contains indices of inliers in objectPoints and imagePoints . int flags = SOLVEPNP_ITERATIVE // method for solving PnP) Ransacï¼šè€ƒè™‘åˆ°æˆ‘ä»¬æä¾›çš„åŒ¹é…é‡Œé¢å­˜åœ¨è¯¯åŒ¹é…çš„æƒ…å†µï¼ŒOpenCVé‡‡ç”¨â€œéšæœºé‡‡æ ·ä¸€è‡´æ€§ç®—æ³•â€ï¼ˆRandom Sample Consensusï¼‰ï¼Œä»ç°æœ‰åŒ¹é…ä¸­éšæœºå–ä¸€éƒ¨åˆ†ç”¨æ¥ä¼°è®¡è¿åŠ¨ï¼ˆPnPçš„è§£æè§£æ³•æœ€å°‘åªéœ€è¦ä¸‰ä¸ªç‚¹å°±èƒ½è®¡ç®—ç›¸å¯¹ä½å§¿ï¼‰ï¼Œæ­£ç¡®çš„åŒ¹é…ç»“æœéƒ½æ˜¯è¿‘ä¼¼çš„ï¼Œä»è€Œå‰”é™¤è¯¯åŒ¹é…ã€‚ inlierï¼šå†…ç‚¹ï¼Œå‡½æ•°æœ€ç»ˆç»™å‡ºçš„åŒ¹é…å¯ä¿¡çš„ç‚¹ã€‚ RANSACåªé‡‡ç”¨å°‘æ•°å‡ ä¸ªéšæœºç‚¹æ¥è®¡ç®—PnPï¼Œå®¹æ˜“å—åˆ°å™ªå£°å½±å“ã€‚å·¥ç¨‹ä¸Šé€šå¸¸ä½¿ç”¨RANSACçš„è§£ä½œä¸ºåˆå€¼ï¼Œå†ä½¿ç”¨éçº¿æ€§ä¼˜åŒ–æ–¹æ³•æ±‚è§£æœ€ä¼˜å€¼ã€‚ 12345678910111213// Ransacç²—åŒ¹é…cv::solvePnPRansac( pts3d, pts2d, K, Mat(), rvec, tvec, false, 100, 4.0, 0.99, inliers );cv::Rodrigues ( rvec, R ); Eigen::Matrix3d rotation_matrix = R.at&lt;double&gt;;T_c_r_estimated_ = SE3d( SO3d(rotation_matrix), Vector3d( tvec.at&lt;double&gt;(0,0), tvec.at&lt;double&gt;(1,0), tvec.at&lt;double&gt;(2,0)) ); // BAå±€éƒ¨ä¼˜åŒ–g2o::VertexSE3Expmap* pose new g2o::VertexSE3Expmap();...pose-&gt;setEstimate(g2o::SE3Quat( T_c_r_estimated_.rotationMatrix(), T_c_r_estimated_.translation()));]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[karto key concepts]]></title>
    <url>%2F2018%2F06%2F05%2Fkarto-key-concepts%2F</url>
    <content type="text"><![CDATA[åŸºäºæå…¶æ ‡å‡†çš„å›¾ä¼˜åŒ–SLAMæ¡†æ¶æ¥å®ç°ï¼š æå‡ºäº†é‡‡ç”¨ç¨€ç–ç‚¹è°ƒæ•´ï¼ˆSPAï¼‰çš„æ–¹æ³•æ¥é«˜æ•ˆæ±‚è§£ä½å§¿å›¾ä¼˜åŒ–é—®é¢˜ï¼Œé’ˆå¯¹æœ¬ç®—æ³•çš„æ–‡çŒ®ä¸ºã€ŠEfficient Sparse Pose Adjustment for 2D Mappingã€‹ã€‚ scan matchingéƒ¨åˆ†çš„å‚è€ƒæ–‡çŒ®ä¸ºã€ŠReal-time correlative scan matchingã€‹ï¼ŒM3RSMçš„å‰èº«ï¼ key conceptsï¼š keyScanï¼šæœºå™¨äººè¿åŠ¨ä¸€å®šçš„è·ç¦»æˆ–è§’åº¦ï¼ˆå…³é”®å¸§ï¼‰ï¼Œå‚¨å­˜åœ¨sensorManagerï¼Œæ— åœ°å›¾ç¼“å­˜ã€‚ look-up tableæŸ¥æ‰¾è¡¨ï¼šæŸ¥æ‰¾è¡¨çš„æ„ä¹‰å°±æ˜¯ç›¸æ¯”äºæš´åŠ›åŒ¹é…ï¼Œä¸éœ€è¦æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—æ¯ä¸ªæ¿€å…‰æ•°æ®ä¿¡æ¯ï¼Œç›¸åŒè§’åº¦ä¸åŒä½ç½®çš„æ¿€å…‰æ•°æ®ä¿¡æ¯åªéœ€è¦è¢«ç´¢å¼•ä¸€æ¬¡ã€‚ responseå“åº”å€¼ï¼šå°†æŸ¥æ‰¾è¡¨ä»¥ä¸€å®šçš„ä½ç§»æŠ•åˆ°å­å›¾ä¸Šï¼Œæ€»å…±æœ‰nä¸ªç‚¹è¢«æŸ¥æ‰¾è¡¨å‡»ä¸­ï¼ˆhitï¼‰ï¼Œå‡»ä¸­çš„æ¯ä¸ªç‚¹å¾—åˆ†ä¸åŒï¼ˆscoreï¼‰ï¼Œç´¯åŠ å¾—åˆ†å¹¶é™¤ä»¥å¯ä»¥è¾¾åˆ°çš„æœ€é«˜åˆ†ã€‚ response = \frac{\sum_{i=0}^n goal_i}{goalmax} åæ–¹å·®ï¼šæ–‡çŒ®ä¸“é—¨ç”¨äº†ä¸€èŠ‚è®¡ç®—åæ–¹å·®ï¼Œä½†æ˜¯æ²¡çœ‹åˆ°ç”¨åœ¨å“ªï¼Œæ˜¯ä¸ºäº†åé¢æ±‚è¯¯å·®åšå‡†å¤‡å—ï¼Ÿï¼Ÿï¼Ÿ addScansæ·»åŠ é¡¶ç‚¹å’Œè¾¹ï¼šè¾¹æ˜¯è¯¯å·®å€¼ï¼Œæ·»åŠ çš„è¾¹çº¦æŸæ¥è‡ªä¸¤éƒ¨åˆ†ï¼Œ ï¼ˆ1ï¼‰link to running scansï¼Œè·å½“å‰å¸§ä¸€å®šèŒƒå›´å†…çš„æ¿€å…‰æ•°æ®é“¾ï¼ˆRunningScan chainï¼‰ã€‚ ï¼ˆ2ï¼‰link to other near chainsï¼Œä»å½“å‰èŠ‚ç‚¹å¼€å§‹å¹¿åº¦ä¼˜å…ˆéå†ä¸€å®šè·ç¦»èŒƒå›´å†…æ‰€æœ‰èŠ‚ç‚¹ï¼Œä¾æ®å½“å‰idä»sensorManagerä¸­åˆ†åˆ«é€’å¢å’Œé€’å‡å¯»æ‰¾ä¸€å®šèŒƒå›´å†…çš„chainï¼ˆä¸ä¸€å®šç›´æ¥ç›¸è¿ï¼‰ã€‚ å›ç¯æ£€æµ‹ï¼šæ“ä½œä¸æ·»åŠ è¾¹çº¦æŸç±»ä¼¼ï¼Œä½å§¿å›¾ä¸Šè¦å»é™¤é‚£äº›å’Œå½“å‰èŠ‚ç‚¹çš„æ—¶é—´ç›¸é‚»çš„èŠ‚ç‚¹ã€‚ ï¼ˆ1ï¼‰æ‰¾åˆ°ä¸€å®šè·ç¦»èŒƒå›´å†…ï¼ˆnearï¼‰å’Œç›¸è¿ï¼ˆadjacentï¼‰çš„èŠ‚ç‚¹æ·»åŠ è¿›nearLinkedScansã€‚ ï¼ˆ2ï¼‰MapperGraph::FindPossibleLoopClosureï¼šä»sensorManagerä¸­ä»å‰åˆ°åï¼Œä¾æ®åºå·æŒ‘é€‰ä¸å½“å‰èŠ‚ç‚¹åœ¨ä¸€å®šè·ç¦»èŒƒå›´å†…ï¼Œä¸”ä¸åœ¨nearLinkedScansä¸­çš„candidateã€‚è¿”å›æ½œåœ¨chainã€‚å…¶ä¸­æ¶‰åŠä¸¤ä¸ªå‚æ•°ï¼š LoopSearchMaximumDistanceï¼šcandidateScanä¸å½“å‰scançš„è·ç¦»å¿…é¡»åœ¨å¯å®¹è®¸çš„è·ç¦»å†…ã€‚ LoopMatchMinimumChainSizeï¼šchainä¸­çš„èŠ‚ç‚¹æ•°å¿…é¡»ä¸å°äºé™å®šå€¼ã€‚ ï¼ˆ3ï¼‰MapperGraph::TryCloseLoopï¼šscan2mapåŒ¹é…ï¼Œå½“responseå’Œcovarianceè¾¾åˆ°ä¸€å®šè¦æ±‚è®¤ä¸ºé—­ç¯æ£€æµ‹åˆ°ï¼Œå¾—åˆ°correct poseï¼ˆä¹Ÿå°±æ˜¯è®¤ä¸ºcandidateScançš„poseæ‰æ˜¯å½“å‰å¸§çš„å®é™…poseï¼‰ã€‚ ï¼ˆ4ï¼‰add link to loopï¼Œæ„æˆå…¨å±€é—­ç¯ã€‚ ï¼ˆ5ï¼‰è§¦å‘correctPoseï¼Œè¿›è¡Œspaä¼˜åŒ–ã€‚ ä»£ç éšæ‰‹è®°ï¼š ROSä¸Šé¢æä¾›ä¸‰ä¸ªå¼€æºåŒ…ï¼šnav2d_karto, open_karto, slam_kartoã€‚ ROS Wikiä¸Šè¿™ä¹ˆæè¿°nav2d_kartoè¿™ä¸ªpackageï¼šGraph-based Simultaneous Localization and Mapping module. Includes OpenKarto GraphSLAM library by â€œSRI Internationalâ€. open_kartoï¼šå¼€æºçš„kartoåŒ…ï¼Œå®ç°åº•å±‚çš„kartoslam slam_kartoï¼šroså±‚ï¼Œåº”ç”¨å±‚çš„kartoslamæ¥å£ The LaserRangeFinder contains parameters for physical laser sensor used by the mapper for scan matching Also contains information about the maximum range of the sensor and provides a threshold for limiting the range of readings. The optimal value for the range threshold depends on the angular resolution of the scan and the desired map resolution. resolutionï¼š0.25 &amp; 0.5 &amp; 1 degree number of range readings (beams)ï¼šï¼ˆmaximumAngle - minimumAngleï¼‰ï¼angularResolution + 1 GridStatesï¼š0 for Unknownï¼Œ100 for Occupiedï¼Œ 255 for Freeã€‚ flipYï¼šæœ€å¼€å§‹æœºå™¨äººåº”è¯¥å¤„åœ¨ä¸–ç•Œåæ ‡ç³»çš„åŸç‚¹ï¼Œä¼ æ„Ÿå™¨åæ ‡ç³»ä¸æœºå™¨äººbaselinkå­˜åœ¨ä¸€ä¸ªåæ ‡å˜æ¢ï¼ŒåŸå§‹çš„ä¼ æ„Ÿå™¨åæ ‡ç³»ä½ç½®åº”è¯¥ä¸åœ°å›¾åæ ‡ç³»é‡åˆï¼Œè¿™å°±æ˜¯worldå’Œgridä¹‹é—´çš„offsetã€‚flipæ˜¯å•¥å‘¢ï¼Ÿï¼Ÿ LookupArray[index]ï¼šCreate lookup tables for point readings at varying angles in grid. This is to speed up finding best angle/position for a localized range scan MapperGraphï¼šèŠ±å¼æ„é€ ä½å§¿å›¾ CorrelationGridï¼šImplementation of a correlation grid used for scan matching Region of Interest ROIï¼š smearï¼šThe point readings are smeared by this value in X and Y to create a smoother response. ä¸ªäººç†è§£è¿™å¥è¯æ˜¯è¯´ç‚¹å®¹æ˜“ç”Ÿæˆçªå˜ï¼Œç”¨ä»¥ç‚¹ä¸ºä¸­å¿ƒçš„ä¸€å°ç‰‡åŒºåŸŸå¹³æ»‘ä¸€ç‚¹ã€‚ ScanMatchï¼šè¿”å›å“åº”å€¼response å‰ç«¯åŒ¹é…è°ƒç”¨m_pSequentialScanMatcher-&gt;MatchScan é—­ç¯æ£€æµ‹è°ƒç”¨m_pLoopScanMatcher-&gt;MatchScan ä¸¤ä¸ªå‡½æ•°ç»§æ‰¿äºScanMatcher::MatchScanï¼š 12345678kt_double ScanMatcher::MatchScan( LocalizedRangeScan* pScan, const LocalizedRangeScanVector&amp; rBaseScans, Pose2&amp; rMean, Matrix3&amp; rCovariance, kt_bool doPenalize, kt_bool doRefineMatch) // default is True, å†³å®šæ˜¯å¦åšç²¾åŒ¹é…// @return: strength of response (best response) å…¶ä¸­ä¼šè°ƒç”¨ScanMatcher::CorrelateScanæ–¹æ³•ã€‚ScanMatcher::CorrelateScanæ–¹æ³•ä¸­è°ƒç”¨ScanMatcher::GetResponseæ–¹æ³•è®¡ç®—å“åº”å€¼ã€‚ 123kt_double ScanMatcher::GetResponse( kt_int32u angleIndex, kt_int32s gridPositionIndex) const GetResponseçš„æ ¸å¿ƒåœ¨kt_int8u* pByteå’Œconst LookupArray* pOffsetsä¸¤ä¸ªæ•°æ®ç»“æ„ï¼š å‰è€…æ˜¯åœ¨correlationGridèŒƒå›´å†…çš„real sensedå æ®æƒ…å†µã€‚ åè€…æ˜¯lookup-tableä¸­ï¼ˆå·²çŸ¥åœ°å›¾ï¼‰è¯»å–çš„æ …æ ¼å æ®æƒ…å†µï¼ŒåªåŒ…å«å æ®çš„æ …æ ¼ï¼Œkeyæ˜¯angularã€‚ è®¡ç®—responseåªè¦çœ‹åœ°å›¾ä¸Šçš„å æ®ç‚¹æ˜¯å¦åœ¨è§‚æµ‹ä¸­æ˜¯å¦ä¹Ÿæ˜¯å æ®çš„ï¼š 123456789101112for (kt_int32u i = 0; i &lt; pOffsets-&gt;GetSize(); i++)&#123; // ignore points that fall off the grid kt_int32s pointGridIndex = gridPositionIndex + pAngleIndexPointer[i]; if (!math::IsUpTo(pointGridIndex, m_pCorrelationGrid-&gt;GetDataSize()) || pAngleIndexPointer[i] == INVALID_SCAN) &#123; continue; &#125; // uses index offsets to efficiently find location of point in the grid response += pByte[pAngleIndexPointer[i]];&#125; æœ€ç»ˆçš„responseè¦normalizeï¼š 123// normalize responseresponse /= (nPoints * GridStates_Occupied); // GridStates_Occupied = 100,assert(fabs(response) &lt;= 1.0); kartoåªåœ¨é—­ç¯çš„æ—¶å€™è§¦å‘åç«¯ä¼˜åŒ–CorrectPoses()ï¼ŒScanSolverçš„å®ç°åœ¨Samplesè·¯å¾„ä¸‹çš„SpaSolverï¼Œè°ƒç”¨äº†ç°æœ‰çš„BAæ±‚è§£å™¨sba(A Generic Sparse Bundle Adjustment C/C++ Package Based on the Levenberg-Marquardt Algorithm)ã€‚ å‚æ•°&amp;ä¼˜åŒ–æ–¹å‘ é—­ç¯ä¸­candidateæ•°é‡çš„è°ƒæ•´ï¼š å‡å°LoopSearchMaximumDistanceï¼Œè¿›å…¥candidateèŒƒå›´çš„èŠ‚ç‚¹æ•°æ®å‡å°‘ å‡å°LoopMatchMinimumChainSizeï¼Œç”¨æ¥è®¡ç®—ä¼˜åŒ–çš„candidateæ•°é‡å‡å°‘ å¢å¤§minimum_travel_distanceå’Œminimum_travel_headingï¼Œè¿™æ ·æ€»ä½“çš„èŠ‚ç‚¹æ•°å‡å°‘ Map_update_intervalï¼šå‘å¸ƒåœ°å›¾çš„é—´éš”ï¼Œå…¶ä¸»è¦è¿‡ç¨‹æ˜¯éå†å½“å‰æ‰€æœ‰èŠ‚ç‚¹æ•°æ®ï¼Œå¯¹æ¯ä¸ªæ …æ ¼çš„å æœ‰çŠ¶æ€è¿›è¡Œåˆ¤å®šï¼Œç”Ÿæˆæ …æ ¼åœ°å›¾ã€‚ ScanBufferSizeå’ŒScanBufferMaximumScanDistanceï¼šæ§åˆ¶bufferä¹Ÿå°±æ˜¯chainçš„å¤§å°ã€‚chainä¸èƒ½å¤ªå¤§ä¹Ÿä¸èƒ½å¤ªå°ï¼Œå¤ªå°ä¼šé€ æˆå‰ç«¯è¯¯å·®ç´¯ç§¯ï¼Œå¤ªå¤§ä¼šå¯¼è‡´æ„å»ºé—­ç¯çš„èŠ‚ç‚¹æ•°ä¸è¶³ã€‚æ¨èå€¼æ˜¯ScanBufferMaximumScanDistanceï¼minimum_travel_distanceã€‚ ä½å§¿çº æ­£ä¸­ï¼š CorrelationSearchSpaceDimensionï¼šThe size of the search grid CorrelationSearchSpaceResolutionï¼šThe size of the correlation grid å›ç¯æ£€æµ‹ä¸­ï¼š LoopSearchMaximumDistanceï¼šé—­ç¯æ£€æµ‹çš„æœç´¢è·ç¦»ï¼Œæ•°å€¼è¶Šå¤§èƒ½è¶Šæ—©å‘ç°é—­ç¯ï¼Œä¹Ÿèƒ½å®¹å¿æ›´å¤§çš„åç¦»è¯¯å·®ã€‚ LoopMatchMinimumResponseCoarseå’ŒLoopMatchMinimumResponseFineï¼šç²—åŒ¹é…å’Œç²¾åŒ¹é…çš„å“åº”é˜ˆå€¼ï¼Œä¸é—­ç¯ä¸­candidateæ•°é‡ç›¸å…³ã€‚é˜ˆå€¼è¿‡ä½ä¼šå¯¼è‡´candidateè¿…é€Ÿè¢«å¡«æ»¡ï¼ŒçœŸæ­£å¥½çš„ç‚¹è¿˜æ²¡æ‰¾åˆ°ã€‚é˜ˆå€¼è¿‡é«˜ä¼šå¯¼è‡´å›ç¯å¤±è´¥ï¼ˆä¸€ç›´æ‰¾ä¸åˆ°å›ç¯ç‚¹ï¼‰ï¼Œåœ°å›¾ä¸Šå‡ºç°é‡å½±ã€‚ CPU Usageç®—æ³•èµ„æºå ç”¨çš„ä¸»è¦å‹åŠ›æ¥æºï¼š åœ°å›¾æ›´æ–° å›ç¯æ£€æµ‹ SPAä¼˜åŒ–]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode79 å•è¯æœç´¢]]></title>
    <url>%2F2018%2F06%2F04%2Fleetcode79-%E5%8D%95%E8%AF%8D%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[æŒ‚ä¸€é“å¾ˆçŒ¥ççš„é¢˜ï¼ŒäºŒç»´ç½‘æ ¼ä¸­æœç´¢å•è¯ï¼ŒåŒä¸€å•å…ƒæ ¼ä¸èƒ½é‡å¤ä½¿ç”¨ï¼š 12345678910board =[ [&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;E&apos;], [&apos;S&apos;,&apos;F&apos;,&apos;C&apos;,&apos;S&apos;], [&apos;A&apos;,&apos;D&apos;,&apos;E&apos;,&apos;E&apos;]]ç»™å®š word = &quot;ABCCED&quot;, è¿”å› true.ç»™å®š word = &quot;SEE&quot;, è¿”å› true.ç»™å®š word = &quot;ABCB&quot;, è¿”å› false. æ²¡å•¥å¥½ç®—æ³•ï¼Œå°±æ˜¯DFSï¼Œä½†æ˜¯å‘åœ¨äºvisitedçš„å­˜å‚¨ï¼Œpythonæ•°ç»„é»˜è®¤æµ…æ‹·è´ï¼Œé€’å½’ä¼ è¿›å»å†å›åˆ°ä¸Šä¸€å±‚ç½‘æ ¼çŠ¶æ€å°±å˜äº†ï¼Œä¹‹å‰ä¸€è´¯çš„åšæ³•å°±æ˜¯æ–°å¼€ä¸€å—å†…å­˜ç©ºé—´ï¼Œä¼ æ–°çš„æ•°ç»„è¿›å»ï¼Œç„¶è€Œè¿™æ¬¡è¶…æ—¶äº†ï¼Œå› ä¸ºæµ‹è¯•ç”¨ä¾‹çš„äºŒç»´æ•°ç»„å°ºå¯¸è´¼å¤§ï¼Œç»ˆäºæœ‰æœºä¼šæ­£è§†è¿™ä¸ªé—®é¢˜ï¼Œå¹¶è·å–æ­£ç¡®çš„æ‰“å¼€æ–¹å¼ï¼š å°ºå¯¸è´¼å¤§çš„äºŒç»´æ•°ç»„ï¼Œæ¯æ¬¡åªéœ€è¦ä¿®æ”¹ä¸€ä¸ªå€¼ï¼Œé‡æ–°åˆ’ç©ºé—´æ‹·è´å†ä¿®æ”¹æ—¶é—´å¤æ‚åº¦ç¬é—´å¢å¤§O(m*n)å€ï¼Œå¾ˆæ˜æ˜¾ä¼ åŸæ¥çš„æ•°ç»„è¿›å»æ¯”è¾ƒåˆé€‚ã€‚ ä½†æ˜¯æ·±å±‚é€’å½’ä¼šä¿®æ”¹ä¼ è¿›å»çš„å‚æ•°ï¼Œå› æ­¤åœ¨æ¯æ¬¡é€’å½’ä¹‹å‰å…ˆåˆ›å»ºä¸€ä¸ªtmpï¼Œè®°å½•ä¿®æ”¹è¡Œä¸ºï¼Œé€’å½’å‡½æ•°è¿›è¡Œå®Œä»¥åï¼Œå†æ ¹æ®è®°å½•æ¢å¤åŸæ¥çš„å‚æ•°ï¼Œä¿è¯æœ¬å±‚å‚æ•°ä¸å˜ã€‚ 123456789101112131415161718def search_tail(board, word, h, w): size = len(word) char = word[0] height, width = len(board), len(board[0]) exist = False if h - 1 &gt;= 0 and board[h-1][w] == char: if size == 1: return True else: tmp = board[h-1][w] board[h-1][w] = 'INF' exist = search_tail(board, word[1:], h-1, w) if exist: return True board[h-1][w] = tmp if h + 1 &lt; height and board[h+1][w] == char: ... ... ç„¶åé’ˆå¯¹æœ¬é¢˜è¿˜æœ‰ä¸€ä¸ªéªšæ“ä½œï¼Œå¾ˆå¤šäººä¸“é—¨åˆ›å»ºä¸€ä¸ªvisitedè¡¨æ¥è®°å½•æœç´¢è·¯å¾„ï¼Œä½†æ˜¯å› ä¸ºæœ¬é¢˜çš„äºŒç»´æ•°ç»„é™å®šå­˜å‚¨å­—æ¯ï¼Œæ‰€ä»¥ä»»æ„ä¸€ä¸ªéå­—æ¯éƒ½å¯ä»¥ä½œä¸ºæ ‡å¿—ä½ï¼Œç¾æ»‹æ»‹åˆçœä¸‹ä¸€ä¸ªO(m*n)ã€‚]]></content>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kd-tree]]></title>
    <url>%2F2018%2F06%2F01%2Fkd-tree%2F</url>
    <content type="text"><![CDATA[Referenceï¼šBentley J L. Multidimensional Binary Search Trees Used for Associative Searching[J]. Communications of the Acm, 1975, 18(9):509-517. å‰é¢æ›´æ–°basic ICPçš„æ—¶å€™ç•™äº†ä¸€ä¸ªå‘â€”â€”æœ€è¿‘é‚»çš„æ±‚æ³•ã€‚çº¿æ€§æ‰«æï¼Ÿæ‰‹åŠ¨æŒ¥æ‰‹ã€‚ 1. å…ˆè¯´è¯´è·ç¦»å§1.1 æ¬§å¼è·ç¦» d(x,y) = \sqrt{(x_1 - y_1)^2 + (x_2-y_2)^2 + ... + (x_n-y_n)^2} = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}1.2 æ ‡å‡†åŒ–æ¬§å¼è·ç¦» é¦–å…ˆå°†æ•°æ®å„ç»´åº¦åˆ†é‡éƒ½æ ‡å‡†åŒ–åˆ°å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼Œå†æ±‚æ¬§å¼è·ç¦»ï¼š X_{stand} = \frac{X - \mu}{\sigma}ç®€å•æ¨å¯¼åå¯ä»¥å‘ç°ï¼Œæ ‡å‡†åŒ–æ¬§å¼è·ç¦»å®é™…ä¸Šå°±æ˜¯ä¸€ç§åŠ æƒæ¬§å¼è·ç¦»ï¼š d(x,y) = \sqrt{\sum_{i=1}^n (\frac{x_i-y_i}{s_i})^2}1.3 é©¬æ°è·ç¦»ï¼ˆMahalanobis Distanceï¼‰ D(X_i,X_j) = \sqrt{(X_i - X_j)S^{-1}(X_i-X_j)}å…¶ä¸­$S$ä¸ºåæ–¹å·®çŸ©é˜µ$Cov$ï¼š Cov(X,Y) = E\{[X-E(X)][Y-E(Y)]\}è‹¥åæ–¹å·®çŸ©é˜µæ˜¯å•ä½é˜µï¼ˆæ ·æœ¬å„ç»´åº¦ä¹‹é—´ç‹¬ç«‹åŒåˆ†å¸ƒï¼‰ï¼Œé‚£å…¬å¼å°±å˜æˆæ¬§å¼è·ç¦»äº†ï¼Œè‹¥åæ–¹å·®çŸ©é˜µæ˜¯å¯¹è§’é˜µï¼Œé‚£å…¬å¼å°±å˜æˆæ ‡å‡†åŒ–æ¬§å¼è·ç¦»äº†ã€‚ 1.4 ç›¸ä¼¼åº¦ ç›¸ä¼¼åº¦ä¹Ÿæ˜¯è·ç¦»çš„ä¸€ç§è¡¨å¾æ–¹å¼ï¼Œè·ç¦»è¶Šç›¸è¿‘ï¼Œç›¸ä¼¼åº¦è¶Šé«˜ã€‚ æ¬§å¼è·ç¦»ç›¸ä¼¼åº¦ï¼šå°†æ¬§å¼è·ç¦»é™å®šåœ¨0 1ä¹‹é—´å˜åŒ– ç›¸ä¼¼åº¦ = \frac{1}{1+æ¬§å¼è·ç¦»} ä½™å¼¦ç›¸ä¼¼åº¦ï¼š-1åˆ°1ä¹‹é—´å˜åŒ– cos\theta = \frac{A\cdot B}{||A||\ ||B||} çš®å°”é€Šç›¸å…³ç³»æ•°ï¼š-1åˆ°1ä¹‹é—´å˜åŒ– 1return numpy.corrcoef(A, B) 2. KDæ ‘ k-NNç®—æ³• æ¨èç³»ç»Ÿ SIFTç‰¹å¾åŒ¹é… ICPè¿­ä»£æœ€è¿‘ç‚¹ æœ€è¿‘åœ¨åšçš„M3RCMä¸­çš„å †ç»“æ„ï¼ˆè¿™ä¸ªæœ‰ç‚¹ç‰µå¼ºï¼Œå› ä¸ºä¸æ˜¯äºŒå‰æ ‘ï¼‰ æ€»ä¹‹ä»¥ä¸Šè¿™äº›åŸºäºåŒ¹é…ï¼æ¯”è¾ƒçš„ç›®çš„è€Œè¿›è¡Œçš„æ•°æ®åº“æŸ¥æ‰¾ï¼å›¾åƒæ£€ç´¢ï¼Œæœ¬è´¨ä¸Šéƒ½å¯ä»¥å½’ç»“ä¸ºé€šè¿‡è·ç¦»å‡½æ•°åœ¨é«˜ç»´çŸ¢é‡ä¹‹é—´çš„ç›¸ä¼¼æ€§æ£€ç´¢é—®é¢˜ã€‚ ä¸€ç»´å‘é‡æœ‰äºŒåˆ†æ³•æŸ¥æ‰¾ï¼Œå¯¹åº”åœ°é«˜ç»´ç©ºé—´æœ‰æ ‘å½¢ç»“æ„ä¾¿äºå¿«é€Ÿæ£€ç´¢ã€‚åˆ©ç”¨æ ‘å½¢ç»“æ„å¯ä»¥çœå»å¯¹å¤§éƒ¨åˆ†æ•°æ®ç‚¹çš„æœç´¢ï¼Œä»è€Œå‡å°‘æ£€ç´¢çš„è®¡ç®—é‡ã€‚ KDæ ‘æ˜¯ä¸€ç§äºŒå‰æ ‘ï¼Œé€šè¿‡ä¸æ–­åœ°ç”¨å‚ç›´äºæŸåæ ‡è½´çš„è¶…å¹³é¢å°†kç»´ç©ºé—´åˆ‡åˆ†æ„é€ è€Œæˆã€‚ 2.1 æ„é€ æ ‘ é€’å½’åˆ›å»ºèŠ‚ç‚¹ï¼šèŠ‚ç‚¹ä¿¡æ¯åŒ…å«åˆ‡åˆ†åæ ‡è½´å’Œåˆ‡åˆ†ç‚¹ï¼Œä»è€Œç¡®å®šè¶…å¹³é¢ï¼Œå°†å½“å‰ç©ºé—´åˆ‡åˆ†ä¸ºå·¦å³ä¸¤ä¸ªå­ç©ºé—´ï¼Œé€’å½’ç›´åˆ°å½“å‰å­ç©ºé—´å†…æ²¡æœ‰å®ä¾‹ä¸ºæ­¢ã€‚ 123456class Node: def __init__(self, point, axis): self.value = point self.axis = axis self.left = None self.right = None ä¸ºäº†ä½¿å¾—æ„é€ å‡ºçš„KDæ ‘å°½å¯èƒ½å¹³è¡¡ï¼ˆé«˜æ•ˆåˆ†å‰²ç©ºé—´ï¼‰ï¼š é€‰æ‹©åæ ‡è½´ï¼šç®€å•ç‚¹çš„æ–¹å¼æ˜¯å¾ªç¯äº¤æ›¿é€‰æ‹©åæ ‡è½´ï¼Œå¤æ‚ç‚¹çš„åšæ³•æ˜¯é€‰æ‹©å½“å‰æ–¹å·®æœ€å¤§çš„è½´ä½œä¸ºåˆ‡åˆ†è½´ã€‚ é€‰æ‹©åˆ‡åˆ†ç‚¹ï¼šå–é€‰å®šåæ ‡è½´ä¸Šæ•°æ®çš„ä¸­å€¼ä½œä¸ºåˆ‡åˆ†ç‚¹ã€‚ æ³¨æ„ï¼šKDæ ‘çš„æ„é€ æ—¨åœ¨é«˜æ•ˆåˆ†å‰²ç©ºé—´ï¼Œå…¶å¶å­èŠ‚ç‚¹å¹¶éæ˜¯æœ€è¿‘é‚»æœç´¢ç­‰åº”ç”¨åœºæ™¯çš„æœ€ä¼˜è§£ã€‚ 12345678910def kdTree(points, depth): if len(points) == 0: return None axis = depth % len(points[0]) points.sort(key=lambda x: x[axis]) cut_idx = centreValue(points) node = Node(points[cut_idx], axis) node.left = kdTree(points[:cut_idx], depth+1) node.right = kdTree(points[cut_idx+1:], depth+1) return node å¯¹äºåŒ…å«nä¸ªå®ä¾‹çš„kç»´æ•°æ®æ¥è¯´ï¼Œæ„é€ KDæ ‘çš„æ—¶é—´å¤æ‚åº¦ä¸ºO(k*n*log n)ã€‚ 2.2 æ–°å¢èŠ‚ç‚¹ é€’å½’å®ç°ï¼šä»æ ¹èŠ‚ç‚¹å¼€å§‹åšæ¯”è¾ƒï¼Œå¤§äºåˆ™æ’å…¥å·¦å­æ ‘ï¼Œå°äºåˆ™æ’å…¥å³å­æ ‘ã€‚ç›´åˆ°è¾¾åˆ°å¶å­èŠ‚ç‚¹ï¼Œå¹¶åˆ›å»ºæ–°çš„å¶å­èŠ‚ç‚¹ã€‚ 2.3 åˆ é™¤èŠ‚ç‚¹ å°†å¾…åˆ é™¤çš„èŠ‚ç‚¹çš„æ‰€æœ‰å­èŠ‚ç‚¹ç»„æˆä¸€ä¸ªé›†åˆï¼Œé‡æ–°æ„å»ºKDå­æ ‘ï¼Œæ›¿æ¢å¾…åˆ é™¤èŠ‚ç‚¹ã€‚ 2.4 æœ€è¿‘é‚»æœç´¢ æœç´¢æœ€è¿‘é‚»ç®—æ³•ä¸»è¦åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šé¦–å…ˆæ˜¯æ·±åº¦ä¼˜å…ˆéå†ï¼Œç›´åˆ°é‡åˆ°å¶å­èŠ‚ç‚¹ï¼Œç”Ÿæˆæœç´¢è·¯å¾„ã€‚ 12345678910111213141516171819202122232425def searchNearest(node, target): # input: node: root node of the tree # target: list # output: nearest: list # dist: distance between target and nearest if node == None: return None # ç”Ÿæˆæœç´¢è·¯å¾„ search_path = deque() nearest = node print("search path: ") while node: print(node.value) search_path.append(node) # if Dist(nearest.value, target) &gt; Dist(node.value, target): # nearest.value = node.value # minDist = Dist(node.value, target) axis = node.axis if target[axis] &gt; node.value[axis]: node = node.right else: node = node.left ... ... ç„¶åæ˜¯å›æº¯æŸ¥æ‰¾ï¼Œå¦‚æœç›®æ ‡ç‚¹å’Œå½“å‰æœ€è¿‘ç‚¹æ„æˆçš„çƒå½¢åŒºåŸŸä¸å…¶ä¸Šæº¯èŠ‚ç‚¹ç›¸äº¤ï¼Œé‚£ä¹ˆå°±æœ‰ä¸€ç§æ½œåœ¨çš„å¯èƒ½â€”â€”ä¸Šæº¯èŠ‚ç‚¹çš„å¦ä¸€ä¸ªå­ç©ºé—´çš„å®ä¾‹å¯èƒ½ä½äºå½“å‰è¿™ä¸ªçƒå½¢åŒºåŸŸå†…ï¼Œå› æ­¤è¦è¿›è¡Œä¸€æ¬¡åˆ¤æ–­ã€‚ 1234567891011121314151617181920212223242526def searchNearest(node, target): # input: node: root node of the tree # target: list # output: nearest: list # dist: distance between target and nearest ... ... # å›æº¯ print("\nsearch backwards: ") nearest = search_path.pop() minDist = Dist(nearest.value, target) while search_path: node = search_path.pop() print(node.value) if node.axis: axis = node.axis if minDist &gt; Dist1(node.value[axis], target[axis]): if target[axis] &gt; node.value[axis]: search_path.append(node.left) else: search_path.append(node.right) if Dist(target, nearest.value) &gt; Dist(node.value, target): nearest = node minDist = Dist(node.value, target) return nearest.value, minDist ä¸¤ä¸ªå‚è€ƒç‚¹ï¼š 123samples = [(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)]target = (2.1, 3.1)target = (2, 4.5) KDæ ‘æœç´¢çš„æ ¸å¿ƒå°±æ˜¯ï¼šå½“æŸ¥è¯¢ç‚¹çš„é‚»åŸŸä¸åˆ†å‰²è¶…å¹³é¢ä¸¤ä¾§ç©ºé—´äº¤å‰²æ—¶ï¼Œéœ€è¦æŸ¥æ‰¾å¦ä¸€ä¾§å­ç©ºé—´ï¼ï¼ï¼ç®—æ³•å¹³å‡å¤æ‚åº¦O(N logN)ã€‚å®é™…æ—¶é—´å¤æ‚åº¦ä¸å®ä¾‹åˆ†å¸ƒæƒ…å†µæœ‰å…³ï¼Œ$t_{worst} = O(kN^{1-\frac{1}{k}})$ï¼Œé€šå¸¸è¦æ±‚æ•°æ®è§„æ¨¡è¾¾åˆ°$N \geq 2^D$æ‰èƒ½è¾¾åˆ°é«˜æ•ˆçš„æœç´¢ã€‚ 3. æ”¹è¿›ç®—æ³•ï¼šBBFç®—æ³•å›æº¯æ˜¯ç”±æŸ¥è¯¢è·¯å¾„å†³å®šçš„ï¼Œå› æ­¤ä¸€ç§ç®—æ³•æ”¹è¿›æ€è·¯å°±æ˜¯å°†æŸ¥è¯¢è·¯å¾„ä¸Šçš„ç»“ç‚¹æ’åºï¼Œå›æº¯æ£€æŸ¥æ€»æ˜¯ä»ä¼˜å…ˆçº§æœ€é«˜çš„æ ‘èŠ‚ç‚¹å¼€å§‹â€”â€”Best-Bin-First BBFç®—æ³•ã€‚è¯¥ç®—æ³•èƒ½ç¡®ä¿ä¼˜å…ˆæ£€ç´¢åŒ…å«æœ€é‚»è¿‘ç‚¹å¯èƒ½æ€§è¾ƒé«˜çš„ç©ºé—´ã€‚ ä¼˜å…ˆé˜Ÿåˆ—ï¼šä¼˜å…ˆçº§å–å†³äºå®ƒä»¬ç¦»æŸ¥è¯¢ç‚¹çš„è·ç¦»ï¼Œè·ç¦»è¶Šè¿‘ï¼Œä¼˜å…ˆçº§è¶Šé«˜ï¼Œå›æº¯çš„æ—¶å€™ä¼˜å…ˆéå†ã€‚ å¯¹å›æº¯å¯èƒ½éœ€è¦è·¯è¿‡çš„ç»“ç‚¹åŠ å…¥é˜Ÿåˆ—ï¼šåˆ‡åˆ†çš„æ—¶å€™ï¼ŒæŠŠæœªé€‰ä¸­çš„é‚£ä¸ªå…„å¼Ÿç»“ç‚¹åŠ å…¥åˆ°é˜Ÿåˆ—ä¸­ã€‚]]></content>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Real-Time loop Loop Closure in 2D Lidar SLAM è®ºæ–‡ç¬”è®°]]></title>
    <url>%2F2018%2F05%2F27%2FReal-Time-loop-Loop-Closure-in-2D-Lidar-SLAM%2F</url>
    <content type="text"><![CDATA[æ–‡ç« çš„æ ¸å¿ƒæ€æƒ³åœ¨äºè§£å†³loop closureé—®é¢˜ã€‚ å…¨å±€åœ°å›¾ç”±ä¸€ç³»åˆ—çš„submapæ„æˆï¼Œæ¯ä¸ªsubmapåˆ™ç”±ä¸€ç³»åˆ—çš„ä½å§¿èŠ‚ç‚¹åŠå¯¹åº”çš„scanæ•°æ®æ„æˆã€‚ æ–‡ç« çš„é‡ç‚¹åœ¨ç¬¬å››éƒ¨åˆ†å’Œç¬¬äº”éƒ¨åˆ†ï¼š ç¬¬å››éƒ¨åˆ†ï¼šlocal 2d slamï¼Œå°†scanä¸å½“å‰submapçš„åŒ¹é…é—®é¢˜è½¬åŒ–æˆä¸€ä¸ªæœ€å°äºŒä¹˜ä¼˜åŒ–é—®é¢˜ï¼Œç”±ceresæ¥æ±‚è§£ã€‚å‚è€ƒæ–‡çŒ®ã€ŠMany-to-Many Multi-Resolution Scan Matching ã€‹ ç¬¬äº”éƒ¨åˆ†ï¼šclosing loopï¼Œé‡‡ç”¨SPAè¿›è¡Œåç«¯loop closureï¼Œæå‡ºä¸€ç§å¹¶è¡Œçš„scanä¸finished submapsåŒ¹é…çš„æ–¹æ³•BBSï¼Œå¤§å¹…æé«˜ç²¾åº¦å’Œé€Ÿåº¦ã€‚å‚è€ƒæ–‡çŒ®ã€ŠEfficient Sparse Pose Adjustment for 2D Mapping(SPA)ã€‹ã€ã€ŠReal-Time Correlative Scan Matching(BBS)ã€‹]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä¸‰ç»´åˆšä½“è¿åŠ¨ & æä»£æ•°]]></title>
    <url>%2F2018%2F05%2F12%2F%E4%B8%89%E7%BB%B4%E5%88%9A%E4%BD%93%E8%BF%90%E5%8A%A8-%E6%9D%8E%E4%BB%A3%E6%95%B0%2F</url>
    <content type="text"><![CDATA[0. å‘é‡ åæ ‡ï¼šé¦–å…ˆç¡®å®šä¸€ä¸ªåæ ‡ç³»ï¼Œä¹Ÿå°±ç¡®å®šäº†ä¸€ç»„åŸº$(e_1, e_2, e_3)$ï¼Œé‚£ä¹ˆå‘é‡$a$çš„åæ ‡ä¸ºï¼š a = [e_1, e_2, e_3] \begin{bmatrix} a_1\\ a_2\\ a_3 \end{bmatrix} = a_1e_1 + a_2e_2 + a_3e_3 å†…ç§¯ï¼šå¯¹å‘é‡$a, b \in R^3$ï¼Œå…¶å†…ç§¯ä¸ºï¼š a \cdot b = a^Tb = \Sigma a_ib_i = |a||b|coså†…ç§¯å¯ä»¥æè¿°å‘é‡é—´çš„æŠ•å½±å…³ç³»ã€‚ å¤–ç§¯ï¼š a \times b = \begin{vmatrix} i & j & k\\ a_1 & a_2 & a_3\\ b_1 & b_2 & b_3 \end{vmatrix}= \begin{bmatrix} 0 & -a_3 & a_2\\ a_3 & 0 & -a_1\\ -a_2 & a_1 & 0 \end{bmatrix}b=a^{\wedge}bå¤–ç§¯çš„æ–¹å‘å‚ç›´ä¸è¿™ä¸¤ä¸ªå‘é‡ï¼Œå¤§å°ä¸º$|a||b|sin$ã€‚ å¤–ç§¯å¯ä»¥è¡¨ç¤ºå‘é‡çš„æ—‹è½¬ï¼Œå‘é‡$a$åˆ°$b$çš„æ—‹è½¬å‘é‡ï¼Œå¤–ç§¯çš„æ–¹å‘æ˜¯æ—‹è½¬å‘é‡çš„æ–¹å‘ï¼Œå¤§å°ç”±å¤¹è§’å†³å®šã€‚ 1. æ—‹è½¬çŸ©é˜µRä¸å˜æ¢çŸ©é˜µT é€šå¸¸è®¾ç½®å›ºå®šçš„ä¸–ç•Œåæ ‡ç³»$O_w$å’Œè¿åŠ¨çš„ç›¸æœºåæ ‡ç³»$O_c$ï¼Œç›¸æœºè¿åŠ¨æ˜¯åˆšä½“è¿åŠ¨ï¼Œä¸¤ä¸ªåæ ‡ç³»ä¹‹é—´çš„å˜æ¢ç§°ä¸ºæ¬§å¼å˜æ¢ã€‚ æ—‹è½¬çŸ©é˜µ$R$ï¼šå¯ä»¥æè¿°ç›¸æœºçš„æ—‹è½¬ åæ ‡ç³»æ—‹è½¬å‰ååŒä¸€ä¸ªå‘é‡çš„åæ ‡å˜æ¢å…³ç³»ï¼š a =\begin{bmatrix} a_1\\ a_2\\ a_3 \end{bmatrix}= \begin{bmatrix} e_1^T\\ e_2^T\\ e_3^T \end{bmatrix} \begin{bmatrix} e_1^{'} & e_2^{'}& e_3^{'} \end{bmatrix} \begin{bmatrix} a_1^{'}\\ a_2^{'}\\ a_3^{'} \end{bmatrix}= Ra^{'}ä¸éš¾éªŒè¯æ—‹è½¬çŸ©é˜µæ˜¯è¡Œåˆ—å¼ä¸º1çš„æ­£äº¤çŸ©é˜µï¼Œå› æ­¤å¯ä»¥æŠŠæ—‹è½¬çŸ©é˜µçš„é›†åˆç‰¹æ®Šæ­£äº¤ç¾¤å®šä¹‰å¦‚ä¸‹ï¼š SO(n) = \{R \in R^{n*n} | RR^T=I, det(R)=1\}ç›¸åçš„æ—‹è½¬ï¼š a = Ra^{'}\\ a^{'} = R^{-1}a = R^Ta æ¬§å¼å˜æ¢ï¼šåŒ…æ‹¬æ—‹è½¬å’Œå¹³ç§» a^{'} = Ra + t é½æ¬¡åæ ‡ï¼šå°„å½±å‡ ä½•çš„æ¦‚å¿µï¼Œæ¯ä¸ªåˆ†é‡åŒä¹˜ä¸€ä¸ªéé›¶å¸¸æ•°ä»ç„¶è¡¨ç¤ºåŒä¸€ä¸ªç‚¹ï¼š \tilde{x} = [x,y,z,w]^T=[x/w, y/w, z/w, 1]^T é½æ¬¡å˜æ¢çŸ©é˜µ$T$ï¼šä½¿å¾—æ¬§å¼å˜æ¢ä»æ—§ä¿æŒçº¿æ€§å…³ç³»ï¼š \begin{bmatrix} a^{'}\\ 1 \end{bmatrix}= \begin{bmatrix} R & t\\ 0 &1 \end{bmatrix} \begin{bmatrix} a\\ 1 \end{bmatrix} =T \begin{bmatrix} a\\ 1 \end{bmatrix}å˜æ¢çŸ©é˜µçš„é›†åˆç‰¹æ®Šæ¬§å¼ç¾¤ï¼š SE(3) = \left\{T= \begin{bmatrix} R & t\\ 0 & 1 \end{bmatrix} \in R^{4*4} | R\in SO(3), t \in R^3 \right\} 2. æ—‹è½¬å‘é‡ Axis-Angleä¸€ä¸ªæ—‹è½¬åªæœ‰3ä¸ªè‡ªç”±åº¦ï¼Œæ—‹è½¬çŸ©é˜µRè¦ç”¨9çš„å‚æ•°æ¥æè¿°ï¼Œæ˜¾ç„¶æ˜¯å†—ä½™çš„ã€‚ä¸€ç§ç´§å‡‘çš„æ–¹å¼â€”â€”ä»»ä½•æ—‹è½¬éƒ½å¯ä»¥ç”¨ä¸€ä¸ªæ—‹è½¬è½´$n$å’Œä¸€ä¸ªæ—‹è½¬è§’$\theta$æ¥åˆ»ç”»ï¼š R = cos\theta I + (1-cos\theta)nn^T + sin\theta n^{\wedge}\\ \theta = arccos(\frac{tr(R)-1}{2})\\æ—‹è½¬è½´ä¸Šçš„å‘é‡åœ¨æ—‹è½¬åä¸å‘ç”Ÿæ”¹å˜ï¼Œå› æ­¤æœ‰ï¼š Rn = nè½¬è½´$n$æ˜¯æ—‹è½¬çŸ©é˜µ$R$çš„ç‰¹å¾å€¼1å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œå¯ä»¥ç”±æ­¤æ¥è®¡ç®—è½¬è½´$n$ã€‚ 3. æ¬§æ‹‰è§’ rpyæŠŠæ—‹è½¬åˆ†è§£åˆ°3ä¸ªè½´ä¸Šï¼Œrpyè§’çš„æ—‹è½¬é¡ºåºæ˜¯ZYXï¼š é¦–å…ˆç»•ç‰©ä½“çš„Zè½´æ—‹è½¬ï¼Œå¾—åˆ°åèˆªè§’yaw ç„¶åç»•æ—‹è½¬ä¹‹åçš„Yè½´æ—‹è½¬ï¼Œå¾—åˆ°ä¿¯ä»°è§’pitch ç»•æ—‹è½¬ä¹‹åçš„Xè½´æ—‹è½¬ï¼Œå¾—åˆ°æ»šè½¬è§’roll ä¸‡å‘é”é—®é¢˜ï¼šåœ¨ä¿¯ä»°è§’ä¸º$\pm 90^{\circ}$æ—¶ï¼Œç¬¬ä¸€æ¬¡å’Œç¬¬ä¸‰æ¬¡æ—‹è½¬ä½¿ç”¨åŒä¸€æ ¹è½´ï¼Œä¸¢äº†è‡ªç”±åº¦â€”â€”å¥‡å¼‚æ€§é—®é¢˜ã€‚ 4. å››å…ƒæ•° qå››å…ƒæ•°æ˜¯ä¸€ç§æ‰©å±•çš„è´Ÿæ•°ï¼Œç”±ä¸€ä¸ªå®éƒ¨å’Œä¸‰ä¸ªè™šéƒ¨ç»„æˆï¼Œå¯ä»¥æŠŠä¸‰ä¸ªè™šéƒ¨è„‘è¡¥æˆç©ºé—´ä¸­çš„ä¸‰æ ¹è½´ï¼š q = q_0 + q_1i + q_2j + q_3k\\ \left\{ \begin{split} & i^2 = j^2=k^2=-1\\ & ij = k, ji = -k\\ & jk = i, kj = -i\\ & ki=j, ik=-j \end{split} \right. ä¹˜ä»¥$i$å¯¹åº”ç€ç»•$i$è½´æ—‹è½¬$180^{\circ}$ ä»»æ„çš„æ—‹è½¬å¯ä»¥ç”±ä¸¤ä¸ªäº’ä¸ºç›¸åæ•°çš„å››å…ƒæ•°è¡¨ç¤º ä¸æ—‹è½¬å‘é‡$n = [n_x, n_y, n_z]^T, \theta$è½¬æ¢å…³ç³»ï¼š q = [cos\frac{\theta}{2}, n_xsin\frac{\theta}{2}, n_ysin\frac{\theta}{2}, n_zsin\frac{\theta}{2}]^T\\ \left\{ \begin{split} & \theta = 2arccos q_0\\ & [n_x, n_y, n_z]^T = [q_1, q_2, q_3]^T/sin\frac{\theta}{2} \end{split} \right. ä¸æ—‹è½¬çŸ©é˜µ$R$çš„å…³ç³»ï¼š R = \begin{bmatrix} 1-2q_2^2 - 2q_3^2 & 2q_1q_2-2q_0q_3 & 2q_1q_3+2q_0q_2\\ 2q_1q_2+2q_0q_3 & 1-2q_1^2 - 2q_3^2 & 2q_2q_3-2q_0q_1\\ 2q_1q_3-2q_0q_2 & 2q_2q_3+2q_0q_1 & 1-2q_1^2 - 2q_2^2 \end{bmatrix}\\ q_0 = \frac{\sqrt{tr(R)+1}}{2}, q_1 = \frac{R_{23}-R_{32}}{4q_0}, q_2 = \frac{R_{31}-R_{13}}{4q_0}, q_3 = \frac{R_{12}-R_{21}}{4q_0} è¡¨ç¤ºæ—‹è½¬ï¼š ç©ºé—´ä¸­ç‚¹$p = [x, y,z]^T\in R^3$ï¼Œå·²çŸ¥æ—‹è½¬è½´è§’$n,\theta$ï¼Œæ—‹è½¬ä¹‹åç‚¹åæ ‡å˜ä¸º$p^{â€˜}$ï¼Œå¦‚æœç”¨æ—‹è½¬çŸ©é˜µæè¿°ï¼š p^{'} = Rpå››å…ƒæ•°$q = [cos\frac{\theta}{2}, nsin\frac{\theta}{2}]$ï¼Œé‚£ä¹ˆæ—‹è½¬åçš„ç‚¹$p^{â€˜}$å¯ä»¥è¡¨ç¤ºä¸ºï¼š p^{'} = qpq^{-1} 5. æç¾¤ä¸Šé¢æåˆ°äº†æ—‹è½¬çŸ©é˜µæ„æˆçš„ç‰¹æ®Šæ­£äº¤ç¾¤$SO(3)$å’Œç”±å˜æ¢çŸ©é˜µæ„æˆçš„ç‰¹æ®Šæ¬§å¼ç¾¤$SE(3)$ï¼š SO(n) = \left\{R \in R^{n*n} | RR^T=I, det(R)=1\right\} \\ SE(3) = \left\{T= \begin{bmatrix} R & t\\ 0 & 1 \end{bmatrix} \in R^{4*4} | R\in SO(3), t \in R^3 \right\} $SO(n)$å’Œ$SE(n)$å¯¹åŠ æ³•ä¸å°é—­ï¼Œå¯¹ä¹˜æ³•æ˜¯å°é—­çš„ã€‚ ç¾¤æ˜¯ä¸€ç§é›†åˆ$A$åŠ ä¸Šä¸€ç§è¿ç®—$\ \cdot \ $çš„ä»£æ•°ç»“æ„ï¼Œè®°ä½œ$G = (A, \ \cdot \ )$ï¼Œç¾¤å†…å…ƒç´ æ»¡è¶³å°é—­æ€§ã€ç»“åˆå¾‹ã€å¹ºå…ƒã€å¯é€†å››ä¸ªæ¡ä»¶ã€‚ æç¾¤æ˜¯æŒ‡å…·æœ‰è¿ç»­æ€§è´¨çš„ç¾¤ã€‚åˆšä½“åœ¨ç©ºé—´ä¸­èƒ½å¤Ÿè¿ç»­åœ°è¿åŠ¨ï¼Œå› æ­¤$SO(n)$å’Œ$SE(n)$æ˜¯æç¾¤ã€‚ 6. æä»£æ•°6.1 å¼•å…¥ å¯¹ä»»æ„æ—‹è½¬çŸ©é˜µ$R$ï¼Œéƒ½æ»¡è¶³$RR^T=I$ã€‚æŠŠå®ƒå†™æˆå…³äºæ—¶é—´çš„å‡½æ•°$R(t)$æœ‰ï¼š R(t)R(t)^T = Iå¯¹ç­‰å¼ä¸¤è¾¹æ±‚å¯¼ï¼š \dot R(t)R(t)^T + R(t)\dot R(t)^T=0\\ \dot R(t)R(t)^T =-\big(\dot R(t)R(t)^T \big)^Tå¯ä»¥çœ‹å‡º$\dot R(t)R(t)^T $æ˜¯ä¸€ä¸ªåå¯¹ç§°é˜µï¼Œå¯¹ä»»æ„ä¸€ä¸ªåå¯¹ç§°é˜µï¼Œéƒ½å¯ä»¥æ‰¾åˆ°ä¸€ä¸ªä¸ä¹‹å¯¹åº”çš„å‘é‡ï¼š a^{\wedge} = A, A^{\vee}=aäºæ˜¯å¯ä»¥æ‰¾åˆ°ä¸€ä¸ªä¸‰ç»´å‘é‡$\phi(t) \in R^3$ä¸ä¹‹å¯¹åº”ï¼š \dot R(t)R(t)^T = \phi(t)^{\wedge}\\ \dot R(t) = \phi(t)^{\wedge}R(t)å¯ä»¥çœ‹åˆ°ï¼Œæ¯å¯¹æ—‹è½¬çŸ©é˜µæ±‚ä¸€æ¬¡å¯¼æ•°ï¼Œåªéœ€å·¦ä¹˜ä¸€ä¸ªåå¯¹ç§°é˜µ$\phi(t)^{\wedge}$å³å¯ã€‚ æ±‚è§£ä¸Šé¢çš„å¾®åˆ†æ–¹ç¨‹ï¼Œå¯ä»¥å¾—åˆ°$R(t) = exp(\phi^{\wedge}t)$ã€‚ä¹Ÿå°±æ˜¯è¯´$\phi$æè¿°äº†$R$åœ¨å±€éƒ¨çš„å¯¼æ•°å…³ç³»ã€‚ 6.2 æä»£æ•° æ¯ä¸ªæç¾¤éƒ½æœ‰ä¸ä¹‹å¯¹åº”çš„æä»£æ•°ã€‚æä»£æ•°æè¿°äº†æç¾¤çš„å±€éƒ¨æ€§è´¨ã€‚ æä»£æ•°ç”±ä¸€ä¸ªé›†åˆ$V$ï¼Œä¸€ä¸ªæ•°åŸŸ$F$ï¼Œå’Œä¸€ä¸ªäºŒå…ƒè¿ç®—ææ‹¬å·$[,]$ç»„æˆï¼Œè®°ä½œ$( V, F, [,])$ã€‚æä»£æ•°çš„å…ƒç´ æ»¡è¶³å°é—­æ€§ã€åŒçº¿æ€§ã€è‡ªåæ€§ã€é›…å¯æ¯”ç­‰ä»·å››æ¡æ€§è´¨ã€‚ ä¸Šä¸€èŠ‚çš„$\phi$å°±æ˜¯$SO(3)$å¯¹åº”çš„æä»£æ•°$so(3)$ï¼Œä¸¤è€…çš„å…³ç³»ç”±æŒ‡æ•°æ˜ å°„ç»™å®šï¼š R = exp(\phi^{\wedge})\\ so(3) = \left\{ \phi \in R^3, \Phi = \phi^{\wedge} \in R^{3*3}\right\} $SE(3)$å¯¹åº”çš„æä»£æ•°$se(3)$ä½äº$R^6$ç©ºé—´ä¸­ï¼š se(3) = \left\{ \xi = \begin{bmatrix} \rho\\ \phi \end{bmatrix} \in R^6, \rho \in so(3), \xi^{\wedge} = \begin{bmatrix} \phi^{\wedge} & \rho\\ 0^T & 0 \end{bmatrix} \in R^{4*4} \right\} æŒ‡æ•°æ˜ å°„ ç”±äº$\phi$æ˜¯ä¸€ä¸ªä¸‰ç»´å‘é‡ï¼Œå› æ­¤å¯ä»¥å†™ä½œ$\theta a$çš„å½¢å¼ï¼Œ$a$æ˜¯ä¸€ä¸ªå•ä½å‘é‡ï¼Œå› æ­¤å…·æœ‰ä»¥ä¸‹æ€§è´¨ï¼š a^{\wedge}a^{\wedge} = aa^T-I\\ a^{\wedge}a^{\wedge}a^{\wedge} = -a^{\wedge}å¯¹$so(3)$æä»£æ•°çš„æŒ‡æ•°æ˜ å°„åšæ³°å‹’å±•å¼€ï¼Œå¯ä»¥å¾—åˆ°ï¼š \begin{split} &R= exp(\phi^{\wedge}) = exp(\theta a^{\wedge})=\Sigma_{n=0}^{\infty}\frac{1}{n!} (\theta a^{\wedge})^n\\ & =cos\theta I + (1-cos\theta)aa^T+sin\theta a^{\wedge} \end{split}å¯ä»¥çœ‹åˆ°$so(3)$å®é™…ä¸Šå°±æ˜¯æ—‹è½¬å‘é‡ç»„æˆçš„ç©ºé—´ï¼ŒæŒ‡æ•°æ˜ å°„å³æ˜¯ç½—å¾·é‡Œæ ¼æ–¯å…¬å¼ã€‚ æŒ‡æ•°æ˜ å°„æ˜¯ä¸€ä¸ªæ»¡å°„ï¼Œæ¯ä¸ª$SO(3)$ä¸­çš„å…ƒç´ ï¼Œéƒ½å¯ä»¥æ‰¾åˆ°è‡³å°‘ä¸€ä¸ª$so(3)$å…ƒç´ ä¸ä¹‹å¯¹åº”ï¼ˆ$\theta + 2k\pi$ï¼‰ã€‚ $se(3)$ä¸Šçš„æŒ‡æ•°æ˜ å°„ä¸ºï¼š T = exp(\xi^{\wedge}) = \begin{bmatrix} R & J\rho\\ 0 &1 \end{bmatrix}\\ J = \frac{sin\theta}{\theta}I + (1-\frac{sin\theta}{\theta})aa^T + \frac{1-cos\theta}{\theta}a^{\wedge} 6.3 æä»£æ•°æ±‚å¯¼ ä¸¤ä¸ªæä»£æ•°æŒ‡æ•°æ˜ å°„ä¹˜ç§¯çš„å®Œæ•´å½¢å¼ç”±BCHå…¬å¼ç»™å‡ºï¼š ln(exp(A)exp(B)) = A+B + \frac{1}{2}[A, B] + \frac{1}{12}[A,[A,B]] + ... å¯¹$ln(exp(\phi_1^{\wedge})exp(\phi_2^{\wedge}))^{\vee}$ï¼Œå½“$\phi_1$æˆ–$\phi_2$ä¸ºå°é‡æ—¶ï¼ŒBCHå…¬å¼ç»™å‡ºäº†çº¿æ€§è¿‘ä¼¼è¡¨è¾¾ï¼š ln(exp(\phi_1^{\wedge})exp(\phi_2^{\wedge}))^{\vee} = \left\{ \begin{split} J_l(\phi_2)^{-1}\phi_1 + \phi_2\ \ \ \ \ \ \phi_1ä¸ºå°é‡\\ J_r(\phi_1)^{-1}\phi_2 + \phi_1\ \ \ \ \ \ \phi_2ä¸ºå°é‡\\ \end{split} \right.BCHè¿‘ä¼¼é›…å¯æ¯”$J_l$å°±æ˜¯ä¸Šä¸€èŠ‚çš„$J$ï¼š J_l = J = \frac{sin\theta}{\theta}I + (1-\frac{sin\theta}{\theta})aa^T + \frac{1-cos\theta}{\theta}a^{\wedge}\\ J_l^{-1} = \frac{\theta}{2}cot\frac{\theta}{2}I + (1-\frac{\theta}{2}cot\frac{\theta}{2})aa^T - \frac{\theta}{2}a^{\wedge}\\ J_r(\phi) = J_l(-\phi)ç”±ä»¥ä¸Šå…¬å¼è¯´æ˜äº†æç¾¤ä¹˜æ³•å’Œæä»£æ•°åŠ æ³•çš„è¿‘ä¼¼è½¬æ¢å…³ç³»ã€‚ åœ¨$SO(3)ã€SE(3)$ä¸Šæ²¡æœ‰è‰¯å¥½å®šä¹‰çš„åŠ æ³•ï¼Œè€Œæä»£æ•°ç”±å‘é‡ç»„æˆï¼Œæœ‰è‰¯å¥½çš„åŠ æ³•è¿ç®—ã€‚å› æ­¤åœ¨è®¡ç®—ä½å§¿çš„å¯¼æ•°æ—¶ï¼Œé€šå¸¸ä½¿ç”¨æä»£æ•°è§£å†³ï¼Œæä»£æ•°æ±‚å¯¼çš„ä¸¤ç§æ€è·¯ï¼š æä»£æ•°æ±‚å¯¼$\delta \phi$ï¼šç”¨æä»£æ•°è¡¨ç¤ºå§¿æ€ï¼Œç„¶åè½¬åŒ–æˆå¯¹æä»£æ•°æ±‚å¯¼$\phi + \delta \phi$ \begin{split} &\frac{\partial (Rp)}{\partial R} = \frac{\partial(exp(\phi^{\wedge})p)}{\partial \phi}\\ &= lim \frac{exp((\phi+\delta\phi)^{\wedge})p-exp(\phi^{\wedge})p}{\partial \phi}\\ &=-(Rp)^{\wedge}J_l \end{split} æ‰°åŠ¨æ¨¡å‹$\Delta R$ï¼šå¯¹$R$è¿›è¡Œæ‰°åŠ¨ï¼Œç„¶åå¯¹æ‰°åŠ¨æ±‚å¯¼$\Delta R R$ \begin{split} &\frac{\partial (Rp)}{\partial R} = lim \frac{exp(\varphi^{\wedge})exp(\phi^{\wedge})p-exp(\phi^{\wedge})p}{\partial \varphi}= -(Rp)^{\wedge}\\ & \frac{\partial Tp}{\partial \delta \xi} = \begin{bmatrix} I & -(Rp+t)^{\wedge}\\ 0 & 0 \end{bmatrix} = (Tp)^{\odot} \end{split}]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++ for record]]></title>
    <url>%2F2018%2F05%2F10%2Fc-for-record%2F</url>
    <content type="text"><![CDATA[æœ€è¿‘å¼€å§‹ç€æ‰‹å†™slamä»£ç ï¼Œçœ‹ä¸€äº›å¸¸ç”¨åº“æºç çš„æ—¶å€™å‘ç°å„ç§åŠ›ä¸ä»å¿ƒï¼Œä¸€äº›c++11çš„éªšæ“ä½œç«Ÿç„¶æ²¡è§è¿‡ï¼Œæ˜¯æ—¶å€™å®Œæ•´æ’¸ä¸€å‘c++ primerç¥­å¤©äº†ã€‚ iostream æ ‡å‡†è¾“å…¥ï¼šcin æ ‡å‡†è¾“å‡ºï¼šcoutã€cerrã€clog 123456#include &lt;iostream&gt;using namespace std;int v1=0, v2=0;cin &gt;&gt; v1 &gt;&gt; v2;cout &lt;&lt; v1+v2 &lt;&lt; endl;cerr &lt;&lt; "This is nonsense." &lt;&lt; endl; &lt;&lt; å’Œ &gt;&gt; çš„æ–¹å‘è¡¨ç¤ºäº†æ•°æ®æµçš„èµ°å‘ï¼Œä¹Ÿå°±æ˜¯èµ‹å€¼çš„æ–¹å‘ã€‚cerrç”¨æ¥è¾“å‡ºé”™è¯¯ä¿¡æ¯ã€‚ æ§åˆ¶æµ whileï¼šæ¯æ¬¡æ‰§è¡Œå¾ªç¯ä¹‹å‰å…ˆæ£€æŸ¥å¾ªç¯æ¡ä»¶ do whileï¼šå…ˆæ‰§è¡Œå¾ªç¯ä½“åæ£€æŸ¥æ¡ä»¶ 123456while (condition) statement do statementwhile (condition); forï¼šæ¯æ¬¡æ‰§è¡Œå¾ªç¯ä¹‹å‰å…ˆæ£€æŸ¥å¾ªç¯æ¡ä»¶ï¼Œæ‰§è¡Œå¾ªç¯ä¹‹åæ‰§è¡Œè¡¨è¾¾å¼ 1234567for (init-statement; condition; expression)&#123; statemnt&#125;// èŒƒå›´forè¯­å¥for (declaration : expression) statement switchï¼š case labelï¼šcaseæ ‡ç­¾å¿…é¡»æ˜¯æ•´å½¢å¸¸é‡è¡¨è¾¾å¼ å¦‚æœæŸä¸ªcaseæ ‡ç­¾åŒ¹é…æˆåŠŸï¼Œä¼šå¾€åé¡ºåºæ‰§è¡Œæ‰€æœ‰caseåˆ†æ”¯ï¼Œç›´åˆ°ç»“å°¾æˆ–è€…é‡åˆ°break defaultæ ‡ç­¾ 123456switch(ch)&#123; case 'a': case 'b': case 'c': ++cnt; break;&#125; breakï¼šè´Ÿè´£ç»ˆæ­¢ç¦»ä»–æœ€è¿‘çš„whileã€do whileã€foræˆ–switchè¯­å¥ã€‚ continueï¼šè´Ÿè´£ç»ˆæ­¢ç¦»ä»–æœ€è¿‘çš„whileã€do whileã€forå¾ªç¯çš„å½“å‰è¿­ä»£ã€‚ gotoï¼šæ— æ¡ä»¶è·³è½¬åˆ°åŒä¸€å‡½æ•°å†…çš„æŸä¸ªå¸¦æ ‡ç­¾è¯­å¥ã€‚ labeled statement: label: statement å¼‚å¸¸ throwï¼šå¼•å‘å¼‚å¸¸ï¼Œåé¢ç´§éšä¸€ä¸ªå¼‚å¸¸ç±»å‹ï¼Œç»ˆæ­¢å½“å‰å‡½æ•°ï¼Œå°†æ§åˆ¶æƒè½¬ç§»ç»™èƒ½å¤Ÿå¤„ç†è¯¥å¼‚å¸¸çš„ä»£ç ã€‚ 123#include &lt;stdexcept&gt;// runtime_error æ ‡å‡†åº“å¼‚å¸¸ç±»å‹throw runtime_error("Data must refer to same name"); tryï¼šå¤„ç†å¼‚å¸¸ï¼Œåé¢ç´§éšä¸€å¥—catchå­å¥ç”¨æ¥å¤„ç†å¼‚å¸¸ã€‚ 1234567try&#123; program statements&#125; catch (exception-declaration) &#123; handler-statements&#125; catch (exception-declaration) &#123; handler-statements&#125; ... tryè¯­å¥å—å†…å£°æ˜çš„å˜é‡åœ¨å—å¤–æ— æ³•è®¿é—®ï¼Œå³ä½¿æ˜¯catchè¯­å¥ã€‚ catchä¸€æ—¦å®Œæˆï¼Œç¨‹åºè·³è½¬åˆ°æœ€åä¸€ä¸ªcatchå­å¥ä¹‹åçš„è¯­å¥ã€‚ ç±» ç±»å‹ &amp; å¯¹è±¡ï¼ˆå®ä¾‹ï¼‰ï¼Œå˜é‡ &amp; è¡Œä¸ºï¼ˆæ–¹æ³•ï¼‰ã€‚ å­˜åœ¨ç±»å†…é»˜è®¤åˆå§‹åŒ– ç±»é€šå¸¸è¢«å®šä¹‰åœ¨å¤´æ–‡ä»¶ä¸­ï¼Œå¤´æ–‡ä»¶åå­—åº”ä¸ç±»çš„åå­—ä¿æŒä¸€è‡´ å¤´æ–‡ä»¶é€šå¸¸åŒ…å«åªèƒ½è¢«å®šä¹‰ä¸€æ¬¡çš„å®ä½“ï¼Œå¦‚ç±»ã€constç­‰ã€‚ å¤´æ–‡ä»¶ä¿æŠ¤ç¬¦#ifndefç³»åˆ—ï¼Œåˆ›å»ºé¢„å¤„ç†å˜é‡ï¼Œé˜²æ­¢å¤šæ¬¡åŒ…å«ã€‚ æ„é€ å‡½æ•°åˆå§‹å€¼åˆ—è¡¨ï¼šå†’å·ä»¥åŠå†’å·å’ŒèŠ±æ‹¬å·ä¹‹é—´çš„ä»£ç  åˆ—è¡¨åªè¯´æ˜ç”¨äºåˆå§‹åŒ–æˆå‘˜çš„å€¼ï¼Œè€Œä¸é™å®šåˆå§‹åŒ–çš„å…·ä½“é¡ºåºã€‚ æˆå‘˜çš„åˆå§‹åŒ–é¡ºåºä¸å®ƒä»¬åœ¨ç±»å®šä¹‰ä¸­çš„å‡ºç°é¡ºåºä¸€è‡´ã€‚ 12345678910// ä¸ºç±»æˆå‘˜åˆå§‹åŒ–Sales_data(const string &amp;s, unsigned n, double p) : bookNo(s), units_sold(n), revenue(p*n) &#123;&#125; // åŒºåˆ«äºèµ‹å€¼Sales_data(const string &amp;s, unsigned n, double p)&#123; bookNo = s; ...&#125; æ¥å£ä¸å°è£…ï¼š å®šä¹‰åœ¨privateè¯´æ˜ç¬¦ä¹‹åçš„æˆå‘˜åªèƒ½è¢«ç±»å†…æˆå‘˜å‡½æ•°è®¿é—®ï¼Œå°è£…äº†ç±»çš„å®ç°ç»†èŠ‚ã€‚ å®šä¹‰åœ¨publicè¯´æ˜ç¬¦ä¹‹åçš„æˆå‘˜å¯ä»¥åœ¨æ•´ä¸ªç¨‹åºå†…è¢«è®¿é—®ï¼Œå®šä¹‰ç±»çš„æ¥å£ã€‚ classå’Œstructçš„åŒºåˆ«ï¼šæˆå‘˜è®¿é—®æƒé™ structï¼šå®šä¹‰åœ¨ç¬¬ä¸€ä¸ªè¯´æ˜ç¬¦ä¹‹å‰çš„æˆå‘˜æ˜¯public classï¼šå®šä¹‰åœ¨ç¬¬ä¸€ä¸ªè¯´æ˜ç¬¦ä¹‹å‰çš„æˆå‘˜æ˜¯private å‹å…ƒï¼šå…è®¸å…¶ä»–ç±»æˆ–å‡½æ•°è®¿é—®å®ƒçš„éå…¬æœ‰æˆå‘˜ï¼Œåœ¨ç±»å†…æ·»åŠ ä»¥friendå…³é”®å­—å¼€å§‹çš„å‹å…ƒå£°æ˜ã€‚ å‹å…ƒçš„å£°æ˜ä»…ä»…æŒ‡å®šäº†è®¿é—®æƒé™ï¼Œè€Œéä¸€ä¸ªé€šå¸¸æ„ä¹‰ä¸Šçš„å‡½æ•°å£°æ˜ã€‚ 12345678class Sales_data &#123;// å‹å…ƒå£°æ˜friend Sales_data add(const Sales_data&amp;, const Sales_data&amp;);// éå…¬æœ‰æˆå‘˜private: string bookNo; double revenue = 0.0;&#125;; é™æ€æˆå‘˜staticï¼šä¸ç±»æœ¬èº«ç›¸å…³è”ï¼Œä¸å±äºä»»ä½•ä¸€ä¸ªå¯¹è±¡ï¼Œå› æ­¤ä¸æ˜¯åœ¨åˆ›å»ºç±»å¯¹è±¡çš„æ—¶å€™è¢«å®šä¹‰çš„ï¼Œå› æ­¤é€šå¸¸åœ¨ç±»çš„å¤–éƒ¨å®šä¹‰å’Œåˆå§‹åŒ–ï¼Œåœ¨ç±»å†…éƒ¨æ·»åŠ ä»¥staticå…³é”®å­—å¼€å§‹çš„é™æ€æˆå‘˜å£°æ˜ã€‚ å†…ç½®ç±»å‹ å†…å­˜ä¸­çš„ä¸€ä¸ªåœ°å€å¯¹åº”ä¸€ä¸ªå­—èŠ‚ unsignedç±»å‹è¡¨ç¤ºå¤§äºç­‰äº0çš„æ•°ï¼ˆ$[0, 2^{n}-1]$ï¼‰ï¼Œè¢«èµ‹ç»™ä¸€ä¸ªè¶…å‡ºè¡¨ç¤ºèŒƒå›´çš„æ•°æ—¶ï¼Œè‡ªåŠ¨å–ä½™ï¼Œä½œä¸ºå¾ªç¯æ¡ä»¶æ—¶å½“å¿ƒè¿›å…¥æ— é™å¾ªç¯ signedç±»å‹æ­£è´Ÿå€¼èŒƒå›´å¹³è¡¡ï¼ˆ$[-2^{n-1}, 2^{n-1}-1]$ï¼‰ï¼Œè¢«èµ‹ç»™ä¸€ä¸ªè¶…å‡ºè¡¨ç¤ºèŒƒå›´çš„æ•°æ—¶ï¼Œç»“æœæœªå®šä¹‰ å­—ç¬¦å‹charï¼Œå•å¼•å·ï¼Œä¸€ä¸ªå­—èŠ‚ å­—ç¬¦ä¸²å‹ï¼ŒåŒå¼•å·ï¼Œå¸¸é‡å­—ç¬¦æ•°ç»„ï¼Œç»“å°¾éšå«ç©ºå­—ç¬¦ â€˜\0â€™ nullptr = 0ï¼ˆä¼ ç»ŸNULLåŒ…å«åœ¨cstdlibå¤´æ–‡ä»¶å†…ï¼‰ å˜é‡ åˆ—è¡¨åˆå§‹åŒ–ï¼ŒèŠ±æ‹¬å· 123456// æ‹·è´åˆå§‹åŒ–int x=0;int x=&#123;0&#125;;// ç›´æ¥åˆå§‹åŒ–int x&#123;0&#125;;int x(0); å˜é‡å£°æ˜externï¼Œæºäºåˆ†ç¦»å¼ç¼–è¯‘æœºåˆ¶ï¼Œä¸€ä¸ªå˜é‡åªèƒ½è¢«å®šä¹‰ä¸€æ¬¡ï¼Œå¯ä»¥å£°æ˜å¤šæ¬¡ ä½œç”¨åŸŸï¼ŒåµŒå¥—ä½œç”¨åŸŸ &amp; å†…éƒ¨é‡å®šä¹‰ å¤åˆç±»å‹ å¼•ç”¨ï¼Œtypename &amp;declarationï¼Œæµ…æ‹·è´ï¼Œç»‘å®šä¸€ä¸ªå¯¹è±¡ï¼Œå¼•ç”¨ä¸æ˜¯å¯¹è±¡ æŒ‡é’ˆï¼Œtypename *declarationï¼Œå­˜æ”¾å¯¹è±¡åœ°å€ 1234int a;int *p, *q=a;p = &amp;a;p = q; å–åœ°å€ç¬¦&amp; 123int *p = a;int *p = &amp;a;// a---&gt;å¯¹è±¡ &amp;a---&gt;åœ°å€ è§£å¼•ç”¨ç¬¦* 123456int a;int *p;*p ---&gt; undefinedp = &amp;a;*p = 10;// p---&gt;æŒ‡é’ˆ *p---&gt;å¯¹è±¡ void* æŒ‡é’ˆï¼Œå¯ä»¥æŒ‡å‘ä»»æ„ç±»å‹çš„å¯¹è±¡ï¼Œä½†æ˜¯ä¸èƒ½è¿›è¡Œå¯¹è±¡æ“ä½œ consté™å®šç¬¦ å‚ä¸ç¼–è¯‘é¢„å¤„ç† è¦å®ç°å¤šä¸ªæ–‡ä»¶å…±äº«ï¼Œå¿…é¡»åœ¨constå˜é‡å®šä¹‰ä¹‹å‰åŠ ä¸Šexternå…³é”®å­— 1234// defineextern const int bufferSize = fcn();// declareextern const int bufferSize; å…è®¸ä»»æ„è¡¨è¾¾å¼ä½œä¸ºåˆå§‹å€¼ï¼ˆå…è®¸éšå¼ç±»å‹è½¬æ¢ï¼‰ å¸¸é‡å¼•ç”¨ï¼Œå…è®¸éå¸¸é‡èµ‹å€¼ï¼Œå®é™…å¼•ç”¨ä¸€ä¸ªå†…å­˜ä¸­çš„â€œä¸´æ—¶å€¼â€ æŒ‡å‘å¸¸é‡çš„æŒ‡é’ˆï¼Œå…è®¸éå¸¸é‡èµ‹å€¼ï¼Œä½†æ˜¯ä¸èƒ½é€šè¿‡è¯¥æŒ‡é’ˆä¿®æ”¹å¯¹è±¡ å¸¸é‡æŒ‡é’ˆï¼ŒæŒ‡é’ˆå§‹ç»ˆæŒ‡å‘åŒä¸€ä¸ªå¯¹è±¡ å¸¸é‡è¡¨è¾¾å¼constexprï¼Œè¡¨è¾¾å¼åœ¨ç¼–è¯‘è¿‡ç¨‹ä¸­å°±èƒ½å¾—åˆ°è®¡ç®—ç»“æœ å¤„ç†ç±»å‹ ç±»å‹åˆ«åtypedef &amp; using 1234567// ä¼ ç»Ÿtypedef double base;typedef base *p; // pæ˜¯doubleæŒ‡é’ˆbase a;p p1=&amp;a;// c++11using base = double; autoç±»å‹è¯´æ˜ç¬¦ï¼Œè®©ç¼–è¯‘å™¨åˆ†æè¡¨è¾¾å¼æ‰€å±ç±»å‹å¹¶ä¸ºå˜é‡èµ‹å€¼ 12// ä¸€æ¡ç±»å‹å£°æ˜è¯­å¥ä¸­æ‰€æœ‰å˜é‡çš„ç±»å‹å¿…é¡»ä¿æŒä¸€è‡´auto i=0, *p=&amp;i; decltypeç±»å‹æŒ‡ç¤ºç¬¦ï¼Œä»…åˆ†æè¡¨è¾¾å¼è¿”å›ç±»å‹ï¼Œä¸åšèµ‹å€¼ï¼ˆå› æ­¤ä¸åšå®é™…è®¡ç®—ï¼‰ 1decltype(f()) a=x; string è¯»å–ï¼Œ&gt;&gt;ä¸è¯»å–ç©ºç™½ï¼Œé‡åˆ°ç©ºç™½ç¬¦åœæ­¢ï¼Œgetlineä¿ç•™ç©ºç™½ç¬¦ï¼Œé‡åˆ°æ¢è¡Œç¬¦åœæ­¢ã€‚ å­—ç¬¦ä¸²å­—é¢å€¼ä¸æ˜¯stringå¯¹è±¡ï¼Œè€Œæ˜¯Cé£æ ¼å­—ç¬¦ä¸²ï¼Œc_str()æˆå‘˜å‡½æ•°èƒ½å¤Ÿå°†stringå¯¹è±¡è½¬åŒ–æˆCé£æ ¼å­—ç¬¦ä¸² éå†ï¼ŒèŒƒå›´forè¯­å¥ï¼Œæ¯æ¬¡è¿­ä»£declareçš„å˜é‡ä¼šè¢«åˆå§‹åŒ–ä¸ºexpressionçš„ä¸‹ä¸€ä¸ªå…ƒç´  123456789for (declaration : expression) statementstring str("some string");// èµ‹å€¼for (auto c: str) cout &lt;&lt; c &lt;&lt; endl;// å¼•ç”¨for (auto &amp;c: str) c = toupper(c); size()è¿”å›çš„ç±»å‹æ˜¯string::size_typeï¼Œé€šå¸¸ç”¨auto vector ç±»æ¨¡ç‰ˆï¼Œç›¸åŒç±»å‹å¯¹è±¡çš„é›†åˆï¼Œå£°æ˜æ—¶å¿…é¡»æä¾›å…ƒç´ ç±»å‹vector&lt;int&gt; æ·»åŠ å…ƒç´ push_back() è¿­ä»£å™¨ æ‰€æœ‰æ ‡å‡†åº“å®¹å™¨éƒ½æ”¯æŒè¿­ä»£å™¨ï¼Œåªæœ‰å°‘æ•°æ”¯æŒä¸‹æ ‡è®¿é—® begin()è¿”å›æŒ‡å‘ç¬¬ä¸€ä¸ªå…ƒç´ çš„è¿­ä»£å™¨ï¼Œend()è¿”å›å°¾åå…ƒç´ çš„è¿­ä»£å™¨ cbegin()å’Œcend()æ“ä½œç±»ä¼¼ï¼Œè¿”å›å€¼æ˜¯const_iteratorï¼Œä¸èƒ½ä¿®æ”¹å¯¹è±¡ è¿­ä»£å™¨çš„ç±»å‹æ˜¯container::iteratorå’Œcontainer::const_iteratorï¼Œé€šå¸¸ç”¨auto è§£å¼•ç”¨è¿­ä»£å™¨å¾—åˆ°å¯¹è±¡ ç®­å¤´è¿ç®—ç¬¦-&gt;ï¼Œç»“åˆè§£å¼•ç”¨+æˆå‘˜è®¿é—®ä¸¤ä¸ªæ“ä½œ è¿­ä»£å™¨å¤±æ•ˆï¼šå®¹å™¨æ”¹å˜å®¹é‡ æ•°ç»„ å¤§å°å›ºå®šï¼Œç¼–è¯‘çš„æ—¶å€™ç»´åº¦åº”è¯¥å·²çŸ¥ï¼Œå› æ­¤å¿…é¡»æ˜¯å¸¸é‡è¡¨è¾¾å¼ ä¸èƒ½ç”¨åšæ‹·è´å’Œèµ‹å€¼ è¡¨è¾¾å¼ å·¦å€¼å’Œå³å€¼ â€‹ Cè¯­è¨€ä¸­ï¼Œå·¦å€¼æŒ‡çš„æ˜¯æ—¢èƒ½å‡ºç°åœ¨ç­‰å·å·¦è¾¹ä¹Ÿèƒ½å‡ºç°åœ¨ç­‰å·å³è¾¹çš„å˜é‡æˆ–è¡¨è¾¾å¼ï¼Œé€šå¸¸æ¥è¯´å°±æ˜¯æœ‰åå­—çš„å˜é‡ï¼Œè€Œå³å€¼åªèƒ½å‡ºç°åœ¨ç­‰å·å³ä¾§ï¼Œé€šå¸¸å°±æ˜¯ä¸€äº›æ²¡æœ‰åå­—ä¹Ÿå–ä¸åˆ°åœ°å€çš„ä¸­é—´ç»“æœã€‚ ç»§æ‰¿åˆ°C++ä¸­å½’çº³æ¥è®²å°±æ˜¯ï¼šå½“ä¸€ä¸ªå¯¹è±¡è¢«ç”¨ä½œå³å€¼çš„æ—¶å€™ï¼Œç”¨çš„æ˜¯å¯¹è±¡çš„å€¼ï¼ˆå†…å®¹ï¼‰ï¼Œå½“è¢«ç”¨ä½œå·¦å€¼çš„æ—¶å€™ï¼Œç”¨çš„æ˜¯å¯¹è±¡çš„èº«ä»½ï¼ˆåœ¨å†…å­˜ä¸­çš„ä½ç½®ï¼‰ã€‚ æ±‚å€¼é¡ºåº æœ‰å››ç§è¿ç®—ç¬¦æ˜ç¡®è§„å®šäº†æ±‚å€¼é¡ºåºï¼Œé€»è¾‘ä¸ï¼ˆ&amp;&amp;ï¼‰ã€é€»è¾‘æˆ–ï¼ˆ||ï¼‰ã€æ¡ä»¶ï¼ˆ?:ï¼‰ã€é€—å·ï¼ˆ,ï¼‰è¿ç®—ç¬¦ã€‚ 12int i = 0;cout &lt;&lt; i &lt;&lt; ++i &lt;&lt; endl; å‰ç½®ç‰ˆæœ¬å’Œåç½®ç‰ˆæœ¬çš„é€’å¢é€’å‡ ç”¨äºå¤åˆè¿ç®—ä¸­æ—¶ï¼Œ å‰ç½®ç‰ˆæœ¬é¦–å…ˆä¿®æ”¹å¯¹è±¡ï¼Œç„¶åå°†å¯¹è±¡æœ¬èº«ä½œä¸ºå·¦å€¼è¿”å›ã€‚ åç½®ç‰ˆæœ¬å°†å¯¹è±¡åŸå§‹å€¼çš„å‰¯æœ¬ä½œä¸ºå³å€¼è¿”å›ã€‚ ä½è¿ç®— æ•´å½¢æå‡ï¼Œchar8-&gt;int32 æ·»0ï¼Œè¶Šç•Œä¸¢å¼ƒ é€—å·è¿ç®—ç¬¦ï¼šå«æœ‰ä¸¤ä¸ªè¿ç®—å¯¹è±¡ï¼Œé¦–å…ˆå¯¹å·¦è¡¨è¾¾å¼æ±‚å€¼ï¼Œç„¶åå°†æ±‚å€¼ç»“æœä¸¢å¼ƒæ‰ï¼Œæœ€å³è¾¹çš„è¡¨è¾¾å¼çš„å€¼å°†ä½œä¸ºæ•´ä¸ªé€—å·è¡¨è¾¾å¼çš„å€¼ã€‚æœ¬è´¨ä¸Šï¼Œé€—å·çš„ä½œç”¨æ˜¯å¯¼è‡´ä¸€äº›åˆ—è¿ç®—è¢«é¡ºåºæ‰§è¡Œã€‚ 12// åˆ†åˆ«å¯¹é€—å·è¡¨è¾¾å¼å†…å¯¹è±¡èµ‹å€¼ï¼Œç„¶åè¿”å›æœ€å³cntçš„å€¼var = (count=19, incr=10, cnt++) å‡½æ•° å±€éƒ¨é™æ€å¯¹è±¡staticï¼šé¦–æ¬¡è°ƒç”¨æ—¶è¢«åˆå§‹åŒ–ï¼Œç›´åˆ°ç¨‹åºç»ˆæ­¢æ‰è¢«é”€æ¯ã€‚ 123456int f()&#123; // åªåˆå§‹åŒ–ä¸€æ¬¡ï¼Œå‡½æ•°è°ƒç”¨ç»“æŸä»¥åè¿™ä¸ªå€¼ä»æœ‰æ•ˆ static cnt = 0; return ++cnt;&#125; å‚æ•°ä¼ é€’ï¼šå¦‚æœå½¢å‚è¢«å£°æ˜ä¸ºå¼•ç”¨ç±»å‹ï¼Œå®ƒå°†ç»‘å®šåˆ°å¯¹åº”çš„å®å‚ä¸Šï¼ˆä¼ å¼•ç”¨è°ƒç”¨ï¼‰ï¼Œå¦åˆ™å°†å®å‚çš„å€¼æ‹·è´åèµ‹ç»™å½¢å‚ï¼ˆä¼ å€¼è°ƒç”¨ï¼‰ã€‚ å«æœ‰å¯å˜å½¢å‚çš„å‡½æ•° æ‰€æœ‰å®å‚ç±»å‹ç›¸åŒï¼Œå¯ä»¥ä½¿ç”¨initializer_listæ¨¡ç‰ˆç±»å‹çš„å½¢å‚ï¼Œåˆ—è¡¨ä¸­å…ƒç´ æ˜¯constã€‚ 12initializer_list&lt;T&gt; lst;initializer_list&lt;T&gt; lst&#123;a, b, c, ...&#125;; ç¼–å†™å¯å˜å‚æ•°æ¨¡ç‰ˆ çœç•¥ç¬¦å½¢å‚ï¼šå¯¹åº”çš„å®å‚æ— éœ€ç±»å‹æ£€æŸ¥ 1234// å¸¦éƒ¨åˆ†å½¢å‚ç±»å‹void foo(parm_list, ...);void foo(...); å†…è”å‡½æ•°inclineï¼šé¿å…å‡½æ•°è°ƒç”¨å¼€é”€ è°ƒè¯•å¸®åŠ© NDEBUGé¢„å¤„ç†å˜é‡ï¼šç”¨äºå…³é—­è°ƒè¯•çŠ¶æ€ï¼Œassertå°†è·³è¿‡ä¸æ‰§è¡Œã€‚ assert (expr) é¢„å¤„ç†å®ï¼šå¦‚æœè¡¨è¾¾å¼ä¸ºå‡ï¼Œassertè¾“å‡ºä¿¡æ¯å¹¶ç»ˆæ­¢ç¨‹åºã€‚ é¢„å¤„ç†åå­—ç”±é¢„å¤„ç†è€Œéç¼–è¯‘å™¨ç®¡ç†ï¼Œå› æ­¤å¯ä»¥ç›´æ¥ä½¿ç”¨åå­—è€Œæ— é¡»æä¾›usingå£°æ˜ã€‚ static_castå’Œdynamic_castå¼ºåˆ¶ç±»å‹è½¬æ¢ static_cast \ (expression)ï¼šæš´åŠ›ç±»å‹è½¬æ¢ï¼Œä¸è¿è¡Œç±»å‹æ£€æŸ¥ã€‚ dynamic_cast\ (expression)ï¼šè¿è¡Œç±»å‹æ£€æŸ¥ï¼Œä¸‹è¡Œè½¬æ¢å®‰å…¨ã€‚ new &amp; deleteï¼šnew [] è¦å’Œ delete []å¯¹åº”ä¸Šã€‚ c++çš„oopç‰¹æ€§ï¼ˆprivate publicâ€¦ï¼‰åªåœ¨ç¼–è¯‘æ—¶åˆ»æœ‰æ„ä¹‰ã€‚åŒä¸€ç±»çš„å¯¹è±¡å¯ä»¥äº’ç›¸è®¿é—®ç§æœ‰æˆå‘˜ã€‚ firendï¼šæ³¨æ„æ–¹å‘æ˜¯give acess toï¼Œæˆæƒfriendè®¿é—®è‡ªå·±çš„privateã€‚ç¼–è¯‘æ—¶åˆ»æ£€æŸ¥ã€‚ compositionï¼šç»„åˆï¼Œç”¨ä¸€ç³»åˆ—å¯¹è±¡æ„é€ å¯¹è±¡ã€‚ inheritanceï¼šç»§æ‰¿ï¼Œç”¨ä¸€äº›ç±»æ¥æ„é€ æ–°çš„ç±»ã€‚ 1234class A;class B : public A&#123; ....&#125;; æ„é€ ï¼šå­ç±»æ„é€ çš„æ—¶å€™è¦å…ˆæ„é€ çˆ¶ç±»ï¼Œææ„çš„æ—¶å€™åè¿‡æ¥ï¼Œå…ˆææ„å­ç±»ã€‚ é‡åï¼šname hidingï¼Œspecial for c++ã€‚ â€‹ protectedï¼šdesigned for sub classã€‚å­ç±»å¯ä»¥ç›´æ¥è®¿é—®ã€‚å…¶ä»–ç±»çœ‹ä¸åˆ°ã€‚ overloadï¼šå‚æ•°è¡¨å¿…é¡»ä¸åŒï¼Œå¦åˆ™ç¼–è¯‘å™¨æ— æ³•è¯†åˆ«ã€‚ default argumentï¼šdefaults must be added from right to leftã€‚must be declared in .h filesã€‚å‘ç”Ÿåœ¨ç¼–è¯‘æ—¶åˆ»ã€‚ inlineï¼šä¸ç”¨çœŸæ­£è°ƒç”¨å‡½æ•°ï¼Œè€Œæ˜¯ç›´æ¥æ’å…¥æ±‡ç¼–ä»£ç æ®µã€‚tradeoff between space and time consumingã€‚åŒºåˆ«äºå®ï¼Œå®æ˜¯æ²¡æœ‰ç±»å‹æ£€æŸ¥çš„ã€‚ const declare a variableï¼šæ˜¯å˜é‡ï¼Œè€Œä¸æ˜¯å¸¸æ•°]]></content>
      <tags>
        <tag>basic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cmake for record]]></title>
    <url>%2F2018%2F05%2F08%2Fcmake%2F</url>
    <content type="text"><![CDATA[0. å˜é‡å˜é‡ä½¿ç”¨ ${ } çš„æ–¹å¼å–å€¼ï¼Œä½†æ˜¯åœ¨ifæ§åˆ¶è¯­å¥ä¸­ç›´æ¥ä½¿ç”¨å˜é‡åã€‚ 1. projectproject ( project_name [CXX] [C] [Java] ) ç”¨æ¥æŒ‡å®šå·¥ç¨‹åç§°å’Œå·¥ç¨‹è¯­è¨€ï¼ˆå¯çœç•¥ï¼‰ï¼ŒæŒ‡ä»¤éšå¼å®šä¹‰äº†projectname_BINARY_DIRå’Œprojectname_SOURCE_DIRä¸¤ä¸ªå˜é‡ï¼ˆå†™åœ¨cmake_cacheé‡Œé¢ï¼‰ï¼ŒæŒ‡çš„æ˜¯ç¼–è¯‘å‘ç”Ÿçš„å½“å‰ç›®å½•ã€‚ 2. setset ( VAR [VALUE] ) ç”¨æ¥æ˜¾å¼å®šä¹‰å˜é‡ï¼Œå¦‚set (SRC_LIST main.c t1.c t2.c) ã€‚ï¼ˆç«Ÿç„¶ä¸ç”¨å¥—æ‹¬å·ï¼Ÿï¼‰ 3. messagemessage ( [SEND_ERROR | STATUS | FATAL_ERROR] â€œmessage to displayâ€ VAR ) ç”¨æ¥å‘ç»ˆç«¯è¾“å‡ºç”¨æˆ·å®šä¹‰çš„ä¿¡æ¯ã€‚ 4. add_executableadd_executable ( executable_filename [source_filename] ) ç”Ÿæˆåå­—ä¸ºexecutable_filenameçš„å¯æ‰§è¡Œæ–‡ä»¶ï¼Œç›¸å…³çš„æºæ–‡ä»¶ [source_filename] å¯ä»¥æ˜¯ä¸€ä¸ªæºæ–‡ä»¶åˆ—è¡¨ã€‚ 5. æ¸…ç†æ„å»ºç»“æœmake clean å¯¹æ„å»ºå‡ºçš„å¯æ‰§è¡Œæ–‡ä»¶è¿›è¡Œæ¸…ç†ã€‚ 6. å¤–éƒ¨æ„å»º1234mkdir buildcd buildcmake ..make æ‰€æœ‰ç¼–è¯‘åŠ¨ä½œå‘ç”Ÿåœ¨ç¼–è¯‘ç›®å½•ï¼Œå¯¹åŸæœ‰å·¥ç¨‹æ²¡æœ‰ä»»ä½•å½±å“ã€‚ 7. add_subdirectoryadd_subdirectory ( source_dir [binary_dir] [EXCLUDE_FROM_ALL] ) å‘å½“å‰å·¥ç¨‹ç›®å½•æ·»åŠ å­˜æ”¾æºæ–‡ä»¶çš„å­ç›®å½•source_dirï¼Œå¹¶æŒ‡å®šå­˜æ”¾ä¸­é—´äºŒè¿›åˆ¶æ–‡ä»¶å’Œç›®æ ‡äºŒè¿›åˆ¶æ–‡ä»¶çš„ä½ç½®binary_dirã€‚æŒ‡ä»¤éšå¼ä¿®æ”¹ EXECUTABLE_OUTPUT_PATH å’Œ LIBRARY_OUTPUT_PATH ä¸¤ä¸ªå˜é‡ã€‚ 8. æ›´åŠ åƒä¸€ä¸ªå·¥ç¨‹ åˆ›å»ºå·¥ç¨‹æ ¹ç›®å½•ï¼Œåˆ›å»ºCMakeLists.txtã€‚ 1234567# æŒ‡å®šæœ€ä½ç¼–è¯‘ç‰ˆæœ¬cmake_minimum_required(VERSION 3.7)# æŒ‡å®šå·¥ç¨‹åå­—PROJECT(HELLO)# æµ‹è¯•ç±»æ‰“å°ä¿¡æ¯MESSAGE(STATUS "This is BINARY dir " $&#123;HELLO_BINARY_DIR&#125;)MESSAGE(STATUS "This is SOURCE dir " $&#123;HELLO_SOURCE_DIR&#125;) æ·»åŠ å­ç›®å½•srcï¼Œç”¨æ¥å­˜æ”¾æºæ–‡ä»¶ï¼Œä¸ºå­ç›®å½•åˆ›å»ºCMakeLists.txtã€‚ 123# åœ¨æ ¹ç›®å½•CMakeLists.txtä¸­æ·»åŠ å­ç›®å½•å£°æ˜add_subdirectory(src bin)# ç¼–è¯‘äº§ç”Ÿçš„ä¸­é—´æ–‡ä»¶ä»¥åŠç›®æ ‡æ–‡ä»¶å°†ä¿å­˜åœ¨ç¼–è¯‘æ–‡ä»¶å¤¹çš„binå­ç›®å½•ä¸‹ 12345# ç¼–å†™å½“å‰å­ç›®å½•çš„CMakeLists.txtadd_executable(hello main.c)# ä¿®æ”¹æœ€ç»ˆç”Ÿæˆçš„å¯æ‰§è¡Œæ–‡ä»¶ä»¥åŠåº“çš„è·¯å¾„ï¼Œè¿™ä¸¤ä¸ªæŒ‡ä»¤è¦è¿½éšå¯¹åº”çš„add_executable()å’Œadd_library()æŒ‡ä»¤set(EXECUTABLE_OUTPUT_PATH $&#123;PROJECT_BINARY_PATH&#125;/bin)set(LIBRARY_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;/lib) æ·»åŠ å­ç›®å½•buildï¼Œä½œä¸ºå¤–éƒ¨ç¼–è¯‘æ–‡ä»¶å¤¹ï¼ˆ ${PROJECT_BINARY_DIR} ï¼‰ï¼Œå­˜æ”¾ç¼–è¯‘çš„è¿‡ç¨‹å’Œç›®æ ‡æ–‡ä»¶ã€‚ 123cd buildcmake ..make æ·»åŠ å­ç›®å½•docï¼Œç”¨æ¥å­˜æ”¾å·¥ç¨‹æ–‡æ¡£hello.txtã€‚ æ·»åŠ æ–‡æœ¬æ–‡ä»¶READMEï¼ŒCOPYRIGHTã€‚ æ·»åŠ runhello.shè„šæœ¬ï¼Œç”¨æ¥è°ƒç”¨å¯æ‰§è¡Œæ–‡ä»¶helloã€‚ 9. æ‰“åŒ…å®‰è£… åœ¨æ ¹ç›®å½•çš„CMakeList.txtä¸­æ·»åŠ å®‰è£…ä¿¡æ¯ 123456# å®‰è£…COPYRIGHT/READMEåˆ°&lt;prefix&gt;/share/doc/cmake/t2INSTALL(FILES COPYRIGHT README DESTINATION share/doc/cmake/t2)# å®‰è£…runhello.shåˆ°&lt;prefix&gt;/binINSTALL(PROGRAMS runhello.sh DESTINATION bin)# å®‰è£…å·¥ç¨‹æ–‡æ¡£åˆ°&lt;prefix&gt;/share/doc/cmake/t2INSTALL(DIRECTORY doc/ DESTINATION share/doc/cmake/t2) åœ¨å­ç›®å½•çš„CMakeList.txtä¸­æ·»åŠ å®‰è£…ä¿¡æ¯ 1234# å®‰è£…è„šæœ¬è¦è°ƒç”¨çš„å¯æ‰§è¡Œæ–‡ä»¶helloåˆ°&lt;prefix&gt;/binï¼Œ# æ³¨æ„install(targets)æŒ‡ä»¤ä¹Ÿè¦è¿½éšå¯¹åº”add_executable()å’Œadd_library()æŒ‡ä»¤çš„è·¯å¾„INSTALL(TARGETS hello RUNTIME DESTINATION bin) å®‰è£…ç¨‹åºåŒ… 12345678910cd build# åœ¨cmakeå‘½ä»¤ä¸­æŒ‡æ˜å®‰è£…ç›®å½•çš„å‰ç¼€&lt;prefix&gt;# CMAKE_INSTALL_PREFIX é»˜è®¤æ˜¯/usr/localcmake -DCMAKE_INSTALL_PREFIX=/Users/carrol/tmp ..makemake install# æŸ¥çœ‹ç›®æ ‡æ–‡ä»¶å¤¹j tmptree -a 10. add_libraryadd_library ( name [SHARED | STATIC | MODULE] [source_filename] ) ç”Ÿæˆåå­—ä¸ºlibname.Xçš„åº“æ–‡ä»¶ã€‚ SHAREDï¼ŒåŠ¨æ€åº“ï¼Œlibname.dylib STATICï¼Œé™æ€åº“ï¼Œlibname.a è®¾ç½®ç›®æ ‡åŠ¨æ€åº“å’Œé™æ€åº“åŒå set_target_properties 12345# è®¾ç½®ç›®æ ‡åŠ¨é™æ€åº“åŒåadd_library(hello SHARED hello.c)add_library(hello_static hello.c)set_target_properties(hello_static PROPERTIES OUTPUT_NAME hello) é˜²æ­¢æ„å»ºä¸­æ¸…ç†åŒåæ–‡ä»¶ set_target_properties cmakeåœ¨æ„å»ºä¸€ä¸ªtargetæ—¶ï¼Œä¼šå°è¯•æ¸…ç†æ‰å…¶ä»–ä½¿ç”¨è¿™ä¸ªåå­—çš„åº“â€”â€”åœ¨æ„å»ºlibhello.aæ—¶ä¼šæ¸…ç†æ‰libhello.dylibã€‚ æˆ‘å®é™…æ“ä½œæ—¶å€™ä¼šä¿ç•™ä¸¤ä¸ªåº“æ–‡ä»¶ï¼Œä½†æ˜¯åœ¨ä½œä¸ºç¬¬ä¸‰æ–¹è¢«å¼•ç”¨çš„æ—¶å€™ä¼šæŠ¥é”™ï¼š dyld: Library not loaded: libhello.dylib Reason: image not found 1234SET_TARGET_PROPERTIES(hello PROPERTIES CLEAN_DIRECT_OUTPUT 1)SET_TARGET_PROPERTIES(hello_static PROPERTIES CLEAN_DIRECT_OUTPUT 1) è®¾ç½®åŠ¨æ€ç‰ˆæœ¬å· set_target_properties 1234# è®¾ç½®åŠ¨æ€åº“ç‰ˆæœ¬å·set_target_properties(hello PROPERTIES VERSION 1.2 SOVERSION 1) ç¼–è¯‘æ–‡ä»¶å¤¹ä¸‹ç”Ÿæˆäº†libhello.1.2.dylibã€libhello.1.dylibã€libhello.dylibä¸‰ä¸ªåŠ¨æ€åº“æ–‡ä»¶ï¼Œåªæœ‰ä¸€ä¸ªæ˜¯çœŸçš„ï¼Œå¦å¤–ä¸¤ä¸ªæ˜¯æ›¿èº«ã€‚ å®‰è£…å…±äº«åº“å’Œå¤´æ–‡ä»¶ ä¿®æ”¹åº“çš„æºæ–‡ä»¶å¤¹ä¸‹çš„CMakeLIsts.txt 123456# åº“æ–‡ä»¶install(TARGETS hello hello_static ARCHIVE DESTINATION lib //é™æ€åº“ LIBRARY DESTINATION lib) //åŠ¨æ€åº“# å¤´æ–‡ä»¶install(FILES hello.h DESTINATION include/hello) 11. include_directoriesinclude_directories( dir1 dir2 â€¦ ) ç”¨æ¥å‘å·¥ç¨‹æ·»åŠ å¤šä¸ªç‰¹å®šçš„å¤´æ–‡ä»¶æœç´¢è·¯å¾„ 12. link_directories &amp; target_link_librarieslink_directories( dir1 dir2 â€¦ ) æ·»åŠ éæ ‡å‡†çš„å…±äº«åº“æœç´¢è·¯å¾„ target_link_libraries( target lib1 lib2 â€¦ ) ç”¨æ¥ä¸ºç›®æ ‡targetæ·»åŠ éœ€è¦é“¾æ¥çš„å…±äº«åº“ï¼Œtargetå¯ä»¥æ˜¯ä¸€ä¸ªå¯æ‰§è¡Œæ–‡ä»¶ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªåº“æ–‡ä»¶ã€‚ æŸ¥çœ‹ç”Ÿæˆç›®æ ‡çš„åº“ä¾èµ–æƒ…å†µ 12345# ç”Ÿæˆçš„ç›®æ ‡å¯æ‰§è¡Œæ–‡ä»¶ä¸ºmain# for OSXotool -L main# for linuxldd main åªèƒ½åˆ—å‡ºåŠ¨æ€åº“ã€‚ 13. å¸¸ç”¨å˜é‡PROJECT_BINARY_DIRï¼šç¼–è¯‘å‘ç”Ÿçš„ç›®å½• PROJECT_SOURCE_DIRï¼šå·¥ç¨‹é¡¶å±‚ç›®å½• CMAKE_CURRENT_SOURCE_DIRï¼šå½“å‰CMakeLists.txtæ‰€åœ¨ç›®å½• CMAKE_MODULE_PATHï¼šè‡ªå®šä¹‰çš„cmakeæ¨¡å—æ‰€åœ¨è·¯å¾„ LIBRARY_OUTPUT_PATHï¼šé‡å®šä¹‰ç›®æ ‡åº“æ–‡ä»¶å­˜æ”¾ç›®å½• EXECUTABLE_OUTPUT_PATHï¼šé‡å®šä¹‰ç›®æ ‡å¯æ‰§è¡Œæ–‡ä»¶å­˜æ”¾ç›®å½• 14. findNAME.cmakeæ¨¡å— åœ¨å·¥ç¨‹ç›®å½•ä¸­åˆ›å»ºcmakeæ–‡ä»¶å¤¹ï¼Œå¹¶åˆ›å»ºFindHELLO.cmakeæ¨¡å—ï¼š 123456789101112131415# ç¤ºä¾‹FIND_PATH(HELLO_INCLUDE_DIR hello.h /usr/local/include/hello)FIND_LIBRARY(HELLO_LIBRARY hello /usr/local/lib)IF (HELLO_INCLUDE_DIR AND HELLO_LIBRARY) SET(HELLO_FOUND TRUE)ENDIF (HELLO_INCLUDE_DIR AND HELLO_LIBRARY)IF (HELLO_FOUND) IF (NOT HELLO_FIND_QUIETLY) MESSAGE(STATUS "Found Hello: $&#123;HELLO_LIBRARY&#125;") ENDIF (NOT HELLO_FIND_QUIETLY)ELSE (HELLO_FOUND) IF (HELLO_FIND_REQUIRED) MESSAGE(FATAL_ERROR "Could not find hello library") ENDIF (HELLO_FIND_REQUIRED)ENDIF (HELLO_FOUND) åœ¨ä¸»ç›®å½•CMakeLists.txtä¸­æ·»åŠ cmakeæ¨¡å—æ‰€åœ¨è·¯å¾„ï¼š 12# ä¸ºfind_package()æŒ‡ä»¤æˆåŠŸæ‰§è¡Œset(CMAKE_MODULE_PATH $&#123;PROJECT_SOURCE_DIR&#125;/cmake) ç„¶åå°±å¯ä»¥åœ¨æºæ–‡ä»¶CMakeLists.txtä¸­è°ƒç”¨ find_packageï¼š find_package ( name [QUIET] [REQUIRED] ) ç”¨æ¥è°ƒç”¨é¢„å®šä¹‰åœ¨CMAKE_MODULE_PATHä¸‹çš„Find\.cmakeæ¨¡å—ã€‚ æ¯ä¸€ä¸ªæ¨¡å—éƒ½ä¼šå®šä¹‰ä»¥ä¸‹å‡ ä¸ªå˜é‡ï¼š NAME_FOUND NAME_INCLUDE_DIR or NAME_INCLUDES NAME_LIBRARY or NAME_LIBRARIES æ ¹æ®æŒ‡ä»¤åé¢çš„å‚æ•°è¿˜ä¼šæœ‰ä»¥ä¸‹å˜é‡ï¼š NAME_FIND_QUIETLYï¼Œå¦‚æœæŒ‡å®šäº†QUIETå‚æ•°ï¼Œå°±ä¸ä¼šæ‰§è¡Œå¦‚ä¸‹è¯­å¥ï¼š 1MESSAGE(STATUS "Found Hello: $&#123;NAME_LIBRARY&#125;") NAME_FIND_REQUIREDï¼Œå¦‚æœæŒ‡å®šäº†REQUIREDå‚æ•°ï¼Œå°±æ˜¯æŒ‡è¿™ä¸ªå…±äº«åº“æ˜¯å·¥ç¨‹å¿…é¡»çš„ï¼Œå¦‚æœæ‰¾ä¸åˆ°ï¼Œå·¥ç¨‹å°±ä¸èƒ½ç¼–è¯‘ï¼Œå¯¹åº”åœ°ä¼šæ‰§è¡Œå¦‚ä¸‹è¯­å¥ï¼š 1MESSAGE(FATAL_ERROR "Could not find NAME library") å¯ä»¥é€šè¿‡\_FOUNDåˆ¤æ–­æ¨¡å—æ˜¯å¦è¢«æ‰¾åˆ°ï¼Œå¹¶æ‰§è¡Œä¸åŒçš„æ“ä½œï¼ˆå¦‚æ·»åŠ éæ ‡å‡†è·¯å¾„ã€è¾“å‡ºé”™è¯¯ä¿¡æ¯ç­‰ï¼‰ã€‚ 15. find_æŒ‡ä»¤ find_path find_path ( VAR name1 path1 path2 â€¦ ) VARå˜é‡ä»£è¡¨åŒ…å«name1æ–‡ä»¶çš„è·¯å¾„â€”â€”è·¯å¾„ã€‚ find_library find_library ( VAR name1 path1 path2 â€¦) VARå˜é‡åŒ…å«æ‰¾åˆ°çš„åº“çš„å…¨è·¯å¾„ï¼ŒåŒ…æ‹¬åº“æ–‡ä»¶åâ€”â€”è·¯å¾„ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ã€‚]]></content>
      <tags>
        <tag>basic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ICP, Iterative Closest Points]]></title>
    <url>%2F2018%2F05%2F05%2FICP-Iterative-Closest-Points%2F</url>
    <content type="text"><![CDATA[1 åŸºæœ¬å®ç°æ•°æ®ç‚¹äº‘é…å‡†ï¼Œæœ€ç»å…¸çš„æ–¹æ³•å°±æ˜¯ICPè¿­ä»£æœ€è¿‘ç‚¹æ³•ã€‚ æœ€è¿‘ç‚¹ï¼šæ¬§å‡ é‡Œå¾—æ„ä¹‰ä¸Šè·ç¦»æœ€è¿‘çš„ç‚¹ã€‚ è¿­ä»£ï¼šè¿­ä»£ç›®æ ‡æ˜¯é€šè¿‡ä¸æ–­æ›´æ–°è¿åŠ¨å‚æ•°ï¼Œä½¿å¾—ä¸¤ä¸ªç‚¹äº‘çš„é‡å éƒ¨åˆ†å……åˆ†å»åˆã€‚ ICPçš„æ±‚è§£åˆ†ä¸ºä¸¤ç§æ–¹å¼ï¼š åˆ©ç”¨çº¿æ€§ä»£æ•°æ±‚è§£ï¼ˆSVDï¼‰ï¼Œåœ¨ç»™å®šäº†åŒ¹é…çš„æƒ…å†µä¸‹ï¼Œæœ€å°äºŒä¹˜é—®é¢˜å®é™…ä¸Šå…·æœ‰è§£æè§£ã€‚ åˆ©ç”¨éçº¿æ€§ä¼˜åŒ–æ–¹å¼æ±‚è§£ï¼Œç±»ä¼¼äºBAæ–¹æ³•ï¼Œé€‚ç”¨äºåŒ¹é…æœªçŸ¥çš„æƒ…å†µã€‚ 2 SVDæ–¹æ³•æ±‚è§£ç®—æ³•æ¨å¯¼å¦‚ä¸‹ï¼š é¦–å…ˆå°†ç‚¹äº‘æ–‡ä»¶è¿›è¡Œç²—åŒ¹é…ï¼Œå¦‚ORBç‰¹å¾ç‚¹åŒ¹é…ã€‚ ä»ç‚¹é›†$P={\overrightarrow{p_1}, \overrightarrow{p_2}, â€¦, \overrightarrow{p_n}}$ä¸­éšæœºé€‰å–æŒ‡å®šæ•°é‡çš„ç‚¹$\{\overrightarrow{p_t}\}$ä½œä¸ºå‚è€ƒç‚¹ï¼Œå‚è€ƒç‚¹çš„æ•°é‡å†³å®šäº†ICPç®—æ³•çš„è®¡ç®—æ•ˆç‡å’Œé…å‡†ç²¾åº¦ã€‚ åœ¨å¦ä¸€ä¸ªç‚¹é›†$Q={\overrightarrow{q_1}, \overrightarrow{q_2}, â€¦, \overrightarrow{q_m}}$æ˜¯å¾…åŒ¹é…çš„ç‚¹query pointsï¼Œé‚£ä¹ˆæƒ³è¦æ‰¾åˆ°ä¸€ä¸ªæ¬§å¼å˜æ¢$R, t$ï¼Œä½¿å¾—$\forall i, p_i = Rq_i + t$ã€‚ æ±‚è§£æ¬§å¼å˜æ¢$T^k$ï¼Œä½¿å¾—$E^k=\Sigma| \overrightarrow{p_t} - T^k \overrightarrow{q_t}|^2$æœ€å°åŒ–ã€‚ å°†ç©ºé—´å˜æ¢åˆ†è§£ä¸ºæ—‹è½¬å’Œå¹³ç§»ä¸¤éƒ¨åˆ†ï¼Œé¦–å…ˆå®šä¹‰ä¸¤ä¸ªç‚¹äº‘çš„è´¨å¿ƒï¼š \overrightarrow{p} = \frac{1}{n} \Sigma \overrightarrow{p_t}, \ \ \overrightarrow{q} = \frac{1}{n} \Sigma \overrightarrow{q_t}ï¼Œè´¨å¿ƒ,\ ç”¨äºæè¿°å¹³ç§»\\ \overrightarrow p_i = \overrightarrow{p_t} - \overrightarrow{p}, \ \ \overrightarrow q_i = \overrightarrow{q_t} - \overrightarrow pï¼Œä¸­å¿ƒåŒ–ç‚¹äº‘,\ ç”¨äºæè¿°æ—‹è½¬\\äºæ˜¯æœ‰ç›®æ ‡å‡½æ•°ï¼š \begin{split} E^k & = \Sigma|\overrightarrow{p_t} - T^k \overrightarrow{q_t}|^2 = \Sigma|(p+p_i) -T (q+q_i)|^2\\ & = \Sigma|(p+p_i) -R (q+q_i) -t|^2\\ & = \Sigma |(p_i - Rq_i) + (p - Rq -t)|^2\\ & = \Sigma( |p_i - Rq_i|^2 + |p - Rq -t|^2)\\ & = \Sigma( |p_i - Rq_i|^2\\ J &= \frac{1}{2} \sum e_i = \frac{1}{2} E^k \end{split}å¯¹ç›®æ ‡å‡½æ•°å±•å¼€ï¼Œè€Œä¸”å·²çŸ¥æ—‹è½¬çŸ©é˜µæ˜¯æ­£äº¤é˜µï¼Œ$R^TR=Iâ€‹$ï¼Œæ‰€ä»¥ç›®æ ‡å‡½æ•°çš„å‰ä¸¤é¡¹éƒ½ä¸$Râ€‹$æ— å…³ï¼š R^* = argmin_R J = \frac{1}{2}\sum p_i^Tp_i + q_i^TR^TRq_i - 2p_i^TRq_iåªæœ‰æœ€åä¸€é¡¹ä¸$R$æœ‰å…³ï¼Œäºæ˜¯å¾—åˆ°å…³äº$R$çš„ç›®æ ‡å‡½æ•°ï¼š J(R) = \sum_{unrelated} -\ p_i^TRq_i = \sum - \ tr(Rq_ip_i^T) = -tr(R\sum_{i=1}^nq_ip_i^T)ç„¶åé€šè¿‡SVDå¥‡å¼‚å€¼åˆ†è§£æ±‚è§£ä¸Šè¿°é—®é¢˜çš„æœ€ä¼˜$R$ï¼Œé¦–å…ˆå®šä¹‰$W = \sum_1^n pq^T$ï¼Œå½“$W$æ»¡ç§©æ—¶ï¼š W = \sum_{i=1}^{n} \overrightarrow{p_i}*\overrightarrow{q_i^T} = U \begin{bmatrix} \sigma1 & 0 & 0 \\ 0 & \sigma2 & 0 \\ 0 & 0 & \sigma3 \end{bmatrix} V^T\\ R = UV^Tç„¶åé—´æ¥å¾—åˆ°å¹³ç§»$t$ï¼š t = {p} - R{q} ä»£ç å®ç°å¦‚ä¸‹ï¼š 1234567891011121314151617181920212223242526272829303132333435363738void pose_estimation_3d3d( const vector&lt;Point3f&gt;&amp; pts1, // point cloud 1 const vector&lt;Point3f&gt;&amp; pts2, // point cloud 2 Mat&amp; R, Mat&amp; t, Eigen::Matrix3d&amp; R_, Eigen::Vector3d&amp; t_ )&#123; Point3f p1, p2; // center of Mass int N = pts1.size(); for(int i=0; i&lt;N; i++) &#123; p1 += pts1[i]; p2 += pts2[i]; &#125; p1 /= N; p2 /= N; vector&lt;Point3f&gt; q1(N), q2(N); // remove the COM for(int i=0; i&lt;N; i++) &#123; q1[i] = pts1[i] - p1; q2[i] = pts2[i] - p2; &#125; Eigen::Matrix3d W = Eigen::Matrix3d::Zero(); // calculate W matrix for(int i=0; i&lt;N; i++) &#123; W += Eigen::Vector3d(q1[i].x, q1[i].y, q1[i].z) * Eigen::Vector3d(q2[i].x, q2[i].y, q2[i].z).transpose(); &#125; // SVD decomposition Eigen::JacobiSVD&lt;Eigen::Matrix3d&gt; svd(W, Eigen::ComputeFullU|Eigen::ComputeFullV); // SVD Eigen::Matrix3d U = svd.matrixU(); Eigen::Matrix3d V = svd.matrixV(); // calculate R,t R_ = U * V.transpose(); t_ = Eigen::Vector3d(p1.x, p1.y, p1.z) - R_ * Eigen::Vector3d(p2.x, p2.y, p2.z); 3 éçº¿æ€§ä¼˜åŒ–æ–¹æ³•å¦ä¸€ç§æ–¹å¼æ˜¯é€šè¿‡è¿­ä»£çš„æ–¹å¼æ¥å¯»æ‰¾æœ€ä¼˜å€¼ï¼Œè¯¯å·®é¡¹çš„è¡¨ç¤ºä¸ä¸Šä¸€èŠ‚ç›¸åŒï¼Œç”¨æä»£æ•°æ¥è¡¨è¾¾ä½å§¿ï¼Œæ—‹è½¬å’Œå¹³ç§»ä¸ç”¨å†è§£è€¦è¡¨ç¤ºï¼Œç›®æ ‡å‡½æ•°ä¸ºï¼š \xi^* = argmin \frac{1}{2}\sum_{i=1}^n ||p_i - exp(\xi ^{\wedge})q_i||_2^2å•ä¸ªè¯¯å·®é¡¹å…³äºä½å§¿çš„å¯¼æ•°å¯ä»¥ä½¿ç”¨æä»£æ•°æ‰°åŠ¨æ¨¡å‹æ¥æè¿°ï¼š \frac{\partial e}{\partial \delta \xi} = (e)^{\odot} = (p_i - exp(\xi^{\wedge})q_i)^{\odot}å…¶ä¸­$p_i$ä½œä¸ºå‚è€ƒç‚¹ï¼Œå¯¹æ‰°åŠ¨çš„å¯¼æ•°ä¸º0ï¼Œå› æ­¤ï¼š \frac{\partial e}{\partial \delta \xi} = - (exp(\xi^{\wedge})q_i)^{\odot}å°†æœ€å°äºŒä¹˜é—®é¢˜è¿›è¡Œå›¾æè¿°ï¼šä¼˜åŒ–å˜é‡ä¸ºæä»£æ•°è¡¨è¾¾çš„ä½å§¿$\xi$ï¼Œå› æ­¤å›¾ä¸­åªæœ‰ä¸€ä¸ªèŠ‚ç‚¹ï¼Œè¯¯å·®é¡¹ä¸ºä¸€å…ƒè¾¹ï¼ˆä»å½“å‰èŠ‚ç‚¹æŒ‡å‘å½“å‰èŠ‚ç‚¹ï¼‰ï¼Œå¯¹è¯¯å·®é¡¹åšçº¿æ€§å±•å¼€ï¼š e_i(\xi + \delta \xi) = e(\xi) + J(\xi)\delta \xiå…¶ä¸­çš„é›…å¯æ¯”çŸ©é˜µä¹Ÿå°±æ˜¯ä¸Šé¢è¯´çš„ï¼Œå•ä¸ªè¯¯å·®é¡¹å…³äºä½å§¿çš„ä¸€é˜¶å¯¼æ•°ã€‚ 4ç®—æ³•ä¼˜åŒ– åˆ é™¤ç‚¹äº‘æ•°æ®é‡‡é›†ä¸­äº§ç”Ÿçš„å™ªå£°åŠå¼‚å¸¸å€¼ã€‚ æŸ¥æ‰¾æœ€è¿‘ç‚¹çš„è¿‡ç¨‹é‡‡ç”¨KD-Treeæ•°æ®ç»“æ„ï¼Œå‡å°‘æ—¶é—´å¤æ‚åº¦ã€‚]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CLion for record]]></title>
    <url>%2F2018%2F05%2F03%2Fclion%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1 cmakeè¯¦è§cmake for recordã€‚ 2 ç®€å•é…ç½®ä¸»è¦å°±æ˜¯keymapå¾ˆä¸é€‚åº”ï¼ŒåŸºæœ¬ä¸Šåˆ é™¤äº†å¤§éƒ¨åˆ†editingçš„é…ç½®ï¼Œå› ä¸ºå¯ä»¥ç”¨vimã€‚å‰©ä¸‹çš„ä¿®æ”¹ä¸»è¦å»¶ç»­sublimeå’ŒOSXçš„ä¹ æƒ¯ã€‚ 2.1 æœç´¢å…¨å±€æœç´¢ï¼šcmd + F å‰©ä¸‹çš„äº¤ç»™vimã€‚ 2.2 å¯¼èˆªsearch for fileï¼šcmd + O search for classï¼šopt + cmd + O search for symbolï¼šshift + cmd + O go to lineï¼šcmd + G backï¼šctrl + cmd + left forwardï¼šctrl + cmd + right å‰©ä¸‹çš„äº¤ç»™vimã€‚ 2.3 æ³¨é‡Šä»£ç å—æ³¨é‡Šï¼šshift + cmd + ï¼ 2.4 æ™ºèƒ½æç¤ºçœ‹è§å°ç¯æ³¡å°±ï¼šopt + enter 2.5 run &amp; buildrunï¼šcmd + R buildï¼šcmd + B 2.6 ä»£ç ç”Ÿæˆinsertï¼šcmd + J æœ€è¿‘åœ¨ç†Ÿæ‚‰Eigenåº“ï¼Œç»å¸¸è¦æ‰“å°ä¸œè¥¿ï¼ŒåŠ äº†ä¸€ä¸ªsplitæ¨¡ç‰ˆå¿«é€Ÿåˆ†å‰²ä»£ç ç‰‡æ®µã€‚ generateï¼šcmd + N è¿˜æœ‰ä¸€äº›vimä¸ideå†²çªçš„é”®ï¼Œå¯ä»¥æ‰‹åŠ¨é€‰æ‹©æ˜¯æœä»ideè¿˜æ˜¯vimã€‚]]></content>
      <tags>
        <tag>ide</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Graph-based Optimization]]></title>
    <url>%2F2018%2F05%2F02%2Fgraph-based-optimization%2F</url>
    <content type="text"><![CDATA[1 ç»¼è¿°åŸºäºå›¾ä¼˜åŒ–çš„slamä¸»è¦åˆ†ä¸ºä»¥ä¸‹ä¸‰ä¸ªéƒ¨åˆ†ï¼š å‰ç«¯ï¼šåŸºäºä¼ æ„Ÿå™¨æ•°æ®å»ºå›¾ï¼ŒåŒ¹é…ç›¸é‚»å¸§ï¼Œæ·»åŠ èŠ‚ç‚¹å’Œè¾¹ï¼ˆraw graphï¼‰ï¼ŒèŠ‚ç‚¹è¡¨ç¤ºæœºå™¨äººçš„ä½å§¿ï¼Œè¾¹è¡¨ç¤ºèŠ‚ç‚¹ä¹‹é—´çš„ä½å§¿è”ç³»ã€‚ä½å§¿ä¿¡æ¯å¯ä»¥æ¥è‡ªé‡Œç¨‹è®¡è®¡ç®—ï¼Œå¯ä»¥æ¥è‡ªICPæ¿€å…‰ç‚¹äº‘åŒ¹é…ï¼Œä¹Ÿå¯ä»¥æ¥è‡ªé—­ç¯æ£€æµ‹åé¦ˆã€‚ åç«¯ï¼šä¼˜åŒ–å›¾ï¼ŒåŸºäºå†å²ä¿¡æ¯çš„çº¦æŸï¼Œè°ƒæ•´æ–°åŠ å…¥çš„æœºå™¨äººä½å§¿é¡¶ç‚¹ä½¿å…¶å°½é‡æ»¡è¶³è¾¹çš„çº¦æŸï¼ˆoptimized graphï¼‰ã€‚ å®è§‚çš„é—­ç¯æ£€æµ‹ï¼šæ ¹æ®é—­ç¯ä¿¡æ¯ä¼˜åŒ–çŸ«æ­£æ•´ä¸ªæ‹“æ‰‘å›¾ã€‚ è¿™é‡Œé¢æ¶‰åŠåˆ°äº†ä¸¤ä¸ªä¼˜åŒ–ï¼Œä¸€ä¸ªæ˜¯åç«¯å±€éƒ¨ä¼˜åŒ–ï¼Œä¸€ä¸ªæ˜¯å…¨å±€é—­ç¯ä¼˜åŒ–ï¼Œä¸¤è€…è®¡ç®—çš„æ€è·¯æ˜¯ä¸€æ ·çš„ã€‚ 2 ä¼˜åŒ–2.1 å…¨å±€é—­ç¯ä¼˜åŒ–ï¼Œç”¨äºçŸ«æ­£æ•´ä¸ªæ‹“æ‰‘å›¾å‰ç«¯åç«¯å®Œæˆçš„äº‹æƒ…æ˜¯æ¢ç´¢å¹¶åˆ›å»ºæ–°çš„èŠ‚ç‚¹ï¼Œè·å¾—æ–°çš„æµ‹é‡å€¼ï¼Œæ·»åŠ æ–°çš„ä½å§¿å…³ç³»æ–¹ç¨‹ï¼š \begin{eqnarray} \begin{split} & x_0 + z_{01} = x_1\\ & x_1 + z_{12} = x_2\\ & ...\\ & x_{k-1} + z_{k-1, k} = x_{k}\\ \end{split} \end{eqnarray}è€Œå…¨å±€é—­ç¯æ£€æµ‹æ·»åŠ å·²çŸ¥èŠ‚ç‚¹ä¹‹é—´çš„ä½å§¿çº¦æŸå…³ç³»ï¼š \begin{equation} x_i + z_{i j} = x_j, \ \ \ \ i,j\in [0, k] \end{equation}å†æ·»åŠ ä¸€ä¸ªåˆå§‹æ¡ä»¶ï¼ˆä¸æ˜¯å¿…é¡»çš„ï¼Œä½†æ˜¯åé¢å®éªŒè¡¨æ˜å›ºå®šä¸€ä¸ªé¡¶ç‚¹æ¯”ä¸å›ºå®šæ•ˆæœè¦å¥½â€”â€”ç›¸å½“äºæœ‰ä¸€ä¸ªæ˜ç¡®å¯ä¿¡çš„åŸºå‡†ï¼‰ï¼š x_0 = 0 ä»¥ä¸Šçº¿æ€§æ–¹ç¨‹ç»„ä¸­ï¼Œé—­ç¯æ£€æµ‹éƒ¨åˆ†çš„æ–¹ç¨‹ä¸­çš„ä¸¤ä¸ªç»“ç‚¹éƒ½åœ¨å‰é¢å‡ºç°è¿‡ï¼Œå› æ­¤ä¸å¢åŠ çŸ©é˜µçš„ç§©ï¼Œå› æ­¤æœ€ç»ˆè¦æ±‚è§£åŒ…å«$k$ä¸ªæ–¹ç¨‹$k+1$ä¸ªæœªçŸ¥æ•°çš„çº¿æ€§æ–¹ç¨‹ç»„ã€‚ é—­ç¯çš„å…³é”®æ€§ï¼šå¦‚æœæ²¡æœ‰é—­ç¯æ¡ä»¶ï¼Œæ–¹ç¨‹ç»„$Ax=b$å·¦å³ä¸¤è¾¹ç§©æ˜¯ç›¸ç­‰çš„â€”â€”æœ‰å”¯ä¸€è§£ï¼Œæ·»åŠ äº†é—­ç¯æ¡ä»¶ä»¥åï¼Œç›¸å½“äºæ–¹ç¨‹ç»„å·¦ä¾§$A$çš„ç§©ä¸å˜ï¼Œä½†æ˜¯å³ä¾§$b$çš„ç§©åˆ™å¢åŠ äº†ï¼Œ$rank(A) &lt; rank(A, b)$â€”â€”æ²¡æœ‰è§£æè§£ï¼Œåªæœ‰æœ€ä¼˜ã€‚ å®é™…ä¸ŠçŠ¶æ€$\textbf x$æ˜¯ä¸€ä¸ªåŒ…å«å¤¹è§’$\theta$çš„å‘é‡$[x, y, \theta]$ï¼Œå®é™…ç›¸å¯¹ä½å§¿çš„è®¡ç®—å¹¶éç®€å•çš„çº¿æ€§å åŠ ï¼š \textbf x \oplus \Delta \textbf x = \begin{pmatrix} x + \Delta x cos\theta - \Delta y sin \theta \\ y + \Delta x sin\theta + \Delta y cos \theta \\ normAngle(\theta + \Delta \theta) \end{pmatrix}ä¸¾ä¸ªæ —å­ï¼š æœºå™¨äººä»èµ·å§‹ä½ç½®$x_0=0$å¤„å‡ºå‘ï¼Œé‡Œç¨‹è®¡æµ‹å¾—å®ƒå‘å‰ç§»åŠ¨äº†1mï¼Œåˆ°è¾¾$x_1$ï¼Œæ¥ç€æµ‹å¾—å®ƒå‘åç§»åŠ¨äº†0.8mï¼Œåˆ°è¾¾$x_2$ï¼Œè¿™æ—¶é€šè¿‡é—­ç¯æ£€æµ‹ï¼Œå‘ç°ä»–å›åˆ°äº†èµ·å§‹ä½ç½®ã€‚ é¦–å…ˆæ ¹æ®ç»™å‡ºä¿¡æ¯æ„å»ºå›¾ï¼š x_0 + 1 = x_1\\ x_1 - 0.8 = x_2ç„¶åæ ¹æ®é—­ç¯æ¡ä»¶æ·»åŠ çº¦æŸï¼š x_2 = x_0è¡¥å……åˆå§‹æ¡ä»¶ï¼š x_0 = 0ä½¿ç”¨æœ€å°äºŒä¹˜æ³•æ±‚ä¸Šè¿°æ–¹ç¨‹ç»„çš„æœ€ä¼˜è§£ï¼Œé¦–å…ˆæ„å»ºæ®‹å·®å¹³æ–¹å’Œå‡½æ•°ï¼š \begin{eqnarray} \begin{split} & f_1 = x_0 = 0\\ & f_2 = x_1 - x_0 - 1 = 0\\ & f_3 = x_2 - x_1 + 0.8 = 0\\ & f_4 = x_2 - x_0 = 0 \end{split} \end{eqnarray} c = \Sigma_{i=1}^4 f_i^2 = x_0^2 + (x_1-x_0-1)^2 + (x_2-x_1+0.8)^2 + (x_2-x_0)^2ç„¶åå¯¹æ¯ä¸ªå‚æ•°æ±‚åå¯¼ï¼š \frac{\partial c}{\partial x_1} = -x_0 + 2x_1-x_2 -1.8=0\\ \frac{\partial c}{\partial x_2} = -x_0 - x_1 +2x_2 + 0.8 = 0è§£å¾—$x_1 = 0.93, x_2 = 0.07$ï¼Œå¯ä»¥çœ‹åˆ°é—­ç¯çŸ«æ­£äº†æ‰€æœ‰èŠ‚ç‚¹çš„ä½å§¿ï¼Œä¼˜åŒ–äº†æ•´ä¸ªæ‹“æ‰‘å›¾ã€‚ 2.2 åç«¯å±€éƒ¨ä¼˜åŒ–ï¼Œç”¨äºçŸ«æ­£å±€éƒ¨åœ°å›¾å†ä¸¾ä¸ªæ —å­ï¼š æœºå™¨äººä»èµ·å§‹ä½ç½®$x_0=0$å¤„å‡ºå‘ï¼Œå¹¶è§‚æµ‹åˆ°å…¶æ­£å‰æ–¹2må¤„æœ‰ä¸€ä¸ªè·¯æ ‡$l_0$ï¼Œé‡Œç¨‹è®¡æµ‹å¾—å®ƒå‘å‰ç§»åŠ¨äº†1mï¼Œåˆ°è¾¾$x_1$ï¼Œè¿™æ—¶è§‚æµ‹åˆ°è·¯æ ‡åœ¨å…¶æ­£å‰æ–¹0.8må¤„ã€‚ é¦–å…ˆæ ¹æ®å‰ç«¯ä¿¡æ¯å»ºå›¾ raw graphï¼ˆè¿™æ ·å»ºå›¾æ˜æ˜¾æ˜¯å­˜åœ¨ç´¯ç§¯è¯¯å·®çš„ï¼‰ï¼š x_0 + 1 = x_1ç„¶åæ·»åŠ é—­ç¯çº¦æŸï¼š x_1 + 0.8 = l_0\\ x_0 + 2 = l_0åˆå§‹æ¡ä»¶ï¼š x_0 = 0æ„å»ºæ®‹å·®å¹³æ–¹å’Œï¼š c = \Sigma_{i=1}^4 f_i^2 = x_0^2 + (x_1-x_0-1)^2 +(l_0-x_0-2)^2 + (l_0-x_1-0.8)^2æ±‚åå¯¼æ±‚è§£ï¼š$x_1 = 1.07, l_0 = 1.93$ï¼Œå¯ä»¥çœ‹åˆ°åç«¯æ˜¯å¯¹å‰ç«¯æ–°æ·»åŠ è¿›æ¥çš„èŠ‚ç‚¹ä½å§¿åšäº†çŸ«æ­£ï¼Œæ¶ˆé™¤éƒ¨åˆ†æµ‹é‡è¯¯å·®ã€‚ è¿™é‡Œé¢æ¶‰åŠåˆ°ä¸¤ç§ä¼ æ„Ÿå™¨ä¿¡æ¯â€”â€”é‡Œç¨‹è®¡å’Œæ¿€å…‰é›·è¾¾ï¼Œä¸¤ç§ä¼ æ„Ÿå™¨çš„ç²¾åº¦æ˜¯æœ‰å·®åˆ«çš„ï¼Œæˆ‘ä»¬å¯¹å…¶çš„ä¿¡ä»»ç¨‹åº¦ä¹Ÿåº”è¯¥ä¸åŒï¼Œåæ˜ åˆ°å…¬å¼ä¸­å°±æ˜¯è¦ä¸ºä¸åŒä¼ æ„Ÿå™¨ä¿¡æ¯èµ‹äºˆä¸åŒçš„æƒé‡ã€‚å‡è®¾ç¼–ç å™¨ä¿¡æ¯æ›´å‡†ç¡®ï¼Œé‚£ä¹ˆï¼š c = \Sigma_{i=1}^4 f_i^2 = x_0^2 + \textbf{10}(x_1-x_0-1)^2 +(l_0-x_0-2)^2 + (l_0-x_1-0.8)^2è°ƒæ•´æƒé‡ä¹‹åè§£å¾—ï¼š$x_1 = 1.01, l_0 = 1.9$ï¼Œå¯ä»¥çœ‹åˆ°è®¡ç®—ç»“æœä¼šå‘ç€æ›´ä¿¡ä»»çš„ä¼ æ„Ÿå™¨çš„æµ‹é‡ç»“æœé è¿‘ã€‚ 2.3 ä¸¥æ ¼æ¨å¯¼2.3.1 ä¿¡æ¯çŸ©é˜µï¼ˆè¯¯å·®æƒé‡çŸ©é˜µ) å›¾ä¼˜åŒ–é—®é¢˜è½¬åŒ–ä¸ºæœ€å°äºŒä¹˜é—®é¢˜ï¼Œé¦–å…ˆæ˜¯å¸¦æƒé‡çš„æ®‹å·®å¹³æ–¹å’Œå‡½æ•°çš„ä¸€èˆ¬å½¢å¼ï¼š F(x) = \Sigma_{i,j} e(x_i, x_j, z_{i,j})^T\Omega_{i,j}e(x_i, x_j, z_{i,j}) x^{*} = argminF(x)å…¶ä¸­çš„$\Omega_{i,j}$é¡¹å°±æ˜¯ä¸Šæ–‡æåˆ°çš„è¯¯å·®æƒé‡çŸ©é˜µï¼Œå®ƒçš„æ­£å¼åå­—å«ä¿¡æ¯çŸ©é˜µã€‚ ä¼ æ„Ÿå™¨çš„æµ‹é‡å€¼ï¼Œå¯ä»¥çœ‹ä½œæ˜¯ä»¥çœŸå€¼ä¸ºä¸­å¿ƒçš„å¤šå…ƒé«˜æ–¯åˆ†å¸ƒï¼š f_x(x_1, x_2, ..., x_k) = \frac{1}{\sqrt{}(2\pi)^k|\Sigma|}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))åæ–¹å·®çŸ©é˜µ$\Sigma$å¯¹è§’çº¿ä¸Šçš„å€¼è¡¨ç¤ºæ¯ä¸€ç»´å¯¹åº”çš„æ–¹å·®ï¼Œè¯¥æ–¹å·®å€¼è¶Šå¤§è¡¨ç¤ºè¯¥ç»´åº¦ä¸ç¡®å®šæ€§è¶Šå¤§ï¼Œå¯¹åº”çš„ä¿¡æ¯æƒé‡åº”è¯¥è¶Šå°ã€‚å®é™…ä¸Šæ‹“æ‰‘å›¾ä¸Šæ¯æ¡è¾¹å¯¹åº”çš„ä¿¡æ¯çŸ©é˜µå°±æ˜¯å¯¹åº”æµ‹é‡åæ–¹å·®çŸ©é˜µçš„é€†ã€‚ 2.3.2 éçº¿æ€§ ä¸Šæ–‡å·²ç»æåˆ°ï¼Œä½å§¿å˜åŒ–éçº¿æ€§â€”â€”éçº¿æ€§æœ€å°äºŒä¹˜é—®é¢˜ï¼Œè¦é‡‡ç”¨è¿­ä»£æ³•æ±‚è§£ã€‚è¿­ä»£æ³•éœ€è¦æœ‰ä¸€ä¸ªå¥½çš„åˆå§‹å‡è®¾å€¼ï¼Œç„¶ååœ¨è¿™ä¸ªå€¼é™„è¿‘å¢é‡å¼è¿­ä»£å¯»æ‰¾æœ€ä¼˜è§£ã€‚ f(x) = \Sigma e^T\Omega e\\ æœ€å°äºŒä¹˜é—®é¢˜ç›®æ ‡å‡½æ•°ï¼šmin \frac{1}{2}||f(x)||_2^2é¦–å…ˆè¦å°†éçº¿æ€§å‡½æ•°è½¬åŒ–æˆå…³äºå¢é‡$\Delta x$çš„çº¿æ€§å‡½æ•°â€”â€”æ³°å‹’å±•å¼€ï¼Œæ ¹æ®å…·ä½“çš„å±•å¼€å½¢å¼åˆåˆ†ä¸ºï¼š ä¸€é˜¶ã€äºŒé˜¶æ¢¯åº¦æ³• ç›´æ¥å¯¹ç›®æ ‡å‡½æ•°åœ¨$x$é™„è¿‘è¿›è¡Œæ³°å‹’å±•å¼€ï¼š ||f(x+\Delta x)||_2^2 \approx ||f(x)||_2^2 +J(x) \Delta x = \frac{1}{2} \Delta x^T H \Delta xä¸€é˜¶æ¢¯åº¦æ³•ï¼ˆæœ€é€Ÿä¸‹é™æ³•ï¼‰ï¼šåªä¿ç•™ä¸€é˜¶æ¢¯åº¦ï¼Œå¹¶å¼•å…¥æ­¥é•¿$\lambda$ï¼š \Delta x^* = -\lambda J^T(x)äºŒé˜¶æ¢¯åº¦æ³•ï¼ˆç‰›é¡¿æ³•ï¼‰ï¼šä¿ç•™ä¸€é˜¶å’ŒäºŒé˜¶æ¢¯åº¦ä¿¡æ¯ J^T(x)+H\Delta x^*=0æœ€é€Ÿä¸‹é™æ³•è¿‡äºè´ªå¿ƒï¼Œå®¹æ˜“èµ°å‡ºé”¯é½¿è·¯çº¿ï¼Œå¢åŠ è¿­ä»£æ¬¡æ•°ã€‚ç‰›é¡¿æ³•éœ€è¦è®¡ç®—ç›®æ ‡å‡½æ•°çš„äºŒé˜¶å¯¼æ•°ï¼ˆHessiançŸ©é˜µï¼‰ï¼Œè®¡ç®—å›°éš¾ã€‚ é«˜æ–¯ç‰›é¡¿æ³• å¯¹$f(x)$è€Œä¸æ˜¯ç›®æ ‡å‡½æ•°$f(x)^2$åœ¨$x$é™„è¿‘è¿›è¡Œä¸€é˜¶æ³°å‹’å±•å¼€ï¼š f(x+\Delta x) \approx f(x) + J(x) \Delta xå¯¹åº”æ¯ä¸€ä¸ªè¯¯å·®å‡½æ•°$e_{ij}$ï¼š \begin{split} & e_{ij}(x_i+\Delta x_i, x_j+\Delta x_j) = e_{i,j}(x+\Delta x) \\ &\approx e_{ij} +\frac{\partial e_{ij}}{\partial x}\Delta x = e_{ij} + J_{ij}\Delta x \end{split}â€‹ å…¶ä¸­$J_{ij}$ä¸ºåˆå§‹å€¼é™„è¿‘çš„é›…å¯æ¯”çŸ©é˜µï¼ˆå®šä¹‰è§å¡å°”æ›¼æ»¤æ³¢ï¼‰ã€‚ å¸¦å…¥ç›®æ ‡å‡½æ•°å¾—åˆ°è¿‘ä¼¼äºŒé˜¶å±•å¼€ï¼š \begin{split} & F_{ij}(x+\Delta x) = e_{ij}(x+\Delta x)^T \Omega_{ij}e_{ij}(x+\Delta x)\\ & \approx (e_{ij} + J_{ij}\Delta x)^T \Omega_{ij}(e_{ij} + J_{ij}\Delta x)\\ & = \underbrace{e_{ij}^T\Omega_{ij}e_{ij}}_{c_{ij}} + 2\underbrace{e_{ij}^T\Omega_{ij}J_{ij}}_{b_{ij}^T}\Delta x + \Delta x^T \underbrace{J_{ij}^T\Omega_{ij}J_{ij}}_{H_{ij}}\Delta x\\ & = c_{ij} + 2b_{ij}^T\Delta x + \Delta x^T H_{ij}\Delta x \end{split}æ±‚è§£å¢é‡$\Delta x$ï¼š 2b + 2H\Delta x^* = 0\\ é«˜æ–¯ç‰›é¡¿æ–¹ç¨‹ï¼šH\Delta x^* = -b\\å¯¹æ¯”ç‰›é¡¿æ³•å¯è§ï¼Œé«˜æ–¯ç‰›é¡¿æ³•ç”¨$J^TJ$ä½œä¸ºäºŒé˜¶HessiançŸ©é˜µçš„è¿‘ä¼¼ï¼Œç®€åŒ–äº†è®¡ç®—ã€‚ ä¸Šè¿°ç®—æ³•è¦æ±‚è¿‘ä¼¼$H$çŸ©é˜µæ˜¯æ­£å®šä¸”å¯é€†çš„ï¼Œå®é™…æ•°æ®å¾ˆéš¾æ»¡è¶³ï¼Œå› è€Œåœ¨ä½¿ç”¨é«˜æ–¯ç‰›é¡¿ç®—æ³•æ—¶å¯èƒ½å‡ºç°$H$ä¸ºå¥‡å¼‚çŸ©é˜µæˆ–ç—…æ€çš„æƒ…å†µï¼Œå¢é‡ç¨³å®šæ€§è¾ƒå·®ï¼Œå¯¼è‡´ç®—æ³•ä¸æ”¶æ•›ã€‚ å›¾å½¢ä¸Šæ¥æ€è€ƒï¼Œå°±æ˜¯è¿‘ä¼¼åçš„æ¢¯åº¦æ–¹å‘ä¸å†æ˜¯æ¢¯åº¦å˜åŒ–æœ€å¿«çš„æ–¹å‘ï¼Œå¯èƒ½å¼•èµ·ä¸ç¨³å®šã€‚ åˆ—æ–‡ä¼¯æ ¼â€”é©¬å¤¸å°”ç‰¹æ³• ä¸º$\Delta x$æ·»åŠ ä¸€ä¸ªä¿¡èµ–åŒºåŸŸï¼Œä¸è®©å®ƒå› ä¸ºè¿‡å¤§è€Œä½¿å¾—è¿‘ä¼¼$f(x+\Delta x) = f(x) + J(x)\Delta x$ä¸å‡†ç¡®ã€‚ \rho = \frac{f(x+\Delta x) - f(x)}{J(x) \Delta x}å¯ä»¥çœ‹åˆ°å¦‚æœ$\rho$æ¥è¿‘1ï¼Œè¯´æ˜è¿‘ä¼¼æ¯”è¾ƒå¥½ã€‚å¦‚æœ$\rho$æ¯”è¾ƒå¤§ï¼Œè¯´æ˜å®é™…å‡å°çš„å€¼è¿œå¤§äºä¼°è®¡å‡å°çš„å€¼ï¼Œéœ€è¦æ”¾å¤§è¿‘ä¼¼èŒƒå›´ï¼Œåä¹‹ä½ æ‡‚çš„ã€‚ ||D\Delta x_k||_2^2 \leq \muå°†æ¯æ¬¡è¿­ä»£å¾—åˆ°çš„$\Delta x$é™å®šåœ¨ä¸€ä¸ªåŠå¾„ä¸ºä¿¡èµ–åŒºåŸŸçš„æ¤­çƒä¸­ï¼Œæ ¹æ®$\rho$çš„å¤§å°ä¿®æ”¹ä¿¡èµ–åŒºåŸŸã€‚äºæ˜¯é—®é¢˜è½¬åŒ–æˆä¸ºäº†å¸¦ä¸ç­‰å¼çº¦æŸçš„ä¼˜åŒ–é—®é¢˜ï¼š min_\Delta x \frac{1}{2}||f(x) + J(x)\Delta x||^2_2, \ \ s.t. ||D \Delta x ||^2 = \muç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜å­è½¬åŒ–æˆæ— çº¦æŸé—®é¢˜ï¼š min_\Delta x \frac{1}{2}||f(x) + J(x)\Delta x||^2_2 + \frac{\lambda}{2}||D \Delta x ||^2å±•å¼€åå¾—åˆ°å¦‚ä¸‹å½¢å¼ï¼š (H + \lambda D^TD)\Delta x^* = -bé€šå¸¸æŠŠ$D$å–å€¼ä¸ºå•ä½é˜µ$I$ï¼Œå¾—åˆ°æ›´ç®€åŒ–å½¢å¼ï¼š (H + \lambda I)\Delta x^* = -bå½“$\lambda$è¾ƒå°æ—¶ï¼Œ$H$å ä¸»è¦åœ°ä½ï¼Œè¯´æ˜äºŒæ¬¡è¿‘ä¼¼æ¨¡å‹è¾ƒå¥½ï¼ŒLMç®—æ³•æ›´æ¥è¿‘é«˜æ–¯ç‰›é¡¿æ³•ã€‚å½“$\lambda$è¾ƒå¤§æ—¶ï¼Œ$\lambda I$å ä¸»è¦åœ°ä½ï¼ŒLMç®—æ³•æ›´æ¥è¿‘ä¸€é˜¶æ¢¯åº¦æ³•ã€‚ä¿®æ­£äº†çº¿æ€§æ–¹ç¨‹ç»„çŸ©é˜µçš„ç—…æ€é—®é¢˜ï¼Œæ¯”é«˜æ–¯ç‰›é¡¿æ³•æ›´åŠ å¥å£®ï¼Œä½†æ˜¯æ”¶æ•›é€Ÿåº¦ä¹Ÿæ›´æ…¢ã€‚ å›¾å½¢ä¸Šæ€è€ƒï¼ŒLMç®—æ³•ä¿®æ­£äº†é«˜æ–¯ç‰›é¡¿æ³•å¾—åˆ°çš„æ¢¯åº¦ï¼Œä»¥æ­¤å›ºå®šä¸€ä¸ªæœç´¢åŒºåŸŸï¼Œåœ¨åŒºåŸŸå†…å¯»æ‰¾æœ€ä¼˜ã€‚ 2.3.3 ç¨€ç–çŸ©é˜µ å¯¹äºè¯¯å·®å‡½æ•°$e_{ij}$ï¼Œå®ƒåªå’Œ$e_i$å’Œ$e_j$æœ‰å…³ï¼Œå› æ­¤å®ƒçš„é›…å¯æ¯”çŸ©é˜µæœ‰å¦‚ä¸‹ç»“æ„ï¼ˆè¡Œæ•°æ˜¯$x$çš„ç»´åº¦ï¼Œåˆ—æ•°æ˜¯æ‹“æ‰‘å›¾ä¸­èŠ‚ç‚¹æ˜ å°„å…³ç³»çš„æ•°ç›®ï¼‰ï¼š J_{ij} = \begin{bmatrix} 0 & ... & 0 & \underbrace{\frac{\partial e_{i}}{\partial x_i}}_{A_{ij}} & 0 & ... & \underbrace{\frac{\partial e_{j}}{\partial x_j}}_{B_{ij}} & 0 & ... & 0 \end{bmatrix}ç›¸åº”åœ°$b_{ij}$æ˜¯ä¸€ä¸ªåŒ…å«å¾ˆå¤š0çš„åˆ—å‘é‡ï¼š \begin{split} b_{ij}^T& = e_{ij}^T \Omega_{ij} J_{ij}\\ &= e_{ij}^T\Omega_{ij} (0 ... A_{ij}...B_{ij}...0)\\ &=(0...e_{ij}^T\Omega_{ij}A_{ij}...e_{ij}^T\Omega_{ij}B_{ij}...0) \end{split}$b = \Sigma_{ij} b_{ij}$ï¼š $H_{ij}$æ˜¯ä¸€ä¸ªåŒ…å«å¾ˆå¤š0çš„å¯¹ç§°é˜µï¼š \begin{split} H_{ij}& = J_{ij}^T \Omega_{ij}J_{ij} \\ & = \begin{pmatrix} ...\\ A_{ij}^T\\ ...\\ B_{ij}^T\\ ... \end{pmatrix} \Omega_{ij} \begin{pmatrix} ... & A_{ij} & ... & B_{ij} & ... \end{pmatrix}\\ & = \begin{pmatrix} & \\ & A_{ij}^T\Omega{ij}A_{ij} & A_{ij}^T\Omega_{ij}B_{ij} & \\ & \\ & B{ij}^T\Omega_{ij}A_{ij} & B_{ij}^T\Omega_{ij}B_{ij}& \\ & \end{pmatrix} \end{split}$H=\Sigma_{ij}H_{ij}$ï¼š æ¢³ç†ä¸€ä¸‹è®¡ç®—æµç¨‹ï¼š$e_{ij} \to J_{ij} \to A_{ij}, B_{ij} \to b_{ij}, H_{ij} \to b, H \to \Delta x^* \to x$ 2.3.4 è¯¯å·®å‡½æ•° å‰é¢å®šä¹‰è¿‡ä½å§¿çš„éçº¿æ€§å åŠ ï¼Œæ˜¾ç„¶ä½å§¿è¯¯å·®ä¹Ÿä¸æ˜¯ç®€å•çš„çº¿æ€§åŠ å‡å…³ç³»ï¼š e_{ij}(x_i, x_j) = t2v(Z_{ij}^{-1}(X_i^{-1}.X_j))å…¶ä¸­çš„$Z_{ij}$ã€$X_i$ã€$X_j$éƒ½æ˜¯çŸ©é˜µå½¢å¼ã€‚$X_i^{-1}X_j$è¡¨ç¤ºèŠ‚ç‚¹jåˆ°èŠ‚ç‚¹iä¹‹é—´çš„ä½å§¿å·®å¼‚$\hat Z_{ij}$ï¼Œå‡è®¾è¿™ä¸ªè½¬ç§»çŸ©é˜µå½¢å¼å¦‚ä¸‹ï¼š \hat Z_{ij} = \begin{bmatrix} R_{2*2} & t_{2*1} \\ 0 & 1 \end{bmatrix}å‡è®¾æµ‹é‡å€¼$Z_{ij}$å½¢å¼å¦‚ä¸‹ï¼š Z_{ij} = \begin{bmatrix} R^{'} & t^{'}\\ 0 & 1 \end{bmatrix}åˆ†å—çŸ©é˜µçš„æ±‚é€†è¿‡ç¨‹å¦‚ä¸‹ï¼š \begin{split} \begin{bmatrix} R & t\\ 0 & 1 \end{bmatrix}^{-1}& = \begin{bmatrix} \begin{bmatrix} I & t\\ 0 & 1 \end{bmatrix} \begin{bmatrix} R & 0\\ 0 & 1 \end{bmatrix} \end{bmatrix}^{-1}= \begin{bmatrix} R & 0\\ 0 & 1 \end{bmatrix}^{-1} \begin{bmatrix} I & t\\ 0 & 1 \end{bmatrix}^{-1}\\ & =\begin{bmatrix} R^T & 0\\ 0 & 1 \end{bmatrix} \begin{bmatrix} I & -t\\ 0 & 1 \end{bmatrix}= \begin{bmatrix} R^T & -R^Tt\\ 0 & 1 \end{bmatrix} \end{split}æ‰€ä»¥è¯¯å·®$e_{ij}$è®¡ç®—å¦‚ä¸‹ï¼š E_{ij} = Z_{ij}^{-1}\hat Z_{ij} = \begin{bmatrix} R^{'} & t^{'}\\ 0 & 1 \end{bmatrix}^{-1} \begin{bmatrix} R & t\\ 0 & 1 \end{bmatrix}= \begin{bmatrix} R^{'T} & -R^{'T}t^{'}\\ 0 & 1 \end{bmatrix} \begin{bmatrix} R & t\\ 0 & 1 \end{bmatrix}= \begin{bmatrix} R^{'T}R & R^{'T}(t-t^{'})\\ 0 & 1 \end{bmatrix} e_{ij} = t2v(E_{ij})= \begin{bmatrix} R_z(t_{\hat z} - t_z)_{2*1}\\ \theta_\hat z - \theta_z \end{bmatrix}_{3*1}= \begin{bmatrix} R_z(x_{\hat z} - x_z)\\ R_z(y_{\hat z} - y_z)\\ \theta_\hat z - \theta_z \end{bmatrix}= \begin{bmatrix} R_z[R_i(x_{j} - x_i) - x_z]\\ R_z[R_i(y_{j} - y_{i}) - y_z]\\ \theta_j - \theta_i - \theta_z \end{bmatrix}æ±‚è§£é›…å¯æ¯”çŸ©é˜µ$J_{ij}$ï¼š A_{ij} = \begin{bmatrix} \frac{\partial e_1}{\partial x_i} & \frac{\partial e_1}{\partial y_i} & \frac{\partial e_1}{\partial \theta_i}\\ \frac{\partial e_2}{\partial x_i} & \frac{\partial e_2}{\partial y_i} & \frac{\partial e_2}{\partial \theta_i}\\ \frac{\partial e_3}{\partial x_i} & \frac{\partial e_3}{\partial y_i} & \frac{\partial e_3}{\partial \theta_i}\\ \end{bmatrix}= \begin{bmatrix} -R_z^TR_i^T & 0 & R_z^T\frac{\partial R_I^T}{\partial \theta_i}(x_j-x_i)\\ 0 & -R_z^TR_i^T & R_z^T\frac{\partial R_I^T}{\partial \theta_i}(y_j-y_i)\\ 0 & 0 & -1 \end{bmatrix} B_{ij} = \begin{bmatrix} R_z^TR_i^T & 0\\ 0 & 1 \end{bmatrix}ç´¯åŠ $b$å’Œ$H$çŸ©é˜µï¼š b_{[i]} += A_{ij}\Omega_{ij}e_{ij}\\ b_{[j]} += B_{ij}^T\Omega_{ij}e_{ij}\\ H_{[ii]} += A_{ij}^T\Omega_{ij}A_{ij}\\ H_{[ij]} += A_{ij}^T\Omega_{ij}B_{ij}\\ H_{[ji]} += B_{ij}^T\Omega_{ij}A_{ij}\\ H_{[jj]} += B_{ij}^T\Omega_{ij}B_{ij}]]></content>
      <tags>
        <tag>slam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å…³è”åˆ†æ]]></title>
    <url>%2F2018%2F04%2F29%2F%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[1 å¼•è¨€é¢‘ç¹é¡¹é›†ï¼šé›†åˆï¼Œ${a, b, c}$ å…³è”è§„åˆ™ï¼šæ˜ å°„ï¼Œ$a\to b$ æ”¯æŒåº¦ï¼šé’ˆå¯¹æŸä¸ªé¢‘ç¹é¡¹é›†ï¼Œ$support(é¢‘ç¹é¡¹é›†a) = \frac{freq(é¢‘ç¹é¡¹é›†a)}{freq(æ‰€æœ‰é¡¹é›†)}$ å¯ä¿¡åº¦ï¼šè¡¡é‡æŸæ¡å…³è”è§„åˆ™ï¼Œ$confidence(a\to b) = \frac{support(a|b)}{support(a)}$ å¯¹äºåŒ…å«Nä¸ªå…ƒç´ çš„æ•°æ®é›†ï¼Œå¯èƒ½çš„é›†åˆæœ‰$2^N - 1$ç§ï¼Œæš´åŠ›éå†æ˜¾ç„¶è¯ä¸¸ï¼Œå› æ­¤å¼•å…¥AprioriåŸç†ã€‚ AprioriåŸç†ï¼šå‡å°‘å¯èƒ½çš„é¡¹é›†ï¼Œé¿å…æŒ‡æ•°å¢é•¿ã€‚ backwardsï¼šå¦‚æœæŸä¸ªé¡¹é›†æ˜¯é¢‘ç¹çš„ï¼Œé‚£ä¹ˆå®ƒçš„æ‰€æœ‰å­é›†ä¹Ÿæ˜¯é¢‘ç¹çš„ã€‚ forwardsï¼šå¦‚æœä¸€ä¸ªé¡¹é›†æ˜¯éé¢‘ç¹é¡¹é›†ï¼Œé‚£ä¹ˆå®ƒçš„æ‰€æœ‰è¶…é›†ä¹Ÿæ˜¯éé¢‘ç¹çš„ã€‚ 2 Aprioriç®—æ³•1def apriori(dataSet, minSupport=0.5): ç®—æ³•æ€è·¯ï¼šä»å•ä¸ªé¡¹é›†å¼€å§‹æ£€æŸ¥ï¼Œå»æ‰é‚£äº›ä¸æ»¡è¶³æœ€å°æ”¯æŒåº¦çš„é¡¹é›†ï¼Œç„¶åå¯¹å‰©ä¸‹çš„é›†åˆè¿›è¡Œç»„åˆï¼Œå¾—åˆ°åŒ…å«ä¸¤ä¸ªå…ƒç´ çš„é¡¹é›†ï¼Œé‡å¤æ‰«æï¼Œç„¶åå°†å‰©ä½™é¡¹é›†ç»„åˆæˆåŒ…å«ä¸‰ä¸ªå…ƒç´ çš„é›†åˆï¼Œä¾æ¬¡ç±»æ¨ï¼Œç›´åˆ°æ‰€æœ‰é¡¹é›†éƒ½è¢«å»æ‰ã€‚ ä¸ºå•¥æœ€åä¼šå¾—åˆ°ç©ºé›†ï¼šå› ä¸ºåŒ…å«æ‰€æœ‰å…ƒç´ çš„é¡¹é›†ä¸€å®šä¸æ˜¯é¢‘ç¹é¡¹é›†ï¼Œå¦åˆ™æ ¹æ®AprioriåŸç†ï¼Œå®ƒçš„å…¨éƒ¨å­é›†éƒ½æ˜¯é¢‘ç¹é¡¹é›†ã€‚ å¦‚ä½•ä»åŒ…å«kä¸ªå…ƒç´ çš„é¡¹é›†é›†åˆç”ŸæˆåŒ…å«k+1ä¸ªå…ƒç´ çš„é¡¹é›†é›†åˆï¼šä»kä¸ªå…ƒç´ çš„é¡¹é›†åˆ°k+1ä¸ªå…ƒç´ é¡¹é›†çš„æ‰©å……ï¼Œåªå…è®¸æœ‰ä¸€ä¸ªå…ƒç´ çš„ä¸åŒï¼Œç®—æ³•ä¸­ä¸ºäº†é¿å…é‡å¤ç»“æœï¼Œåªå¯¹å‰k-1ä¸ªå…ƒç´ ç›¸åŒçš„ä¸¤ä¸ªé¡¹é›†æ±‚å¹¶é›†ã€‚ ä»£ç å®ç°è¿‡ç¨‹ä¸­å‘ç°äº†å‡ ä¸ªçŸ¥è¯†è®°å½•ä¸€ä¸‹ï¼š mapå‡½æ•°çš„è¿”å›å€¼ï¼špython2ä¸‹ç›´æ¥è¿”å›åˆ—è¡¨ï¼Œpython3ä¸‹è¿”å›çš„æ˜¯è¿­ä»£å™¨ï¼š 12345map(frozenset, C1)# è¿”å› &lt;map object at 0x101e78940&gt; list(map(frozenset, C1))# è¿”å› list[frozenset1(), frozenset2(), ...] å­—å…¸çš„updateæ–¹æ³•ï¼š 12# å°†dict2çš„é”®å€¼æ·»åŠ åˆ°dict1ä¸­ï¼Œåœ¨æ¶‰åŠè¿­ä»£æ“ä½œæ—¶å¯ä»¥çœç•¥ä¼ é€’ä¸­é—´å€¼dict1.update(dict2) set &amp; frozensetï¼šsetæ— æ’åºä¸”ä¸é‡å¤ï¼Œå¹¶ä¸”å¯å˜ï¼Œå› æ­¤unhashableã€‚frozensetä¸å¯å˜ï¼Œå¯ä»¥ç”¨ä½œå­—å…¸çš„keyã€‚ 3 å…³è”è§„åˆ™å¯¹ä¸€ä¸ªåŒ…å«kä¸ªå…ƒç´ çš„é¢‘ç¹é¡¹é›†ï¼Œå…¶ä¸­å¯èƒ½çš„å…³è”è§„åˆ™æœ‰ï¼š C_N^1 + C_N^2 + ... + C_N^{N-1}æš´åŠ›éå†è‚¯å®šåˆè¯ä¸¸ï¼Œå› æ­¤å»¶ç»­Aprioriçš„æ€è·¯ï¼Œå…³è”è§„åˆ™ä¹Ÿæœ‰ä¸€æ¡ç±»ä¼¼çš„å±æ€§ï¼š å¦‚æœæŸæ¡è§„åˆ™çš„å‰ä»¶ä¸æ»¡è¶³æœ€å°å¯ä¿¡åº¦è¦æ±‚ï¼Œé‚£ä¹ˆå®ƒçš„æ‰€æœ‰å­é›†ä¹Ÿä¸æ»¡è¶³æœ€å°å¯ä¿¡åº¦è¦æ±‚ã€‚ å¯¹åº”çš„ï¼Œå¦‚æœæŸæ¡è§„åˆ™çš„åä»¶ä¸æ»¡è¶³æœ€å°å¯ä¿¡åº¦è¦æ±‚ï¼Œé‚£ä¹ˆå®ƒçš„æ‰€æœ‰è¶…é›†ä¹Ÿä¸æ»¡è¶³ã€‚ ç®—æ³•æ€è·¯ï¼šå¯¹æ¯ä¸ªè‡³å°‘åŒ…å«ä¸¤ä¸ªå…ƒç´ çš„é¢‘ç¹é¡¹é›†ï¼Œä»åéƒ¨åªåŒ…å«ä¸€ä¸ªå…ƒç´ çš„è§„åˆ™å¼€å§‹ï¼Œå¯¹è¿™äº›è§„åˆ™è¿›è¡Œæµ‹è¯•ï¼Œæ¥ä¸‹æ¥å¯¹æ‰€æœ‰å‰©ä½™è§„åˆ™çš„åä»¶è¿›è¡Œç»„åˆï¼Œå¾—åˆ°åŒ…å«ä¸¤ä¸ªå…ƒç´ çš„åä»¶ï¼ˆå¯¹åº”çš„è¡¥é›†å°±æ˜¯å‰ä»¶ï¼‰ï¼Œä¾æ¬¡ç±»æ¨ï¼Œç›´åˆ°æµ‹è¯•å®Œæ‰€æœ‰å¯èƒ½çš„åä»¶ã€‚ ä¸ºå•¥åªæ£€æŸ¥å‰åä»¶äº’è¡¥çš„è§„åˆ™ï¼šå› ä¸ºä¸€ä¸ªé¢‘ç¹é¡¹é›†çš„æ‰€æœ‰å­é›†ä¹Ÿéƒ½æ˜¯é¢‘ç¹é¡¹é›†ï¼Œæ‰€ä»¥ä¸€ä¸ªé¢‘ç¹é¡¹é›†ä¸­ä¸äº’è¡¥çš„è§„åˆ™å°†æ˜¯è¯¥é¢‘ç¹é¡¹é›†çš„æŸä¸ªå­é›†çš„äº’è¡¥è§„åˆ™ã€‚ 4 FP-growthç®—æ³•Aprioriç®—æ³•é¿å…äº†æš´åŠ›éå†å­é¡¹é›†çš„æŒ‡æ•°å¼å¢é•¿ï¼Œä½†æ˜¯å¯¹æ¯ä¸€ä¸ªæ–°ç”Ÿæˆçš„é¢‘ç¹é¡¹é›†ï¼Œéƒ½è¦æ‰«ææ•´ä¸ªæ•°æ®é›†ï¼Œå½“æ•°æ®é›†å¾ˆå¤§æ—¶ï¼Œè¿™ç§æŠ›ç‰©çº¿å¼å¢é•¿çš„æ—¶é—´å¤æ‚åº¦ä¹Ÿä¸å¤ªä»¤äººæ»¡æ„ã€‚ FP-growthç®—æ³•å€ŸåŠ©ä¸€ç§ç§°ä¸ºFPæ ‘çš„æ•°æ®ç»“æ„å­˜å‚¨æ•°æ®ï¼Œæ¥æŠ½è±¡åŸå§‹æ•°æ®é›†ï¼š é¡¹é›†ä»¥è·¯å¾„çš„æ–¹å¼å­˜å‚¨åœ¨æ ‘ä¸­ ç›¸ä¼¼é¡¹ä¹‹é—´ç›¸è¿æ¥æˆé“¾è¡¨ ä¸€ä¸ªå…ƒç´ é¡¹å¯ä»¥åœ¨FPæ ‘ä¸­å‡ºç°å¤šæ¬¡ FPæ ‘å­˜å‚¨çš„æ˜¯å…ƒç´ çš„å‡ºç°é¢‘ç‡ é¡¹é›†å®Œå…¨ä¸åŒæ—¶ï¼Œæ ‘æ‰ä¼šåˆ†å‰ï¼Œå¦åˆ™ä¼šæœ‰å¤ç”¨è·¯å¾„ â€‹ç®—æ³•æ€è·¯ï¼šé¦–å…ˆéå†ä¸€éåŸå§‹æ•°æ®é›†ï¼Œè®°å½•å…ƒç´ çš„å‡ºç°é¢‘ç‡ï¼Œå»æ‰ä¸æ»¡è¶³æœ€å°æ”¯æŒåº¦çš„å…ƒç´ ã€‚ç„¶åå†éå†ä¸€éå‰©ä¸‹çš„é›†åˆå…ƒç´ ï¼Œæ„å»ºFPæ ‘ã€‚ç„¶åå°±å¯ä»¥é€šè¿‡FPæ ‘æŒ–æ˜é¢‘ç¹é¡¹é›†ã€‚ æ„å»ºFPæ ‘ï¼šä¾æ¬¡éå†æ¯ä¸€ä¸ªé¡¹é›†ï¼Œé¦–å…ˆå°†å…¶ä¸­çš„éé¢‘ç¹é¡¹ç§»é™¤ï¼Œå¹¶æŒ‰ç…§å…ƒç´ å‡ºç°é¢‘ç‡å¯¹è¿‡æ»¤åçš„å…ƒç´ è¿›è¡Œé‡æ’åºã€‚å¯¹è¿‡æ»¤ã€æ’åºåçš„é›†åˆï¼Œå¦‚æœæ ‘ä¸­å·²å­˜åœ¨ç°æœ‰å…ƒç´ ï¼Œåˆ™å¢åŠ ç°æœ‰å…ƒç´ çš„è®¡æ•°å€¼ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œåˆ™å‘æ ‘ä¸­æ·»åŠ ä¸€ä¸ªåˆ†æ”¯ï¼Œæ–°å¢èŠ‚ç‚¹çš„åŒæ—¶è¿˜è¦æ›´æ–°é“¾è¡¨å…ƒç´ ã€‚ä¸»è¦å°±æ¶‰åŠä¸¤ä¸ªæ•°æ®ç»“æ„ï¼š 12345678910111213# è‡ªå®šä¹‰èŠ‚ç‚¹æ•°æ®ç»“æ„class treeNode: def __init__(self, nameValue, numOccur, parentNode): self.name self.count self.nodeLink # é“¾è¡¨ä¿¡æ¯ï¼ŒæŒ‡å‘ä¸‹ä¸€ä¸ªç›¸ä¼¼é¡¹ self.parent self.children # ç”¨äºå­˜å‚¨å…ƒç´ frequencyä»¥åŠé“¾æ¥ç›¸ä¼¼é¡¹çš„å­—å…¸æ•°æ®ç»“æ„freq = &#123;&#125;freq[node_name] = [frequency, node1, node2, ...] å› ä¸ºé›†åˆä¸­å…ƒç´ çš„å‡ºç°é¢‘ç‡å¯èƒ½ç›¸ç­‰ï¼Œå› æ­¤è¿‡æ»¤æ’åºçš„ç»“æœä¸å”¯ä¸€ï¼Œç”Ÿæˆçš„æ ‘ç»“æ„ä¹Ÿä¼šæœ‰å·®å¼‚ã€‚ ç¬¬ä¸€æ¬¡éå†åˆ é™¤éé¢‘ç¹å…ƒç´ æ—¶ï¼Œå‘ç°å­—å…¸åœ¨è¿­ä»£è¿‡ç¨‹ä¸­ä¸èƒ½åˆ é™¤itemï¼Œæˆ‘è½¬åŒ–æˆlistæš´åŠ›è§£å†³äº†ï¼Œä¸çŸ¥é“æœ‰æ²¡æœ‰ä»€ä¹ˆä¼˜é›…çš„æ–¹å¼ã€‚ 123456del freq[item]# è¿”å› RuntimeError: dictionary changed size during iterationfor item in list(freq.keys()): if freq[item] &lt; minSupport: del(freq[item]) æŒ–æ˜é¢‘ç¹é¡¹é›†ï¼šé¦–å…ˆåˆ›å»ºæ¡ä»¶æ¨¡å¼åŸºï¼Œç„¶ååˆ©ç”¨æ¡ä»¶æ¨¡å¼åŸºï¼Œæ„å»ºæ¡ä»¶FPæ ‘ã€‚ 1 æ¡ä»¶æ¨¡å¼åŸºï¼šä»¥æ‰€æŸ¥æ‰¾å…ƒç´ é¡¹ä¸ºç»“å°¾çš„å‰ç¼€è·¯å¾„é›†åˆï¼Œå¹¶ä¸”æ¯æ¡å‰ç¼€è·¯å¾„éƒ½ä¸èµ·å§‹å…ƒç´ é¡¹çš„è®¡æ•°å€¼ç›¸å…³è”ã€‚ï¼ˆè¿™é‡Œé¢ç”¨åˆ°äº†å‰é¢å®šä¹‰çš„parentå’ŒnodeLinkå±æ€§ï¼‰ 2 æ„é€ æ¡ä»¶FPæ ‘ï¼šä¸æ„é€ æ ‘çš„è¿‡ç¨‹ç›¸åŒï¼Œä½¿ç”¨çš„dataSetæ¢æˆäº†æ¡ä»¶æ¨¡å¼åŸºè€Œå·²ï¼Œå‡½æ•°å‚æ•°countå°±æ˜¯é¢„ç•™å½©è›‹ã€‚è¿™æ ·å¾—åˆ°çš„å°±æ˜¯æŒ‡å®šé¢‘ç¹é¡¹çš„æ¡ä»¶FPæ ‘ã€‚ 12345def updateTree(cond_set, myTree, freq_header, count):# cond_set: ä¸€æ¡path# myTree: æ ¹èŠ‚ç‚¹# freq_header: dict[node_name] = [frequency, head_node]# count: pathå¯¹åº”çš„count æ„é€ çš„æ¡ä»¶FPæ ‘è¿‡æ»¤æ‰äº†æ¡ä»¶æ¨¡å¼åŸºä¸­çš„ä¸€äº›å…ƒç´ ï¼šè¿™äº›å…ƒç´ æœ¬èº«æ˜¯é¢‘ç¹é¡¹ï¼Œä½†æ˜¯ä¸æŒ‡å®šå…ƒç´ ç»„åˆçš„é›†åˆä¸æ˜¯é¢‘ç¹çš„ã€‚ ç›¸åº”åœ°ï¼Œæ¡ä»¶æ ‘ä¸­å‰©ä½™å…ƒç´ ä¸æŒ‡å®šé¢‘ç¹é¡¹ç»„åˆçš„é›†åˆæ˜¯é¢‘ç¹çš„ã€‚ 3 è¿­ä»£ï¼šä»ç”Ÿæˆçš„æ¡ä»¶FPæ ‘ä¸­ï¼Œå¯ä»¥å¾—åˆ°æ›´å¤æ‚çš„é¢‘ç¹é¡¹ã€‚æ±‚è§£å¤æ‚é¢‘ç¹é¡¹çš„æ¡ä»¶æ¨¡å¼åŸºï¼Œè¿›è€Œç”Ÿæˆå¯¹åº”çš„æ¡ä»¶FPæ ‘ï¼Œå°±èƒ½å¾—åˆ°æ›´å¤æ‚çš„é¢‘ç¹é¡¹ï¼Œä¾æ¬¡ç±»æ¨è¿›è¡Œè¿­ä»£ï¼Œç›´åˆ°FPæ ‘ä¸ºç©ºã€‚ â€‹]]></content>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sublimeæ³¨å†Œç è¢«æ— é™æ¬¡ç§»é™¤]]></title>
    <url>%2F2018%2F04%2F25%2Fsublime%E6%B3%A8%E5%86%8C%E7%A0%81%E8%A2%AB%E6%97%A0%E9%99%90%E6%AC%A1%E7%A7%BB%E9%99%A4%2F</url>
    <content type="text"><![CDATA[æœ€è¿‘ä¸çŸ¥é“sublime3æŠ½ä»€ä¹ˆé£ï¼Œçªç„¶å¼€å§‹éªŒè¯æ³¨å†Œç äº†ï¼Œè¾“å…¥ä¸€ä¸ªvalid codeåˆ†åˆ†é’Ÿç»™ä½ ç§»é™¤ã€‚ æ”¶è—ä¸€ä¸ªè§£å†³åŠæ³•ï¼Œæœ‰æ•ˆæ€§å¾…éªŒè¯ï¼š 1234# add the following to your host file(/private/etc/hosts)127.0.0.1 license.sublimehq.com127.0.0.1 45.55.255.55127.0.0.1 45.55.41.223]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VTKç¼–è¯‘æŠ¥é”™no override found for vtkpolydatamapper]]></title>
    <url>%2F2018%2F04%2F12%2FVTK%E7%BC%96%E8%AF%91%E6%8A%A5%E9%94%99no-override-found-for-vtkpolydatamapper%2F</url>
    <content type="text"><![CDATA[æŠ¥é”™åŸå› æ˜¯é€šè¿‡IDEç¼–è¯‘è€Œä¸æ˜¯ç›´æ¥é€šè¿‡cmakeï¼Œå› æ­¤è¦æ·»åŠ å¦‚ä¸‹ä»£ç æ®µï¼š 123#include "vtkAutoInit.h" VTK_MODULE_INIT(vtkRenderingOpenGL2); VTK_MODULE_INIT(vtkInteractionStyle); å…ˆè®°å½•è§£å†³åŠæ³•ï¼Œmore details ç•™åˆ°ä»¥åã€‚ åŸºç¡€æµ‹è¯•ï¼š 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include "vtkAutoInit.h"VTK_MODULE_INIT(vtkRenderingOpenGL2); // VTK was built with vtkRenderingOpenGL2VTK_MODULE_INIT(vtkInteractionStyle);#include &lt;vtkSphereSource.h&gt;#include &lt;vtkPolyData.h&gt;#include &lt;vtkSmartPointer.h&gt;#include &lt;vtkPolyDataMapper.h&gt;#include &lt;vtkActor.h&gt;#include &lt;vtkRenderWindow.h&gt;#include &lt;vtkRenderer.h&gt;#include &lt;vtkRenderWindowInteractor.h&gt;int main(int, char *[])&#123; // Create a sphere vtkSmartPointer&lt;vtkSphereSource&gt; sphereSource = vtkSmartPointer&lt;vtkSphereSource&gt;::New(); sphereSource-&gt;SetCenter(0.0, 0.0, 0.0); sphereSource-&gt;SetRadius(5.0); //mapper vtkSmartPointer&lt;vtkPolyDataMapper&gt; mapper = vtkSmartPointer&lt;vtkPolyDataMapper&gt;::New(); mapper-&gt;SetInputConnection(sphereSource-&gt;GetOutputPort()); //actor vtkSmartPointer&lt;vtkActor&gt; actor = vtkSmartPointer&lt;vtkActor&gt;::New(); actor-&gt;SetMapper(mapper); //renderer ,renderWindow, renderWindowInteractor. vtkSmartPointer&lt;vtkRenderer&gt; renderer = vtkSmartPointer&lt;vtkRenderer&gt;::New(); vtkSmartPointer&lt;vtkRenderWindow&gt; renderWindow = vtkSmartPointer&lt;vtkRenderWindow&gt;::New(); renderWindow-&gt;AddRenderer(renderer); vtkSmartPointer&lt;vtkRenderWindowInteractor&gt; renderWindowInteractor = vtkSmartPointer&lt;vtkRenderWindowInteractor&gt;::New(); renderWindowInteractor-&gt;SetRenderWindow(renderWindow); renderer-&gt;AddActor(actor); renderer-&gt;SetBackground(.3, .6, .3); // Background color green renderWindow-&gt;Render(); renderWindowInteractor-&gt;Start(); return EXIT_SUCCESS;&#125;]]></content>
      <tags>
        <tag>basic</tag>
      </tags>
  </entry>
</search>
