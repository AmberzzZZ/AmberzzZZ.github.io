<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="要糖有糖，要猫有猫">
<meta property="og:type" content="website">
<meta property="og:title" content="Less is More">
<meta property="og:url" content="https://amberzzzz.github.io/page/9/index.html">
<meta property="og:site_name" content="Less is More">
<meta property="og:description" content="要糖有糖，要猫有猫">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Less is More">
<meta name="twitter:description" content="要糖有糖，要猫有猫">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://amberzzzz.github.io/page/9/"/>





  <title>Less is More</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Less is More</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://amberzzzz.github.io/2020/03/13/attention系列/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="amber.zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Less is More">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/13/attention系列/" itemprop="url">attention系列</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-13T16:26:14+08:00">
                2020-03-13
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/03/13/attention系列/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2020/03/13/attention系列/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="0-综述"><a href="#0-综述" class="headerlink" title="0. 综述"></a>0. 综述</h2><ol>
<li><p>attention的方式分为两种（<a href="https://blog.csdn.net/yideqianfenzhiyi/article/details/79422857?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">Reference</a>）</p>
<ul>
<li>学习权重分布<ul>
<li>部分加权（hard attention）／全部加权（soft attention）</li>
<li>原图上加权／特征图上加权</li>
<li>空间尺度加权／channel尺度加权／时间域加权／混合域加权</li>
<li>CAM系列、SE-block系列：花式加权，学习权重，non-local的模块，作用于某个维度</li>
</ul>
</li>
<li>任务分解<ul>
<li>设计不同的网络结构（或分支）专注于不同的子任务，</li>
<li>重新分配网络的学习能力，从而降低原始任务的难度，使网络更加容易训练</li>
<li>STN、deformable conv：添加显式的模块负责学习形变/receptive field的变化，local模块，apply by pixel</li>
</ul>
</li>
<li>local / non-local<ul>
<li>local模块的结果是pixel-specific的</li>
<li>non-local模块的结果是全局共同计算的的</li>
</ul>
</li>
</ul>
</li>
<li><p>基于权重的attention（<a href="https://blog.csdn.net/bigbug_sec/article/details/89025318" target="_blank" rel="noopener">Reference</a>）</p>
<ul>
<li>注意力机制通常由一个连接在原神经网络之后的额外的神经网络实现</li>
<li>整个模型仍然是端对端的，因此注意力模块能够和原模型一起同步训练</li>
<li>对于soft attention，注意力模块对其输入是可微的，所以整个模型仍可用梯度方法来优化</li>
<li>而hard attention要离散地选择其输入的一部分，这样整个系统对于输入不再是可微的</li>
</ul>
</li>
<li><p>papers</p>
<ul>
<li><p>[STN] <a href="https://amberzzzz.github.io/2021/02/03/transform-in-CNN/">Spatial Transformer Networks</a></p>
</li>
<li><p>[deformable conv] Deformable Convolutional Networks </p>
</li>
<li><p>[CBAM] CBAM: Convolutional Block Attention Module </p>
</li>
<li><p>[SE-Net] <a href="https://amberzzzz.github.io/2020/04/30/SE-block/">Squeeze-and-Excitation Networks</a></p>
</li>
<li><p>[SE-block的一系列变体] SC-SE（for segmentation）、CMPE-SE（复杂又没用）</p>
</li>
<li><p>[SK-Net] Selective Kernel Networks：是attension module，但是主要改进点在receptive field，trick大杂烩</p>
</li>
<li><p>[GC-Net] GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond </p>
</li>
</ul>
</li>
</ol>
<h2 id="CBAM-Convolutional-Block-Attention-Module"><a href="#CBAM-Convolutional-Block-Attention-Module" class="headerlink" title="CBAM: Convolutional Block Attention Module"></a>CBAM: Convolutional Block Attention Module</h2><ol>
<li>动机<ul>
<li>attention module </li>
<li>lightweight and general  </li>
<li>improvements in classification and detection </li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>deeper： can obtain richer representation </li>
<li>increased width：can outperform an extremely deep network</li>
<li>cardinality：results in stronger representation power than depth and width </li>
<li>attention：improves the representation of interests <ul>
<li>humans exploit a sequence of partial glimpses and selectively focus on salient parts </li>
<li>Residual Attention Network：computes 3d attention map</li>
<li>we decompose the process that learns channel attention and spatial attention separately </li>
<li>SE-block：use global average-pooled features</li>
<li>we suggest to use max-pooled features as well  </li>
</ul>
</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li>sequentially infers a <strong>1D channel attention map</strong> and a <strong>2D spatial attention map</strong>  </li>
<li><p>broadcast and element-wise multiplication </p>
<p><img src="/2020/03/13/attention系列/CBAM.png" width="45%;"></p>
</li>
<li><p>Channel attention module</p>
<ul>
<li>focuses on ‘what’ is meaningful  </li>
<li>squeeze the spatial dimension </li>
<li>use both average-pooled and max-pooled features simultaneously </li>
<li>both descriptors are then forwarded to a shared MLP to reduce dimension</li>
<li>【QUESTION】看论文MLP是线性的吗，没写激活函数</li>
<li>then use element-wise summation  </li>
<li>sigmoid function </li>
</ul>
</li>
<li>Spatial attention module <ul>
<li>focuses on ‘where’ </li>
<li>apply average-pooling and max-pooling  along the channel axis and concatenate </li>
<li>7x7 conv</li>
<li>sigmoid function </li>
</ul>
</li>
<li><p>Arrangement of attention modules</p>
<ul>
<li>in a parallel or sequential manner </li>
<li>we found sequential better than parallel </li>
<li>we found channel-first order slightly better than the spatial-first</li>
</ul>
<p><img src="/2020/03/13/attention系列/module.png" width="45%;"></p>
</li>
<li><p>integration</p>
<ul>
<li>apply CBAM on the convolution outputs in each block </li>
<li>in residual path</li>
<li><p>before the add operation</p>
<p><img src="/2020/03/13/attention系列/integration.png" width="55%;"></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>实验</p>
<ul>
<li><p>Ablation studies </p>
<ul>
<li>Channel attention：两个pooling path都有效，一起用最好</li>
<li>Spatial attention：1x1conv直接squeeze也行，avg+max更好，7x7conv略好于3x3conv</li>
<li>arrangement：前面说了，比SE的单spacial squeeze好，channel在前好于在后，串行好于并行</li>
</ul>
</li>
<li><p>Classification results：outperform baselines and SE</p>
</li>
<li>Network Visualization  <ul>
<li>cover the target object regions better </li>
<li>the target class scores also increase accordingly </li>
</ul>
</li>
<li>Object Detection results<ul>
<li>apply to detectors：right before every classifier </li>
<li>apply to backbone</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="SK-Net-Selective-Kernel-Networks"><a href="#SK-Net-Selective-Kernel-Networks" class="headerlink" title="SK-Net: Selective Kernel Networks"></a>SK-Net: Selective Kernel Networks</h2><ol>
<li><p>动机</p>
<ul>
<li>生物的神经元的感受野是随着刺激变化而变化的</li>
<li>propose a selective kernel unit<ul>
<li>adaptively adjust the RF</li>
<li>multiple branches with different kernel sizes</li>
<li>guided fusion</li>
<li>大杂烩：multi-branch&amp;kernel，group conv，dilated conv，attention mechanism</li>
</ul>
</li>
<li>SKNet<ul>
<li>by stacking multiple SK units</li>
<li>在分类任务上验证</li>
</ul>
</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>multi-scale aggregation<ul>
<li>inception block就有了</li>
<li>but linear aggregation approach may be insufficient</li>
</ul>
</li>
<li>multi-branch network<ul>
<li>two-branch：以resnet为代表，主要是为了easier to train</li>
<li>multi-branch：以inception为代表，主要为了得到multifarious features </li>
</ul>
</li>
<li>grouped/depthwise/dilated conv<ul>
<li>grouped conv：reduce computation，提升精度</li>
<li>depthwise conv：reduce computation，牺牲精度</li>
<li>dilated conv：enlarge RF，比dense large kernel节省参数量</li>
</ul>
</li>
<li>attention mechanism<ul>
<li>加权系列：<ul>
<li>SENet&amp;CBAM：</li>
<li>相比之下SKNet多了adaptive RF</li>
</ul>
</li>
<li>动态卷积系列：<ul>
<li>STN不好训练，训好以后变换就定死了</li>
<li>deformable conv能够在inference的时候也动态的变化变换，但是没有multi-scale和nonlinear aggregation</li>
</ul>
</li>
</ul>
</li>
<li>thus we propose SK convolution <ul>
<li>multi-kernels：大size的conv kernel是用了dilated conv</li>
<li>nonlinear aggregation</li>
<li>computationally lightweight </li>
<li>could successfully embedded into small models</li>
<li>workflow<ul>
<li>split</li>
<li>fuse</li>
<li>select</li>
</ul>
</li>
<li>main difference from inception<ul>
<li>less customized</li>
<li>adaptive selection instead of equally addition</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>selective kernel convolution</p>
<ul>
<li><p>split</p>
<ul>
<li>multi-branch with different kernel size</li>
<li>grouped/depthwise conv + BN + ReLU</li>
<li>5x5 kernel can be further replaced with dilated conv</li>
</ul>
</li>
<li><p>fuse</p>
<ul>
<li>to learn the control of information flow from different branches</li>
<li>element-wise summation</li>
<li>global average pooling</li>
<li>fc-BN-ReLU：reduce dimension，at least 32</li>
</ul>
</li>
<li><p>select</p>
<ul>
<li><p>channel-wise weighting factor A &amp; B &amp; more：A+B + more = 1</p>
</li>
<li><p>fc-softmax</p>
<p>  <img src="/2020/03/13/attention系列/softmax.png" width="50%;"></p>
</li>
<li><p>在2分支的情况下，一个权重矩阵A就够了，B是冗余的，因为可以间接算出来</p>
</li>
<li><p>reweighting</p>
</li>
</ul>
<p><img src="/2020/03/13/attention系列/sk.png" width="80%;"></p>
</li>
</ul>
</li>
<li><p>network</p>
<ul>
<li>start from resnext</li>
<li>repeated SK units：类似bottleneck<ul>
<li>1x1 conv</li>
<li>SK conv</li>
<li>1x1 conv</li>
<li>hyperparams<ul>
<li>number of branches M=2</li>
<li>group number G=32：cardinality of each path</li>
<li>reduction ratio r=16：fuse operator中dim-reduction的参数</li>
</ul>
</li>
</ul>
</li>
<li><p>嵌入到轻量的网络结构</p>
<ul>
<li>MobileNet/shuffleNet</li>
<li>把其中的3x3 depthwise卷积替换成SK conv</li>
</ul>
<p><img src="/2020/03/13/attention系列/skNet.png" width="90%;"></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>实验</p>
<ul>
<li><p>比sort的resnet、densenet、resnext精度都要好</p>
<p>  <img src="/2020/03/13/attention系列/error.png" width="40%;"></p>
</li>
</ul>
</li>
</ol>
<h2 id="GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond"><a href="#GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond" class="headerlink" title="GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond"></a>GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</h2><ol>
<li><p>动机</p>
<ul>
<li>Non-Local Network (NLNet)<ul>
<li>capture long-range dependencies</li>
<li>obtain query-specific global context</li>
<li>but we found global contexts are almost the same for different query positions</li>
</ul>
</li>
<li>we produce<ul>
<li>query-independent formulation</li>
<li>smiliar structure as SE-Net</li>
<li>aims at global context modeling</li>
</ul>
</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li><p>Capturing long-range dependency </p>
<ul>
<li><p>mainly by sdeeply stacking conv layers：inefficient </p>
</li>
<li><p>non-local network</p>
<ul>
<li>via self-attention mechanism </li>
<li>computes the pairwise relations between the query position then aggregate</li>
<li><p>但是不同位置query得到的attention map基本一致</p>
<p><img src="/2020/03/13/attention系列/NLNet.png" width="40%;"></p>
</li>
</ul>
</li>
<li><p>we simply the non-local block</p>
<ul>
<li>query-independent </li>
<li>maintain acc &amp; save computation</li>
</ul>
</li>
</ul>
</li>
<li><p>our proposed GC-block</p>
<ul>
<li>unifies both the NL block and the SE block</li>
<li>three steps<ul>
<li>global context modeling：</li>
<li>feature transform module：capture channel-wise interdependency</li>
<li>fusion module：merge into the original features</li>
</ul>
</li>
</ul>
</li>
<li><p>多种任务上均有涨点</p>
<ul>
<li>但都是在跟resnet50对比</li>
</ul>
</li>
</ul>
</li>
<li><p>revisit NLNet</p>
<ul>
<li><p>non-local block</p>
<ul>
<li><p><img src="/2020/03/13/attention系列/NL formulation.png" width="45%;"></p>
</li>
<li><p>$f(x_i, x_j)$：</p>
<ul>
<li>encodes the relationship between position i &amp; j</li>
<li>计算方式有Gaussian、Embedded Gaussian、Dot product、Concat</li>
<li>different instantiations achieve comparable performance</li>
</ul>
</li>
<li><p>$C(x)$：norm factor</p>
</li>
<li><p>$x_i + \sum^{N_p} F(x_j)$：aggregates a specific global feature on $x_i$</p>
</li>
<li><p>widely-used Embedded Gaussian：</p>
<p>  <img src="/2020/03/13/attention系列/NL block.png" width="50%;"></p>
</li>
<li><p>嵌入方式：</p>
<ul>
<li>Mask R-CNN with FPN and Res50 </li>
<li>only add one non-local block right before the last residual block of res4</li>
</ul>
</li>
<li><p>observations &amp; inspirations</p>
<ul>
<li>distances among inputs show that input features are discriminated</li>
<li>outputs &amp; attention maps are almost the same：global context after training is actually independent of query position</li>
<li>inspirations<ul>
<li>simplify the Non-local block</li>
<li>no need of query-specific</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>simplifying form of NL block：SNL</p>
<ul>
<li><p>求一个common的global feature，share给全图每个position</p>
<p><img src="/2020/03/13/attention系列/simplify1.png" width="45%;"></p>
</li>
<li><p>进一步简化：把$x_j$的1x1 conv提到前面，FLOPs大大减少，因为feature scale从HW变成了1x1</p>
<p>  <img src="/2020/03/13/attention系列/simplify2.png" width="45%;"></p>
</li>
<li><p>the SNL block achieves comparable performance to the NL block with significantly lower FLOPs</p>
</li>
</ul>
</li>
<li><p>global context modeling</p>
<ul>
<li>SNL可以抽象成三部分：<ul>
<li>global attention pooling：通过$W_k$ &amp; softmax获取attention weights，然后进行global pooling</li>
<li>feature transform：1x1 conv</li>
<li>feature aggregation：broadcast element-wise add</li>
</ul>
</li>
<li><p>SE-block也可以分解成类似的抽象</p>
<ul>
<li>global attention pooling：用了简单的global average pooling</li>
<li>feature transform：用了squeeze &amp; excite的fc-relu-fc-sigmoid</li>
<li>feature aggregation：broadcast element-wise multiplication </li>
</ul>
<p><img src="/2020/03/13/attention系列/gc-block.png" width="80%;"></p>
</li>
</ul>
</li>
<li><p>Global Context Block</p>
<ul>
<li>integrate the benefits of both<ul>
<li>SNL global attention pooling：effective modeling on long-range dependency</li>
<li>SE bottleneck transform：light computation（只要ratio大于2就会节省参数量和计算量）</li>
</ul>
</li>
<li>特别地，在SE transform的squeeze layer上，又加了BN<ul>
<li>ease optimization </li>
<li>benefit generalization </li>
</ul>
</li>
<li>fusion：add</li>
<li>嵌入方式：<ul>
<li>GC-ResNet50</li>
<li>add GC-block to all layers (c3+c4+c5) in resnet50 with se ratio of 16</li>
</ul>
</li>
</ul>
</li>
<li><p>relationship to SE-block</p>
<ul>
<li>首先是fusion method reflects different goals<ul>
<li>SE基于全局信息rescales the channels，间接使用</li>
<li>GC直接使用，将long-range dependency加在每个position上</li>
</ul>
</li>
<li>其次是norm layer<ul>
<li>ease optimization</li>
</ul>
</li>
<li>最后是global attention pooling<ul>
<li>SE的GAP是a special case</li>
<li>weighting factors shows superior</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://amberzzzz.github.io/2020/02/24/Deeplab系列/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="amber.zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Less is More">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/24/Deeplab系列/" itemprop="url">Deeplab系列</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-24T18:20:49+08:00">
                2020-02-24
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/02/24/Deeplab系列/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2020/02/24/Deeplab系列/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><ol>
<li><p>papers</p>
<ul>
<li>deeplabV1: SEMANTIC IMAGE SEGMENTATION WITH DEEP CONVOLUTIONAL NETS AND FULLY CONNECTED CRFS，主要贡献提出了空洞卷积，使得feature extraction阶段输出的特征图维持较高的resolution</li>
<li>deeplabV2: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs，主要贡献是多尺度ASPP结构</li>
</ul>
</li>
</ol>
<ul>
<li>deeplabV3: Rethinking Atrous Convolution for Semantic Image Segmentation，提出了基于ResNet的串行&amp;并行两种结构，细节上提到了multi-grid，改进了ASPP模块<ul>
<li>deeplabV3+: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation </li>
</ul>
</li>
<li>Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation </li>
</ul>
<ol>
<li><p>分割结果比较粗糙的原因</p>
<ul>
<li>池化：将全图抽象化，降低分辨率，会丢失细节信息，平移不变性，使得边界信息不清晰</li>
<li>没有利用标签之间的概率关系：CNN缺少对空间、边缘信息等约束</li>
</ul>
<p>对此，deeplabV1引入了</p>
<ul>
<li><a href="https://www.jianshu.com/p/f743bd9041b3" target="_blank" rel="noopener">空洞卷积</a>：VGG中提出的多个小卷积核代替大卷积核的方法，只能使感受野线性增长，而多个空洞卷积串联，可以实现指数增长。</li>
<li>全连接条件随机场CRF：作为stage2，提高模型捕获细节的能力，提升边界分割精度</li>
</ul>
</li>
<li><p>大小物体同时分割</p>
<p>deeplabV2引入</p>
<ul>
<li>多尺度ASPP(Atrous Spatial Pyramid Pooling)：并行的采用多个采样率的空洞卷积提取特征，再进行特征融合</li>
<li>backbone model change：VGG16改为ResNet</li>
<li>使用不同的学习率</li>
</ul>
</li>
<li><p>进一步改进模型架构</p>
<p>deeplabV3引入</p>
<ul>
<li>ASPP嵌入ResNet后几个block</li>
<li>去掉了CRF</li>
</ul>
</li>
<li><p>使用原始的Conv/pool操作，得到的low resolution score map，pool stride会使得过程中丢弃一部分信息，上采样会得到较大的失真图像，使用空洞卷积，保留特征图上的全部信息，同时keep resolution，减少了信息损失</p>
</li>
<li><p>DeeplabV3的ASPP相比较于V2，增加了一条1x1 conv path和一条image pooling path，加GAP这条path是因为，实验中发现，随着rate的增大，有效的weight数目开始减少（<strong>部分超出边界无法有效捕捉远距离信息</strong>），因此利用global average pooling提取了image-level的特征并与ASPP的特征并在一起，来补充因为dilation丢失的信息</p>
<p>空洞卷积的path，V2是每条path分别空洞卷积然后接两个1x1conv（没有BN），V3是空洞卷积和BatchNormalization组合</p>
<p>fusion方式，V2是sum fusion，V3是所有path concat然后1x1 conv，得到最终score map</p>
</li>
<li><p>DeeplabV3的串行版本，“In order to maintain original image size, convolutions are replaced with strous convolutions with rates that differ from each other with factor 2”，ppt上说后面几个block复制了block4，每个block里面三层conv，其中最后一层conv stride2，然后为了maintain output size，空洞rate*2，这个不太理解。</p>
<p>multi-grid method：对每个block里面的三层卷积采用不同空洞率，unit rate（e.g.(1,2,4)） * rate （e.g. 2）</p>
</li>
</ol>
<h2 id="deeplabV1-SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CONVOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS"><a href="#deeplabV1-SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CONVOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS" class="headerlink" title="deeplabV1: SEMANTIC IMAGE SEGMENTATION WITH DEEP CONVOLUTIONAL NETS AND FULLY CONNECTED CRFS"></a>deeplabV1: SEMANTIC IMAGE SEGMENTATION WITH DEEP CONVOLUTIONAL NETS AND FULLY CONNECTED CRFS</h2><ol>
<li><p>动机</p>
<ul>
<li>brings together methods from Deep Convolutional Neural Networks and probabilistic graphical models </li>
<li>poor localization property of deep networks</li>
<li>combine a fully connected Conditional Random Field (CRF) </li>
<li>be able to localize segment boundaries beyond previous accuracies</li>
<li>speed: atrous</li>
<li>accuracy: </li>
<li>simplicity: cascade modules</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>DCNN learns hierarchical abstractions of data, which is desirable for high-level vision tasks (classification)</li>
<li>but it hampers low-level tasks,  such as pose estimation and semantic segmentation, where we want precise localization, rather than <strong>abstraction of spatial details</strong></li>
<li>two technical hurdles in DCNNs when applying to image labeling tasks<ul>
<li>pooling, loss of resolution: we employ the ‘atrous’ (with holes) for efficient dense computation </li>
<li>spacial invariance: we use the fully connected pairwise CRF to capture fine edge details </li>
</ul>
</li>
<li><p>Our approach </p>
<ul>
<li>treats every pixel as a CRF node</li>
<li>exploits long-range dependencies</li>
<li>and uses CRF inference to directly optimize a DCNN-driven cost function </li>
</ul>
<p><img src="/2020/02/24/Deeplab系列/deeplabV1.png" width="65%"></p>
</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>structure</p>
<ul>
<li>fully convolutional VGG-16</li>
<li>keep the first 3 subsampling blocks for a target stride of 8 </li>
<li>use hole algorithm conv filters for the last two blocks</li>
<li>keep the pooling layers for the purpose of fine-tuing，change strides from 2 to 1</li>
<li>for dense map(h/8), the first fully convolutional 7*7*4096 is computational, thus change to 4*4 / 3*3 convs</li>
<li>further computation decreasement: reduce the fc channels from 4096 to 1024</li>
</ul>
</li>
<li><p>train</p>
<ul>
<li>label：ground truth subsampled by 8 </li>
<li>loss function：cross-entropy </li>
</ul>
</li>
<li><p>test</p>
<ul>
<li>x8：simply bilinear interpolation </li>
<li>fcn：stride32 forces them to use learned upsampling layers, significantly increasing the complexity and training time</li>
</ul>
</li>
<li><p>CRF</p>
<ul>
<li><p>short-range：used to smooth noisy </p>
</li>
<li><p>fully connected model：to recover detailed local structure rather than further smooth it   </p>
</li>
<li><p>energy function:</p>
<script type="math/tex; mode=display">
  E(x) = \sum_{i}\theta_i(x_i) + \sum_{ij}\theta_{ij}(x_i, x_j)\\
  \theta_i(x_i) = -logP(x_i)\\</script><p>  $P(x_i)$ is the bi-linear interpolated probability output of DCNN.</p>
<script type="math/tex; mode=display">
  \theta_{ij}(x_i, x_j) = \mu(x_i, x_j)\sum_{m=1}^K \omega_m k^m (f_i,f_j)\\
  \mu(x_i, x_j) = \begin{cases}
  1& \text{if }x_i \neq x_j\\
  0& \text{otherwise}
  \end{cases}</script><p>  $k^m(f_i, f_j)$ is the Gaussian kernel depends on features (involving pixel positions &amp; pixel color intensities)</p>
</li>
</ul>
</li>
<li><p>multi-scale prediction</p>
<ul>
<li>to increase the boundary localization accuracy </li>
<li>we attach to the input image and the output of each of the first four max pooling layers a two-layer MLP (first layer: 128 3x3 convolutional filters, second layer: 128 1x1 convolutional filters)  </li>
<li>the feature maps above is <strong>concatenated</strong> to the main network’s last layer feature map </li>
<li>the new outputs is enhanced by 128*5=640 channels</li>
<li>we only adjust the newly added weights</li>
<li>introducing these extra direct connections from fine-resolution layers improves localization performance, <strong>yet the effect is not as dramatic as the one obtained with the fully-connected CRF</strong></li>
</ul>
</li>
</ul>
</li>
<li><p>空洞卷积dilated convolution</p>
<p> <img src="/2020/02/24/Deeplab系列/holes.png" width="65%"></p>
<ul>
<li><p>空洞卷积相比较于正常卷积，多了一个 hyper-parameter——dilation rate，指的是kernel的间隔数量(正常的convolution dilatation rate是1)</p>
</li>
<li><p>fcn：先pooling再upsampling，过程中有信息损失，能不能设计一种新的操作，不通过pooling也能<strong>有较大的感受野</strong>看到更多的信息呢？</p>
</li>
<li><p>如图(b)的2-dilated conv，kernel size只有3x3，但是这个卷积的感受野已经增大到了7x7（假设前一层是3x3的1-dilated conv）</p>
</li>
<li><p>如图(c)的4-dilated conv，kernel size只有3x3，但是这个卷积的感受野已经增大到了15x15（假设前两层是3x3的1-dilated conv和3x3的2-dilated conv）</p>
</li>
<li><p>而传统的三个3x3的1-dilated conv堆叠，只能达到7x7的感受野</p>
</li>
<li><p>dilated使得在不做pooling损失信息的情况下，加大了感受野，让每个卷积输出都包含较大范围的信息</p>
</li>
<li><p><strong>The Gridding Effect</strong>：如下图，多次叠加3x3的2-dilated conv，会发现我们将愿输入离散化了。因此叠加卷积的 dilation rate 不能有大于1的公约数。</p>
<p>  <img src="/2020/02/24/Deeplab系列/grid.png" width="60%"></p>
</li>
<li><p><strong>Long-ranged information</strong>：增大dilation rate对大物体有效果，对小物体可能有弊无利</p>
</li>
<li><p>HDC(Hybrid Dilated Convolution)设计结构</p>
<ul>
<li><p>叠加卷积的 dilation rate 不能有大于1的公约数，如[2,4,6]</p>
</li>
<li><p>将 dilation rate 设计成锯齿状结构，例如 [1, 2, 5, 1, 2, 5] 循环结构，锯齿状能够同时满足小物体大物体的分割要求(小 dilation rate 来关心近距离信息，大 dilation rate 来关心远距离信息)</p>
</li>
<li><p>满足$M_i = max [M_{i+1}-2r_i, M_{i+1}-2(M_{i+1}-r_i), r_i]$，$M_i$是第i层最大dilation rate</p>
</li>
<li><p>一个可行方案[1,2,5]：</p>
<p>  <img src="/2020/02/24/Deeplab系列/hdc.png" width="65%"></p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="deeplabV2-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs"><a href="#deeplabV2-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs" class="headerlink" title="deeplabV2: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs"></a>deeplabV2: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</h2><ol>
<li><p>动机</p>
<ul>
<li>atrous convolution：control the resolution </li>
<li>atrous spatial pyramid pooling (ASPP) ：multiple sampling rates </li>
<li>fully connected Conditional Random Field (CRF) </li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li><p>three challenges in the application of DCNNs to semantic image segmentation </p>
<ul>
<li>reduced feature resolution：max-pooling and downsampling (‘striding’)  —&gt; atrous convolution</li>
<li>existence of objects at multiple scales：multi input scale —&gt; ASPP</li>
<li>reduced localization accuracy due to DCNN invariance：skip-layers —&gt; CRF</li>
</ul>
<p><img src="/2020/02/24/Deeplab系列/deeplabV2.png" width="70%"></p>
</li>
<li><p>improvements compared to its first version </p>
<ul>
<li>better segment objects at multiple scales</li>
<li>ResNet replaces VGG16</li>
<li>a more comprehensive experimental evaluation on models &amp; dataset</li>
</ul>
</li>
<li>related works<ul>
<li>jointly learning of the DCNN and CRF to form an end-to-end trainable feed-forward network </li>
<li>while in our work still a 2 stage process</li>
<li>use a series of atrous convolutional layers with increasing rates to aggregate multiscale context </li>
<li>while in our structure using parallel instead of serial </li>
</ul>
</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>atrous convolution</p>
<ul>
<li>在下采样以后的特征图上，运行普通卷积，相当于在原图上运行上采样的filter<ul>
<li>1-D示意图上可以看出，两者感受野相同</li>
<li>同时能保持high resolution</li>
</ul>
</li>
<li><p>while both the number of filter parameters and the number of operations per position stay constant</p>
<p><img src="/2020/02/24/Deeplab系列/1d.png" width="40%">   <img src="/2020/02/24/Deeplab系列/2d.png" width="40%">  </p>
</li>
<li><p>把backbone中下采样的层(pooling/conv)中的stride改成1，然后将接下来的conv层都改成2-dilated conv：could allow us to compute feature responses at the original image resolution</p>
</li>
<li>efficiency/accuracy trade-off：using atrous convolution to increase the resolution by a factor of 4</li>
<li>followed by fast bilinear interpolation by a factor of 8 to the original image resolution </li>
<li><p>Bilinear interpolation is sufficient in this setting because the class score maps are quite smooth <strong>unlike FCN</strong></p>
<p><img src="/2020/02/24/Deeplab系列/scoremap.png" width="40%"></p>
</li>
<li><p>Atrous convolution offers easily control of the field-of-view and finds the best trade-off between accurate localization (small field-of-view) and context assimilation (large field-of-view)：大感受野，抽象融合上下文，大感受野，low-level局部信息准确</p>
</li>
<li>实现：（1）根据定义，给filter上采样，插0；（2）给feature map下采样得到k*k个reduced resolution maps，然后run orgin conv，组合位移结果</li>
</ul>
</li>
<li><p>ASPP</p>
<ul>
<li><p>multi input scale：</p>
<ul>
<li>run parallel DCNN branches that share the same parameters</li>
<li>fuse by taking at each position the maximum response across scales</li>
<li>computing</li>
</ul>
</li>
<li><p>spatial pyramid pooling</p>
<ul>
<li>run multiple parallel filters with different rates</li>
<li>multi-scale features are further processed in separate branches：fc7&amp;fc8</li>
<li><p>fuse：sum fusion</p>
<p><img src="/2020/02/24/Deeplab系列/aspp.png" width="55%"></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>CRF：keep the same as V1</p>
</li>
</ul>
</li>
</ol>
<h2 id="deeplabV3-Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation"><a href="#deeplabV3-Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation" class="headerlink" title="deeplabV3: Rethinking Atrous Convolution for Semantic Image Segmentation"></a>deeplabV3: Rethinking Atrous Convolution for Semantic Image Segmentation</h2><ol>
<li><p>动机</p>
<ul>
<li>for segmenting objects at multiple scales<ul>
<li>employ atrous convolution in cascade or in parallel with multiple atrous rates</li>
<li>augment ASPP with image-level features encoding global context and further boost performance</li>
</ul>
</li>
<li>without DenseCRF</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>our proposed module consists of atrous convolution with various rates and <strong>batch normalization layers</strong></li>
<li>modules in cascade or in parallel：when applying a 3*3  atrous convolution with an extremely large rate, it fails to capture long range information due to image boundary effects</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>Atrous Convolution</p>
<p>  for each location $i$ on the output $y$ and a filter $w$,  an $r$-rate atrous convolution is applied over the input feature map $x$：</p>
<script type="math/tex; mode=display">
  y[i] = \sum_k x[i+rk]w[k]</script></li>
<li><p>in cascade</p>
<ul>
<li>duplicate several copies of the last ResNet block (block4)</li>
<li><p>extra block5, block6, block7 as replicas of block4 </p>
</li>
<li><p>multi-rates</p>
<p><img src="/2020/02/24/Deeplab系列/cascade.png" width="70%"></p>
</li>
</ul>
</li>
<li><p>ASPP</p>
<ul>
<li><p>we include batch normalization within ASPP</p>
</li>
<li><p>as the sampling rate becomes larger, the number of valid filter weights becomes smaller (beyond boundary)</p>
</li>
<li><p>to incorporate global context information：we adopt image-level features by GAP on the last feature map of the model </p>
<p>  GAP —&gt; 1*1*256 conv —&gt; BN —&gt; bilinearly upsample </p>
</li>
<li><p>fusion: concatenated + 1*1 conv</p>
<p><img src="/2020/02/24/Deeplab系列/aspp+gap.png" width="70%"></p>
</li>
<li><p>seg：final 1*1*n_classes conv</p>
</li>
</ul>
</li>
<li><p>training details</p>
<ul>
<li>large crop size required to make sure the large atrous rates effective </li>
<li>upsample the output: it is important to keep the groundtruths intact and instead upsample the final logits</li>
</ul>
</li>
</ul>
</li>
<li><p>结论</p>
<ul>
<li><p>output stride=8 好过16，但是运算速度慢了几倍</p>
<p><img src="/2020/02/24/Deeplab系列/result.png" width="70%"></p>
</li>
</ul>
</li>
</ol>
<h2 id="deeplabV3-Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation"><a href="#deeplabV3-Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation" class="headerlink" title="deeplabV3+: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation"></a>deeplabV3+: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</h2><ol>
<li><p>动机</p>
<ul>
<li>spatial pyramid pooling module captures rich contextual information  </li>
<li>encode-decoder structure captures sharp object boundaries </li>
<li>combine the above two methods</li>
<li>propose a simple yet effective decoder module </li>
<li>explore Xception backbone</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>even though rich semantic information is encoded through ASPP, detailed information related to object boundaries is missing due to striding operations </li>
<li>atrous convolution could alleviate but suffer the computational balance</li>
<li>while encoder-decoder models lend themselves to faster computation (since no features are dilated) in the encoder path and gradually recover sharp object boundaries in the decoder path </li>
<li><p>所谓encoder-decoder structure，就是通过encoder和decoder之间的短连接来将不同尺度的特征集成起来，增加这样的shortcut，同时增大网络的下采样率（encoder path上不使用空洞卷积，因此为了达到同样的感受野，得增加pooling，然后保留最底端的ASPP block），既减少了计算，又enrich了local border这种细节特征</p>
<p><img src="/2020/02/24/Deeplab系列/deeplabV3+.png" width="50%"></p>
</li>
<li><p>applying the atrous separable convolution to both the ASPP and decoder modules：最后又引入可分离卷积，进一步提升计算效率</p>
</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>atrous separable convolution </p>
<ul>
<li><p>significantly reduces the computation complexity while maintaining similar (or better) performance</p>
<p><img src="/2020/02/24/Deeplab系列/atrousDW.png" width="50%"></p>
</li>
</ul>
</li>
<li><p>DeepLabv3 as encoder</p>
<ul>
<li>output_stride=16/8：remove the striding of the last 1/2 blocks</li>
<li>atrous convolution：apply atrous convolution to the blocks without striding</li>
<li>ASPP：run 1x1 conv in the end to set the output channel to 256</li>
</ul>
</li>
<li><p>proposed decoder</p>
<ul>
<li>naive decoder：bilinearly upsampled by 16 </li>
<li>proposed：first bilinearly upsampled by 4, then concatenated with the corresponding low-level features</li>
<li>low-level features：<ul>
<li>apply 1x1 conv on the low-level features to reduce the number of channels to avoid <strong>outweigh the importance</strong></li>
<li>the last feature map in res2x residual block before striding </li>
</ul>
</li>
<li>combined features：apply 3x3 conv(2 layers, 256 channels) to obtain sharper segmentation results </li>
<li>more shortcut：observed no significant improvement</li>
</ul>
<p><img src="/2020/02/24/Deeplab系列/en-de.png" width="45%"></p>
</li>
<li><p>modified Xception backbone</p>
<ul>
<li>deeper</li>
<li>all the max pooling operations are replaced with depthwise separable convolutions with striding </li>
<li><p>DWconv-BN-ReLU-PWconv-BN-ReLU</p>
<p><img src="/2020/02/24/Deeplab系列/xception.png" width="50%"></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>实验</p>
<ol>
<li><p>decoder effect on border</p>
<p> <img src="/2020/02/24/Deeplab系列/border.png" width="50%"></p>
</li>
</ol>
</li>
<li><p>f</p>
</li>
</ol>
<h2 id="Auto-DeepLab-Hierarchical-Neural-Architecture-Search-for-Semantic-Image-Segmentation"><a href="#Auto-DeepLab-Hierarchical-Neural-Architecture-Search-for-Semantic-Image-Segmentation" class="headerlink" title="Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation"></a>Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation</h2>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://amberzzzz.github.io/2020/01/08/RCNN系列/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="amber.zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Less is More">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/08/RCNN系列/" itemprop="url">RCNN系列</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-08T16:03:10+08:00">
                2020-01-08
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/01/08/RCNN系列/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2020/01/08/RCNN系列/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><ol>
<li><p>papers</p>
<p> [R-CNN] R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation </p>
<p> [SPP] SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</p>
<p> [Fast R-CNN] Fast R-CNN: Fast Region-based Convolutional Network</p>
<p> [Faster R-CNN] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</p>
<p> [Mask R-CNN] Mask R-CNN  </p>
<p> [FPN] FPN: Feature Pyramid Networks for Object Detection</p>
<p> [Cascade R-CNN] Cascade R-CNN: Delving into High Quality Object Detection</p>
</li>
</ol>
<h2 id="R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation"><a href="#R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation" class="headerlink" title="R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation"></a>R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation</h2><ol>
<li><p>动机</p>
<ul>
<li>localizing objects with a deep network and training a high-capacity model with only a small quantity of annotated detection data<ul>
<li>apply CNN to region proposals: R-CNN represents ‘Regions with CNN features’</li>
<li>supervised pre-training </li>
</ul>
</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>model as a regression problem: not fare well in practice  </li>
<li>build a sliding-window detector: have to maintain high spatial resolution</li>
<li><strong>what we do: </strong>our method gener- ates around 2000 category-independent region proposals for the input image, extracts a fixed-length feature vector from each proposal using a CNN, and then classifies each region with category-specific linear SVMs </li>
<li>conventional solution to training a large CNN is ‘using unsupervised pre-training, followed by supervised fine-tuning’</li>
<li><strong>what we do: </strong>‘supervised pre-training on a large auxiliary dataset (ILSVRC), followed by domain specific fine-tuning on a small dataset (PASCAL)’</li>
<li><strong>we also demonstrate: </strong>a simple bounding box regression method significantly reduces mislocalizations</li>
<li><strong>R-CNN operates on regions:</strong> it is natural to extend it to the task of semantic segmentation </li>
</ul>
</li>
<li><p>要素</p>
<ul>
<li>category-independent region proposals </li>
<li>a large convolutional neural network that extracts a fixed-length feature vector from each region </li>
<li><p>a set of class-specific linear SVMs</p>
<p><img src="/2020/01/08/RCNN系列/R-CNN.png" width="40%;"></p>
</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>Region proposals: we use selective search</p>
</li>
<li><p>Feature extraction: we use Krizhevsky CNN, 227*227 RGB input, 5 convs, 2 fcs, 4096 output</p>
<ul>
<li>we first dilate the tight bounding box (padding=16)</li>
<li><p>then warp the bounding box to the required size (各向异性缩放)</p>
<p><img src="/2020/01/08/RCNN系列/warp.png" width="40%;"></p>
</li>
</ul>
</li>
<li><p>Test-time detection:</p>
<ul>
<li>we score each extracted feature vector using the SVM trained for each class</li>
<li>we apply a greedy non-maximum suppression (for each class independently)  </li>
<li>对留下的这些框进行canny边缘检测，就可以得到bounding-box</li>
<li>(then B-BoxRegression)</li>
</ul>
</li>
<li><p>Supervised pre-training: pre-trained the CNN on a large auxiliary dataset (ILSVRC 2012) with <strong>image-level annotations</strong></p>
</li>
<li><p>Domain-specific fine-tuning: </p>
<ul>
<li>continue SGD training of the CNN using only warped region proposals from VOC </li>
<li>replace the 1000-way classification layer with a randomly initialized 21-way layer (20 VOC classes plus background)</li>
<li><strong>class label: all region proposals with ≥ 0.5 IoU overlap with a ground-truth box as positives, else negatives </strong></li>
<li>1/10th of the initial pre-training rate</li>
<li>uniformly sample 32 positive windows (over all classes) and 96 background windows to construct a mini-batch of size 128 </li>
</ul>
</li>
<li><p>Object category classifiers:</p>
<ul>
<li>considering a binary classifier for a specific class</li>
<li><strong>class label: take IoU overlap threshold &lt;0.3 as negatives, take only regions tightly enclosing the object as positives </strong> </li>
<li>take the ground-truth bounding boxes for each class as positives</li>
</ul>
</li>
<li><p><strong>unexplained:</strong></p>
<ul>
<li><p>the positive and negative examples are defined differently in CNN fine-tuning versus SVM training</p>
<p>  CNN容易过拟合，需要大量的训练数据，所以在CNN训练阶段我们对Bounding box的位置限制条件限制的比较松(IOU只要大于0.5都被标注为正样本)，svm适用于少样本训练，所以对于训练样本数据的IOU要求比较严格，我们只有当bounding box把整个物体都包含进去了，我们才把它标注为物体类别。</p>
</li>
<li><p>it’s necessary to train detection classifiers rather than simply use outputs of the fine-tuned CNN</p>
<p>  上一个回答其实同时也解释了CNN的head已经是一个分类器了，还要用SVM分类：按照上述正负样本定义，CNN softmax的输出比采用svm精度低。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>分析</p>
<ul>
<li><p>learned features:</p>
<ul>
<li>compute the units’ activations on a large set of held-out region proposals  </li>
<li>sort from the highest to low</li>
<li>perform non-maximum suppression</li>
<li><p>display the top-scoring regions</p>
<p><img src="/2020/01/08/RCNN系列/activations.png" width="70%;"></p>
</li>
</ul>
</li>
<li><p>Ablation studies:</p>
<ul>
<li><strong>without fine-tuning:</strong> features from fc7 generalize worse than features from fc6, indicating that most of the CNN’s representational power comes from its convolutional layers</li>
<li><strong>with fine-tuning: </strong>The boost from fine-tuning is much larger for fc6 and fc7 than for pool5, suggests that pool features learned from ImageNet are <strong>general</strong> and that most of the improvement is gained from learning <strong>domain-specific</strong> non-linear classifiers on top of them</li>
</ul>
</li>
<li><p>Detection error analysis:</p>
<ul>
<li>more of our errors result from poor localization rather than confusion </li>
<li>CNN features are much more discriminative than HOG </li>
<li>Loose localization likely results from our use of bottom-up region proposals and the positional invariance learned from pre-training the CNN for whole-image classification(粗暴的IOU判定前背景，二值化label，无法体现定位好坏差异)</li>
</ul>
</li>
<li><p>Bounding box regression：</p>
<ul>
<li>a linear regression model use the pool5 features for a selective search region proposal as input</li>
<li>输出为xy方向的缩放和平移</li>
<li>训练样本：判定为本类的候选框中和真值重叠面积大于0.6的候选框</li>
</ul>
</li>
<li><p>Semantic segmentation：</p>
<ul>
<li>three strategies for computing features:<ul>
<li>‘<em>full</em> ‘ ignores the region’s shape, two regions with different shape might have very similar bounding boxes(信息不充分)</li>
<li>‘<em>fg</em> ‘ slightly outperforms <em>full</em>, indicating that the masked region shape provides a stronger signal</li>
<li>‘<em>full+fg</em> ‘ achieves the best, indicating that the context provided by the <em>full</em> features is highly informative even given the <em>fg</em> features(形状和context信息都重要)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="SPP-net-Spatial-Pyramid-Pooling-in-Deep-Convolutional-Networks-for-Visual-Recognition"><a href="#SPP-net-Spatial-Pyramid-Pooling-in-Deep-Convolutional-Networks-for-Visual-Recognition" class="headerlink" title="SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition"></a>SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</h2><ol>
<li><p>动机：</p>
<ul>
<li>propose a new pooling strategy, “spatial pyramid pooling”</li>
<li>can generate a fixed-length representation regardless of image size/scale</li>
<li>also robust to object deformations </li>
</ul>
</li>
<li><p>论点：</p>
<ul>
<li>existing CNNs require a fixed-size input<ul>
<li>reduce accuracy for sub-images of an arbitrary size/scale (need cropping/warping)</li>
<li>cropped region lost content, while warped content generates unwanted distortion</li>
<li>overlooks the issues involving scales </li>
</ul>
</li>
<li>convolutional layers do not require a fixed image size, whle the fully-connected layers need to have fixed- size/length input by their definition</li>
<li>by introducing the SPP layer<ul>
<li>between the last convolutional layer and the first fully-connected layer</li>
<li>pools the features and generates fixed- length outputs</li>
</ul>
</li>
<li>Spatial pyramid pooling <ul>
<li>partitions the image into divisions from finer to coarser levels, and aggregates local features in them</li>
<li>generates fixed- length output </li>
<li>uses multi-level spatial bins(robust to object deformations )</li>
<li>can run at variable scales </li>
<li>also allows varying sizes or scales <strong>during training</strong>: <ul>
<li>train the network with different input size at different epoch</li>
<li>increases scale-invariance </li>
<li>reduces over-fitting </li>
</ul>
</li>
<li>in object detection <ul>
<li>run the convolutional layers only <em>once</em> on the entire image </li>
<li>then extract features by SPP-net on the feature maps </li>
<li>speedup </li>
<li>accuracy </li>
</ul>
</li>
</ul>
</li>
</ul>
<ol>
<li><p>方法：</p>
<ul>
<li><p>Convolutional Layers and Feature Maps</p>
<ul>
<li>the outputs of the convolutional layers are known as feature maps</li>
<li><p>feature maps involve not only the strength of the responses(the strength of activation), but also their spatial positions(the reception field)</p>
<p><img src="/2020/01/08/RCNN系列/feature maps.png" width="80%;"></p>
</li>
</ul>
</li>
<li><p>The Spatial Pyramid Pooling Layer</p>
<ul>
<li>it can maintain spatial information by pooling in local spatial bins</li>
<li>the spatial bins have sizes proportional to the image size(k-level: 1*1, 2*2, …, k*k)</li>
<li>we can resize the input image to any scale, which is important for the accuracy  </li>
<li><p>the coarsest pyramid level has a single bin that covers the entire image, which is in fact a “global pooling” operation </p>
<p><img src="/2020/01/08/RCNN系列/spp.png" width="40%;"></p>
</li>
<li><p>for a feature map of $a×a$, with a pyramid level of $n×n$ bins:</p>
<script type="math/tex; mode=display">
  the\ window\ size:\ win = ceiling(a/n)\\
  the\ stride:\ str = floor(a/n)</script></li>
</ul>
</li>
</ul>
</li>
</ol>
<ul>
<li><p>Training the Network</p>
<ul>
<li>Single-size training: fixed-size input (224×224) cropped from images, cropping for data augmentation<ul>
<li>Multi-size training: rather than cropping, we resize the aforementioned 224×224 region to 180×180, then we train two fixed-size networks that share parameters by altenate epoch</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>分析</p>
<ul>
<li><strong>50 bins vs. 30 bins: </strong>the gain of multi-level pooling is not simply due to more parameters, it is because the multi-level pooling is robust to the variance in object deformations and spatial layout<ul>
<li><strong>multi-size vs. single-size:  </strong>multi results are more or less better than the single-size version</li>
<li><strong>full vs. crop: </strong>shows the importance of maintaining the complete content</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>SPP-NET FOR OBJECT DETECTION</strong></p>
<ul>
<li><p>We extract the feature maps from the entire image only once </p>
</li>
<li><p>we apply the spatial pyramid pooling on each candidate window of the feature maps  </p>
</li>
<li><p>These representations are provided to the fully-connected layers of the network </p>
</li>
<li><p>SVM samples: We use the ground-truth windows to generate the positive samples, use the samples with IOU&lt;30% as the negative samples </p>
</li>
<li><p>multi-scale feature extraction: </p>
<ul>
<li>We resize the image at {480, 576, 688, 864, 1200}, and compute the feature maps of conv5 for each scale. <ul>
<li>we choose a single scale s ∈ S such that the scaled candidate window has a number of pixels closest to 224×224. </li>
<li>And we use the corresponding feature map to compute the feature for this window</li>
<li>this is roughly equivalent to resizing the window to 224×224 </li>
</ul>
</li>
</ul>
</li>
<li><p>fine-tuning:</p>
<ul>
<li>Since our features are pooled from the conv5 feature maps from windows of any sizes<ul>
<li>for simplicity we only fine-tune the fully-connected layers </li>
</ul>
</li>
</ul>
</li>
<li><p>Mapping a Window to Feature Maps**</p>
<ul>
<li><p>we project the corner point of a window onto a pixel in the feature maps, such that this corner point in the image domain is closest to the center of the receptive field of that feature map pixel.  </p>
<p><img src="/2020/01/08/RCNN系列/mapping.png" width="50%;"></p>
<p>​    确定原图上的两个角点（左上角和右下角），映射到 feature map上的两个对应点，使得映射点$(x^{‘}, y^{‘})$在原始图上<strong>感受野（上图绿色框）的中心点</strong>与$(x,y)$尽可能接近。</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Fast-R-CNN-Fast-Region-based-Convolutional-Network"><a href="#Fast-R-CNN-Fast-Region-based-Convolutional-Network" class="headerlink" title="Fast R-CNN: Fast Region-based Convolutional Network"></a>Fast R-CNN: Fast Region-based Convolutional Network</h2><ol>
<li><p>动机</p>
<ul>
<li>improve training and testing speed</li>
<li>increase detection accuracy</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>current approaches train models in multi-stage pipelines that are slow and inelegant<ul>
<li>R-CNN &amp; SPPnet: CNN+SVM+bounding-box regression</li>
<li>disk storage: features are written to disk </li>
<li>SPPnet: can only fine-tuning the fc layers, limits the accuracy of very deep networks</li>
</ul>
</li>
<li>task complexity:<ul>
<li>numerous candidate proposals</li>
<li>rough localization proposals must be refined </li>
</ul>
</li>
<li>We propose:<ul>
<li>a single-stage training algorithm </li>
<li>multi-task: jointly learns to classify object proposals and refine their spatial locations </li>
</ul>
</li>
</ul>
</li>
<li><p>要素</p>
<ul>
<li>input: an entire image and a set of object proposals </li>
<li>convs</li>
<li>a region of interest (RoI) pooling layer: extracts a fixed-length feature vector from the feature map</li>
<li>fcs that finally branch into two sibling output layers</li>
<li><p>multi-outputs:</p>
<ul>
<li>one produces softmax probability over K+1 classes</li>
<li>one outputs four bounding-box regression offsets per class</li>
</ul>
<p><img src="/2020/01/08/RCNN系列/fastRCNN.png" width="40%;"></p>
</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>RoI pooling</p>
<ul>
<li>an RoI is a rectangular window inside a conv feature map, which can be defined by (r, c, h, w)  </li>
<li>the RoI pooling layer converts the features inside any valid RoI into a small feature map with a fixed size H × W </li>
<li>it is a special case of SPPnet when there is only one pyramid level (pooling window size = h/H * w/W)</li>
</ul>
</li>
<li><p>Initializing from pre-trained networks</p>
<ul>
<li>the last max pooling layer is replaced by a RoI pooling layer </li>
<li>the last fully connected layer and softmax is replaced by the wo sibling layers + respective head (softmax &amp; regressor)</li>
<li>modified to take two inputs</li>
</ul>
</li>
<li><p>Fine-tuning for detection</p>
<ul>
<li><p>why SPPnet is unable to update weights below the spatial pyramid pooling layer: </p>
<ul>
<li><p>原文提到feature vector来源于不同尺寸的图像——不是主要原因</p>
</li>
<li><p>feature vector在原图上的感受野通常很大（接近全图）——forward pass的计算量就很大</p>
</li>
<li><p>不同的图片forward pass的计算结果不能复用（when each training sample (<em>i.e</em>. RoI) comes from a different image, which is exactly how R-CNN and SPPnet networks are trained）</p>
</li>
</ul>
</li>
<li><p>We propose:</p>
<ul>
<li><p>takes advantage of feature sharing </p>
</li>
<li><p>mini-batches are sampled hierarchically: N images and R/N RoIs from each image</p>
</li>
<li><p>RoIs from the same image share computation and memory in the forward and backward passes </p>
</li>
<li><p>jointly optimize the two tasks</p>
<p>  each RoI is labeled with a ground-truth class $u$ and a ground-truth bounding-box regression target $v$ </p>
<p>  the network outputs are K+1 probability $p=(p_0,…p_k)$ and K b-box regression offsets $t^k=(t_x^k, t_y^k, t_w^k,t_h^k)$</p>
<script type="math/tex; mode=display">
  L(p, u, t^u, v) = L_{cls}(p,u) + \lambda[u>0]L_{loc}(t^u,v)\\</script><p>  $L_{cls}$:</p>
<script type="math/tex; mode=display">
  L_{cls}(p,u) = -log p_u\\</script><p>  $L_{loc}$:</p>
<script type="math/tex; mode=display">
  L_{loc}(t^u, v) = \sum_{i \in \{x,y,w,h\}}smooth_{L_1}(t^u_i - v_i)\\
  smooth_{L_1}(x) = 
  \begin{cases}
  0.5x^2\ \ \ \ \ \ \ \ \ \ \ if |x|<1\\
  |x| - 0.5\ \ \ \ \ \ otherwise
  \end{cases}</script><p>  作者表示这种形式可以增强模型对异常数据的鲁棒性</p>
<p>  <img src="/2020/01/08/RCNN系列/smoothL1.png" width="30%;"></p>
</li>
<li><p>class label: take $IoU\geq0.5$ as a foreground object, take negatives with $IoU \in [0.1,0.5)$</p>
<p>  The lower threshold of 0.1 appears to act as a heuristic for hard example mining </p>
</li>
<li></li>
</ul>
</li>
</ul>
</li>
<li><p>Truncated SVD for faster detection</p>
<ul>
<li>Large fully connected layers are easily accelerated by compressing them with truncated SVD <script type="math/tex; mode=display">
  W \approx U \Sigma_t V^T</script></li>
</ul>
</li>
</ul>
</li>
</ol>
<ul>
<li><p>the single fully connected layer corresponding to W is replaced by two fully connected layers, <strong>without non-linearity</strong></p>
</li>
<li><p>The first layers uses the weight matrix $\Sigma_t V^T$(and no biases)</p>
</li>
<li><p>the second uses U (with the original biases)</p>
</li>
</ul>
<ol>
<li><p>分析</p>
<ul>
<li>Fast R-CNN vs. SPPnet: even though Fast R-CNN uses single-scale training and testing, <strong>fine-tuning the conv layers</strong> provides a large improvement in mAP </li>
<li>Truncated SVD can reduce detection time by more than 30% with only a small (0.3 percent- age point) drop in mAP </li>
<li>deep vs. small networks: <ul>
<li>for very deep networks fine-tuning the conv layers is important </li>
<li>in the smaller networks (S and M) we find that conv1 is generic and task independent </li>
<li>all Fast R-CNN results in this paper using models L fine-tune layers conv3_1 and up</li>
<li>all experiments with models S and M fine-tune layers conv2 and up </li>
</ul>
</li>
<li>multi-task training vs. stage-wise: it has the potential to improve results because the tasks influence each other through a shared representation (the ConvNet) </li>
<li>single-scale vs. multi-scale: <ul>
<li>single-scale detection performs almost as well as multi-scale detection </li>
<li>deep ConvNets are adept at directly learning scale invariance </li>
<li>single-scale processing offers the best tradeoff be- tween speed and accuracy thus we choose single-scale</li>
</ul>
</li>
<li>softmax vs. SVM:<ul>
<li>“one-shot” fine-tuning is sufficient compared to previous multi-stage training approaches</li>
<li>softmax introduces competition, while SVMs are one-vs-rest </li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks"><a href="#Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks" class="headerlink" title="Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"></a>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</h2><ol>
<li><p>动机</p>
<ul>
<li>shares the convolutional features</li>
<li>merge the system using the concept of “attention” mechanisms </li>
<li>sharing convolutions across proposals —-&gt; across tasks</li>
<li>translation-Invariant &amp; scale/ratio-Invariant</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>proposals are now the test-time computational bottleneck in state-of-the-art detection systems</li>
<li>the region proposal methods are generally implemented on the CPU</li>
<li>we observe that the convolutional feature maps used by region-based detectors, like Fast R- CNN, can also be used for generating region proposals</li>
</ul>
</li>
<li><p>要素</p>
<ul>
<li>RPN: On top of the convolutional features, we construct an RPN by adding a few additional convolutional layers that simultaneously regress region bounds and objectness scores at each location <strong>on a regular grid</strong></li>
<li>anchor: serves as references at multiple scales and aspect ratios </li>
<li><p>unify RPN and Fast R-CNN detector: we propose a training scheme that alternately fine-tuning the region proposal task and the object detection task</p>
<p><img src="/2020/01/08/RCNN系列/fasterRCNN.png" width="35%;"></p>
</li>
</ul>
</li>
<li><p>方法</p>
<p> 4.1 Region Proposal Networks</p>
<p> <img src="/2020/01/08/RCNN系列/RPN.png" width="65%;"></p>
<ul>
<li><p>This architecture is naturally implemented with an n×n convolutional layer followed by two sibling 1 × 1 convolutional layers (for <em>reg</em> and <em>cls</em>, respectively)</p>
</li>
<li><p>conv: an n × n sliding window </p>
</li>
<li><p>feature: 256-d for ZF(5 convs backbone) and 512-d for VGG(13 convs backbone)</p>
</li>
<li><p>two sibling fully-connected layers + respective output layer</p>
</li>
<li><p>anchors</p>
<ul>
<li>predict multiple region proposals: denoted as k</li>
<li>the reg head has 4k outputs, the cls head has 2k outputs</li>
<li>the k proposals are parameterized relative to <strong>k reference boxes</strong>————the anchors</li>
<li>an anchor box is centered at the sliding window in question, and is associated with a scale and aspect ratio </li>
<li>for a convolutional feature map of a size W × H , that is WHk anchors in total</li>
</ul>
</li>
<li><p>class label</p>
<ul>
<li>positives1: the anchors with the highest IoU with a ground-truth box </li>
<li>positives2: the anchors that has an IoU higher than 0.7 with any ground-truth box </li>
<li>negatives: non-positive anchors if their IoU is lower than 0.3 <strong>for all</strong> ground-truth boxes</li>
<li>the left: do not contribute </li>
<li>ignored: all cross-boundary anchors </li>
</ul>
</li>
<li><p>Loss function</p>
<ul>
<li><p>similar multi-task loss as fast-RCNN, with a normalization term</p>
</li>
<li><p>with $x,y,w,h$ denoting the box’s <strong>center coordinates</strong> and its width and height, the regression branch outputs $t_i$:</p>
<script type="math/tex; mode=display">
  t_x = (x - x_a) / w_a\\
  t_y = (y - y_a) / h_a\\
  t_w = log(w/ w_a)\\
  t_h = log(h/ h_a)</script></li>
</ul>
</li>
<li><p>mini-batch: sampled the positive and negative anchors from a single image with the ratio of 1:1 </p>
<p>4.2 the unified network </p>
</li>
<li><p>Alternating training</p>
<ul>
<li>ImageNet-pre-trained model, fine-tuning end-to-end for the region proposal task </li>
<li>ImageNet-pre-trained model, using the RPN proposals, fine-tuning end-to-end for the detection task </li>
<li>fixed detection network convs, fine-tuning the unique layers for region proposal </li>
<li>fixed detection network convs, fine-tuning the unique layers for detection</li>
</ul>
</li>
<li><p>Approximate joint training</p>
<ul>
<li>multi-task loss</li>
<li>approximate </li>
</ul>
<p>4.3 at training time</p>
</li>
<li><p>the total stride is 16 (input size / feature map size)</p>
</li>
<li>for a typical 1000 × 600 image, there will be roughly 20000 (60*40*9) anchors in total </li>
<li><p>we ignore all cross-boundary anchors, there will be about 6000 anchors per image left for training</p>
<p>4.4 at testing time</p>
</li>
<li><p>we use NMS(iou_thresh=0.7), that leaves 2000 proposals per image</p>
</li>
<li>then we use the top-N ranked proposal regions for detection</li>
</ul>
</li>
<li><p>分析</p>
</li>
</ol>
<h2 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h2><ol>
<li><p>动机</p>
<ul>
<li><strong>instance segmentation: </strong><ul>
<li>detects objects while simultaneously generating instance mask</li>
<li>注意不仅仅是目标检测了</li>
</ul>
</li>
<li>easy to generalize to other tasks: <ul>
<li>instance segmentation</li>
<li>bounding-box object detection</li>
<li>person keypoint detection</li>
</ul>
</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>challenging:<ul>
<li>requires the correct detection of objects</li>
<li>requires precisely segmentation of instances</li>
</ul>
</li>
<li><strong>a simple, flexible, and fast system can surpass all</strong><ul>
<li>adding a branch for predicting segmentation on Faster-RCNN</li>
<li>in parallel with the existing branch for classification and regression </li>
<li>the mask branch is a small FCN applied to each RoI</li>
</ul>
</li>
<li>Faster R- CNN was not designed for pixel-to-pixel alignment  </li>
<li><strong>we propose RoIAlign to preserve exact spatial locations</strong></li>
<li>FCNs usually perform per-pixel multi-class categorization, which couples segmentation and classification</li>
<li><p><strong>we predict a binary mask for each class independently, decouple mask(mask branch) and class(cls branch)</strong></p>
</li>
<li><p>other combining methods are multi-stage</p>
</li>
<li><strong>our method is based on parallel prediction</strong>  </li>
<li>FCIS also run the system in parallel but exhibits systematic errors on overlapping instances and creates spurious edges </li>
<li>segmentation-first strategies attempt to cut the pixels of the same category into different instances </li>
<li><strong>Mask R-CNN is based on an instance-first strategy</strong></li>
</ul>
</li>
<li><p>要素</p>
<ul>
<li>a mask branch with $Km^2$-dims outputs for each RoI, m denotes the resolution, K denotes the number of classes</li>
<li>bce is key for good instance segmentation results:  $L_{mask} = [y&gt;0]\frac{1}{m^2}\sum bce_loss$</li>
<li>RoI features that are well aligned to the per-pixel input </li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>RoIAlign</p>
<ul>
<li>Quantizations in RoIPool: (1) RoI to feature map $[x/16]$; (2) feature map to spatial bins $[a/b]$; $[]$ denotes roundings</li>
<li>These quantizations introduce misalignments </li>
<li><p>We use bilinear interpolation to avoid quantization</p>
<ul>
<li>sample several points in the spatial bins</li>
<li>computes the value of each sampling point by bilinear interpolation from the nearby grid points on the feature map</li>
<li>aggregate the results of sampling points (using max or average) </li>
</ul>
<p><img src="/2020/01/08/RCNN系列/RoIAlign.png" width="50%;"></p>
</li>
</ul>
</li>
<li><p>Architecture </p>
<ul>
<li>backbone: using a ResNet-FPN backbone for feature extraction gives excellent gains in both accuracy and speed </li>
<li><p>head: use previous heads in ResNet/FPN(res5 contained in head/backbone)</p>
<p><img src="/2020/01/08/RCNN系列/mrcnnheads.png" width="45%;"></p>
</li>
</ul>
</li>
<li><p>Implementation Details </p>
<ul>
<li>positives: RoIs with IoU at least 0.5, otherwise negative</li>
<li>loss: dice loss defined only on positive RoIs</li>
<li>mini-batch: 2 images, N RoIs</li>
<li>at training time: parallel computation  for 3 branches</li>
<li>at test time: <ul>
<li>serial computation</li>
<li>proposals -&gt; box prediction -&gt; NMS -&gt; run mask branch on the highest scoring 100 detection boxes  </li>
<li>it speeds up inference and improves accuracy </li>
<li>the $28*28$ floating-number mask output is resized to the RoI size, and binarized at a threshold of 0.5</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>分析</p>
<ul>
<li>on overlapping instances: FCIS+++ exhibits systematic artifacts </li>
<li>architecture: it benefits from deeper networks (50 vs. 101) and advanced designs including FPN and ResNeXt</li>
<li>FCN vs. MLP for mask branch</li>
<li><strong>Human Pose Estimation</strong><ul>
<li>We model a keypoint’s location as a <strong>one-hot mask</strong>, and adopt Mask R-CNN to predict K masks, one for each of K keypoint types </li>
<li>the training target is a one-hot $m<em>m$ binary mask where only a </em>single* pixel is labeled as foreground </li>
<li>use the cross-entropy loss </li>
<li>We found that a relatively high resolution output ($56*56$ compared to masks) is required for keypoint-level localization accuracy</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="FPN-Feature-Pyramid-Networks-for-Object-Detection"><a href="#FPN-Feature-Pyramid-Networks-for-Object-Detection" class="headerlink" title="FPN: Feature Pyramid Networks for Object Detection"></a>FPN: Feature Pyramid Networks for Object Detection</h2><ol>
<li><p>动机</p>
<ul>
<li>for object detection in multi-scale</li>
<li>struct feature pyramids with marginal extra cost</li>
<li>practical and accurate</li>
<li>leverage the pyramidal shape of a ConvNet’s feature hierarchy while creating a feature pyramid that has strong semantics at all scales</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>single scale offers a good trade-off between accuracy and speed while multi-scale still performs better, especially for small objects</li>
<li>featurized image pyramids form the basis solution for multi-scale</li>
<li>ConvNets are proved robust to variance in scale and thus facilitate recognition from features computed on a single input scale </li>
<li>SSD uses the naturely feature hierarchy generated by ConvNet which introduces large semantic gaps caused by different depths<ul>
<li>high-level features are low-resolution but <strong>semantically strong</strong></li>
<li>low-level features are of lower-level semantics, but their activations are more <strong>accurately localized</strong> as subsampled fewer times</li>
</ul>
</li>
<li><p>thus we propose FPN:</p>
<ul>
<li>combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections </li>
<li>has rich semantics at all levels </li>
<li>built from a single scale</li>
<li>can be easily extended to mask proposals</li>
<li>can be trained end-to- end with all scales </li>
</ul>
<p><img src="/2020/01/08/RCNN系列/pyramids.png" width="40%;"></p>
</li>
<li><p>similar architectures make predictions only on a fine resolution  </p>
<p><img src="/2020/01/08/RCNN系列/similar structs.png" width="40%;"></p>
</li>
</ul>
</li>
<li><p>要素</p>
<ul>
<li>takes a single-scale image of an arbitrary size as input </li>
<li>outputs proportionally sized feature maps at multiple levels</li>
<li>structure <ul>
<li>a bottom-up pathway:  the feed-forward computation of the backbone ConvNet</li>
<li>a top-down pathway and lateral connection: <ul>
<li>upsampling the spatially coarser, but semantically stronger, feature maps from higher pyramid levels</li>
<li>then enhance with features from the bottom-up pathway via lateral connections</li>
<li>a $3<em>3$ conv is appended on each merged map <em>*to reduce the aliasing effect of upsampling</em></em></li>
<li>shared classifiers/regressors among all levels, thus using fixed 256 channels convs</li>
<li>upsamling uses nearest neighbor interpolation</li>
<li>low-level features undergoes a $1*1$ conv to reduce channel dimensions </li>
<li>merge operation is a <strong>by element-wise addition</strong></li>
</ul>
</li>
</ul>
</li>
<li>adopt the method in RPN &amp; Fast-RCNN for demonstration</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>RPN</p>
<ul>
<li>original design: <ul>
<li>backbone Convs -&gt; single-scale feature map -&gt; dense 3×3 sliding windows -&gt; head($3<em>3$ convs + 2 sibling $1</em>1$ conv branches)</li>
<li>for regressor: multi-scale anchors(e.g. 3 scales 3 ratios -&gt; 9 anchors)</li>
</ul>
</li>
<li>new design: <ul>
<li>adapt FPN -&gt; multi-scale feature map -&gt; sharing heads</li>
<li>for regressor: set single-scale anchor for each level respectively (e.g. 5 level 3 ratios -&gt; 15 anchors)</li>
</ul>
</li>
<li>sharing heads:<ul>
<li>vs. not sharing: similar accuracy </li>
<li>indicates all levels of FPN share similar semantic levels (contrasted with naturally feature hierarchy of CNNs)</li>
</ul>
</li>
</ul>
</li>
<li><p>Fast  R-CNN</p>
<ul>
<li><p>original design: take the ROI feature map from the output of last conv layer</p>
</li>
<li><p>new design: take the specific level of ROI feature map based on ROI area</p>
<ul>
<li><p>with a $w*h$ ROI on the input image, $k_0$ refers to the target level on which an RoI with $w×h=224^2$ should be mapped into </p>
<script type="math/tex; mode=display">
k = [k_0 + log_2 (\sqrt{wh}/224)]</script></li>
<li><p>the smaller the ROI area, the lower the level k, the finer the resolution of the feature map</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>分析</p>
<ul>
<li>RPN<ul>
<li>use or not FPN: boost on small objects</li>
<li>use or not top-down pathway: semantic gaps</li>
<li>use or not lateral connection: locations </li>
<li>use or not multi-levels feature maps: <ul>
<li>using P2 alone leads to more anchors </li>
<li>more anchors are not sufficient to improve accuracy</li>
</ul>
</li>
</ul>
</li>
<li>Fast  R-CNN<ul>
<li>using P2 alone is <strong>marginally worse</strong> than that of using all pyramid levels </li>
<li>we argue that this is because <strong>RoI pooling is a warping-like operation</strong>, which is less sensitive to the region’s scales</li>
</ul>
</li>
<li><p>Faster R-CNN</p>
<ul>
<li>sharing features improves accuracy by a small margin</li>
<li>but reduces the testing time</li>
</ul>
</li>
<li><p><strong>Segmentation Proposals</strong></p>
<ul>
<li>use a fully convolutional setup for both training and inference </li>
<li><p>apply a small 5×5 MLP to predict 14×14 masks </p>
<p><img src="/2020/01/08/RCNN系列/FPNseg.png" width="40%;"></p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="衍生应用："><a href="#衍生应用：" class="headerlink" title="衍生应用："></a>衍生应用：<lung nodules="" detection="" and="" segmentation="" using="" 3d="" mask-rcnn=""></lung></h2><ol>
<li>动机<ul>
<li>3D volume detection and segmentation </li>
<li>ROI ／ full scan</li>
<li>LUNA16：lung nodules size evaluation </li>
</ul>
</li>
<li>论点<ul>
<li>variety among nodules &amp; similarity among non-nodules</li>
</ul>
</li>
<li>方法<ul>
<li>use overlapping sliding windows </li>
<li>use focal loss improve class result</li>
<li>use IOU loss improve mask result</li>
<li>use heavy augmentation</li>
</ul>
</li>
</ol>
<h2 id="Cascade-R-CNN-Delving-into-High-Quality-Object-Detection"><a href="#Cascade-R-CNN-Delving-into-High-Quality-Object-Detection" class="headerlink" title="Cascade R-CNN: Delving into High Quality Object Detection"></a>Cascade R-CNN: Delving into High Quality Object Detection</h2><ol>
<li><p>动机</p>
<ul>
<li>an detector trained with low IoU threshold usually produces noisy detections：低质量框issue</li>
<li>但是又不能简单地提高IoU threshold<ul>
<li>正样本会急剧减少，导致过拟合</li>
<li>inference-time mismatch，训练阶段只有高质量框，但是测试阶段啥质量框都有</li>
</ul>
</li>
<li>we propose Cascade R-CNN<ul>
<li>multi-stage object detection architecture</li>
<li>consists of a sequence of detectors trained with increasing IoU thresholds</li>
<li>trained stage by stage</li>
</ul>
</li>
<li>surpass all single-model on COCO</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>object detections two main tasks<ul>
<li>recognition problem：foreground/backgroud &amp; object class</li>
<li>localization problem：bounding box</li>
<li>loose requirement for positives<ul>
<li>an low IoU thresh(0.5) is required to define positives/negatives：looss</li>
<li>noisy bounding boxes：close false positives</li>
</ul>
</li>
</ul>
</li>
<li>quality<ul>
<li>将一个框和gt的IoU定义为它的quality</li>
<li>将一个detector训练用的IoU thresh定义为它的quality</li>
<li>detector的quality和input proposals的quality是相关的：a single detector work on a specific quality level of hypotheses</li>
</ul>
</li>
<li>Cascade R-CNN<ul>
<li>multi-stage extension of R-CNN</li>
<li>sequentially more selective against close false positives</li>
</ul>
</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>formulation</p>
<p> <img src="/2020/01/08/RCNN系列/formulation.png" width="80%;"></p>
<ul>
<li>first stage：<ul>
<li>proposal network H0</li>
<li>applied to entire image</li>
</ul>
</li>
<li>second stage<ul>
<li>region-of-interest detection sub-network H1 (detection head)</li>
<li>run on proposals</li>
</ul>
</li>
<li>C和B是classification score &amp; bounding box regression</li>
<li>we focus on modeling the second stage</li>
</ul>
</li>
<li><p>bounding box regression 针对回归质量</p>
<ul>
<li>an image patch $x$</li>
<li>a bounding box $b = (b_x, b_y, b_w, b_h)$</li>
<li>use a regressor $f(x,b)$ to fit the target $g$<ul>
<li>use L1 loss $L_{loc}(f(x_i,b_i), g_i)$</li>
<li>compute on 相对量 &amp; std normalization</li>
<li>invariant to scale and location</li>
<li>results in minor adjustments on $b$：所以regression loss通常比cls loss小得多</li>
</ul>
</li>
<li>Iterative BBox<ul>
<li>a single regression step is not sufficient for accurate localization</li>
<li>所以就搞了N个一样的regression heads串联</li>
<li>但还是那个问题：一个regressor只针对某一个quality level的proposals是performance optimal的，但是每个iteration以后框的distribution是剧烈变化的</li>
<li>所以迭代两次以上基本没有gain了</li>
</ul>
</li>
</ul>
</li>
<li><p>detection quality 针对分类质量</p>
<ul>
<li>an image patch x</li>
<li>M foreground classes and 1 background</li>
<li>use a classifier $h(x)$ to learn the target class label among M+1<ul>
<li>use CE $L_{cls}(h(x_i),y_i)$</li>
<li>the class label is determined by IoU thresh：如果image patch和gt box的IoU大于阈值，那么这个image patch的class label就是gt box的label，否则是背景</li>
<li>the IoU thresh defines the quality of a detector</li>
</ul>
</li>
<li>challenging<ul>
<li>如果阈值调高了，positives里面包含更少的背景（高质量前景），但是样本量少</li>
<li>如果阈值低了，前景样本多了，但是内容更加diversified，更难reject close false positives</li>
<li>所以一个分类器在不同的IoU阈值下，要面临不同的问题，在inference阶段，it is very difficult to perform uniformly well over all IoU levels</li>
</ul>
</li>
<li>Integral loss<ul>
<li>训练好几个分类器，针对不同的IoU level，然后inference阶段ensemble</li>
<li>还是没有解决高IoU阈值的那个分类器会因为样本量少过拟合的问题，而且高质量分类器在infernce阶段还是要处理所有的低质量框</li>
</ul>
</li>
</ul>
</li>
<li><p>Cascade R-CNN</p>
<ul>
<li>Cascaded Bounding Box Regression<ul>
<li>cascade specialized regressors</li>
<li>differs from Iterative BBox<ul>
<li>Iterative BBox是个后处理手段，一个regressor在0.5level的boxes上面优化，然后在inference proposals上面反复迭代</li>
<li>Cascade R-CNN是个resampling method，多个不同的regressor级连，训练测试同操作同分布</li>
</ul>
</li>
</ul>
</li>
<li><p>Cascaded Detection</p>
<ul>
<li>resamping manner<ul>
<li>keep上一阶段的positives</li>
<li>同时丢掉一些outliers</li>
</ul>
</li>
<li>实现就是每个stage的IoU threshold逐渐提高</li>
<li><p>loss还是所有proposals的cls loss + 定义为前景proposals的reg loss</p>
</li>
<li><p>outliers</p>
<p> <img src="/2020/01/08/RCNN系列/outlier.png" width="40%;"></p>
</li>
<li><p>proposal quality</p>
<p> <img src="/2020/01/08/RCNN系列/quality.png" width="50%;"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://amberzzzz.github.io/2020/01/03/CNN-Visualization系列/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="amber.zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Less is More">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/03/CNN-Visualization系列/" itemprop="url">CNN Visualization系列</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-03T17:03:05+08:00">
                2020-01-03
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/01/03/CNN-Visualization系列/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2020/01/03/CNN-Visualization系列/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1-Visualizing-and-Understanding-Convolutional-Networks"><a href="#1-Visualizing-and-Understanding-Convolutional-Networks" class="headerlink" title="1. Visualizing and Understanding Convolutional Networks"></a>1. Visualizing and Understanding Convolutional Networks</h3><ol>
<li><p>动机</p>
<ul>
<li>give insight into the internal operation and behavior of the complex models</li>
<li>then one can design better models</li>
<li>reveal which parts of the scene in image are important for classification</li>
<li>explore the generalization ability of the model to other datasets </li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li><p>most visualizing methods limited to the 1st layer where projections to pixel space are possible </p>
</li>
<li><p>Our approach propose a method that could projects high level feature maps to the pixel space</p>
</li>
</ul>
</li>
</ol>
<pre><code>* some methods give some insight into invariances basing on a simple quadratic approximation 
* Our approach, by contrast, provides a non-parametric view of invariance 



* some methods associate patches that responsible for strong activations at higher layers
* In our approach they are not just crops of input images, but rather top-down projections that reveal structures  
</code></pre><ol>
<li><p>方法</p>
<p> 3.1 Deconvnet: use deconvnet to project the feature activations back to the input pixel space </p>
<ul>
<li>To examine a given convnet activation, we <strong>set all other activations in the layer to zero</strong> and pass the feature maps as input to the attached deconvnet layer</li>
<li>Then successively (i) unpool, (ii) rectify and (iii) filter to reconstruct the activity of the layer beneath until the input pixel space is reached</li>
<li>【Unpooling】using switches</li>
<li>【Rectification】the convnet uses relu to ensure always positive, same for back projection</li>
<li>【Filtering】transposed conv</li>
<li><p>Due to unpooling, the reconstruction obtained from a single activation resembles a small piece of the original input image</p>
<p><img src="/2020/01/03/CNN-Visualization系列/deconv.png" width="55%;"></p>
<p>3.2 CNN model</p>
<p><img src="/2020/01/03/CNN-Visualization系列/CNN.png" width="70%;"></p>
<p>3.3 visualization among layers</p>
</li>
<li><p>for each layer, we take the top9 strongest activation across the validation data</p>
</li>
<li>calculate the back projection separately</li>
<li><p>alongside we provide the corresponding image patches</p>
<p>3.4 visualization during training</p>
</li>
<li><p>randomly choose several strongest activation of a given feature map</p>
</li>
<li><p>lower layers converge fast, higher layers conversely</p>
<p>3.5 visualizing the Feature Invariance</p>
</li>
<li><p>5 sample images being translated, rotated and scaled by varying degrees</p>
</li>
<li>Small transformations have a dramatic effect in the first layer of the model(c2 &amp; c3对比)</li>
<li><p>the network is stable to translations and scalings, but not invariant to rotation </p>
<p>3.6 architecture selection</p>
</li>
<li><p>old architecture(stride4, filterSize11)：The first layer filters are a mix of extremely high and low frequency information, with little coverage of the mid frequencies.  The 2nd layer visualization shows aliasing artifacts caused by the large stride 4 used in the 1st layer convolutions. (这点可以参考之前vnet中提到的，deconv导致的棋盘格伪影，大stride会更明显)</p>
</li>
<li><p>smaller stride &amp; smaller filter(stride2, filterSize7)：more coverage of mid frequencies, no aliasing, no dead feature</p>
<p>3.7</p>
</li>
<li><p>对于物体的关键部分遮挡之后会极大的影响分类结果</p>
</li>
<li>第二个和第三个例子中分别是文字和人脸的响应更高，但是却不是关键部分。</li>
</ul>
</li>
<li><p>理解</p>
<p> 4.1 总的来说，网络学习到的特征，是具有辨别性的特征，通过可视化就可以看到我们提取到的特征忽视了背景，而是把关键的信息给提取出来了。从layer 1、layer 2学习到的特征基本上是颜色、边缘等低层特征；layer 3则开始稍微变得复杂，学习到的是纹理特征，比如上面的一些网格纹理；layer 4学习到的则是较多的类别信息，比如狗头；layer 5对应着更强的不变性，可以包含物体的整体信息。。</p>
<p> 4.2 在网络迭代的过程中，特征图出现了sudden jumps。低层在训练的过程中基本没啥变化，比较容易收敛，高层的特征学习则变化很大。这解释了低层网络的从训练开始，基本上没有太大的变化，因为梯度弥散。高层网络刚开始几次的迭代，变化不是很大，但是到了40~50的迭代的时候，变化很大，因此我们以后在训练网络的时候，不要着急看结果，看结果需要保证网络收敛。</p>
<p> 4.3 图像的平移、缩放、旋转，可以看出第一层中对于图像变化非常敏感，第7层就接近于线性变化。</p>
</li>
</ol>
<h3 id="2-Striving-for-Simplicity-The-All-Convolutional-Net"><a href="#2-Striving-for-Simplicity-The-All-Convolutional-Net" class="headerlink" title="2. Striving for Simplicity: The All Convolutional Net"></a>2. Striving for Simplicity: The All Convolutional Net</h3><ol>
<li><p>动机</p>
<ul>
<li>traditional pipeline: alternating <strong>convolution</strong> and <strong>max-pooling</strong> layers followed by a small number of <strong>fully connected layers</strong></li>
<li>questioning the necessity of different components in the pipeline, <strong>max-pooling</strong> layer to be specified</li>
<li>to analyze the network we introduce a new variant of the “deconvolution approach” for visualizing features</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>two major improving directions based on traditional pipeline<ul>
<li>using more complex activation functions</li>
<li>building multiple conv modules</li>
</ul>
</li>
<li>we study the most simple architecture we could conceive<ul>
<li>a homogeneous network solely consisting of convolutional layers </li>
<li>without the need for complicated activation functions, any response normalization or max-pooling</li>
<li>reaches state of the art performance </li>
</ul>
</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>replace the pooling layers with standard convolutional layers with stride two</p>
<ul>
<li>the spatial dimensionality reduction performed by pooling makes covering larger parts of the input in higher layers possible </li>
<li>which is crucial for achieving good performance with CNNs </li>
</ul>
</li>
<li><p>make use of small convolutional layers </p>
<ul>
<li>greatly reduce the number of parameters in a network and thus serve as a form of regularization</li>
<li>if the topmost convolutional layer covers a portion of the image large enough to recognize its content then fully connected layers can also be replaced by simple 1-by-1 convolutions</li>
</ul>
</li>
<li><p>the overall architecture consists only of convolutional layers with rectified linear non-linearities and an averaging + softmax layer to produce predictions  </p>
<p>  <img src="/2020/01/03/CNN-Visualization系列/basemodel.png" width="60%;"></p>
<p>  <img src="/2020/01/03/CNN-Visualization系列/modifiedmodel.png" width="60%;"></p>
<ul>
<li>Strided-CNN-C: pooling is removed and the preceded conv stride is increase</li>
<li>ConvPool-CNN-C: a dense conv is placed, to show the effect of increasing parameters</li>
<li>All-CNN-C: max-pooling is replaced by conv</li>
<li>when pooling is replaced by an additional convolution layer with stride 2, performance stabilizes and even improves  </li>
<li>small 3 × 3 convolutions stacked after each other seem to be enough to achieve the best performance </li>
</ul>
</li>
<li><p>guided backpropagation</p>
<ul>
<li>the paper above proposed ‘deconvnet’, which we observe that it does not always work well without max-pooling layers </li>
<li><strong>For higher layers</strong> of our network the method of Zeiler and Fergus fails to produce <strong>sharp, recognizable image structure</strong></li>
<li>Our architecture does not include max-pooling, thus we can ’deconvolve’ <strong>without switches</strong>, i.e. not conditioning on an input image</li>
<li><p>In order to obtain a reconstruction conditioned on an input image from our network without pooling layers we to combine the simple backward pass and the deconvnet</p>
<p><img src="/2020/01/03/CNN-Visualization系列/backwardpass.png" width="70%;"></p>
</li>
<li><p>Interestingly, the very first layer of the network does not learn the usual Gabor filters, but higher layers do</p>
<p><img src="/2020/01/03/CNN-Visualization系列/gabor.png" width="60%;"></p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="3-Cam-Learning-Deep-Features-for-Discriminative-Localization"><a href="#3-Cam-Learning-Deep-Features-for-Discriminative-Localization" class="headerlink" title="3. Cam: Learning Deep Features for Discriminative Localization"></a>3. Cam: Learning Deep Features for Discriminative Localization</h3><ol>
<li><p>动机</p>
<ul>
<li>we found that CNNs actually behave as object detectors despite no supervision on the location </li>
<li>this ability is lost when fully-connected layers are used for classification </li>
<li>we found that the advantages of global average pooling layers are beyond simply acting as a regularizer</li>
<li>it makes it easily to localize the discriminative image regions despite not being trained for them</li>
</ul>
</li>
<li><p>论点</p>
<p> 2.1 Weakly-supervised object localization</p>
<ul>
<li>previous methods are not trained end-to-end and require multiple forward passes</li>
<li><p>Our approach is trained end-to-end and can localize objects in a single forward pass</p>
<p>2.2 Visualizing CNNs </p>
</li>
<li><p>previous methods only analyze the convolutional layers, ignoring the fully connected thereby painting an incomplete picture of the full story</p>
</li>
<li>we are able to understand our network from the beginning to the end</li>
</ul>
</li>
<li><p>方法</p>
<p> 3.1 Class Activation Mapping</p>
<ul>
<li>A class activation map for a particular category indicates the discriminative image regions used by the network to identify that category  </li>
<li>the network architecture: convs—-gap—-fc+softmax</li>
<li>we can identify the importance of the image regions by projecting back the weights of the output layer on to the convolutional feature maps </li>
<li><p>by simply upsampling the class activation map to the size of the input image we can identify the image regions most relevant to the particular category</p>
<p><img src="/2020/01/03/CNN-Visualization系列/cam.png" width="55%;"></p>
<p>3.2 Weakly-supervised Object Localization </p>
</li>
<li><p>our technique does not adversely impact the classification performance when learning to localize  </p>
</li>
<li>we found that the localization ability of the networks improved when the last convolutional layer before GAP had a higher spatial resolution, thus we removed several convolutional layers from the origin networks</li>
<li>overall we find that the classification performance is largely preserved for our GAP networks compared with the origin fc structure</li>
<li>our CAM approach significantly outperforms the backpropagation approach on generating bounding box </li>
<li><p>low mapping resolution prevents the network from obtaining accurate localizations</p>
<p>3.3 Visualizing Class-Specific Units </p>
</li>
<li><p>the convolutional units of various layers of CNNs act as visual concept detec- tors, identifying low-level concepts like textures or mate- rials, to high-level concepts like objects or scenes </p>
</li>
<li>Deeper into the network, the units become increasingly discriminative</li>
<li>given the fully-connected layers in many networks, it can be difficult to identify the importance of different units for identifying different categories </li>
</ul>
</li>
</ol>
<h3 id="4-Grad-CAM-Visual-Explanations-from-Deep-Networks-via-Gradient-based-Localization"><a href="#4-Grad-CAM-Visual-Explanations-from-Deep-Networks-via-Gradient-based-Localization" class="headerlink" title="4. Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization"></a>4. Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization</h3><h3 id="5-Grad-CAM-Improved-Visual-Explanations-for-Deep-Convolutional-Networks"><a href="#5-Grad-CAM-Improved-Visual-Explanations-for-Deep-Convolutional-Networks" class="headerlink" title="5. Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks"></a>5. Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks</h3><h2 id="6-综述"><a href="#6-综述" class="headerlink" title="6. 综述"></a>6. 综述</h2><ol>
<li><p>GAP</p>
<p> 首先回顾一下GAP，NiN中提出了GAP，主要为了解决全连接层参数过多，不易训练且容易过拟合等问题。</p>
<p> 对大多数分类任务来说不会因为做了gap让特征变少而让模型性能下降。因为GAP层是一个非线性操作层，这C个特征相当于是从kxkxC经过非线性变化选择出来的强特征。</p>
</li>
<li><p>heatmap</p>
<p> step1. 图像经过卷积网络后最后得到的特征图，在全连接层分类的权重（$w_{k,n}$）肯定不同，</p>
<p> step2. 利用反向传播求出每张特征图的权重，</p>
<p> step3. 用每张特征图乘以权重得到带权重的特征图，在第三维求均值，<strong>relu激活</strong>，归一化处理</p>
<ul>
<li>relu只保留wx大于0的值——我们正响应是对当前类别有用的特征，负响应会拉低$\sum wx$，即会降低当前类别的置信度</li>
<li><p>如果没有relu，定位图谱显示的不仅仅是某一类的特征。而是所有类别的特征。</p>
<p>step4. 将特征图resize到原图尺寸，便于叠加显示</p>
</li>
</ul>
</li>
<li><p>CAM</p>
<p> CAM要求必须使用GAP层，</p>
<p> CAM选择softmax层值最大的节点反向传播，<strong>求GAP层的梯度</strong>作为特征图的权重，每个GAP的节点对应一张特征图。</p>
</li>
<li><p>Grad-CAM</p>
<p> Grad-CAM不需要限制模型结构，</p>
<p> Grad-CAM选择softmax层值最大的节点反向传播，对<strong>最后一层卷积层</strong>求梯度，用每张特征图的梯度的均值作为该特征图的权重。</p>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://amberzzzz.github.io/2019/12/25/NiN-network-in-network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="amber.zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Less is More">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/12/25/NiN-network-in-network/" itemprop="url">NiN: network in network</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-12-25T17:08:30+08:00">
                2019-12-25
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/12/25/NiN-network-in-network/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/12/25/NiN-network-in-network/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Network-In-Network"><a href="#Network-In-Network" class="headerlink" title="Network In Network"></a>Network In Network</h3><ol>
<li><p>动机</p>
<ul>
<li>enhance model discriminability(获得更好的特征描述)：propose mlpconv</li>
<li>less prone to overfitting：propose global average pooling</li>
</ul>
</li>
<li><p>论点</p>
<p> <strong>comparison 1:</strong></p>
<ul>
<li>conventional CNN uses linear filter, which implicitly makes the assumption that the latent concepts are linearly separable. </li>
<li>traditional CNN is stacking [linear filters+nonlinear activation/linear+maxpooling+nonlinear]：这里引出了一个激活函数和池化层先后顺序的问题，对于avg_poolling，两种操作得到的结果是不一样的，先接激活函数会丢失部分信息，所以应该先池化再激活，对于MAX_pooling，两种操作结果一样，但是先池化下采样，可以减少激活函数的计算量，<strong>总结就是先池化再激活</strong>。但是好多网络实际实现上都是relu紧跟着conv，后面接pooling，这样比较interpretable——cross feature map pooling </li>
<li><p>mlpconv layer can be regarded as a highly nonlinear function(filter-fc-activation-fc-activation-fc-activation…)</p>
<p><img src="/2019/12/25/NiN-network-in-network/mlpconv.png" width="50%;"></p>
<p><strong>comparison 2:</strong></p>
</li>
<li><p>maxout network imposes the prior that instances of a latent concept lie within a convex set in the input space【QUESTION HERE】</p>
</li>
<li><p>mlpconv layer is a universal function approximator instead of a convex function approximator  </p>
<p><strong>comparison 3:</strong></p>
</li>
<li><p>fully connected layers are prone to overfitting and heavily depend on dropout regularization </p>
</li>
<li>global average pooling is more meaningful and interpretable, moreover it itself is a structural regularizer【QUESTION HERE】</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li>use <strong>mlpconv layer</strong> to replace conventional GLM(linear filters)</li>
<li>use <strong>global average pooling</strong> to replace traditional fully connected layers</li>
<li>the overall structure is a stack of mlpconv layers, on top of which lie the global average pooling and the objective cost layer</li>
<li>Sub-sampling layers can be added in between the mlpconv as in CNN</li>
<li>dropout is applied on the outputs of all but the last mlpconv layers for regularization</li>
<li>another regularizer applied is weight decay </li>
</ul>
<p><img src="/2019/12/25/NiN-network-in-network/NIN.png" width="70%;"></p>
</li>
<li><p>细节</p>
<ul>
<li><p>preprocessing：global contrast normalization and ZCA whitening </p>
</li>
<li><p>augmentation：translation and horizontal flipping</p>
</li>
<li><p>GAP for conventional CNN：CNN+FC+DROPOUT &lt; CNN+GAP &lt; CNN+FC</p>
<ul>
<li>gap is effective as a regularizer</li>
<li>slightly worse than the dropout regularizer result for some reason</li>
</ul>
</li>
<li><p>confidence maps </p>
<ul>
<li>explicitly enforce feature maps in the last mlpconv layer of NIN to be confidence maps of the categories by means of global average pooling：NiN将GAP的输出直接作为output layer，因此每一个类别对应的feature map可以近似认为是 confidence map。</li>
<li>the strongest activations appear roughly at the same region of the object in the original image：特征图上高响应区域基本与原图上目标区域对应。</li>
<li>this motivates the possibility of performing object detection via NIN</li>
</ul>
</li>
<li><p>architecture：实际中多层感知器使用1x1conv来实现，增加的多层感知器相当于是一个含参的池化层，通过对多个特征图进行含参池化，再传递到下一层继续含参池化，这种级联的<strong>跨通道的含参池化</strong>让网络有了更复杂的表征能力。</p>
<p>  <img src="/2019/12/25/NiN-network-in-network/architecture.png" width="55%;"></p>
</li>
</ul>
</li>
<li><p>总结</p>
<ol>
<li>mlpconv：stronger local reception unit</li>
<li>gap：regularizer &amp; bring confidence maps</li>
</ol>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://amberzzzz.github.io/2019/12/05/unet-vnet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="amber.zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Less is More">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/12/05/unet-vnet/" itemprop="url">unet & vnet</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-12-05T09:38:21+08:00">
                2019-12-05
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/12/05/unet-vnet/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/12/05/unet-vnet/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="U-NET-Convolutional-Networks-for-Biomedical-Image-Segmentation"><a href="#U-NET-Convolutional-Networks-for-Biomedical-Image-Segmentation" class="headerlink" title="U-NET: Convolutional Networks for Biomedical Image Segmentation"></a>U-NET: Convolutional Networks for Biomedical Image Segmentation</h2><ol>
<li><p>动机：</p>
<ul>
<li>train from very few images </li>
<li>outperforms more precisely on segmentation tasks</li>
<li>fast</li>
</ul>
</li>
<li><p>要素：</p>
<ul>
<li>编码：a contracting path to capture context</li>
<li>解码：a symmetric expanding path that enables precise localization</li>
<li>实现：pooling operators &amp; upsampling operators</li>
</ul>
</li>
<li><p>论点：</p>
<ul>
<li><p>when we talk about deep convolutional networks：</p>
<ul>
<li>larger and deeper</li>
<li>millions of parameters </li>
<li>millions of training samples </li>
</ul>
</li>
<li><p>representative method：run a sliding-window and predict a pixel label based on its‘ patch</p>
</li>
<li>drawbacks：<ul>
<li>calculating redundancy of overlapping patches</li>
<li>big patch：more max-pooling layers that reduce the localization accuracy</li>
<li>small patch：less involvement of context</li>
</ul>
</li>
<li>metioned but not further explained：cascade structure</li>
</ul>
</li>
<li><p>方法：</p>
<ol>
<li><p>In order to localize, high resolution features from the contracting path are combined with the upsampled output. A successive convolution layer can then learn to assemble a more precise output based on this information. </p>
<p> 理解：深层特征层感受野较大，带有全局信息，将其上采样用于提供localization information，而横向add过来特征层带有局部特征信息。两个3*3的conv block用于将两类信息整合，输出更精确的表达。</p>
</li>
<li><p>In the upsampling part we have also a large number of feature channels, which allow the network to propagate context information to higher resolution layers.  </p>
<p> 理解：应该是字面意思吧，为上采样的卷积层保留更多的特征通道，就相当于保留了更多的上下文信息。</p>
</li>
<li><p>we use excessive data augmentation.</p>
</li>
</ol>
</li>
<li><p>细节：</p>
<ol>
<li><p>contracting path：</p>
<ul>
<li>typical CNN：blocks of [2 3*3 unpadded convs+ReLU+2*2 stride2 maxpooling]</li>
<li>At each downsampling step we double the number of feature channels</li>
</ul>
</li>
<li><p>expansive path：</p>
<ul>
<li>upsampling：<ul>
<li>2*2 up-conv that half the channels</li>
<li>concatenation the corresponding cropped feature map from the contracting path</li>
<li>2 [3x3 conv+ReLU]</li>
</ul>
</li>
<li>final layer：use a 1*1 conv to map the feature vectors to class vectors</li>
</ul>
<p><img src="/2019/12/05/unet-vnet/unet.png" width="60%"></p>
</li>
<li><p>train：</p>
<ul>
<li>prefer larger input size to larger batch size</li>
<li>sgd with 0.99 momentum so that the previously seen samples dominate the optimization</li>
</ul>
</li>
<li><p>loss：softmax &amp; cross entropy </p>
</li>
<li><p>unbalanced weight：</p>
<ul>
<li>pre-compute the weight map base on the frequency of pixels for a certain class </li>
<li>add the weight for a certain element to force the learning emphasis：e.g. the small separation borders </li>
<li>initialization：Gaussian distribution </li>
</ul>
</li>
<li><p>data augmentation：</p>
<ul>
<li>deformations </li>
<li>“Drop-out layers at the end of the contracting path perform further implicit data augmentation”</li>
</ul>
</li>
<li><p>metrics：“warping error”, the “Rand error” and the “pixel error”  for EM segmentation challenge  and average IOU for ISBI cell tracking challenge </p>
</li>
<li><p>prediction：</p>
<p>按照论文的模型结构，<strong>输入和输出的维度是不一样的</strong>——在valid padding的过程中有边缘信息损失。</p>
<p>那么如果我们想要预测黄框内的分割结果，需要输入一张更大的图（蓝框）作为输入，在图片边缘的时候，我们通过镜像的方式补全。</p>
<p><img src="/2019/12/05/unet-vnet/predict.png" width="60%;"></p>
<p><strong>因果关系：</strong></p>
<ul>
<li>首先因为内存限制，输入的不是整张图，是图片patch，</li>
<li>为了保留上下文信息，使得预测更准确，我们给图片patch添加一圈border的上下文信息（实际感兴趣的是黄框区域）</li>
<li>在训练时，为了避免重叠引入的计算，卷积层使用了valid padding</li>
<li>因此在网络的输出层，输出尺寸才是我们真正关注的部分</li>
<li>如果训练样本尺寸不那么huge，完全可以全图输入，然后使用same padding，直接预测全图mask</li>
</ul>
</li>
</ol>
</li>
<li><p>总结：</p>
<ul>
<li>train from very few images —-&gt; data augmentation</li>
<li>fast —-&gt; full convolution layers</li>
<li>precise —-&gt; global?</li>
</ul>
</li>
</ol>
<h2 id="V-Net-Fully-Convolutional-Neural-Networks-for-Volumetric-Medical-Image-Segmentation"><a href="#V-Net-Fully-Convolutional-Neural-Networks-for-Volumetric-Medical-Image-Segmentation" class="headerlink" title="V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation"></a>V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation</h2><ol>
<li><p>动机</p>
<ul>
<li>entire 3D volume</li>
<li>imbalance between the number of foreground and background voxels：dice coefficient</li>
<li>limited data：apply random non-linear transformations and histogram matching</li>
<li>fast and accurate</li>
</ul>
</li>
<li><p>论点：</p>
<ul>
<li>early approaches based on patches<ul>
<li>local context</li>
<li>challenging modailities</li>
<li>efficiency issues</li>
</ul>
</li>
<li>fully convolutional networks<ul>
<li>2D so far</li>
</ul>
</li>
<li>imbalance issue：the anatomy of interest occupies only a very small region of the scan thus predictions are strongly biased towards the background.<ul>
<li>re-weighting</li>
<li>dice coefficient claims to be better that above</li>
</ul>
</li>
</ul>
</li>
<li><p>要素：</p>
<ul>
<li>a compression path</li>
<li><p>a decompression path</p>
<p><img src="/2019/12/05/unet-vnet/vnet.png" width="55%"></p>
</li>
</ul>
</li>
<li><p>方法：</p>
<ul>
<li><p>compression：</p>
<ul>
<li>add residual能够加速收敛</li>
<li>resolution is reduced by [2*2*2 conv with stride 2]相比于maxpooling节省了bp所需switch map的memory消耗</li>
<li>double the number of feature maps as we reduce their resolution</li>
<li>PReLU</li>
</ul>
</li>
<li><p>decompression：</p>
<ul>
<li>horizontal connections：1) gather fine grained detail that would be otherwise lost in the compression path  2) improve the convergence time </li>
<li><p>residual conv：blocks of [5*5*5 conv with stride 1] 提取特征继续增大感受野</p>
<p><img src="/2019/12/05/unet-vnet/receptive.png" width="50%"></p>
</li>
<li><p>up-conv：expands the spatial support of the lower resolution feature maps </p>
<p><img src="/2019/12/05/unet-vnet/deconv.png" width="60%"></p>
</li>
<li><p>last layer：run [1*1*1conv with 2 channel+softmax] to obtain the voxelwise probabilistic segmentations of the foreground and background </p>
</li>
</ul>
</li>
<li><p>dice coefficient： [0,1] which we aim to maximise，assume $p_i$、$g_i$ belong to two <strong>binary volumes</strong></p>
<script type="math/tex; mode=display">
  D = \frac{2\sum_i^N p_i g_i}{\sum_i^N p_i^2 + \sum_i^N g_i^2}</script></li>
<li><p>train：</p>
<ul>
<li>input fix size 128 × 128 × 64 voxels and a spatial resolution of 1 × 1 × 1.5 millimeters</li>
<li>each mini-batch contains 2 volumes</li>
<li>online augmentation：<ul>
<li>randomly deformation</li>
<li>vary the intensity distribution：随机选取样本的灰度分布作为当前训练样本的灰度分布</li>
</ul>
</li>
<li>used a momentum of 0.99 and a initial learning rate of 0.0001 which decreases by one order of magnitude every 25K iterations</li>
</ul>
</li>
<li><p>metrics：</p>
<ul>
<li>Dice coefficient</li>
<li>Hausdorff distance of the predicted delineation to the ground truth annotation</li>
<li>the score obtained on the challenge </li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="dice-loss-amp-focal-loss"><a href="#dice-loss-amp-focal-loss" class="headerlink" title="dice loss &amp; focal loss"></a>dice loss &amp; focal loss</h2><ol>
<li><p>CE &amp; BCE</p>
<ul>
<li><p>CE：categorical_crossentropy，针对所有类别计算，类别间互斥</p>
<script type="math/tex; mode=display">
CE(x) = -\sum_{i=1}^{n\_class}y_i log f_i(x)</script><blockquote>
<p>$x$是输入样本，$y_i$是第$i$个类别对应的真实标签，$f_i(x)$是对应的模型输出值。</p>
<p>对分类问题，$y_i$是one-hot，$f_i(x)$是个一维向量。最终得到一个数值。</p>
</blockquote>
</li>
<li><p>BCE：binary_crossentropy，针对每个类别计算</p>
<script type="math/tex; mode=display">
BCE(x)_i = - [y_i log f_i(x) + (1-y_i)log(1-f_i(x))]</script><blockquote>
<p>$i$是类别编号，最终得到一个维度为$n_class$的向量。</p>
<p>再求类均值得到一个数值作为单个样本的loss。</p>
</blockquote>
<script type="math/tex; mode=display">
BCE(x) = \frac{\sum_{i=1}^{n\_class}BCE_i(x)}{n\_class}</script></li>
<li><p>batch loss：对batch中所有样本的loss求均值。</p>
</li>
<li><p>从公式上看，CE的输出通常是经过了softmax，softmax的某一个输出增大，必然导致其它类别的输出减小，因此在计算loss的时候关注正确类别的预测值是否被拉高即可。使用BCE的场景通常是使用sigmoid，类别间不会互相压制，因此既要考虑所属类别的预测概率够高，也要考虑不所属类别的预测概率足够低（这一项在softmax中被实现了故CE不需要这一项）。</p>
</li>
<li>场景：<ul>
<li>二分类：只有一个输出节点，$f(x) \in (0,1)$，应该使用sigmoid+BCE作为最后的输出层配置。</li>
<li>单标签多分类：应该使用softmax+CE的方案，BCE也同样适用。</li>
<li>多标签多分类：multi-label每个标签的输出是相互独立的，因此常用配置是sigmoid+BCE。</li>
</ul>
</li>
<li>对分割场景来说，输出的每一个channel对应一个类别的预测map，可以看成是多个channel间的单标签多分类（softmax+CE），也可以看成是每个独立通道类别map的二分类（sigmoid+BCE）。unet论文用了weighted的softmax+CE。vnet论文用了dice_loss。</li>
</ul>
</li>
<li><p>re-weighting(WCE)</p>
<p> 基于CE&amp;BCE，给了样本不同的权重。</p>
<p> unet论文中提到了基于pixel frequency为不同的类别创建了weight map。</p>
<p> 一种实现：基于每个类别的weight map，在实现CE的时候改成加权平均即可。</p>
<p> 另一种实现：基于每个样本的weight map，作为网络的附加输入，在实现CE的时候乘在loss map上。</p>
</li>
<li><p>focal loss</p>
<p> 提出是在目标检测领域，用于解决正负样本比例严重失调的问题。</p>
<p> 也是一种加权，但是相比较于re-weighting，<strong>困难样本的权重由网络自行推断出</strong>，通过添加$(\alpha)$和$(-)^\lambda$这一加权项：</p>
<script type="math/tex; mode=display">
 focal\_loss(x)_i = -[\alpha y_i (1-p_i)^\lambda log (p_i)+(1-\alpha)(1-y_i)p_i^\lambda log(1-p_i)]</script><ul>
<li><p>对于类别间不均衡的情况（通常负样本远远多于正样本），$(\alpha)$项用于平衡正负样本权重。</p>
</li>
<li><p>对于类内困难样本的挖掘，$(-)^\lambda$项用于调整简单样本和困难样本的权重，预测概率更接近真实label的样本（简单样本）的权重会衰减更快，预测概率比较不准确的样本（苦难样本）的权重则更高些。</p>
<p>由于分割网络的输出的单通道／多通道的图片，直接使用focal loss会导致loss值很大。</p>
<p>​    1. 通常与其他loss加权组合使用</p>
<p>​    2. sum可以改成mean</p>
<p>​    3.不建议在训练初期就加入，可在训练后期用于优化模型</p>
<p>​    4. 公式中含log计算，可能导致nan，要对log中的元素clip</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">focal_loss</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    gamma = <span class="number">2.</span></span><br><span class="line">    alpha = <span class="number">0.25</span></span><br><span class="line">    <span class="comment"># score = alpha * y_true * K.pow(1 - y_pred, gamma) * K.log(y_pred) +            # this works when y_true==1</span></span><br><span class="line">    <span class="comment">#         (1 - alpha) * (1 - y_true) * K.pow(y_pred, gamma) * K.log(1 - y_pred)  # this works when y_true==0</span></span><br><span class="line">    pt_1 = tf.where(tf.equal(y_true, <span class="number">1</span>), y_pred, tf.ones_like(y_pred))</span><br><span class="line">    pt_0 = tf.where(tf.equal(y_true, <span class="number">0</span>), y_pred, tf.zeros_like(y_pred))</span><br><span class="line">    <span class="comment"># avoid nan</span></span><br><span class="line">    pt_1 = K.clip(pt_1, <span class="number">1e-3</span>, <span class="number">.999</span>)</span><br><span class="line">    pt_0 = K.clip(pt_0, <span class="number">1e-3</span>, <span class="number">.999</span>)</span><br><span class="line">    score = -K.sum(alpha * K.pow(<span class="number">1.</span> - pt_1, gamma) * K.log(pt_1)) -  \</span><br><span class="line">            K.sum((<span class="number">1</span> - alpha) * K.pow(pt_0, gamma) * K.log(<span class="number">1.</span> - pt_0))</span><br><span class="line">    <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>dice loss</p>
<p>dice定义两个mask的相似程度：</p>
<script type="math/tex; mode=display">
dice = \frac{2 * A \bigcap B}{|A|+|B|} = \frac{2 * TP}{2*TP + FN + FP}</script><ul>
<li>分子是TP——只关注前景</li>
<li><p>分母可以是$|A|$（逐个元素相加），也可以是平方形式$|A|^2$</p>
</li>
<li><p><strong>梯度：</strong>“使用dice loss有时会不可信，原因是对于softmax或log loss其梯度简言之是p-t ，t为目标值，p为预测值。而dice loss 为 2t2 / (p+t)2</p>
<p> 如果p，t过小会导致梯度变化剧烈，导致训练困难。”</p>
<p> 【详细解释下】交叉熵loss：$L=-(1-|t-p|)log(1-|t-p|)$，求导得到$\frac{\partial L}{\partial p}=-log(1-|t-p|)$，其实就可以简化看作$t-p$，很显然这个梯度是有界的，因此使用交叉熵loss的优化过程比较稳定。而dice loss的两种形式（不平方&amp;平方）：$L=\frac{2pt}{p+t}\ or\  L=\frac{2pt}{p^2+t^2}$，求导以后分别是$\frac{\partial L}{\partial p} = \frac{t^2+2pt}{(p+t)^2} \ or\ \frac{3tp^2+t^3}{(p^2+t^2)^2}$计算结果比较复杂，pt都很小的情况下，梯度值可能很大，可能导致训练不稳定，loss曲线混乱。</p>
</li>
</ul>
</li>
</ol>
<p>  vnet论文中的定义在分母上稍有不同（see below）。smoothing的好处：</p>
<ul>
<li>避免分子除0</li>
<li><p>减少过拟合</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dice_coef</span><span class="params">(y_true, y_pred)</span>:</span> </span><br><span class="line">  	smooth = <span class="number">1.</span></span><br><span class="line">    intersection = K.sum(y_true * y_pred, axis=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) </span><br><span class="line">    union = K.sum(y_true, axis=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) + K.sum(y_pred, axis=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) </span><br><span class="line">    <span class="keyword">return</span> K.mean( (<span class="number">2.</span> * intersection + smooth) / (union + smooth), axis=<span class="number">0</span>) </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dice_coef_loss</span><span class="params">(y_true, y_pred)</span>:</span> </span><br><span class="line">    <span class="number">1</span> - dice_coef(y_true, y_pred, smooth=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ol>
<li><p>iou loss</p>
<p> dice loss衍生，intersection over union：</p>
<script type="math/tex; mode=display">
 iou = \frac{A \bigcap B}{A \bigcup B}</script><p> 分母上比dice少了一个intersection。</p>
<ul>
<li>“IOU loss的缺点同DICE loss，训练曲线可能并不可信，训练的过程也可能并不稳定，有时不如使用softmax loss等的曲线有直观性，通常而言softmax loss得到的loss下降曲线较为平滑。”</li>
</ul>
</li>
<li><p>boundary loss</p>
<p> dice loss和iou loss是基于<strong>区域面积匹配度</strong>去学习，我们也可以使用<strong>边界匹配度</strong>去监督网络的学习。</p>
<p> 只对边界上的像素进行评估，和GT的边界吻合则为0，不吻合的点，根据其距离边界的距离评估它的Loss。</p>
</li>
<li><p>Hausdorff distance</p>
<p> 用于度量两个点集之间的相似程度，denote 点集$A\{a_1, a_2, …, a_p\}$，点集$B\{b_1, b_2, …, b_p\}$：</p>
<script type="math/tex; mode=display">
 HD(A, B) = max\{hd(A,B), hd(B,A)\}\\
 hd(A,B) = max_{a \in A} min_{b in B} ||a-b||\\
 hd(B,A) = max_{b \in B} min_{a in A} ||b-a||</script><p> 其中HD(A,B)是Hausdorff distance的基本形式，称为双向距离</p>
<p> hd(A,B)描述的是单向距离，首先找到点集A中每个点在点集B中距离最近的点作为匹配点，然后计算这些a-b-pair的距离的最大值。</p>
<p> HD(A,B)取单向距离中的最大值，描述了两个点集合的最大不匹配程度。</p>
</li>
<li><p>mix loss</p>
<ul>
<li>BCE + dice loss：在数据较为平衡的情况下有改善作用，但是在数据极度不均衡的情况下，交叉熵损失会在几个训练之后远小于Dice 损失，效果会损失。</li>
<li>focal loss + dice loss：数量级问题</li>
</ul>
</li>
<li><p>MSE</p>
<p> 关键点检测有时候也会采用分割框架，这时候ground truth是高斯map，dice是针对二值化mask的，这时候还可以用MSE。</p>
</li>
<li><p>ohnm</p>
<p>online hard negative mining 困难样本挖掘</p>
</li>
<li><p>Tversky loss</p>
<p>一种加权的dice loss，dice loss会平等的权衡FP（精度，假阳）和FN（召回，假阴），但是医学图像中病灶数目远少于背景数量，很可能导致训练结果偏向高精度但是低召回率，Tversky loss控制loss更偏向FN：</p>
<script type="math/tex; mode=display">
loss = 1-\frac{|PG|}{|PG|+\alpha|P\backslash G|+\beta|G\backslash P|}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tversky_loss</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    y_true_pos = K.flatten(y_true)</span><br><span class="line">    y_pred_pos = K.flatten(y_pred)</span><br><span class="line">    <span class="comment"># TP</span></span><br><span class="line">    true_pos = K.sum(y_true_pos * y_pred_pos)</span><br><span class="line">    <span class="comment"># FN</span></span><br><span class="line">    false_neg = K.sum(y_true_pos * (<span class="number">1</span>-y_pred_pos))</span><br><span class="line">    <span class="comment"># FP</span></span><br><span class="line">    false_pos = K.sum((<span class="number">1</span>-y_true_pos) * y_pred_pos)</span><br><span class="line">    alpha = <span class="number">0.7</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - (true_pos + K.epsilon())/(true_pos + alpha * false_neg + (<span class="number">1</span>-alpha) * false_pos + K.epsilon())</span><br></pre></td></tr></table></figure>
</li>
<li><p>Lovasz hinge &amp; Lovasz-Softmax loss</p>
<p>IOU loss衍生，jaccard loss只适用于离散情况，而网络预测是连续值，如果不使用某个超参将神经元输出二值化，就不可导。blabla</p>
<p>不是很懂直接用吧：<a href="https://github.com/bermanmaxim/LovaszSoftmax" target="_blank" rel="noopener">https://github.com/bermanmaxim/LovaszSoftmax</a></p>
</li>
</ol>
<h2 id="一些补充"><a href="#一些补充" class="headerlink" title="一些补充"></a>一些补充</h2><ol>
<li><p>改进：</p>
<ol>
<li>dropout、batch normalization：从论文上看，unet只在最深层卷积层后面添加了dropout layer，BN未表，而common sense用每一个conv层后面接BN层能够替换掉dropout并能获得性能提升的。</li>
<li>UpSampling2D、Conv2DTranspose：unet使用了上采样，vnet使用了deconv，但是“DeConv will produce image with checkerboard effect, which can be revised by upsample and conv”(<a href="https://distill.pub/2016/deconv-checkerboard/" target="_blank" rel="noopener">Reference</a>)。</li>
<li>valid padding、same padding：unet论文使用图像patch作为输入，特征提取时使用valid padding，损失边缘信息。</li>
<li>network blocks：unet用的conv block是两个一组的3*3conv，vnet稍微不同一点，可以尝试的block有ResNet／ResNext、DenseNet、DeepLab等。</li>
<li>pretrained encoder：feature extraction path使用一些现有的backbone，可以加载预训练权重(<a href="https://arxiv.org/abs/1801.05746" target="_blank" rel="noopener">Reference</a>)，加速训练，防止过拟合。</li>
<li>加入SE模块(<a href="https://zhuanlan.zhihu.com/p/36890585" target="_blank" rel="noopener">Reference</a>)：对每个通道的特征加权</li>
<li>attention mechanisms：</li>
<li>引用nn-Unet主要<strong>结构改进</strong>合集：“Just to provide some prominent examples: variations of encoder-decoder style architectures with skip connections, first introduced by the U-Net [12], include the introduction of residual connections [9], dense connections [6], at- tention mechanisms [10], additional loss layers [5], feature recalibration [13], and others [11]. </li>
</ol>
</li>
<li><p>衍生：</p>
<ol>
<li><p>TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation </p>
</li>
<li><p>nnU-Net: Breaking the Spell on Successful Medical Image Segmentation</p>
</li>
</ol>
</li>
</ol>
<h2 id="TernausNet-U-Net-with-VGG11-Encoder-Pre-Trained-on-ImageNet-for-Image-Segmentation"><a href="#TernausNet-U-Net-with-VGG11-Encoder-Pre-Trained-on-ImageNet-for-Image-Segmentation" class="headerlink" title="TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation"></a>TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation</h2><ol>
<li><p>动机：</p>
<ul>
<li>neural network initialized with pre-trained weights usually shows better performance than those trained from scratch on a small dataset. </li>
<li>保留encoder-decoder的结构，同时充分利用迁移学习的优势</li>
</ul>
</li>
<li><p>论点：</p>
<ul>
<li>load pretrained weights</li>
<li>用huge dataset做预训练</li>
</ul>
</li>
<li><p>方法：</p>
<ul>
<li>用vgg11替换原始的encoder，并load pre-trained weights on ImageNet：</li>
<li>最深层输入(maxpooling5)：use a single conv of 512 channels that <strong>serves as a bottleneck central part</strong> of the network</li>
</ul>
<p><img src="/2019/12/05/unet-vnet/TernausNet.png" width="50%;">                      <img src="/2019/12/05/unet-vnet/vgg11.png" width="30%;"></p>
<ul>
<li><p>upsampling换成了convTranspose</p>
</li>
<li><p>loss function：IOU + BCE：</p>
<script type="math/tex; mode=display">
 L = BCE - log(IOU)</script></li>
<li><p>inference：choose a <strong>threshold 0.3</strong>, all pixel values below which are set to be zero</p>
</li>
</ul>
</li>
<li><p>结论：</p>
<ol>
<li>converge faster</li>
<li>better IOU</li>
</ol>
</li>
</ol>
<h2 id="nnU-Net-Breaking-the-Spell-on-Successful-Medical-Image-Segmentation"><a href="#nnU-Net-Breaking-the-Spell-on-Successful-Medical-Image-Segmentation" class="headerlink" title="nnU-Net: Breaking the Spell on Successful Medical Image Segmentation"></a>nnU-Net: Breaking the Spell on Successful Medical Image Segmentation</h2><ol>
<li><p>动机</p>
<ul>
<li>many proposed methods fail to generalize: 对于分割任务，从unet出来之后的几年里，在网络结构上已经没有多少的突破了，结构修改越多，反而越容易过拟合</li>
<li>relies on just a simple U-Net architecture embedded in a robust training scheme</li>
<li>automate necessary adaptations such as preprocessing, the exact patch size, batch size, and inference settings based on the properties of a given dataset: 更多的提升其实在于理解数据，针对数据采用适当的预处理和训练方法和技巧</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>the diversity and individual peculiarities of imaging datasets make it difficult to generalize </li>
<li>prominent modifications focus on architectural modifications, merely brushing over all the other hyperparameters</li>
<li>we propose: 使用基础版unet：nnUNet（no-new-Net）<ul>
<li>a formalism for automatic adaptation to new datasets</li>
<li>automatically designs and executes a network training pipeline </li>
<li>without any manual fine-tuning</li>
</ul>
</li>
</ul>
</li>
<li><p>要素</p>
<p>a segmentation task: $f_{\theta}(X) = \hat Y$,  in this paper we seek for a $g(X,Y)=\theta$.</p>
<p>First we distinguish two type of hyperparameters:</p>
<ul>
<li>static params：in this case the network architecture and a robust training scheme </li>
<li>dynamic params：those that need to be changed in dependence of $X$ and $Y$</li>
</ul>
<p>Second we define g——a set of heuristics rules covering the entire process of the task:</p>
<ul>
<li>预处理：resampling和normalization</li>
<li>训练：loss，optimizer设置、数据增广</li>
<li>推理：patch-based策略、test-time-augmentations集成和模型集成等</li>
<li>后处理：增强单连通域等</li>
</ul>
</li>
<li><p>方法</p>
<ol>
<li><p>Preprocessing</p>
<ul>
<li>Image Normalization：<ul>
<li>CT：$normed_intensity = (intensity  - fg_mean) / fg_standard_deviation$,   $fg$ for $[0.05,0.95]$ foreground intensity</li>
<li>not CT：$normed_intensity = (intensity  - mean) / standard_deviation $</li>
</ul>
</li>
<li>Voxel Spacing：<ul>
<li>for each axis chooses the median as the target spacing</li>
<li>image resampled with third order spline interpolation</li>
<li>z-axis using nearest neighbor interpolation if ‘anisotropic spacing’ occurs</li>
<li>mask resampled with third order spline interpolation</li>
</ul>
</li>
</ul>
</li>
<li><p>Training Procedure </p>
<ul>
<li><p>Network Architecture：</p>
<ul>
<li><p>3 <strong>independent</strong> model：a 2D U-Net, a 3D U-Net and a cascade of two 3D U-Net  </p>
<p>  <img src="/2019/12/05/unet-vnet/nnUnet.jpg" width="80%;"></p>
</li>
<li><p>padded convolutions：to achieve identical output and input shapes </p>
</li>
<li><p><strong>instance normalization</strong>：“BN适用于判别模型，比如图片分类模型。因为BN注重对每个batch进行归一化，从而保证数据分布的一致性，而判别模型的结果正是取决于数据整体分布。但是BN对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布；IN适用于生成模型，比如图片风格迁移。因为图片生成的结果主要依赖于某个图像实例，所以对整个batch归一化不适合图像风格化，在风格迁移中使用Instance Normalization不仅可以加速模型收敛，并且可以保持每个图像实例之间的独立。”</p>
</li>
<li><p>Leaky ReLUs</p>
</li>
</ul>
</li>
<li><p>Network Hyperparameters：</p>
<ul>
<li>sets the batch size, patch size and number of pooling operations for each axis based on the memory consumption </li>
<li>large patch sizes are favored over large batch sizes </li>
<li>pooling along each axis is done until the voxel size=4</li>
<li>start num of filters=30, double after each pooling</li>
<li>If the selected patch size covers less than 25% of the voxels, train the 3D U-Net cascade on a downsampled version of the training data  to keep sufficient context </li>
</ul>
</li>
<li><p>Network Training:</p>
<ul>
<li>five-fold cross-validation </li>
<li>One epoch is defined as processing 250 batches </li>
<li>loss = dice loss + cross-entropy loss </li>
<li>Adam(lr=3e-4, decay=3e-5)</li>
<li>lrReduce: EMA(train_loss), 30 epoch, factor=0.2</li>
<li>earlyStop: earning rate drops below 10 6 or 1000 epochs are exceeded</li>
<li>data augmentation: elastic deformations, random scaling and random rotations as well as <strong>gamma augmentation</strong>($g(x,y)=f(x,y)^{gamma}$)</li>
<li>keep transformations in 2D-plane if ‘anisotropic spacing’ occurs</li>
</ul>
</li>
<li><p>Inference </p>
<ul>
<li>sliding window with half the patch size: this increases the weight of the predictions close to the center relative to the borders</li>
<li>ensemble:<ul>
<li>U-Net configurations (2D, 3D and cascade)</li>
<li>furthermore uses the five models (five-fold cross-validation)</li>
</ul>
</li>
<li></li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
<li><p>Ablation studies</p>
<p><img src="/2019/12/05/unet-vnet/nn-Unet Ablation studies.png" width="70%;"></p>
</li>
</ol>
<h2 id="3D-U-Net-Learning-Dense-Volumetric-Segmentation-from-Sparse-Annotation"><a href="#3D-U-Net-Learning-Dense-Volumetric-Segmentation-from-Sparse-Annotation" class="headerlink" title="3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation"></a>3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation</h2><ol>
<li><p>动机</p>
<ul>
<li>learns from sparsely/full annotated volumetric images (user annotates some slices)</li>
<li>provides a dense 3D segmentation </li>
</ul>
</li>
</ol>
<ol>
<li><p>要素</p>
<ul>
<li>3D operations </li>
<li>avoid bottlenecks and use batch normalization for faster convergence</li>
<li>on-the-fly elastic deformation</li>
<li>train from scratch</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>neighboring slices show almost the same information </li>
<li>many biomedical applications generalizes reasonably well because medical images comprises repetitive structures  </li>
<li>thus we suggest dense-volume-segmentation-network that only requires some annotated 2D slices for training</li>
<li><p>scenarios</p>
<ul>
<li>manual annotated 一部分slice，然后训练网络实现dense seg</li>
<li>用一部分 sparsely annotated的dataset作为training set，然后训练的网络实现在新的数据集上dense seg</li>
</ul>
<p><img src="/2019/12/05/unet-vnet/scenarios.png" width="50%;"></p>
</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>Network Architecture </p>
<ul>
<li><p>compression：2*3x3x3 convs(+BN)+relu+2x2x2 maxpooling</p>
</li>
<li><p>decompression：2x2x2 upconv+2*3x3x3 convs+relu</p>
</li>
<li><p>head：1x1x1 conv</p>
</li>
<li><p>concat shortcut connections</p>
</li>
<li><p>【QUESTION】avoid bottlenecks by doubling the number of channels already before max pooling</p>
<p>  个人理解这个double channel是在跟原始的unet结构对比，原始unet每个stage的两个conv的filter num是一样的，然后进行max pooling会损失部分信息，但是分割任务本身是个dense prediction，所以增大channel来减少信息损失</p>
<p>  但是不理解什么叫“avoid bottlenecks”</p>
<p>  原文说是参考了《Rethinking the inception architecture for computer vision》大名鼎鼎的inception V3</p>
<p>  可能对应的是“1. Avoid representational bottlenecks, especially early in the network.”，从输入到输出，要逐渐减少feature map的尺寸，同时要逐渐增加feature map的数量。</p>
</li>
</ul>
<p><img src="/2019/12/05/unet-vnet/3Dunet.png" width="50%;"></p>
<ul>
<li>input：132x132x116 voxel tile </li>
<li>output：44x44x28</li>
<li>BN：before each ReLU</li>
</ul>
</li>
</ul>
</li>
</ol>
<ul>
<li><p><strong>weighted softmax loss function</strong>：setting the weights of unlabeled pixels to zero makes it possible to learn from only the labelled ones and, hence, to generalize to the whole volume（是不是random set the loss zeros of some samples总能让网络更好的generalize？）</p>
<ul>
<li><p>Data</p>
<ul>
<li>manually annotated some orthogonal xy, xz, and yz slices </li>
<li>annotation slices were sampled uniformly</li>
</ul>
</li>
<li><p>ran on down-sampled versions of the original resolution by factor of two </p>
</li>
<li><p>labels：0: “inside the tubule”; 1: “tubule”; 2: “background”, and 3: “unlabeled”.  </p>
</li>
<li><p>Training</p>
<ul>
<li>rotation, scaling and gray value augmentation</li>
</ul>
</li>
<li>a smooth dense deformation：random vector, normal distribution, B-spline interpolation <ul>
<li>weighted cross-entropy loss：increase weights  “inside the tubule”, reduce weights “background”, set zero “unlabeled”</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="2-5D-UNet-Automatic-Segmentation-of-Vestibular-Schwannoma-from-T2-Weighted-MRI-by-Deep-Spatial-Attention-with-Hardness-Weighted-Loss"><a href="#2-5D-UNet-Automatic-Segmentation-of-Vestibular-Schwannoma-from-T2-Weighted-MRI-by-Deep-Spatial-Attention-with-Hardness-Weighted-Loss" class="headerlink" title="2.5D-UNet: Automatic Segmentation of Vestibular Schwannoma from T2-Weighted MRI by Deep Spatial Attention with Hardness-Weighted Loss"></a>2.5D-UNet: Automatic Segmentation of Vestibular Schwannoma from T2-Weighted MRI by Deep Spatial Attention with Hardness-Weighted Loss</h2><ol>
<li><p>专业术语</p>
<ul>
<li>Vestibular Schwannoma(VS) tumors：前庭神经鞘瘤</li>
<li>through-plane resolution：层厚</li>
<li>isotropic resolution：各向同性</li>
<li>anisotropic resolutions：各向异性</li>
</ul>
</li>
<li><p>动机</p>
<ul>
<li><p>tumor的精确自动分割</p>
</li>
<li><p>challenge</p>
<ul>
<li>low contrast：hardness-weighted Dice loss functio </li>
<li>small target region：attention module </li>
<li>low through-plane resolution：2.5D</li>
</ul>
</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>segment small structures from large image contexts<ul>
<li>coarse-to-fine </li>
<li>attention map </li>
<li>Dice loss </li>
<li>our method<ul>
<li>end-to-end supervision on the learning of attention map </li>
<li>voxel-level hardness- weighted Dice loss function </li>
</ul>
</li>
</ul>
</li>
<li>CNN<ul>
<li>2D CNNs ignore inter-slice correlation </li>
<li>3D CNNs most applied to images with isotropic resolution requiring upsampling</li>
<li>to balance the physical receptive field (in terms of mm rather than voxels)：memory rise</li>
<li>our method<ul>
<li>high in-plane resolution &amp; low through-plane resolution </li>
<li>2.5D CNN combining 2D and 3D convolutions </li>
<li>use inter-slice features </li>
<li>more efficient than 3D CNNs</li>
</ul>
</li>
</ul>
</li>
<li>数据<ul>
<li>T2-weighted MR images of 245 patients with VS tumor</li>
<li>high in-plane resolution around 0.4 mm×0.4 mm，512x512</li>
<li>slice thickness and inter-slice spacing 1.5 mm，slice number 19 to 118</li>
<li>cropped cube size：100 mm×50 mm×50 mm </li>
</ul>
</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>architecture</p>
<ul>
<li>five levels：L1、L2 use 2D，L3、L4、L5 use 3D</li>
<li>After the first two max-pooling layers that downsample the feature maps only in 2D, the feature maps in L3 and the followings have a near- isotropic 3D resolution.  </li>
<li>start channels：16</li>
<li>conv block：conv-BN-pReLU</li>
<li><p>add a spatial attention module to each level of the decoder </p>
<p><img src="/2019/12/05/unet-vnet/2.5D UNet.png" width="70%;"></p>
</li>
</ul>
</li>
<li><p>spatial attention module </p>
<ul>
<li>A spatial attention map can be seen as a single-channel image of attention coefficient </li>
<li>input：feature map with channel $N_l$</li>
<li>conv1+ReLU： channel $N_l/2$</li>
<li>conv2+Sigmoid：channel 1，outputs the attention map</li>
<li>multiplied the feature map with the attention map</li>
<li>a residual connection</li>
<li>explicit supervision <ul>
<li>multi-scale attention loss </li>
<li>$L_{attention} = \frac{1}{L} \sum_{L} l(A_l, G_l^f)$</li>
<li>$A_l$是每一层的attention map，$G_l^f$是每一层是前景ground truth average-pool到当前resolution的mask</li>
</ul>
</li>
</ul>
</li>
<li><p>Voxel-Level Hardness-Weighted Dice Loss</p>
<ul>
<li><p>automatic hard voxel weighting：$w_i = \lambda * abs(p_i - g_i) + (1-\lambda)$</p>
</li>
<li><p>$\lambda \in [0,1]$，controls the degree of hard voxel weighting</p>
</li>
<li><p>hardness-weighted Dice loss (HDL) ：</p>
<script type="math/tex; mode=display">
  l(P,G) = 1.0 - \frac{1}{C}\sum_{C} \frac{2\sum_i w_i p_i g_i + \epsilon}{\sum_i w_i (p_i + g_i) + \epsilon}</script></li>
<li><p>total loss：</p>
<script type="math/tex; mode=display">
  L = \frac{1}{L} \sum_{L} l(A_l, G_l^f) + l(P,G)</script></li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Combining-analysis-of-multi-parametric-MR-images-into-a-convolutional-neural-network-Precise-target-delineation-for-vestibular-schwannoma-treatment-planning"><a href="#Combining-analysis-of-multi-parametric-MR-images-into-a-convolutional-neural-network-Precise-target-delineation-for-vestibular-schwannoma-treatment-planning" class="headerlink" title="Combining analysis of multi-parametric MR images into a convolutional neural network: Precise target delineation for vestibular schwannoma treatment planning"></a>Combining analysis of multi-parametric MR images into a convolutional neural network: Precise target delineation for vestibular schwannoma treatment planning</h2><p>只有摘要和一幅图</p>
<p><img src="/2019/12/05/unet-vnet/two-pathway-unet.jpg" width="80%;"></p>
<ul>
<li>multi-parametric MR images：T1W、T2W、T1C</li>
<li>two-pathway U-Net model<ul>
<li>kernel 3 × 3 × 1 and 1 × 1 × 3 respectively</li>
<li>to extract the in-plane and through-plane features of the anisotropic MR images</li>
</ul>
</li>
<li>结论<ul>
<li>The proposed two-pathway U-Net model outperformed the single-pathway U-Net model when segmenting VS using anisotropic MR images.</li>
<li>multi-inputs（T1、T2）outperforms single-inputs</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://amberzzzz.github.io/2019/11/28/yolo系列/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="amber.zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Less is More">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/28/yolo系列/" itemprop="url">yolo系列</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-28T18:34:51+08:00">
                2019-11-28
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/11/28/yolo系列/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/11/28/yolo系列/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><ol>
<li>[yolov1] Yolov1: You Only Look Once: Unified, Real-Time Object Detection</li>
<li>[yolov2] Yolov2: YOLO9000: Better, Faster, Stronger</li>
<li>[yolov3] Yolov3: An Incremental Improvement </li>
<li>[yolov4] YOLOv4: Optimal Speed and Accuracy of Object Detection </li>
<li>[poly-yolo] POLY-YOLO: HIGHER SPEED, MORE PRECISE DETECTION AND INSTANCE SEGMENTATION FOR YOLOV3 </li>
<li>[scaled-yolov4] Scaled-YOLOv4: Scaling Cross Stage Partial Network </li>
</ol>
<h2 id="0-review"><a href="#0-review" class="headerlink" title="0. review"></a>0. review</h2><ol>
<li><p>review0121：关于yolo loss</p>
<p> 之前看keras版的yolo loss，包含分类的bce，回归的l2/mse，以及confidence的回归loss，其中conf loss被建模成单纯的0-1分类问题，用bce来实现。</p>
<p> 事实上原版的yolo loss中，objectness是iou（pred和gt的iou），从意义上，不仅指示当前格子有无目标，还对当前的box prediction做了评估</p>
<ul>
<li>回传梯度</li>
<li><p>不回传梯度</p>
<p>iou是通过xywh计算的，scaled_yolov4中把这个梯度截断，只作为一个值，对confidence进行梯度回传，</p>
<p>梯度不截断也没有问题，相当于对xywh再回传一个iou的loss</p>
</li>
</ul>
</li>
</ol>
<h2 id="1-Yolov1-You-Only-Look-Once-Unified-Real-Time-Object-Detection"><a href="#1-Yolov1-You-Only-Look-Once-Unified-Real-Time-Object-Detection" class="headerlink" title="1. Yolov1: You Only Look Once: Unified, Real-Time Object Detection"></a>1. Yolov1: You Only Look Once: Unified, Real-Time Object Detection</h2><ol>
<li>动机:<ul>
<li>end-to-end: 2 stages —-&gt; 1 stage</li>
<li>real-time</li>
</ul>
</li>
<li><p>论点：</p>
<ul>
<li>past methods:  complex pipelines, hard to optimize(trained separately) <ul>
<li>DPM use a sliding window and a classifier to evaluate an object at various locations </li>
<li>R-CNN use region proposal and run classifier on the proposed boxes,  then post-processing </li>
</ul>
</li>
<li>in this paper:  you only look once at an image <ul>
<li>rebuild the framework as a <strong>single</strong> regression problem:  single stands for <strong>you don’t have to run classifiers on each patch</strong></li>
<li><strong>straight</strong> from image pixels to bounding box coordinates and class probabilities:  straight stands for <strong>you obtain the bounding box and the classification results side by side, comparing to the previous serial pipeline </strong></li>
</ul>
</li>
</ul>
</li>
<li><p>advantages：</p>
<ul>
<li>fast &amp; twice the <strong>mean average precision</strong> of other real-time systems</li>
<li>CNN sees the entire image thus encodes contextual information </li>
<li>generalize better</li>
</ul>
</li>
<li><p>disadvantage:</p>
<ul>
<li>accuracy: “ it struggles to precisely localize some objects, especially small ones”</li>
</ul>
</li>
<li><p>细节：</p>
<ul>
<li><p>grid：</p>
<p>Our system divides the input image into an S × S grid. </p>
<p>If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object. </p>
<p><img src="/2019/11/28/yolo系列/grid.png" width="60%;"></p>
</li>
<li><p>prediction：</p>
<p> Each grid cell predicts B bounding boxes,  confidence scores <strong>for these boxes</strong> , and C conditional class probabilities <strong>for each grid</strong></p>
<p> that is an <script type="math/tex">S*S*(B*5+C)</script> tensor</p>
<ul>
<li>We normalize the bounding box width and height by the image width and height so that they fall between 0 and 1.</li>
<li>We parametrize the bounding box x and y coordinates to be offsets of a particular grid cell so they are also bounded between 0 and 1. </li>
</ul>
</li>
<li><p>at test time：</p>
<p> We obtain the <strong>class-specific confidence</strong> for individual box by multiply the class probability and box confidence：</p>
<script type="math/tex; mode=display">
 Pr(Class_i | Object) * Pr(Object)* IOU^{truth}_{pred} = Pr(Class_i)* IOU^{truth}_{pred}</script></li>
<li><p>network：</p>
<p>the convolutional layers extract features from the image </p>
<p>while the fully connected layers predict the probabilities and coordinates</p>
<p><img src="/2019/11/28/yolo系列/network.png" width="60%"></p>
</li>
<li><p>training：</p>
<p> activation：use a linear activation function for the final layer and leaky rectified linear activation all the other layers</p>
<p> optimization：use sum-squared error, however it does not perfectly align with the goal of maximizing average precision</p>
<p> ​    * weights equally the localization error and classification error：$\lambda_{coord}$</p>
<p> ​    * weights equally the grid cells containing and not-containing objects：$\lambda_{noobj}$</p>
<p> ​    * weights equally the large boxes and small boxes：square roots the h&amp;w insteand of the straight h&amp;w</p>
<p> loss：pick the box predictor has the highest current IOU with the ground truth per grid cell</p>
<p> avoid overfitting：dropout &amp; data augmentation</p>
<p> ​    * use dropout after the first connected layer,</p>
<p> ​    * introduce random scaling and translations of up to 20% of the original image size for data augmentation</p>
<p> ​    * randomly adjust the exposure and saturation of the image by up to a factor of 1.5 in the HSV color space for data augmentation</p>
</li>
<li><p>inference：</p>
<p> multiple detections：some objects locates near the border of multiple cells and <strong>can be well localized by multiple cells</strong>. <strong>Non-maximal suppression</strong> is proved critical, adding 2- 3% in mAP. </p>
</li>
</ul>
</li>
<li><p>Limitations：</p>
<ul>
<li>strong spatial constraints：decided by the settings of bounding boxes</li>
<li><p>softmax classification：can only have one class for each grid </p>
<blockquote>
<p>“This spatial constraint lim- its the number of nearby objects that our model can pre- dict. Our model struggles with small objects that appear in groups, such as flocks of birds. “</p>
<p>“ It struggles to generalize to objects in new or unusual aspect ratios or configurations. “</p>
</blockquote>
</li>
<li><p>coarse bounding box prediction：the architecture has multiple downsampling layers</p>
</li>
<li><p>the loss function treats errors the same in small bounding boxes versus large bounding boxes：</p>
<blockquote>
<p>The same error has much greater effect on a small box’s IOU than a big box.</p>
<p>“Our main source of error is incorrect localizations. “</p>
</blockquote>
</li>
</ul>
</li>
<li><p>Comparison：</p>
<ul>
<li>mAP among <strong>real-time detectors</strong> and <strong>Less Than Real-Time detectors</strong>：less mAP than fast-rcnn but much faster</li>
<li>error analysis between yolo and fast-rcnn：greater localization error and less background false-positive</li>
<li>combination analysis：[fast-rcnn+yolo] defeats [fast-rcnn+fast-rcnn] since YOLO makes different kinds of mistakes  with fast-rcnn</li>
<li>generalizability：RCNN degrades more because the Selective Search is tuned for natural images, change of dataset makes the proposals get worse. YOLO degrades less because it models the size and shape of objects, change of dataset varies less at object level but more at pixel level.</li>
</ul>
</li>
</ol>
<h2 id="2-Yolov2-YOLO9000-Better-Faster-Stronger"><a href="#2-Yolov2-YOLO9000-Better-Faster-Stronger" class="headerlink" title="2. Yolov2: YOLO9000: Better, Faster, Stronger"></a>2. Yolov2: YOLO9000: Better, Faster, Stronger</h2><ol>
<li>动机：<ul>
<li>run at varying sizes：offering an easy tradeoff between speed and accuracy</li>
<li>recognize a wide variety of objects ：jointly train on object detection and classification, so that the model can predict objects that aren’t labelled in detection data</li>
<li>better performance but still fast</li>
</ul>
</li>
<li><p>论点：</p>
<ul>
<li>Current object detection datasets are limited compared to classification datasets  <ul>
<li>leverage the classification data to expand the scope of current detection system</li>
<li>joint training algorithm making the object detectors working on both detection and classification data</li>
</ul>
</li>
<li><strong>Better performance</strong> often hinges on larger networks or ensembling multiple models. However we want a more accurate detector that is <strong>still fast</strong></li>
<li>YOLOv1’s shortcomings<ul>
<li>more localization errors</li>
<li>low recall</li>
</ul>
</li>
</ul>
</li>
<li><p>要素：</p>
<ol>
<li><p>better</p>
<p> <img src="/2019/11/28/yolo系列/better.png" width="70%;"></p>
</li>
<li><p>faster</p>
<ul>
<li>backbone</li>
</ul>
</li>
<li><p>stronger</p>
<ul>
<li><p>uses labeled detection images to learn to precisely localize objects </p>
</li>
<li><p>uses classification images to increase its vocabulary and robustness</p>
</li>
</ul>
</li>
</ol>
</li>
<li><p>方法：</p>
<ol>
<li><p>better：</p>
<ol>
<li><p>batch normalization：convergence &amp; regularization</p>
<blockquote>
<p>add batch normalization on all of the convolutional layers </p>
<p>remove dropout from the model </p>
</blockquote>
</li>
<li><p>high resolution classifier：pretrain a hi-res classifier </p>
<blockquote>
<p>first fine tune the classification network at the full 448 × 448 resolution for 10 epochs on ImageNet</p>
<p>then fine tune the resulting network on detection </p>
</blockquote>
</li>
<li><p>convolutional with anchor boxes：</p>
<p>YOLOv1通过网络最后的<strong>全连接层</strong>，直接预测每个grid上bounding box的坐标</p>
<p>而RPN基于先验框，使用最后一层<strong>卷积层</strong>，在特征图的各位置预测bounding box的offset和confidence</p>
<blockquote>
<p> “Predicting offsets instead of coordinates simplifies the problem and makes it easier for the network to learn”</p>
</blockquote>
<p>YOLOv2去掉了全连接层，也使用anchor box来回归bounding box</p>
<blockquote>
<p>eliminate one pooling layer to make the network output have higher resolution</p>
<p>shrink the network input to 416<em>416 to obtain an odd number so that there is a <em>*single center cell</em></em> in the feature map</p>
<p>predict class and objectness for every anchor box(offset prediction) instead of nothing(direct location&amp;scale prediction)</p>
</blockquote>
</li>
<li><p>dimension clustering：</p>
<p>what we want are priors that lead to good IOU scores, thus comes the distance metric：</p>
<script type="math/tex; mode=display">
d(box, centroid) = 1 - IOU(box, centroid)</script></li>
<li><p>direct location prediction：</p>
<p>YOLOv1 encounter model instability issue for predicting the (x, y) locations for the box</p>
<p>RPN also takes a long time to stabilize  by predicting a (tx, ty) and obtain the (x, y) center coordinates indirectly because this formulation is unconstrained so any anchor box can end up at any point in the image：</p>
<script type="math/tex; mode=display">
x = t_x * w_a - x_a\\
y = t_y * h_a - y_a</script><blockquote>
<p>学习RPN：回归一个相对量，比盲猜回归一个绝对location（YOLOv1）更好学习</p>
<p>学习YOLOv1：基于cell的预测，将bounding box限定在有限区域，不是全图飞（RPN）</p>
</blockquote>
<p>YOLOv2对每个cell，基于5个prior anchor size，预测5个bounding box，每个bounding box具有5维：</p>
<script type="math/tex; mode=display">
b_x = \sigma(t_x) + c_x\\
b_y = \sigma(t_y) + c_y\\
b_w = p_w e^{t_w}\\
b_h = p_h e^{t_h}\\
Pr(object)*IOU(b,object) = \sigma(t_o)</script><ul>
<li>$t_x\ \&amp;\ t_y$用于回归bounding box的位置，通过sigmoid激活函数被限定在0-1，通过上式能够间接得到bounding box的归一化位置（相对原图）</li>
<li>$t_w\ \&amp;\ t_h$用于回归bounding box的尺度，输出应该不是0-1限定，$p_w\ \&amp;\ p_h$是先验框的归一化尺度，通过上式能够间接得到bounding box的归一化尺度（相对原图）</li>
<li>$t_o$用于回归objectness，通过sigmoid限定在0-1之间，因为$Pr(object)\ \&amp;\ IOU(b,object)$都是0-1之间的值，IOU通过前面四个值能够求解，进而可以解耦objectness</li>
</ul>
<p><img src="/2019/11/28/yolo系列/regression.png" width="40%;"></p>
</li>
<li><p>fine-grained features：</p>
<p>motive：小物体的检测依赖更加细粒度的特征</p>
<p>cascade：Faster R-CNN and SSD both run their proposal networks at various feature maps in the network to get a range of resolutions</p>
<p>【QUESTION】YOLOv2 simply adds a passthrough layer from an earlier layer at 26 × 26 resolution：</p>
<blockquote>
<p>latter featuremap —-&gt; upsampling</p>
<p>concatenate with early featuremap</p>
<p>the detector runs on top of this expanded feature map </p>
</blockquote>
<p>predicts a $N<em>N</em>(3*(4+1+80))$ tensor for each scale</p>
</li>
<li><p>multi-scale training：</p>
<p>模型本身不限定输入尺寸：model only uses convolutional and pooling layers thus it can be resized on the fly </p>
</li>
</ol>
<ul>
<li>forces the network to learn to predict well across a variety of input dimensions <ul>
<li>the same network can predict detections at different resolutions</li>
</ul>
</li>
</ul>
<ol>
<li><p>loss：<strong>cited from the latter yolov3 paper</strong></p>
<ul>
<li>use sum of squared error loss for box coordinate(x,y,w,h)：then the gradient is $y_{true} - y_{pred}$</li>
</ul>
</li>
</ol>
<ul>
<li>use logistic regression for objectness score：which should be 1 if the bounding box prior overlaps a ground truth object by more than any other bounding box prior<ul>
<li>if a bounding box prior is not assigned to a ground truth object it incurs no loss(coordinate&amp;objectness)</li>
</ul>
</li>
<li>use binary cross-entropy loss for multilabel classification</li>
</ul>
</li>
</ol>
</li>
<li><p>faster：</p>
<ol>
<li><p>darknet-19：</p>
<p> YOLOv1中讨论过换VGG-16和YOLOv1使用的backbone对比，前者有map提升，但是耗时。</p>
<p> YOLOv2的新backbone，参数更少，而且相对于VGG16在ImageNet上精度更高。</p>
<p> <img src="/2019/11/28/yolo系列/darknet19.png" width="40%;"></p>
</li>
<li><p>training for classification：</p>
<pre><code> * first train on ImageNet using 224*224

 * then fine-tuning on 448*448
</code></pre><ol>
<li><p>training for detection：</p>
<ul>
<li><p>remove the last convolutional layer</p>
</li>
<li><p>add on three 3 × 3 convolutional layers with 1024 filters each followed by a final 1×1 convolutional layer with the number of outputs we need for detection </p>
</li>
<li>add a passthrough from the final 3 × 3 × 512 layer to the second to last convolutional layer so that our model can use fine grain features. </li>
</ul>
</li>
</ol>
</li>
<li><p>stronger：</p>
<p> jointly training：以后再填坑</p>
<ul>
<li>构造标签树</li>
<li>classification sample用cls loss，detection sample用detect loss</li>
<li>预测正确的classification sample给一个.3 IOU的假设值用于计算objectness loss</li>
</ul>
</li>
</ol>
</li>
</ol>
<h2 id="3-Yolov3-An-Incremental-Improvement"><a href="#3-Yolov3-An-Incremental-Improvement" class="headerlink" title="3. Yolov3: An Incremental Improvement"></a>3. Yolov3: An Incremental Improvement</h2><ol>
<li><p>动机：</p>
<p> nothing like super interesting, just a bunch of small changes that make it better</p>
</li>
<li><p>方法：</p>
<ol>
<li><p>bounding box prediction：</p>
<p> use anchor boxes and predicts offsets for each bounding box</p>
<p> use sum of squared error loss for training</p>
<p> predicts the objectness score for each bounding box using logistic regression</p>
<p> one ground truth coresponds to one best box and one loss</p>
</li>
<li><p>class prediction：</p>
<p> use binary cross-entropy loss for multilabel classification</p>
</li>
<li><p><strong>【NEW】prediction across scales：</strong></p>
<p> the detector：a few more convolutional layers following the feature map, the last of which predicts a 3-d(for 3 priors) tensor encoding bounding box, objectness, and class predictions</p>
<p> expanded feature map：upsampling the deeper feature map by 2X and concatenating with the former features</p>
<blockquote>
<p>“With the new multi-scale predictions, YOLOv3 has better perfomance on small objects and comparatively worse performance on medium and larger size objects “</p>
</blockquote>
</li>
<li><p><strong>【NEW】feature extractor：</strong></p>
<p> darknet-53 !</p>
<p> <img src="/2019/11/28/yolo系列/darknet53.png" width="40%;"></p>
</li>
<li><p>training：common skills</p>
</li>
</ol>
</li>
</ol>
<h2 id="4-一些补充"><a href="#4-一些补充" class="headerlink" title="4. 一些补充"></a>4. 一些补充</h2><ol>
<li><p>metrics：mAP</p>
<p> 最早由PASCAL VOC提出，输出结果是一个ranked list，每一项包含框、confidence、class，</p>
<p> yolov3提到了一个“COCOs weird average mean AP metric ”</p>
<ul>
<li><p>IoU：预测框与ground truth的交并比，也被称为Jaccard指数，我们通常用其来判断每个检测的正确性。PASCAL VOC数据集用0.5为阈值来判定预测框是True Positive还是False Positive，COCO数据集则建议对不同的IoU阈值进行计算。</p>
<p><img src="/2019/11/28/yolo系列/iou.png" width="20%;"></p>
</li>
<li><p>置信度：通过改变置信度阈值，我们可以改变一个预测框是Positive还是 Negative。</p>
</li>
<li><p>precision &amp; recall：precision = TP ／(TP + FP)，recall = TP／(TP + FN)。图片中我们没有预测到的每个部分都是Negative，因此计算True Negatives比较难办。但是我们只需要计算False Negatives，即我们模型所漏检的物体。</p>
<p>  <img src="/2019/11/28/yolo系列/pr.png" width="30%;"></p>
</li>
<li><p>AP：不同的置信度下会得到不同的precision-recall。为了得到precision-recall曲线，首先对模型预测结果进行排序，按照各个预测值置信度降序排列。给定不同的置信度阈值，就有不同的ranked output，Recall和Precision仅在高于该rank值的预测结果中计算。这里共选择11个不同的recall（[0, 0.1, …, 0.9, 1.0]），那么AP就定义为在这11个recall下precision的平均值，其可以表征整个precision-recall曲线（曲线下面积）。给定recall下的precision计算，是通过一种插值的方式：</p>
<script type="math/tex; mode=display">
  AP = \frac{1}{11}\sum_{r\in\{0,0.1,...,1.0\}}p_{interp}(r) \\
  p_{interp}(r) = max_{\tilde r: \tilde r > r} p(\tilde r)</script></li>
<li><p>mAP：此度量指标在<strong>信息检索</strong>和<strong>目标检测</strong>领域有不同的计算方式。对于目标检测，对于各个类别，分别按照上述方式计算AP，取所有类别的AP平均值就是mAP。</p>
<p>  <img src="/2019/11/28/yolo系列/map.png" width="40%;"></p>
</li>
</ul>
</li>
<li><p>eval：</p>
<ol>
<li>yolo_head输出：box_xy是box的中心坐标，(0~1)相对值；box_wh是box的宽高，(0~1)相对值；box_confidence是框中物体置信度；box_class_probs是类别置信度；</li>
<li>yolo_correct_boxes函数：能够将box中心的相对信息转换成[y_min,x_min,y_max,x_max]的绝对值</li>
<li>yolo_boxes_and_scores函数：输出网络预测的所有box</li>
<li>yolo_eval函数：基于score_threshold、max_boxes两项过滤，类内NMS，得到最终输出</li>
</ol>
</li>
</ol>
<h2 id="4-YOLOv4-Optimal-Speed-and-Accuracy-of-Object-Detection"><a href="#4-YOLOv4-Optimal-Speed-and-Accuracy-of-Object-Detection" class="headerlink" title="4. YOLOv4: Optimal Speed and Accuracy of Object Detection"></a>4. YOLOv4: Optimal Speed and Accuracy of Object Detection</h2><ol>
<li><p>动机</p>
<ul>
<li>Practical testing the tricks of improving CNN</li>
<li>some features<ul>
<li>work for certain problems/dataset exclusively</li>
<li>applicable to the majority of models, tasks, and datasets</li>
<li>only increase the training cost  [bag-of-freebies]</li>
<li>only increase the inference cost by a small amount but can significantly improve the accuracy  [bag-of-specials]</li>
</ul>
</li>
<li>Optimal Speed and Accuracy </li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>head：<ul>
<li>predict classes and bounding boxes </li>
<li>one-stage head<ul>
<li>YOLO, SSD, RetinaNet </li>
<li>anchor-free：CenterNet, CornerNet, FCOS  </li>
</ul>
</li>
<li>two-stage head<ul>
<li>R-CNN series </li>
<li>anchor-free：RepPoints </li>
</ul>
</li>
</ul>
</li>
<li>neck：<ul>
<li>collect feature maps from different stages </li>
<li>FPN, PAN, BiFPN, NAS-FPN </li>
</ul>
</li>
<li><p>backbone：</p>
<ul>
<li>pre-trained on ImageNet</li>
<li>VGG, ResNet, ResNeXt, DenseNet</li>
</ul>
<p><img src="/2019/11/28/yolo系列/detector.png" width="70%;"></p>
</li>
<li><p>Bag of freebies</p>
<ul>
<li>data augmentation <ul>
<li>pixel-wise adjustments<ul>
<li>photometric distortions：brightness, contrast, hue, saturation, and noise </li>
<li>geometric distortions：random scaling, cropping, flipping, and rotating </li>
</ul>
</li>
<li>object-wise<ul>
<li>cut：<ul>
<li>to image：CutOut </li>
<li>to featuremaps：DropOut, DropConnect, DropBlock </li>
</ul>
</li>
<li>add：MixUp, CutMix, GAN </li>
</ul>
</li>
</ul>
</li>
<li>data imbalance for classification<ul>
<li>two-stage：hard example mining </li>
<li>one-stage：focal loss, soft label  </li>
</ul>
</li>
<li>bounding box regression <ul>
<li>MSE-regression：treat [x,y,w,h] as independent variables</li>
<li>IoU loss：consider the integrity &amp; scale invariant </li>
</ul>
</li>
</ul>
</li>
<li>Bag of specials <ul>
<li>enlarging receptive field：improved SPP, ASPP, RFB</li>
<li>introducing attention mechanism <ul>
<li>channel-wise attention：SE, increase the inference time by about 10%</li>
<li>point-wise attention：Spatial Attention Module (SAM), does not affect the speed of inference</li>
</ul>
</li>
<li>strengthening feature integration<ul>
<li>channel-wise level：SFAM</li>
<li>point-wise level：ASFF </li>
<li>scale-wise level：BiFPN </li>
</ul>
</li>
<li>activation function：A good activation function can make the gradient more efficiently propagated</li>
<li>post-processing：各种NMS</li>
</ul>
</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>choose a backbone —- CSPDarknet53 </p>
<ul>
<li>Higher input network size (resolution) – for detecting multiple small-sized objects </li>
<li>More conv layers – for a higher receptive field to cover the increased size of input network </li>
<li>More parameters – for greater capacity of a model to detect multiple objects of different sizes in a single image </li>
</ul>
</li>
<li><p>add the SPP block over the CSPDarknet53 </p>
<ul>
<li>significantly increases the receptive field </li>
<li>separates out the most significant context features  </li>
<li>causes almost no re- duction of the network operation speed</li>
</ul>
</li>
<li><p>use PANet as the method of parameter aggregation </p>
<ul>
<li>Modified PAN</li>
<li><p>replace shortcut connection of PAN to concatenation </p>
<p><img src="/2019/11/28/yolo系列/pan.png" width="30%;"></p>
</li>
</ul>
</li>
<li><p>use YOLOv3 (anchor based) head </p>
</li>
<li><p>Mosaic data augmentation</p>
<ul>
<li>mixes 4 training images </li>
<li>allows detection of objects outside their normal context</li>
<li><p>reduces the need for a large mini-batch size </p>
<p><img src="/2019/11/28/yolo系列/mosaic.png" width="40%;"></p>
</li>
</ul>
</li>
<li><p>Self-Adversarial Training (SAT) data augmentation</p>
<ul>
<li>1st stage alters images</li>
<li>2nd stage train on the modified images</li>
</ul>
</li>
<li><p>CmBN：a CBN modified version </p>
<p>  <img src="/2019/11/28/yolo系列/cmbn.png" width="40%;"></p>
</li>
<li><p>modified SAM：from spatial-wise attention to point- wise attention </p>
<p>  <img src="/2019/11/28/yolo系列/sam.png" width="35%;"></p>
</li>
</ul>
</li>
<li><p>实验</p>
<ul>
<li><p>Influence of different features on Classifier training </p>
<ul>
<li><p>Bluring和Swish没有提升</p>
<p><img src="/2019/11/28/yolo系列/cls.png" width="35%;"></p>
</li>
</ul>
</li>
<li><p>Influence of different features on Detector training </p>
<ul>
<li>IoU threshold, CmBN, Cosine annealing sheduler, CIOU有提升</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="POLY-YOLO-HIGHER-SPEED-MORE-PRECISE-DETECTION-AND-INSTANCE-SEGMENTATION-FOR-YOLOV3"><a href="#POLY-YOLO-HIGHER-SPEED-MORE-PRECISE-DETECTION-AND-INSTANCE-SEGMENTATION-FOR-YOLOV3" class="headerlink" title="POLY-YOLO: HIGHER SPEED, MORE PRECISE DETECTION AND INSTANCE SEGMENTATION FOR YOLOV3"></a>POLY-YOLO: HIGHER SPEED, MORE PRECISE DETECTION AND INSTANCE SEGMENTATION FOR YOLOV3</h2><ol>
<li><p>动机</p>
<ul>
<li><p>yoloV3’s weakness</p>
<ul>
<li>rewritten labels </li>
<li>inefficient distribution of anchors </li>
</ul>
</li>
<li><p>light backbone：</p>
<ul>
<li>stairstep upsampling </li>
</ul>
</li>
<li>single scale output </li>
<li>to extend instance segmentation <ul>
<li>detect size-independent polygons defined on a polar grid </li>
<li>real-time processing </li>
</ul>
</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li><p>yolov3</p>
<ul>
<li>real-time</li>
<li>low precision cmp with RetinaNet, EfficientDet <ul>
<li>low precision of the detection of big boxes </li>
<li>rewriting of labels by each-other due to the coarse resolution </li>
</ul>
</li>
</ul>
</li>
<li><p>this paper solution：</p>
<ul>
<li>解决yolo精度问题：propose a brand-new feature decoder with a single ouput tensor that goes to head with higher resolution</li>
<li>多尺度特征融合：utilize stairstep upscaling</li>
<li>实例分割：bounding polygon within a poly grid</li>
</ul>
</li>
<li><p>instance segmentation</p>
<ul>
<li>two-stage：mask-rcnn</li>
<li>one-stage：<ul>
<li>top-down：segmenting this object within a bounding box</li>
<li>bottom-up：start with clustering pixels</li>
<li>direct methods：既不需要bounding box也不需要clustered pixels，PolarMask </li>
</ul>
</li>
</ul>
</li>
<li><p>cmp with PolarMask </p>
<ul>
<li>size-independent：尺度，大小目标都能检测</li>
<li>dynamic number of vertices：多边形定点可变</li>
</ul>
</li>
<li><p>yolov3 issues</p>
<ul>
<li><p>rewriting of labels：</p>
<ul>
<li>两个目标如果落在同一个格子里，在一个尺度上ground truth label只会保留一个box</li>
<li><p>对越小的特征图，grid越大，这个问题越严重</p>
<p><img src="/2019/11/28/yolo系列/rewriting.png" width="90%;"></p>
</li>
</ul>
</li>
<li><p>imbalanced distribution of anchors across output scales </p>
<ul>
<li>anchor如果选的不合理，会导致特征图尺度和anchor尺度不匹配</li>
<li>most of the boxes will be captured by the middle output layer and the two other layers will be underused</li>
<li>如上面车的case，大多数车的框很小，聚类出的给level0和level1的anchor shape还是很小，但是level0是稀疏grid<ul>
<li>一方面，grid shape和anchor shape不匹配</li>
<li>一方面，label rewriten问题会升级</li>
</ul>
</li>
<li>反过来，如果dense grid上预测大目标，会受到感受野的制约</li>
<li>一种解决方案是基于感受野首先对gt box分成三组，然后分别聚类，然后9选1</li>
</ul>
</li>
<li><p>yolov3原文：YOLOv3 has relatively high $AP_{small}$ performance. However, it has comparatively worse performance on medium and larger size objects. More investigation is needed to get to the bottom of this.</p>
<ul>
<li>小目标performance更好，大目标worse，主要是就是因为coarse grid上存在label rewriten问题，存在部分gt box被抑制掉了。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>architecture </p>
<p>  <img src="/2019/11/28/yolo系列/poly-yolo.png" width="80%;"></p>
<ul>
<li><p>single output  </p>
</li>
<li><p>higher resolution：stride4</p>
</li>
<li><p>handle all the anchors at once </p>
</li>
<li><p>cross-scale fusion</p>
<ul>
<li>hypercolumn technique：add operation</li>
<li><p>stairstep interpolation：x2 x2 …</p>
<p><img src="/2019/11/28/yolo系列/hypercolumn.png" width="80%;"></p>
</li>
</ul>
</li>
<li><p>SE-blocks</p>
</li>
<li>reduced the number of convolutional filters to 75%  in the feature extraction phase </li>
</ul>
</li>
<li><p>bounding polygons </p>
<ul>
<li>extend the box tuple：$b_i=\{b_i^{x^1},b_i^{y^1},b_i^{x^2},b_i^{y^2},V_i\}$</li>
<li>The center of a bounding box is used as the origin </li>
<li>polygon tuple：$v_{i,j}=\{\alpha_{i,j},\beta_{i,j},\gamma_{i,j}\}$</li>
<li>polar coordinate：distance &amp; oriented angle，相对距离（相对anchor box的对角线），相对角度（norm到[0,1]）</li>
<li><p>polar cell：一定角度的扇形区域 内，如果sector内没有定点，conf=0</p>
<p><img src="/2019/11/28/yolo系列/polar.png" width="80%;"></p>
</li>
<li><p>general shape：</p>
<ul>
<li>不同尺度，形状相同的object，在polar coord下表示是一样的</li>
<li>distance*anchor box的对角线，转换成绝对尺度</li>
<li>bounding box的两个对角预测，负责尺度估计，polygon只负责预测形状</li>
<li>sharing values should make the learning easier</li>
</ul>
</li>
</ul>
</li>
<li><p>mix loss</p>
<ul>
<li>output：a*(4+1+3*n_vmax)</li>
<li>box center loss：bce</li>
<li>box wh loss：l2 loss</li>
<li>conf loss：bce with ignore mask</li>
<li>cls loss：bce</li>
<li>polygon loss：$\gamma<em>(log(\frac{\alpha}{anchor^d})-\hat a)^2 + \gamma</em>bce(\beta,\hat{beta})+bce(\gamma, \hat \gamma)$</li>
<li>auxiliary task learning：<ul>
<li>任务间相互boost</li>
<li>converge faster </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Scaled-YOLOv4-Scaling-Cross-Stage-Partial-Network"><a href="#Scaled-YOLOv4-Scaling-Cross-Stage-Partial-Network" class="headerlink" title="Scaled-YOLOv4: Scaling Cross Stage Partial Network"></a>Scaled-YOLOv4: Scaling Cross Stage Partial Network</h2><ol>
<li><p>动机</p>
<ul>
<li>model scaling method</li>
<li>redesign yolov4 and propose yolov4-CSP</li>
<li>develop scaled yolov4<ul>
<li>yolov4-tiny</li>
<li>yolov4-large</li>
</ul>
</li>
<li>没什么技术细节，就是网络结构大更新</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li><p>common technique changes depth &amp; width of the backbone</p>
</li>
<li><p>recently there are NAS</p>
</li>
<li><p>model scaling</p>
<ul>
<li><p>input size、width、depth对网络计算量呈现square, linear, and square increase</p>
<p><img src="/2019/11/28/yolo系列/flop.png" width="50%;"></p>
</li>
<li><p>改成CSP版本以后，能够减少参数量、计算量，提高acc，缩短inference time</p>
<p><img src="/2019/11/28/yolo系列/CSPflop.png" width="50%;"></p>
</li>
<li><p>检测的准确性高度依赖reception field，RF随着depth线性增长，随着stride倍数增长，所以一般先组合调节input size和stage，然后再根据算力调整depth和width</p>
<p>  <img src="/2019/11/28/yolo系列/reception.png" width="50%;"></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>backbone：CSPDarknet53 </p>
</li>
<li><p>neck：CSP-PAN，减少40%计算量，SPP</p>
<p>  <img src="/2019/11/28/yolo系列/CSPpan.png" width="50%;"></p>
</li>
<li><p>yoloV4-tiny</p>
<p>  <img src="/2019/11/28/yolo系列/tiny.png" width="40%;"></p>
</li>
<li><p>yoloV4-large：P456</p>
<p>  <img src="/2019/11/28/yolo系列/large.png" width="80%;"></p>
</li>
</ul>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://amberzzzz.github.io/2019/11/13/triplet-center-loss论文/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="amber.zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Less is More">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/13/triplet-center-loss论文/" itemprop="url">triplet-center-loss论文</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-13T18:45:59+08:00">
                2019-11-13
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/11/13/triplet-center-loss论文/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/11/13/triplet-center-loss论文/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="0-before-reading"><a href="#0-before-reading" class="headerlink" title="0. before reading"></a>0. before reading</h2><p>结合：</p>
<ul>
<li><p>triplet loss：考虑类间关系，但计算复杂度高，困难样本难挖掘</p>
</li>
<li><p>center loss：考虑类内关系</p>
</li>
<li><p>TCL：同时增加类内数据的紧实度（compactness）和类间的分离度（separability）</p>
<p>  三元组只考虑样本、所属类中心、最近邻类的中心。避免了建立triplets的复杂度和mining hard samples的难度。</p>
</li>
</ul>
<h2 id="title：Triplet-Center-Loss-for-Multi-View-3D-Object-Retrieval"><a href="#title：Triplet-Center-Loss-for-Multi-View-3D-Object-Retrieval" class="headerlink" title="title：Triplet-Center Loss for Multi-View 3D Object Retrieval"></a>title：Triplet-Center Loss for Multi-View 3D Object Retrieval</h2><ol>
<li><p>动机：<em>deep metric learning</em> </p>
<ul>
<li><p>the learned features using softmax loss are not discriminative enough in nature </p>
</li>
<li><p>although samples of the two classes are separated by the decision boundary elaborately, there exists significant intra-class variations</p>
</li>
<li><p><strong>QUESTION1</strong>：so what? how does this affect the current task? 动机描述不充分。</p>
</li>
<li><p><strong>QUESTION2</strong>：在二维平面上overlap不代表在高维空间中overlap，这种illustration究竟是否有意义。</p>
</li>
<li><p><strong>ANSWER for above</strong>：高维空间可分，投影到二维平面不一定可分，但是反过来，二维平面上高度可分，映射会高维空间数据仍旧是高度可分的。只能说，后者能够确保不同类别数据离散性更好，不能说明前者数据离散性不好（如果定义了高维距离，也可以说明）。</p>
<p> <img src="/2019/11/13/triplet-center-loss论文/distance.png" width="80%"></p>
</li>
</ul>
</li>
<li><p>应用场景：<em>3D object retrieval</em> </p>
</li>
<li><p>要素：</p>
<ul>
<li>learns a center for each class </li>
<li>requires that the distances between samples and centers from the same class are smaller than those from different classes, in this way the samples are pulled closer to the corresponding center and meanwhile pushed away from the  different centers</li>
<li>both the inter-class separability  and the intra-class variations are considered</li>
</ul>
</li>
<li><p>论点：</p>
<ul>
<li><p>Compared with triplet loss, TCL avoids the complex construction of triplets and hard sample mining mechanism. </p>
</li>
<li><p>Compared with center loss, TCL not only considers to reduce the intra-class variations.</p>
</li>
<li><p><strong>QUESTION</strong>：what about the comparison with [softmax loss + center loss]?</p>
</li>
<li><p><strong>ANSWER for above</strong>：center-loss is actually representing for the joint loss [softmax loss + center loss]. </p>
<p>  ‘’Since the class centers are updated at each iteration based on a mini-batch instead of the whole dataset, which can be very unstable, it has to be under the joint supervision of softmax loss during training. ‘’</p>
</li>
</ul>
</li>
<li><p>本文做法：</p>
<ul>
<li>the proposed TCL is used as the supervision loss</li>
<li>the softmax loss could be also combined in as an addition</li>
</ul>
</li>
<li><p>细节：</p>
<ul>
<li><p>TCL：</p>
<script type="math/tex; mode=display">
  L_{tc} = \sum_{i=1}^Mmax(D(f_i, c_{y^i}) + m - min_{j\neq y^i}D(f_i, c_j), 0)</script><p>  前半部分是center-loss，类内欧几里得距离，后半部分是每个样本和与其最近的negative center之间的距离。</p>
</li>
<li><p>‘Unlike center loss, TCL can be used independently from softmax loss. <strong>However</strong>… ‘</p>
<p>  作者解释说，因为center layer是随机初始化出来的，而且是batch updating，因此开始阶段会比较tricky，’while softmax loss could serve as a good guider for seeking better class centers ‘</p>
</li>
<li><p>调参中提到’m is fixed to 5’，说明本文对feature vector没有做normalization（相比之下facenet做了归一化，限定所有embedding分布在高维球面上）。</p>
</li>
<li><p>衡量指标：AUC和MAP，这是一个retrieval任务，最终需要的是embedding，给定Query，召回top matches。</p>
</li>
</ul>
</li>
<li><p>reviews：</p>
<ul>
<li>个人理解：<ol>
<li>softmax分类器旨在数据可分，对于分类边界、feature vector的空间意义不存在一个具象的描述。deep metric learning能够引入这种具象的、图像学的意义，在此基础上，探讨distance、center才有意义。</li>
<li>就封闭类数据（类别有限且已知）分类来讲，分类边界有无图像学描述其实意义不大。已知的数据分布尽可能discriminative的主要意义是针对未知类别，我们希望给到模型一个<strong>未知数据</strong>时，它能够检测出来，而不是划入某个已知类（softmax）。</li>
<li>TCL的最大贡献应该是想到用center替代样本来进行metric judgement，改善triplet-loss复杂计算量这一问题，后者实际训起来太难了，没有感情的GPU吞噬机器。</li>
</ol>
</li>
<li>XXX：</li>
<li></li>
</ul>
</li>
</ol>
<p>能够引入这种具象的、图像学的意义，在此基础上，我们探讨distance、center才有意义。</p>
<p>​        </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://amberzzzz.github.io/2019/11/11/dicomReader/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="amber.zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Less is More">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/11/dicomReader/" itemprop="url">dicomReader</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-11T10:28:21+08:00">
                2019-11-11
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/11/11/dicomReader/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/11/11/dicomReader/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ol>
<li><p>read a dcm file</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> SimpleITK <span class="keyword">as</span> sitk</span><br><span class="line"></span><br><span class="line">image = sitk.ReadImage(dcm_file)</span><br><span class="line">image_arr = sitk.GetArrayFromImage(image)</span><br></pre></td></tr></table></figure>
</li>
<li><p>read a dcm series</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">series_IDs = sitk.ImageSeriesReader.GetGDCMSeriesIDs(series_path)</span><br><span class="line"></span><br><span class="line">nb_series = len(series_IDs)</span><br><span class="line">print(nb_series)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认获取第一个序列的所有切片路径</span></span><br><span class="line">dicom_names = sitk.ImageSeriesReader.GetGDCMSeriesFileNames(file_path)</span><br><span class="line">series_reader = sitk.ImageSeriesReader()</span><br><span class="line">series_reader.SetFileNames(dicom_names)</span><br><span class="line">image3D = series_reader.Execute()</span><br></pre></td></tr></table></figure>
</li>
<li><p>read a dcm case</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">series_IDs = sitk.ImageSeriesReader.GetGDCMSeriesIDs(case_path)</span><br><span class="line"><span class="keyword">for</span> series_id <span class="keyword">in</span> series_IDs:</span><br><span class="line">    dicom_names = sitk.ImageSeriesReader.GetGDCMSeriesFileNames(case_path, series_id)</span><br><span class="line">    series_reader = sitk.ImageSeriesReader()</span><br><span class="line">    series_reader.SetFileNames(dicom_names)</span><br><span class="line">    image3D = series_reader.Execute()</span><br></pre></td></tr></table></figure>
</li>
<li><p>read tag</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先得到image对象</span></span><br><span class="line">Image_type = image.GetMetaData(<span class="string">"0008|0008"</span>) <span class="keyword">if</span> image.HasMetaData(<span class="string">"0008|0008"</span>) <span class="keyword">else</span> <span class="string">'Nan'</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>发现一种序列，每张图的尺寸不同，这样执行series_reader的时候会报错，因为series_reader会依照第一层的图像尺寸申请空间，所以要么异常要么逐张读。</p>
<p> reference: <a href="http://itk-users.7.n7.nabble.com/ITK-users-Reader-InvalidRequestedRegionError-td38608.html" target="_blank" rel="noopener">http://itk-users.7.n7.nabble.com/ITK-users-Reader-InvalidRequestedRegionError-td38608.html</a></p>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://amberzzzz.github.io/2019/11/06/c-tricks-in-engineering/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="amber.zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Less is More">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/06/c-tricks-in-engineering/" itemprop="url">c++ tricks in engineering</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-06T14:14:52+08:00">
                2019-11-06
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/11/06/c-tricks-in-engineering/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/11/06/c-tricks-in-engineering/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ol>
<li><p>数组传参</p>
<p> 工程化被坑了好多回！</p>
<p> C/C++ 传递数组，虽然传递的是首地址地址，但是<strong>参数到了函数内就成了普通指针</strong>。</p>
<p> 所以试图在调用函数中求取所传递数组的长度是行不通的。</p>
</li>
<li><p>vector传参</p>
<p> 传值—&gt;拷贝构造，传引用／指针—&gt;不发生拷贝构造。</p>
<p> 实际工程化中遇到的问题是，构建了一个vector\<cv::mat\> imgs对象，传入函数以后，在函数内部创建空间cv::Mat img，然后将img push进vector。在函数外读取该vector的时候发现其内部没值。</cv::mat\></p>
<p> <strong>要点：1. 要传引用，2. push clone：imgs.push_back(img)</strong></p>
<p> 另外，vector可以作为函数返回值。</p>
</li>
<li></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/8/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/10/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpeg"
                alt="amber.zhang" />
            
              <p class="site-author-name" itemprop="name">amber.zhang</p>
              <p class="site-description motion-element" itemprop="description">要糖有糖，要猫有猫</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">134</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">73</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/AmberzzZZ" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-globe"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">amber.zhang</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'u9EV4x74hJaETIaNF0uX3490-gzGzoHsz',
        appKey: 'asMAPmAVtavwP5Orm1xcyxxK',
        placeholder: 'leave your comment ...',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
