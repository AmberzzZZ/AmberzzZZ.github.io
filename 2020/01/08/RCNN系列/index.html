<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="目标检测，two-stage," />










<meta name="description" content="综述 papers  [R-CNN] R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation   [SPP] SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognitio">
<meta name="keywords" content="目标检测，two-stage">
<meta property="og:type" content="article">
<meta property="og:title" content="RCNN系列">
<meta property="og:url" content="https://amberzzzz.github.io/2020/01/08/RCNN系列/index.html">
<meta property="og:site_name" content="Less is More">
<meta property="og:description" content="综述 papers  [R-CNN] R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation   [SPP] SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognitio">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://amberzzzz.github.io/2020/01/08/RCNN系列/R-CNN.png">
<meta property="og:image" content="https://amberzzzz.github.io/2020/01/08/RCNN系列/warp.png">
<meta property="og:image" content="https://amberzzzz.github.io/2020/01/08/RCNN系列/activations.png">
<meta property="og:image" content="https://amberzzzz.github.io/2020/01/08/RCNN系列/feature%20maps.png">
<meta property="og:image" content="https://amberzzzz.github.io/2020/01/08/RCNN系列/spp.png">
<meta property="og:image" content="https://amberzzzz.github.io/2020/01/08/RCNN系列/mapping.png">
<meta property="og:image" content="https://amberzzzz.github.io/2020/01/08/RCNN系列/fastRCNN.png">
<meta property="og:image" content="https://amberzzzz.github.io/2020/01/08/RCNN系列/smoothL1.png">
<meta property="og:image" content="https://amberzzzz.github.io/2020/01/08/RCNN系列/fasterRCNN.png">
<meta property="og:image" content="https://amberzzzz.github.io/2020/01/08/RCNN系列/RPN.png">
<meta property="og:image" content="https://amberzzzz.github.io/2020/01/08/RCNN系列/RoIAlign.png">
<meta property="og:image" content="https://amberzzzz.github.io/2020/01/08/RCNN系列/mrcnnheads.png">
<meta property="og:image" content="https://amberzzzz.github.io/2020/01/08/RCNN系列/pyramids.png">
<meta property="og:image" content="https://amberzzzz.github.io/2020/01/08/RCNN系列/similar%20structs.png">
<meta property="og:image" content="https://amberzzzz.github.io/2020/01/08/RCNN系列/FPNseg.png">
<meta property="og:image" content="https://amberzzzz.github.io/2020/01/08/RCNN系列/formulation.png">
<meta property="og:image" content="https://amberzzzz.github.io/2020/01/08/RCNN系列/outlier.png">
<meta property="og:image" content="https://amberzzzz.github.io/2020/01/08/RCNN系列/quality.png">
<meta property="og:updated_time" content="2021-07-14T05:05:50.723Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RCNN系列">
<meta name="twitter:description" content="综述 papers  [R-CNN] R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation   [SPP] SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognitio">
<meta name="twitter:image" content="https://amberzzzz.github.io/2020/01/08/RCNN系列/R-CNN.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://amberzzzz.github.io/2020/01/08/RCNN系列/"/>





  <title>RCNN系列 | Less is More</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Less is More</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://amberzzzz.github.io/2020/01/08/RCNN系列/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="amber.zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Less is More">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">RCNN系列</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-08T16:03:10+08:00">
                2020-01-08
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/01/08/RCNN系列/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2020/01/08/RCNN系列/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><ol>
<li><p>papers</p>
<p> [R-CNN] R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation </p>
<p> [SPP] SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</p>
<p> [Fast R-CNN] Fast R-CNN: Fast Region-based Convolutional Network</p>
<p> [Faster R-CNN] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</p>
<p> [Mask R-CNN] Mask R-CNN  </p>
<p> [FPN] FPN: Feature Pyramid Networks for Object Detection</p>
<p> [Cascade R-CNN] Cascade R-CNN: Delving into High Quality Object Detection</p>
</li>
</ol>
<h2 id="R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation"><a href="#R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation" class="headerlink" title="R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation"></a>R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation</h2><ol>
<li><p>动机</p>
<ul>
<li>localizing objects with a deep network and training a high-capacity model with only a small quantity of annotated detection data<ul>
<li>apply CNN to region proposals: R-CNN represents ‘Regions with CNN features’</li>
<li>supervised pre-training </li>
</ul>
</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>model as a regression problem: not fare well in practice  </li>
<li>build a sliding-window detector: have to maintain high spatial resolution</li>
<li><strong>what we do: </strong>our method gener- ates around 2000 category-independent region proposals for the input image, extracts a fixed-length feature vector from each proposal using a CNN, and then classifies each region with category-specific linear SVMs </li>
<li>conventional solution to training a large CNN is ‘using unsupervised pre-training, followed by supervised fine-tuning’</li>
<li><strong>what we do: </strong>‘supervised pre-training on a large auxiliary dataset (ILSVRC), followed by domain specific fine-tuning on a small dataset (PASCAL)’</li>
<li><strong>we also demonstrate: </strong>a simple bounding box regression method significantly reduces mislocalizations</li>
<li><strong>R-CNN operates on regions:</strong> it is natural to extend it to the task of semantic segmentation </li>
</ul>
</li>
<li><p>要素</p>
<ul>
<li>category-independent region proposals </li>
<li>a large convolutional neural network that extracts a fixed-length feature vector from each region </li>
<li><p>a set of class-specific linear SVMs</p>
<p><img src="/2020/01/08/RCNN系列/R-CNN.png" width="40%;"></p>
</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>Region proposals: we use selective search</p>
</li>
<li><p>Feature extraction: we use Krizhevsky CNN, 227*227 RGB input, 5 convs, 2 fcs, 4096 output</p>
<ul>
<li>we first dilate the tight bounding box (padding=16)</li>
<li><p>then warp the bounding box to the required size (各向异性缩放)</p>
<p><img src="/2020/01/08/RCNN系列/warp.png" width="40%;"></p>
</li>
</ul>
</li>
<li><p>Test-time detection:</p>
<ul>
<li>we score each extracted feature vector using the SVM trained for each class</li>
<li>we apply a greedy non-maximum suppression (for each class independently)  </li>
<li>对留下的这些框进行canny边缘检测，就可以得到bounding-box</li>
<li>(then B-BoxRegression)</li>
</ul>
</li>
<li><p>Supervised pre-training: pre-trained the CNN on a large auxiliary dataset (ILSVRC 2012) with <strong>image-level annotations</strong></p>
</li>
<li><p>Domain-specific fine-tuning: </p>
<ul>
<li>continue SGD training of the CNN using only warped region proposals from VOC </li>
<li>replace the 1000-way classification layer with a randomly initialized 21-way layer (20 VOC classes plus background)</li>
<li><strong>class label: all region proposals with ≥ 0.5 IoU overlap with a ground-truth box as positives, else negatives </strong></li>
<li>1/10th of the initial pre-training rate</li>
<li>uniformly sample 32 positive windows (over all classes) and 96 background windows to construct a mini-batch of size 128 </li>
</ul>
</li>
<li><p>Object category classifiers:</p>
<ul>
<li>considering a binary classifier for a specific class</li>
<li><strong>class label: take IoU overlap threshold &lt;0.3 as negatives, take only regions tightly enclosing the object as positives </strong> </li>
<li>take the ground-truth bounding boxes for each class as positives</li>
</ul>
</li>
<li><p><strong>unexplained:</strong></p>
<ul>
<li><p>the positive and negative examples are defined differently in CNN fine-tuning versus SVM training</p>
<p>  CNN容易过拟合，需要大量的训练数据，所以在CNN训练阶段我们对Bounding box的位置限制条件限制的比较松(IOU只要大于0.5都被标注为正样本)，svm适用于少样本训练，所以对于训练样本数据的IOU要求比较严格，我们只有当bounding box把整个物体都包含进去了，我们才把它标注为物体类别。</p>
</li>
<li><p>it’s necessary to train detection classifiers rather than simply use outputs of the fine-tuned CNN</p>
<p>  上一个回答其实同时也解释了CNN的head已经是一个分类器了，还要用SVM分类：按照上述正负样本定义，CNN softmax的输出比采用svm精度低。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>分析</p>
<ul>
<li><p>learned features:</p>
<ul>
<li>compute the units’ activations on a large set of held-out region proposals  </li>
<li>sort from the highest to low</li>
<li>perform non-maximum suppression</li>
<li><p>display the top-scoring regions</p>
<p><img src="/2020/01/08/RCNN系列/activations.png" width="70%;"></p>
</li>
</ul>
</li>
<li><p>Ablation studies:</p>
<ul>
<li><strong>without fine-tuning:</strong> features from fc7 generalize worse than features from fc6, indicating that most of the CNN’s representational power comes from its convolutional layers</li>
<li><strong>with fine-tuning: </strong>The boost from fine-tuning is much larger for fc6 and fc7 than for pool5, suggests that pool features learned from ImageNet are <strong>general</strong> and that most of the improvement is gained from learning <strong>domain-specific</strong> non-linear classifiers on top of them</li>
</ul>
</li>
<li><p>Detection error analysis:</p>
<ul>
<li>more of our errors result from poor localization rather than confusion </li>
<li>CNN features are much more discriminative than HOG </li>
<li>Loose localization likely results from our use of bottom-up region proposals and the positional invariance learned from pre-training the CNN for whole-image classification(粗暴的IOU判定前背景，二值化label，无法体现定位好坏差异)</li>
</ul>
</li>
<li><p>Bounding box regression：</p>
<ul>
<li>a linear regression model use the pool5 features for a selective search region proposal as input</li>
<li>输出为xy方向的缩放和平移</li>
<li>训练样本：判定为本类的候选框中和真值重叠面积大于0.6的候选框</li>
</ul>
</li>
<li><p>Semantic segmentation：</p>
<ul>
<li>three strategies for computing features:<ul>
<li>‘<em>full</em> ‘ ignores the region’s shape, two regions with different shape might have very similar bounding boxes(信息不充分)</li>
<li>‘<em>fg</em> ‘ slightly outperforms <em>full</em>, indicating that the masked region shape provides a stronger signal</li>
<li>‘<em>full+fg</em> ‘ achieves the best, indicating that the context provided by the <em>full</em> features is highly informative even given the <em>fg</em> features(形状和context信息都重要)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="SPP-net-Spatial-Pyramid-Pooling-in-Deep-Convolutional-Networks-for-Visual-Recognition"><a href="#SPP-net-Spatial-Pyramid-Pooling-in-Deep-Convolutional-Networks-for-Visual-Recognition" class="headerlink" title="SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition"></a>SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</h2><ol>
<li><p>动机：</p>
<ul>
<li>propose a new pooling strategy, “spatial pyramid pooling”</li>
<li>can generate a fixed-length representation regardless of image size/scale</li>
<li>also robust to object deformations </li>
</ul>
</li>
<li><p>论点：</p>
<ul>
<li>existing CNNs require a fixed-size input<ul>
<li>reduce accuracy for sub-images of an arbitrary size/scale (need cropping/warping)</li>
<li>cropped region lost content, while warped content generates unwanted distortion</li>
<li>overlooks the issues involving scales </li>
</ul>
</li>
<li>convolutional layers do not require a fixed image size, whle the fully-connected layers need to have fixed- size/length input by their definition</li>
<li>by introducing the SPP layer<ul>
<li>between the last convolutional layer and the first fully-connected layer</li>
<li>pools the features and generates fixed- length outputs</li>
</ul>
</li>
<li>Spatial pyramid pooling <ul>
<li>partitions the image into divisions from finer to coarser levels, and aggregates local features in them</li>
<li>generates fixed- length output </li>
<li>uses multi-level spatial bins(robust to object deformations )</li>
<li>can run at variable scales </li>
<li>also allows varying sizes or scales <strong>during training</strong>: <ul>
<li>train the network with different input size at different epoch</li>
<li>increases scale-invariance </li>
<li>reduces over-fitting </li>
</ul>
</li>
<li>in object detection <ul>
<li>run the convolutional layers only <em>once</em> on the entire image </li>
<li>then extract features by SPP-net on the feature maps </li>
<li>speedup </li>
<li>accuracy </li>
</ul>
</li>
</ul>
</li>
</ul>
<ol>
<li><p>方法：</p>
<ul>
<li><p>Convolutional Layers and Feature Maps</p>
<ul>
<li>the outputs of the convolutional layers are known as feature maps</li>
<li><p>feature maps involve not only the strength of the responses(the strength of activation), but also their spatial positions(the reception field)</p>
<p><img src="/2020/01/08/RCNN系列/feature maps.png" width="80%;"></p>
</li>
</ul>
</li>
<li><p>The Spatial Pyramid Pooling Layer</p>
<ul>
<li>it can maintain spatial information by pooling in local spatial bins</li>
<li>the spatial bins have sizes proportional to the image size(k-level: 1*1, 2*2, …, k*k)</li>
<li>we can resize the input image to any scale, which is important for the accuracy  </li>
<li><p>the coarsest pyramid level has a single bin that covers the entire image, which is in fact a “global pooling” operation </p>
<p><img src="/2020/01/08/RCNN系列/spp.png" width="40%;"></p>
</li>
<li><p>for a feature map of $a×a$, with a pyramid level of $n×n$ bins:</p>
<script type="math/tex; mode=display">
  the\ window\ size:\ win = ceiling(a/n)\\
  the\ stride:\ str = floor(a/n)</script></li>
</ul>
</li>
</ul>
</li>
</ol>
<ul>
<li><p>Training the Network</p>
<ul>
<li>Single-size training: fixed-size input (224×224) cropped from images, cropping for data augmentation<ul>
<li>Multi-size training: rather than cropping, we resize the aforementioned 224×224 region to 180×180, then we train two fixed-size networks that share parameters by altenate epoch</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>分析</p>
<ul>
<li><strong>50 bins vs. 30 bins: </strong>the gain of multi-level pooling is not simply due to more parameters, it is because the multi-level pooling is robust to the variance in object deformations and spatial layout<ul>
<li><strong>multi-size vs. single-size:  </strong>multi results are more or less better than the single-size version</li>
<li><strong>full vs. crop: </strong>shows the importance of maintaining the complete content</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>SPP-NET FOR OBJECT DETECTION</strong></p>
<ul>
<li><p>We extract the feature maps from the entire image only once </p>
</li>
<li><p>we apply the spatial pyramid pooling on each candidate window of the feature maps  </p>
</li>
<li><p>These representations are provided to the fully-connected layers of the network </p>
</li>
<li><p>SVM samples: We use the ground-truth windows to generate the positive samples, use the samples with IOU&lt;30% as the negative samples </p>
</li>
<li><p>multi-scale feature extraction: </p>
<ul>
<li>We resize the image at {480, 576, 688, 864, 1200}, and compute the feature maps of conv5 for each scale. <ul>
<li>we choose a single scale s ∈ S such that the scaled candidate window has a number of pixels closest to 224×224. </li>
<li>And we use the corresponding feature map to compute the feature for this window</li>
<li>this is roughly equivalent to resizing the window to 224×224 </li>
</ul>
</li>
</ul>
</li>
<li><p>fine-tuning:</p>
<ul>
<li>Since our features are pooled from the conv5 feature maps from windows of any sizes<ul>
<li>for simplicity we only fine-tune the fully-connected layers </li>
</ul>
</li>
</ul>
</li>
<li><p>Mapping a Window to Feature Maps**</p>
<ul>
<li><p>we project the corner point of a window onto a pixel in the feature maps, such that this corner point in the image domain is closest to the center of the receptive field of that feature map pixel.  </p>
<p><img src="/2020/01/08/RCNN系列/mapping.png" width="50%;"></p>
<p>​    确定原图上的两个角点（左上角和右下角），映射到 feature map上的两个对应点，使得映射点$(x^{‘}, y^{‘})$在原始图上<strong>感受野（上图绿色框）的中心点</strong>与$(x,y)$尽可能接近。</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Fast-R-CNN-Fast-Region-based-Convolutional-Network"><a href="#Fast-R-CNN-Fast-Region-based-Convolutional-Network" class="headerlink" title="Fast R-CNN: Fast Region-based Convolutional Network"></a>Fast R-CNN: Fast Region-based Convolutional Network</h2><ol>
<li><p>动机</p>
<ul>
<li>improve training and testing speed</li>
<li>increase detection accuracy</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>current approaches train models in multi-stage pipelines that are slow and inelegant<ul>
<li>R-CNN &amp; SPPnet: CNN+SVM+bounding-box regression</li>
<li>disk storage: features are written to disk </li>
<li>SPPnet: can only fine-tuning the fc layers, limits the accuracy of very deep networks</li>
</ul>
</li>
<li>task complexity:<ul>
<li>numerous candidate proposals</li>
<li>rough localization proposals must be refined </li>
</ul>
</li>
<li>We propose:<ul>
<li>a single-stage training algorithm </li>
<li>multi-task: jointly learns to classify object proposals and refine their spatial locations </li>
</ul>
</li>
</ul>
</li>
<li><p>要素</p>
<ul>
<li>input: an entire image and a set of object proposals </li>
<li>convs</li>
<li>a region of interest (RoI) pooling layer: extracts a fixed-length feature vector from the feature map</li>
<li>fcs that finally branch into two sibling output layers</li>
<li><p>multi-outputs:</p>
<ul>
<li>one produces softmax probability over K+1 classes</li>
<li>one outputs four bounding-box regression offsets per class</li>
</ul>
<p><img src="/2020/01/08/RCNN系列/fastRCNN.png" width="40%;"></p>
</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>RoI pooling</p>
<ul>
<li>an RoI is a rectangular window inside a conv feature map, which can be defined by (r, c, h, w)  </li>
<li>the RoI pooling layer converts the features inside any valid RoI into a small feature map with a fixed size H × W </li>
<li>it is a special case of SPPnet when there is only one pyramid level (pooling window size = h/H * w/W)</li>
</ul>
</li>
<li><p>Initializing from pre-trained networks</p>
<ul>
<li>the last max pooling layer is replaced by a RoI pooling layer </li>
<li>the last fully connected layer and softmax is replaced by the wo sibling layers + respective head (softmax &amp; regressor)</li>
<li>modified to take two inputs</li>
</ul>
</li>
<li><p>Fine-tuning for detection</p>
<ul>
<li><p>why SPPnet is unable to update weights below the spatial pyramid pooling layer: </p>
<ul>
<li><p>原文提到feature vector来源于不同尺寸的图像——不是主要原因</p>
</li>
<li><p>feature vector在原图上的感受野通常很大（接近全图）——forward pass的计算量就很大</p>
</li>
<li><p>不同的图片forward pass的计算结果不能复用（when each training sample (<em>i.e</em>. RoI) comes from a different image, which is exactly how R-CNN and SPPnet networks are trained）</p>
</li>
</ul>
</li>
<li><p>We propose:</p>
<ul>
<li><p>takes advantage of feature sharing </p>
</li>
<li><p>mini-batches are sampled hierarchically: N images and R/N RoIs from each image</p>
</li>
<li><p>RoIs from the same image share computation and memory in the forward and backward passes </p>
</li>
<li><p>jointly optimize the two tasks</p>
<p>  each RoI is labeled with a ground-truth class $u$ and a ground-truth bounding-box regression target $v$ </p>
<p>  the network outputs are K+1 probability $p=(p_0,…p_k)$ and K b-box regression offsets $t^k=(t_x^k, t_y^k, t_w^k,t_h^k)$</p>
<script type="math/tex; mode=display">
  L(p, u, t^u, v) = L_{cls}(p,u) + \lambda[u>0]L_{loc}(t^u,v)\\</script><p>  $L_{cls}$:</p>
<script type="math/tex; mode=display">
  L_{cls}(p,u) = -log p_u\\</script><p>  $L_{loc}$:</p>
<script type="math/tex; mode=display">
  L_{loc}(t^u, v) = \sum_{i \in \{x,y,w,h\}}smooth_{L_1}(t^u_i - v_i)\\
  smooth_{L_1}(x) = 
  \begin{cases}
  0.5x^2\ \ \ \ \ \ \ \ \ \ \ if |x|<1\\
  |x| - 0.5\ \ \ \ \ \ otherwise
  \end{cases}</script><p>  作者表示这种形式可以增强模型对异常数据的鲁棒性</p>
<p>  <img src="/2020/01/08/RCNN系列/smoothL1.png" width="30%;"></p>
</li>
<li><p>class label: take $IoU\geq0.5$ as a foreground object, take negatives with $IoU \in [0.1,0.5)$</p>
<p>  The lower threshold of 0.1 appears to act as a heuristic for hard example mining </p>
</li>
<li></li>
</ul>
</li>
</ul>
</li>
<li><p>Truncated SVD for faster detection</p>
<ul>
<li>Large fully connected layers are easily accelerated by compressing them with truncated SVD <script type="math/tex; mode=display">
  W \approx U \Sigma_t V^T</script></li>
</ul>
</li>
</ul>
</li>
</ol>
<ul>
<li><p>the single fully connected layer corresponding to W is replaced by two fully connected layers, <strong>without non-linearity</strong></p>
</li>
<li><p>The first layers uses the weight matrix $\Sigma_t V^T$(and no biases)</p>
</li>
<li><p>the second uses U (with the original biases)</p>
</li>
</ul>
<ol>
<li><p>分析</p>
<ul>
<li>Fast R-CNN vs. SPPnet: even though Fast R-CNN uses single-scale training and testing, <strong>fine-tuning the conv layers</strong> provides a large improvement in mAP </li>
<li>Truncated SVD can reduce detection time by more than 30% with only a small (0.3 percent- age point) drop in mAP </li>
<li>deep vs. small networks: <ul>
<li>for very deep networks fine-tuning the conv layers is important </li>
<li>in the smaller networks (S and M) we find that conv1 is generic and task independent </li>
<li>all Fast R-CNN results in this paper using models L fine-tune layers conv3_1 and up</li>
<li>all experiments with models S and M fine-tune layers conv2 and up </li>
</ul>
</li>
<li>multi-task training vs. stage-wise: it has the potential to improve results because the tasks influence each other through a shared representation (the ConvNet) </li>
<li>single-scale vs. multi-scale: <ul>
<li>single-scale detection performs almost as well as multi-scale detection </li>
<li>deep ConvNets are adept at directly learning scale invariance </li>
<li>single-scale processing offers the best tradeoff be- tween speed and accuracy thus we choose single-scale</li>
</ul>
</li>
<li>softmax vs. SVM:<ul>
<li>“one-shot” fine-tuning is sufficient compared to previous multi-stage training approaches</li>
<li>softmax introduces competition, while SVMs are one-vs-rest </li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks"><a href="#Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks" class="headerlink" title="Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"></a>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</h2><ol>
<li><p>动机</p>
<ul>
<li>shares the convolutional features</li>
<li>merge the system using the concept of “attention” mechanisms </li>
<li>sharing convolutions across proposals —-&gt; across tasks</li>
<li>translation-Invariant &amp; scale/ratio-Invariant</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>proposals are now the test-time computational bottleneck in state-of-the-art detection systems</li>
<li>the region proposal methods are generally implemented on the CPU</li>
<li>we observe that the convolutional feature maps used by region-based detectors, like Fast R- CNN, can also be used for generating region proposals</li>
</ul>
</li>
<li><p>要素</p>
<ul>
<li>RPN: On top of the convolutional features, we construct an RPN by adding a few additional convolutional layers that simultaneously regress region bounds and objectness scores at each location <strong>on a regular grid</strong></li>
<li>anchor: serves as references at multiple scales and aspect ratios </li>
<li><p>unify RPN and Fast R-CNN detector: we propose a training scheme that alternately fine-tuning the region proposal task and the object detection task</p>
<p><img src="/2020/01/08/RCNN系列/fasterRCNN.png" width="35%;"></p>
</li>
</ul>
</li>
<li><p>方法</p>
<p> 4.1 Region Proposal Networks</p>
<p> <img src="/2020/01/08/RCNN系列/RPN.png" width="65%;"></p>
<ul>
<li><p>This architecture is naturally implemented with an n×n convolutional layer followed by two sibling 1 × 1 convolutional layers (for <em>reg</em> and <em>cls</em>, respectively)</p>
</li>
<li><p>conv: an n × n sliding window </p>
</li>
<li><p>feature: 256-d for ZF(5 convs backbone) and 512-d for VGG(13 convs backbone)</p>
</li>
<li><p>two sibling fully-connected layers + respective output layer</p>
</li>
<li><p>anchors</p>
<ul>
<li>predict multiple region proposals: denoted as k</li>
<li>the reg head has 4k outputs, the cls head has 2k outputs</li>
<li>the k proposals are parameterized relative to <strong>k reference boxes</strong>————the anchors</li>
<li>an anchor box is centered at the sliding window in question, and is associated with a scale and aspect ratio </li>
<li>for a convolutional feature map of a size W × H , that is WHk anchors in total</li>
</ul>
</li>
<li><p>class label</p>
<ul>
<li>positives1: the anchors with the highest IoU with a ground-truth box </li>
<li>positives2: the anchors that has an IoU higher than 0.7 with any ground-truth box </li>
<li>negatives: non-positive anchors if their IoU is lower than 0.3 <strong>for all</strong> ground-truth boxes</li>
<li>the left: do not contribute </li>
<li>ignored: all cross-boundary anchors </li>
</ul>
</li>
<li><p>Loss function</p>
<ul>
<li><p>similar multi-task loss as fast-RCNN, with a normalization term</p>
</li>
<li><p>with $x,y,w,h$ denoting the box’s <strong>center coordinates</strong> and its width and height, the regression branch outputs $t_i$:</p>
<script type="math/tex; mode=display">
  t_x = (x - x_a) / w_a\\
  t_y = (y - y_a) / h_a\\
  t_w = log(w/ w_a)\\
  t_h = log(h/ h_a)</script></li>
</ul>
</li>
<li><p>mini-batch: sampled the positive and negative anchors from a single image with the ratio of 1:1 </p>
<p>4.2 the unified network </p>
</li>
<li><p>Alternating training</p>
<ul>
<li>ImageNet-pre-trained model, fine-tuning end-to-end for the region proposal task </li>
<li>ImageNet-pre-trained model, using the RPN proposals, fine-tuning end-to-end for the detection task </li>
<li>fixed detection network convs, fine-tuning the unique layers for region proposal </li>
<li>fixed detection network convs, fine-tuning the unique layers for detection</li>
</ul>
</li>
<li><p>Approximate joint training</p>
<ul>
<li>multi-task loss</li>
<li>approximate </li>
</ul>
<p>4.3 at training time</p>
</li>
<li><p>the total stride is 16 (input size / feature map size)</p>
</li>
<li>for a typical 1000 × 600 image, there will be roughly 20000 (60*40*9) anchors in total </li>
<li><p>we ignore all cross-boundary anchors, there will be about 6000 anchors per image left for training</p>
<p>4.4 at testing time</p>
</li>
<li><p>we use NMS(iou_thresh=0.7), that leaves 2000 proposals per image</p>
</li>
<li>then we use the top-N ranked proposal regions for detection</li>
</ul>
</li>
<li><p>分析</p>
</li>
</ol>
<h2 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h2><ol>
<li><p>动机</p>
<ul>
<li><strong>instance segmentation: </strong><ul>
<li>detects objects while simultaneously generating instance mask</li>
<li>注意不仅仅是目标检测了</li>
</ul>
</li>
<li>easy to generalize to other tasks: <ul>
<li>instance segmentation</li>
<li>bounding-box object detection</li>
<li>person keypoint detection</li>
</ul>
</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>challenging:<ul>
<li>requires the correct detection of objects</li>
<li>requires precisely segmentation of instances</li>
</ul>
</li>
<li><strong>a simple, flexible, and fast system can surpass all</strong><ul>
<li>adding a branch for predicting segmentation on Faster-RCNN</li>
<li>in parallel with the existing branch for classification and regression </li>
<li>the mask branch is a small FCN applied to each RoI</li>
</ul>
</li>
<li>Faster R- CNN was not designed for pixel-to-pixel alignment  </li>
<li><strong>we propose RoIAlign to preserve exact spatial locations</strong></li>
<li>FCNs usually perform per-pixel multi-class categorization, which couples segmentation and classification</li>
<li><p><strong>we predict a binary mask for each class independently, decouple mask(mask branch) and class(cls branch)</strong></p>
</li>
<li><p>other combining methods are multi-stage</p>
</li>
<li><strong>our method is based on parallel prediction</strong>  </li>
<li>FCIS also run the system in parallel but exhibits systematic errors on overlapping instances and creates spurious edges </li>
<li>segmentation-first strategies attempt to cut the pixels of the same category into different instances </li>
<li><strong>Mask R-CNN is based on an instance-first strategy</strong></li>
</ul>
</li>
<li><p>要素</p>
<ul>
<li>a mask branch with $Km^2$-dims outputs for each RoI, m denotes the resolution, K denotes the number of classes</li>
<li>bce is key for good instance segmentation results:  $L_{mask} = [y&gt;0]\frac{1}{m^2}\sum bce_loss$</li>
<li>RoI features that are well aligned to the per-pixel input </li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>RoIAlign</p>
<ul>
<li>Quantizations in RoIPool: (1) RoI to feature map $[x/16]$; (2) feature map to spatial bins $[a/b]$; $[]$ denotes roundings</li>
<li>These quantizations introduce misalignments </li>
<li><p>We use bilinear interpolation to avoid quantization</p>
<ul>
<li>sample several points in the spatial bins</li>
<li>computes the value of each sampling point by bilinear interpolation from the nearby grid points on the feature map</li>
<li>aggregate the results of sampling points (using max or average) </li>
</ul>
<p><img src="/2020/01/08/RCNN系列/RoIAlign.png" width="50%;"></p>
</li>
</ul>
</li>
<li><p>Architecture </p>
<ul>
<li>backbone: using a ResNet-FPN backbone for feature extraction gives excellent gains in both accuracy and speed </li>
<li><p>head: use previous heads in ResNet/FPN(res5 contained in head/backbone)</p>
<p><img src="/2020/01/08/RCNN系列/mrcnnheads.png" width="45%;"></p>
</li>
</ul>
</li>
<li><p>Implementation Details </p>
<ul>
<li>positives: RoIs with IoU at least 0.5, otherwise negative</li>
<li>loss: dice loss defined only on positive RoIs</li>
<li>mini-batch: 2 images, N RoIs</li>
<li>at training time: parallel computation  for 3 branches</li>
<li>at test time: <ul>
<li>serial computation</li>
<li>proposals -&gt; box prediction -&gt; NMS -&gt; run mask branch on the highest scoring 100 detection boxes  </li>
<li>it speeds up inference and improves accuracy </li>
<li>the $28*28$ floating-number mask output is resized to the RoI size, and binarized at a threshold of 0.5</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>分析</p>
<ul>
<li>on overlapping instances: FCIS+++ exhibits systematic artifacts </li>
<li>architecture: it benefits from deeper networks (50 vs. 101) and advanced designs including FPN and ResNeXt</li>
<li>FCN vs. MLP for mask branch</li>
<li><strong>Human Pose Estimation</strong><ul>
<li>We model a keypoint’s location as a <strong>one-hot mask</strong>, and adopt Mask R-CNN to predict K masks, one for each of K keypoint types </li>
<li>the training target is a one-hot $m<em>m$ binary mask where only a </em>single* pixel is labeled as foreground </li>
<li>use the cross-entropy loss </li>
<li>We found that a relatively high resolution output ($56*56$ compared to masks) is required for keypoint-level localization accuracy</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="FPN-Feature-Pyramid-Networks-for-Object-Detection"><a href="#FPN-Feature-Pyramid-Networks-for-Object-Detection" class="headerlink" title="FPN: Feature Pyramid Networks for Object Detection"></a>FPN: Feature Pyramid Networks for Object Detection</h2><ol>
<li><p>动机</p>
<ul>
<li>for object detection in multi-scale</li>
<li>struct feature pyramids with marginal extra cost</li>
<li>practical and accurate</li>
<li>leverage the pyramidal shape of a ConvNet’s feature hierarchy while creating a feature pyramid that has strong semantics at all scales</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>single scale offers a good trade-off between accuracy and speed while multi-scale still performs better, especially for small objects</li>
<li>featurized image pyramids form the basis solution for multi-scale</li>
<li>ConvNets are proved robust to variance in scale and thus facilitate recognition from features computed on a single input scale </li>
<li>SSD uses the naturely feature hierarchy generated by ConvNet which introduces large semantic gaps caused by different depths<ul>
<li>high-level features are low-resolution but <strong>semantically strong</strong></li>
<li>low-level features are of lower-level semantics, but their activations are more <strong>accurately localized</strong> as subsampled fewer times</li>
</ul>
</li>
<li><p>thus we propose FPN:</p>
<ul>
<li>combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections </li>
<li>has rich semantics at all levels </li>
<li>built from a single scale</li>
<li>can be easily extended to mask proposals</li>
<li>can be trained end-to- end with all scales </li>
</ul>
<p><img src="/2020/01/08/RCNN系列/pyramids.png" width="40%;"></p>
</li>
<li><p>similar architectures make predictions only on a fine resolution  </p>
<p><img src="/2020/01/08/RCNN系列/similar structs.png" width="40%;"></p>
</li>
</ul>
</li>
<li><p>要素</p>
<ul>
<li>takes a single-scale image of an arbitrary size as input </li>
<li>outputs proportionally sized feature maps at multiple levels</li>
<li>structure <ul>
<li>a bottom-up pathway:  the feed-forward computation of the backbone ConvNet</li>
<li>a top-down pathway and lateral connection: <ul>
<li>upsampling the spatially coarser, but semantically stronger, feature maps from higher pyramid levels</li>
<li>then enhance with features from the bottom-up pathway via lateral connections</li>
<li>a $3<em>3$ conv is appended on each merged map <em>*to reduce the aliasing effect of upsampling</em></em></li>
<li>shared classifiers/regressors among all levels, thus using fixed 256 channels convs</li>
<li>upsamling uses nearest neighbor interpolation</li>
<li>low-level features undergoes a $1*1$ conv to reduce channel dimensions </li>
<li>merge operation is a <strong>by element-wise addition</strong></li>
</ul>
</li>
</ul>
</li>
<li>adopt the method in RPN &amp; Fast-RCNN for demonstration</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>RPN</p>
<ul>
<li>original design: <ul>
<li>backbone Convs -&gt; single-scale feature map -&gt; dense 3×3 sliding windows -&gt; head($3<em>3$ convs + 2 sibling $1</em>1$ conv branches)</li>
<li>for regressor: multi-scale anchors(e.g. 3 scales 3 ratios -&gt; 9 anchors)</li>
</ul>
</li>
<li>new design: <ul>
<li>adapt FPN -&gt; multi-scale feature map -&gt; sharing heads</li>
<li>for regressor: set single-scale anchor for each level respectively (e.g. 5 level 3 ratios -&gt; 15 anchors)</li>
</ul>
</li>
<li>sharing heads:<ul>
<li>vs. not sharing: similar accuracy </li>
<li>indicates all levels of FPN share similar semantic levels (contrasted with naturally feature hierarchy of CNNs)</li>
</ul>
</li>
</ul>
</li>
<li><p>Fast  R-CNN</p>
<ul>
<li><p>original design: take the ROI feature map from the output of last conv layer</p>
</li>
<li><p>new design: take the specific level of ROI feature map based on ROI area</p>
<ul>
<li><p>with a $w*h$ ROI on the input image, $k_0$ refers to the target level on which an RoI with $w×h=224^2$ should be mapped into </p>
<script type="math/tex; mode=display">
k = [k_0 + log_2 (\sqrt{wh}/224)]</script></li>
<li><p>the smaller the ROI area, the lower the level k, the finer the resolution of the feature map</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>分析</p>
<ul>
<li>RPN<ul>
<li>use or not FPN: boost on small objects</li>
<li>use or not top-down pathway: semantic gaps</li>
<li>use or not lateral connection: locations </li>
<li>use or not multi-levels feature maps: <ul>
<li>using P2 alone leads to more anchors </li>
<li>more anchors are not sufficient to improve accuracy</li>
</ul>
</li>
</ul>
</li>
<li>Fast  R-CNN<ul>
<li>using P2 alone is <strong>marginally worse</strong> than that of using all pyramid levels </li>
<li>we argue that this is because <strong>RoI pooling is a warping-like operation</strong>, which is less sensitive to the region’s scales</li>
</ul>
</li>
<li><p>Faster R-CNN</p>
<ul>
<li>sharing features improves accuracy by a small margin</li>
<li>but reduces the testing time</li>
</ul>
</li>
<li><p><strong>Segmentation Proposals</strong></p>
<ul>
<li>use a fully convolutional setup for both training and inference </li>
<li><p>apply a small 5×5 MLP to predict 14×14 masks </p>
<p><img src="/2020/01/08/RCNN系列/FPNseg.png" width="40%;"></p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="衍生应用："><a href="#衍生应用：" class="headerlink" title="衍生应用："></a>衍生应用：<lung nodules="" detection="" and="" segmentation="" using="" 3d="" mask-rcnn=""></lung></h2><ol>
<li>动机<ul>
<li>3D volume detection and segmentation </li>
<li>ROI ／ full scan</li>
<li>LUNA16：lung nodules size evaluation </li>
</ul>
</li>
<li>论点<ul>
<li>variety among nodules &amp; similarity among non-nodules</li>
</ul>
</li>
<li>方法<ul>
<li>use overlapping sliding windows </li>
<li>use focal loss improve class result</li>
<li>use IOU loss improve mask result</li>
<li>use heavy augmentation</li>
</ul>
</li>
</ol>
<h2 id="Cascade-R-CNN-Delving-into-High-Quality-Object-Detection"><a href="#Cascade-R-CNN-Delving-into-High-Quality-Object-Detection" class="headerlink" title="Cascade R-CNN: Delving into High Quality Object Detection"></a>Cascade R-CNN: Delving into High Quality Object Detection</h2><ol>
<li><p>动机</p>
<ul>
<li>an detector trained with low IoU threshold usually produces noisy detections：低质量框issue</li>
<li>但是又不能简单地提高IoU threshold<ul>
<li>正样本会急剧减少，导致过拟合</li>
<li>inference-time mismatch，训练阶段只有高质量框，但是测试阶段啥质量框都有</li>
</ul>
</li>
<li>we propose Cascade R-CNN<ul>
<li>multi-stage object detection architecture</li>
<li>consists of a sequence of detectors trained with increasing IoU thresholds</li>
<li>trained stage by stage</li>
</ul>
</li>
<li>surpass all single-model on COCO</li>
</ul>
</li>
<li><p>论点</p>
<ul>
<li>object detections two main tasks<ul>
<li>recognition problem：foreground/backgroud &amp; object class</li>
<li>localization problem：bounding box</li>
<li>loose requirement for positives<ul>
<li>an low IoU thresh(0.5) is required to define positives/negatives：looss</li>
<li>noisy bounding boxes：close false positives</li>
</ul>
</li>
</ul>
</li>
<li>quality<ul>
<li>将一个框和gt的IoU定义为它的quality</li>
<li>将一个detector训练用的IoU thresh定义为它的quality</li>
<li>detector的quality和input proposals的quality是相关的：a single detector work on a specific quality level of hypotheses</li>
</ul>
</li>
<li>Cascade R-CNN<ul>
<li>multi-stage extension of R-CNN</li>
<li>sequentially more selective against close false positives</li>
</ul>
</li>
</ul>
</li>
<li><p>方法</p>
<ul>
<li><p>formulation</p>
<p> <img src="/2020/01/08/RCNN系列/formulation.png" width="80%;"></p>
<ul>
<li>first stage：<ul>
<li>proposal network H0</li>
<li>applied to entire image</li>
</ul>
</li>
<li>second stage<ul>
<li>region-of-interest detection sub-network H1 (detection head)</li>
<li>run on proposals</li>
</ul>
</li>
<li>C和B是classification score &amp; bounding box regression</li>
<li>we focus on modeling the second stage</li>
</ul>
</li>
<li><p>bounding box regression 针对回归质量</p>
<ul>
<li>an image patch $x$</li>
<li>a bounding box $b = (b_x, b_y, b_w, b_h)$</li>
<li>use a regressor $f(x,b)$ to fit the target $g$<ul>
<li>use L1 loss $L_{loc}(f(x_i,b_i), g_i)$</li>
<li>compute on 相对量 &amp; std normalization</li>
<li>invariant to scale and location</li>
<li>results in minor adjustments on $b$：所以regression loss通常比cls loss小得多</li>
</ul>
</li>
<li>Iterative BBox<ul>
<li>a single regression step is not sufficient for accurate localization</li>
<li>所以就搞了N个一样的regression heads串联</li>
<li>但还是那个问题：一个regressor只针对某一个quality level的proposals是performance optimal的，但是每个iteration以后框的distribution是剧烈变化的</li>
<li>所以迭代两次以上基本没有gain了</li>
</ul>
</li>
</ul>
</li>
<li><p>detection quality 针对分类质量</p>
<ul>
<li>an image patch x</li>
<li>M foreground classes and 1 background</li>
<li>use a classifier $h(x)$ to learn the target class label among M+1<ul>
<li>use CE $L_{cls}(h(x_i),y_i)$</li>
<li>the class label is determined by IoU thresh：如果image patch和gt box的IoU大于阈值，那么这个image patch的class label就是gt box的label，否则是背景</li>
<li>the IoU thresh defines the quality of a detector</li>
</ul>
</li>
<li>challenging<ul>
<li>如果阈值调高了，positives里面包含更少的背景（高质量前景），但是样本量少</li>
<li>如果阈值低了，前景样本多了，但是内容更加diversified，更难reject close false positives</li>
<li>所以一个分类器在不同的IoU阈值下，要面临不同的问题，在inference阶段，it is very difficult to perform uniformly well over all IoU levels</li>
</ul>
</li>
<li>Integral loss<ul>
<li>训练好几个分类器，针对不同的IoU level，然后inference阶段ensemble</li>
<li>还是没有解决高IoU阈值的那个分类器会因为样本量少过拟合的问题，而且高质量分类器在infernce阶段还是要处理所有的低质量框</li>
</ul>
</li>
</ul>
</li>
<li><p>Cascade R-CNN</p>
<ul>
<li>Cascaded Bounding Box Regression<ul>
<li>cascade specialized regressors</li>
<li>differs from Iterative BBox<ul>
<li>Iterative BBox是个后处理手段，一个regressor在0.5level的boxes上面优化，然后在inference proposals上面反复迭代</li>
<li>Cascade R-CNN是个resampling method，多个不同的regressor级连，训练测试同操作同分布</li>
</ul>
</li>
</ul>
</li>
<li><p>Cascaded Detection</p>
<ul>
<li>resamping manner<ul>
<li>keep上一阶段的positives</li>
<li>同时丢掉一些outliers</li>
</ul>
</li>
<li>实现就是每个stage的IoU threshold逐渐提高</li>
<li><p>loss还是所有proposals的cls loss + 定义为前景proposals的reg loss</p>
</li>
<li><p>outliers</p>
<p> <img src="/2020/01/08/RCNN系列/outlier.png" width="40%;"></p>
</li>
<li><p>proposal quality</p>
<p> <img src="/2020/01/08/RCNN系列/quality.png" width="50%;"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/目标检测，two-stage/" rel="tag"># 目标检测，two-stage</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/01/03/CNN-Visualization系列/" rel="next" title="CNN Visualization系列">
                <i class="fa fa-chevron-left"></i> CNN Visualization系列
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/02/24/Deeplab系列/" rel="prev" title="Deeplab系列">
                Deeplab系列 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpeg"
                alt="amber.zhang" />
            
              <p class="site-author-name" itemprop="name">amber.zhang</p>
              <p class="site-description motion-element" itemprop="description">要糖有糖，要猫有猫</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">139</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">76</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/AmberzzZZ" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-globe"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#综述"><span class="nav-number">1.</span> <span class="nav-text">综述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation"><span class="nav-number">2.</span> <span class="nav-text">R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SPP-net-Spatial-Pyramid-Pooling-in-Deep-Convolutional-Networks-for-Visual-Recognition"><span class="nav-number">3.</span> <span class="nav-text">SPP-net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fast-R-CNN-Fast-Region-based-Convolutional-Network"><span class="nav-number">4.</span> <span class="nav-text">Fast R-CNN: Fast Region-based Convolutional Network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks"><span class="nav-number">5.</span> <span class="nav-text">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Mask-R-CNN"><span class="nav-number">6.</span> <span class="nav-text">Mask R-CNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FPN-Feature-Pyramid-Networks-for-Object-Detection"><span class="nav-number">7.</span> <span class="nav-text">FPN: Feature Pyramid Networks for Object Detection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#衍生应用："><span class="nav-number">8.</span> <span class="nav-text">衍生应用：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cascade-R-CNN-Delving-into-High-Quality-Object-Detection"><span class="nav-number">9.</span> <span class="nav-text">Cascade R-CNN: Delving into High Quality Object Detection</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">amber.zhang</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'u9EV4x74hJaETIaNF0uX3490-gzGzoHsz',
        appKey: 'asMAPmAVtavwP5Orm1xcyxxK',
        placeholder: 'leave your comment ...',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->





  

  

  

  
  

  
  


  

  

</body>
</html>
